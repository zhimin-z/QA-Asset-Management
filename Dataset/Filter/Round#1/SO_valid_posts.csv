Question_title,Question_body,Question_answer_count,Question_comment_count,Question_creation_date,Question_favorite_count,Question_last_edit_date,Question_score,Question_tags,Question_view_count,Owner_creation_date,Owner_last_access_date,Owner_location,Owner_reputation,Owner_up_votes,Owner_down_votes,Owner_views,Answer_body,Answer_comment_count,Answer_creation_date,Answer_last_edit_date,Answer_score,Question_valid_tags
How to schedule tasks on SageMaker,<p>I have a notebook on SageMaker I would like to run every night. What's the best way to schedule this task. Is there a way to run a bash script and schedule Cron job from SageMaker?</p>,5,0,2018-03-30 22:37:21.927000 UTC,3.0,,14,amazon-web-services|jupyter-notebook|amazon-sagemaker,17339,2014-12-31 04:45:02.893000 UTC,2021-11-29 22:53:47.930000 UTC,,173,15,0,25,"<p>Amazon SageMaker is a set of API that can help various machine learning and data science tasks. These API can be invoked from various sources, such as CLI, <a href=""https://aws.amazon.com/tools/"" rel=""noreferrer"">SDK</a> or specifically from schedule AWS Lambda functions (see here for documentation: <a href=""https://docs.aws.amazon.com/lambda/latest/dg/with-scheduled-events.html"" rel=""noreferrer"">https://docs.aws.amazon.com/lambda/latest/dg/with-scheduled-events.html</a> )</p>

<p>The main parts of Amazon SageMaker are notebook instances, training and tuning jobs, and model hosting for real-time predictions. Each one has different types of schedules that you might want to have. The most popular are:</p>

<ul>
<li><strong>Stopping and Starting Notebook Instances</strong> - Since the notebook instances are used for interactive ML models development, you don't really need them running during the nights or weekends. You can schedule a Lambda function to call the <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/API_StopNotebookInstance.html"" rel=""noreferrer"">stop-notebook-instance</a> API at the end of the working day (8PM, for example), and the <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/API_StartNotebookInstance.html"" rel=""noreferrer"">start-notebook-instance</a> API in the morning. Please note that you can also run crontab on the notebook instances (after opening the local terminal from the Jupyter interface).</li>
<li><strong>Refreshing an ML Model</strong> - Automating the re-training of models, on new data that is flowing into the system all the time, is a common issue that with SageMaker is easier to solve. Calling <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateTrainingJob.html"" rel=""noreferrer"">create-training-job</a> API from a scheduled Lambda function (or even from a <a href=""https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html"" rel=""noreferrer"">CloudWatch Event</a> that is monitoring the performance of the existing models), pointing to the S3 bucket where the old and new data resides, can <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateModel.html"" rel=""noreferrer"">create a refreshed model</a> that you can now deploy into an <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/API_UpdateEndpointWeightsAndCapacities.html"" rel=""noreferrer"">A/B testing environment</a> .</li>
</ul>

<p>----- UPDATE (thanks to @snat2100 comment) -----</p>

<ul>
<li><strong>Creating and Deleting Real-time Endpoints</strong> - If your realtime endpoints are not needed 24/7 (for example, serving internal company users working during workdays and hours), you can also <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateEndpoint.html"" rel=""noreferrer"">create the endpoints</a> in the morning and <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/API_DeleteEndpoint.html"" rel=""noreferrer"">delete them</a> at night. </li>
</ul>",2.0,2018-04-08 18:45:15.337000 UTC,2019-10-04 14:51:41.000000 UTC,19.0,['amazon-sagemaker']
How to update a previous run into MLFlow?,"<p>I would like to update previous runs done with MLFlow, ie. changing/updating a parameter value to accommodate a change in the implementation. Typical uses cases:</p>
<ul>
<li>Log runs using a parameter A, and much later, log parameters A and B. It would be useful to update the value of parameter B of previous runs using its default value.</li>
<li>&quot;Specialize&quot; a parameter. Implement a model using a boolean flag as a parameter. Update the implementation to take a string instead. Now we need to update the values of the parameter for the previous runs so that it stays consistent with the new behavior.</li>
<li>Correct a wrong parameter value loggued in the previous runs.</li>
</ul>
<p>It is not always easy to trash the whole experiment as I need to keep the previous runs for statistical purpose. I would like also not to generate new experiments just for a single new parameter, to keep a single database of runs.</p>
<p>What is the best way to do this?</p>",2,2,2020-10-05 13:04:07.533000 UTC,2.0,2020-12-12 16:01:03.850000 UTC,6,logging|data-science|mlflow,2834,2012-09-10 21:25:47.147000 UTC,2022-09-24 18:14:33.217000 UTC,,1022,1127,19,66,"<p>To add or correct a parameter, metric or artifact of an existing run, pass run_id instead of experiment_id to mlflow.start_run function</p>
<pre><code>with mlflow.start_run(run_id=&quot;your_run_id&quot;) as run:
    mlflow.log_param(&quot;p1&quot;,&quot;your_corrected_value&quot;)
    mlflow.log_metric(&quot;m1&quot;,42.0) # your corrected metrics
    mlflow.log_artifact(&quot;data_sample.html&quot;) # your corrected artifact file
</code></pre>
<p>You can correct, add to, or delete any MLflow run any time after it is complete. Get the run_id either from the UI or by using <a href=""https://www.mlflow.org/docs/latest/python_api/mlflow.html#mlflow.search_runs"" rel=""noreferrer"">mlflow.search_runs</a>.</p>
<p>Source: <a href=""https://towardsdatascience.com/5-tips-for-mlflow-experiment-tracking-c70ae117b03f"" rel=""noreferrer"">https://towardsdatascience.com/5-tips-for-mlflow-experiment-tracking-c70ae117b03f</a></p>",3.0,2020-12-02 14:45:49.240000 UTC,,10.0,['mlflow']
How to use azureml.core.runconfig.DockerConfiguration class in azureml.core.Environment or azureml.core.ScriptRunConfig class,"<p>I use Microsoft Azure Machine Learning (Azure-ml) to run my (python) experiments.</p>
<p>For specifying the VM and python environment I use:</p>
<pre><code>from azureml.core import Environment
from azureml.core import ScriptRunConfig

# Other imports and code...

# Specify VM and Python environment:
vm_env = Environment.from_conda_specification(name='my-test-env', file_path=PATH_TO_YAML_FILE)
vm_env.docker.enabled = True
vm_env.docker.base_image = 'mcr.microsoft.com/azureml/openmpi3.1.2-cuda10.2-cudnn7-ubuntu18.04'

# Finally, use the environment in the ScriptRunConfig:
src = ScriptRunConfig(source_directory=DEPLOY_CONTAINER_FOLDER_PATH,
                      script=SCRIPT_FILE_TO_EXECUTE,
                      arguments=EXECUTE_ARGUMENTS,
                      compute_target=compute_target,
                      environment=vm_env)
</code></pre>
<p>I get the following warning for the line <code>vm_env.docker.enabled = True</code>:</p>
<pre><code>'enabled' is deprecated. Please use the azureml.core.runconfig.DockerConfiguration object with the 'use_docker' param instead.
</code></pre>
<p>The documentation about the <a href=""https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.environment.dockersection?view=azure-ml-py"" rel=""noreferrer""><code>DockerSection Class</code></a> and <a href=""https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.runconfig.dockerconfiguration?view=azure-ml-py"" rel=""noreferrer""><code>DockerConfiguration Class</code></a> is not very clear about applying the <code>DockerConfiguration Class</code>.</p>
<p><strong>I can't figure out how to use the <code>azureml.core.runconfig.DockerConfiguration</code> object. Can someone provide me with an example? Thank you!</strong></p>",2,0,2021-05-04 14:59:03.337000 UTC,2.0,2021-05-10 07:10:19.490000 UTC,8,python|azure-machine-learning-service|azureml-python-sdk,2662,2017-02-15 12:44:09.613000 UTC,2022-09-24 19:22:12.820000 UTC,,755,3010,0,245,"<p>The <code>ScriptRunConfig</code> class now accepts a <code>docker_runtime_config</code> argument, which is where you pass the <code>DockerConfiguration</code> object.</p>
<p>So, the code would look something like this:</p>
<pre><code>from azureml.core import Environment
from azureml.core import ScriptRunConfig
from azureml.core.runconfig import DockerConfiguration

# Other imports and code...

# Specify VM and Python environment:
vm_env = Environment.from_conda_specification(name='my-test-env', file_path=PATH_TO_YAML_FILE)
vm_env.docker.base_image = 'mcr.microsoft.com/azureml/openmpi3.1.2-cuda10.2-cudnn7-ubuntu18.04'

docker_config = DockerConfiguration(use_docker=True)

# Finally, use the environment in the ScriptRunConfig:
src = ScriptRunConfig(source_directory=DEPLOY_CONTAINER_FOLDER_PATH,
                      script=SCRIPT_FILE_TO_EXECUTE,
                      arguments=EXECUTE_ARGUMENTS,
                      compute_target=compute_target,
                      environment=vm_env,
                      docker_runtime_config=docker_config)
</code></pre>",3.0,2021-05-06 14:23:42.350000 UTC,2021-05-10 09:22:15.880000 UTC,12.0,['azure-machine-learning-service']
What's the difference between regular and ml AWS EC2 instances?,"<p>I'm experimenting with <a href=""https://aws.amazon.com/sagemaker/"" rel=""nofollow noreferrer"">AWS Sagemaker</a> using a Free Tier account. According to the <a href=""https://aws.amazon.com/sagemaker/pricing/"" rel=""nofollow noreferrer"">Sagemaker pricing</a>, I can use 50 hours of m4.xlarge and m5.xlarge instances for training in the free tier. (I am safely within the two-month limit.) But when I attempt to train an algorithm with the XGBoost container using m5.xlarge, I get the error shown below the code.</p>
<p>Are the ml-type and non-ml-type instances the same with just a fancy prefix for those that one would use with Sagemaker or are they entirely different? The <a href=""https://aws.amazon.com/ec2/instance-types/"" rel=""nofollow noreferrer"">EC2 page</a> doesn't even list the ml instances.</p>
<pre><code>sess = sagemaker.Session()

xgb = sagemaker.estimator.Estimator(container,
                                    role, 
                                    instance_count=1, 
                                    instance_type='m5.xlarge',
                                    output_path=output_location,
                                    sagemaker_session=sess)
</code></pre>
<blockquote>
<p>ClientError: An error occurred (ValidationException) when calling the
CreateTrainingJob operation: 1 validation error detected: Value
'm5.xlarge' at 'resourceConfig.instanceType' failed to satisfy
constraint: Member must satisfy enum value set: [ml.p2.xlarge,
ml.m5.4xlarge, ml.m4.16xlarge, ml.p4d.24xlarge, ml.c5n.xlarge,
ml.p3.16xlarge, ml.m5.large, ml.p2.16xlarge, ml.c4.2xlarge,
ml.c5.2xlarge, ml.c4.4xlarge, ml.c5.4xlarge, ml.c5n.18xlarge,
ml.g4dn.xlarge, ml.g4dn.12xlarge, ml.c4.8xlarge, ml.g4dn.2xlarge,
ml.c5.9xlarge, ml.g4dn.4xlarge, ml.c5.xlarge, ml.g4dn.16xlarge,
ml.c4.xlarge, ml.g4dn.8xlarge, ml.c5n.2xlarge, ml.c5n.4xlarge,
ml.c5.18xlarge, ml.p3dn.24xlarge, ml.p3.2xlarge, ml.m5.xlarge,
ml.m4.10xlarge, ml.c5n.9xlarge, ml.m5.12xlarge, ml.m4.xlarge,
ml.m5.24xlarge, ml.m4.2xlarge, ml.p2.8xlarge, ml.m5.2xlarge,
ml.p3.8xlarge, ml.m4.4xlarge]</p>
</blockquote>",2,0,2020-12-13 13:13:01.387000 UTC,1.0,2020-12-15 13:34:25.823000 UTC,5,amazon-web-services|amazon-ec2|amazon-sagemaker,7690,2017-04-25 08:35:17.327000 UTC,2022-09-24 18:42:49.843000 UTC,,1152,20,8,113,"<p>The instances with the <code>ml</code> prefix are instance classes specifically for use in Sagemaker.</p>
<p>In addition to being used within the Sagemaker service, the instance will be running an AMI with all the necessary libraries and packages such as Jupyter.</p>",7.0,2020-12-13 13:18:12.453000 UTC,,12.0,['amazon-sagemaker']
How to use a pretrained model from s3 to predict some data?,"<p>I have trained a semantic segmentation model using the sagemaker and the out has been saved to a s3 bucket. I want to load this model from the s3 to predict some images in sagemaker. </p>

<p>I know how to predict if I leave the notebook instance running after the training as its just an easy deploy but doesn't really help if I want to use an older model.</p>

<p>I have looked at these sources and been able to come up with something myself but it doesn't work hence me being here:</p>

<p><a href=""https://course.fast.ai/deployment_amzn_sagemaker.html#deploy-to-sagemaker"" rel=""noreferrer"">https://course.fast.ai/deployment_amzn_sagemaker.html#deploy-to-sagemaker</a>
<a href=""https://aws.amazon.com/getting-started/tutorials/build-train-deploy-machine-learning-model-sagemaker/"" rel=""noreferrer"">https://aws.amazon.com/getting-started/tutorials/build-train-deploy-machine-learning-model-sagemaker/</a></p>

<p><a href=""https://sagemaker.readthedocs.io/en/stable/pipeline.html"" rel=""noreferrer"">https://sagemaker.readthedocs.io/en/stable/pipeline.html</a></p>

<p><a href=""https://github.com/awslabs/amazon-sagemaker-examples/blob/master/advanced_functionality/inference_pipeline_sparkml_xgboost_abalone/inference_pipeline_sparkml_xgboost_abalone.ipynb"" rel=""noreferrer"">https://github.com/awslabs/amazon-sagemaker-examples/blob/master/advanced_functionality/inference_pipeline_sparkml_xgboost_abalone/inference_pipeline_sparkml_xgboost_abalone.ipynb</a></p>

<p>My code is this:</p>

<pre class=""lang-py prettyprint-override""><code>from sagemaker.pipeline import PipelineModel
from sagemaker.model import Model

s3_model_bucket = 'bucket'
s3_model_key_prefix = 'prefix'
data = 's3://{}/{}/{}'.format(s3_model_bucket, s3_model_key_prefix, 'model.tar.gz')
models = ss_model.create_model() # ss_model is my sagemaker.estimator

model = PipelineModel(name=data, role=role, models= [models])
ss_predictor = model.deploy(initial_instance_count=1, instance_type='ml.c4.xlarge')
</code></pre>",2,0,2019-05-22 10:50:48.223000 UTC,1.0,2019-05-22 11:21:34.857000 UTC,6,python|amazon-web-services|amazon-s3|boto3|amazon-sagemaker,7404,2019-05-22 09:45:28.153000 UTC,2022-09-02 09:43:13.340000 UTC,"London, UK",79,6,0,16,"<p>You can actually instantiate a Python SDK <code>model</code> object from existing artifacts, and deploy it to an endpoint. This allows you to deploy a model from trained artifacts, without having to retrain in the notebook. For example, for the semantic segmentation model:</p>

<pre><code>trainedmodel = sagemaker.model.Model(
    model_data='s3://...model path here../model.tar.gz',
    image='685385470294.dkr.ecr.eu-west-1.amazonaws.com/semantic-segmentation:latest',  # example path for the semantic segmentation in eu-west-1
    role=role)  # your role here; could be different name

trainedmodel.deploy(initial_instance_count=1, instance_type='ml.c4.xlarge')
</code></pre>

<p>And similarly, you can instantiate a predictor object on a deployed endpoint from any authenticated client supporting the SDK, with the following command:</p>

<pre><code>predictor = sagemaker.predictor.RealTimePredictor(
    endpoint='endpoint name here',
    content_type='image/jpeg',
    accept='image/png')
</code></pre>

<p>More on those abstractions:</p>

<ul>
<li><code>Model</code>: <a href=""https://sagemaker.readthedocs.io/en/stable/model.html"" rel=""noreferrer"">https://sagemaker.readthedocs.io/en/stable/model.html</a></li>
<li><code>Predictor</code>:
<a href=""https://sagemaker.readthedocs.io/en/stable/predictors.html"" rel=""noreferrer"">https://sagemaker.readthedocs.io/en/stable/predictors.html</a></li>
</ul>",2.0,2019-05-23 14:25:59.713000 UTC,,13.0,['amazon-sagemaker']
"Panda AssertionError columns passed, passed data had 2 columns","<p>I am working on Azure ML implementation on text analytics with NLTK, the following execution is throwing </p>

<pre><code>AssertionError: 1 columns passed, passed data had 2 columns\r\nProcess returned with non-zero exit code 1
</code></pre>

<p>Below is the code </p>

<pre><code># The script MUST include the following function,
# which is the entry point for this module:
# Param&lt;dataframe1&gt;: a pandas.DataFrame
# Param&lt;dataframe2&gt;: a pandas.DataFrame
def azureml_main(dataframe1 = None, dataframe2 = None):
    # import required packages
    import pandas as pd
    import nltk
    import numpy as np
    # tokenize the review text and store the word corpus
    word_dict = {}
    token_list = []
    nltk.download(info_or_id='punkt', download_dir='C:/users/client/nltk_data')
    nltk.download(info_or_id='maxent_treebank_pos_tagger', download_dir='C:/users/client/nltk_data')
    for text in dataframe1[""tweet_text""]:
        tokens = nltk.word_tokenize(text.decode('utf8'))
        tagged = nltk.pos_tag(tokens)


      # convert feature vector to dataframe object
    dataframe_output = pd.DataFrame(tagged, columns=['Output'])
    return [dataframe_output]
</code></pre>

<p>Error is throwing here </p>

<pre><code> dataframe_output = pd.DataFrame(tagged, columns=['Output'])
</code></pre>

<p>I suspect this to be the tagged data type passed to dataframe, can some one let me know the right approach to add this to dataframe.</p>",1,0,2016-08-12 22:23:17.197000 UTC,3.0,,7,python|pandas|dataframe|nltk|azure-machine-learning-studio,48200,2013-06-11 04:20:18.390000 UTC,2022-09-18 05:28:20.357000 UTC,"Toronto, ON, Canada",1748,136,55,339,"<p>Try this:</p>

<pre><code>dataframe_output = pd.DataFrame(tagged, columns=['Output', 'temp'])
</code></pre>",0.0,2016-08-12 22:26:09.603000 UTC,,13.0,['azure-machine-learning-studio']
"Permission ""artifactregistry.repositories.downloadArtifacts"" denied on resource","<p>While the artifact repository was successfully creating, running a docker push to push the image to the google artifact registry fails with a permissions error even after granting all artifact permissions to the accounting I am using on gcloud cli.</p>
<p><strong>Command used to push image:</strong></p>
<pre><code>docker push us-central1-docker.pkg.dev/project-id/repo-name:v2
</code></pre>
<p><strong>Error message:</strong></p>
<pre><code>The push refers to repository [us-central1-docker.pkg.dev/project-id/repo-name]
6f6f4a472f31: Preparing
bc096d7549c4: Preparing
5f70bf18a086: Preparing
20bed28d4def: Preparing
2a3255c6d9fb: Preparing
3f5d38b4936d: Waiting
7be8268e2fb0: Waiting
b889a93a79dd: Waiting
9d4550089a93: Waiting
a7934564e6b9: Waiting
1b7cceb6a07c: Waiting
b274e8788e0c: Waiting
78658088978a: Waiting
denied: Permission &quot;artifactregistry.repositories.downloadArtifacts&quot; denied on resource &quot;projects/project-id/locations/us-central1/repositories/repo-name&quot; (or it may not exist)


</code></pre>",2,3,2022-05-15 20:00:57.243000 UTC,,2022-05-16 02:21:18.670000 UTC,12,python|google-cloud-platform|google-cloud-ml|google-cloud-vertex-ai|docker-push,5722,2015-03-21 06:55:29.353000 UTC,2022-09-02 13:58:42.187000 UTC,Bangkok,194,8,0,12,"<p>I was able to recreate your use case. This happens when you are trying to push an image on a <code>repository</code> in which its specific hostname (associated with it's repository location) is not yet  added to the credential helper configuration for authentication. You may refer to this <a href=""https://cloud.google.com/artifact-registry/docs/docker/authentication"" rel=""noreferrer"">Setting up authentication for Docker </a> as also provided by @DazWilkin in the comments for more details.</p>
<p>In my example, I was trying to push an image on a repository that has a location of <code>us-east1</code> and got the same error since it is not yet added to the credential helper configuration.
<a href=""https://i.stack.imgur.com/NQeIf.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/NQeIf.png"" alt=""enter image description here"" /></a></p>
<p>And after I ran the authentication using below command (specifically for us-east1 since it is the <code>location</code> of my repository), the image was successfully pushed:</p>
<pre><code>gcloud auth configure-docker us-east1-docker.pkg.dev
</code></pre>
<p><a href=""https://i.stack.imgur.com/q2Q9x.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/q2Q9x.png"" alt=""enter image description here"" /></a></p>
<p><em><strong>QUICK TIP</strong></em>: You may  get your authentication command specific for your repository when you open your desired repository in the <a href=""https://console.cloud.google.com/artifacts"" rel=""noreferrer"">console</a>, and then click on the <code>SETUP INSTRUCTIONS</code>.
<a href=""https://i.stack.imgur.com/KBjqa.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/KBjqa.png"" alt=""enter image description here"" /></a></p>",4.0,2022-05-16 06:40:03.067000 UTC,2022-06-05 22:36:23.460000 UTC,23.0,['google-cloud-vertex-ai']
"Pros and Cons of Amazon SageMaker VS. Amazon EMR, for deploying TensorFlow-based deep learning models?","<p>I want to build some <strong>neural network</strong> models for NLP and recommendation applications. The framework I want to use is <strong>TensorFlow</strong>. I plan to train these models and make predictions on Amazon web services. The application will be most likely <strong>distributed computing</strong>.</p>

<p>I am wondering what are the pros and cons of SageMaker and EMR for TensorFlow applications?</p>

<p>They both have TensorFlow integrated. </p>",2,0,2018-09-21 06:09:49.837000 UTC,1.0,2018-09-21 06:47:31.193000 UTC,8,amazon-web-services|tensorflow|amazon-emr|amazon-sagemaker,10776,2012-07-11 00:42:40.210000 UTC,2022-09-19 04:56:15.667000 UTC,,2832,482,1,368,"<p>In general terms, they serve different purposes.</p>

<p><a href=""https://aws.amazon.com/emr/"" rel=""noreferrer""><strong>EMR</strong></a> is when you need to process massive amounts of data and heavily rely on Spark, Hadoop, and MapReduce (EMR = Elastic MapReduce). Essentially, if your data is in large enough volume to make use of the efficiencies of Spark, Hadoop, Hive, HDFS, HBase and Pig stack then go with EMR.</p>

<p><strong>EMR Pros:</strong></p>

<ul>
<li>Generally, low cost compared to EC2 instances</li>
<li>As the name suggests Elastic meaning you can provision what you need when you need it</li>
<li>Hive, Pig, and HBase out of the box</li>
</ul>

<p><strong>EMR Cons:</strong></p>

<ul>
<li>You need a very specific use case to truly benefit from all the offerings in EMR. Most don't take advantage of its entire offering</li>
</ul>

<p><a href=""https://aws.amazon.com/sagemaker/"" rel=""noreferrer""><strong>SageMaker</strong></a> is an attempt to make Machine Learning easier and distributed. The marketplace provides out of the box algos and models for quick use. It's a great service if you conform to the workflows it enforces. Meaning creating training jobs, deploying inference endpoints </p>

<p><strong>SageMaker Pros:</strong></p>

<ul>
<li>Easy to get up and running with Notebooks</li>
<li>Rich marketplace to quickly try existing models</li>
<li>Many different example notebooks for popular algorithms</li>
<li>Predefined kernels that minimize configuration</li>
<li>Easy to deploy models</li>
<li>Allows you to distribute inference compute by deploying endpoints</li>
</ul>

<p><strong>SageMaker Cons:</strong></p>

<ul>
<li>Expensive!</li>
<li>Enforces a certain workflow making it hard to be fully custom</li>
<li>Expensive!</li>
</ul>",1.0,2019-11-13 17:08:24.093000 UTC,,10.0,['amazon-sagemaker']
How do I load python modules which are not available in Sagemaker?,<p>I want to install spacy which is not available as part of the Sagemaker platform. How should can I pip install it?</p>,2,2,2018-04-05 06:09:19.597000 UTC,,,2,amazon-web-services|amazon-sagemaker,2578,2014-09-17 16:42:55.307000 UTC,2022-09-22 07:49:15.917000 UTC,,1124,156,1,153,"<p>When creating you model, you can specify the requirements.txt as an environment variable. </p>

<p>For Eg. </p>

<pre><code>env = {
    'SAGEMAKER_REQUIREMENTS': 'requirements.txt', # path relative to `source_dir` below.
}
sagemaker_model = TensorFlowModel(model_data = 's3://mybucket/modelTarFile,
                                  role = role,
                                  entry_point = 'entry.py',
                                  code_location = 's3://mybucket/runtime-code/',
                                  source_dir = 'src',
                                  env = env,
                                  name = 'model_name',
                                  sagemaker_session = sagemaker_session,
                                 )
</code></pre>

<p>This would ensure that the requirements file is run after the docker container is created, before running any code on it. </p>",0.0,2018-04-05 15:23:55.117000 UTC,2018-04-05 16:22:03.343000 UTC,10.0,['amazon-sagemaker']
SageMaker and TensorFlow 2.0,"<p>What is the best way to run TensorFlow 2.0 with AWS Sagemeker?</p>

<p>As of today (Aug 7th, 2019) AWS does not provide TensorFlow 2.0 <a href=""https://github.com/aws/sagemaker-tensorflow-container"" rel=""noreferrer"">SageMaker containers</a>, so my understanding is that I need to build my own.</p>

<p>What is the best Base image to use? Example Dockerfile?</p>",4,1,2019-08-07 14:00:51.297000 UTC,1.0,2019-08-07 14:06:30.803000 UTC,13,tensorflow|amazon-sagemaker|tensorflow2.0,4136,2015-11-07 01:25:10.543000 UTC,2022-09-24 20:44:03.983000 UTC,"Toronto, Canada",3259,104,7,233,"<p>EDIT: <strong>Amazon SageMaker does now support TF 2.0 and higher.</strong></p>
<ul>
<li>SageMaker + TensorFlow docs: <a href=""https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/using_tf.html"" rel=""nofollow noreferrer"">https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/using_tf.html</a></li>
<li>Supported Tensorflow versions (and Docker URIs): <a href=""https://aws.amazon.com/releasenotes/available-deep-learning-containers-images"" rel=""nofollow noreferrer"">https://aws.amazon.com/releasenotes/available-deep-learning-containers-images</a></li>
</ul>
<hr />
<p><em>Original answer</em></p>
<p>Here is an example Dockerfile that uses <a href=""https://github.com/aws/sagemaker-containers"" rel=""nofollow noreferrer"">the underlying SageMaker Containers library</a> (this is what is used in the official pre-built Docker images):</p>
<pre><code>FROM tensorflow/tensorflow:2.0.0b1

RUN pip install sagemaker-containers

# Copies the training code inside the container
COPY train.py /opt/ml/code/train.py

# Defines train.py as script entrypoint
ENV SAGEMAKER_PROGRAM train.py
</code></pre>
<p>For more information on this approach, see <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/build-container-to-train-script-get-started.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/build-container-to-train-script-get-started.html</a></p>",4.0,2019-09-04 22:22:31.827000 UTC,2020-07-17 17:54:03.227000 UTC,10.0,['amazon-sagemaker']
Download an entire folder from AWS sagemaker to laptop,"<p>I have a folder with predicted masks on AWS Sagemaker. ( It has 4 folders inside it and lot of files inside those folders. ) I want to download the entire folder to my laptop. 
This might sound so simple and easy, but I could not find a way to do it. Appreciate any help.</p>

<p>Thanks</p>",2,0,2019-02-28 17:36:21.970000 UTC,8.0,,17,amazon-web-services|download|amazon-sagemaker,13015,2013-04-21 07:52:05.213000 UTC,2022-09-22 19:54:43.253000 UTC,"California, USA",440,149,1,24,"<p>You can do that by opening a terminal on sagemaker. Navigate to the path where your folder is. Run the command to zip it</p>

<pre><code>zip -r -X archive_name.zip folder_to_compress
</code></pre>

<p>You will find the zipped folder. You can then select it and download it.</p>",1.0,2019-02-28 17:45:26.143000 UTC,,35.0,['amazon-sagemaker']
Is there a way to get log the descriptive stats of a dataset using MLflow?,<p>Is there a way to get log the descriptive stats of a dataset using MLflow? If any could you please share the details?</p>,2,0,2019-04-24 04:52:09.530000 UTC,,,3,python|mlflow,4592,2014-09-22 04:46:57.027000 UTC,2022-09-03 08:12:58.187000 UTC,"Bengaluru, Karnataka, India",569,41,3,123,"<p>Generally speaking you can log arbitrary output from your code using the mlflow_log_artifact() function.  From <a href=""https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.log_artifact"" rel=""noreferrer"">the docs</a>:</p>
<blockquote>
<p><strong>mlflow.log_artifact(local_path, artifact_path=None)</strong>
Log a local file or directory as an artifact of the currently active run.</p>
</blockquote>
<blockquote>
<p><strong>Parameters:</strong><br />
<em>local_path</em> – Path to the file to write.
<em>artifact_path</em> – If provided, the directory in artifact_uri to write to.</p>
</blockquote>
<p>As an example, say you have your statistics in a pandas dataframe, <code>stat_df</code>.</p>
<pre><code>## Write csv from stats dataframe
stat_df.to_csv('dataset_statistics.csv')

## Log CSV to MLflow
mlflow.log_artifact('dataset_statistics.csv')
</code></pre>
<p>This will show up under the artifacts section of this MLflow run in the Tracking UI.  If you explore the docs further you'll see that you can also log an entire directory and the objects therein.  In general, MLflow provides you a lot of flexibility - anything you write to your file system you can track with MLflow.  Of course that doesn't mean you should. :)</p>",2.0,2019-05-08 01:32:42.457000 UTC,2021-01-26 04:34:50.617000 UTC,9.0,['mlflow']
Randomforest in amazon aws sagemaker?,"<p>I am looking to recreate a randomforest model built locally, and deploy it through sagemaker. The model is very basic, but for comparison I would like to use the same in sagemaker. I don't see randomforest among sagemaker's built in algorithms (which seems weird) - is my only option to go the route of <a href=""https://aws.amazon.com/blogs/machine-learning/train-and-host-scikit-learn-models-in-amazon-sagemaker-by-building-a-scikit-docker-container/"" rel=""nofollow noreferrer"">deploying my own custom model</a>? Still learning about containers, and it seems like a lot of work for something that is just a simple randomforestclassifier() call locally. I just want to baseline against the out of the box randomforest model, and show that it works the same when deployed through AWS sagemaker.</p>",2,1,2019-06-24 16:27:13.193000 UTC,2.0,,4,amazon-web-services|docker|containers|random-forest|amazon-sagemaker,4295,2015-01-15 17:43:03.700000 UTC,2022-08-23 22:54:25.603000 UTC,,1387,51,1,153,"<p><em>edit 03/30/2020: adding a link to the the <a href=""https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/scikit_learn_randomforest/Sklearn_on_SageMaker_end2end.ipynb"" rel=""noreferrer"">SageMaker Sklearn random forest demo</a></em></p>

<p><br/></p>

<p>in SageMaker you have 3 options to write scientific code:</p>

<ul>
<li><strong>Built-in algorithms</strong></li>
<li><strong>Open-source pre-written containers</strong> (available
for sklearn, tensorflow, pytorch, mxnet, chainer. Keras can be
written in the tensorflow and mxnet containers)</li>
<li><strong>Bring your own container</strong> (for R for example)</li>
</ul>

<p><strong>At the time of writing this post there is no random forest classifier nor regressor in the built-in library</strong>. There is an algorithm called <a href=""https://aws.amazon.com/blogs/machine-learning/use-the-built-in-amazon-sagemaker-random-cut-forest-algorithm-for-anomaly-detection/"" rel=""noreferrer"">Random Cut Forest</a> in the built-in library but it is an unsupervised algorithm for anomaly detection, a different use-case than the <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"" rel=""noreferrer"">scikit-learn random forest</a> used in a supervised fashion (also <a href=""https://stackoverflow.com/questions/56728230/aws-sagemaker-randomcutforest-rcf-vs-scikit-lean-randomforest-rf?noredirect=1&amp;lq=1"">answered in StackOverflow here</a>). But it is easy to use the open-source pre-written scikit-learn container to implement your own. There is a <a href=""https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/scikit_learn_randomforest/Sklearn_on_SageMaker_end2end.ipynb"" rel=""noreferrer"">demo showing how to use Sklearn's random forest in SageMaker</a>, with training orchestration bother from the high-level SDK and <code>boto3</code>. You can also use this other <a href=""https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/scikit_learn_iris/Scikit-learn%20Estimator%20Example%20With%20Batch%20Transform.ipynb"" rel=""noreferrer"">public sklearn-on-sagemaker demo</a> and change the model. A benefit of the pre-written containers over the ""Bring your own"" option is that the dockerfile is already written, and web serving stack too.</p>

<p>Regarding your surprise that Random Forest is not featured in the built-in algos, the library and its 18 algos already cover a rich set of use-cases. For example for supervised learning over structured data (the usual use-case for the random forest), if you want to stick to the built-ins, depending on your priorities (accuracy, inference latency, training scale, costs...) you can use SageMaker XGBoost (XGBoost has been winning tons of datamining competitions - every winning team in the top10 of the KDDcup 2015 used XGBoost <a href=""https://arxiv.org/pdf/1603.02754.pdf"" rel=""noreferrer"">according to the XGBoost paper</a> - and scales well) and linear learner, which is extremely fast at inference and can be trained at scale, in mini-batch fashion over GPU(s). <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/fact-machines-howitworks.html"" rel=""noreferrer"">Factorization Machines</a> (linear + 2nd degree interaction with weights being column embedding dot-products) and <a href=""https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-supports-knn-classification-and-regression/"" rel=""noreferrer"">SageMaker kNN</a> are other options. Also, things are not frozen in stone, and the list of built-in algorithms is being improved fast.</p>",1.0,2019-06-25 19:38:06.350000 UTC,2020-03-30 16:46:46.547000 UTC,10.0,['amazon-sagemaker']
Azure: plot without labels,"<p>Suppose I have a dataframe (myDataframe) with two column of values (third and fourth). I want to plot them in a bi-dimensional graph. If I do it in R it works, but it returns me a graph without labels when I run the script from Azure Machine Learning. Someone with ideas?</p>

<pre><code>...
plot(myDataframe[,3],myDataframe[,4], 
       main=""my title"",
       xlab= ""x""
       ylab= ""y"",
       col= ""blue"", pch = 19, cex = 0.1, lty = ""solid"", lwd = 2)

# lines(x,y=x, col=""yellow"")

# add LABELS
text(DF_relativo[,A], DF_relativo[,B], 
       labels=DF_relativo$names, cex= 0.7, pos=2)
...
</code></pre>",1,0,2016-01-22 13:48:38.060000 UTC,,,0,r|plot|label|azure-machine-learning-studio,91,2015-07-09 09:05:28.610000 UTC,2022-09-19 17:14:25.487000 UTC,"Colleferro, Italy",809,109,0,361,"<p>using ggplot2() in AzureML is bit different. It can use to have plots with labels. Here's the <a href=""https://gallery.cortanaintelligence.com/Experiment/b1c26728eb6c4e4d80dddceae992d653"" rel=""nofollow"">Cortana Intelligence gallery example</a> for the particular task.  </p>",0.0,2016-06-20 05:57:16.047000 UTC,,-1.0,['azure-machine-learning-studio']
How do I make this IAM role error in aws sagemaker go away?,"<p>I suspect this has to more to do with IAM roles than Sagemaker.</p>

<p>I'm following the example <a href=""https://github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/tensorflow/README.rst"" rel=""noreferrer"">here</a></p>

<p>Specifically, when it makes this call</p>

<pre><code>tf_estimator.fit('s3://bucket/path/to/training/data')
</code></pre>

<p>I get this error</p>

<pre><code>ClientError: An error occurred (AccessDenied) when calling the GetRole operation: User: arn:aws:sts::013772784144:assumed-role/AmazonSageMaker-ExecutionRole-20181022T195630/SageMaker is not authorized to perform: iam:GetRole on resource: role SageMakerRole
</code></pre>

<p>My notebook instance has an IAM role attached to it.
That role has the <code>AmazonSageMakerFullAccess</code> policy. It also has a custom policy that looks like this</p>

<pre><code>{
""Version"": ""2012-10-17"",
""Statement"": [
    {
        ""Effect"": ""Allow"",
        ""Action"": [
            ""s3:GetObject"",
            ""s3:PutObject"",
            ""s3:DeleteObject"",
            ""s3:ListBucket""
        ],
        ""Resource"": [
            ""arn:aws:s3:::*""
        ]
    }
]
</code></pre>

<p>}</p>

<p>My input files and .py script is in an s3 bucket with the phrase <code>sagemaker</code> in it.</p>

<p>What else am I missing?</p>",3,0,2018-11-22 02:27:05.410000 UTC,,,6,amazon-web-services|amazon-iam|amazon-sagemaker,8160,2011-10-21 21:58:08.810000 UTC,2022-09-17 00:51:12.053000 UTC,,4966,744,11,304,"<p>If you're running the example code on a SageMaker notebook instance, you can use the execution_role which has the <code>AmazonSageMakerFullAccess</code> attached.</p>
<pre><code>from sagemaker import get_execution_role
sagemaker_session = sagemaker.Session()
role = get_execution_role()
</code></pre>
<p>And you can pass this role when initializing <code>tf_estimator</code>.
You can check out the example <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-ex-role.html"" rel=""nofollow noreferrer"">here</a> for using <code>execution_role</code> with S3 on notebook instance.</p>",1.0,2018-11-23 22:08:24.777000 UTC,2021-05-06 09:35:48.347000 UTC,8.0,['amazon-sagemaker']
How to register model from the Azure ML Pipeline Script step,"<p>I am running the <code>pipeline.submit()</code> in AzureML, which has a <code>PythonScriptStep</code>.
Inside this step, I download a model from tensorflow-hub, retrain it and save it as a <code>.zip</code>, and finally, I would like to register it in the Azure ML.
But as inside the script I do not have a workspace, <code>Model.register()</code> is not the case.
So I am trying to use <code>Run.register_model()</code> method as below:</p>

<pre><code>os.replace(os.path.join('.', archive_name + '.zip'), 
           os.path.join('.', 'outputs', archive_name + '.zip'))

print(os.listdir('./outputs'))
print('========================')

run_context = Run.get_context()
finetuning_model = run_context.register_model(model_name='finetuning_similarity_model',
                                              model_path=os.path.join(archive_name+'.zip'),
                                              tags={},
                                              description=""Finetuning Similarity model"")
</code></pre>

<p>But then I have got an error:</p>

<blockquote>
  <p>ErrorResponse 
  {
      ""error"": {
          ""message"": ""Could not locate the provided model_path retrained.zip in the set of files uploaded to the run:</p>
</blockquote>

<p>despite I have the retrained <code>.zip</code> in the <code>./outputs</code> dir as we can see from the log:</p>

<pre><code>['retrained.zip']
========================
</code></pre>

<p>I guess that I am doing something wrong?</p>",2,0,2019-11-19 11:56:24.153000 UTC,2.0,2020-01-11 12:03:44.353000 UTC,7,python|azure-machine-learning-service,3429,2019-11-19 11:24:15.727000 UTC,2021-09-24 08:42:39.343000 UTC,,75,2,0,6,"<p>I was able to fix the same issue (<a href=""https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.exceptions.modelpathnotfoundexception?view=azure-ml-py"" rel=""noreferrer""><code>ModelPathNotFoundException</code></a>) by explicitly uploading the model into the run history record before trying to register the model:</p>

<pre><code>run.upload_file(""outputs/my_model.pickle"", ""outputs/my_model.pickle"")
</code></pre>

<p>Which I found surprising because this wasn't mentioned in many of the official examples and according to the <code>upload_file()</code> <a href=""https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.run.run?view=azure-ml-py#upload-file-name--path-or-stream-"" rel=""noreferrer"">documentation</a>:</p>

<blockquote>
  <p>Runs automatically capture file in the specified output directory, which defaults to ""./outputs"" for most run types. Use upload_file only when additional files need to be uploaded or an output directory is not specified.</p>
</blockquote>",2.0,2020-01-11 12:33:03.587000 UTC,2020-01-11 12:38:39.987000 UTC,14.0,['azure-machine-learning-service']
How do I use an environment in an ML Azure Pipeline,"<p><strong>Background</strong></p>

<p>I have created an ML Workspace environment from a conda <code>environment.yml</code> plus some docker config and environment variables. I can access it from within a Python notebook:</p>

<pre><code>env = Environment.get(workspace=ws, name='my-environment', version='1')
</code></pre>

<p>I can use this successfully to run a Python script as an experiment, i.e.</p>

<pre><code>runconfig = ScriptRunConfig(source_directory='script/', script='my-script.py', arguments=script_params)
runconfig.run_config.target = compute_target
runconfig.run_config.environment = env
run = exp.submit(runconfig)
</code></pre>

<p><strong>Problem</strong></p>

<p>I would now like to run this same script as a Pipeline, so that I can trigger multiple runs with different parameters. I have created the Pipeline as follows:</p>

<pre><code>pipeline_step = PythonScriptStep(
    source_directory='script', script_name='my-script.py',
    arguments=['-a', param1, '-b', param2],
    compute_target=compute_target,
    runconfig=runconfig
)
steps = [pipeline_step]
pipeline = Pipeline(workspace=ws, steps=steps)
pipeline.validate()
</code></pre>

<p>When I then try to run the Pipeline:</p>

<pre><code>pipeline_run = Experiment(ws, 'my_pipeline_run').submit(
    pipeline, pipeline_parameters={...}
)
</code></pre>

<p>I get the following error: <code>Response status code does not indicate success: 400 (Conda dependencies were not specified. Please make sure that all conda dependencies were specified i).</code></p>

<p>When I view the pipeline run in the Azure Portal it seems that the environment has not been picked up: none of my conda dependencies are configured, hence the code does not run. What am I doing wrong?</p>",1,0,2020-03-03 11:37:15.267000 UTC,,2020-03-03 23:14:33.840000 UTC,3,python|azure|azure-machine-learning-studio|azure-machine-learning-service,1972,2014-06-25 12:11:55.160000 UTC,2022-09-24 10:12:50.483000 UTC,"London, UK",1534,171,3,56,"<p>You're almost there, but you need to use <code>RunConfiguration</code> instead of <code>ScriptRunConfig</code>. More info <a href=""https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-getting-started.ipynb"" rel=""noreferrer"">here</a></p>

<pre class=""lang-py prettyprint-override""><code>from azureml.core.runconfig import RunConfiguration

env = Environment.get(workspace=ws, name='my-environment', version='1')
# create a new runconfig object
runconfig = RunConfiguration()
runconfig.environment = env

pipeline_step = PythonScriptStep(
    source_directory='script', script_name='my-script.py',
    arguments=['-a', param1, '-b', param2],
    compute_target=compute_target,
    runconfig=runconfig
)

pipeline = Pipeline(workspace=ws, steps=[pipeline_step])

pipeline_run = Experiment(ws, 'my_pipeline_run').submit(pipeline)
</code></pre>",0.0,2020-03-03 17:07:50.993000 UTC,,8.0,"['azure-machine-learning-studio', 'azure-machine-learning-service']"
Pip installation stuck in infinite loop if unresolvable conflicts in dependencies,"<p>Pip installation is stuck in an infinite loop if there are unresolvable conflicts in dependencies. To reproduce, <code>pip==20.3.0</code> and:</p>
<pre><code>pip install pyarrow==2.0.0 azureml-defaults==1.18.0
</code></pre>",1,6,2020-12-02 16:54:05.387000 UTC,,2020-12-23 16:06:48.113000 UTC,13,python|pip|azure-machine-learning-service,2146,2019-09-16 18:20:32.913000 UTC,2021-07-06 00:01:05.640000 UTC,"Redmond, WA, USA",133,1,0,3,"<p>Workarounds:</p>
<p>Local environment:
Downgrade pip to &lt; 20.3</p>
<p>Conda environment created from yaml:
This will be seen only if conda-forge is highest priority channel, anaconda channel doesn't have pip 20.3 (as of now). To mitigate the issue please explicitly specify pip&lt;20.3 (!=20.3 or =20.2.4 pin to other version) as a conda dependency in the conda specification file</p>
<p>AzureML experimentation:
Follow the case above to make sure pinned pip resulted as a conda dependency in the environment object, either from yml file or programmatically</p>",2.0,2020-12-02 17:02:08.563000 UTC,,12.0,['azure-machine-learning-service']
How to pass a request to sagemaker using postman,"<p>I've trained a model on sagemaker and have created the endpoint. I'm trying to invoke the endpoint using postman. But when training the model and even after that, I have not specified any header for the training data. I'm at a loss as to how to create payload while sending a post request to sagemaker</p>",2,0,2018-04-05 15:03:12.383000 UTC,2.0,,8,web-services|amazon-web-services|postman|amazon-sagemaker,6746,2014-09-17 16:42:55.307000 UTC,2022-09-22 07:49:15.917000 UTC,,1124,156,1,153,"<p>Once the endpoint is created, you can invoke it as any other restful service, with credentials and payload. </p>

<p>I am guessing, there could be two places where might be stuck. 
One could be, sending an actual PostMan Request with all the headers and everything. 
Newer version of Postman has AWS Signature as one of the Authorization types. You can use that to invoke the service. There are no other spacial headers required. Note that there is a bug in Postman still open (<a href=""https://github.com/postmanlabs/postman-app-support/issues/1663"" rel=""noreferrer"">issue-1663</a>) that only affects if you are a AWS federated account. Individual accounts should not be affected by this issue. </p>

<p>Or, you could be stuck at the actual payload. When you invoke the SageMaker endpoint, the payload is passed as is to the model. If you want to preprocess the input before feeding it to the model, you'd have to implement an input_fn method and specify that when instantiating the model. </p>

<p>You might also be able to invoke SageMaker endpoint using AWS SDK boto3 as follows </p>

<pre><code>import boto3
runtime= boto3.client('runtime.sagemaker')

payload = getImageData()


result  = runtime.invoke_endpoint(
    EndpointName='my_endpoint_name',
    Body=payload,
    ContentType='image/jpeg'
)
</code></pre>

<p>Hope this helps.</p>",0.0,2018-04-05 16:37:35.433000 UTC,,11.0,['amazon-sagemaker']
How to use SageMaker Estimator for model training and saving,"<p>The documentations of how to use SageMaker estimators are scattered around, sometimes obsolete, incorrect. Is there a one stop location which gives the comprehensive views of how to use SageMaker SDK Estimator to train and save models?</p>",1,0,2021-09-02 04:01:47.170000 UTC,14.0,,27,amazon-web-services|amazon-sagemaker,6655,2014-11-22 09:22:35.470000 UTC,2022-09-24 22:13:03.237000 UTC,,14749,641,62,968,"<h1>Answer</h1>
<p>There is no one such resource from AWS that provides the comprehensive view of how to use SageMaker SDK Estimator to train and save models.</p>
<h2>Alternative Overview Diagram</h2>
<p>I put a diagram and brief explanation to get the overview on how SageMaker Estimator runs a training.</p>
<ol>
<li><p>SageMaker sets up a docker container for a training job where:</p>
<ul>
<li>Environment variables are set as in <a href=""https://github.com/aws/sagemaker-containers#important-environment-variables"" rel=""noreferrer"">SageMaker Docker Container. Environment Variables</a>.</li>
<li>Training data is setup under <code>/opt/ml/input/data</code>.</li>
<li>Training script codes are setup under <code>/opt/ml/code</code>.</li>
<li><code>/opt/ml/model</code> and <code>/opt/ml/output</code> directories are setup to store training outputs.</li>
</ul>
</li>
</ol>
<pre><code>/opt/ml
├── input
│   ├── config
│   │   ├── hyperparameters.json  &lt;--- From Estimator hyperparameter arg
│   │   └── resourceConfig.json
│   └── data
│       └── &lt;channel_name&gt;        &lt;--- From Estimator fit method inputs arg
│           └── &lt;input data&gt;
├── code
│   └── &lt;code files&gt;              &lt;--- From Estimator src_dir arg
├── model
│   └── &lt;model files&gt;             &lt;--- Location to save the trained model artifacts
└── output
    └── failure                   &lt;--- Training job failure logs
</code></pre>
<ol start=""2"">
<li><p>SageMaker Estimator <code>fit(inputs)</code> method executes the training script. Estimator <code>hyperparameters</code> and <code>fit</code> method <code>inputs</code> are provided as its command line arguments.</p>
</li>
<li><p>The training script saves the model artifacts in the <code>/opt/ml/model</code> once the training is completed.</p>
</li>
<li><p>SageMaker archives the artifacts under <code>/opt/ml/model</code> into <code>model.tar.gz</code> and save it to the S3 location specified to <code>output_path</code> Estimator parameter.</p>
</li>
<li><p>You can set Estimator <code>metric_definitions</code> parameter to extract model metrics from the training logs. Then you can monitor the training progress in the SageMaker console metrics.</p>
</li>
</ol>
<p><a href=""https://i.stack.imgur.com/gi8bU.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/gi8bU.png"" alt=""enter image description here"" /></a></p>
<p>I believe AWS needs to stop mass-producing verbose, redundant, wordy, scattered, and obsolete documents. AWS needs to understand <strong>A picture is worth thousand words</strong>.</p>
<p>Have diagrams and piece document parts together in a <strong>context</strong> with a clear objective to achieve.</p>
<hr />
<h1>Problem</h1>
<p>AWS documentations need serious re-design and re-structuring. Just to understand <strong>how to train and save a model</strong> forces us going through dozens of scattered,  fragmented, verbose, redundant documentations, which are often obsolete, incomplete, and sometime incorrect.</p>
<p>It is well-summarized in <a href=""https://nandovillalba.medium.com/why-i-think-gcp-is-better-than-aws-ea78f9975bda"" rel=""noreferrer"">Why I think GCP is better than AWS</a>:</p>
<blockquote>
<p>It’s not that AWS is harder to use than GCP, it’s that <strong>it is needlessly hard</strong>; a disjointed, sprawl of infrastructure primitives with poor cohesion between them.  <br><br>
A challenge is nice, a confusing mess is not, and <strong>the problem with AWS is that a large part of your working hours will be spent untangling their documentation and weeding through features and products to find what you want</strong>, rather than focusing on cool interesting challenges.</p>
</blockquote>
<p>Especially the SageMaker team keeps changing implementations without updating documents. Its roll-out was also inconsistent, e.g. SDK version 2 was rolled out in the SageMaker Studio making the AWS examples in Github incompatible without announcing it. Whereas SageMaker instance still had SDK 1, hence code worked in Instance but not in Studio.</p>
<p>It is mind-boggling that we have to go through these many documents below to understand how to use the SageMaker SDK Estimator for training.</p>
<h2>Documents for Model Training</h2>
<ul>
<li><a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-training.html"" rel=""noreferrer"">Train a Model with Amazon SageMaker</a></li>
</ul>
<p>This document gives 20,000 feet overview of how SageMaker training but does not give any clue what to do.</p>
<ul>
<li><a href=""https://sagemaker-workshop.com/custom/containers.html"" rel=""noreferrer"">Running a container for Amazon SageMaker training</a></li>
</ul>
<p>This document gives an overview of how SageMaker training looks like. However, this is not up-to-date as it is based on <a href=""https://github.com/aws/sagemaker-containers"" rel=""noreferrer"">SageMaker Containers</a> which is obsolete.</p>
<blockquote>
<p>WARNING: This package has been deprecated. Please use the SageMaker Training Toolkit for model training and the SageMaker Inference Toolkit for model serving.</p>
</blockquote>
<ul>
<li><a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-train-model.html"" rel=""noreferrer"">Step 4: Train a Model</a></li>
</ul>
<p>This document layouts the steps for training.</p>
<blockquote>
<p>The Amazon SageMaker Python SDK provides framework estimators and generic estimators to train your model while orchestrating the machine learning (ML) lifecycle accessing the SageMaker features for training and the AWS infrastructures</p>
</blockquote>
<ul>
<li><a href=""https://sagemaker.readthedocs.io/en/stable/overview.html#train-a-model-with-the-sagemaker-python-sdk"" rel=""noreferrer"">Train a Model with the SageMaker Python SDK</a></li>
</ul>
<blockquote>
<p>To train a model by using the SageMaker Python SDK, you:</p>
<ul>
<li>Prepare a training script</li>
<li>Create an estimator</li>
<li>Call the fit method of the estimator</li>
</ul>
</blockquote>
<p>Finally this document gives concrete steps and ideas. However still missing comprehensiv details about Environment Variables, Directory structure in the SageMaker docker container**, S3 for uploading code, placing data, S3 where the trained model is saved, etc.</p>
<ul>
<li><a href=""https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/using_tf.html"" rel=""noreferrer"">Use TensorFlow with the SageMaker Python SDK</a></li>
</ul>
<p>This documents is focused on TensorFlow Estimator implementation steps. Use <a href=""https://github.com/aws/amazon-sagemaker-examples/blob/master/frameworks/tensorflow/get_started_mnist_train.ipynb"" rel=""noreferrer"">Training a Tensorflow Model on MNIST</a> Github example to accompany with to follow the actual implementation.</p>
<h2>Documents for passing parameters and data locations</h2>
<ul>
<li><a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo-running-container.html#your-algorithms-training-algo-running-container-inputdataconfig"" rel=""noreferrer"">How Amazon SageMaker Provides Training Information</a></li>
</ul>
<blockquote>
<p>This section explains how SageMaker makes training information, such as training data, hyperparameters, and other configuration information, available to your Docker container.</p>
</blockquote>
<p>This document finally gives the idea of how parameters and data are passed around but again, not comprehensive.</p>
<ul>
<li><a href=""https://github.com/aws/sagemaker-containers#important-environment-variables"" rel=""noreferrer"">SageMaker Docker Container Environment Variables</a></li>
</ul>
<p>This documentation is marked as <strong>deprecated</strong> but the only document which explains the SageMaker Environment Variables.</p>
<blockquote>
<h3>IMPORTANT ENVIRONMENT VARIABLES</h3>
<ul>
<li>SM_MODEL_DIR</li>
<li>SM_CHANNELS</li>
<li>SM_CHANNEL_{channel_name}</li>
<li>SM_HPS</li>
<li>SM_HP_{hyperparameter_name}</li>
<li>SM_CURRENT_HOST</li>
<li>SM_HOSTS</li>
<li>SM_NUM_GPUS</li>
</ul>
<h3>List of provided environment variables by SageMaker Containers</h3>
<ul>
<li>SM_NUM_CPUS</li>
<li>SM_LOG_LEVEL</li>
<li>SM_NETWORK_INTERFACE_NAME</li>
<li>SM_USER_ARGS</li>
<li>SM_INPUT_DIR</li>
<li>SM_INPUT_CONFIG_DIR</li>
<li>SM_OUTPUT_DATA_DIR</li>
<li>SM_RESOURCE_CONFIG</li>
<li>SM_INPUT_DATA_CONFIG</li>
<li>SM_TRAINING_ENV</li>
</ul>
</blockquote>
<h2>Documents for SageMaker Docker Container Directory Structure</h2>
<ul>
<li><a href=""https://sagemaker-workshop.com/custom/containers.html"" rel=""noreferrer"">Running a container for Amazon SageMaker training</a></li>
</ul>
<pre><code>/opt/ml
├── input
│   ├── config
│   │   ├── hyperparameters.json
│   │   └── resourceConfig.json
│   └── data
│       └── &lt;channel_name&gt;
│           └── &lt;input data&gt;
├── model
│   └── &lt;model files&gt;
└── output
    └── failure
</code></pre>
<p>This document explains the directory structure and purpose of each directory.</p>
<blockquote>
<h3>The input</h3>
<ul>
<li>/opt/ml/input/config contains information to control how your program runs. hyperparameters.json is a JSON-formatted dictionary of hyperparameter names to values. These values will always be strings, so you may need to convert them. resourceConfig.json is a JSON-formatted file that describes the network layout used for distributed training. Since scikit-learn doesn’t support distributed training, we’ll ignore it here.</li>
<li>/opt/ml/input/data/&lt;channel_name&gt;/ (for File mode) contains the input data for that channel. The channels are created based on the call to CreateTrainingJob but it’s generally important that channels match what the algorithm expects. The files for each channel will be copied from S3 to this directory, preserving the tree structure indicated by the S3 key structure.</li>
<li>/opt/ml/input/data/&lt;channel_name&gt;_&lt;epoch_number&gt; (for Pipe mode) is the pipe for a given epoch. Epochs start at zero and go up by one each time you read them. There is no limit to the number of epochs that you can run, but you must close each pipe before reading the next epoch.</li>
</ul>
<h3>The output</h3>
<ul>
<li>/opt/ml/model/ is the directory where you write the model that your algorithm generates. Your model can be in any format that you want. It can be a single file or a whole directory tree. SageMaker will package any files in this directory into a compressed tar archive file. This file will be available at the S3 location returned in the DescribeTrainingJob result.</li>
<li>/opt/ml/output is a directory where the algorithm can write a file failure that describes why the job failed. The contents of this file will be returned in the FailureReason field of the DescribeTrainingJob result. For jobs that succeed, there is no reason to write this file as it will be ignored.</li>
</ul>
</blockquote>
<p>However, this is not up-to-date as it is based on <a href=""https://github.com/aws/sagemaker-containers"" rel=""noreferrer"">SageMaker Containers</a> which is obsolete.</p>
<h2>Documents for Model Saving</h2>
<p>The information on where the trained model is saved and in what format are fundamentally missing. The training script needs to save the model under <code>/opt/ml/model</code> and the format and sub-directory structure depend on the frameworks e,g TensorFlow, Pytorch. This is because SageMaker deployment uses the Framework dependent model-serving, e,g. TensorFlow Serving for TensorFlow framework.</p>
<p>This is not clearly documented and causing confusions. The developer needs to specify which format to use and under which sub-directory to save.</p>
<p>To use TensorFlow Estimator training and deployment:</p>
<ul>
<li><a href=""https://sagemaker-examples.readthedocs.io/en/latest/aws_sagemaker_studio/frameworks/keras_pipe_mode_horovod/keras_pipe_mode_horovod_cifar10.html#Deploy-the-trained-model"" rel=""noreferrer"">Deploy the trained model</a></li>
</ul>
<blockquote>
<p>Because <strong>we’re using TensorFlow Serving for deployment</strong>, our training script <strong>saves the model in TensorFlow’s SavedModel format</strong>.</p>
</blockquote>
<ul>
<li><a href=""https://github.com/aws/amazon-sagemaker-examples/blob/master/frameworks/tensorflow/code/train.py#L159-L166"" rel=""noreferrer"">amazon-sagemaker-examples/frameworks/tensorflow/code/train.py </a></li>
</ul>
<pre><code>    # Save the model
    # A version number is needed for the serving container
    # to load the model
    version = &quot;00000000&quot;
    ckpt_dir = os.path.join(args.model_dir, version)
    if not os.path.exists(ckpt_dir):
        os.makedirs(ckpt_dir)
    model.save(ckpt_dir)
</code></pre>
<p>The code is saving the model in <code>/opt/ml/model/00000000</code> because this is for TensorFlow serving.</p>
<ul>
<li><a href=""https://www.tensorflow.org/guide/saved_model"" rel=""noreferrer"">Using the SavedModel format</a></li>
</ul>
<blockquote>
<p>The save-path follows a convention used by TensorFlow Serving where the last path component (1/ here) is a version number for your model - it allows tools like Tensorflow Serving to reason about the relative freshness.</p>
</blockquote>
<ul>
<li><a href=""https://www.tensorflow.org/tfx/tutorials/serving/rest_simple#save_your_model"" rel=""noreferrer"">Train and serve a TensorFlow model with TensorFlow Serving</a></li>
</ul>
<blockquote>
<p>To load our trained model into TensorFlow Serving we first need to save it in SavedModel format. This will create a protobuf file in a well-defined directory hierarchy, and will include a version number. TensorFlow Serving allows us to select which version of a model, or &quot;servable&quot; we want to use when we make inference requests. Each version will be exported to a different sub-directory under the given path.</p>
</blockquote>
<h2>Documents for API</h2>
<p>Basically the SageMaker SDK Estimator implements the <a href=""https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateTrainingJob.html"" rel=""noreferrer"">CreateTrainingJob</a> API for training part. Hence, better to understand how it is designed and what parameters need to be defined. Otherwise working on Estimators are like walking in the dark.</p>
<hr />
<h1>Example</h1>
<h2>Jupyter Notebook</h2>
<pre><code>import sagemaker
from sagemaker import get_execution_role

sagemaker_session = sagemaker.Session()
role = get_execution_role()
bucket = sagemaker_session.default_bucket()

metric_definitions = [
    {&quot;Name&quot;: &quot;train:loss&quot;, &quot;Regex&quot;: &quot;.*loss: ([0-9\\.]+) - accuracy: [0-9\\.]+.*&quot;},
    {&quot;Name&quot;: &quot;train:accuracy&quot;, &quot;Regex&quot;: &quot;.*loss: [0-9\\.]+ - accuracy: ([0-9\\.]+).*&quot;},
    {
        &quot;Name&quot;: &quot;validation:accuracy&quot;,
        &quot;Regex&quot;: &quot;.*step - loss: [0-9\\.]+ - accuracy: [0-9\\.]+ - val_loss: [0-9\\.]+ - val_accuracy: ([0-9\\.]+).*&quot;,
    },
    {
        &quot;Name&quot;: &quot;validation:loss&quot;,
        &quot;Regex&quot;: &quot;.*step - loss: [0-9\\.]+ - accuracy: [0-9\\.]+ - val_loss: ([0-9\\.]+) - val_accuracy: [0-9\\.]+.*&quot;,
    },
    {
        &quot;Name&quot;: &quot;sec/sample&quot;,
        &quot;Regex&quot;: &quot;.* - \d+s (\d+)[mu]s/sample - loss: [0-9\\.]+ - accuracy: [0-9\\.]+ - val_loss: [0-9\\.]+ - val_accuracy: [0-9\\.]+&quot;,
    },
]

import uuid

checkpoint_s3_prefix = &quot;checkpoints/{}&quot;.format(str(uuid.uuid4()))
checkpoint_s3_uri = &quot;s3://{}/{}/&quot;.format(bucket, checkpoint_s3_prefix)

from sagemaker.tensorflow import TensorFlow

# --------------------------------------------------------------------------------
# 'trainingJobName' msut satisfy regular expression pattern: ^[a-zA-Z0-9](-*[a-zA-Z0-9]){0,62}
# --------------------------------------------------------------------------------
base_job_name = &quot;fashion-mnist&quot;
hyperparameters = {
    &quot;epochs&quot;: 2, 
    &quot;batch-size&quot;: 64
}
estimator = TensorFlow(
    entry_point=&quot;fashion_mnist.py&quot;,
    source_dir=&quot;src&quot;,
    metric_definitions=metric_definitions,
    hyperparameters=hyperparameters,
    role=role,
    input_mode='File',
    framework_version=&quot;2.3.1&quot;,
    py_version=&quot;py37&quot;,
    instance_count=1,
    instance_type=&quot;ml.m5.xlarge&quot;,
    base_job_name=base_job_name,
    checkpoint_s3_uri=checkpoint_s3_uri,
    model_dir=False
)
estimator.fit()
</code></pre>
<h2>fashion_mnist.py</h2>
<pre><code>import os
import argparse
import json
import multiprocessing

import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Flatten, BatchNormalization
from tensorflow.keras.layers import Conv2D, MaxPooling2D
from tensorflow.keras.layers.experimental.preprocessing import Normalization
from tensorflow.keras import backend as K

print(&quot;TensorFlow version: {}&quot;.format(tf.__version__))
print(&quot;Eager execution is: {}&quot;.format(tf.executing_eagerly()))
print(&quot;Keras version: {}&quot;.format(tf.keras.__version__))


image_width = 28
image_height = 28


def load_data():
    fashion_mnist = tf.keras.datasets.fashion_mnist
    (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()

    number_of_classes = len(set(y_train))
    print(&quot;number_of_classes&quot;, number_of_classes)

    x_train = x_train / 255.0
    x_test = x_test / 255.0
    x_full = np.concatenate((x_train, x_test), axis=0)
    print(x_full.shape)

    print(type(x_train))
    print(x_train.shape)
    print(x_train.dtype)
    print(y_train.shape)
    print(y_train.dtype)

    # ## Train
    # * C: Convolution layer
    # * P: Pooling layer
    # * B: Batch normalization layer
    # * F: Fully connected layer
    # * O: Output fully connected softmax layer

    # Reshape data based on channels first / channels last strategy.
    # This is dependent on whether you use TF, Theano or CNTK as backend.
    # Source: https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py
    if K.image_data_format() == 'channels_first':
        x = x_train.reshape(x_train.shape[0], 1, image_width, image_height)
        x_test = x_test.reshape(x_test.shape[0], 1, image_width, image_height)
        input_shape = (1, image_width, image_height)
    else:
        x_train = x_train.reshape(x_train.shape[0], image_width, image_height, 1)
        x_test = x_test.reshape(x_test.shape[0], image_width, image_height, 1)
        input_shape = (image_width, image_height, 1)

    return x_train, y_train, x_test, y_test, input_shape, number_of_classes

# tensorboard --logdir=/full_path_to_your_logs

validation_split = 0.2
verbosity = 1
use_multiprocessing = True
workers = multiprocessing.cpu_count()


def train(model, x, y, args):
    # SavedModel Output
    tensorflow_saved_model_path = os.path.join(args.model_dir, &quot;tensorflow/saved_model/0&quot;)
    os.makedirs(tensorflow_saved_model_path, exist_ok=True)

    # Tensorboard Logs
    tensorboard_logs_path = os.path.join(args.model_dir, &quot;tensorboard/&quot;)
    os.makedirs(tensorboard_logs_path, exist_ok=True)

    tensorboard_callback = tf.keras.callbacks.TensorBoard(
        log_dir=tensorboard_logs_path,
        write_graph=True,
        write_images=True,
        histogram_freq=1,  # How often to log histogram visualizations
        embeddings_freq=1,  # How often to log embedding visualizations
        update_freq=&quot;epoch&quot;,
    )  # How often to write logs (default: once per epoch)

    model.compile(
        optimizer='adam',
        loss=tf.keras.losses.sparse_categorical_crossentropy,
        metrics=['accuracy']
    )
    history = model.fit(
        x,
        y,
        shuffle=True,
        batch_size=args.batch_size,
        epochs=args.epochs,
        validation_split=validation_split,
        use_multiprocessing=use_multiprocessing,
        workers=workers,
        verbose=verbosity,
        callbacks=[
            tensorboard_callback
        ]
    )
    return history


def create_model(input_shape, number_of_classes):
    model = Sequential([
        Conv2D(
            name=&quot;conv01&quot;,
            filters=32,
            kernel_size=(3, 3),
            strides=(1, 1),
            padding=&quot;same&quot;,
            activation='relu',
            input_shape=input_shape
        ),
        MaxPooling2D(
            name=&quot;pool01&quot;,
            pool_size=(2, 2)
        ),
        Flatten(),  # 3D shape to 1D.
        BatchNormalization(
            name=&quot;batch_before_full01&quot;
        ),
        Dense(
            name=&quot;full01&quot;,
            units=300,
            activation=&quot;relu&quot;
        ),  # Fully connected layer
        Dense(
            name=&quot;output_softmax&quot;,
            units=number_of_classes,
            activation=&quot;softmax&quot;
        )
    ])
    return model


def save_model(model, args):
    # Save the model
    # A version number is needed for the serving container
    # to load the model
    version = &quot;00000000&quot;
    model_save_dir = os.path.join(args.model_dir, version)
    if not os.path.exists(model_save_dir):
        os.makedirs(model_save_dir)
    print(f&quot;saving model at {model_save_dir}&quot;)
    model.save(model_save_dir)


def parse_args():
    # --------------------------------------------------------------------------------
    # https://docs.python.org/dev/library/argparse.html#dest
    # --------------------------------------------------------------------------------
    parser = argparse.ArgumentParser()

    # --------------------------------------------------------------------------------
    # hyperparameters Estimator argument are passed as command-line arguments to the script.
    # --------------------------------------------------------------------------------
    parser.add_argument('--epochs', type=int, default=10)
    parser.add_argument('--batch-size', type=int, default=64)

    # /opt/ml/model
    # sagemaker.tensorflow.estimator.TensorFlow override 'model_dir'.
    # See https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/\
    # sagemaker.tensorflow.html#sagemaker.tensorflow.estimator.TensorFlow
    parser.add_argument('--model_dir', type=str, default=os.environ['SM_MODEL_DIR'])

    # /opt/ml/output
    parser.add_argument(&quot;--output_dir&quot;, type=str, default=os.environ[&quot;SM_OUTPUT_DIR&quot;])

    args = parser.parse_args()
    return args


if __name__ == &quot;__main__&quot;:
    args = parse_args()
    print(&quot;---------- key/value args&quot;)
    for key, value in vars(args).items():
        print(f&quot;{key}:{value}&quot;)

    x_train, y_train, x_test, y_test, input_shape, number_of_classes = load_data()
    model = create_model(input_shape, number_of_classes)

    history = train(model=model, x=x_train, y=y_train, args=args)
    print(history)
    
    save_model(model, args)
    results = model.evaluate(x_test, y_test, batch_size=100)
    print(&quot;test loss, test accuracy:&quot;, results)
</code></pre>
<h2>SageMaker Console</h2>
<p><a href=""https://i.stack.imgur.com/ctcLy.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/ctcLy.png"" alt=""enter image description here"" /></a></p>
<h2>Notebook output</h2>
<pre><code>2021-09-03 03:02:04 Starting - Starting the training job...
2021-09-03 03:02:16 Starting - Launching requested ML instancesProfilerReport-1630638122: InProgress
......
2021-09-03 03:03:17 Starting - Preparing the instances for training.........
2021-09-03 03:04:59 Downloading - Downloading input data
2021-09-03 03:04:59 Training - Downloading the training image...
2021-09-03 03:05:23 Training - Training image download completed. Training in progress.2021-09-03 03:05:23.966037: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.
2021-09-03 03:05:23.969704: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.
2021-09-03 03:05:24.118054: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.
2021-09-03 03:05:26,842 sagemaker-training-toolkit INFO     Imported framework sagemaker_tensorflow_container.training
2021-09-03 03:05:26,852 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)
2021-09-03 03:05:27,734 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:
/usr/local/bin/python3.7 -m pip install -r requirements.txt
WARNING: You are using pip version 21.0.1; however, version 21.2.4 is available.
You should consider upgrading via the '/usr/local/bin/python3.7 -m pip install --upgrade pip' command.

2021-09-03 03:05:29,028 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)
2021-09-03 03:05:29,045 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)
2021-09-03 03:05:29,062 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)
2021-09-03 03:05:29,072 sagemaker-training-toolkit INFO     Invoking user script

Training Env:

{
    &quot;additional_framework_parameters&quot;: {},
    &quot;channel_input_dirs&quot;: {},
    &quot;current_host&quot;: &quot;algo-1&quot;,
    &quot;framework_module&quot;: &quot;sagemaker_tensorflow_container.training:main&quot;,
    &quot;hosts&quot;: [
        &quot;algo-1&quot;
    ],
    &quot;hyperparameters&quot;: {
        &quot;batch-size&quot;: 64,
        &quot;epochs&quot;: 2
    },
    &quot;input_config_dir&quot;: &quot;/opt/ml/input/config&quot;,
    &quot;input_data_config&quot;: {},
    &quot;input_dir&quot;: &quot;/opt/ml/input&quot;,
    &quot;is_master&quot;: true,
    &quot;job_name&quot;: &quot;fashion-mnist-2021-09-03-03-02-02-305&quot;,
    &quot;log_level&quot;: 20,
    &quot;master_hostname&quot;: &quot;algo-1&quot;,
    &quot;model_dir&quot;: &quot;/opt/ml/model&quot;,
    &quot;module_dir&quot;: &quot;s3://sagemaker-us-east-1-316725000538/fashion-mnist-2021-09-03-03-02-02-305/source/sourcedir.tar.gz&quot;,
    &quot;module_name&quot;: &quot;fashion_mnist&quot;,
    &quot;network_interface_name&quot;: &quot;eth0&quot;,
    &quot;num_cpus&quot;: 4,
    &quot;num_gpus&quot;: 0,
    &quot;output_data_dir&quot;: &quot;/opt/ml/output/data&quot;,
    &quot;output_dir&quot;: &quot;/opt/ml/output&quot;,
    &quot;output_intermediate_dir&quot;: &quot;/opt/ml/output/intermediate&quot;,
    &quot;resource_config&quot;: {
        &quot;current_host&quot;: &quot;algo-1&quot;,
        &quot;hosts&quot;: [
            &quot;algo-1&quot;
        ],
        &quot;network_interface_name&quot;: &quot;eth0&quot;
    },
    &quot;user_entry_point&quot;: &quot;fashion_mnist.py&quot;
}

Environment variables:

SM_HOSTS=[&quot;algo-1&quot;]
SM_NETWORK_INTERFACE_NAME=eth0
SM_HPS={&quot;batch-size&quot;:64,&quot;epochs&quot;:2}
SM_USER_ENTRY_POINT=fashion_mnist.py
SM_FRAMEWORK_PARAMS={}
SM_RESOURCE_CONFIG={&quot;current_host&quot;:&quot;algo-1&quot;,&quot;hosts&quot;:[&quot;algo-1&quot;],&quot;network_interface_name&quot;:&quot;eth0&quot;}
SM_INPUT_DATA_CONFIG={}
SM_OUTPUT_DATA_DIR=/opt/ml/output/data
SM_CHANNELS=[]
SM_CURRENT_HOST=algo-1
SM_MODULE_NAME=fashion_mnist
SM_LOG_LEVEL=20
SM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main
SM_INPUT_DIR=/opt/ml/input
SM_INPUT_CONFIG_DIR=/opt/ml/input/config
SM_OUTPUT_DIR=/opt/ml/output
SM_NUM_CPUS=4
SM_NUM_GPUS=0
SM_MODEL_DIR=/opt/ml/model
SM_MODULE_DIR=s3://sagemaker-us-east-1-316725000538/fashion-mnist-2021-09-03-03-02-02-305/source/sourcedir.tar.gz
SM_TRAINING_ENV={&quot;additional_framework_parameters&quot;:{},&quot;channel_input_dirs&quot;:{},&quot;current_host&quot;:&quot;algo-1&quot;,&quot;framework_module&quot;:&quot;sagemaker_tensorflow_container.training:main&quot;,&quot;hosts&quot;:[&quot;algo-1&quot;],&quot;hyperparameters&quot;:{&quot;batch-size&quot;:64,&quot;epochs&quot;:2},&quot;input_config_dir&quot;:&quot;/opt/ml/input/config&quot;,&quot;input_data_config&quot;:{},&quot;input_dir&quot;:&quot;/opt/ml/input&quot;,&quot;is_master&quot;:true,&quot;job_name&quot;:&quot;fashion-mnist-2021-09-03-03-02-02-305&quot;,&quot;log_level&quot;:20,&quot;master_hostname&quot;:&quot;algo-1&quot;,&quot;model_dir&quot;:&quot;/opt/ml/model&quot;,&quot;module_dir&quot;:&quot;s3://sagemaker-us-east-1-316725000538/fashion-mnist-2021-09-03-03-02-02-305/source/sourcedir.tar.gz&quot;,&quot;module_name&quot;:&quot;fashion_mnist&quot;,&quot;network_interface_name&quot;:&quot;eth0&quot;,&quot;num_cpus&quot;:4,&quot;num_gpus&quot;:0,&quot;output_data_dir&quot;:&quot;/opt/ml/output/data&quot;,&quot;output_dir&quot;:&quot;/opt/ml/output&quot;,&quot;output_intermediate_dir&quot;:&quot;/opt/ml/output/intermediate&quot;,&quot;resource_config&quot;:{&quot;current_host&quot;:&quot;algo-1&quot;,&quot;hosts&quot;:[&quot;algo-1&quot;],&quot;network_interface_name&quot;:&quot;eth0&quot;},&quot;user_entry_point&quot;:&quot;fashion_mnist.py&quot;}
SM_USER_ARGS=[&quot;--batch-size&quot;,&quot;64&quot;,&quot;--epochs&quot;,&quot;2&quot;]
SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate
SM_HP_BATCH-SIZE=64
SM_HP_EPOCHS=2
PYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/local/lib/python37.zip:/usr/local/lib/python3.7:/usr/local/lib/python3.7/lib-dynload:/usr/local/lib/python3.7/site-packages

Invoking script with the following command:

/usr/local/bin/python3.7 fashion_mnist.py --batch-size 64 --epochs 2


TensorFlow version: 2.3.1
Eager execution is: True
Keras version: 2.4.0
---------- key/value args
epochs:2
batch_size:64
model_dir:/opt/ml/model
output_dir:/opt/ml/output
</code></pre>",4.0,2021-09-02 04:01:47.170000 UTC,2022-07-04 05:43:30.063000 UTC,65.0,['amazon-sagemaker']
Azure Machine Learning Request Response latency,"<p>I have made an Azure Machine Learning Experiment which takes a small dataset (12x3 array) and some parameters and does some calculations using a few Python modules (a linear regression calculation and some more). This all works fine.</p>

<p>I have deployed the experiment and now want to throw data at it from the front-end of my application. The API-call goes in and comes back with correct results, but it takes up to 30 seconds to calculate a simple linear regression. Sometimes it is 20 seconds, sometimes only 1 second. I even got it down to 100 ms one time (which is what I'd like), but 90% of the time the request takes more than 20 seconds to complete, which is unacceptable.</p>

<p>I guess it has something to do with it still being an experiment, or it is still in a development slot, but I can't find the settings to get it to run on a faster machine.</p>

<p>Is there a way to speed up my execution?</p>

<p>Edit: To clarify: The varying timings are obtained with the same test data, simply by sending the same request multiple times. This made me conclude it must have something to do with my request being put in a queue, there is some start-up latency or I'm throttled in some other way.</p>",1,0,2016-01-25 10:40:39.993000 UTC,5.0,2016-01-27 16:15:36.527000 UTC,8,python|azure|azure-machine-learning-studio,1128,2015-10-29 11:07:20.793000 UTC,2020-05-19 23:12:48.357000 UTC,"Antwerp, Belgium",311,18,0,34,"<p>First, I am assuming you are doing your timing test on the published AML endpoint.</p>

<p>When a call is made to the AML the first call must warm up the container. By default a web service has 20 containers. Each container is cold, and a cold container can cause a large(30 sec) delay. In the string returned by the AML endpoint, only count requests that have the <code>isWarm</code> flag set to true. By smashing the service with MANY requests(relative to how many containers you have running) can get all your containers warmed.</p>

<p>If you are sending out dozens of requests a instance, the endpoint might be getting throttled. You can adjust the number of calls your endpoint can accept by going to manage.windowsazure.com/</p>

<ol>
<li>manage.windowsazure.com/</li>
<li>Azure ML Section from left bar</li>
<li>select your workspace</li>
<li>go to web services tab</li>
<li>Select your web service from list</li>
<li>adjust the number of calls with slider</li>
</ol>

<p>By enabling debugging onto your endpoint you can get logs about the execution time for each of your modules to complete. You can use this to determine if a module is not running as you intended which may add to the time.</p>

<p>Overall, there is an overhead when using the Execute python module, but I'd expect this request to complete in under 3 secs. </p>",11.0,2016-01-26 18:20:06.127000 UTC,2016-01-27 16:10:48.927000 UTC,8.0,['azure-machine-learning-studio']
How to run authentication on a mlFlow server?,"<p>As I am logging my entire models and params into mlflow I thought it will be a good idea to have  it protected under a user name and password.</p>

<p>I use the following code to run the mlflow server</p>

<p><code>mlflow server --host 0.0.0.0 --port 11111</code>
works perfect,in mybrowser i type <code>myip:11111</code> and i see everything (which eventually is the problem)</p>

<p>If I understood the documentation and the following <a href=""https://groups.google.com/forum/#!topic/mlflow-users/E9QW4HdS8a8"" rel=""noreferrer"">https://groups.google.com/forum/#!topic/mlflow-users/E9QW4HdS8a8</a> link here correct, I should use nginx to create the authentication.</p>

<p>I installed <code>nginx open sourcre</code>  and <code>apache2-utils</code></p>

<p>created <code>sudo htpasswd -c /etc/apache2/.htpasswd user1</code> user and passwords.</p>

<p>I edited my <code>/etc/nginx/nginx.conf</code> to the following:</p>

<pre><code>server {
        listen 80;
        listen 443 ssl;

        server_name my_ip;
        root NOT_SURE_WHICH_PATH_TO_PUT_HERE, THE VENV?;
        location / {
            proxy_pass                      my_ip:11111/;
            auth_basic                      ""Restricted Content"";
            auth_basic_user_file /home/path to the password file/.htpasswd;
        }
    }
</code></pre>

<p><strong>but no authentication appears.</strong></p>

<p>if I change the conf to listen to  <code>listen 11111</code>
I get an error that the port is already in use ( of course, by the mlflow server....)</p>

<p>my wish is to have a authentication window before anyone can enter by the mlflow with a browser.</p>

<p>would be happy to hear any suggestions.</p>",4,1,2019-11-20 14:16:40.087000 UTC,3.0,,11,nginx|basic-authentication|mlflow,13870,2019-04-03 13:42:48.017000 UTC,2022-09-10 19:35:30.057000 UTC,wondeland,1540,21,3,118,"<p>the problem here is that both <code>mlflow</code> and <code>nginx</code> are trying to run on the <strong>same port</strong>... </p>

<ol>
<li><p>first lets deal with nginx:</p>

<p>1.1 in /etc/nginx/sites-enable make a new file <code>sudo nano mlflow</code> and delete the exist default.</p>

<p>1.2 in mlflow file:</p></li>
</ol>

<pre><code>server {
    listen YOUR_PORT;
    server_name YOUR_IP_OR_DOMAIN;
    auth_basic           “Administrator’s Area”;
    auth_basic_user_file /etc/apache2/.htpasswd; #read the link below how to set username and pwd in nginx

    location / {
        proxy_pass http://localhost:8000;
        include /etc/nginx/proxy_params;
        proxy_redirect off;
    }
}
</code></pre>

<p>1.3.  restart nginx <code>sudo systemctl restart nginx</code></p>

<ol start=""2"">
<li>on your server run mlflow  <code>mlflow server --host localhost --port 8000</code></li>
</ol>

<p>Now if you try access the YOUR_IP_OR_DOMAIN:YOUR_PORT within your browser an auth popup should appear, enter your host and pass and now you in mlflow</p>

<ol start=""3"">
<li><p>now there are 2 options to tell the mlflow server about it:</p>

<p>3.1 set username and pwd as environment variable 
<code>export MLFLOW_TRACKING_USERNAME=user export MLFLOW_TRACKING_PASSWORD=pwd</code></p>

<p>3.2 edit in your <code>/venv/lib/python3.6/site-packages/mlflowpackages/mlflow/tracking/_tracking_service/utils.py</code> the function </p></li>
</ol>

<pre><code>def _get_rest_store(store_uri, **_):
    def get_default_host_creds():
        return rest_utils.MlflowHostCreds(
            host=store_uri,
            username=replace with nginx user
            password=replace with nginx pwd
            token=os.environ.get(_TRACKING_TOKEN_ENV_VAR),
            ignore_tls_verification=os.environ.get(_TRACKING_INSECURE_TLS_ENV_VAR) == 'true',
        )
</code></pre>

<p>in your .py file where you work with mlflow:</p>

<pre><code>import mlflow
remote_server_uri = ""YOUR_IP_OR_DOMAIN:YOUR_PORT"" # set to your server URI
mlflow.set_tracking_uri(remote_server_uri)
mlflow.set_experiment(""/my-experiment"")
with mlflow.start_run():
    mlflow.log_param(""a"", 1)
    mlflow.log_metric(""b"", 2)
</code></pre>

<p>A link to nginx authentication doc <a href=""https://docs.nginx.com/nginx/admin-guide/security-controls/configuring-http-basic-authentication/"" rel=""noreferrer"">https://docs.nginx.com/nginx/admin-guide/security-controls/configuring-http-basic-authentication/</a></p>",3.0,2019-12-13 16:37:32.617000 UTC,,8.0,['mlflow']
How to download the entire scored dataset from Azure machine studio?,"<p>I have an experiment in azure machine learning studio, and I would like to the see entire scored dataset.</p>

<p>Naturally I used the 'visualise' option on the scored dataset but these yields only 100 rows (the test dataset is around 500 rows)</p>

<p>I also tired the 'save as dataset' option, but then file does not open well with excel or text editor (special character encoding)</p>

<p>Basically, I want to see the entire test data with scored labels as table or download as .csv maybe</p>",2,0,2016-04-12 04:42:39.040000 UTC,,,1,azure-machine-learning-studio,5296,2014-07-25 05:27:39.940000 UTC,2022-09-15 08:56:29.147000 UTC,"Linköping, Sweden",1677,82,2,221,"<p>Please try the Convert to CSV module: <a href=""https://msdn.microsoft.com/library/azure/faa6ba63-383c-4086-ba58-7abf26b85814"" rel=""noreferrer"">https://msdn.microsoft.com/library/azure/faa6ba63-383c-4086-ba58-7abf26b85814</a></p>

<p>After you run the experiment, right click on the output of the module to download the CSV file.</p>",1.0,2016-04-12 04:57:38.280000 UTC,,14.0,['azure-machine-learning-studio']
Azure ML Loops through the different tasks,"<p>I've got the following data structure in an Azure DB Table:</p>

<pre><code>Client_ID | Customer_ID | Item | Preference_Score
</code></pre>

<p>The table can contain different datasets from different clients but the data structure is always the same. Then, the table is imported in Azure ML.</p>

<p>What I need is to repeat the same sequence of tasks in Azure ML for all the Client_ID in the above mentioned table.</p>

<p>So that in the end I will train a single model for each client and score the data of each single client individually and append the scored data and store it again in Azure SQL.</p>

<p>Is there any for each task in Azure ML like in SSIS? What's the best way to do this? </p>

<p>Thanks.</p>",1,0,2016-06-14 07:05:08.690000 UTC,,,1,loops|azure|machine-learning|azure-machine-learning-studio,575,2015-03-31 12:18:20.743000 UTC,2022-09-22 23:58:12.317000 UTC,,983,153,2,259,"<p>You can start from Azure Data Factory for automating batch scoring. In your model instead of web service output, you can use DataWriter exporter module to write the output directly into an Azure Table etc. You can check Microsoft MyDriving reference guide (<a href=""http://aka.ms/mydrivingdocs"" rel=""nofollow"">http://aka.ms/mydrivingdocs</a>) at page 107-8 where the machine learning section starts at page 100.</p>",0.0,2016-06-27 05:39:27.403000 UTC,,-1.0,['azure-machine-learning-studio']
How to connect AMLS to ADLS Gen 2?,"<p>I would like to register a dataset from ADLS Gen2 in my Azure Machine Learning workspace (<code>azureml-core==1.12.0</code>). Given that service principal information is not required in the Python SDK <a href=""https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.datastore.datastore?view=azure-ml-py#register-azure-data-lake-gen2-workspace--datastore-name--filesystem--account-name--tenant-id-none--client-id-none--client-secret-none--resource-url-none--authority-url-none--protocol-none--endpoint-none--overwrite-false-"" rel=""noreferrer"">documentation</a> for <code>.register_azure_data_lake_gen2()</code>, I successfully used the following code to register ADLS gen2 as a datastore:</p>
<pre class=""lang-py prettyprint-override""><code>from azureml.core import Datastore

adlsgen2_datastore_name = os.environ['adlsgen2_datastore_name']
account_name=os.environ['account_name'] # ADLS Gen2 account name
file_system=os.environ['filesystem']

adlsgen2_datastore = Datastore.register_azure_data_lake_gen2(
    workspace=ws,
    datastore_name=adlsgen2_datastore_name,
    account_name=account_name, 
    filesystem=file_system
)
</code></pre>
<p>However, when I try to register a dataset, using</p>
<pre class=""lang-py prettyprint-override""><code>from azureml.core import Dataset
adls_ds = Datastore.get(ws, datastore_name=adlsgen2_datastore_name)
data = Dataset.Tabular.from_delimited_files((adls_ds, 'folder/data.csv'))
</code></pre>
<p>I get an error</p>
<blockquote>
<p>Cannot load any data from the specified path. Make sure the path is accessible and contains data.
<code>ScriptExecutionException</code> was caused by <code>StreamAccessException</code>.
StreamAccessException was caused by AuthenticationException.
<code>'AdlsGen2-ReadHeaders'</code> for '[REDACTED]' on storage failed with status code 'Forbidden' (This request is not authorized to perform this operation using this permission.), client request ID &lt;CLIENT_REQUEST_ID&gt;, request ID &lt;REQUEST_ID&gt;. Error message: [REDACTED]
| session_id=&lt;SESSION_ID&gt;</p>
</blockquote>
<p>Do I need the to enable the service principal to get this to work? Using the ML Studio UI, it appears that the service principal is required even to register the datastore.</p>
<p>Another issue I noticed is that AMLS is trying to access the dataset here:
<code>https://adls_gen2_account_name.**dfs**.core.windows.net/container/folder/data.csv</code> whereas the actual URI in ADLS Gen2 is: <code>https://adls_gen2_account_name.**blob**.core.windows.net/container/folder/data.csv</code></p>",1,0,2020-09-14 20:39:51.930000 UTC,1.0,2020-09-15 09:03:51.357000 UTC,7,python|azure-machine-learning-service|azure-data-lake-gen2,3331,2020-05-17 18:00:51.347000 UTC,2022-06-27 19:36:47.687000 UTC,,179,2,0,53,"<p>According to this <a href=""https://docs.microsoft.com/en-us/azure/machine-learning/how-to-access-data#azure-data-lake-storage-generation-2"" rel=""noreferrer"">documentation</a>,you need to enable the service principal.</p>
<p>1.you need to register your application and grant the service principal with <strong>Storage Blob Data Reader access</strong>.</p>
<p><a href=""https://i.stack.imgur.com/FZl8O.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/FZl8O.png"" alt=""enter image description here"" /></a></p>
<p>2.try this code:</p>
<pre><code>adlsgen2_datastore = Datastore.register_azure_data_lake_gen2(workspace=ws,
                                                             datastore_name=adlsgen2_datastore_name,
                                                             account_name=account_name,
                                                             filesystem=file_system,
                                                             tenant_id=tenant_id,
                                                             client_id=client_id,
                                                             client_secret=client_secret
                                                             )

adls_ds = Datastore.get(ws, datastore_name=adlsgen2_datastore_name)
dataset = Dataset.Tabular.from_delimited_files((adls_ds,'sample.csv'))
print(dataset.to_pandas_dataframe())
</code></pre>
<p><strong>Result:</strong></p>
<p><a href=""https://i.stack.imgur.com/50mit.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/50mit.png"" alt=""enter image description here"" /></a></p>",0.0,2020-09-15 07:41:56.360000 UTC,2020-09-15 10:47:14.147000 UTC,9.0,['azure-machine-learning-service']
Load S3 Data into AWS SageMaker Notebook,"<p>I've just started to experiment with AWS SageMaker and would like to load data from an S3 bucket into a pandas dataframe in my SageMaker python jupyter notebook for analysis.</p>

<p>I could use boto to grab the data from S3, but I'm wondering whether there is a more elegant method as part of the SageMaker framework to do this in my python code?</p>

<p>Thanks in advance for any advice.</p>",8,0,2018-01-15 14:07:26.727000 UTC,15.0,,57,python|amazon-web-services|amazon-s3|machine-learning|amazon-sagemaker,78494,2017-02-17 17:02:25.117000 UTC,2021-01-07 13:51:11.447000 UTC,,673,14,0,20,"<p>If you have a look <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-dg.pdf"" rel=""noreferrer"">here</a> it seems you can specify this in the <em>InputDataConfig</em>. Search for ""S3DataSource"" (<a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/API_S3DataSource.html"" rel=""noreferrer"">ref</a>) in the document. The first hit is even in Python, on page 25/26.</p>",0.0,2018-01-15 17:16:02.537000 UTC,,11.0,['amazon-sagemaker']
sagemaker notebook instance Elastic Inference tensorflow model local deployment,"<p>I am trying to replicate <a href=""https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/tensorflow_serving_using_elastic_inference_with_your_own_model/tensorflow_serving_pretrained_model_elastic_inference.ipynb"" rel=""nofollow noreferrer"">https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/tensorflow_serving_using_elastic_inference_with_your_own_model/tensorflow_serving_pretrained_model_elastic_inference.ipynb</a></p>

<p>My elastic inference accelerator is attached to notebook instance. I am using conda_amazonei_tensorflow_p36 kernel. According to documentation I made the changes for local EI:</p>

<pre><code>%%time
import boto3

region = boto3.Session().region_name
saved_model = 's3://sagemaker-sample-data-{}/tensorflow/model/resnet/resnet_50_v2_fp32_NCHW.tar.gz'.format(region)

import sagemaker
from sagemaker.tensorflow.serving import Model

role = sagemaker.get_execution_role()

tensorflow_model = Model(model_data=saved_model,
role=role,
framework_version='1.14')
tf_predictor = tensorflow_model.deploy(initial_instance_count=1,
instance_type='local',
accelerator_type='local_sagemaker_notebook')
</code></pre>

<p>I am getting following log in the notebook:</p>

<pre><code>Attaching to tmp6uqys1el_algo-1-7ynb1_1
algo-1-7ynb1_1 | INFO:main:starting services
algo-1-7ynb1_1 | INFO:main:using default model name: Servo
algo-1-7ynb1_1 | INFO:main:tensorflow serving model config:
algo-1-7ynb1_1 | model_config_list: {
algo-1-7ynb1_1 | config: {
algo-1-7ynb1_1 | name: ""Servo"",
algo-1-7ynb1_1 | base_path: ""/opt/ml/model/export/Servo"",
algo-1-7ynb1_1 | model_platform: ""tensorflow""
algo-1-7ynb1_1 | }
algo-1-7ynb1_1 | }
algo-1-7ynb1_1 |
algo-1-7ynb1_1 |
algo-1-7ynb1_1 | INFO:main:nginx config:
algo-1-7ynb1_1 | load_module modules/ngx_http_js_module.so;
algo-1-7ynb1_1 |
algo-1-7ynb1_1 | worker_processes auto;
algo-1-7ynb1_1 | daemon off;
algo-1-7ynb1_1 | pid /tmp/nginx.pid;
algo-1-7ynb1_1 | error_log /dev/stderr error;
algo-1-7ynb1_1 |
algo-1-7ynb1_1 | worker_rlimit_nofile 4096;
algo-1-7ynb1_1 |
algo-1-7ynb1_1 | events {
algo-1-7ynb1_1 | worker_connections 2048;
algo-1-7ynb1_1 | }
algo-1-7ynb1_1 |
algo-1-7ynb1_1 | http {
algo-1-7ynb1_1 | include /etc/nginx/mime.types;
algo-1-7ynb1_1 | default_type application/json;
algo-1-7ynb1_1 | access_log /dev/stdout combined;
algo-1-7ynb1_1 | js_include tensorflow-serving.js;
algo-1-7ynb1_1 |
algo-1-7ynb1_1 | upstream tfs_upstream {
algo-1-7ynb1_1 | server localhost:8501;
algo-1-7ynb1_1 | }
algo-1-7ynb1_1 |
algo-1-7ynb1_1 | upstream gunicorn_upstream {
algo-1-7ynb1_1 | server unix:/tmp/gunicorn.sock fail_timeout=1;
algo-1-7ynb1_1 | }
algo-1-7ynb1_1 |
algo-1-7ynb1_1 | server {
algo-1-7ynb1_1 | listen 8080 deferred;
algo-1-7ynb1_1 | client_max_body_size 0;
algo-1-7ynb1_1 | client_body_buffer_size 100m;
algo-1-7ynb1_1 | subrequest_output_buffer_size 100m;
algo-1-7ynb1_1 |
algo-1-7ynb1_1 | set $tfs_version 1.14;
algo-1-7ynb1_1 | set $default_tfs_model Servo;
algo-1-7ynb1_1 |
algo-1-7ynb1_1 | location /tfs {
algo-1-7ynb1_1 | rewrite ^/tfs/(.) /$1 break;
algo-1-7ynb1_1 | proxy_redirect off;
algo-1-7ynb1_1 | proxy_pass_request_headers off;
algo-1-7ynb1_1 | proxy_set_header Content-Type 'application/json';
algo-1-7ynb1_1 | proxy_set_header Accept 'application/json';
algo-1-7ynb1_1 | proxy_pass http://tfs_upstream;
algo-1-7ynb1_1 | }
algo-1-7ynb1_1 |
algo-1-7ynb1_1 | location /ping {
algo-1-7ynb1_1 | js_content ping;
algo-1-7ynb1_1 | }
algo-1-7ynb1_1 |
algo-1-7ynb1_1 | location /invocations {
algo-1-7ynb1_1 | js_content invocations;
algo-1-7ynb1_1 | }
algo-1-7ynb1_1 |
algo-1-7ynb1_1 | location ~ ^/models/(.)/invoke {
algo-1-7ynb1_1 | js_content invocations;
algo-1-7ynb1_1 | }
algo-1-7ynb1_1 |
algo-1-7ynb1_1 | location /models {
algo-1-7ynb1_1 | proxy_pass http://gunicorn_upstream/models;
algo-1-7ynb1_1 | }
algo-1-7ynb1_1 |
algo-1-7ynb1_1 | location / {
algo-1-7ynb1_1 | return 404 '{""error"": ""Not Found""}';
algo-1-7ynb1_1 | }
algo-1-7ynb1_1 |
algo-1-7ynb1_1 | keepalive_timeout 3;
algo-1-7ynb1_1 | }
algo-1-7ynb1_1 | }
algo-1-7ynb1_1 |
algo-1-7ynb1_1 |
algo-1-7ynb1_1 | INFO:main:tensorflow version info:
algo-1-7ynb1_1 | TensorFlow ModelServer: 1.14.0-rc0+dev.sha.34d9e85
algo-1-7ynb1_1 | TensorFlow Library: 1.14.0
algo-1-7ynb1_1 | EI Version: EI-1.4
algo-1-7ynb1_1 | INFO:main:tensorflow serving command: tensorflow_model_server --port=9000 --rest_api_port=8501 --model_config_file=/sagemaker/model-config.cfg
algo-1-7ynb1_1 | INFO:main:started tensorflow serving (pid: 8)
algo-1-7ynb1_1 | INFO:main:nginx version info:
algo-1-7ynb1_1 | nginx version: nginx/1.16.1
algo-1-7ynb1_1 | built by gcc 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.11)
algo-1-7ynb1_1 | built with OpenSSL 1.0.2g 1 Mar 2016
algo-1-7ynb1_1 | TLS SNI support enabled
algo-1-7ynb1_1 | configure arguments: --prefix=/etc/nginx --sbin-path=/usr/sbin/nginx --modules-path=/usr/lib/nginx/modules --conf-path=/etc/nginx/nginx.conf --error-log-path=/var/log/nginx/error.log --http-log-path=/var/log/nginx/access.log --pid-path=/var/run/nginx.pid --lock-path=/var/run/nginx.lock --http-client-body-temp-path=/var/cache/nginx/client_temp --http-proxy-temp-path=/var/cache/nginx/proxy_temp --http-fastcgi-temp-path=/var/cache/nginx/fastcgi_temp --http-uwsgi-temp-path=/var/cache/nginx/uwsgi_temp --http-scgi-temp-path=/var/cache/nginx/scgi_temp --user=nginx --group=nginx --with-compat --with-file-aio --with-threads --with-http_addition_module --with-http_auth_request_module --with-http_dav_module --with-http_flv_module --with-http_gunzip_module --with-http_gzip_static_module --with-http_mp4_module --with-http_random_index_module --with-http_realip_module --with-http_secure_link_module --with-http_slice_module --with-http_ssl_module --with-http_stub_status_module --with-http_sub_module --with-http_v2_module --with-mail --with-mail_ssl_module --with-stream --with-stream_realip_module --with-stream_ssl_module --with-stream_ssl_preread_module --with-cc-opt='-g -O2 -fPIE -fstack-protector-strong -Wformat -Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -fPIC' --with-ld-opt='-Wl,-Bsymbolic-functions -fPIE -pie -Wl,-z,relro -Wl,-z,now -Wl,--as-needed -pie'
algo-1-7ynb1_1 | INFO:main:started nginx (pid: 10)
algo-1-7ynb1_1 | 2020-06-17 05:02:08.888114: I tensorflow_serving/model_servers/server_core.cc:462] Adding/updating models.
algo-1-7ynb1_1 | 2020-06-17 05:02:08.888186: I tensorflow_serving/model_servers/server_core.cc:561] (Re-)adding model: Servo
algo-1-7ynb1_1 | 2020-06-17 05:02:08.988623: I tensorflow_serving/core/basic_manager.cc:739] Successfully reserved resources to load servable {name: Servo version: 1527887769}
algo-1-7ynb1_1 | 2020-06-17 05:02:08.988688: I tensorflow_serving/core/loader_harness.cc:66] Approving load for servable version {name: Servo version: 1527887769}
algo-1-7ynb1_1 | 2020-06-17 05:02:08.988728: I tensorflow_serving/core/loader_harness.cc:74] Loading servable version {name: Servo version: 1527887769}
algo-1-7ynb1_1 | 2020-06-17 05:02:08.988762: I external/org_tensorflow/tensorflow/contrib/session_bundle/bundle_shim.cc:363] Attempting to load native SavedModelBundle in bundle-shim from: /opt/ml/model/export/Servo/1527887769
algo-1-7ynb1_1 | 2020-06-17 05:02:08.988783: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:31] Reading SavedModel from: /opt/ml/model/export/Servo/1527887769
algo-1-7ynb1_1 | 2020-06-17 05:02:09.001922: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:54] Reading meta graph with tags { serve }
algo-1-7ynb1_1 | 2020-06-17 05:02:09.082734: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:202] Restoring SavedModel bundle.
algo-1-7ynb1_1 | 2020-06-17 05:02:09.613725: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:151] Running initialization op on SavedModel bundle at path: /opt/ml/model/export/Servo/1527887769
algo-1-7ynb1_1 | Using Amazon Elastic Inference Client Library Version: 1.5.3
algo-1-7ynb1_1 | Number of Elastic Inference Accelerators Available: 1
algo-1-7ynb1_1 | Elastic Inference Accelerator ID: eia-813285f77ceb448c849e2331116f251b
algo-1-7ynb1_1 | Elastic Inference Accelerator Type: eia2.medium
algo-1-7ynb1_1 | Elastic Inference Accelerator Ordinal: 0
algo-1-7ynb1_1 |
!algo-1-7ynb1_1 | 172.18.0.1 - - [17/Jun/2020:05:02:10 +0000] ""GET /ping HTTP/1.1"" 200 3 ""-"" ""-""
algo-1-7ynb1_1 | [Wed Jun 17 05:02:11 2020, 662569us] [Execution Engine] Error getting application context for [TensorFlow][2]
algo-1-7ynb1_1 | [Wed Jun 17 05:02:11 2020, 662722us] [Execution Engine][TensorFlow][2] Failed - Last Error:
algo-1-7ynb1_1 | EI Error Code: [3, 16, 8]
algo-1-7ynb1_1 | EI Error Description: Unable to authenticate with accelerator
algo-1-7ynb1_1 | EI Request ID: TF-D66B9810-D81A-448F-ACE2-703FFFA0F194 -- EI Accelerator ID: eia-813285f77ceb448c849e2331116f251b
algo-1-7ynb1_1 | EI Client Version: 1.5.3
algo-1-7ynb1_1 | 2020-06-17 05:02:11.668412: F external/org_tensorflow/tensorflow/contrib/ei/session/eia_session.cc:1219] Non-OK-status: SwapExStateWithEI(tmp_inputs, tmp_outputs, tmp_freeze) status: Internal: Failed to get the initial operator whitelist from server.
algo-1-7ynb1_1 | WARNING:main:unexpected tensorflow serving exit (status: 6). restarting.
algo-1-7ynb1_1 | INFO:main:tensorflow version info:
algo-1-7ynb1_1 | TensorFlow ModelServer: 1.14.0-rc0+dev.sha.34d9e85
algo-1-7ynb1_1 | TensorFlow Library: 1.14.0
algo-1-7ynb1_1 | EI Version: EI-1.4
algo-1-7ynb1_1 | INFO:main:tensorflow serving command: tensorflow_model_server --port=9000 --rest_api_port=8501 --model_config_file=/sagemaker/model-config.cfg
algo-1-7ynb1_1 | INFO:main:started tensorflow serving (pid: 38)`enter code here`
algo-1-7ynb1_1 | 2020-06-17 05:02:11.759706: I tensorflow_serving/model_servers/server_core.cc:462] Adding/updating models.
algo-1-7ynb1_1 | 2020-06-17 05:02:11.759783: I tensorflow_serving/model_servers/server_core.cc:561] (Re-)adding model: Servo
algo-1-7ynb1_1 | 2020-06-17 05:02:11.860242: I tensorflow_serving/core/basic_manager.cc:739] Successfully reserved resources to load servable {name: Servo version: 1527887769}
algo-1-7ynb1_1 | 2020-06-17 05:02:11.860309: I tensorflow_serving/core/loader_harness.cc:66] Approving load for servable version {name: Servo version: 1527887769}
algo-1-7ynb1_1 | 2020-06-17 05:02:11.860333: I tensorflow_serving/core/loader_harness.cc:74] Loading servable version {name: Servo version: 1527887769}
algo-1-7ynb1_1 | 2020-06-17 05:02:11.860365: I external/org_tensorflow/tensorflow/contrib/session_bundle/bundle_shim.cc:363] Attempting to load native SavedModelBundle in bundle-shim from: /opt/ml/model/export/Servo/1527887769
algo-1-7ynb1_1 | 2020-06-17 05:02:11.860382: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:31] Reading SavedModel from: /opt/ml/model/export/Servo/1527887769
algo-1-7ynb1_1 | 2020-06-17 05:02:11.873381: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:54] Reading meta graph with tags { serve }
algo-1-7ynb1_1 | 2020-06-17 05:02:11.949421: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:202] Restoring SavedModel bundle.
algo-1-7ynb1_1 | 2020-06-17 05:02:12.512935: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:151] Running initialization op on SavedModel bundle at path: /opt/ml/model/export/Servo/1527887769
algo-1-7ynb1_1 | Using Amazon Elastic Inference Client Library Version: 1.5.3
algo-1-7ynb1_1 | Number of Elastic Inference Accelerators Available: 1
algo-1-7ynb1_1 | Elastic Inference Accelerator ID: eia-813285f77ceb448c849e2331116f251b
algo-1-7ynb1_1 | Elastic Inference Accelerator Type: eia2.medium
algo-1-7ynb1_1 | Elastic Inference Accelerator Ordinal: 0
`
</code></pre>

<p>The log never stops in notebook. It keeps throwing in notebook cells. I am not sure whether the model is deployed correctly.</p>

<p>I can see the docker of the model running
<a href=""https://i.stack.imgur.com/YRXKL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YRXKL.png"" alt=""enter image description here""></a></p>

<p>When I try to infer/predict from that model, I get error:</p>

<pre><code>algo-1-iikpj_1 | [Wed Jun 17 05:29:47 2020, 761607us] [Execution Engine] Error getting application context for [TensorFlow][2]

algo-1-iikpj_1 | [Wed Jun 17 05:29:47 2020, 761691us] [Execution Engine][TensorFlow][2] Failed - Last Error:
algo-1-iikpj_1 | EI Error Code: [3, 16, 8]
algo-1-iikpj_1 | EI Error Description: Unable to authenticate with accelerator
algo-1-iikpj_1 | EI Request ID: TF-ADECD8EF-7138-4B5F-9C37-ADFDC8122DF1 -- EI Accelerator ID: eia-813285f77ceb448c849e2331116f251b
algo-1-iikpj_1 | EI Client Version: 1.5.3
algo-1-iikpj_1 | 2020-06-17 05:29:47.768249: F external/org_tensorflow/tensorflow/contrib/ei/session/eia_session.cc:1219] Non-OK-status: SwapExStateWithEI(tmp_inputs, tmp_outputs, tmp_freeze) status: Internal: Failed to get the initial operator whitelist from server.
algo-1-iikpj_1 | WARNING:main:unexpected tensorflow serving exit (status: 6). restarting.
algo-1-iikpj_1 | INFO:main:tensorflow version info:
algo-1-iikpj_1 | TensorFlow ModelServer: 1.14.0-rc0+dev.sha.34d9e85
algo-1-iikpj_1 | TensorFlow Library: 1.14.0
algo-1-iikpj_1 | EI Version: EI-1.4
algo-1-iikpj_1 | INFO:main:tensorflow serving command: tensorflow_model_server --port=9000 --rest_api_port=8501 --model_config_file=/sagemaker/model-config.cfg
algo-1-iikpj_1 | INFO:main:started tensorflow serving (pid: 1052)
algo-1-iikpj_1 | 2020-06-17 05:29:47.854331: I tensorflow_serving/model_servers/server_core.cc:462] Adding/updating models.
algo-1-iikpj_1 | 2020-06-17 05:29:47.854405: I tensorflow_serving/model_servers/server_core.cc:561] (Re-)adding model: Servo
algo-1-iikpj_1 | 2020/06/17 05:29:47 [error] 11#11: *2 connect() failed (111: Connection refused) while connecting to upstream, client: 172.18.0.1, server: , request: ""POST /invocations HTTP/1.1"", subrequest: ""/v1/models/Servo:predict"", upstream: ""http://127.0.0.1:8501/v1/models/Servo:predict"", host: ""localhost:8080""
algo-1-iikpj_1 | 2020/06/17 05:29:47 [error] 11#11: *2 connect() failed (111: Connection refused) while connecting to upstream, client: 172.18.0.1, server: , request: ""POST /invocations HTTP/1.1"", subrequest: ""/v1/models/Servo:predict"", upstream: ""http://127.0.0.1:8501/v1/models/Servo:predict"", host: ""localhost:8080""
algo-1-iikpj_1 | 172.18.0.1 - - [17/Jun/2020:05:29:47 +0000] ""POST /invocations HTTP/1.1"" 502 157 ""-"" ""-""
algo-1-iikpj_1 | 2020-06-17 05:29:47.954825: I tensorflow_serving/core/basic_manager.cc:739] Successfully reserved resources to load servable {name: Servo version: 1527887769}
algo-1-iikpj_1 | 2020-06-17 05:29:47.954887: I tensorflow_serving/core/loader_harness.cc:66] Approving load for servable version {name: Servo version: 1527887769}
algo-1-iikpj_1 | 2020-06-17 05:29:47.955448: I tensorflow_serving/core/loader_harness.cc:74] Loading servable version {name: Servo version: 1527887769}
algo-1-iikpj_1 | 2020-06-17 05:29:47.955494: I external/org_tensorflow/tensorflow/contrib/session_bundle/bundle_shim.cc:363] Attempting to load native SavedModelBundle in bundle-shim from: /opt/ml/model/export/Servo/1527887769
algo-1-iikpj_1 | 2020-06-17 05:29:47.955859: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:31] Reading SavedModel from: /opt/ml/model/export/Servo/1527887769
algo-1-iikpj_1 | 2020-06-17 05:29:47.969511: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:54] Reading meta graph with tags { serve }
JSONDecodeError Traceback (most recent call last)
in ()

~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/sagemaker/tensorflow/serving.py in predict(self, data, initial_args)
116 args[""CustomAttributes""] = self._model_attributes
117
--&gt; 118 return super(Predictor, self).predict(data, args)
119
120

~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/sagemaker/predictor.py in predict(self, data, initial_args, target_model)
109 request_args = self._create_request_args(data, initial_args, target_model)
110 response = self.sagemaker_session.sagemaker_runtime_client.invoke_endpoint(**request_args)
--&gt; 111 return self._handle_response(response)
112
113 def _handle_response(self, response):

~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/sagemaker/predictor.py in _handle_response(self, response)
119 if self.deserializer is not None:
120 # It's the deserializer's responsibility to close the stream
--&gt; 121 return self.deserializer(response_body, response[""ContentType""])
122 data = response_body.read()
123 response_body.close()

~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/sagemaker/predictor.py in call(self, stream, content_type)
578 """"""
579 try:
--&gt; 580 return json.load(codecs.getreader(""utf-8"")(stream))
581 finally:
582 stream.close()

~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/json/init.py in load(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)
297 cls=cls, object_hook=object_hook,
298 parse_float=parse_float, parse_int=parse_int,
--&gt; 299 parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)
300
301

~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/json/init.py in loads(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)
352 parse_int is None and parse_float is None and
353 parse_constant is None and object_pairs_hook is None and not kw):
--&gt; 354 return _default_decoder.decode(s)
355 if cls is None:
356 cls = JSONDecoder

~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/json/decoder.py in decode(self, s, _w)
337
338 """"""
--&gt; 339 obj, end = self.raw_decode(s, idx=_w(s, 0).end())
340 end = _w(s, end).end()
341 if end != len(s):

~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/json/decoder.py in raw_decode(self, s, idx)
355 obj, end = self.scan_once(s, idx)
356 except StopIteration as err:
--&gt; 357 raise JSONDecodeError(""Expecting value"", s, err.value) from None
358 return obj, end

JSONDecodeError: Expecting value: line 1 column 1 (char 0)

algo-1-iikpj_1 | 2020-06-17 05:29:48.047106: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:202] Restoring SavedModel bundle.
algo-1-iikpj_1 | 2020-06-17 05:29:48.564452: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:151] Running initialization op on SavedModel bundle at path: /opt/ml/model/export/Servo/1527887769
algo-1-iikpj_1 | Using Amazon Elastic Inference Client Library Version: 1.5.3
</code></pre>

<p>I tried several ways to solve JSONDecodeError: Expecting value: line 1 column 1 (char 0) using json.loads, json.dumps etc but nothing helps.
I also tried Rest API post to docker deployed model:</p>

<pre><code>curl -v -X POST \ -H 'content-type:application/json' \ -d '{""data"": {""inputs"": [[[[0.13075708159043742, 0.048010725848070535, 0.9012465727287071], [0.1643217202482622, 0.7392467524276859, 0.5618572640643519], [0.7697097217983989, 0.9829998452540657, 0.08567413146192027]]]]} }' \ http://127.0.0.1:8080/v1/models/Servo:predict
but still getting error:
[![enter image description here][1]][1]
</code></pre>

<p>Please help me to resolve the issue. Initially, I was trying to use my tensorflow serving model and getting the same errors. Then I thought of following with the same model which was used in AWS example notebook (resnet_50_v2_fp32_NCHW.tar.gz'). So, the above experiment is using AWS example notebook with model provided by sagemaker-sample-data.</p>

<p>Please help me out. Thanks</p>",1,0,2020-06-17 06:25:29.670000 UTC,,,2,tensorflow|amazon-sagemaker,350,2011-07-15 03:14:53.433000 UTC,2022-09-22 06:31:23.927000 UTC,,2889,79,0,62,"<p>Solved it. The error I was getting is due to roles/permission of elastic inference attached to notebook. Once fixed these permissions by our devops team. It worked as expected.  See <a href=""https://github.com/aws/sagemaker-tensorflow-serving-container/issues/142"" rel=""nofollow noreferrer"">https://github.com/aws/sagemaker-tensorflow-serving-container/issues/142</a></p>",2.0,2020-06-23 01:37:54.340000 UTC,,-1.0,['amazon-sagemaker']
Intel optimized Python on Machine Learning Service Compute,"<p>Is it possible to run a Python script or Estimator step on the Azure Machine Learning Service in a container with the intel optimized Python distribution?
I understand this is available on the <a href=""https://azure.microsoft.com/en-in/blog/intel-and-microsoft-bring-optimizations-to-deep-learning-on-azure/"" rel=""nofollow noreferrer"">Azure Data Science VMs</a> (<a href=""https://www.intel.ai/intel-optimized-data-science-virtual-machine-azure/#gs.yuntlp"" rel=""nofollow noreferrer"">or described here</a>), but I could not find out how to use this as an Azure Machine Learning Service Compute target.</p>

<p>For my current use case I am specifically interested in using an mkl linked numpy package in the aml service container.</p>

<p>Note: Running numpy.show_config() inside the container suggests numpy is linked against openblas and not mkl</p>

<pre><code>blas_mkl_info:
  NOT AVAILABLE
blis_info:
  NOT AVAILABLE
openblas_info:
    libraries = ['openblas', 'openblas']
    library_dirs = ['/usr/local/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None)]
blas_opt_info:
    libraries = ['openblas', 'openblas']
    library_dirs = ['/usr/local/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None)]
lapack_mkl_info:
  NOT AVAILABLE
openblas_lapack_info:
    libraries = ['openblas', 'openblas']
    library_dirs = ['/usr/local/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None)]
lapack_opt_info:
    libraries = ['openblas', 'openblas']
    library_dirs = ['/usr/local/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None)]
</code></pre>",1,0,2020-03-04 11:54:09.310000 UTC,,2020-03-04 20:46:17.670000 UTC,0,azure-machine-learning-service,81,2020-02-20 08:37:04.717000 UTC,2021-04-11 10:00:37.967000 UTC,,23,0,0,3,"<p>The Azure ML base images use <a href=""https://docs.conda.io/en/latest/miniconda.html"" rel=""nofollow noreferrer"">Miniconda</a> Python distribution, which uses MKL.</p>

<p>You can find the details of the base images here:<a href=""https://github.com/Azure/AzureML-Containers"" rel=""nofollow noreferrer"">https://github.com/Azure/AzureML-Containers</a></p>

<p>Also, if you install using Anaconda numpy in following way</p>

<pre><code>conda_dep.add_conda_package(""numpy"")
runconfig.run_config.environment.python.conda_dependencies = conda_dep
</code></pre>

<p>you should see this kind of output from <code>numpy.show_config()</code>. </p>

<blockquote>
  <p>blas_mkl_info:</p>
  
  <p>libraries = ['blas', 'cblas', 'lapack', 'pthread', 'blas', 'cblas',
  'lapack']</p>
  
  <p>library_dirs =
  ['/azureml-envs/azureml_a8ad8e485613e21e6e8adc1bfda86b40/lib']</p>
  
  <p>define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]</p>
  
  <p>include_dirs =
  ['/azureml-envs/azureml_a8ad8e485613e21e6e8adc1bfda86b40/include']</p>
</blockquote>",5.0,2020-03-04 18:14:24.733000 UTC,2020-03-05 19:46:14.647000 UTC,-1.0,['azure-machine-learning-service']
upload data to S3 with sagemaker,"<p>I have a problem with SageMaker when I try to upload Data into S3 bucket . I get this error : </p>

<blockquote>
  <hr>

<pre><code>NameError                                 Traceback (most recent call last)
&lt;ipython-input-26-d21b1cb0fcab&gt; in &lt;module&gt;()
     19 download('http://data.mxnet.io/data/caltech-256/caltech-256-60-train.rec')
     20 
---&gt; 21 upload_to_s3('train', 'caltech-256-60-train.rec')

&lt;ipython-input-26-d21b1cb0fcab&gt; in upload_to_s3(channel, file)
     13     data = open(file, ""rb"")
     14     key = channel + '/' + file
---&gt; 15     s3.Bucket(bucket).put_object(Key=key, Body=data)
     16 
     17 

NameError: name 'bucket' is not defined
</code></pre>
</blockquote>

<p>Here is the script:</p>

<pre class=""lang-python prettyprint-override""><code>import os
import urllib.request
import boto3

def download(url):
    filename = url.split(""/"")[-1]
    if not os.path.exists(filename):
        urllib.request.urlretrieve(url, filename)


def upload_to_s3(channel, file):
    s3 = boto3.resource('s3')
    data = open(file, ""rb"")
    key = channel + '/' + file
    s3.Bucket(bucket).put_object(Key=key, Body=data)


# caltech-256 download('http://data.mxnet.io/data/caltech-256/caltech-256-60-train.rec')

upload_to_s3('train', 'caltech-256-60-train.rec')
</code></pre>",1,0,2018-04-23 09:31:28.687000 UTC,1.0,2018-12-06 22:53:27.193000 UTC,5,amazon-s3|amazon-sagemaker,9575,2013-03-27 10:20:38.763000 UTC,2022-05-09 06:03:41.087000 UTC,,187,10,0,108,"<p>It is exactly as the error say, the variable <code>bucket</code> is not defined. 
you might want to do something like </p>

<pre><code>bucket = &lt;name of already created bucket in s3&gt;
</code></pre>

<p>before you call </p>

<pre><code>s3.Bucket(bucket).put_object(Key=key, Body=data)
</code></pre>",1.0,2018-04-23 16:25:22.233000 UTC,,8.0,['amazon-sagemaker']
Jupyter notebook kernel dies when creating dummy variables with pandas,"<p>I am working on the Walmart Kaggle competition and I'm trying to create a dummy column of of the ""FinelineNumber"" column. For context, <code>df.shape</code> returns <code>(647054, 7)</code>. I am trying to make a dummy column for <code>df['FinelineNumber']</code>, which has 5,196 unique values. The results should be a dataframe of shape <code>(647054, 5196)</code>, which I then plan to <code>concat</code> to the original dataframe. </p>

<p>Nearly every time I run <code>fineline_dummies = pd.get_dummies(df['FinelineNumber'], prefix='fl')</code>, I get the following error message <code>The kernel appears to have died. It will restart automatically.</code> I am running python 2.7 in jupyter notebook on a MacBookPro with 16GB RAM.</p>

<p>Can someone explain why this is happening (and why it happens most of the time but not every time)? Is it a jupyter notebook or pandas bug? Also, I thought it might have to do with not enough RAM but I get the same error on a Microsoft Azure Machine Learning notebook with >100 GB of RAM. On Azure ML, the kernel dies every time - almost immediately.</p>",1,0,2015-12-02 16:24:31.520000 UTC,1.0,2016-02-01 04:22:08.203000 UTC,5,python|pandas|ipython-notebook|azure-machine-learning-studio,5063,2013-07-10 17:00:15.300000 UTC,2022-09-21 17:11:58.293000 UTC,"New York, United States",2037,61,1,193,"<p>It very much could be memory usage - a 647054, 5196 data frame has 3,362,092,584 elements, which would be 24GB just for the pointers to the objects on a 64-bit system.  On AzureML while the VM has a large amount of memory you're actually limited in how much memory you have available (currently 2GB, soon to be 4GB) - and when you hit the limit the kernel typically dies.  So it seems very likely it is a memory usage issue.</p>

<p>You might try doing <a href=""http://pandas.pydata.org/pandas-docs/stable/sparse.html"" rel=""noreferrer"">.to_sparse()</a> on the data frame first before doing any additional manipulations.  That should allow Pandas to keep most of the data frame out of memory.</p>",0.0,2015-12-10 17:25:00.887000 UTC,,8.0,['azure-machine-learning-studio']
How do I implement a PyTorch Dataset for use with AWS SageMaker?,"<p>I have implemented a PyTorch <code>Dataset</code> that works locally (on my own desktop), but when executed on AWS SageMaker, it breaks. My <code>Dataset</code> implementation is as follows.</p>

<pre><code>class ImageDataset(Dataset):
    def __init__(self, path='./images', transform=None):
        self.path = path
        self.files = [join(path, f) for f in listdir(path) if isfile(join(path, f)) and f.endswith('.jpg')]
        self.transform = transform
        if transform is None:
            self.transform = transforms.Compose([
                transforms.Resize((128, 128)),
                transforms.ToTensor(),
                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
            ])

    def __len__(self):
        return len(files)

    def __getitem__(self, idx):
        img_name = self.files[idx]

        # we may infer the label from the filename
        dash_idx = img_name.rfind('-')
        dot_idx = img_name.rfind('.')
        label = int(img_name[dash_idx + 1:dot_idx])

        image = Image.open(img_name)

        if self.transform:
            image = self.transform(image)

        return image, label
</code></pre>

<p>I am following this <a href=""https://github.com/aws/sagemaker-python-sdk/tree/master/src/sagemaker/pytorch"" rel=""noreferrer"">example</a> and this <a href=""https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/pytorch_cnn_cifar10/pytorch_local_mode_cifar10.ipynb"" rel=""noreferrer"">one too</a>, and I run the <code>estimator</code> as follows.</p>

<pre><code>inputs = {
 'train': 'file://images',
 'eval': 'file://images'
}
estimator = PyTorch(entry_point='pytorch-train.py',
                            role=role,
                            framework_version='1.0.0',
                            train_instance_count=1,
                            train_instance_type=instance_type)
estimator.fit(inputs)
</code></pre>

<p>I get the following error.</p>

<blockquote>
  <p>FileNotFoundError: [Errno 2] No such file or directory: './images'</p>
</blockquote>

<p>In the example that I am following, they upload the CFAIR dataset (which is downloaded locally) to S3.</p>

<pre><code>inputs = sagemaker_session.upload_data(path='data', bucket=bucket, key_prefix='data/cifar10')
</code></pre>

<p>If I take a peek at <code>inputs</code>, it is just a string literal <code>s3://sagemaker-us-east-3-184838577132/data/cifar10</code>. The code to create a <code>Dataset</code> and a <code>DataLoader</code> is shown <a href=""https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/pytorch_mnist/mnist.py#L41"" rel=""noreferrer"">here</a>, which does not help unless I track down the source and step through the logic.</p>

<p>I think what needs to happen inside my <code>ImageDataset</code> is to supply the <code>S3</code> path and use the <code>AWS CLI</code> or something to query the files and acquire their content. I do not think the <code>AWS CLI</code> is the right approach as this relies on the console and I will have to execute some sub-process commands and then parse through. </p>

<p>There must be a recipe or something to create a custom <code>Dataset</code> backed by <code>S3</code> files, right?</p>",2,0,2019-01-02 08:02:40.377000 UTC,,,8,python|amazon-s3|pytorch|amazon-sagemaker,4597,2013-03-15 17:49:38.320000 UTC,2022-09-22 03:51:42.617000 UTC,,7643,416,10,515,"<p>I was able to create a PyTorch <code>Dataset</code> backed by S3 data using <code>boto3</code>. Here's the snippet if anyone is interested.</p>

<pre><code>class ImageDataset(Dataset):
    def __init__(self, path='./images', transform=None):
        self.path = path
        self.s3 = boto3.resource('s3')
        self.bucket = self.s3.Bucket(path)
        self.files = [obj.key for obj in self.bucket.objects.all()]
        self.transform = transform
        if transform is None:
            self.transform = transforms.Compose([
                transforms.Resize((128, 128)),
                transforms.ToTensor(),
                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
            ])

    def __len__(self):
        return len(files)

    def __getitem__(self, idx):
        img_name = self.files[idx]

        # we may infer the label from the filename
        dash_idx = img_name.rfind('-')
        dot_idx = img_name.rfind('.')
        label = int(img_name[dash_idx + 1:dot_idx])

        # we need to download the file from S3 to a temporary file locally
        # we need to create the local file name
        obj = self.bucket.Object(img_name)
        tmp = tempfile.NamedTemporaryFile()
        tmp_name = '{}.jpg'.format(tmp.name)

        # now we can actually download from S3 to a local place
        with open(tmp_name, 'wb') as f:
            obj.download_fileobj(f)
            f.flush()
            f.close()
            image = Image.open(tmp_name)

        if self.transform:
            image = self.transform(image)

        return image, label
</code></pre>",2.0,2019-01-08 16:41:37.560000 UTC,,12.0,['amazon-sagemaker']
Update SageMaker Jupyterlab environment,<p>How can I update my SageMaker notebook's jupyter environment to the latest alpha release and then restart the process?</p>,1,0,2019-04-08 19:10:35.030000 UTC,,,4,amazon-sagemaker|jupyter-lab,1720,2013-02-20 05:47:52.693000 UTC,2022-09-23 20:45:28.400000 UTC,NYC,6281,430,17,958,"<p>Hi and thank you for using SageMaker!</p>

<p>To restart Jupyter from within a SageMaker Notebook Instance, you can issue the following command: <code>sudo initctl restart jupyter-server --no-wait</code>.</p>

<p>Best,
Kevin</p>",1.0,2019-04-11 18:28:36.017000 UTC,,8.0,['amazon-sagemaker']
How to pass dependency files to sagemaker SKLearnProcessor and use it in Pipeline?,"<p>I need to import function from different python scripts, which will used inside <code>preprocessing.py</code> file. I was not able to find a way to pass the dependent files to <code>SKLearnProcessor</code> Object, due to which I am getting <code>ModuleNotFoundError</code>.</p>
<p><strong>Code:</strong></p>
<pre><code>from sagemaker.sklearn.processing import SKLearnProcessor
from sagemaker.processing import ProcessingInput, ProcessingOutput

sklearn_processor = SKLearnProcessor(framework_version='0.20.0',
                                     role=role,
                                     instance_type='ml.m5.xlarge',
                                     instance_count=1)


sklearn_processor.run(code='preprocessing.py',
                      inputs=[ProcessingInput(
                        source=input_data,
                        destination='/opt/ml/processing/input')],
                      outputs=[ProcessingOutput(output_name='train_data',
                                                source='/opt/ml/processing/train'),
                               ProcessingOutput(output_name='test_data',
                                                source='/opt/ml/processing/test')],
                      arguments=['--train-test-split-ratio', '0.2']
                     )
</code></pre>
<p>I would like to pass,
<code>dependent_files = ['file1.py', 'file2.py', 'requirements.txt']</code>. So, that <code>preprocessing.py</code> have access to all the dependent modules.</p>
<p>And also need to install libraries from <code>requirements.txt</code> file.</p>
<p>Can you share any work around or a right way to do this?</p>
<p><strong>Update-25-11-2021:</strong></p>
<p><strong>Q1.</strong>(Answered but looking to solve using <code>FrameworkProcessor</code>)</p>
<p><a href=""https://github.com/aws/sagemaker-python-sdk/blob/99f023e76a5db060907a796d4d8fee550f005844/src/sagemaker/processing.py#L1426"" rel=""noreferrer"">Here</a>, the <code>get_run_args</code> function, is handling <code>dependencies</code>, <code>source_dir</code> and <code>code</code> parameters by using <a href=""https://github.com/aws/sagemaker-python-sdk/blob/99f023e76a5db060907a796d4d8fee550f005844/src/sagemaker/processing.py#L1265"" rel=""noreferrer"">FrameworkProcessor</a>. Is there any way that we can set this parameters from <code>ScriptProcessor</code> or <code>SKLearnProcessor</code> or any other <code>Processor</code> to set them?</p>
<p><strong>Q2.</strong></p>
<p>Can you also please show some reference to use our <code>Processor</code> as <code>sagemaker.workflow.steps.ProcessingStep</code> and then use in <code>sagemaker.workflow.pipeline.Pipeline</code>?</p>
<p>For having <code>Pipeline</code>, do we need <code>sagemaker-project</code> as mandatory or can we create <code>Pipeline</code> directly without any <code>Sagemaker-Project</code>?</p>",2,5,2021-09-03 14:59:45.260000 UTC,2.0,2021-11-26 14:18:30.430000 UTC,11,python|amazon-web-services|scikit-learn|amazon-sagemaker,2139,2017-07-23 15:35:48.410000 UTC,2022-09-24 13:11:02.210000 UTC,India,4419,434,324,962,"<p>There are a couple of options for you to accomplish that.</p>
<p>One that is really simple is adding all additional files to a folder, example:</p>
<pre><code>.
├── my_package
│   ├── file1.py
│   ├── file2.py
│   └── requirements.txt
└── preprocessing.py
</code></pre>
<p>Then send this entire folder as another input under the same <code>/opt/ml/processing/input/code/</code>, example:</p>
<pre class=""lang-py prettyprint-override""><code>from sagemaker.sklearn.processing import SKLearnProcessor
from sagemaker.processing import ProcessingInput, ProcessingOutput

sklearn_processor = SKLearnProcessor(
    framework_version=&quot;0.20.0&quot;,
    role=role,
    instance_type=&quot;ml.m5.xlarge&quot;,
    instance_count=1,
)

sklearn_processor.run(
    code=&quot;preprocessing.py&quot;,  # &lt;- this gets uploaded as /opt/ml/processing/input/code/preprocessing.py
    inputs=[
        ProcessingInput(source=input_data, destination='/opt/ml/processing/input'),
        # Send my_package as /opt/ml/processing/input/code/my_package/
        ProcessingInput(source='my_package/', destination=&quot;/opt/ml/processing/input/code/my_package/&quot;)
    ],
    outputs=[
        ProcessingOutput(output_name=&quot;train_data&quot;, source=&quot;/opt/ml/processing/train&quot;),
        ProcessingOutput(output_name=&quot;test_data&quot;, source=&quot;/opt/ml/processing/test&quot;),
    ],
    arguments=[&quot;--train-test-split-ratio&quot;, &quot;0.2&quot;],
)
</code></pre>
<p>What happens is that <code>sagemaker-python-sdk</code> is going to put your argument <code>code=&quot;preprocessing.py&quot;</code> under <code>/opt/ml/processing/input/code/</code> and you will have <code>my_package/</code> under the same directory.</p>
<p><strong>Edit:</strong></p>
<p>For the <code>requirements.txt</code>, you can add to your <code>preprocessing.py</code>:</p>
<pre class=""lang-py prettyprint-override""><code>import sys
import subprocess

subprocess.check_call([
    sys.executable, &quot;-m&quot;, &quot;pip&quot;, &quot;install&quot;, &quot;-r&quot;,
    &quot;/opt/ml/processing/input/code/my_package/requirements.txt&quot;,
])
</code></pre>",5.0,2021-11-24 19:39:22.627000 UTC,2021-11-24 19:47:08.437000 UTC,17.0,['amazon-sagemaker']
How to use different remotes for different folders?,"<p>I want my data and models stored in separate Google Cloud buckets. The idea is that I want to be able to share the data with others without sharing the models.</p>

<p>One idea I can think of is using separate git submodules for data and models. But that feels cumbersome and imposes some additional requirements from the end user (e.g. having to do <code>git submodule update</code>).</p>

<p>So can I do this without using git submodules?</p>",2,0,2019-11-20 11:10:29.420000 UTC,1.0,,12,dvc,1984,2011-07-22 10:25:49.880000 UTC,2022-09-21 15:11:42.327000 UTC,Tel Aviv,3784,472,0,342,"<p>You can first add the different <a href=""https://dvc.org/doc/command-reference/remote"" rel=""nofollow noreferrer"">DVC remotes</a> you want to establish (let's say you call them <code>data</code> and <code>models</code>, each one pointing to a different <a href=""https://cloud.google.com/storage/docs/json_api/v1/buckets"" rel=""nofollow noreferrer"">GC bucket</a>). <strong>But don't set any remote as the project's default</strong>; This way, <a href=""https://dvc.org/doc/command-reference/push"" rel=""nofollow noreferrer""><code>dvc push</code></a> won't work without the <code>-r</code> (or <code>--remote</code>) option.</p>
<p>You would then need to push each directory or file individually to the appropriate remote, like <code>dvc push data/ -r data</code> and <code>dvc push model.dat -r models</code>.</p>
<p>Note that a feature request to configure this exists on the DVC repo too. See <a href=""https://github.com/iterative/dvc/issues/2095"" rel=""nofollow noreferrer"">Specify file types that can be pushed to remote</a>.</p>",2.0,2019-11-20 16:31:15.363000 UTC,2022-01-18 17:46:31.693000 UTC,13.0,['dvc']
How to load trained model in amazon sagemaker?,"<p>I am following <a href=""https://github.com/mtm12/SageMakerDemo"" rel=""noreferrer"">this example</a> on how to train a machine learning model in Amazon-sagemaker.</p>
<pre><code>data_location = 's3://{}/kmeans_highlevel_example/data'.format(bucket)
output_location = 's3://{}/kmeans_highlevel_example/output'.format(bucket)

print('training data will be uploaded to: {}'.format(data_location))
print('training artifacts will be uploaded to: {}'.format(output_location))

kmeans = KMeans(role=role,
                train_instance_count=2,
                train_instance_type='ml.c4.8xlarge',
                output_path=output_location,
                k=10,
                epochs=100,
                data_location=data_location)
</code></pre>
<p>So after calling the fit function the model should be saved in the S3 bucket?? How can you load this model next time?</p>",1,0,2020-07-19 12:36:37.333000 UTC,2.0,,6,amazon-web-services|amazon-s3|amazon-sagemaker,4776,2011-07-16 13:02:36.880000 UTC,2022-09-24 20:19:39.590000 UTC,Slovenia,14913,307,1,1093,"<p>This can be done by using the sagemaker library combined with the <a href=""https://sagemaker.readthedocs.io/en/stable/api/inference/model.html"" rel=""noreferrer"">Inference Model</a>.</p>
<pre><code>model = sagemaker.model.Model(
    image=image
    model_data='s3://bucket/model.tar.gz',
    role=role_arn)
</code></pre>
<p>The options you're passing in are:</p>
<ul>
<li><code>image</code> - This is the ECR image you're using for inference (which should be for the algorithm you're trying to use). Paths are available <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html"" rel=""noreferrer"">here</a>.</li>
<li><code>model_data</code> - This is the path of where your model is stored (in a <code>tar.gz</code> compressed archive).</li>
<li><code>role</code> - This is the arn of a role that is capable of both pulling the image from ECR and getting the s3 archive.</li>
</ul>
<p>Once you've successfully done this you will need to setup an endpoint, this can be done by performing the following in your notebook through the <a href=""https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.model.Model.deploy"" rel=""noreferrer"">deploy function</a>.</p>
<pre><code>model.deploy(
   initial_instance_count=1,
   instance_type='ml.p2.xlarge'
)
</code></pre>",2.0,2020-07-19 12:49:45.810000 UTC,,8.0,['amazon-sagemaker']
Difference between git-lfs and dvc,<p>What is the difference between these two? We used git-lfs in my previous job and we are starting to use dvc alongside git in my current one. They both place some kind of index instead of file and can be downloaded on demand. Has dvc some improvements over the former one?</p>,2,0,2019-10-24 12:19:46.097000 UTC,5.0,,27,git|git-lfs|dvc,6255,2013-07-12 12:04:03.250000 UTC,2022-09-24 13:14:35.130000 UTC,,382,185,1,11,"<p>DVC is a better replacement for <code>git-lfs</code>. </p>

<p>Unlike git-lfs, DVC doesn't require installing a dedicated server; It can be used on-premises (NAS, SSH, for example) or with any major cloud provider (S3, Google Cloud, Azure).</p>

<p>For more information: <a href=""https://dvc.org/doc/use-cases/data-and-model-files-versioning"" rel=""noreferrer"">https://dvc.org/doc/use-cases/data-and-model-files-versioning</a></p>",5.0,2019-10-24 13:54:37.763000 UTC,,10.0,['dvc']
AWS SageMaker pd.read_pickle() doesn't work but read_csv() does?,"<p>I've recently been trying to train some models on an AWS SageMaker jupyter notebook instance.</p>

<p>Everything is worked very well until I tried to load in some custom dataset (REDD) through files.</p>

<p>I have the dataframes stored in Pickle (.pkl) files on an S3 bucket. I couldn't manage to read them into sagemaker so I decided to convert them to csv's as this seemed to work but I ran into a problem. This data has an index of type datetime64 and when using <code>.to_csv()</code> this index gets converted to pure text and it loses it's data structure (and I need to keep this specific index for correct plotting.)</p>

<p>So I decided to try the Pickle files again but I can't get it to work and have no idea why.</p>

<p>The following code for csv's works but I can't use it due to the index problem:</p>

<pre><code>bucket = 'sagemaker-peno'
houses_dfs = {}
data_key = 'compressed_data/'
data_location = 's3://{}/{}'.format(bucket, data_key)
for file in range(6):
    houses_dfs[file+1] = pd.read_csv(data_location+'house_'+str(file+1)+'.csv', index_col='Unnamed: 0')
</code></pre>

<p>But this code does NOT work even though it uses almost the exact same syntax:</p>

<pre><code>bucket = 'sagemaker-peno'
houses_dfs = {}
data_key = 'compressed_data/'
data_location = 's3://{}/{}'.format(bucket, data_key)
for file in range(6):
    houses_dfs[file+1] =  pd.read_pickle(data_location+'house_'+str(file+1)+'.pkl')
</code></pre>

<p>Yes it's 100% the correct path, because the csv and pkl files are stored in the same directory (compressed_data).</p>

<p>It throws me this error while using the Pickle method:</p>

<pre><code>FileNotFoundError: [Errno 2] No such file or directory: 's3://sagemaker-peno/compressed_data/house_1.pkl'
</code></pre>

<p>I hope to find someone who has dealt with this before and can solve the <code>read_pickle()</code> issue or as an alternative fix my datetime64 type issue with csv's.</p>

<p>Thanks in advance!</p>",1,0,2018-11-13 12:20:48.037000 UTC,,,1,pandas|csv|amazon-s3|pickle|amazon-sagemaker,788,2015-08-09 12:51:41.797000 UTC,2022-07-07 20:29:24.310000 UTC,"Leuven, België",87,8,0,31,<p>read_pickle() likes the full path more than a relative path from where it was run. This fixed my issue.</p>,0.0,2018-11-26 20:32:53.167000 UTC,,-1.0,['amazon-sagemaker']
"How Do You ""Permanently"" Delete An Experiment In Mlflow?","<p>Permanent deletion of an experiment isn't documented anywhere. I'm using Mlflow w/ backend postgres db</p>

<p>Here's what I've run: </p>

<pre><code>client = MlflowClient(tracking_uri=server)
client.delete_experiment(1)
</code></pre>

<p>This deletes the the experiment, but when I run a new experiment with the same name as the one I just deleted, it will return this error:</p>

<pre><code>mlflow.exceptions.MlflowException: Cannot set a deleted experiment 'cross-sell' as the active experiment. You can restore the experiment, or permanently delete the  experiment to create a new one.
</code></pre>

<p>I cannot find anywhere in the documentation that shows how to permanently delete everything.</p>",5,0,2020-02-06 06:26:41.043000 UTC,3.0,,20,python|mlflow,13984,2015-09-26 00:03:29.767000 UTC,2022-09-23 01:58:54.270000 UTC,"Vancouver, BC, Canada",2332,133,3,560,"<p>Unfortunately it seems there is no way to do this via the UI or CLI at the moment :-/</p>

<p>The way to do it depends on the type of backend file store that you are using.</p>

<p><strong>Filestore</strong>:</p>

<p>If you are using the filesystem as a storage mechanism (the default) then it is easy. The 'deleted' experiments are moved to a <code>.trash</code> folder. You just need to clear that out:</p>

<pre class=""lang-sh prettyprint-override""><code>rm -rf mlruns/.trash/*
</code></pre>

<p>As of the current version of the <a href=""https://www.mlflow.org/docs/latest/cli.html#mlflow-experiments-delete"" rel=""noreferrer"">documentation</a> (1.7.2), they remark:</p>

<blockquote>
  <p>It is recommended to use a cron job or an alternate workflow mechanism to clear <code>.trash</code> folder.</p>
</blockquote>

<p><strong>SQL Database:</strong></p>

<p>This is more tricky, as there are dependencies that need to be deleted. I am using MySQL, and these commands work for me:</p>

<pre class=""lang-sql prettyprint-override""><code>USE mlflow_db;  # the name of your database
DELETE FROM experiment_tags WHERE experiment_id=ANY(
    SELECT experiment_id FROM experiments where lifecycle_stage=""deleted""
);
DELETE FROM latest_metrics WHERE run_uuid=ANY(
    SELECT run_uuid FROM runs WHERE experiment_id=ANY(
        SELECT experiment_id FROM experiments where lifecycle_stage=""deleted""
    )
);
DELETE FROM metrics WHERE run_uuid=ANY(
    SELECT run_uuid FROM runs WHERE experiment_id=ANY(
        SELECT experiment_id FROM experiments where lifecycle_stage=""deleted""
    )
);
DELETE FROM tags WHERE run_uuid=ANY(
    SELECT run_uuid FROM runs WHERE experiment_id=ANY(
        SELECT experiment_id FROM experiments where lifecycle_stage=""deleted""
    )
);
DELETE FROM runs WHERE experiment_id=ANY(
    SELECT experiment_id FROM experiments where lifecycle_stage=""deleted""
);
DELETE FROM experiments where lifecycle_stage=""deleted"";
</code></pre>",1.0,2020-03-26 14:05:13.453000 UTC,,22.0,['mlflow']
How to use Serializer and Deserializer in Sagemaker 2,"<p>I spin up a Sagemaker notebook using the <code>conda_python3</code> kernel, and follow the <a href=""https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/random_cut_forest/random_cut_forest.ipynb"" rel=""noreferrer"">example</a> Notebook for <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/randomcutforest.html"" rel=""noreferrer"">Random Cut Forest</a>.</p>
<p>As of this writing, the <a href=""https://sagemaker.readthedocs.io/en/stable/index.html"" rel=""noreferrer"">Sagemaker SDK</a> that comes with <code>conda_python3</code> is version 1.72.0, but I want to use new features, so I update my notebook to use the latest</p>
<pre class=""lang-py prettyprint-override""><code>%%bash
pip install -U sagemaker
</code></pre>
<p>And I see it updates.</p>
<pre class=""lang-py prettyprint-override""><code>print(sagemaker.__version__)

# 2.4.1
</code></pre>
<p>A change from version 1.x to 2.x was the <a href=""https://sagemaker.readthedocs.io/en/stable/v2.html#serializer-and-deserializer-classes"" rel=""noreferrer"">serializer/deserializer classes</a></p>
<p>Previously (in version 1.72.0) I'd update my predictor to use the proper serializer/deserializer, and could run inference on my model</p>
<pre class=""lang-py prettyprint-override""><code>from sagemaker.predictor import csv_serializer, json_deserializer


rcf_inference = rcf.deploy(
    initial_instance_count=1,
    instance_type='ml.m4.xlarge',
)

rcf_inference.content_type = 'text/csv'
rcf_inference.serializer = csv_serializer
rcf_inference.accept = 'application/json'
rcf_inference.deserializer = json_deserializer

results = rcf_inference.predict(some_numpy_array)
</code></pre>
<p>(Note this all comes from the <a href=""https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/random_cut_forest/random_cut_forest.ipynb"" rel=""noreferrer"">example</a></p>
<p>I try and replicate this using sagemaker 2.4.1 like so</p>
<pre class=""lang-py prettyprint-override""><code>from sagemaker.deserializers import JSONDeserializer
from sagemaker.serializers import CSVSerializer

rcf_inference = rcf.deploy(
    initial_instance_count=1,
    instance_type='ml.m5.xlarge',
    serializer=CSVSerializer,
    deserializer=JSONDeserializer
)

results = rcf_inference.predict(some_numpy_array)
</code></pre>
<p>And I receive an error of</p>
<pre><code>TypeError: serialize() missing 1 required positional argument: 'data'
</code></pre>
<p>I know I'm using the serliaizer/deserializer incorrectly, but can't find good documentation on how this should be used</p>",2,0,2020-08-24 20:26:02.780000 UTC,4.0,,8,python-3.x|amazon-web-services|amazon-sagemaker,5910,2012-09-28 14:12:28.500000 UTC,2022-09-23 20:25:27.910000 UTC,,5182,1902,7,315,"<p>in order to use the new serializers/deserializers, you will need to init them, for example:</p>
<pre><code>from sagemaker.deserializers import JSONDeserializer
from sagemaker.serializers import CSVSerializer

rcf_inference = rcf.deploy(
    initial_instance_count=1,
    instance_type='ml.m5.xlarge',
    serializer=CSVSerializer(),
    deserializer=JSONDeserializer()
)
</code></pre>",2.0,2020-08-27 04:48:39.773000 UTC,2020-08-28 11:59:53.680000 UTC,16.0,['amazon-sagemaker']
Use mlflow to serve a custom python model for scoring,"<p>I am using Python code generated from an ml software with mlflow to read a dataframe, perform some table operations and output a dataframe. I am able to run the code successfully and save the new dataframe as an artifact. However I am unable to log the model using log_model because it is not a lr or classifier model where we train and fit. I want to log a model for this so that it can be served with new data and deployed with a rest API</p>
<pre><code>df = pd.read_csv(r&quot;/home/xxxx.csv&quot;)


with mlflow.start_run():
    def getPrediction(row):
        
        perform_some_python_operaions 

        return [Status_prediction, Status_0_probability, Status_1_probability]
    columnValues = []
    for column in columns:
        columnValues.append([])

    for index, row in df.iterrows():
        results = getPrediction(row)
        for n in range(len(results)):
            columnValues[n].append(results[n])

    for n in range(len(columns)):
        df[columns[n]] = columnValues[n]

    df.to_csv('dataset_statistics.csv')
    mlflow.log_artifact('dataset_statistics.csv')
   
</code></pre>",1,0,2021-01-25 15:00:24.463000 UTC,3.0,,4,python|deployment|mlflow|mlops,3026,2019-11-14 13:58:10.560000 UTC,2022-09-23 08:37:32.563000 UTC,,115,16,0,25,"<p>MLflow supports <a href=""https://mlflow.org/docs/latest/models.html#custom-python-models"" rel=""nofollow noreferrer"">custom models</a> of mlflow.pyfunc flavor.  You can create a custom  class  inherited from the <code>mlflow.pyfunc.PythonModel</code>, that needs to provide function <code>predict</code> for performing predictions, and optional <code>load_context</code> to load the necessary artifacts, like this (adopted from the docs):</p>
<pre class=""lang-py prettyprint-override""><code>class MyModel(mlflow.pyfunc.PythonModel):

    def load_context(self, context):
        # load your artifacts

    def predict(self, context, model_input):
        return my_predict(model_input.values)
</code></pre>
<p>You can log to MLflow whatever artifacts you need for your models, define Conda environment if necessary, etc.<br />
Then you can use <code>save_model</code> with your class to save your implementation, that could be loaded with <code>load_model</code> and do the <code>predict</code> using your model:</p>
<pre class=""lang-py prettyprint-override""><code>mlflow.pyfunc.save_model(
        path=mlflow_pyfunc_model_path, 
        python_model=MyModel(), 
        artifacts=artifacts)

# Load the model in `python_function` format
loaded_model = mlflow.pyfunc.load_model(mlflow_pyfunc_model_path)
</code></pre>",3.0,2021-01-25 16:41:54.947000 UTC,2021-10-14 05:05:40.523000 UTC,9.0,['mlflow']
AWS SageMaker on GPU,"<p>I am trying to train a neural network (Tensorflow) on AWS. I have some AWS credits. From my understanding AWS SageMaker is the one best for the job. I managed to load the Jupyter Lab console on SageMaker and tried to find a GPU kernel since, I know it is the best for training neural networks. However, I could not find such kernel. </p>

<p>Would anyone be able to help in this regard.</p>

<p>Thanks &amp; Best Regards</p>

<p>Michael</p>",2,1,2020-03-26 13:22:02.137000 UTC,2.0,,11,amazon-web-services|tensorflow|amazon-sagemaker,13664,2017-01-15 13:07:28.903000 UTC,2022-09-23 03:24:49.200000 UTC,Melbourne Australia,605,54,0,133,"<p>You train models on GPU in the SageMaker ecosystem via 2 different components:</p>

<ol>
<li><p>You can instantiate a GPU-powered <strong><a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/nbi.html"" rel=""noreferrer"">SageMaker Notebook Instance</a></strong>, for example <code>p2.xlarge</code> (NVIDIA K80) or <code>p3.2xlarge</code> (NVIDIA V100). This is convenient for interactive development - you have the GPU right under your notebook and can run code on the GPU interactively and monitor the GPU via <code>nvidia-smi</code> in a terminal tab - a great development experience. However when you develop directly from a GPU-powered machine, there are times when you may not use the GPU. For example when you write code or browse some documentation. All that time you pay for a GPU that sits idle. In that regard, it may not be the most cost-effective option for your use-case. </p></li>
<li><p>Another option is to use a <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-training.html"" rel=""noreferrer""><strong>SageMaker Training Job</strong></a> running on a GPU instance. This is a preferred option for training, because training metadata (data and model path, hyperparameters, cluster specification, etc) is persisted in the SageMaker metadata store, logs and metrics stored in Cloudwatch and the instance automatically shuts down itself at the end of training. Developing on a small CPU instance and launching training tasks using SageMaker Training API will help you make the most of your budget, while helping you retain metadata and artifacts of all your experiments. You can see <a href=""https://aws.amazon.com/fr/blogs/machine-learning/using-tensorflow-eager-execution-with-amazon-sagemaker-script-mode/"" rel=""noreferrer"">here a well documented TensorFlow example</a></p></li>
</ol>",0.0,2020-03-26 22:32:27.503000 UTC,,20.0,['amazon-sagemaker']
MLflow: INVALID_PARAMETER_VALUE: Unsupported URI './mlruns' for model registry store,"<p>I got this error when I was trying to have a model registered in the model registry. Could someone help me?</p>
<pre><code>RestException: INVALID_PARAMETER_VALUE: Unsupported URI './mlruns' for model registry store. 
Supported schemes are: ['postgresql', 'mysql', 'sqlite', 'mssql']. 
See https://www.mlflow.org/docs/latest/tracking.html#storage for how to setup a compatible server.
</code></pre>",1,0,2020-08-04 21:54:55.187000 UTC,2.0,2020-08-30 19:18:05.667000 UTC,10,python|mlflow,12594,2014-12-26 18:42:31.727000 UTC,2022-02-07 21:07:16.150000 UTC,,125,8,0,16,"<p>Mlflow required DB as datastore for Model Registry
So you have to run tracking server with DB as backend-store and log model to this tracking server.
The easiest way to use DB is to use SQLite.</p>
<pre><code>mlflow server \
    --backend-store-uri sqlite:///mlflow.db \
    --default-artifact-root ./artifacts \
    --host 0.0.0.0
</code></pre>
<p>And set MLFLOW_TRACKING_URI environment variable to <em>http://localhost:5000</em> or</p>
<pre><code>mlflow.set_tracking_uri(&quot;http://localhost:5000&quot;)
</code></pre>
<p>After got to http://localhost:5000 and you can register a logged model from UI or from the code.</p>",5.0,2020-08-05 11:19:29.480000 UTC,,27.0,['mlflow']
AWS Sagemaker - Install External Library and Make it Persist,<p>I have a sagemaker instance up and running and I have a few libraries that I frequently use with it but each time I restart the instance they get wiped and I have to reinstall them. Is it possible to install my libraries to one of the anaconda environments and have the change remain?</p>,2,0,2018-06-30 17:34:14.783000 UTC,1.0,,13,amazon-web-services|amazon-sagemaker,10578,2017-03-23 00:32:36.263000 UTC,2021-03-22 21:38:40.833000 UTC,,401,17,0,13,"<p>The supported way to do this for Sagemaker notebook instances is with <strong>Lifecycle Configurations</strong>.</p>

<p>You can create an <strong>onStart</strong> lifecycle hook that can install the required packages into the respective Conda environments each time your notebook instance starts.</p>

<p>Please see the following blog post for more details</p>

<p><a href=""https://aws.amazon.com/blogs/machine-learning/customize-your-amazon-sagemaker-notebook-instances-with-lifecycle-configurations-and-the-option-to-disable-internet-access/"" rel=""noreferrer"">https://aws.amazon.com/blogs/machine-learning/customize-your-amazon-sagemaker-notebook-instances-with-lifecycle-configurations-and-the-option-to-disable-internet-access/</a></p>",1.0,2018-07-02 18:19:26.567000 UTC,,12.0,['amazon-sagemaker']
Uploading a Dataframe to AWS S3 Bucket from SageMaker,"<p>I am new to AWS environment and trying to solve how the data flow works. After successfully uploading CSV files from S3 to SageMaker notebook instance, I am stuck on doing the reverse. </p>

<p>I have a dataframe and want to upload that to S3 Bucket as CSV or JSON. The code that I have is below:</p>

<pre><code>bucket='bucketname'
data_key = 'test.csv'
data_location = 's3://{}/{}'.format(bucket, data_key)
df.to_csv(data_location)
</code></pre>

<p>I assumed since I successfully used <code>pd.read_csv()</code> while loading, using <code>df.to_csv()</code> would also work but it didn't. Probably it is generating error because this way I cannot pick the privacy options while uploading a file manually to S3. Is there a way to upload the data to S3 from SageMaker?</p>",2,2,2019-06-28 00:36:23.347000 UTC,2.0,2019-06-28 02:22:19.710000 UTC,9,python|pandas|amazon-web-services|amazon-s3|amazon-sagemaker,16471,2018-11-11 21:34:52.477000 UTC,2022-09-24 16:12:16.107000 UTC,"Santa Clara, CA, USA",731,65,0,51,"<p>One way to solve this would be to save the CSV to the local storage on the SageMaker notebook instance, and then use the S3 API's via <code>boto3</code> to upload the file as an s3 object. 
<a href=""https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Client.upload_file"" rel=""noreferrer"">S3 docs for <code>upload_file()</code> available here.</a></p>

<p>Note, you'll need to ensure that your SageMaker hosted notebook instance has proper <code>ReadWrite</code> permissions in its IAM role, otherwise you'll receive a permissions error.</p>

<pre><code># code you already have, saving the file locally to whatever directory you wish
file_name = ""mydata.csv"" 
df.to_csv(file_name)
</code></pre>

<pre><code># instantiate S3 client and upload to s3
import boto3

s3 = boto3.resource('s3')
s3.meta.client.upload_file(file_name, 'YOUR_S3_BUCKET_NAME', 'DESIRED_S3_OBJECT_NAME')
</code></pre>

<p>Alternatively, <code>upload_fileobj()</code> may help for parallelizing as a multi-part upload. </p>",1.0,2019-06-28 01:20:44.023000 UTC,2019-06-28 01:45:21.643000 UTC,10.0,['amazon-sagemaker']
get the run id for an mlflow experiment with the name?,"<p>I currently created an experiment in mlflow and created multiple runs in the experiment.</p>
<pre><code>from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
import mlflow

experiment_name=&quot;experiment-1&quot;
mlflow.set_experiment(experiment_name)

no_of_trees=[100,200,300]
depths=[2,3,4]
for trees in no_of_trees:
    for depth in depths:
        with mlflow.start_run() as run:
            model=RandomForestRegressor(n_estimators=trees, criterion='mse',max_depth=depth)
            model.fit(x_train, y_train)
            predictions=model.predict(x_cv)
            mlflow.log_metric('rmse',mean_squared_error(y_cv, predictions))
</code></pre>
<p>after creating the runs, I wanted to get the best run_id for this experiment. for now, I can get the best run by looking at the UI of mlflow but how can we do right the program?</p>",1,0,2020-12-16 02:45:27.487000 UTC,,,6,python|mlflow,6374,2016-11-15 06:12:07.737000 UTC,2022-08-15 17:25:10.400000 UTC,"R G U K T , basar, Andhra Pradesh, India",2470,265,22,251,"<p>we can get the experiment id from the experiment name and we can use python API to get the best runs.</p>
<pre><code>experiment_name = &quot;experiment-1&quot;
current_experiment=dict(mlflow.get_experiment_by_name(experiment_name))
experiment_id=current_experiment['experiment_id']
</code></pre>
<p>By using the experiment id, we can get all the runs and we can sort them based on metrics like below. In the below code, rmse is my metric name (so it may be different for you based on metric name)</p>
<pre><code>df = mlflow.search_runs([experiment_id], order_by=[&quot;metrics.rmse DESC&quot;])
best_run_id = df.loc[0,'run_id']
</code></pre>",0.0,2020-12-16 02:45:27.487000 UTC,,15.0,['mlflow']
Isolation Forest vs Robust Random Cut Forest in outlier detection,"<p>I am examining different methods in outlier detection. I came across sklearn's implementation of Isolation Forest and Amazon sagemaker's implementation of RRCF (Robust Random Cut Forest). Both are ensemble methods based on decision trees, aiming to isolate every single point. The more isolation steps there are, the more likely the point is to be an inlier, and the opposite is true.</p>
<p>However, even after looking at the original papers of the algorithms, I am failing to understand exactly the difference between both algorithms. In what way do they work differently? Is one of them more efficient than the other?</p>
<p>EDIT: I am adding the links to the research papers for more information, as well as some tutorials discussing the topics.</p>
<p>Isolation Forest:</p>
<p><a href=""https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf?q=isolation-forest"" rel=""noreferrer"">Paper</a> <a href=""https://towardsdatascience.com/outlier-detection-with-isolation-forest-3d190448d45e"" rel=""noreferrer"">Tutorial</a></p>
<p>Robust Random Cut Forest:</p>
<p><a href=""http://proceedings.mlr.press/v48/guha16.pdf"" rel=""noreferrer"">Paper</a> <a href=""https://freecontent.manning.com/the-randomcutforest-algorithm/"" rel=""noreferrer"">Tutorial</a></p>",2,0,2020-07-27 12:59:41.210000 UTC,3.0,2020-07-28 11:54:56.953000 UTC,5,python|scikit-learn|amazon-sagemaker|outliers|anomaly-detection,6062,2020-03-30 17:07:50.093000 UTC,2022-08-10 09:27:57.740000 UTC,"Strasbourg, France",123,21,0,19,"<p>In part of my answers I'll assume you refer to Sklearn's Isolation Forest. I believe those are the 4 main differences:</p>
<ol>
<li><p><strong>Code availability:</strong> Isolation Forest has a popular open-source implementation in Scikit-Learn (<a href=""https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html"" rel=""noreferrer""><code>sklearn.ensemble.IsolationForest</code></a>), while both AWS implementation of Robust Random Cut Forest (RRCF) are closed-source, in <a href=""https://docs.aws.amazon.com/kinesisanalytics/latest/sqlref/sqlrf-random-cut-forest.html"" rel=""noreferrer"">Amazon Kinesis</a> and <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/randomcutforest.html"" rel=""noreferrer"">Amazon SageMaker</a>. There is an interesting third party RRCF open-source implementation on GitHub though: <a href=""https://github.com/kLabUM/rrcf"" rel=""noreferrer"">https://github.com/kLabUM/rrcf</a> ; but unsure how popular it is yet</p>
</li>
<li><p><strong>Training design:</strong> RRCF can work on streams, as highlighted in the paper and as exposed in the streaming analytics service Kinesis Data Analytics. On the other hand, the absence of <code>partial_fit</code> method hints me that Sklearn's Isolation Forest is a batch-only algorithm that cannot readily work on data streams</p>
</li>
<li><p><strong>Scalability:</strong> SageMaker RRCF is more scalable. Sklearn's Isolation Forest is single-machine code, which can nonetheless be parallelized over CPUs with the <code>n_jobs</code> parameter. On the other hand, SageMaker RRCF can be used over <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html"" rel=""noreferrer"">one machine or multiple machines</a>. Also, it supports SageMaker Pipe mode (streaming data via unix pipes) which makes it able to learn on much bigger data than what fits on disk</p>
</li>
<li><p><strong>the way features are sampled</strong> at each recursive isolation: RRCF gives more weight to dimension with higher variance (according to <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/rcf_how-it-works.html"" rel=""noreferrer"">SageMaker doc</a>), while I think isolation forest samples at random, which is one reason why RRCF is expected to perform better in high-dimensional space (picture from the RRCF paper)
<a href=""https://i.stack.imgur.com/3FXmE.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/3FXmE.png"" alt=""enter image description here"" /></a></p>
</li>
</ol>",4.0,2020-07-28 12:23:30.300000 UTC,,11.0,['amazon-sagemaker']
AWS Sagemaker AttributeError: can't set attribute error,"<p>I am new to python programming. Following the AWS learning path:</p>
<p><a href=""https://aws.amazon.com/getting-started/hands-on/build-train-deploy-machine-learning-model-sagemaker/?trk=el_a134p000003yWILAA2&amp;trkCampaign=DS_SageMaker_Tutorial&amp;sc_channel=el&amp;sc_campaign=Data_Scientist_Hands-on_Tutorial&amp;sc_outcome=Product_Marketing&amp;sc_geo=mult"" rel=""nofollow noreferrer"">https://aws.amazon.com/getting-started/hands-on/build-train-deploy-machine-learning-model-sagemaker/?trk=el_a134p000003yWILAA2&amp;trkCampaign=DS_SageMaker_Tutorial&amp;sc_channel=el&amp;sc_campaign=Data_Scientist_Hands-on_Tutorial&amp;sc_outcome=Product_Marketing&amp;sc_geo=mult</a></p>
<p>I am getting an error when excuting the following block (in conda_python3):</p>
<pre><code>test_data_array = test_data.drop(['y_no', 'y_yes'], axis=1).values #load the data into an array
xgb_predictor.content_type = 'text/csv' # set the data type for an inference
xgb_predictor.serializer = csv_serializer # set the serializer type
predictions = xgb_predictor.predict(test_data_array).decode('utf-8') # predict!
predictions_array = np.fromstring(predictions[1:], sep=',') # and turn the prediction into an 
array
print(predictions_array.shape)
</code></pre>
<blockquote>
<p>AttributeError                            Traceback (most recent call last)
 in 
1 test_data_array = test_data.drop(['y_no', 'y_yes'], axis=1).values #load the data into an array
----&gt; 2 xgb_predictor.content_type = 'text/csv' # set the data type for an inference
3 xgb_predictor.serializer = csv_serializer # set the serializer type
4 predictions = xgb_predictor.predict(test_data_array).decode('utf-8') # predict!
5 predictions_array = np.fromstring(predictions[1:], sep=',') # and turn the prediction into an array</p>
</blockquote>
<blockquote>
<p>AttributeError: can't set attribute</p>
</blockquote>
<p>I have looked at several prior questions but couldn't find much information related to this error when it comes to creating data types.</p>
<p>Thanks in advance for any help.</p>",2,0,2020-12-27 11:11:37.070000 UTC,,,3,python|amazon-web-services|amazon-s3|amazon-sagemaker,2479,2013-07-20 01:45:54.810000 UTC,2022-09-25 05:27:32.800000 UTC,,279,49,0,93,"<p>If you just remove it then the prediction will work. Therefore, recommend removing this code line.
xgb_predictor.content_type = 'text/csv'</p>",1.0,2020-12-31 07:17:48.573000 UTC,,8.0,['amazon-sagemaker']
Pyathena is super slow compared to querying from Athena,"<p>I run a query from AWS <strong>Athena console</strong> and takes 10s.
The same query run from <strong>Sagemaker</strong> using <strong>PyAthena</strong> takes 155s.
Is PyAthena slowing it down or is the data transfer from Athena to sagemaker so time consuming?</p>
<p>What could I do to speed this up?</p>",1,0,2020-10-02 11:28:26.830000 UTC,,2020-10-02 12:20:59.400000 UTC,3,python|amazon-web-services|amazon-athena|amazon-sagemaker|pyathena,3188,2017-02-05 16:38:14.643000 UTC,2022-09-21 10:19:38.630000 UTC,"München, Germany",668,38,3,85,"<p>Just figure out a way of boosting the queries:</p>
<p>Before I was trying:</p>
<pre><code>import pandas as pd
from pyathena import connect

conn = connect(s3_staging_dir=STAGIN_DIR,
             region_name=REGION)
pd.read_sql(QUERY, conn)
# takes 160s
</code></pre>
<p>Figured out that using a <em>PandasCursor</em> instead of a <em>connection</em> is way faster</p>
<pre><code>import pandas as pd
pyathena import connect
from pyathena.pandas.cursor import PandasCursor

cursor = connect(s3_staging_dir=STAGIN_DIR,
                 region_name=REGION,
                 cursor_class=PandasCursor).cursor()
df = cursor.execute(QUERY).as_pandas()
# takes 12s
</code></pre>
<p>Ref: <a href=""https://github.com/laughingman7743/PyAthena/issues/46"" rel=""noreferrer"">https://github.com/laughingman7743/PyAthena/issues/46</a></p>",3.0,2020-10-02 11:48:43.903000 UTC,2021-08-19 13:38:44.770000 UTC,13.0,['amazon-sagemaker']
Is it possible to set/change mlflow run name after run initial creation?,"<p>I could not find a way yet of setting the runs name after the first start_run for that run (we can pass a name there). </p>

<p>I Know we can use tags but that is not the same thing. I would like to add a run relevant name, but very often we know the name only after run evaluation or while we're running the run interactively in notebook for example.</p>",3,0,2019-07-25 10:09:51.490000 UTC,1.0,2019-07-31 00:38:47.097000 UTC,11,mlflow,7689,2014-02-22 09:53:28.773000 UTC,2021-04-14 22:23:03.790000 UTC,Portugal,133,0,0,6,"<p>It is possible to edit run names from the MLflow UI. First, click into the run whose name you'd like to edit.</p>

<p>Then, edit the run name by clicking the dropdown next the run name (i.e. the downward-pointing caret in this image):</p>

<p><a href=""https://i.stack.imgur.com/sl6Qs.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/sl6Qs.png"" alt=""Rename run dropdown""></a></p>

<p>There's currently no stable public API for setting run names - however, you can programmatically set/edit run names by setting the tag with key <code>mlflow.runName</code>, which is what the UI (currently) does under the hood.</p>",1.0,2019-07-29 06:02:35.503000 UTC,,9.0,['mlflow']
How to pass arguments to scoring file when deploying a Model in AzureML,"<p>I am deploying a trained model to an ACI endpoint on Azure Machine Learning, using the Python SDK.
I have created my score.py file, but I would like that file to be called with an argument being passed (just like with a training file) that I can interpret using <code>argparse</code>.
However, I don't seem to find how I can pass arguments
This is the code I have to create the InferenceConfig environment and which obviously does not work.  Should I fall back on using the extra Docker file steps or so?</p>

<pre class=""lang-py prettyprint-override""><code>from azureml.core.conda_dependencies import CondaDependencies
from azureml.core.environment import Environment
from azureml.core.model import InferenceConfig

env = Environment('my_hosted_environment')
env.python.conda_dependencies = CondaDependencies.create(
    conda_packages=['scikit-learn'],
    pip_packages=['azureml-defaults'])
scoring_script = 'score.py --model_name ' + model_name
inference_config = InferenceConfig(entry_script=scoring_script, environment=env)
</code></pre>

<p>Adding the score.py for reference on how I'd love to use the arguments in that script:</p>

<pre class=""lang-py prettyprint-override""><code>#removed imports
import argparse

def init():
    global model

    parser = argparse.ArgumentParser(description=""Load sklearn model"")
    parser.add_argument('--model_name', dest=""model_name"", required=True)
    args, _ = parser.parse_known_args()

    model_path = Model.get_model_path(model_name=args.model_name)
    model = joblib.load(model_path)

def run(raw_data):
    try:
        data = json.loads(raw_data)['data']
        data = np.array(data)
        result = model.predict(data)
        return result.tolist()

    except Exception as e:
        result = str(e)
        return result
</code></pre>

<p>Interested to hear your thoughts</p>",3,2,2020-03-11 13:27:40.433000 UTC,,2020-03-12 09:38:40.357000 UTC,4,python|azure-machine-learning-service,1681,2013-02-12 07:50:30.743000 UTC,2022-09-21 18:28:12.907000 UTC,Belgium,2947,297,16,355,"<p>How to deploy using environments can be found here <a href=""https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2FAzure%2FMachineLearningNotebooks%2Fblob%2Fmaster%2Fhow-to-use-azureml%2Fdeployment%2Fdeploy-to-cloud%2Fmodel-register-and-deploy.ipynb&amp;data=02%7C01%7CRamprasad.Mula%40microsoft.com%7Ce06d310b0447416ab46b08d7bc836a81%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637185146436156499&amp;sdata=uQo332dpjuiNqWFCguvs3Kgg7UUMN8MBEzLxTPyH4MM%3D&amp;reserved=0"" rel=""nofollow noreferrer"">model-register-and-deploy.ipynb</a> .  InferenceConfig class accepts  source_directory and entry_script <a href=""https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.model.inferenceconfig?view=azure-ml-py#parameters"" rel=""nofollow noreferrer"">parameters</a>, where source_directory  is a path to the folder that contains all files(score.py and any other additional files) to create the image. </p>

<p>This <a href=""https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/deployment/deploy-multi-model/multi-model-register-and-deploy.ipynb"" rel=""nofollow noreferrer"">multi-model-register-and-deploy.ipynb</a> has code snippets on how to create InferenceConfig with source_directory and entry_script.</p>

<pre><code>from azureml.core.webservice import Webservice
from azureml.core.model import InferenceConfig
from azureml.core.environment import Environment

myenv = Environment.from_conda_specification(name=""myenv"", file_path=""myenv.yml"")
inference_config = InferenceConfig(entry_script=""score.py"", environment=myenv)

service = Model.deploy(workspace=ws,
                       name='sklearn-mnist-svc',
                       models=[model], 
                       inference_config=inference_config,
                       deployment_config=aciconfig)

service.wait_for_deployment(show_output=True)

print(service.scoring_uri)
</code></pre>",4.0,2020-03-12 09:36:25.480000 UTC,2020-03-12 11:19:48.083000 UTC,-2.0,['azure-machine-learning-service']
"Automatically ""stop"" Sagemaker notebook instance after inactivity?","<p>I have a Sagemaker Jupyter notebook instance that I keep leaving online overnight by mistake, unnecessarily costing money... </p>

<p>Is there any way to automatically stop the Sagemaker notebook instance when there is no activity for say, 1 hour? Or would I have to make a custom script?</p>",4,0,2018-12-04 09:18:11.383000 UTC,2.0,,19,amazon-web-services|aws-lambda|amazon-cloudwatch|amazon-sagemaker,12683,2011-03-10 10:26:00.990000 UTC,2022-09-24 23:25:14.187000 UTC,,2563,121,0,167,"<p>You can use <a href=""https://aws.amazon.com/blogs/machine-learning/customize-your-amazon-sagemaker-notebook-instances-with-lifecycle-configurations-and-the-option-to-disable-internet-access/"" rel=""noreferrer"">Lifecycle configurations</a> to set up an automatic job that will stop your instance after inactivity.</p>

<p>There's <a href=""https://github.com/aws-samples/amazon-sagemaker-notebook-instance-lifecycle-config-samples"" rel=""noreferrer"">a GitHub repository</a> which has samples that you can use. In the repository, there's a <a href=""https://github.com/aws-samples/amazon-sagemaker-notebook-instance-lifecycle-config-samples/blob/master/scripts/auto-stop-idle/on-start.sh"" rel=""noreferrer"">auto-stop-idle</a> script which will shutdown your instance once it's idle for more than 1 hour.</p>

<p>What you need to do is</p>

<ol>
<li>to create a Lifecycle configuration using the script and</li>
<li>associate the configuration with the instance. You can do this when you edit or create a Notebook instance.</li>
</ol>

<p>If you think 1 hour is too long you can tweak the script. <a href=""https://github.com/aws-samples/amazon-sagemaker-notebook-instance-lifecycle-config-samples/blob/master/scripts/auto-stop-idle/on-start.sh#L17"" rel=""noreferrer"">This line</a> has the value.</p>",1.0,2019-10-15 09:46:13.200000 UTC,,26.0,['amazon-sagemaker']
Difference in usecases for AWS Sagemaker vs Databricks?,"<p>I was looking at Databricks because it integrates with AWS services like Kinesis, but it looks to me like SageMaker is a direct competitor to Databricks? We are heavily using AWS, is there any reason to add DataBricks into the stack or odes SageMaker fill the same role?</p>",2,0,2019-03-13 00:23:26.600000 UTC,3.0,,9,apache-spark|pyspark|databricks|amazon-sagemaker,11894,2015-01-15 17:43:03.700000 UTC,2022-08-23 22:54:25.603000 UTC,,1387,51,1,153,"<p>SageMaker is a great tool for deployment, it simplifies a lot of processes configuring containers, you only need to write 2-3 lines to deploy the model as an endpoint and use it.  SageMaker also provides the dev platform (Jupyter Notebook) which supports Python and Scala (sparkmagic kernal) developing, and i managed installing external scala kernel in jupyter notebook. Overall, SageMaker provides end-to-end ML services. Databricks has unbeatable Notebook environment for Spark development. </p>

<p>Conclusion</p>

<ol>
<li><p>Databricks is a better platform for Big data(scala, pyspark) Developing.(unbeatable notebook environment)</p></li>
<li><p>SageMaker is better for Deployment. and if you are not working on big data, SageMaker is a perfect choice working with (Jupyter notebook + Sklearn + Mature containers + Super easy deployment). </p></li>
<li><p>SageMaker provides ""real time inference"", very easy to build and deploy, very impressive. you can check the official SageMaker Github.
<a href=""https://github.com/awslabs/amazon-sagemaker-examples/tree/master/sagemaker-python-sdk/scikit_learn_inference_pipeline"" rel=""noreferrer"">https://github.com/awslabs/amazon-sagemaker-examples/tree/master/sagemaker-python-sdk/scikit_learn_inference_pipeline</a></p></li>
</ol>",4.0,2019-03-20 21:40:34.270000 UTC,,14.0,['amazon-sagemaker']
How best to convert from azure blob csv format to pandas dataframe while running notebook in azure ml,"<p>I have a number of large csv (tab delimited) data stored as azure blobs, and I want to create a pandas dataframe from these. I can do this locally as follows:</p>

<pre><code>from azure.storage.blob import BlobService
import pandas as pd
import os.path

STORAGEACCOUNTNAME= 'account_name'
STORAGEACCOUNTKEY= ""key""
LOCALFILENAME= 'path/to.csv'        
CONTAINERNAME= 'container_name'
BLOBNAME= 'bloby_data/000000_0'

blob_service = BlobService(account_name=STORAGEACCOUNTNAME, account_key=STORAGEACCOUNTKEY)

# Only get a local copy if haven't already got it
if not os.path.isfile(LOCALFILENAME):
    blob_service.get_blob_to_path(CONTAINERNAME,BLOBNAME,LOCALFILENAME)

df_customer = pd.read_csv(LOCALFILENAME, sep='\t')
</code></pre>

<p>However, when running the notebook on azure ML notebooks, I can't 'save a local copy' and then read from csv, and so I'd like to do the conversion directly (something like pd.read_azure_blob(blob_csv) or just pd.read_csv(blob_csv) would be ideal).</p>

<p>I can get to the desired end result (pandas dataframe for blob csv data), if I first create an azure ML workspace, and then read the datasets into that, and finally using <a href=""https://github.com/Azure/Azure-MachineLearning-ClientLibrary-Python"" rel=""noreferrer"">https://github.com/Azure/Azure-MachineLearning-ClientLibrary-Python</a> to access the dataset as a pandas dataframe, but I'd prefer to just read straight from the blob storage location.</p>",5,0,2015-10-12 23:31:58.393000 UTC,3.0,2015-10-14 09:20:26.277000 UTC,12,python|azure|pandas|azure-blob-storage|azure-machine-learning-studio,22789,2012-08-30 20:36:52.420000 UTC,2018-06-14 21:10:57.573000 UTC,,395,15,0,26,"<p>I think you want to use <code>get_blob_to_bytes</code>, <code>or get_blob_to_text</code>; these should output a string which you can use to create a dataframe as</p>

<pre><code>from io import StringIO
blobstring = blob_service.get_blob_to_text(CONTAINERNAME,BLOBNAME)
df = pd.read_csv(StringIO(blobstring))
</code></pre>",2.0,2015-10-12 23:38:59.743000 UTC,2018-12-18 06:59:18.717000 UTC,17.0,['azure-machine-learning-studio']
How can I access the Workspace object from a training script in AzureML?,"<p>I want to access the Workspace object in my <code>train.py</code> script, when running in an Estimator.  </p>

<p>I currently can access the Run object, using the following code:</p>

<pre class=""lang-py prettyprint-override""><code>run = Run.get_context()
</code></pre>

<p>But I cannot seem to get my hands on the Workspace object in my training script.  I would use this mostly to get access to the Datastores and Datasets (as I would hope to keep all data set references inside the training script, instead of passing them as input datasets)</p>

<p>Any idea if/how this is possible ?</p>",1,0,2020-06-03 11:28:19.477000 UTC,1.0,,4,python|azure|azure-machine-learning-service,1148,2013-02-12 07:50:30.743000 UTC,2022-09-21 18:28:12.907000 UTC,Belgium,2947,297,16,355,"<p>Sure, try this:</p>

<pre><code>from azureml.core.run import Run
run = Run.get_context()
ws = run.experiment.workspace
</code></pre>",0.0,2020-06-03 15:25:55.080000 UTC,,12.0,['azure-machine-learning-service']
Use Azure Machine learning to detect symbol within an image,"<p>4 years ago I posted <a href=""https://stackoverflow.com/q/6999920/411094"">this question</a> and got a few answers that were unfortunately outside my skill level.  I just attended a build tour conference where they spoke about machine learning and this got me thinking of the possibility of using ML as a solution to my problem.  i found <a href=""https://gallery.azureml.net/MachineLearningAPI/02ce55bbc0ab4fea9422fe019995c02f"" rel=""noreferrer"">this</a> on the azure site but i dont think it will help me because its scope is pretty narrow.</p>

<p>Here is what i am trying to achieve:</p>

<p>i have a source image:</p>

<p><img src=""https://i.stack.imgur.com/6y76s.jpg"" alt=""source image""></p>

<p>and i want to which one of the following symbols (if any) are contained in the image above:</p>

<p><img src=""https://i.stack.imgur.com/SuHkU.jpg"" alt=""symbols""></p>

<p>the compare needs to support minor distortion, scaling, color differences, rotation, and brightness differences.</p>

<p>the number of symbols to match will ultimately at least be greater than 100.</p>

<p>is ML a good tool to solve this problem?  if so, any starting tips?</p>",1,0,2015-06-16 05:58:53.360000 UTC,3.0,2017-05-23 12:10:45.110000 UTC,14,opencv|azure|image-processing|machine-learning|azure-machine-learning-studio,6324,2010-08-04 17:41:06.900000 UTC,2022-09-23 19:56:34.233000 UTC,"Los Angeles, CA",1141,32,0,109,"<p>As far as I know, Project Oxford (MS Azure CV API) wouldn't be suitable for your task. Their APIs are very focused to Face related tasks (detection, verification, etc), OCR and Image description. And apparently you can't extend their models or train new ones from the existing ones.</p>

<p>However, even though I don't know an out of the box solution for your object detection problem; there are easy enough approaches that you could try and that would give you some start point results.</p>

<p>For instance, here is a naive method you could use:</p>

<p><strong>1) Create your dataset:</strong>
    This is probably the more tedious step and paradoxically a crucial one. I will assume you have a good amount of images to work with. What would you need to do is to pick a fixed window size and extract positive and negative examples.
<img src=""https://i.stack.imgur.com/H4uC5.png"" alt=""enter image description here""></p>

<p>If some of the images in your dataset are in different sizes you would need to rescale them to a common size. You don't need to get too crazy about the size, probably 30x30 images would be more than enough. To make things easier I would turn the images to gray scale too. </p>

<p><strong>2) Pick a classification algorithm and train it:</strong>
    There is an awful amount of classification algorithms out there. But if you are new to machine learning I will pick the one I would understand the most. Keeping that in mind, I would check out logistic regression which give decent results, it's easy enough for starters and have a lot of libraries and tutorials. For instance, <a href=""http://blog.yhathq.com/posts/logistic-regression-and-python.html"" rel=""noreferrer"">this one</a> or <a href=""https://msdn.microsoft.com/en-us/magazine/dn948113.aspx"" rel=""noreferrer"">this one</a>. At first I would say to focus in a binary classification problem (like if there is an UD logo in the picture or not) and when you master that one you can jump to the multi-class case. There are resources for that <a href=""http://www.codeproject.com/Articles/821347/MultiClass-Logistic-Classifier-in-Python"" rel=""noreferrer"">too</a> or you can always have several models one per logo and run this recipe for each one separately. </p>

<p>To train your model, you just need to read the images generated in the step 1 and turn them into a vector and label them accordingly. That would be the  dataset that will feed your model. If you are using images in gray scale, then each position in the vector would correspond to a pixel value in the range 0-255. Depending on the algorithm you might need to rescale those values to the range [0-1] (this is because some algorithms perform better with values in that range). Notice that rescaling the range in this case is fairly easy (new_value = value/255).</p>

<p>You also need to split your dataset, reserving some examples for training, a subset for validation and another one for testing. Again, there are different ways to do this, but I'm keeping this answer as naive as possible.</p>

<p><strong>3) Perform the detection:</strong>
    So now let's start the fun part. Given any image you want to run your model and produce coordinates in the picture where there is a logo. There are different ways to do this and I will describe one that probably <strong>is not the best nor the more efficient</strong>, but it's easier to develop in my opinion.</p>

<p>You are going to scan the picture, extracting the pixels in a ""window"", rescaling those pixels to the size you selected in step 1 and then feed them to your model. </p>

<p><img src=""https://i.stack.imgur.com/VGk3f.png"" alt=""Extracting windows to feed the model""></p>

<p>If the model give you a positive answer then you mark that window in the original image. Since the logo might appear in different scales you need to repeat this process with different window sizes. You also would need to tweak the amount of space between windows.</p>

<p><strong>4) Rinse and repeat:</strong>
    At the first iteration it's very likely that you will get a lot of false positives. Then you need to take those as negative examples and retrain your model. This would be an iterative process and hopefully on each iteration you will have less and less false positives and fewer false negatives.</p>

<p>Once you are reasonable happy with your solution, you might want to improve it. You might want to try other classification algorithms like <a href=""https://en.wikipedia.org/wiki/Support_vector_machine"" rel=""noreferrer"">SVM</a> or <a href=""https://en.wikipedia.org/wiki/Deep_learning"" rel=""noreferrer"">Deep Learning Artificial Neural Networks</a>, or to try better object detection frameworks like <a href=""https://en.wikipedia.org/wiki/Viola%E2%80%93Jones_object_detection_framework"" rel=""noreferrer"">Viola-Jones</a>. Also, you will probably need to use <a href=""https://en.wikipedia.org/wiki/Cross-validation_%28statistics%29"" rel=""noreferrer"">crossvalidation</a> to compare all your solutions (you can actually use crossvalidation from the beginning). By this moment I bet you would be confident enough that you would like to use OpenCV or another ready to use framework in which case you will have a fair understanding of what is going on under the hood. </p>

<p>Also you could just disregard all this answer and go for an OpenCV object detection tutorial like this <a href=""http://note.sonots.com/SciSoftware/haartraining.html"" rel=""noreferrer"">one</a>. Or take another answer from another question like this <a href=""https://stackoverflow.com/questions/10168686/algorithm-improvement-for-coca-cola-can-shape-recognition?rq=1"">one</a>. Good luck!</p>",3.0,2015-06-16 15:38:50.290000 UTC,2017-05-23 12:18:30.023000 UTC,22.0,['azure-machine-learning-studio']
By how much can i approx. reduce disk volume by using dvc?,"<p>I want to classify ~1m+ documents and have a Version Control System for in- and Output of the corresponding model. </p>

<p>The data changes over time:</p>

<ul>
<li>sample size increases over time</li>
<li>new Features might appear</li>
<li>anonymization procedure might Change over time</li>
</ul>

<p>So basically ""everything"" might change: amount of observations, Features and the values.
We are interested in making the ml model Building reproducible without using 10/100+ GB 
of disk volume, because we save all updated versions of Input data. Currently the volume size of the data is ~700mb.</p>

<p>The most promising tool i found is: <a href=""https://github.com/iterative/dvc"" rel=""noreferrer"">https://github.com/iterative/dvc</a>. Currently the data
is stored in a database in loaded in R/Python from there.</p>

<p><strong>Question:</strong></p>

<p>How much disk volume can be (very approx.) saved by using dvc? </p>

<p>If one can roughly estimate that. I tried to find out if only the ""diffs"" of the data are saved. I didnt find much info by reading through: <a href=""https://github.com/iterative/dvc#how-dvc-works"" rel=""noreferrer"">https://github.com/iterative/dvc#how-dvc-works</a> or other documentation. </p>

<p><strong>I am aware that this is a very vague question. And it will highly depend on the dataset. However, i would still be interested in getting a very approximate idea.</strong></p>",1,0,2020-02-23 18:31:41.247000 UTC,3.0,2020-02-23 19:28:48.287000 UTC,7,python|sql|r|git|dvc,689,2017-08-30 12:46:30.907000 UTC,2022-03-11 18:10:58.673000 UTC,,1365,145,3,193,"<p>Let me try to summarize how does DVC store data and I hope you'll be able to figure our from this how much space will be saved/consumed in your specific scenario.</p>

<p><strong>DVC is storing and deduplicating data on the individual <em>file level</em>.</strong> So, what does it usually mean from a practical perspective.</p>

<p>I will use <code>dvc add</code> as an example, but the same logic applies to all commands that save data files or directories into DVC cache - <code>dvc add</code>, <code>dvc run</code>, etc.</p>

<h2>Scenario 1: Modifying file</h2>

<p>Let's imagine I have a single 1GB XML file. I start tracking it with DVC:</p>

<pre class=""lang-sh prettyprint-override""><code>$ dvc add data.xml
</code></pre>

<p>On the modern file system (or if <code>hardlinks</code>, <code>symlinks</code> are enabled, see <a href=""https://dvc.org/doc/user-guide/large-dataset-optimization"" rel=""noreferrer"">this</a> for more details) after this command we still consume 1GB (even though file is moved into DVC cache and is still present in the workspace).</p>

<p>Now, let's change it a bit and save it again:</p>

<pre class=""lang-sh prettyprint-override""><code>$ echo ""&lt;test/&gt;"" &gt;&gt; data.xml
$ dvc add data.xml
</code></pre>

<p>In this case we will have 2GB consumed. <strong>DVC does not do diff between two versions of the same file</strong>, neither it splits files into chunks or blocks to understand that only small portion of data has changed.</p>

<blockquote>
  <p>To be precise, it calculates <code>md5</code> of each file and save it in the content addressable key-value storage. <code>md5</code> of the files serves as a key (path of the file in cache) and value is the file itself:</p>
  
  <pre class=""lang-sh prettyprint-override""><code>(.env) [ivan@ivan ~/Projects/test]$ md5 data.xml
0c12dce03223117e423606e92650192c

(.env) [ivan@ivan ~/Projects/test]$ tree .dvc/cache
.dvc/cache
└── 0c
   └── 12dce03223117e423606e92650192c

1 directory, 1 file

(.env) [ivan@ivan ~/Projects/test]$ ls -lh data.xml
data.xml ----&gt; .dvc/cache/0c/12dce03223117e423606e92650192c (some type of link)
</code></pre>
</blockquote>

<h2>Scenario 2: Modifying directory</h2>

<p>Let's now imagine we have a single large 1GB directory <code>images</code> with a lot of files:</p>

<pre class=""lang-sh prettyprint-override""><code>$ du -hs images
1GB

$ ls -l images | wc -l
1001

$ dvc add images
</code></pre>

<p>At this point we still consume 1GB. Nothing has changed. But if we modify the directory by adding more files (or removing some of them):</p>

<pre><code>$ cp /tmp/new-image.png images

$ ls -l images | wc -l
1002

$ dvc add images
</code></pre>

<p>In this case, after saving the new version we <strong>still close to 1GB</strong> consumption. <strong>DVC calculates diff on the directory level.</strong> It won't be saving all the files that were existing before in the directory.</p>

<p>The same logic applies to all commands that save data files or directories into DVC cache - <code>dvc add</code>, <code>dvc run</code>, etc.</p>

<p>Please, let me know if it's clear or we need to add more details, clarifications.</p>",4.0,2020-02-23 19:57:47.857000 UTC,,12.0,['dvc']
How can I get Pickle ML model from Minio and send it as a fastapi response as an API?,"<p><strong>This code i am running to return pickle model to fastapi but its throwing error. How can i pass pickle model by connecting to minio</strong></p>
<pre><code>@app.post(f&quot;{ROOT_PATH}/model/&quot;)
async def get_model():
    mlflow.set_tracking_uri(&quot;xxxxx&quot;)
    client = Minio(&quot;xxxxxx&quot;)
    bucket=&quot;mlflow&quot;
    path=&quot;6/d344njdnjndkdkj/artifacts/model/model.pkl&quot;
    ss = client.get_object(bucket, path, &quot;model.pkl&quot;)
    return ss
</code></pre>
<p>Its throwing error in fastapi docs</p>",0,3,2022-09-16 11:22:40.903000 UTC,,,-2,python|fastapi|minio|mlflow,24,2022-09-16 11:13:41.660000 UTC,2022-09-23 12:47:45.133000 UTC,,1,0,0,1,,,,,,['mlflow']
Accessing pickle file through API,"<p>I have a pickle file parameters.pkl containing some parameters and their values of a model. Is there a process to store it in Microsoft Azure Machine Learning Studio as endpoint. So that we can access the parameters and their values through API at some later stage.</p>
<pre><code>The pickle file has been created through the following process:

dict={'scaler': scaler,
'features': z_tags,
'Z_reconstruction_loss': Z_reconstruction_loss}

pickle.dump(dict, open('parameters.pkl', 'wb'))
</code></pre>",0,1,2022-08-11 13:26:57.597000 UTC,,,0,python-3.x|azure|python-2.7|pickle|azure-machine-learning-studio,33,2015-05-15 10:32:06.620000 UTC,2022-09-21 10:30:16.813000 UTC,,543,17,2,125,,,,,,['azure-machine-learning-studio']
Sagemaker lifecycle config: could not find conda environment conda_python3 | works fine in terminal,"<p>I have prototyped some code using conda_python3 environment in SageMaker notebook instance. When I open terminal, activate environment and run code it works well but when I try to automate the process by using lifecycle configuration, system is unable to identify the environment. I get the following message on cloud watch</p>
<pre><code>2020-07-16T11:25:37.576+05:30
Could not find conda environment: conda_python3

2020-07-16T11:25:37.576+05:30
You can list all discoverable environments with `conda info --envs`.
</code></pre>
<p>My life cycle config code:</p>
<pre><code>#!bin/bash

set -e

ENVIRONMENT=conda_python3

source /home/ec2-user/anaconda3/bin/activate &quot;$ENVIRONMENT&quot;

python /home/ec2-user/SageMaker/scrub_testing.py

source /home/ec2-user/anaconda3/bin/deactivate
</code></pre>
<p>What am I doing wrong?</p>",1,0,2020-07-16 06:35:58.587000 UTC,,,1,python-3.x|amazon-web-services|anaconda|conda|amazon-sagemaker,760,2015-09-25 10:16:37.327000 UTC,2020-11-25 09:09:00.757000 UTC,"Dallas, TX, USA",19,0,0,3,,,,,,['amazon-sagemaker']
"ValueError: Exception encountered when calling layer ""lstm_cell"" (type LSTMCell)","<p>I am trying to build a combination of deep CNN-LSTM networks for binary classification using small datasets. I use reshape function to change the shape, and that perfectly works in Google Colab and my local Jupyter notebook but the following a value error while compiling this code in <strong>AWS Sagemaker</strong>.</p>
<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
/tmp/ipykernel_4202/2122205439.py in &lt;cell line: 3&gt;()
      1 tf.keras.backend.clear_session()
      2 
----&gt; 3 model_06 = get_model()

/tmp/ipykernel_4202/3426630573.py in get_model()
     29     x = Reshape((64, 256))(x)
     30     print(f&quot;After Reshaping(): {x.shape}&quot;)
---&gt; 31     x = (LSTM(512, activation = &quot;relu&quot;))(x)
     32 
     33     # Fully Connected layer

~/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/keras/layers/rnn/base_rnn.py in __call__(self, inputs, initial_state, constants, **kwargs)
    513 
    514     if initial_state is None and constants is None:
--&gt; 515       return super(RNN, self).__call__(inputs, **kwargs)
    516 
    517     # If any of `initial_state` or `constants` are specified and are Keras

~/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/keras/utils/traceback_utils.py in error_handler(*args, **kwargs)
     65     except Exception as e:  # pylint: disable=broad-except
     66       filtered_tb = _process_traceback_frames(e.__traceback__)
---&gt; 67       raise e.with_traceback(filtered_tb) from None
     68     finally:
     69       del filtered_tb

~/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/keras/backend.py in bias_add(x, bias, data_format)
   6362   if len(bias_shape) == 1:
   6363     if data_format == 'channels_first':
-&gt; 6364       return tf.nn.bias_add(x, bias, data_format='NCHW')
   6365     return tf.nn.bias_add(x, bias, data_format='NHWC')
   6366   if ndim(x) in (3, 4, 5):

ValueError: Exception encountered when calling layer &quot;lstm_cell&quot; (type LSTMCell).

Shape must be at least rank 3 but is rank 2 for '{{node lstm/lstm_cell/BiasAdd}} = BiasAdd[T=DT_FLOAT, data_format=&quot;NCHW&quot;](lstm/lstm_cell/add, lstm/lstm_cell/BiasAdd/ReadVariableOp)' with input shapes: [?,2048], [2048].

Call arguments received by layer &quot;lstm_cell&quot; (type LSTMCell):
  • inputs=tf.Tensor(shape=(None, 256), dtype=float32)
  • states=('tf.Tensor(shape=(None, 512), dtype=float32)', 'tf.Tensor(shape=(None, 512), dtype=float32)')
  • training=None
</code></pre>
<p>Here is the model,</p>
<pre><code>    # input layer
    inputs = tf.keras.layers.Input(shape=(3, 256, 256), name=&quot;input_layer&quot;)
    
    # 2nd convolution block
    x = tf.keras.layers.Conv2D(filters=16, kernel_size=(3, 3), activation='relu', padding='same')(inputs)
    x = tf.keras.layers.MaxPooling2D((2, 2))(x)
    
    # 2nd convolution block
    x = tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same')(x)
    x = tf.keras.layers.MaxPooling2D((2, 2))(x)
    
    # 3rd convolution block
    x = tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same')(x)
    x = tf.keras.layers.MaxPooling2D((2, 2))(x)
    
    # 4th convolution block
    x = tf.keras.layers.Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same')(x)
    x = tf.keras.layers.MaxPooling2D((2, 2))(x)
    
    # 5th convolution block
    x = tf.keras.layers.Conv2D(filters=256, kernel_size=(3, 3), activation='relu', padding='same')(x)
    x = tf.keras.layers.MaxPooling2D((2, 2))(x)
    
    print(f&quot;Before Reshaping(): {x.shape}&quot;)
    
    # LSTM
    x = Reshape((64, 256))(x)
    print(f&quot;After Reshaping(): {x.shape}&quot;)
    x = (LSTM(50, activation = &quot;relu&quot;))(x)

    # Fully Connected layer
    x = Flatten()(x)
    x = tf.keras.layers.Dense(1024, activation = 'relu')(x)

    # Output layer
    outputs = tf.keras.layers.Dense(1, activation=&quot;sigmoid&quot;, name=&quot;output_layer&quot;)(x)

    # Combine the inputs with the outputs into a model
    model = tf.keras.Model(inputs, outputs)
    
    return model
</code></pre>",0,2,2022-08-20 17:09:01.097000 UTC,,,0,deep-learning|conv-neural-network|lstm|tensorflow2.0|amazon-sagemaker,67,2021-11-12 12:31:01.433000 UTC,2022-09-23 17:02:05.963000 UTC,,21,5,0,3,,,,,,['amazon-sagemaker']
difference between start_logging and run.get_context,"<p>I just wanted to know the difference between  start_logging and run.get_context in azure ml.
when they are used? what is the purpose of each function??</p>",2,0,2021-03-10 16:04:20.933000 UTC,,,1,azure|azure-functions|azure-machine-learning-service,206,2020-01-10 04:31:18.200000 UTC,2021-07-13 08:43:48.770000 UTC,,11,0,0,6,,,,,,['azure-machine-learning-service']
Is it possible to output a list of artifacts of the same type using kubeflow pipelines?,"<p>I'm trying to output multiple confusion matrices from a kubeflow pipeline component, like in this example that only outputs one:</p>
<pre><code>def eval_model(
    test_set: Input[Dataset],
    xgb_model: Input[Model],
    metrics: Output[ClassificationMetrics],
    smetrics: Output[Metrics]
):
    from xgboost import XGBClassifier
    import pandas as pd

    data = pd.read_csv(test_set.path)
    model = XGBClassifier()
    model.load_model(xgb_model.path)

    score = model.score(
        data.drop(columns=[&quot;target&quot;]),
        data.target,
    )

    from sklearn.metrics import roc_curve
    y_scores =  model.predict_proba(data.drop(columns=[&quot;target&quot;]))[:, 1]
    fpr, tpr, thresholds = roc_curve(
        y_true=data.target.to_numpy(), y_score=y_scores, pos_label=True
    )
    metrics.log_roc_curve(fpr.tolist(), tpr.tolist(), thresholds.tolist())

    from sklearn.metrics import confusion_matrix
    y_pred = model.predict(data.drop(columns=[&quot;target&quot;]))

    metrics.log_confusion_matrix(
       [&quot;False&quot;, &quot;True&quot;],
       confusion_matrix(
            data.target, y_pred
       ).tolist(),  # .tolist() to convert np array to list.
    )

    xgb_model.metadata[&quot;test_score&quot;] = float(score)
    smetrics.log_metric(&quot;score&quot;, float(score))
</code></pre>
<p>Do you know if it's possible to output multiple confusion matrices without having to define multiple Output args?</p>
<p>I can only think of defining the function as:</p>
<pre><code>def eval_model(
    test_set: Input[Dataset],
    xgb_model: Input[Model],
    metrics: Output[List[ClassificationMetrics]],
    smetrics: Output[Metrics]
 ):
</code></pre>
<p>but that doesn't work because the list cannot call <code>log_confusion_matrix</code></p>
<p>In their source code I didn't find much: <a href=""https://github.com/kubeflow/pipelines/blob/55a2fb5c20011b01945c9867ddff0d39e9db1964/sdk/python/kfp/v2/components/types/artifact_types.py#L255-L256"" rel=""nofollow noreferrer"">https://github.com/kubeflow/pipelines/blob/55a2fb5c20011b01945c9867ddff0d39e9db1964/sdk/python/kfp/v2/components/types/artifact_types.py#L255-L256</a></p>",1,4,2021-12-17 17:55:32.867000 UTC,,,2,kubeflow|kubeflow-pipelines|google-cloud-vertex-ai,738,2018-02-10 21:10:15.053000 UTC,2022-09-23 15:42:30.400000 UTC,Spain,21,2,0,2,,,,,,['google-cloud-vertex-ai']
Autolog metrics mlflow,"<p>I have a question about autologging. I use it and I want to record another metrics.
Can I change the recorded metrics for autologging?</p>
<p>I found a class in the documentation <code>_AutologgingMetricsManager</code> but I don't know how can I use it.</p>
<p>Thanks,
Irina</p>",0,1,2021-08-23 04:15:08.827000 UTC,,2021-08-23 06:51:30.177000 UTC,1,metrics|mlflow,54,2021-08-23 04:05:03.070000 UTC,2022-07-06 11:13:57.790000 UTC,,11,0,0,0,,,,,,['mlflow']
not be able to split dataset using recommender split in azure Ml studio,"<p>I am doing a crop recommender system using the Matchbox recommender system in Azure ml studio.
<a href=""https://i.stack.imgur.com/BLirz.png"" rel=""nofollow noreferrer"">Dataset</a></p>
<p>when I split data, it did not make the split. one dataset becomes full and another becomes null.</p>
<p>how to overcome this?</p>
<p><a href=""https://i.stack.imgur.com/slLVh.png"" rel=""nofollow noreferrer"">this is the development</a></p>",2,1,2021-09-12 06:27:51.887000 UTC,,,0,azure|recommendation-engine|azure-machine-learning-studio,99,2019-09-13 07:40:20.910000 UTC,2022-07-20 18:08:27.490000 UTC,"Kelaniya, Sri Lanka",3,0,0,13,,,,,,['azure-machine-learning-studio']
how to access file or folders from azure ml service dataset?,"<p>Currently I am working on azure ml service where I have dataset in azure ml named as 'voice_recognition_expreimnt'.
I access this dataset by this code:</p>
<pre><code>file_dataset =  Dataset.get_by_name(workspace=ws, name='voice_recognition_expreimnt')
</code></pre>
<p>Now I want to access all file or folders in the dataset. So how can I traverse through all path in my dataset. I search a lot but I can't find any solution. So please help me</p>",1,1,2020-09-17 06:18:37.177000 UTC,,,0,python|azure|azure-machine-learning-service,1229,2020-07-23 21:56:14.623000 UTC,2022-03-26 12:48:37.827000 UTC,,49,8,0,28,,,,,,['azure-machine-learning-service']
Error in Call to Sagemaker Endpoint with Lambda and API Gateway,"<p>I try make predictions with a TensorFlow-Keras model in Sagemaker, but recive the next errors:</p>
<p>In Amazon Cloudwatch, for Lambda Function:</p>
<pre><code>An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (415) from model with message &quot;
{
    &quot;error&quot;: &quot;Unsupported Media Type: application/x-image&quot;
}
</code></pre>
<p>In Cloudwatch, for Sagemaker:</p>
<pre><code>F external/org_tensorflow/tensorflow/core/util/tensor_format.h:426] Check failed: index &gt;= 0 &amp;&amp; index &lt; num_total_dims Invalid index from the dimension: 3, 0, C
</code></pre>
<p>Data is an image send in base64, the Lambda function convert this img to bytes, Lambda Function is:</p>
<pre><code>def lambda_handler(event, context):
    print(&quot;Received event: &quot; + json.dumps(event, indent=2))
    
    data = json.loads(json.dumps(event))
    payload = data['foto']
    image = base64.b64decode(payload)
    print(type(image))
    
    try:
        response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME,
                                            ContentType='application/x-image',
                                            Body=image)
        print(response)
    except Exception as e:
        print(&quot;error en inferencia:&quot;)
        print(e)

    return payload # only for test

</code></pre>",2,0,2021-02-22 13:11:30.490000 UTC,1.0,2021-03-04 08:33:20.787000 UTC,1,amazon-web-services|tensorflow|aws-lambda|amazon-sagemaker,655,2020-06-30 03:52:09.130000 UTC,2021-12-24 19:50:54.610000 UTC,"Concepción, Chile",13,0,0,6,,,,,,['amazon-sagemaker']
Model pkl not found by SageMaker inference,"<p>I have the following model.tar.gz structure (as described <a href=""https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html#deploy-pytorch-models"" rel=""nofollow noreferrer"">here</a>):</p>
<pre><code>model
|- cuts34.pkl
|- code
  |- inference.py
  |- requirements.txt
</code></pre>
<p>I deploy locally my trained model (for inference) like this:</p>
<pre><code>class Predictor(RealTimePredictor):
    def __init__(self, endpoint_name, sagemaker_session):
        super().__init__(endpoint_name,
                         sagemaker_session=sagemaker_session,
                         serializer=None,
                         deserializer=None,
                         content_type='text/plain'
                         )

entry_point = './model/code/inference.py'
model_data = 'model.tar.gz'

model = PyTorchModel(model_data=model_data,
                     framework_version=&quot;1.5.0&quot;,
                     py_version=&quot;py3&quot;,
                     role=role,
                     entry_point=entry_point
                     )

serverless_config = ServerlessInferenceConfig()
predictor = model.deploy(instance_type=&quot;local&quot;, initial_instance_count=1)
</code></pre>
<p><code>model_fn</code> in <code>inference.py</code> looks like this:</p>
<pre><code>def model_fn(model_dir):
    logger.info(&quot;*** Loading the model ...&quot;)
    path = os.path.join(model_dir, &quot;cuts34.pkl&quot;)
    learner = load_learner(path)
    return learner
</code></pre>
<p>When invoking the <code>sagemaker.local.LocalSagemakerRuntimeClient()</code> with</p>
<pre><code>response = runtime.invoke_endpoint(
    EndpointName=endpoint_name,
    ContentType=content_type,
    Body=payload
)
</code></pre>
<p>I keep getting the following error:</p>
<pre><code>b'[Errno 2] No such file or directory: \'/.sagemaker/mms/models/model/cuts34.pkl\'\nTraceback (most recent call last):\n  File &quot;/opt/conda/lib/python3.6/site-packages/sagemaker_inference/transformer.py&quot;, line 110, in transform\n    self.validate_and_initialize(model_dir=model_dir)\n  File &quot;/opt/conda/lib/python3.6/site-packages/sagemaker_inference/transformer.py&quot;, line 158, in validate_and_initialize\n    self._model = self._model_fn(model_dir)\n  File &quot;/opt/ml/model/code/inference.py&quot;, line 265, in model_fn\n    learner = torch.load(path, map_location=device)\n  File &quot;/opt/conda/lib/python3.6/site-packages/torch/serialization.py&quot;, line 584, in load\n    with _open_file_like(f, \'rb\') as opened_file:\n  File &quot;/opt/conda/lib/python3.6/site-packages/torch/serialization.py&quot;, line 234, in _open_file_like\n    return _open_file(name_or_buffer, mode)\n  File &quot;/opt/conda/lib/python3.6/site-packages/torch/serialization.py&quot;, line 215, in __init__\n    super(_open_file, self).__init__(open(name, mode))\nFileNotFoundError: [Errno 2] No such file or directory: \'/.sagemaker/mms/models/model/cuts34.pkl\'\n'
</code></pre>
<p>It seems that the model pkl cannot be found. Any thoughts on why not?</p>",1,2,2022-05-02 14:18:13.670000 UTC,,2022-05-03 10:01:01.627000 UTC,0,python|pytorch|amazon-sagemaker,135,2012-11-16 20:38:04.370000 UTC,2022-09-23 09:27:40.107000 UTC,"Targu-Mures, Romania",309,95,1,98,,,,,,['amazon-sagemaker']
Predict batch of images with a SageMaker model,"<p>Thanks by advance for your help to solve this issue.
I trained a model on Sagemaker. This is a TensorFlow estimator taking images as input, computing high-level features (ie bottlenecks) with InceptionV3, then using a dense layer to predict new classes.</p>

<p>It kinda works: I can train it, serve it, and predict new images ONE AFTER ANOTHER.</p>

<p>Now I'd like to predict a whole batch of images at once, in one unique HTTP call / predict() call. How?</p>

<p>Here is how I do:</p>

<pre><code>from IPython.display import Image
import numpy as np
from keras.preprocessing import image

estimator = TensorFlow(entry_point=..., ...)
estimator.fit(train_data_location)
predictor = estimator.deploy(initial_instance_count=1,
                         instance_type='ml.m4.xlarge')

image_list = [
    'e9bfa679-31bb-464e-9d9f-3bdb0ef9c121.jpeg',  # 131
    'b27880e1-6de8-43cf-a684-bb02aef1e44b.jpeg',  # 170
]
directory = '/path/to/dir/'
images = np.empty((len(image_list), 299, 299, 3), dtype=np.float32)
# for filename in image_list:
for i,filename in enumerate(image_list):
    path = os.path.join(directory, filename)
    Image(path)
    img = image.load_img(path, target_size=(299, 299))
    x = image.img_to_array(img)
    images[i] = x

print(images.shape)
# to_send = images[-1]  # works for a unique image
to_send = images  # doesn't work for a batch of images
# some other attempts that did not work
# to_send = images.tolist()
# to_send = [images[0].tolist(), images[1].tolist()]
print(np.shape(to_send))

predict_response = predictor.predict(to_send)
print('The model predicted the following classes: \n{}'.format(
    predict_response['outputs']['classes']['int64Val']))
</code></pre>

<p>This fires the following results:</p>

<blockquote>
  <p>(2, 299, 299, 3)</p>
  
  <p>(2, 299, 299, 3)  # Notice here the shape of what I send. So why does it complain about the shape [1,2,299,299,3] in the logs below ??</p>
  
  <p>ModelError: An error occurred
  (ModelError) when calling the InvokeEndpoint operation: Received
  server error (500) from model with message """". See
  <a href=""https://eu-west-1.console.aws.amazon.com/cloudwatch/home?region=eu-west-1#logEventViewer:group=/aws/sagemaker/Endpoints/sagemaker-tensorflow-py2-cpu-2018-02-05-16-48-38-496"" rel=""nofollow noreferrer"">https://eu-west-1.console.aws.amazon.com/cloudwatch/home?region=eu-west-1#logEventViewer:group=/aws/sagemaker/Endpoints/sagemaker-tensorflow-py2-cpu-2018-02-05-16-48-38-496</a>
  in account 047562184710 for more information</p>
</blockquote>

<p>So here are the logs from AWS:</p>

<pre><code># [2018-02-06 09:29:20,937] ERROR in serving: AbortionError(code=StatusCode.INVALID_ARGUMENT, details=""input must be 4-dimensional[1,2,299,299,3]
# #011 [[Node: ResizeBilinear = ResizeBilinear[T=DT_FLOAT, _output_shapes=[[1,299,299,3]], align_corners=false, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](ExpandDims, ResizeBilinear/size)]]"")
# Traceback (most recent call last):
# File ""/opt/amazon/lib/python2.7/site-packages/container_support/serving.py"", line 161, in _invoke
# self.transformer.transform(content, input_content_type, requested_output_content_type)
# File ""/opt/amazon/lib/python2.7/site-packages/tf_container/serve.py"", line 255, in transform
# return self.transform_fn(data, content_type, accepts), accepts
# File ""/opt/amazon/lib/python2.7/site-packages/tf_container/serve.py"", line 180, in f
# prediction = self.predict_fn(input)
# File ""/opt/amazon/lib/python2.7/site-packages/tf_container/serve.py"", line 195, in predict_fn
# return self.proxy_client.request(data)
# File ""/opt/amazon/lib/python2.7/site-packages/tf_container/proxy_client.py"", line 51, in request
# return request_fn(data)
# File ""/opt/amazon/lib/python2.7/site-packages/tf_container/proxy_client.py"", line 79, in predict
# result = stub.Predict(request, self.request_timeout)
# File ""/usr/local/lib/python2.7/dist-packages/grpc/beta/_client_adaptations.py"", line 310, in __call__
# self._request_serializer, self._response_deserializer)
# File ""/usr/local/lib/python2.7/dist-packages/grpc/beta/_client_adaptations.py"", line 196, in _blocking_unary_unary
# raise _abortion_error(rpc_error_call)
# AbortionError: AbortionError(code=StatusCode.INVALID_ARGUMENT, details=""input must be 4-dimensional[1,2,299,299,3]
# #011 [[Node: ResizeBilinear = ResizeBilinear[T=DT_FLOAT, _output_shapes=[[1,299,299,3]], align_corners=false, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](ExpandDims, ResizeBilinear/size)]]"")
# 2018-02-06 09:29:20,937 ERROR - model server - AbortionError(code=StatusCode.INVALID_ARGUMENT, details=""input must be 4-dimensional[1,2,299,299,3]
# #011 [[Node: ResizeBilinear = ResizeBilinear[T=DT_FLOAT, _output_shapes=[[1,299,299,3]], align_corners=false, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](ExpandDims, ResizeBilinear/size)]]"")
# Traceback (most recent call last):
# File ""/opt/amazon/lib/python2.7/site-packages/container_support/serving.py"", line 161, in _invoke
# self.transformer.transform(content, input_content_type, requested_output_content_type)
# File ""/opt/amazon/lib/python2.7/site-packages/tf_container/serve.py"", line 255, in transform
# return self.transform_fn(data, content_type, accepts), accepts
# File ""/opt/amazon/lib/python2.7/site-packages/tf_container/serve.py"", line 180, in f
# prediction = self.predict_fn(input)
# File ""/opt/amazon/lib/python2.7/site-packages/tf_container/serve.py"", line 195, in predict_fn
# return self.proxy_client.request(data)
# File ""/opt/amazon/lib/python2.7/site-packages/tf_container/proxy_client.py"", line 51, in request
# return request_fn(data)
# File ""/opt/amazon/lib/python2.7/site-packages/tf_container/proxy_client.py"", line 79, in predict
# result = stub.Predict(request, self.request_timeout)
# File ""/usr/local/lib/python2.7/dist-packages/grpc/beta/_client_adaptations.py"", line 310, in __call__
# self._request_serializer, self._response_deserializer)
# File ""/usr/local/lib/python2.7/dist-packages/grpc/beta/_client_adaptations.py"", line 196, in _blocking_unary_unary
# raise _abortion_error(rpc_error_call)
# AbortionError: AbortionError(code=StatusCode.INVALID_ARGUMENT, details=""input must be 4-dimensional[1,2,299,299,3]
# #011 [[Node: ResizeBilinear = ResizeBilinear[T=DT_FLOAT, _output_shapes=[[1,299,299,3]], align_corners=false, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](ExpandDims, ResizeBilinear/size)]]"")
# [2018-02-06 09:29:20,956] ERROR in serving: AbortionError(code=StatusCode.INVALID_ARGUMENT, details=""input must be 4-dimensional[1,2,299,299,3]
# #011 [[Node: ResizeBilinear = ResizeBilinear[T=DT_FLOAT, _output_shapes=[[1,299,299,3]], align_corners=false, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](ExpandDims, ResizeBilinear/size)]]"")
# 2018-02-06 09:29:20,956 ERROR - model server - AbortionError(code=StatusCode.INVALID_ARGUMENT, details=""input must be 4-dimensional[1,2,299,299,3]
# #011 [[Node: ResizeBilinear = ResizeBilinear[T=DT_FLOAT, _output_shapes=[[1,299,299,3]], align_corners=false, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](ExpandDims, ResizeBilinear/size)]]"")
# 10.32.0.1 - - [06/Feb/2018:09:29:20 +0000] ""POST /invocations HTTP/1.1"" 500 0 ""-"" ""AHC/2.0""
</code></pre>

<p>BTW, I experience the same problem if I load the images with PIL instead of Keras:</p>

<pre><code>image = Image.open(path)
image_array = np.array(image)
</code></pre>

<p>And here is the code on server side:</p>

<pre><code>def serving_input_fn(params):
"""""" See https://www.tensorflow.org/programmers_guide/
saved_model#using_savedmodel_with_estimators
and
See https://github.com/aws/sagemaker-python-sdk#creating-a-serving_input_fn
and https://docs.aws.amazon.com/sagemaker/latest/dg/
tf-training-inference-code-template.html
""""""
# Download InceptionV3 if need be, in order to 
# compute high level features (called bottleneck here),
# which are then fed into the model
model_dir = './pretrained_model/'
maybe_download_and_extract(params['data_url'],
                           dest_directory=model_dir)
model_path = os.path.join(model_dir, params['model_file_name'])
with tf.gfile.FastGFile(model_path, 'rb') as f:
    graph_def = tf.GraphDef()
    graph_def.ParseFromString(f.read())
    bottleneck_tensor, resized_input_tensor, input_tensor = (
        tf.import_graph_def(
            graph_def,
            name='',
            input_map=None,
            return_elements=[
                params['bottleneck_tensor_name'],
                params['resized_input_tensor_name'],
                'DecodeJpeg:0',
            ]))
return tf.estimator.export.ServingInputReceiver(bottleneck_tensor, {
    INPUT_TENSOR_NAME: input_tensor
})
</code></pre>",1,2,2018-02-06 10:19:42.210000 UTC,,2018-05-06 18:09:49.593000 UTC,2,python|tensorflow-serving|tensorflow-estimator|amazon-sagemaker,2088,2013-05-03 12:46:05.167000 UTC,2022-09-23 16:30:55.857000 UTC,,157,20,0,26,,,,,,['amazon-sagemaker']
Deploy Yolov5 custom object detection model to AWS Sagemaker,<p>I want to deploy trained Yolov5 custom object detection to AWS Sagamaker. I could not find any resources.</p>,0,1,2022-05-05 16:13:49.620000 UTC,2.0,,0,amazon-web-services|amazon-sagemaker|yolo,306,2016-10-28 05:14:28.520000 UTC,2022-09-18 10:24:57.750000 UTC,"Maltepe/İstanbul, Türkiye",21,5,0,5,,,,,,['amazon-sagemaker']
"AWS SageMaker Invoke Endpoint Error: could not convert string ""xxxxx"" to float","<p>I have an endpoint configured in AWS SageMaker. I am attempting to invoke the endpoint and pass a test datapoint to obtain the predictions. However, I am running into some datatype conversion issue. My model is trained on numeric and categorical variables.</p>
<pre><code>import pandas as pd

df = pd.DataFrame({
                       
                       'Year': 2000,
                       'Val': 85,
                       'Score': 60,
                       'Area': 'x1isz2',
                       'Rate': 0.05312,

                }, index=[0])

point_X = np.expand_dims(df.to_numpy(), axis=0)

np.savetxt(&quot;test_point.csv&quot;, point_X[0][0], delimiter=&quot;,&quot;, fmt='%s')

file_name = (&quot;test_point.csv&quot;)  
</code></pre>
<p>Either do this:</p>
<pre><code>with open(file_name, &quot;r&quot;) as f:
    payload = f.read().strip()
    
</code></pre>
<p>or try this:</p>
<pre><code>payload = ','.join([str(item) for item in point_X[0][0]])
print(payload)

response = runtime_client.invoke_endpoint(
    EndpointName=endpoint_name, ContentType=&quot;text/csv&quot;, Body=payload
)


2000, 85, 60, x1isz2, 0.05312


169.254.178.2 - - [23/Nov/2021:00:12:55 +0000] &quot;GET /ping HTTP/1.1&quot; 200 0 &quot;-&quot; &quot;AHC/2.0&quot;
169.254.178.2 - - [23/Nov/2021:00:13:00 +0000] &quot;GET /ping HTTP/1.1&quot; 200 0 &quot;-&quot; &quot;AHC/2.0&quot;
169.254.178.2 - - [23/Nov/2021:00:13:05 +0000] &quot;GET /ping HTTP/1.1&quot; 200 0 &quot;-&quot; &quot;AHC/2.0&quot;
169.254.178.2 - - [23/Nov/2021:00:13:10 +0000] &quot;GET /ping HTTP/1.1&quot; 200 0 &quot;-&quot; &quot;AHC/2.0&quot;
169.254.178.2 - - [23/Nov/2021:00:13:15 +0000] &quot;GET /ping HTTP/1.1&quot; 200 0 &quot;-&quot; &quot;AHC/2.0&quot;
[2021-11-23:00:13:15:INFO] Determined delimiter of CSV input is ','
[2021-11-23:00:13:15:ERROR] Loading csv data failed with Exception, please ensure data is in csv format:
 &lt;class 'ValueError'&gt;
 could not convert string to float: 'Year'
Traceback (most recent call last):
  File &quot;/miniconda3/lib/python3.6/site-packages/sagemaker_xgboost_container/algorithm_mode/serve_utils.py&quot;, line 47, in parse_content_data
    dtest = encoder.csv_to_dmatrix(decoded_payload, dtype=np.float)
  File &quot;/miniconda3/lib/python3.6/site-packages/sagemaker_xgboost_container/encoder.py&quot;, line 50, in csv_to_dmatrix
    np_payload = np.array(list(map(lambda x: _clean_csv_string(x, delimiter), string_like.split('\n')))).astype(dtype)
ValueError: could not convert string to float: 'Year'
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File &quot;/miniconda3/lib/python3.6/site-packages/sagemaker_xgboost_container/algorithm_mode/serve.py&quot;, line 155, in invocations
    dtest, content_type = serve_utils.parse_content_data(payload, flask.request.content_type)
  File &quot;/miniconda3/lib/python3.6/site-packages/sagemaker_xgboost_container/algorithm_mode/serve_utils.py&quot;, line 50, in parse_content_data
    &quot;please ensure data is in csv format:\n 
</code></pre>",1,4,2021-11-23 01:12:28.587000 UTC,,2021-11-23 18:32:22.767000 UTC,0,python|amazon-web-services|machine-learning|amazon-sagemaker,374,2014-03-04 21:04:07.770000 UTC,2022-09-24 00:09:39.853000 UTC,"San Francisco, CA, USA",1503,179,3,429,,,,,,['amazon-sagemaker']
AWS Sagemaker java.lang.IllegalArgumentException,"<p>I have a SageMaker endpoint. The SageMaker endpoint is being invoked via a Lambda function. Part of that Lambda function code that invokes it is:</p>
<pre><code>SM_ENDPOINT_NAME = &quot;pytorch-inference-2021-xx-xx&quot;
sm_runtime= boto3.client('runtime.sagemaker')
txt = &quot;Canon Compact Photo Printer&quot;
response = sm_runtime.invoke_endpoint(EndpointName=SM_ENDPOINT_NAME, ContentType='text/plain', Body=txt)
print(response)
</code></pre>
<p>The <code>response</code> does not get logged. It is supposed to return a <code>vector</code></p>
<p>When the Lambda function calls the SageMaker endpoint it times out. I started seeing this error suddenly today in the SageMaker Endpoint logs:</p>
<pre><code>java.lang.IllegalArgumentException: reasonPhrase contains one of the following prohibited characters: \r\n: tokenizers&gt;=0.10.1,&lt;0.11 is required for a normal functioning of this module, but found tokenizers==0.11.2.

Try: pip install transformers -U or pip install -e '.[dev]' if you're working with git master
</code></pre>
<p>For context, The ML Model is supposed to accept a string input and return a <code>vector</code> (list). All of my code base is in Python3 . The Java exception seems to be from the SageMaker Backend.</p>
<p>I have not updated the SageMaker endpoint or the Lambda function in a few months. This seems like it has to be an issue with SageMaker.</p>
<p>Is this an issue I can fix or do I need to contact AWS? If so how can I do that?</p>",1,0,2022-01-18 21:30:14.520000 UTC,,,0,java|python-3.x|amazon-web-services|machine-learning|amazon-sagemaker,71,2016-09-30 22:34:59.710000 UTC,2022-09-22 13:53:27.543000 UTC,,586,83,21,103,,,,,,['amazon-sagemaker']
Is there any module in Azure ML Studio to identify features correlated with a Target Variable and correlated input features?,"<p>Is there any module which outputs high/low correlated input features with respect to a given target variable, and which can identify highly correlated input features in a data set in Azure Machine Learning Studio?</p>

<p>Such a module would help creating better training data sets and that would allow to create better ML models.</p>",0,2,2019-07-25 19:49:40.007000 UTC,,2019-07-28 17:53:16.770000 UTC,0,machine-learning|feature-selection|azure-machine-learning-studio,57,2014-12-19 06:23:57.687000 UTC,2022-09-21 02:48:41.847000 UTC,"New Jersey, USA",747,48,2,96,,,,,,['azure-machine-learning-studio']
AutoML Vertex AI - How to set continuous value for labels and default values?,"<p>I have an image dataset where it needs multi-labeling support. Additionally, there is a task that needs a count of how many of a specific item there is in each photo. Therefore, I need an input where the user can specify the number (from 0-100 for example).</p>
<p>Is there anyway to do this? Additionally, is there a way to set default values to the labels?</p>",1,0,2021-10-25 16:49:11.527000 UTC,,,0,google-cloud-platform|computer-vision|google-cloud-automl|google-cloud-vertex-ai,134,2014-02-18 22:34:13.817000 UTC,2022-09-22 15:59:48.447000 UTC,,955,20,0,94,,,,,,['google-cloud-vertex-ai']
Timing test on azure ml,"<p>I have created data sets of various sizes say 1GB, 2GB, 3GB, 4GB (&lt; 10 GB) and executing various machine learning models on Azure ML. </p>

<p>1) Can I know what is the server specifications (RAM, CPU) that is provided in the Azure ML service.</p>

<p>2) Also at times the reader says ""Memory exhaust"" for >4GB of data.Though azure ml should be able to handle 10GB of data as per documentation.</p>

<p>3) If I run multiple experiments(in different tabs of browser) in parallel, its taking more time.</p>

<p>4) Is there any way to set the RAM, CPU cores in Azure ML</p>",2,4,2016-02-25 07:56:55.953000 UTC,1.0,2016-03-08 08:23:15.533000 UTC,5,performance|azure|cortana-intelligence|azure-machine-learning-studio,155,2016-02-25 07:48:14.000000 UTC,2019-04-08 11:19:06.970000 UTC,Dubai - United Arab Emirates,51,1,0,2,,,,,,['azure-machine-learning-studio']
"How to use SageMaker, BlazingText, and Labeling Jobs","<p>I have done some searching on how to use the UI console for creating a ground truth labeling job to creating a model from this labeling job and not getting the outputs I expect.</p>

<p>The first question is, I have labeled probably a hundred parts of a text in the ground truth job and submitted the labels. But when I look at the labeled objects/total, it shows 0.</p>

<p>Then, after creating a training job from the ground truth labeling job's output, creating a model package from the training job, and finally creating a batch transform job from that model package, the output from the batch transform job is not what is expected.</p>

<p>The expected output should be a json object containing the the text with the label, but the output I'm getting is below.</p>

<p>What do I need to do to get the expected results?</p>

<p>[{""vector"": <a href=""http://0.0"" rel=""nofollow noreferrer"">http://0.0</a>, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ""word"": ""t-6 - hard leather tobacco r11 24\"" x 48\"" = 744 sf""}, {""vector"": <a href=""http://0.0"" rel=""nofollow noreferrer"">http://0.0</a>, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ""word"": ""t-6 - hard leather tobacco r11 24\"" x 48\"" = 264 sf""}, {""vector"":...</p>",1,0,2020-06-02 19:08:53.233000 UTC,,,0,amazon-sagemaker,106,2011-04-18 21:03:14.513000 UTC,2021-08-27 15:35:14.480000 UTC,,369,9,0,34,,,,,,['amazon-sagemaker']
How to build custom pipeline in GCP using Vertex AI,"<p>I was exploring the vertex AI AutoML feature in GCP, which lets users import datasets, train, deploy and predict ML models. My use case is to do the data pre-processing on my own (I didn't get satisfied with AutoML data preprocessing) and want to feed that data directly to a pipeline where it trains and deploys the model.
Also, I want to feed the new data to the dataset. It should take care of the entire pipeline (from data preprocessing to deploying the latest model).
I want insight as to how to approach this problem?</p>",1,1,2022-04-20 14:43:05.530000 UTC,,2022-04-21 14:35:47.143000 UTC,2,machine-learning|google-cloud-platform|google-cloud-automl|google-cloud-vertex-ai|gcp-ai-platform-training,264,2019-02-04 18:35:39.163000 UTC,2022-09-24 12:01:19.733000 UTC,"Bangalore, Karnataka, India",454,43,1,79,,,,,,['google-cloud-vertex-ai']
Compressing image before sending to SageMaker TF Serving,"<p>I'm able to successfully send batches of images as a numpy array to my SageMaker endpoint set up using TF serving and get a response, like this:. </p>

<pre><code>def predict_random_batch(self, batch_size, verbose=0, initial_args=None): 
    batch = np.random.uniform(low=-1.0, high=1.0, size=(batch_size,self.size,self.size,3))
    data = {'instances': np.asarray(batch).astype(self.dtype)}
    if verbose: self.total_size(data)
    request_args = self._create_request_args(data, initial_args)
    response = self.sagemaker_session.sagemaker_runtime_client.invoke_endpoint(**request_args)
    probs = self._handle_response(response)['predictions']
    return probs

predictor.predict_random_batch(3)
</code></pre>

<p>However, the numpy array is very big. I'm trying to compress the image batches before sending. This is what I'm attempting:</p>

<pre><code>def predict_random_batch_TEST(self, batch_size, verbose=0, initial_args=None): 
    import base64
    batch = np.random.uniform(low=-1.0, high=1.0, size=(batch_size,self.size,self.size,3))
    batch = batch.astype(self.dtype)
    encoded_input_string = base64.b64encode(batch)
    input_string = encoded_input_string.decode(""utf-8"")
    instance = [{""b64"": input_string}]
    data = json.dumps({""instances"": instance})        
    request_args = self._create_request_args(data, initial_args)
    if verbose: self.total_size(data)
    response = self.sagemaker_session.sagemaker_runtime_client.invoke_endpoint(**request_args)
    probs = self._handle_response(response)['predictions']
    return probs
</code></pre>

<p>However this returns the error:</p>

<blockquote>
  <p>ModelError: An error occurred (ModelError) when calling the
  InvokeEndpoint operation: Received client error (400) from model with
  message ""{ ""error"": ""JSON Value: . . .  Is not object"" }""</p>
</blockquote>

<p>Does anybody know how I can compress a batch of images to be able to send a larger batch size? Apparently SM imposes a 5MB payload limit which is not very big when sending as a numpy array.</p>",1,0,2019-01-19 17:31:11.173000 UTC,1.0,,2,python-3.x|rest|tensorflow-serving|image-compression|amazon-sagemaker,633,2013-02-20 05:47:52.693000 UTC,2022-09-23 20:45:28.400000 UTC,NYC,6281,430,17,958,,,,,,['amazon-sagemaker']
How to retrieve auth keys for an ACI deployment in Azure Portal (or Cloud Shell)?,"<p>I have created a <a href=""https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-deploy-and-where#aci"" rel=""nofollow noreferrer"">deployment on ACI with Azure ML service</a>, and its status is healthy.<br>
When deploying, I set <code>auth_enabled=True</code>, so that the service requires authorization keys to respond.</p>

<p>I can get the service auth keys for that deployment in my Azure ML service workspace <code>ws</code> in a Python console via</p>

<pre><code>from azureml.core.webservice import Webservice
services = Webservice.list(ws)
services[0].get_keys()
</code></pre>

<p>However, it would be convenient to access to this information through Azure Portal or the Cloud Shell. </p>

<p>In Azure Portal (differently to what happens for AKS) there's no auth fields shown, also when accessing Advanced Settings by trying to edit the deployment:</p>

<p><a href=""https://i.stack.imgur.com/3Qudh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3Qudh.png"" alt=""enter image description here""></a></p>

<p>Can you suggest ways to access those credentials?</p>",1,0,2019-05-31 10:58:22.887000 UTC,,2019-05-31 17:32:04.667000 UTC,0,azureportal|azure-aks|azure-container-instances|azure-cloud-shell|azure-machine-learning-service,169,2014-11-11 16:17:30.717000 UTC,2022-09-24 20:31:18.173000 UTC,"Verona, VR, Italy",4811,376,73,713,,,,,,['azure-machine-learning-service']
Azure Machine Learning (AML) Webservice REST API with Multiple endpoints,"<p>I've been working on developing an API to serve a machine learning model using Azure Machine Learning (AML) webservice deployments on a Kubernetes target as outlined here: <a href=""https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-deploy-and-where#prepare-to-deploy"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-deploy-and-where#prepare-to-deploy</a></p>

<p>My particular use case requires more than simple scoring of data through the model. The API needs to have multiple endpoints to perform different actions related to the model. For example, an endpoint to upload data, an endpoint to delete data, an endpoint to list existing data, an endpoint to score previously uploaded data, an endpoint to change preprocessing parameters, etc... </p>

<p>I can build all of this logic, but I am struggling with the fact that AML web services only provides one endpoint (The service URI ending in ""/score""). Is there a way to add more endpoints to an AML service? For example, I would like to have a way for users to be able to POST, GET, DELETE, PUT ""/data"", GET ""/predictions"", and POST, GET, DELETE, PUT ""/parameters"", etc..</p>

<p>Is there a way to do this in AML or is this not the right tool for what I am trying to accomplish? Is there a better solution within Azure that is more suited for my needs? </p>

<p>Thank you!</p>",2,0,2019-12-15 21:04:19.623000 UTC,1.0,,2,azure|azure-machine-learning-service,776,2019-12-15 20:56:27.797000 UTC,2022-07-30 20:25:43.823000 UTC,,21,4,0,2,,,,,,['azure-machine-learning-service']
Correct parameters for training AWS Sagemaker with multiple classes per image,"<p>I've found consistently that ""multi_label"" to ""1"" for image classification jobs, they crash with the following error:</p>

<pre><code>Algorithm Error: Internal Server Error
[15:56:08] /opt/brazil-pkg-cache/packages/MXNetECL/MXNetECL-master.657.0/AL2012/generic-flavor/src/src/operator/custom/custom.cc:418: Check failed: reinterpret_cast&lt;CustomOpFBFunc&gt;(params.info-&gt;callbacks[kCustomOpBackward])( ptrs.size(), const_cast&lt;void**&gt;(ptrs.data()), const_cast&lt;int*&gt;(tags.data()), reinterpret_cast&lt;const int*&gt;(req.data()), static_cast&lt;int&gt;(ctx.is_train), params.info-&gt;contexts[kC
15:56:08 Stack trace returned 7 entries:
15:56:08 [bt] (0) /opt/amazon/lib/libaialgsdataiter.so(dmlc::StackTrace()+0x3d) [0x7f85e19f179d]
15:56:08 [bt] (1) /opt/amazon/lib/libaialgsdataiter.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x1a) [0x7f85e19f1a3a] 
15:56:08 [bt] (2) /opt/amazon/lib/libmxnet.so(+0x26da8fd) [0x7f85d0edb8fd]
15:56:08 [bt] (3) /opt/amazon/lib/libmxnet.so(std::thread::_Impl&lt;std::_Bind_simple&lt;mxnet::op::custom::CustomOperator::CustomOperator()::{lambda()#1} ()&gt; &gt;::_M_run()+0x12f) [0x7f85d0ede0ef]
15:56:08 [bt] (4) /opt/amazon/lib/libstdc++.so.6(+0xce440) [0x7f85cc9ea440]
15:56:08 [bt] (5) /lib64/libpthread.so.0(+0x7dc5) [0x7f85e31e1dc5]
15:56:08 [bt] (6) /lib64/libc.so.6(clone+0x6d) [0x7f85e25de6ed]
15:56:08 Algorithm Error: Internal Server Error
</code></pre>

<p>Based my understanding of the documentation, this parameter should let you assign multiple tags to each image - is there a trick to get it to work, or to debugging these stack traces? (<a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/IC-Hyperparameter.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/IC-Hyperparameter.html</a>)</p>",2,0,2019-02-10 21:43:13.730000 UTC,,,0,amazon-web-services|amazon-sagemaker,142,2012-09-23 23:56:52.630000 UTC,2022-09-22 19:11:36.750000 UTC,,346,23,0,22,,,,,,['amazon-sagemaker']
Saving model artifacts with Kubeflow without Pipeline,"<p>I am using mlflow as of now in my jupyterhub environment for model tracking and I feel its easy to keep track of artifacts in mlflow simply by calling the run like:</p>

<pre><code>with mlflow.start_run():
    lr = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=42)
    lr.fit(train_x, train_y)

    predicted_qualities = lr.predict(test_x)

    (rmse, mae, r2) = eval_metrics(test_y, predicted_qualities)

    mlflow.log_param(""alpha"", alpha)
    mlflow.log_param(""l1_ratio"", l1_ratio)
    mlflow.log_metric(""rmse"", rmse)
    mlflow.log_metric(""r2"", r2)
    mlflow.log_metric(""mae"", mae)

    mlflow.sklearn.log_model(lr, ""model"")
</code></pre>

<p>I am moving to Kubeflow now and not sure if I can do the same thing here without creating a pipleline. What I could find is:</p>

<pre><code>client.run_pipleline(exp.id, ....)
</code></pre>

<p>Is there any way I can keep track of experiments like mlflow in Kubeflow?</p>",1,0,2019-08-26 07:28:20.460000 UTC,,2019-08-26 11:18:36.080000 UTC,1,python|kubeflow|mlflow,472,2014-05-29 12:37:30.427000 UTC,2020-10-15 08:35:10.407000 UTC,Kuala Lumpur Federal Territory of Kuala Lumpur Malaysia,1990,49,1,268,,,,,,['mlflow']
Issues in opening the mlflow ui with pygrok,"<p>I am very new to MLops and Ml flow. I am trying to use MLflow on google Colab to track models. however, i am not able to open the ui on the local server.
I got a couple of errors:</p>
<pre><code>The connection to http:xxxx was successfully tunneled to your ngrok client, but the client failed to establish a connection to the local address localhost:80.

Make sure that a web service is running on localhost:80 and that it is a valid address.

The error encountered was: dial tcp 127.0.0.1:80: connect: connection refused
</code></pre>
<p>Post this error, i did certain changes to the environment and downloaded ngrok.
and provided the auth token to <code>NGROK_AUTH_TOKEN = &quot;xxxx&quot;</code>
Now i am getting the below message:</p>
<p><a href=""https://i.stack.imgur.com/1o5pe.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1o5pe.png"" alt=""enter image description here"" /></a></p>
<p>The code that i am using is:</p>
<pre><code>!pip install pyngrok --quiet
from pyngrok import ngrok

ngrok.kill()

NGROK_AUTH_TOKEN = &quot;&quot;
ngrok.set_auth_token(NGROK_AUTH_TOKEN)

public_url = ngrok.connect(port=&quot;127.0.0.1:5000 &quot;, proto=&quot;http&quot;, options={&quot;bind_tls&quot;: True})
print(&quot;MLflow Tracking UI:&quot;, public_url)
</code></pre>
<p>Any help is highly appreciated.
TIA...</p>",0,0,2022-07-15 21:42:58.923000 UTC,,,0,google-colaboratory|ngrok|mlflow|pyngrok,51,2015-09-11 13:52:59.697000 UTC,2022-09-22 11:13:17.457000 UTC,,1405,16,0,123,,,,,,['mlflow']
Azure ml: how to cache data prepared in data preparation step,"<p>Let's say I have raw data in azure storage that I need to read and refine it for the azure ml pipeline training step. Running pipeline for different models/params and experimenting with code, prepared data won't change for a while. I think about caching it, since preparation steps takes good amount of resources (Regexes involved). I'll still need to re-run the preparation step when raw data has changed, but only once in a while. I wonder what are good practices for doing it right from Python code and with help of Azure SDK</p>",0,0,2022-05-31 18:42:58.463000 UTC,,,0,azure-machine-learning-service,50,2011-05-05 00:52:42.350000 UTC,2022-09-25 04:46:30.543000 UTC,,4550,321,19,287,,,,,,['azure-machine-learning-service']
AWS sagemaker training job (Tensorflow) halts at Epoch 1,"<p>I am trying to train Maskrcnn with custom dataset. The code is running fine on my local machine in the same docker container, however, it gets stuck at the first epoch when I use aws sagemaker.</p>
<p><a href=""https://i.stack.imgur.com/C3Z1T.png"" rel=""nofollow noreferrer"">The log my error seen on sagemaker notebook for training job</a></p>
<p>I am using Tensorflow 2 implementing the github code provided by <a href=""https://github.com/simone-viozzi/Mask-RCNN-training-with-docker-containers-on-Sagemaker"" rel=""nofollow noreferrer"">https://github.com/simone-viozzi/Mask-RCNN-training-with-docker-containers-on-Sagemaker</a></p>",1,3,2022-02-19 18:08:30.193000 UTC,,,0,amazon-sagemaker,117,2022-02-19 17:57:16.933000 UTC,2022-07-29 06:49:35.647000 UTC,,1,0,0,0,,,,,,['amazon-sagemaker']
AWS Sagemaker - using cross validation instead of dedicated validation set?,"<p>When I train my model locally I use a 20% test set and then cross validation. Sagameker seems like it needs a dedicated valdiation set (at least in the tutorials I've followed). Currently I have 20% test, 10% validation leaving 70% to train - so I lose 10% of my training data compared to when I train locally, and there is some performance loss as a results of this. </p>

<p>I could just take my locally trained models and overwrite the sagemaker models stored in s3, but that seems like a bit of a work around. Is there a way to use Sagemaker without having to have a dedicated validation set? </p>

<p>Thanks</p>",2,0,2019-08-23 16:47:42.943000 UTC,,,3,amazon-web-services|cross-validation|amazon-sagemaker,1401,2015-01-15 17:43:03.700000 UTC,2022-08-23 22:54:25.603000 UTC,,1387,51,1,153,,,,,,['amazon-sagemaker']
Reuse existing conda env for mlflow project?,"<p>I have configured a mlflow project file. First hard knock was that the extension is not required. The current problem is that I have exported an existing conda environment using:</p>
<pre class=""lang-bash prettyprint-override""><code>conda env export --name ENVNAME &gt; envname.yml
</code></pre>
<p>substituting the <code>ENVNAME</code>. This <strong>envname.yml</strong> file has the actual path where the env is located. Next, I have placed the <strong>envname.yml</strong> and defined entry points correctly.</p>
<pre class=""lang-yaml prettyprint-override""><code>name: pytorch
channels:
  - defaults
prefix: /data/krishnan/software/anaconda3/envs/pytorch
</code></pre>
<p>When I run the project using <code>mlflow run .</code>, I find that mlflow tries to create yet one more temporary environment based on this Conda file which is Python 2. It ignores that the specified env exists and all packages are correct.</p>
<p>Is there anything incorrect in what I am doing?</p>",0,6,2021-12-03 15:45:22.610000 UTC,,2021-12-03 19:59:13.400000 UTC,1,python|conda|mlflow,379,2016-05-05 05:51:57.847000 UTC,2022-03-04 13:01:23.960000 UTC,,21,0,0,3,,,,,,['mlflow']
"Azure-machine-learning: Compiling R-script, but different results","<p>I've built a ML model in Azure and most of my data transformation / feature generation steps have been carried out in R. 
At first, I transformed the data in R itself (works faster) and uploaded the created csv to ML studio.</p>

<p>Now that my model is finished I tried to run the R-code in ML studio itself, instead of manually uploading a transformed dataset. It runs flawlessly. However, when I compare the resulting datasets (the one uploaded and the one created) they differ. The columns have different means, formats and the model performs less. The actual data entries/cells look fine. </p>

<p>I feel it has to do with the format of the columns so I tried stuff like converted the columns to character type, or converting the resulting dataset to csv (in ML studio) and letting ML studio figure out how to format them.</p>

<p>So far, no results.</p>

<p>Has anyone already faced this problem? What is the solution?</p>",1,0,2015-07-28 09:40:33.613000 UTC,,,1,r|machine-learning|azure-machine-learning-studio,241,2014-06-21 10:23:16.767000 UTC,2019-01-08 09:59:19.827000 UTC,,31,0,0,5,,,,,,['azure-machine-learning-studio']
MLFLOW: Registering a model remotely doesn't work while running locally inside azure VM does,"<p>I have been having issues trying to connect to an MLFLOW I created on an azure VM using the following tutorial:
<a href=""https://medium.com/swlh/how-to-setup-mlflow-on-azure-5ba67c178e7d"" rel=""nofollow noreferrer"">https://medium.com/swlh/how-to-setup-mlflow-on-azure-5ba67c178e7d</a>
Whenever running the following script on the server it works fine, but when running the same script remotely I get an error.
Is there anyone around here that has experience in deploying mlflow to Azure?</p>
<p>the script (censored IP address intentionally):</p>
<pre><code>from sklearn.ensemble import RandomForestRegressor
import mlflow
import mlflow.sklearn
mlflow.set_tracking_uri(&quot;http://xx.xxx.xx.xxx:5000/&quot;)
mlflow.set_registry_uri(&quot;http://xx.xxx.xx.xxx:5000/&quot;)
mlflow.set_experiment(&quot;test experiment4&quot;)
with mlflow.start_run(run_name=&quot;YOUR_RUN_NAME&quot;) as run:
    sk_learn_rfr = RandomForestRegressor()
    mlflow.sklearn.log_model(sk_model=sk_learn_rfr,artifact_path=&quot;sklearn-model_local&quot;,registered_model_name=&quot;sk-learn-random-forest-reg-model&quot;)
</code></pre>
<p>error :</p>
<pre><code> File &quot;C:\Users\JasperBusschers\PycharmProjects\mlflow\venv\lib\site-packages\azure\core\pipeline\_base.py&quot;, line 103, in send
    self._sender.send(request.http_request, **request.context.options),
  File &quot;C:\Users\JasperBusschers\PycharmProjects\mlflow\venv\lib\site-packages\azure\storage\blob\_shared\base_client.py&quot;, line 333, in send
    return self._transport.send(request, **kwargs)
  File &quot;C:\Users\JasperBusschers\PycharmProjects\mlflow\venv\lib\site-packages\azure\storage\blob\_shared\base_client.py&quot;, line 333, in send
    return self._transport.send(request, **kwargs)
  File &quot;C:\Users\JasperBusschers\PycharmProjects\mlflow\venv\lib\site-packages\azure\core\pipeline\transport\_requests_basic.py&quot;, line 361, in send
    raise error
azure.core.exceptions.ServiceRequestError: &lt;urllib3.connection.HTTPSConnection object at 0x000002B063025AC0&gt;: Failed to establish a new connection: [Errno 11001] getaddrinfo failed
</code></pre>
<p>I also tried using socket to try to connect and experienced the same error:</p>
<pre><code>import socket
s = socket.socket()
s.connect(('http://20.XXX.XX.XXX', 5000))
Traceback (most recent call last):
  File &quot;&lt;input&gt;&quot;, line 4, in &lt;module&gt;
socket.gaierror: [Errno 11001] getaddrinfo failed
</code></pre>",0,0,2022-04-12 10:34:21.837000 UTC,,2022-04-13 10:46:52.673000 UTC,0,azure|virtual-machine|mlflow,163,2014-01-05 17:37:36.473000 UTC,2022-09-13 14:14:14.983000 UTC,,9,0,0,15,,,,,,['mlflow']
Tensorflow embedded column throwing exception based on the size of vocabulary,"<p>I'm stuck with a very weird behavior in tensorflow while trying to use <code>tf.feature_column.embedding_column</code> API. While I don't think is relevant I'm generating the input data via <code>tf.data.Dataset.from_generator</code>.</p>
<p>Now quickly jumping to the issue. When I'm trying to use the feature_column it works or fails based on size of vocabulary size. Code snippet for creating feature column</p>
<pre class=""lang-py prettyprint-override""><code>    for colmn_name in indicator_colms:
        feature_col = feature_column.categorical_column_with_vocabulary_list(
            colmn_name, unique_values(colmn_name))
        indicator_column = feature_column.indicator_column(feature_col)
        feature_columns.append(indicator_column)
</code></pre>
<p>For small vocabulary sizes everything works fine but the moment I try to add more elements to my vocabulary I weirdly get the following exception:</p>
<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/keras/feature_column/dense_features.py in call(self, features, cols_to_output_tensors, training)
    165           tensor = column.get_dense_tensor(
--&gt; 166               transformation_cache, self._state_manager, training=training)
    167         except TypeError:

TypeError: get_dense_tensor() got an unexpected keyword argument 'training'

During handling of the above exception, another exception occurred:

TypeError                                 Traceback (most recent call last)
~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column_v2.py in get(self, key, state_manager, training)
   2457       transformed = column.transform_feature(
-&gt; 2458           self, state_manager, training=training)
   2459     except TypeError:

TypeError: transform_feature() got an unexpected keyword argument 'training'

During handling of the above exception, another exception occurred:

TypeError                                 Traceback (most recent call last)
~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column_v2.py in get(self, key, state_manager, training)
   2457       transformed = column.transform_feature(
-&gt; 2458           self, state_manager, training=training)
   2459     except TypeError:

TypeError: transform_feature() got an unexpected keyword argument 'training'

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
&lt;ipython-input-69-8136fbe34af6&gt; in &lt;module&gt;
      2     feature_columns = _create_feature_columns()
      3     feature_layer = tf.keras.layers.DenseFeatures(feature_columns)
----&gt; 4     demo(feature_columns)

&lt;ipython-input-10-65d9d3c8db7a&gt; in demo(feature_column)
      1 def demo(feature_column):
      2     demo_feature_layer = layers.DenseFeatures(feature_column)
----&gt; 3     print(demo_feature_layer(example_batch).numpy())

~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/keras/engine/base_layer.py in __call__(self, *args, **kwargs)
   1035         with autocast_variable.enable_auto_cast_variables(
   1036             self._compute_dtype_object):
-&gt; 1037           outputs = call_fn(inputs, *args, **kwargs)
   1038 
   1039         if self._activity_regularizer:

~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/keras/feature_column/dense_features.py in call(self, features, cols_to_output_tensors, training)
    167         except TypeError:
    168           tensor = column.get_dense_tensor(transformation_cache,
--&gt; 169                                            self._state_manager)
    170         processed_tensors = self._process_dense_tensor(column, tensor)
    171         if cols_to_output_tensors is not None:

~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column_v2.py in get_dense_tensor(self, transformation_cache, state_manager)
   4301     # Feature has been already transformed. Return the intermediate
   4302     # representation created by transform_feature.
-&gt; 4303     return transformation_cache.get(self, state_manager)
   4304 
   4305   @deprecation.deprecated(_FEATURE_COLUMN_DEPRECATION_DATE,

~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column_v2.py in get(self, key, state_manager, training)
   2458           self, state_manager, training=training)
   2459     except TypeError:
-&gt; 2460       transformed = column.transform_feature(self, state_manager)
   2461     if transformed is None:
   2462       raise ValueError('Column {} is not supported.'.format(column.name))

~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column_v2.py in transform_feature(self, transformation_cache, state_manager)
   4238     &quot;&quot;&quot;
   4239     id_weight_pair = self.categorical_column.get_sparse_tensors(
-&gt; 4240         transformation_cache, state_manager)
   4241     return self._transform_id_weight_pair(id_weight_pair,
   4242                                           self.variable_shape[-1])

~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column_v2.py in get_sparse_tensors(self, transformation_cache, state_manager)
   3725     &quot;&quot;&quot;See `CategoricalColumn` base class.&quot;&quot;&quot;
   3726     return CategoricalColumn.IdWeightPair(
-&gt; 3727         transformation_cache.get(self, state_manager), None)
   3728 
   3729   @deprecation.deprecated(_FEATURE_COLUMN_DEPRECATION_DATE,

~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column_v2.py in get(self, key, state_manager, training)
   2458           self, state_manager, training=training)
   2459     except TypeError:
-&gt; 2460       transformed = column.transform_feature(self, state_manager)
   2461     if transformed is None:
   2462       raise ValueError('Column {} is not supported.'.format(column.name))

~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column_v2.py in transform_feature(self, transformation_cache, state_manager)
   3703     input_tensor = _to_sparse_input_and_drop_ignore_values(
   3704         transformation_cache.get(self.key, state_manager))
-&gt; 3705     return self._transform_input_tensor(input_tensor, state_manager)
   3706 
   3707   @deprecation.deprecated(_FEATURE_COLUMN_DEPRECATION_DATE,

~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column_v2.py in _transform_input_tensor(self, input_tensor, state_manager)
   3691             num_oov_buckets=self.num_oov_buckets,
   3692             dtype=key_dtype,
-&gt; 3693             name=name)
   3694       if state_manager is not None:
   3695         state_manager.add_resource(self, name, table)

~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/ops/lookup_ops.py in index_table_from_tensor(vocabulary_list, num_oov_buckets, default_value, hasher_spec, dtype, name)
   1507 
   1508   with ops.name_scope(name, &quot;string_to_index&quot;):
-&gt; 1509     keys = ops.convert_to_tensor(vocabulary_list)
   1510     if keys.dtype.is_integer != dtype.is_integer:
   1511       raise ValueError(

~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/profiler/trace.py in wrapped(*args, **kwargs)
    161         with Trace(trace_name, **trace_kwargs):
    162           return func(*args, **kwargs)
--&gt; 163       return func(*args, **kwargs)
    164 
    165     return wrapped

~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)
   1564 
   1565     if ret is None:
-&gt; 1566       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
   1567 
   1568     if ret is NotImplemented:

~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py in _constant_tensor_conversion_function(v, dtype, name, as_ref)
    344                                          as_ref=False):
    345   _ = as_ref
--&gt; 346   return constant(v, dtype=dtype, name=name)
    347 
    348 

~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py in constant(value, dtype, shape, name)
    270   &quot;&quot;&quot;
    271   return _constant_impl(value, dtype, shape, name, verify_shape=False,
--&gt; 272                         allow_broadcast=True)
    273 
    274 

~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast)
    281       with trace.Trace(&quot;tf.constant&quot;):
    282         return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)
--&gt; 283     return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)
    284 
    285   g = ops.get_default_graph()

~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py in _constant_eager_impl(ctx, value, dtype, shape, verify_shape)
    306 def _constant_eager_impl(ctx, value, dtype, shape, verify_shape):
    307   &quot;&quot;&quot;Creates a constant on the current device.&quot;&quot;&quot;
--&gt; 308   t = convert_to_eager_tensor(value, ctx, dtype)
    309   if shape is None:
    310     return t

~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py in convert_to_eager_tensor(value, ctx, dtype)
    104       dtype = dtypes.as_dtype(dtype).as_datatype_enum
    105   ctx.ensure_initialized()
--&gt; 106   return ops.EagerTensor(value, ctx.device_name, dtype)
    107 
    108 

ValueError: Can't convert Python sequence with mixed types to Tensor.
</code></pre>
<p>I'd absolutely appreciate any help on this.
P.S. I'm running this on sagemaker I've tried training it as a training job as well as on the notebook. The behavior is consistent.</p>",0,4,2022-03-08 12:41:24.543000 UTC,,,2,tensorflow|keras|deep-learning|amazon-sagemaker,61,2015-01-26 17:42:03.203000 UTC,2022-09-14 20:34:22.083000 UTC,,21,0,0,3,,,,,,['amazon-sagemaker']
"VERTEX AI, BATCH PREDICTION","<p>Does anyone know how I can choose the option &quot;Files on Cloud Storage (file list) in the program? How can I [enter image description here choose the list format?</p>
<p><img src=""https://i.stack.imgur.com/aIBA9.png"" alt=""1"" /></p>",0,0,2022-07-22 10:27:43.377000 UTC,,2022-07-23 19:08:04.180000 UTC,0,google-cloud-platform|google-cloud-vertex-ai,57,2022-07-22 10:09:30.563000 UTC,2022-07-26 06:03:32.120000 UTC,,1,0,0,0,,,,,,['google-cloud-vertex-ai']
"RandomizedSearchCV - IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed","<p>I'm using HDBSCAN clustering algorithm and using RandomizedSearchCV. When I fit the features with labels, I get error &quot;IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed&quot;. Shape of embedding is (5000,4) and of hdb_labels is (5000,). Below is my code</p>
<pre><code># UMAP
umap_hdb = umap.UMAP(n_components=4, random_state = 42)
embedding = umap_hdb.fit_transform(customer_data_hdb)
    
# creating HDBSCAN wrapper
class HDBSCANWrapper(hdbscan.HDBSCAN):
    def predict(self,X):
      return self.labels_.astype(int)

# HBDSCAN
clusterer_hdb = HDBSCANWrapper(min_samples=40, min_cluster_size=1000, metric='manhattan', gen_min_span_tree=True).fit(embedding)

hdb_labels = clusterer_hdb.labels_

# specify parameters and distributions to sample from
param_dist = {'min_samples': [10,30,50,60,100,150],
              'min_cluster_size':[100,200,300,400,500],  
              'cluster_selection_method' : ['eom','leaf'],
              'metric' : ['euclidean','manhattan'] 
             }

# validity_scroer
validity_scorer = make_scorer(hdbscan.validity.validity_index,greater_is_better=True)


n_iter_search = 20
random_search = RandomizedSearchCV(clusterer_hdb
                                   ,param_distributions=param_dist
                                   ,n_iter=n_iter_search
                                   ,scoring=validity_scorer 
                                   ,random_state=42)

random_search.fit(embedding, hdb_labels)
</code></pre>
<p>I'm getting an error in the <strong>random_search.fit</strong> and could not get rid of it. Any suggestions/help would be appreciated.</p>",0,0,2022-07-10 01:27:14.880000 UTC,,,0,arrays|amazon-sagemaker|hdbscan,62,2020-07-24 10:56:54.697000 UTC,2022-07-13 23:03:11.747000 UTC,Las Vegas,1,0,0,2,,,,,,['amazon-sagemaker']
How to connect labeled data in Azure Machine Learning Studio/Data Labeling to Power BI?,"<p>I am trying to connect already labeled dataset in <strong>Azure ML Studio</strong> in <strong>Data Labeling</strong> to <strong>PowerBI</strong>. I would like to see the progress of labeling or the result of labeled data without exporting it manually and connecting the exported files one by one.
Thank you!</p>",1,0,2022-03-15 19:41:42.570000 UTC,,,0,azure|powerbi|azure-machine-learning-studio,224,2021-03-08 10:44:49.193000 UTC,2022-09-22 08:56:37.667000 UTC,"Miláno, Milán, Itálie",1,0,0,1,,,,,,['azure-machine-learning-studio']
Update a Sagemaker Endpoint when changing the docker image,"<p>I am looking for the simplest solution for updating a Sagemaker Endpoint. The only thing I want to change is the docker image (to update the code).</p>
<p>I am building the new Docker image on my computer, and then I upload it on ECR (I plan to do this in a CI/CD in the near future).</p>
<p>From my understanding, it seems that the simplest way is to create a new <code>EndpointConfig</code>, then call the <code>UpdateEndpoint</code> API to switch the endpoint to the new config, then delete the old <code>EndpointConfig</code>. Does anyone know a simpler way? Or can anyone confirm that this is the simplest approach to doing this?</p>",0,3,2020-09-30 15:35:21.327000 UTC,1.0,2020-10-01 15:30:27.710000 UTC,6,amazon-web-services|amazon-sagemaker,742,2015-04-24 21:10:35.550000 UTC,2022-09-23 10:44:33.863000 UTC,,675,89,2,36,,,,,,['amazon-sagemaker']
How do I generate inferences locally from a model fit in Sagemaker?,"<p>I built a custom container on Sagemaker to allow me to tune a Catboost model.</p>
<p>I then fit the model with the best hyperparameters (as if I were to deploy on Sagemaker).</p>
<p>I downloaded the tar.gz file onto my machine.</p>
<p>I can read the file:</p>
<pre><code>import tarfile

file = tarfile.open('model.tar.gz')
</code></pre>
<p>I can extract from the file:</p>
<pre><code>file.extractall('./output')
</code></pre>
<p>I now have a file named <code>catboost_model.dump</code>, but I am unsure where to go from here.</p>
<p>Is it possible load in this <code>.dump</code> file to generate inferences?</p>",1,0,2022-02-14 16:55:48.883000 UTC,,,0,python|amazon-sagemaker,98,2016-09-24 19:32:34.527000 UTC,2022-09-19 18:01:28.293000 UTC,"Scottsdale, AZ, USA",1153,189,0,188,,,,,,['amazon-sagemaker']
Azure ML sales prediction,"<p>I have a simple SQL table, with historical sales data for products. The fields are Product Identifier, Sale Date and Quantity.  Each product has multiple entries/sales.</p>

<p>I want to be able to generate sales forecasts for all products using Azure ML, for certain time periods.</p>

<p>I've found examples/tutorials(e.g. <a href=""https://gallery.azure.ai/Collection/Retail-Forecasting-Template-1"" rel=""nofollow noreferrer"">https://gallery.azure.ai/Collection/Retail-Forecasting-Template-1</a> ) but they are too complicated for my use case. </p>

<p>What would be a flow to achieve what I need? Thank you.</p>",1,0,2019-11-12 08:27:51.803000 UTC,1.0,2019-11-13 14:17:56.660000 UTC,0,azure|machine-learning|azure-machine-learning-studio,108,2014-07-22 07:14:59.957000 UTC,2022-05-11 11:27:47.533000 UTC,,450,67,4,105,,,,,,['azure-machine-learning-studio']
Is there a better way to handle warnings for built-in algorithms when using TrainingJobAnalytics function in sagemaker sdk?,"<p><a href=""https://i.stack.imgur.com/7BWoE.png"" rel=""nofollow noreferrer""></a>
When using <a href=""https://github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/analytics.py"" rel=""nofollow noreferrer"">TrainingJobAnalytics</a> function in sagemaker sdk to view training job results, I get a bunch of scary looking warnings which I understand are harmless.</p>
<p>The function checks for metric statistics from cloud watch for each metric definition defined in training job configuration. From what I understand the implementation allows to keep the logic generic enough to handle custom(byoc) algorithms as well as inbuilt and avoid hard-coding.</p>
<p>I want to know if there is a better way to do this and avoid the warnings instead of suppressing them.</p>
<p><a href=""https://i.stack.imgur.com/7BWoE.png"" rel=""nofollow noreferrer"">warnings image</a></p>",1,0,2020-09-10 10:26:42.723000 UTC,,2020-09-10 10:32:20.490000 UTC,0,amazon-sagemaker,33,2020-09-10 08:10:06.837000 UTC,2021-02-26 11:25:47.890000 UTC,,1,0,0,1,,,,,,['amazon-sagemaker']
Which Azure storage technology for weather forecast data,"<p>I would like some advice/tips about the right technology to select in order to store some forecast data on Azure technologies.
My team and I are scraping some weather forecast data everyday from various sources and store it as is on a Azure File Storage. The files format is ""grib2"" which is a standard format of weather forecast data.
We are able to extract the data from those ""grib2"" files using python script running on a Azure VM.</p>

<p>We now have several files that represent hundreds gigabytes of data to store and I'm struggling to find which data store from the Azure technologies suits the best our needs in term of praticity and cost.<br/>
We started using ""Azure Table Storage"" first because it's cheap solution,
 but I've read on many posts that it is a bit old and not  very adapted to our solution as it for example does not allow more than 1,000 entites per query and no aggregation on data.<br><br>
I considered using Azure SQL db but it seems that it can become very expensive very fast.
<br>
I also considered the Azure Data Lake Storage Gen2 (and HDinsight) technologies but am not very at ease with those blob storages and am not really able to say if it can suit my needs in terms of praticity and if it is ""easy to query"".
<br><br>
<strong>By now we just plan to achieve that :</strong><br></p>

<blockquote>
  <p>1) Extract data from grib2 files thanks to a python script running on an Azure VM
  <br/> 2) Insert the transformed data into [Azure storage]
  <br/> 3) Query the [Azure storage] from Azure Machine Learning Service or a local R script (for example)
  <br/> 4) Insert the computed data into  [Azure storage]</p>
</blockquote>

<p>where [Azure Storage] technology is to determine.
<br/><br/> 
Any help or advice would be much appreciated, thanks.</p>",1,3,2019-05-17 13:16:19.200000 UTC,,,0,azure|azure-sql-database|azure-storage|azure-data-lake|azure-machine-learning-studio,230,2013-05-22 13:00:19.920000 UTC,2022-09-21 16:35:12.243000 UTC,Paris,918,50,25,227,,,,,,['azure-machine-learning-studio']
How can I use machine learning modules from Azure ML Studio in Azure ML Workbench,"<p>So I've been using Azure ML Studio for a while and now Microsoft have come up with the new tool called Azure ML Workbench.  The workbench seems to be pretty low level and it seems that the majority of functions need to be hand coded in Python.</p>

<p>So if I have an experiment in Azure ML Studio using some of the drag and drop Training methods such as Matchbox and Boosted Decision Trees.  How can I convert these to run in Azure ML Workbench ?    </p>",1,1,2017-12-03 04:12:51.540000 UTC,,,1,azure-machine-learning-studio|azure-machine-learning-workbench,192,2011-07-19 03:51:21.463000 UTC,2022-09-24 05:34:01.230000 UTC,French Polynesia,2229,560,20,240,,,,,,"['azure-machine-learning-workbench', 'azure-machine-learning-studio']"
"In Azure ML - After Web service deployment, getting column name not found error","<p>After converting the categorical variables to indicator values built the model using numeric &amp; indicator values variables. Web service deployment was successful, but however, while checking for Test response it's returning an error stating that:</p>
<pre><code>Select Columns in the dataset: Error 0001: Column with name or index &quot; product_ Sugar_content_No_ Sugar&quot; not found
Report Error.
Where &quot;product_ Sugar_content_No_ Sugar&quot; is an indicator column that is already added during model building.
</code></pre>
<p>Can anyone please help me to resolve this error?</p>
<p>I am getting this error only when I am using both numeric &amp; indicator variables for the model, but if I use only numeric variables I am able to get the Test response output.</p>
<p>Request your support.</p>",0,0,2022-03-14 07:06:03.353000 UTC,,2022-03-30 12:18:20.950000 UTC,0,azure-machine-learning-service,57,2022-03-05 11:56:32.057000 UTC,2022-04-25 14:03:07.030000 UTC,,1,0,0,2,,,,,,['azure-machine-learning-service']
SageMaker: How to manually run a Model Quality Monitor Job?,"<p>I'm using the SageMaker SDK class <code>ModelQualityMonitor</code> to configure Model Evaluation over time.</p>
<p>The monitoring schedule does not support monthly or weekly time periods, in my case, the ground Truth period is longer than one month.</p>
<p>There's an example from <a href=""https://github.com/aws-samples/amazon-sagemaker-mlops-workshop/blob/main/labs/05_model_monitor/05_model_monitor.ipynb"" rel=""nofollow noreferrer"">SageMaker Labs</a> to run Data Quality Monitor manually, the idea seems to provide the input <code>constraints</code> and <code>statistics</code> with a <code>ProcessingInput</code>, but not sure how to do this.</p>
<p>Is it possible to run the SageMaker Model Quality Monitor job manually?</p>",1,0,2022-04-21 17:40:09.903000 UTC,,,0,python|python-3.x|amazon-web-services|amazon-sagemaker,196,2019-12-05 03:20:17.430000 UTC,2022-09-23 18:22:37.093000 UTC,"Mexico City, CDMX, México",4882,1650,2,260,,,,,,['amazon-sagemaker']
How can I solve getting Unauthorized Access 401 error when pulling aws deep learning container with docker?,"<p>I tried building a detectron2 image with docker, in order to use with AWS SageMaker.  The dockerfile looks like this:</p>
<pre><code>ARG REGION=&quot;eu-central-1&quot;

FROM 763104351884.dkr.ecr.$REGION.amazonaws.com/pytorch-training:1.6.0-gpu-py36-cu101-ubuntu16.04

RUN pip install --upgrade torch==1.6.0+cu101 torchvision==0.7.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html

############# Detectron2 section ##############
RUN pip install \
    --no-cache-dir pycocotools~=2.0.0 \
    --no-cache-dir https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.6/detectron2-0.4%2Bcu101-cp36-cp36m-linux_x86_64.whl

   
ENV FORCE_CUDA=&quot;1&quot;
# Build D2 only for Volta architecture - V100 chips (ml.p3 AWS instances)
# ENV TORCH_CUDA_ARCH_LIST=&quot;Volta&quot;

# Set a fixed model cache directory. Detectron2 requirement
ENV FVCORE_CACHE=&quot;/tmp&quot;

############# SageMaker section ##############

COPY container_training/sku-110k /opt/ml/code
WORKDIR /opt/ml/code

ENV SAGEMAKER_SUBMIT_DIRECTORY /opt/ml/code
ENV SAGEMAKER_PROGRAM training.py

WORKDIR /

ENTRYPOINT [&quot;bash&quot;, &quot;-m&quot;, &quot;start_with_right_hostname.sh&quot;]
</code></pre>
<p>The problem is that when I run the docker build command, it fails at pulling the image from the AWS ECR repository. It throws the error</p>
<blockquote>
<p>ERROR [internal] load metadata for
763104351884.dkr.ecr.eu-central-1.amazonaws.com/pytorch-training:1.6.0-gpu  0.4s ------                                                                                                                   &gt; [internal] load metadata for 763104351884.dkr.ecr.eu-central-1.amazonaws.com/pytorch-training:1.6.0-gpu-py36-cu101-ubuntu16.04:
------                                                                                                                  failed to solve with frontend dockerfile.v0: failed to create LLB
definition: unexpected status code [manifests
1.6.0-gpu-py36-cu101-ubuntu16.04]: 401 Unauthorized</p>
</blockquote>
<p>I have to mention that I successfully login before trying to build and I have full ECR permissions on my user.</p>",1,0,2021-05-31 13:22:54.520000 UTC,,2021-05-31 16:54:00.470000 UTC,4,amazon-web-services|docker|deep-learning|amazon-sagemaker|amazon-ecr,5029,2019-01-15 00:35:21.967000 UTC,2022-03-04 14:37:17.533000 UTC,"Cluj-Napoca, România",61,5,0,9,,,,,,['amazon-sagemaker']
How to create AzureML environement and add required packages,<p>I want to use add some packages to the environment for AzureML environment. What is the way for that?</p>,1,0,2020-12-05 17:17:08.863000 UTC,,,-1,azure|azure-machine-learning-service|azureml-python-sdk,118,2017-01-19 06:38:11.853000 UTC,2022-09-20 02:56:14.233000 UTC,"Gurugram, Haryana, India",623,29,1,54,,,,,,['azure-machine-learning-service']
Sagemaker object detection prediction,"<p>I created a sagemaker object detection training job and the subsequent endpoint. I have just two classes. However, when I try to use the endpoint for prediction I get multiple rows in the response, like so:</p>

<pre><code>{""prediction"": [
[1.0, 0.632090151309967, 0.0, 0.003549516201019287, 1.0, 1.0], 
[0.0, 0.4135304093360901, 0.0, 0.006693154573440552, 1.0, 0.9729366302490234], 
[0.0, 0.018929673358798027, 0.0, 0.044431887567043304, 0.6495294570922852, 0.23290297389030457], 
[0.0, 0.01802791841328144, 0.0, 0.11557215452194214, 0.6625108122825623, 0.7412691712379456], 
[0.0, 0.015324527397751808, 0.0, 0.267954021692276, 0.6784608960151672, 0.39592066407203674], 
[0.0, 0.013910820707678795, 0.0, 0.8590829372406006, 0.7399784326553345, 1.0], 
[1.0, 0.013243389315903187, 0.928236186504364, 0.0, 1.0, 0.07348344475030899], 
[0.0, 0.012794392183423042, 0.9662157893180847, 0.0, 1.0, 0.057823698967695236], 
[0.0, 0.011968772858381271, 0.0, 0.9265779256820679, 0.04517384618520737, 1.0], 
[0.0, 0.011287822388112545, 0.953392744064331, 0.9526442885398865, 1.0, 1.0], 
[1.0, 0.01005781814455986, 0.8989022970199585, 0.9481537342071533, 1.0, 1.0]
]
}
</code></pre>

<p>Why are there multiple rows in the response?</p>",1,2,2019-01-08 16:08:51.253000 UTC,,,0,detection|amazon-sagemaker,213,2017-03-21 20:02:58.690000 UTC,2021-02-16 10:06:36.017000 UTC,"Atlanta, GA, United States",23,0,0,14,,,,,,['amazon-sagemaker']
Download checkpoint from AWS,<p>How can I download the checkpoints and logged statistics after I run my deep learning algorithm on AWS using SageMaker?</p>,1,0,2020-11-30 11:32:53.450000 UTC,,,0,amazon-web-services|amazon-s3|amazon-ec2|amazon-sagemaker,44,2017-04-11 11:57:20.993000 UTC,2022-05-20 08:49:10.053000 UTC,,1957,47,1,278,,,,,,['amazon-sagemaker']
How to use a csv with header for sagemaker batch transform?,"<p>I am performing a sagemaker batch transform using a transformer created out of an xgboost estimator. The csv input for prediction/batch transform has both, an ID column and a header (with names of columns). For example, something like this:</p>
<p>Name |Age |Height|Weight</p>
<p>Sam  |10  |2     |3</p>
<p>John |20  |3     |4</p>
<p>Jane |30  |4     |5</p>
<p>Of course, what needs to be passed is just the model inputs without the index (in this case, Name) or header (first row)</p>
<p>We can exclude the index (i.e. 0th) column by using the InputFilter argument when creating the job as follows:</p>
<pre><code>DataProcessing = { 
      &quot;InputFilter&quot;: &quot;$[1:]&quot;}
</code></pre>
<p>My question is how do we exclude the header? What JSONPath can be used for that?</p>",1,0,2022-05-25 14:44:48.157000 UTC,,,0,amazon-web-services|data-science|amazon-sagemaker|jsonpath|json-path-expression,287,2022-05-25 14:16:41.497000 UTC,2022-05-31 04:14:48.747000 UTC,"Revere, MA",1,0,0,1,,,,,,['amazon-sagemaker']
Azure machine learning - data set contains NA but it is not showing as missing value,"<p>I am new to azure machine learning, I am working on house price prediction data set.
This data set contain NA values in multiple columns both numeric and categorical but when I am trying to clean it, it is not getting cleaned, because azure is not considering it as missing value. Please let me know how to treat NA missing values.</p>
<p><strong>What I tried :</strong>
i tried 'convert dataset' to replace NA with 0 but it is replacing all NA's in categorical as well as numeric columns which i don't want. I want to replace NA in numeric column with only mean not 0.</p>
<p><strong>What I want</strong> : I want to replace NA in categorical columns with 'Not available' string and in numeric columns with mean.</p>",0,0,2022-08-05 18:17:46.450000 UTC,,,0,python|azure|azure-pipelines|azure-machine-learning-studio,47,2017-11-22 18:48:57.900000 UTC,2022-09-06 08:04:12.870000 UTC,India,1180,164,4,88,,,,,,['azure-machine-learning-studio']
Accessing MLFlow UI with a folder name different than mlruns,"<p>I set the <code>tracking_uri</code> to a folder name different than <code>mlruns</code>. </p>

<p>Is there a way I can open the <strong>MLFlow UI</strong> pointing to the new folder name for mlruns? </p>

<p>I know I can rename the folder back to <code>mlruns</code>, which gets me access to all of my metrics and parameters for each experiment, but the artifacts are not accessible, since they were logged to a different folder name than mlruns. </p>",1,1,2019-02-19 19:41:07.453000 UTC,,2019-02-19 20:50:20.313000 UTC,0,mlflow,1521,2019-02-19 19:38:23.667000 UTC,2019-12-06 03:47:18.253000 UTC,,1,0,0,3,,,,,,['mlflow']
reading a csv.gz file from sagemaker using pyspark kernel mode,"<p>i am trying to read a compressed csv file in pyspark. but i am unable to read in pyspark kernel mode in sagemaker.</p>
<p>The same file i can read using pandas when the kernel is conda-python3 (in sagemaker)</p>
<p>What I tried :</p>
<pre><code>file1 =  's3://testdata/output1.csv.gz'
file1_df = spark.read.csv(file1, sep='\t')
</code></pre>
<p>Error message :</p>
<pre><code>An error was encountered:
An error occurred while calling 104.csv.
: java.io.IOException: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: 7FF77313; S3 Extended Request ID: 
</code></pre>
<p>Kindly let me know if i am missing anything</p>",1,0,2020-10-07 06:26:37.903000 UTC,,,0,python|apache-spark|amazon-s3|pyspark|amazon-sagemaker,790,2015-09-09 17:49:17.717000 UTC,2021-06-11 07:42:58.193000 UTC,,351,26,0,101,,,,,,['amazon-sagemaker']
GPU error: Sagemaker mp.p2.xlarge instance using tensorflow==2.3.0,"<p>I got the following error when trying to train my tensorflow model on sagemaker ml.p2.xlarge instance. I use tensorflow==2.3.0. I wonder whether this is because of the tensorflow version incompatibility with cuda. sagemaker ml.p2.xlarge seems to use cuda 10.0</p>
<pre><code>GPU error:
2020-08-31 08:46:46.429756: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/openmpi/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
2020-08-31 08:47:02.170819: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/openmpi/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
2020-08-31 08:47:02.764874: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1753] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
</code></pre>",0,0,2020-08-31 10:24:19.907000 UTC,,2020-08-31 11:31:16.760000 UTC,4,tensorflow|amazon-sagemaker,361,2020-08-27 07:26:30.250000 UTC,2022-01-07 03:46:29.657000 UTC,,51,0,0,2,,,,,,['amazon-sagemaker']
Add run id when registering ml.azure model via python (pipeline),"<p>I have registed a model in this way:</p>
<pre><code>from azureml.core.model import Model
model = Model.register(model_path=&quot;sklearn_regression_model.pkl&quot;,
                      model_name=&quot;sklearn_regression_model&quot;,
                      tags={'area': &quot;diabetes&quot;, 'type': &quot;regression&quot;},
                      description=&quot;Ridge regression model to predict diabetes&quot;,
                      workspace=ws)
</code></pre>
<p>However I would like to add run id, from the experiment, so I can always back-track the model to the experiment that created the model. In azure ml there is a column indicating that it is possible to add run id to a registered model, however the model class doesn't have this parameter.</p>",2,1,2021-11-04 11:17:06.163000 UTC,,,1,azure-machine-learning-service,391,2021-11-04 10:17:16.363000 UTC,2022-06-20 09:09:39.263000 UTC,Denmark,11,0,0,2,,,,,,['azure-machine-learning-service']
Conda(mini) hangs Collecting package metadata when creating 2 environments on the same script,"<p>We need to create(and install packages on) 2 conda(we are using mini-conda) environments on the same shell script(AWS linux instance for Sagemaker), the first creation and package installation works fine, but for the second, it hangs forever in</p>
<blockquote>
<p>Collecting package metadata (repodata.json): ...working... done</p>
</blockquote>
<p>I know this has been discussed(and some solutions proposed) before, but none has worked for me, according to some of the workarounds I have tried:</p>
<ul>
<li>Remove conda-forge from the channels: this created a version incompatibility nightmare, and some of the needed packages are not in the base channels.</li>
<li>conda config --set channel_priority flexible: did not try it because conda config --describe channel_priority shows it is already flexible.</li>
<li>Add more RAM: I doubled memory and the problem still persists</li>
<li>edit conda/conda/core/subdir_data.py and change the line  and modify the line Executor = (DummyExecutor if context.debug or context.repodata_threads == 1</li>
<li>Install and use Mamba: it got errors and inconsistencies warnings, searching for the errors I found mamba only works with python 2.7(I need python&gt;=3.7)</li>
<li>Probably some others I don't remember</li>
</ul>
<p>Has anyone installed more than 1 conda env. using a single script? Any ideas on how to fix the that hanging issue?</p>
<p><strong>Update:</strong> in case someone else finds this issue, after trying everything without success, by chance we remove one package(snowflake connector) in the script(removed the conda install) and then the installation proceeded, then it got stuck again when checking the logs it was the snowflake-sql alchemy), not sure if it is just a coincidence that both packages are related to snowflake, but removed them from conda and installed them with pip and the script works.</p>",1,0,2022-06-16 16:13:40.273000 UTC,,2022-06-20 15:50:16.410000 UTC,0,python|amazon-web-services|anaconda|conda|amazon-sagemaker,280,2016-06-15 18:26:00.217000 UTC,2022-09-20 17:52:45.723000 UTC,"Guatemala City, Guatemala Department, Guatemala",3287,99,24,435,,,,,,['amazon-sagemaker']
"Mlflow-got error no host supplied,provided uri tracking,help me to resolve it","<p>In below image can see i mention tracking uri and trying to load model but facing error in host supplied. <a href=""https://i.stack.imgur.com/GmLeq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GmLeq.png"" alt=""enter image description here"" /></a></p>",0,0,2022-09-16 14:34:29.023000 UTC,,,0,mlflow|mlops,12,2019-12-12 05:07:25.140000 UTC,2022-09-24 21:10:52.373000 UTC,"Mumbai, Maharashtra, India",21,0,0,3,,,,,,['mlflow']
Can we generate custom logs for Sagemaker notebook instance in CloudWatch?,"<p>CloudWatch records logs for Sagemaker instance such as Kernel Started, Kernel shutdown, Notebook Saved etc by default. Though, I want to list some custom logs along with these default logs.</p>
<p>Please have a look at the picture attached.</p>
<p><a href=""https://i.stack.imgur.com/g680f.png"" rel=""nofollow noreferrer"">Sample image of How default logs for a Sagemaker notebook instance look in CloudWatch</a></p>
<p>The goal is to be able to see some custom logs with these. For example - 'Cell 1 executed!'</p>",1,5,2021-04-12 09:57:11.750000 UTC,,,3,amazon-web-services|logging|amazon-cloudwatch|amazon-sagemaker|amazon-cloudwatchlogs,1411,2020-03-12 04:59:38.040000 UTC,2021-10-08 05:08:54.623000 UTC,"Jaipur, Rajasthan, India",47,1,0,7,,,,,,['amazon-sagemaker']
AzureML - CondaDependencies + Base Image Issue,"<p>I'm trying to create a new environment in AzureML.
For that, i wrote this :</p>
<pre class=""lang-py prettyprint-override""><code>from azureml.core import Workspace,Environment
from azureml.core.conda_dependencies import CondaDependencies

ws=Workspace.from_config()
# We get the existing base environment
BaseEnv = Environment.get(workspace=ws, name=&quot;AzureML-pytorch-1.10-ubuntu18.04-py38-cuda11-gpu&quot;)
new_env = BaseEnv.clone(&quot;vldtorch_110&quot;)
whl_url = Environment.add_private_pip_wheel(workspace=ws,file_path = &quot;lib/carrow-1.0.1-py3-none-any.whl&quot;,exist_ok=True)
conda_dep = CondaDependencies()
# Installs numpy version 1.17.0 conda package
#conda_dep.add_conda_package(&quot;numpy==1.17.0&quot;)
# Installs pillow package

conda_dep.add_pip_package(&quot;albumentations==1.2.0&quot;)
conda_dep.add_pip_package(&quot;scikit-learn==1.1.1&quot;)
conda_dep.add_pip_package(&quot;pandas==1.4.3&quot;)
conda_dep.add_pip_package(&quot;tqdm==4.64.0&quot;)
conda_dep.add_pip_package(whl_url)
# Adds dependencies to PythonSection of myenv
new_env.python.user_managed_dependencies
new_env.python.conda_dependencies=conda_dep

# Register the environment
new_env.register(workspace=ws)
new_env.build(workspace=ws)
</code></pre>
<p>The creation is successful :<br>
<a href=""https://i.stack.imgur.com/ug3D3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ug3D3.png"" alt=""enter image description here"" /></a></p>
<p>I see my dependencies in the context :
<a href=""https://i.stack.imgur.com/YnfeH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YnfeH.png"" alt=""enter image description here"" /></a></p>
<p>But when i use this environment i don't see my conda dependencies.
For exemple albumentations.</p>
<p>I tried another way in order to avoid starting from the base environment. I only reused the docker image.</p>
<pre class=""lang-py prettyprint-override""><code>from azureml.core import Workspace,Environment
from azureml.core.conda_dependencies import CondaDependencies

ws=Workspace.from_config()
# We get the existing base environment
new_env = Environment(&quot;vldtorch_110&quot;)
conda_dep = CondaDependencies()
conda_dep.add_channel(&quot;conda-forge&quot;)
conda_dep.add_channel(&quot;pytorch&quot;)
# Installs conda packages
conda_dep.add_conda_package(&quot;python==3.8&quot;)
conda_dep.add_conda_package(&quot;pip==20.2.4&quot;)
conda_dep.add_conda_package(&quot;albumentations==1.0.3&quot;)
conda_dep.add_conda_package(&quot;scikit-learn==1.1.1&quot;)
conda_dep.add_conda_package(&quot;pandas==1.4.3&quot;)
conda_dep.add_conda_package(&quot;tqdm==4.64.0&quot;)
conda_dep.add_conda_package(&quot;pytorch==1.10.0&quot;)
conda_dep.add_conda_package(&quot;torchvision==0.11.1&quot;)
conda_dep.add_conda_package(&quot;cudatoolkit==11.1.1&quot;)
conda_dep.add_conda_package(&quot;nvidia-apex==0.1.0&quot;)
conda_dep.add_conda_package(&quot;gxx_linux-64&quot;)

# Installs pillow packages
whl_url = Environment.add_private_pip_wheel(workspace=ws,file_path = &quot;lib/carrow-1.0.1-py3-none-any.whl&quot;,exist_ok=True)
conda_dep.add_pip_package(whl_url)
# Adds dependencies to PythonSection of myenv
new_env.python.conda_dependencies=conda_dep
new_env.docker.base_image=&quot;mcr.microsoft.com/azureml/openmpi4.1.0-cuda11.1-cudnn8-ubuntu18.04:20220616.v1&quot;
# Register the environment
new_env.register(workspace=ws)
new_env.build(workspace=ws)
</code></pre>
<p>But with this configuration, i reach the 90 minutes timeout here :<br>
<a href=""https://i.stack.imgur.com/cwoMm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cwoMm.png"" alt=""enter image description here"" /></a>
<a href=""https://i.stack.imgur.com/2aUf7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2aUf7.png"" alt=""enter image description here"" /></a></p>
<p>Do you have any ideas about what's going wrong ?
Thanks !</p>",0,0,2022-06-28 09:41:43.203000 UTC,,2022-06-28 12:11:05.673000 UTC,0,python|azure|environment|azure-machine-learning-service,77,2014-01-27 18:51:27.727000 UTC,2022-09-23 14:16:36.043000 UTC,"Vichy, France",205,6,0,48,,,,,,['azure-machine-learning-service']
SageMaker Studio Notebook Cell Continues Running After Training Job is Complete,"<p>I have an issue where the cell that calls <code>fit</code> stalls out, even when the training job is &quot;Complete&quot; in <code>SageMaker &gt; Training jobs</code>.</p>
<p>I've tried several examples, the simplest being <a href=""https://sagemaker-examples.readthedocs.io/en/latest/aws_sagemaker_studio/sagemaker_algorithms/linear_learner_mnist/linear_learner_mnist.html"" rel=""nofollow noreferrer"">one that uses SM's library</a>.</p>
<p>I copied the code exactly into cells in my SM Studio notebook, set up the kernel to Data Science and the instance to m5.xlarge (m4 used in example but doesn't exist in dropdown).</p>
<p>I run the block that calls <code>linear.fit({&quot;train&quot;: s3_train_data})</code>, the job shows up as completed, but in the notebook the cell continues to run, and I am unable to execute any subsequent cells.</p>
<p>The only output I receive is: <code>2021-11-29 21:26:47 Starting - Starting the training job</code></p>
<p>Considering this uses demo code, I feel like something's going wrong on a higher level.</p>
<p>What could be causing a SageMaker Studio notebook cell to continue running after the training job is complete?</p>
<p><strong>As per mod's request:</strong></p>
<p>Error message: there isn't one; the code cell stalls as described in the post</p>
<p>Desired outcome: For the notebook to not stall despite saying the training job was complete, as described in the post</p>
<p>Sample code: from the example I linked in the post</p>
<pre><code>import sagemaker
bucket = sagemaker.Session().default_bucket()
prefix = &quot;sagemaker/DEMO-linear-mnist&quot;
import boto3
import re
from sagemaker import get_execution_role
role = get_execution_role()

import pickle, gzip, numpy, urllib.request, json

fobj = boto3.client('s3').get_object(
    Bucket='sagemaker-sample-files',
    Key='datasets/image/MNIST/mnist.pkl.gz'
)['Body'].read()

with open('mnist.pkl.gz', 'wb') as f:
    f.write(fobj)

# Load the dataset
with gzip.open(&quot;mnist.pkl.gz&quot;, &quot;rb&quot;) as f:
    train_set, valid_set, test_set = pickle.load(f, encoding=&quot;latin1&quot;)
%matplotlib inline
import matplotlib.pyplot as plt

plt.rcParams[&quot;figure.figsize&quot;] = (2, 10)


def show_digit(img, caption=&quot;&quot;, subplot=None):
    if subplot == None:
        _, (subplot) = plt.subplots(1, 1)
    imgr = img.reshape((28, 28))
    subplot.axis(&quot;off&quot;)
    subplot.imshow(imgr, cmap=&quot;gray&quot;)
    plt.title(caption)


show_digit(train_set[0][30], &quot;This is a {}&quot;.format(train_set[1][30]))
import io
import numpy as np
import sagemaker.amazon.common as smac

vectors = np.array([t.tolist() for t in train_set[0]]).astype(&quot;float32&quot;)
labels = np.where(np.array([t.tolist() for t in train_set[1]]) == 0, 1, 0).astype(&quot;float32&quot;)

buf = io.BytesIO()
smac.write_numpy_to_dense_tensor(buf, vectors, labels)
buf.seek(0)
import boto3
import os

key = &quot;recordio-pb-data&quot;
boto3.resource(&quot;s3&quot;).Bucket(bucket).Object(os.path.join(prefix, &quot;train&quot;, key)).upload_fileobj(buf)
s3_train_data = &quot;s3://{}/{}/train/{}&quot;.format(bucket, prefix, key)
print(&quot;uploaded training data location: {}&quot;.format(s3_train_data))
output_location = &quot;s3://{}/{}/output&quot;.format(bucket, prefix)
print(&quot;training artifacts will be uploaded to: {}&quot;.format(output_location))
from sagemaker.image_uris import retrieve

container = retrieve(&quot;linear-learner&quot;, boto3.Session().region_name)
import boto3

sess = sagemaker.Session()

linear = sagemaker.estimator.Estimator(
    container,
    role,
    train_instance_count=1,
    train_instance_type=&quot;ml.c4.xlarge&quot;,
    output_path=output_location,
    sagemaker_session=sess,
)
linear.set_hyperparameters(feature_dim=784, predictor_type=&quot;binary_classifier&quot;, mini_batch_size=200)

linear.fit({&quot;train&quot;: s3_train_data})
</code></pre>",1,0,2021-11-30 16:16:32.267000 UTC,,,0,jupyter-notebook|amazon-sagemaker,174,2015-12-16 08:36:12.380000 UTC,2022-09-23 16:18:23.423000 UTC,,171,15,0,11,,,,,,['amazon-sagemaker']
Running two commands with terraform local-exec provisioner,"<p>I need to run two commands but the second command depends on the value of first command.</p>
<pre><code>resource &quot;null_resource&quot; &quot;datastore&quot; {
  triggers = {
    always_run = &quot;${timestamp()}&quot;
  }
  provisioner &quot;local-exec&quot; {
     command = &quot;az keyvault secret show --vault-name &quot;nameofkv&quot; --name &quot;secretname&quot; --query &quot;value&quot; &amp;&amp; xargs -I{} az ml datastore attach-adls-gen2 --account-name [] --client-id [] --client-secret {} --file-system refined zone --name refinedzonetest --tenant-id [] --workspace-name [] --endpoint core.usgovcloudapi.net --subscription-id [] --resource-url https://storage.azure.com --authority-url https://login.microsoftonline.us&quot;
}
 depends_on = [azurerm_machine_learning_workspace.ml]
</code></pre>
<p>}</p>
<p>the --secret {} depends on the first command. However, terraform gives an error saying &quot;new line must exist after argument&quot;</p>
<p>so If I put the second command in a new line, the second command does not run. any input in this issue or how I can improve it.</p>",0,1,2022-06-01 15:08:51.133000 UTC,,,0,terraform|azure-cli|azure-machine-learning-service,369,2022-04-25 18:37:53.863000 UTC,2022-09-23 17:44:33.570000 UTC,,1,0,0,2,,,,,,['azure-machine-learning-service']
Boto3 / MLflow model logging via temporary AWS credentials,"<p>We are building an ML tracking service using MLflow as a backend. One issue we've run into is that in order to log models via MLflow's python API​, the user needs to have AWS credentials configured on their machine. Since our service is outward-facing, we can't really let users have the access key for our S3 bucket. Is there a mechanism for authenticating a boto3 client used by MLflow via some temporary AWS credentials? We can generate a signed URL with write permissions to the bucket, but it's unclear how we would then pass it onto the boto3 client / MLflow python api. Or can we do something with environment variables? In any case, if someone knows of a good way to do this - I'd greatly appreciate the help.
Best,
SP</p>",0,0,2021-03-16 15:02:05.970000 UTC,,,2,python|amazon-web-services|boto3|credentials|mlflow,264,2017-10-27 20:19:08.107000 UTC,2022-06-28 13:46:29.090000 UTC,,53,6,0,15,,,,,,['mlflow']
Problem crating a Ranger model with R to use for MLflow,"<p>I am trying to use MLflow in R. According to <a href=""https://www.mlflow.org/docs/latest/models.html#r-function-crate"" rel=""nofollow noreferrer"">https://www.mlflow.org/docs/latest/models.html#r-function-crate</a>, the crate flavor needs to be used for the model. My model uses the Random Forest function implemented in the ranger package:</p>
<pre><code>model &lt;- ranger::ranger(formula    = model_formula, 
                        data       = trainset,
                        importance = &quot;impurity&quot;, 
                        probability=T, 
                        num.trees  = 500, 
                        mtry       = 10)
</code></pre>
<p>The model itself works and I can do the prediction on a testset:</p>
<pre><code>test_prediction &lt;- predict(model, testset)
</code></pre>
<p>As a next step, I try to bring the model in the crate flavor. I follow here the approach shown in <a href=""https://docs.databricks.com/_static/notebooks/mlflow/mlflow-quick-start-r.html"" rel=""nofollow noreferrer"">https://docs.databricks.com/_static/notebooks/mlflow/mlflow-quick-start-r.html</a>.</p>
<pre><code>predictor &lt;- crate(function(x) predict(model,.x))
</code></pre>
<p>This results however in an error, when I apply the &quot;predictor&quot; on the testset</p>
<pre><code>predictor(testset)
Error in predict(model, .x) : could not find function &quot;predict&quot;
</code></pre>
<p>Does anyone know how to solve this issue? To I have to transfer the prediction function differently in the crate function? Any help is highly appreciated ;-)</p>",1,0,2021-07-03 15:47:25.660000 UTC,,,4,r|mlflow|crate,191,2021-07-02 15:07:43.353000 UTC,2022-06-08 05:33:49.967000 UTC,,41,0,0,4,,,,,,['mlflow']
how to properly install mlflow in R studio?,"<p>I followed the official document but found error</p>
<pre><code>&gt; install.package(&quot;mlflow&quot;)
Error in install.package(&quot;mlflow&quot;) : 
  could not find function &quot;install.package&quot;
&gt; install.packages(&quot;mlflow&quot;)
Installing package into ‘C:/Users/fzhu/AppData/Local/R/win-library/4.2’
(as ‘lib’ is unspecified)
trying URL 'https://cran.rstudio.com/bin/windows/contrib/4.2/mlflow_1.28.0.zip'
Content type 'application/zip' length 237435 bytes (231 KB)
downloaded 231 KB

package ‘mlflow’ successfully unpacked and MD5 sums checked

The downloaded binary packages are in
    C:\Users\fzhu\AppData\Local\Temp\RtmpgzY6Uy\downloaded_packages
&gt; mlflow::install_mlflow()
Error in mlflow_conda_bin() : 
  Unable to find conda binary. Is Anaconda installed?
  If you are not using conda, you can set the environment variable MLFLOW_PYTHON_BIN to the path of your python executable.
&gt; Sys.getenv(&quot;MLFLOW_PYTHON_BIN&quot;)
[1] &quot;C:\\py310\\python.exe&quot;
&gt; Sys.getenv(&quot;MLFLOW_BIN&quot;)
[1] &quot;C:\\py310\\Scripts\\mlflow&quot;

</code></pre>
<p>any help is appreciated!</p>
<p>refer:
<a href=""https://github.com/mlflow/mlflow/issues/6738"" rel=""nofollow noreferrer"">https://github.com/mlflow/mlflow/issues/6738</a></p>",0,4,2022-09-08 19:14:47.383000 UTC,,,0,r|installation|rstudio|mlflow,27,2013-11-04 03:19:11.877000 UTC,2022-09-23 02:06:10.570000 UTC,,502,24,1,40,,,,,,['mlflow']
Python support for Azure ML -- speed issue,"<p>We are trying to create an Azure ML web-service that will receive a (.csv) data file, do some processing, and return two similar files. The Python support recently added to the azure ML platform was very helpful and we were able to successfully port our code, run it in experiment mode and publish the web-service.</p>

<p>Using the ""batch processing"" API, we are now able to direct a file from blob-storage to the service and get the desired output. However, run-time for small files (a few KB) is significantly slower than on a local machine, and more importantly, the process seems to never return for slightly larger input data files (40MB). Processing time on my local machine for the same file is under 1 minute. </p>

<p>My question is if you can see anything we are doing wrong, or if there is a way to get this to speed up. Here is the DAG representation of the experiment:</p>

<p><img src=""https://i.stack.imgur.com/MalhQ.png"" alt=""The DAG representation of the experiment""></p>

<p>Is this the way the experiment should be set up? </p>",1,3,2015-03-08 17:51:08.693000 UTC,,2015-03-09 08:41:08.080000 UTC,0,python|azure|azure-machine-learning-studio,491,2014-12-29 15:50:47.680000 UTC,2018-08-24 12:34:36.180000 UTC,,877,46,0,55,,,,,,['azure-machine-learning-studio']
MLFlow how to change backend store uri from file storage to DB,"<p>I am using mlflow tracking with file storage as backend store for a while, I have a lot of runs logged in the system.<br />
Lately I wanted to start using the model registry but unfortunately this feature is currently supported only with DB as the backend store.<br />
How can I change the backend store without loosing all the runs that I have already logged?</p>
<p>The command that I am using to run the server:</p>
<pre><code>mlflow server --backend-store-uri /storage/mlflow/runs/ --default-artifact-root /storage/mlflow/artifactory/ --host 0.0.0.0 --port 5000
</code></pre>",3,1,2019-12-04 16:51:07.300000 UTC,3.0,2020-07-17 21:06:30.860000 UTC,9,mlflow,4351,2019-12-04 16:41:44.413000 UTC,2021-08-30 13:27:09.853000 UTC,,91,0,0,5,,,,,,['mlflow']
Defining sagemaker pipeline resource in Terraform,"<p>I'm rewriting a CloudFormation template into terraform and there is a CF resource I don't know the equivalent in TF.</p>
<p>The CF resource is <strong>AWS::SageMaker::Pipeline</strong></p>
<p>Below is the fragment of the template.yaml</p>
<pre><code>  pipeline:
    Type: AWS::SageMaker::Pipeline
    Properties:
      PipelineName: !Ref pPipelineName
      PipelineDisplayName: !Ref pPipelineName
      PipelineDescription: !Ref pPipelineDescription
      PipelineDefinition:
          PipelineDefinitionBody: !Sub &quot;{\&quot;Version\&quot;:\&quot;2020-12-01\&quot;,........}}]}&quot;
      RoleArn: !Ref pPipelineRoleArn
      Tags:
            - Key: project
              Value: !Ref pProjectName
</code></pre>
<p>Have someone defined this resource in Terraform?</p>",1,0,2022-02-09 14:20:02.067000 UTC,,,0,terraform|amazon-cloudformation|amazon-sagemaker,293,2020-09-07 19:05:46.577000 UTC,2022-03-23 13:23:08.600000 UTC,Córdoba Argentina,1,0,0,3,,,,,,['amazon-sagemaker']
Can AZ ML workbench reference multiple data sources from Data Prep Transform Dataflow expression,"<p>Using AZ ML workbench for a class project (required tool) I coded the desired logic below in an exploration notebook but cannot find a way to include this into a Data-prep Transform Data flow.</p>

<p><code>all_columns = df.columns
sum_columns = [col_name for col_name in all_columns if col_name not in ['NPI', 'Gender', 'State', 'Credentials', 'Specialty']]
sum_op_columns = list(set(sum_columns) &amp; set(df_op['Drug Name'].values))</code></p>

<p>The logic is using the column names from one data source df_op (opioid drugs) to choose which subset of columns to include from another data source df (all drugs). When adding a py script/expression Transform Data Flow I'm only seeing the ability to reference the single df. Alternatives?</p>",1,0,2018-02-10 20:10:50.697000 UTC,,,0,azure-machine-learning-studio|azure-machine-learning-workbench,71,2016-10-03 17:37:35.767000 UTC,2019-05-28 23:53:53.907000 UTC,,1,0,0,2,,,,,,"['azure-machine-learning-workbench', 'azure-machine-learning-studio']"
AWS notebook instance boto3 permission denied when reading from s3,"<p>I have an AWS instance running in a loop which loads files from an s3 folder as they appear using boto3, reads them, does some processing, then deletes the file. Instance is a sagemaker instance, with full access to the s3 folder.</p>
<p>This process below works fine when there are any number of files in the s3 folder to work through. However if a new file is created while the below loop is running, then when it tries to load that new file at some later point (dataframe = read_csv(filepath, header=None)) then I get a permission denied error. 'is_file_available' spots the file is there, but error occurs when triying to open the file.</p>
<p>Is there something I am missing, e.g.. closing connection?</p>
<p>I have to close / restart the kernel and restart the process to fix the issue.</p>
<pre><code># Check if file is available to predict and return file id (int)
def is_file_available():
    my_bucket = s3.Bucket('processing-ml')
    id = -1
    for obj in my_bucket.objects.filter(Prefix='to-process/acc'): #Delimiter=''):
        filename = obj.key
        id = mk_int(filename)
        print('acc.csv found id = ',id)
        
    return id

# load a single file as a numpy array
def load_file(filepath):
    dataframe = read_csv(filepath, header=None)
    return dataframe.values

#load data
def load_dataset_group(id):
    filepath = 's3://processing-ml/to-process/acc' + str(id) + '.csv'
    print('filepath',filepath)
    data = load_file(filepath)
    loaded = list()
    loaded.append(data)
    print(data.shape)
    return loaded

while True:
    #Run forever
    file_id = is_file_available()
    if file_id != -1:
        data = load_dataset_group(file_id)

        ... do stuff with data ...

        #delete the file in s3 now finished with it
        s3.Object('processing-ml', 'to-process/acc' + str(file_id) + '.csv').delete()

    time.sleep(1)
</code></pre>",1,1,2020-07-24 12:03:10.147000 UTC,,2020-07-24 12:18:33.233000 UTC,0,amazon-web-services|amazon-s3|boto3|amazon-sagemaker,233,2012-06-28 06:01:45.523000 UTC,2021-10-27 19:48:01.737000 UTC,,177,4,0,14,,,,,,['amazon-sagemaker']
SageMaker XGBoost hyperparameter tuning versus XGBoost python package,"<p>I am trying to do hyperparameter tuning of xgboost model. I started with AWS Sagemaker Hyperparameter Tuning, with the following parameter range:</p>

<pre><code>xgb.set_hyperparameters(eval_metric='auc',
                        objective='binary:logistic',
                        early_stopping_rounds=500,
                        rate_drop=0.1,
                        colsample_bytree=0.8,
                        subsample=0.75,
                        min_child_weight=0)

hyperparameter_ranges = {'eta': ContinuousParameter(0.01, 0.3),
                         'lambda': ContinuousParameter(0.1, 2),
                         'alpha': ContinuousParameter(0.5, 2),
                         'max_depth': IntegerParameter(5, 10),
                         'num_round': IntegerParameter(500, 2000)}

objective_metric_name = 'validation:auc'

tuner = HyperparameterTuner(xgb,
                            objective_metric_name,
                            hyperparameter_ranges,
                            max_jobs=10,  
                            max_parallel_jobs=3,
                            tags=[{'Key': 'Application', 'Value': 'cxxx'}])
</code></pre>

<p>And get a best model with the following set of hyperparameters:</p>

<pre><code>{
  ""alpha"": ""1.4009334471163981"",
  ""eta"": ""0.05726016655019904"",
  ""lambda"": ""1.2070623852474922"",
  ""max_depth"": ""7"",
  ""num_round"": ""1052""
}
</code></pre>

<p>Out of curiosity, I hooked up these hyperparameters into xgboost python package, as such:</p>

<pre><code>xgb_model = xgb.XGBClassifier(max_depth = 7,
                          silent = False,
                          random_state = 42,
                          n_estimators = 1052,
                          learning_rate = 0.05726016655019904,
                          objective = 'binary:logistic',
                          verbosity = 1,
                          reg_alpha = 1.4009334471163981,
                          reg_lambda = 1.2070623852474922,
                          rate_drop=0.1,
                          colsample_bytree=0.8,
                          subsample=0.75,
                          min_child_weight=0
                        )

</code></pre>

<p>I retrained the model and realized the results I got from the latter is better than that from SageMaker.
xgboost (auc of validation set): 0.766
SageMaker best model (auc of validation set):0.751</p>

<p>I wonder why SageMaker perform so poorly? If SageMaker usually perform worse than xgboost python package, how do people usually do xgboost hyperparameter tuning? Thanks for any hints!</p>",1,2,2020-02-26 22:44:18.690000 UTC,,,1,xgboost|amazon-sagemaker|hyperparameters,1300,2014-05-25 15:30:58.507000 UTC,2022-07-24 23:13:53.413000 UTC,"Seattle, WA, United States",1021,92,0,120,,,,,,['amazon-sagemaker']
How can I use the trained image classification model in Sagemaker to perform inference on images upload through the web page?,"<p>I have trained my image classification model using sagemaker and have deployed using endpoint. I have also create a web page that accepts image. Also, I was able to create the api. I am not sure, how to create the lambda function to accept the image and perform inference on the image. Also, should I include pre-processing steps in the lambda function. </p>

<p>Thank you in advance.</p>",1,0,2020-03-27 00:35:55.750000 UTC,,2020-09-20 01:26:55.833000 UTC,0,amazon-web-services|aws-lambda|detection|amazon-sagemaker|inference,212,2020-03-25 05:16:27.997000 UTC,2020-04-14 00:11:32.707000 UTC,"Toronto, ON, Canada",1,0,0,2,,,,,,['amazon-sagemaker']
Register model from pipeline (CLI v2),"<p>I have a pipeline where I do training for a model and at the end I'd like to register that model but I can't find any information on this.</p>
<p>Here's how my pipeline looks now:</p>
<pre class=""lang-yaml prettyprint-override""><code>type: pipeline_job

compute:
  target: azureml:gpu-cluster

inputs:
  input_images:
    data:
      datastore: azureml:dualcam
      path: /image-20210701*

jobs:
  training:
    type: component_job
    component: file:./components/training.yml
    inputs:
      input_images: inputs.input_images
</code></pre>",1,0,2021-07-15 14:45:22.023000 UTC,,,2,azure-machine-learning-service,205,2012-01-26 14:27:40.553000 UTC,2022-09-24 16:26:41.580000 UTC,,802,288,0,91,,,,,,['azure-machine-learning-service']
How to schedule Azure Machine learning experiment periodically and also give admin to run it when he wants,"<p>I have a published AzureML experiment and now I want to schedule that experiment periodically and also want to give flexibility to admin to run that experiment when he wants.</p>

<p>I tried running experiment periodically using Logic App azure service but getting an error ""This session has timed out. To see the latest run status, navigate to the runs history blade."". Can anyone help me out?</p>",0,7,2019-03-12 18:27:35.573000 UTC,,,0,azure|scheduler|azure-logic-apps|azure-machine-learning-studio,329,2019-03-11 16:56:29.193000 UTC,2020-05-15 08:41:04.897000 UTC,,19,0,0,16,,,,,,['azure-machine-learning-studio']
Updating pandas via conda on Sagemaker EXTREMELY slow,"<p>Sagemaker default python environments hosted in my work environment have outdated pandas, and therefore must have their conda environment updated.  However, this is incredibly slow (15-30 mins), and I would like to find a faster way to get a working environment</p>
<p>I update with the following:</p>
<pre><code>!conda update pandas fsspec --yes
</code></pre>
<p>Which gives the following output, with the key problem being an inconsistent starting environment (How?) as shown by
<code>failed with repodata from current_repodata.json, will retry with next repodata source. Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source. Collecting package metadata (repodata.json): done</code></p>
<p>output:</p>
<pre><code>Collecting package metadata (current_repodata.json): done
Solving environment: / 
The environment is inconsistent, please check the package plan carefully
The following packages are causing the inconsistency:

  - defaults/linux-64::pandas==1.0.1=py36h0573a6f_0
  - defaults/noarch::jupyterlab==1.2.6=pyhf63ae98_0
  - defaults/linux-64::scikit-learn==0.22.1=py36hd81dba3_0
  - defaults/linux-64::python-language-server==0.31.7=py36_0
  - defaults/linux-64::bkcharts==0.2=py36_0
  - defaults/linux-64::nb_conda==2.2.1=py36_0
  - defaults/noarch::numpydoc==0.9.2=py_0
  - defaults/linux-64::pytest-arraydiff==0.3=py36h39e3cac_0
  - defaults/linux-64::bottleneck==1.3.2=py36heb32a55_0
  - defaults/linux-64::pywavelets==1.1.1=py36h7b6447c_0
  - defaults/noarch::pytest-astropy==0.8.0=py_0
  - defaults/linux-64::numexpr==2.7.1=py36h423224d_0
  - defaults/noarch::anaconda-project==0.8.4=py_0
  - defaults/noarch::boto3==1.9.162=py_0
  - defaults/linux-64::s3transfer==0.2.1=py36_0
  - defaults/linux-64::nbconvert==5.6.1=py36_0
  - defaults/linux-64::h5py==2.10.0=py36h7918eee_0
  - defaults/linux-64::bokeh==1.4.0=py36_0
  - defaults/noarch::jupyterlab_server==1.0.6=py_0
  - defaults/linux-64::numpy-base==1.18.1=py36hde5b4d6_1
  - defaults/noarch::botocore==1.12.189=py_0
  - defaults/linux-64::jupyter==1.0.0=py36_7
  - defaults/linux-64::astropy==4.0=py36h7b6447c_0
  - defaults/linux-64::patsy==0.5.1=py36_0
  - defaults/linux-64::scikit-image==0.16.2=py36h0573a6f_0
  - defaults/linux-64::matplotlib-base==3.1.3=py36hef1b27d_0
  - defaults/linux-64::imageio==2.6.1=py36_0
  - defaults/linux-64::pytables==3.6.1=py36h71ec239_0
  - defaults/linux-64::nb_conda_kernels==2.2.4=py36_0
  - defaults/linux-64::mkl_fft==1.0.15=py36ha843d7b_0
  - defaults/linux-64::statsmodels==0.11.0=py36h7b6447c_0
  - defaults/linux-64::spyder==4.0.1=py36_0
  - defaults/noarch::seaborn==0.10.0=py_0
  - defaults/linux-64::requests==2.22.0=py36_1
  - defaults/linux-64::numba==0.48.0=py36h0573a6f_0
  - defaults/linux-64::scipy==1.4.1=py36h0b6359f_0
  - defaults/noarch::pytest-doctestplus==0.5.0=py_0
  - defaults/linux-64::mkl_random==1.1.0=py36hd6b4f25_0
  - defaults/noarch::dask==2.11.0=py_0
  - defaults/noarch::ipywidgets==7.5.1=py_0
  - defaults/linux-64::widgetsnbextension==3.5.1=py36_0
  - defaults/noarch::s3fs==0.4.2=py_0
  - defaults/linux-64::notebook==6.0.3=py36_0
  - defaults/linux-64::matplotlib==3.1.3=py36_0
  - defaults/linux-64::anaconda-client==1.7.2=py36_0
  - defaults/linux-64::numpy==1.18.1=py36h4f9e942_0
failed with repodata from current_repodata.json, will retry with next repodata source.
Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.
Collecting package metadata (repodata.json): done
Solving environment: | 
The environment is inconsistent, please check the package plan carefully
The following packages are causing the inconsistency:

  - defaults/noarch::jupyterlab==1.2.6=pyhf63ae98_0
  - defaults/linux-64::python-language-server==0.31.7=py36_0
  - defaults/linux-64::nb_conda==2.2.1=py36_0
  - defaults/noarch::numpydoc==0.9.2=py_0
  - defaults/noarch::anaconda-project==0.8.4=py_0
  - defaults/noarch::boto3==1.9.162=py_0
  - defaults/linux-64::s3transfer==0.2.1=py36_0
  - defaults/linux-64::nbconvert==5.6.1=py36_0
  - defaults/linux-64::bokeh==1.4.0=py36_0
  - defaults/noarch::jupyterlab_server==1.0.6=py_0
  - defaults/noarch::botocore==1.12.189=py_0
  - defaults/linux-64::jupyter==1.0.0=py36_7
  - defaults/linux-64::scikit-image==0.16.2=py36h0573a6f_0
  - defaults/linux-64::imageio==2.6.1=py36_0
  - defaults/linux-64::nb_conda_kernels==2.2.4=py36_0
  - defaults/linux-64::spyder==4.0.1=py36_0
  - defaults/linux-64::requests==2.22.0=py36_1
  - defaults/noarch::dask==2.11.0=py_0
  - defaults/noarch::ipywidgets==7.5.1=py_0
  - defaults/linux-64::widgetsnbextension==3.5.1=py36_0
  - defaults/noarch::s3fs==0.4.2=py_0
  - defaults/linux-64::notebook==6.0.3=py36_0
  - defaults/linux-64::anaconda-client==1.7.2=py36_0
done


==&gt; WARNING: A newer version of conda exists. &lt;==
  current version: 4.8.4
  latest version: 4.9.2

Please update conda by running

    $ conda update -n base conda



## Package Plan ##

  environment location: /home/ec2-user/anaconda3/envs/python3

  added / updated specs:
    - fsspec
    - pandas
    - s3fs


The following packages will be downloaded:

    package                    |            build
    ---------------------------|-----------------
    astroid-2.4.2              |   py36h9f0ad1d_1         297 KB  conda-forge
    certifi-2020.12.5          |   py36h5fab9bb_1         143 KB  conda-forge
    docutils-0.16              |   py36h5fab9bb_3         738 KB  conda-forge
    pandas-1.1.4               |   py36hd87012b_0        10.5 MB  conda-forge
    pillow-7.1.2               |   py36hb39fc2d_0         604 KB
    pylint-2.6.0               |   py36h9f0ad1d_1         446 KB  conda-forge
    sphinx-3.4.3               |     pyhd8ed1ab_0         1.5 MB  conda-forge
    toml-0.10.2                |     pyhd8ed1ab_0          18 KB  conda-forge
    urllib3-1.25.11            |             py_0          93 KB  conda-forge
    ------------------------------------------------------------
                                           Total:        14.3 MB

The following NEW packages will be INSTALLED:

  astroid            conda-forge/linux-64::astroid-2.4.2-py36h9f0ad1d_1
  bleach             conda-forge/noarch::bleach-3.2.1-pyh9f0ad1d_0
  brotlipy           conda-forge/linux-64::brotlipy-0.7.0-py36he6145b8_1001
  docutils           conda-forge/linux-64::docutils-0.16-py36h5fab9bb_3
  pillow             pkgs/main/linux-64::pillow-7.1.2-py36hb39fc2d_0
  pylint             conda-forge/linux-64::pylint-2.6.0-py36h9f0ad1d_1
  sphinx             conda-forge/noarch::sphinx-3.4.3-pyhd8ed1ab_0
  toml               conda-forge/noarch::toml-0.10.2-pyhd8ed1ab_0
  urllib3            conda-forge/noarch::urllib3-1.25.11-py_0

The following packages will be UPDATED:

  ca-certificates                      2020.11.8-ha878542_0 --&gt; 2020.12.5-ha878542_0
  certifi                          2020.11.8-py36h5fab9bb_0 --&gt; 2020.12.5-py36h5fab9bb_1
  fsspec                       pkgs/main::fsspec-0.6.2-py_0 --&gt; conda-forge::fsspec-0.8.5-pyhd8ed1ab_0
  pandas             pkgs/main::pandas-1.0.1-py36h0573a6f_0 --&gt; conda-forge::pandas-1.1.4-py36hd87012b_0



Downloading and Extracting Packages
pillow-7.1.2         | 604 KB    | ##################################### | 100% 
astroid-2.4.2        | 297 KB    | ##################################### | 100% 
pylint-2.6.0         | 446 KB    | ##################################### | 100% 
sphinx-3.4.3         | 1.5 MB    | ##################################### | 100% 
pandas-1.1.4         | 10.5 MB   | ##################################### | 100% 
docutils-0.16        | 738 KB    | ##################################### | 100% 
urllib3-1.25.11      | 93 KB     | ##################################### | 100% 
certifi-2020.12.5    | 143 KB    | ##################################### | 100% 
toml-0.10.2          | 18 KB     | ##################################### | 100% 
Preparing transaction: done
Verifying transaction: done
Executing transaction: done
</code></pre>
<p>Happy to take any suggestions for  how to get a python notebook up in sagemaker as quickly as possible with modern packages.</p>
<p>Other attempted solutions:</p>
<ul>
<li>a fast <code>pip install -U</code> doesn't work due to dependency issues -- the
local environment in the notebook will try to point pandas to
outdated fsspec and it will crash</li>
<li>Following AWS documentation for adding my conda requests to the startup script doesn't work because there is a timeout on the startup script (10 mins I think?) so a 15+ minute <code>conda update</code> process just ensures the sagemaker instance cannot start</li>
</ul>",1,0,2021-01-15 21:43:38.893000 UTC,2.0,,1,python|pandas|conda|amazon-sagemaker,797,2015-07-18 09:54:07.467000 UTC,2022-09-22 21:38:41.353000 UTC,,1084,163,3,104,,,,,,['amazon-sagemaker']
How do I print debugging info from Sagemaker training?,"<p>I have a jupyter notebook script that just launches a training script, presumably in a docker container.</p>

<p>I added some print statements in that training script but it's not showing up in the notebook or CloudWatch.</p>

<p>I'm using regular print() statement. How should I log debugging from the training script?</p>",4,6,2018-12-11 02:02:11.903000 UTC,2.0,2018-12-11 06:13:11.003000 UTC,10,jupyter-notebook|amazon-sagemaker,4981,2011-10-21 21:58:08.810000 UTC,2022-09-17 00:51:12.053000 UTC,,4966,744,11,304,,,,,,['amazon-sagemaker']
logging models in mlflow with a pyspark process in Kerberized HDP 3.1.5,"<p>I'm currently testing mlflow to log pyspark models in a HDP3.1.x Cluster KERBERIZED.
I've configured mlflow to use HDFS (of the same HDP cluster) for model storage.</p>
<p>Whenever I launch a pyspark process to log a model on MLFlow with &quot;spark-submit --deploy-mode=cluster ...&quot;, I've got the exception</p>
<blockquote>
<p>AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS]java.io.IOException: DestHost:destPort
namenode01.hdp.site:8020 , LocalHost:localPort
worker05.hdp.site/192.168.0.208:0. Failed on local exception:
java.io.IOException:
<strong>org.apache.hadoop.security.AccessControlException: Client cannot
authenticate via:[TOKEN, KERBEROS]</strong></p>
<p>(...)</p>
<p>Caused by: java.io.IOException:
org.apache.hadoop.security.AccessControlException: Client cannot
authenticate via:[TOKEN, KERBEROS]    at
org.apache.hadoop.ipc.Client$Connection$1.run(Client.java:758)    at
java.security.AccessController.doPrivileged(Native Method)    at
javax.security.auth.Subject.doAs(Subject.java:422)*</p>
</blockquote>
<p>It seems that libhdfs used by mlflow cannot properly authenticate with delegation tokens. Do you know any way to fix or circumvent this problem?</p>
<p>Thanks</p>",0,0,2021-10-07 14:09:48.157000 UTC,,2021-10-07 15:53:49.737000 UTC,0,apache-spark|pyspark|kerberos|mlflow|hdp,102,2021-10-07 13:53:47.870000 UTC,2022-09-23 09:02:25.037000 UTC,,1,0,0,3,,,,,,['mlflow']
InvalidMountException while loading model from mlflow-registry in databricks,"<p>I am trying to load my spark model that I have registered using mlflow in databricks ( referring  this documentation : <a href=""https://docs.microsoft.com/en-us/azure/databricks/applications/mlflow/model-registry-example#load-versions-of-the-registered-model-using-the-api"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/databricks/applications/mlflow/model-registry-example#load-versions-of-the-registered-model-using-the-api</a>)</p>
<p>My Code is :</p>
<pre class=""lang-py prettyprint-override""><code>model_uri = &quot;models:/{model_name}/{stage_name}&quot;.format(
   model_name=model_name,stage_name=stage_name)

model = mlflow.spark.load_model(model_uri )
</code></pre>
<p>But this gives me following error :</p>
<pre><code>: com.databricks.backend.daemon.data.common.InvalidMountException: 
  Error while using path /databricks/mlflow-registry/run-id/models/spark-model/sparkml 
  for resolving path '/run-id/models/spark-model/sparkml' 
  within mount at '/databricks/mlflow-registry'.
</code></pre>
<p>(Note: <code>artifact_path</code> passed by me while registering model was 'spark-model')</p>
<p>How can this error be resolved?</p>",0,1,2020-10-26 17:09:21.650000 UTC,,2020-11-01 17:49:03.517000 UTC,2,apache-spark|databricks|mlflow,324,2020-10-13 08:19:36.863000 UTC,2022-09-22 10:59:26.177000 UTC,,31,0,0,1,,,,,,['mlflow']
I am exploring the Incremental Learning feature in Amazon Sagemaker,"<p>I have access to AWS Console and started exploring the Sagemaker Incremental Learning documentation. It is mentioned that only three built-in algorithms currently support incremental training: Object Detection Algorithm, Image Classification Algorithm, and Semantic Segmentation Algorithm.</p>
<p>I want to know if we can successfully train the initial model using new incremental sample data for  other (non-built in algorithms) training images. For example, a classification or a regression.</p>",1,0,2022-08-01 12:04:46.950000 UTC,,2022-08-07 19:30:35.597000 UTC,0,amazon-web-services|amazon-s3|aws-lambda|aws-sdk|amazon-sagemaker,83,2022-04-25 08:20:14.367000 UTC,2022-09-22 11:46:27.527000 UTC,,49,3,0,8,,,,,,['amazon-sagemaker']
Data Factory: AzureSQL in- and output for pipeline activity type AzureMLBatchExecution,"<p>In Azure Data Factory, I’m trying to call an Azure Machine Learning model by a Data Factory Pipeline. I want to use a Azure SQL table as input and another Azure SQL table for the output.
First I deployed a Machine Learning (classic) web service. Then I created an Azure Data Factory Pipeline, using a LinkedService (type= ‘AzureML’, using Request URI and API key of the ML-webservice) and a input and output dataset (‘AzureSqlTable’ type).</p>

<p>Deploying and Provisioning is succeeded. The pipeline starts as scheduled, but keeps ‘Running’ without any result. The pipeline activity is not being shown in the Monitor&amp;Manage: Activity Windows.</p>

<p>On different sites and tutorials, I only find JSON-scripts using the activity type ‘AzureMLBatchExecution’ with BLOB in- and outputs. I want to use AzureSQL in- and output but I can’t get this working.</p>

<p>Can someone provide a sample JSON-script or tell me what’s possibly wrong with the code below?</p>

<p>Thanks!</p>

<pre><code>{
    ""name"": ""Predictive_ML_Pipeline"",
    ""properties"": {
        ""description"": ""use MyAzureML model"",
        ""activities"": [
            {
                ""type"": ""AzureMLBatchExecution"",
                ""typeProperties"": {},
                ""inputs"": [
                    {
                        ""name"": ""AzureSQLDataset_ML_Input""
                    }
                ],
                ""outputs"": [
                    {
                        ""name"": ""AzureSQLDataset_ML_Output""
                    }
                ],
                ""policy"": {
                    ""timeout"": ""02:00:00"",
                    ""concurrency"": 3,
                    ""executionPriorityOrder"": ""NewestFirst"",
                    ""retry"": 1
                },
                ""scheduler"": {
                    ""frequency"": ""Week"",
                    ""interval"": 1
                },
                ""name"": ""My_ML_Activity"",
                ""description"": ""prediction analysis on ML batch input"",
                ""linkedServiceName"": ""AzureMLLinkedService""
            }
        ],
        ""start"": ""2017-04-04T09:00:00Z"",
        ""end"": ""2017-04-04T18:00:00Z"",
        ""isPaused"": false,
        ""hubName"": ""myml_hub"",
        ""pipelineMode"": ""Scheduled""
    }
}
</code></pre>",1,2,2017-04-04 09:08:25.000000 UTC,1.0,2017-04-04 11:32:10.090000 UTC,2,json|machine-learning|azure-sql-database|azure-data-factory|azure-machine-learning-studio,550,2017-04-04 08:53:49.757000 UTC,2017-05-05 14:13:36.757000 UTC,,21,0,0,2,,,,,,['azure-machine-learning-studio']
What does the minimum number of nodes in an AzureML compute cluster imply?,"<p>When defining an AzureML compute cluster in the AzureML Studio there is a setting that relates to the minimum number of nodes:</p>
<blockquote>
<p>Azure Machine Learning Compute can be reused across runs. The compute
can be shared with other users in the workspace and is retained
between runs, automatically scaling nodes up or down based on the
number of runs submitted, and the max_nodes set on your cluster. The
min_nodes setting controls the minimum nodes available.</p>
</blockquote>
<p>(From <a href=""https://docs.microsoft.com/en-us/azure/machine-learning/how-to-create-attach-compute-cluster?tabs=python"" rel=""nofollow noreferrer"">here</a>.)</p>
<p>I do not understand what <em>min_nodes</em> actually is. Is it the number of nodes that the cluster will keep allocated even when idle (i.e. something one might want to speed start-up time)?</p>",1,0,2021-04-16 09:46:48.227000 UTC,,2021-04-16 14:06:14.377000 UTC,1,azure-machine-learning-studio|azure-machine-learning-service,227,2011-01-14 10:19:43.010000 UTC,2022-09-22 05:33:33.343000 UTC,"Cambridge, United Kingdom",15147,2159,27,1915,,,,,,"['azure-machine-learning-studio', 'azure-machine-learning-service']"
"sagemaker-sklearn-container 1.0 requires pandas==0.25.*, but you have pandas 1.3.5 which is incompatible","<p><strong>ERROR:</strong> pip's dependency resolver does not currently take into account all the packages that are installed. This behavior is the source of the following dependency conflicts.</p>
<p><code>sagemaker-sklearn-container 1.0 requires pandas==0.25.*,</code> but you have <code>pandas 1.3.5</code> which is incompatible.</p>
<p>I am running my notebook locally under python virtual machine , and I have <code>pandas 0.25.3</code> version but when I am training the model on sagemaker , it shows an error that amazon scikitlearn container <code>1.0 using 0.25.*</code> but I have <code>1.3.5</code>,</p>
<p>I don't understand how can I solve it , though locally I have <code>0.25.3</code></p>",1,1,2022-02-03 14:57:43.570000 UTC,,2022-02-15 06:19:02.447000 UTC,0,python|pandas|scikit-learn|dependencies|amazon-sagemaker,191,2022-02-03 14:50:49.347000 UTC,2022-09-20 14:09:13.803000 UTC,,1,0,0,0,,,,,,['amazon-sagemaker']
Packaging MultiLabelBinarizer into scikit-learn Pipeline for inference on new data,"<p>I'm building a multilabel classifier to predict labels based on a text field. For example, predicting genres based on movie title. I'd like to use <code>MultiLabelBinarizer()</code> to binarize a column containing all applicable genre labels. For example, <code>['action','comedy','drama']</code> gets split into three columns with 0/1 values. </p>

<p>The reason I'm using <code>MultiLabelBinarizer()</code> is so that I can use the built-in <code>inverse_transform()</code> function to turn the output array (e.g. <code>array([0, 0, 1, 0, 1])</code> directly into user-friendly text output (<code>['action','drama']</code>). </p>

<p>The classifier works, but I'm having issues predicting on new data. I can't find a way to integrate the <code>MultiLabelBinarizer()</code> into my Pipeline so that it can be saved and re-loaded for inference on new data. One solution is to save it as a pickle object separately and load it back each time, but I'd like to avoid having this dependency in production.</p>

<p>I know that this is similar to the tf-idf vector I've built into my Pipeline, but different in the sense that it's applied to the target column (genre labels) instead of my independent variable (the text comment). Here's my code for training the multilabel SVM:</p>

<pre><code>def svm_train(df):  
  mlb = MultiLabelBinarizer()
  y = mlb.fit_transform(df['Genres'])

  with mlflow.start_run():
    x_train, x_test, y_train, y_test = train_test_split(df['Movie Title'], y, test_size=0.3)

    # Instantiate TF-IDF Vectorizer and SVM Model
    tfidf_vect = TfidfVectorizer()
    mdl = OneVsRestClassifier(LinearSVC(loss='hinge'))
    svm_pipeline = Pipeline([('tfidf', tfidf_vect), ('clf', mdl)])

    svm_pipeline.fit(x_train, y_train)
    prediction = svm_pipeline.predict(x_test)

    report = classification_report(y_test, prediction, target_names=mlb.classes_)

    mlflow.sklearn.log_model(svm_pipeline, ""Multilabel Classifier"")
    mlflow.log_artifact(mlb, ""MLB"")

  return(report)

svm_train(df)
</code></pre>

<p>Inference consists of re-loading the saved model from MLflow (same as loading back in a pickle file) in a separate Databricks notebook and predicting using the Pipeline:</p>

<pre><code>def predict_labels(new_data):
  model_uri = '...MLflow path...'
  model = mlflow.sklearn.load_model(model_uri)
  predictions = model.predict(new_data)
  # If I can't package the MultiLabelBinarizer() into the Pipeline, this 
  # is where I'd have to load the pickle object mlb
  # so that I can inverse_transform()
  return mlb.inverse_transform(predictions)

new_data = ['Some movie title']
predict_labels(new_data)

['action','comedy']
</code></pre>

<p>Here's all of the libraries I'm using:</p>

<pre><code>import pandas as pd
import numpy as np
import mlflow
import mlflow.sklearn
import glob, os
from pyspark.sql import DataFrame
from sklearn.pipeline import Pipeline
from sklearn import preprocessing
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.multiclass import OneVsRestClassifier
from sklearn import svm
from sklearn.svm import LinearSVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score, precision_score, recall_score
</code></pre>",1,1,2019-09-13 13:52:16.800000 UTC,1.0,,3,python|pandas|machine-learning|scikit-learn|mlflow,1137,2010-10-07 00:08:50.503000 UTC,2020-06-08 16:56:03.537000 UTC,,75,0,0,11,,,,,,['mlflow']
How to Connect ADLS Gen-1 with Azure ML Studio,"<p>Want to connect ADLS Gen-1 with AzureML Studio.</p>

<p>I try to find out some solution but could not get</p>",1,0,2019-06-14 12:50:31.910000 UTC,,,0,azure-data-lake|azure-machine-learning-studio,194,2017-05-30 03:43:03.657000 UTC,2022-09-22 10:03:13.390000 UTC,"Gurgaon, Haryana, India",295,0,0,102,,,,,,['azure-machine-learning-studio']
What is the difference between sagemaker-pytorch-training-toolkit and sagemaker-training-toolkit in SageMaker?,"<p>When porting PyTorch code / models to SageMaker, which one should we use:</p>
<p><strong>PyTorch Training Toolkit</strong> (<code>https://github.com/aws/sagemaker-pytorch-training-toolkit/</code>) or
<strong>SageMaker Training Toolkit</strong> (<code>https://github.com/aws/sagemaker-training-toolkit</code>)? What's the difference when using these toolkits?</p>",2,0,2022-09-12 20:03:07.273000 UTC,,,0,amazon-web-services|pytorch|amazon-sagemaker,23,2022-09-08 15:07:33.073000 UTC,2022-09-12 19:57:11.407000 UTC,,3,0,0,1,,,,,,['amazon-sagemaker']
How to integrate CEPH with Amazon-S3?,"<p>I'm trying to adapt the open-source project <a href=""https://github.com/open-mmlab/mmfashion"" rel=""nofollow noreferrer"">mmfashion</a> on Amazon SageMaker that requires the <a href=""https://ceph.io/"" rel=""nofollow noreferrer"">CEPH</a> module for backend. Unfortunately <code>pip install ceph</code> doesn't work. The only work-around was to build the <a href=""https://github.com/ceph/ceph"" rel=""nofollow noreferrer"">ceph source-code</a> manually by running in my container:</p>
<pre><code>!git clone git://github.com/ceph/ceph 
!git submodule update --init --recursive
</code></pre>
<p>This does allow me to import <code>ceph</code> successfully. But it throws the following error when it comes to fecthing data from Amazon S3:</p>
<pre><code>AttributeError: module 'ceph' has no attribute 'S3Client'
</code></pre>
<p>Has someone integrated <a href=""https://ceph.io/"" rel=""nofollow noreferrer"">CEPH</a> with Amazon S3 Bucket or has suggestions in the same line on how to tackle this?</p>",1,0,2021-02-14 21:45:37.467000 UTC,,,0,amazon-s3|amazon-sagemaker|ceph|cephfs|cephadm,197,2013-06-26 10:56:55.017000 UTC,2021-11-08 07:29:55.553000 UTC,"Kolkata, India",1,0,0,10,,,,,,['amazon-sagemaker']
Pyspark: How to save and apply IndexToString to convert labels back to original values in a new predicted dataset,"<p>I am using pyspark.ml.RandomForestClassifier and one of the steps here involves <strong>StringIndexer</strong> on the training data target variable to convert it into labels.</p>
<pre><code>indexer = StringIndexer(inputCol = target_variable_name, outputCol = 'label').fit(df)
df = indexer.transform(df)
</code></pre>
<p>After fitting the final model I am saving it using mlflow.spark.log_model(). So, when applying the model on a new dataset in future, I just load the model again and apply to the new data:</p>
<pre><code>model = mlflow.sklearn.load_model(&quot;models:/RandomForest_model/None&quot;)
predictions = rfModel.transform(new_data)
</code></pre>
<p>In the new_data the prediction will come as <strong>labels</strong> and not in original value. So, if I have to get the original values I have to use <strong>IndexToString</strong></p>
<pre><code>labelConverter = IndexToString(inputCol=&quot;prediction&quot;, outputCol=&quot;predictedLabel&quot;,labels=indexer.labels)
predictions = labelConverter.transform(predictions)
</code></pre>
<p>So, the question is, my model doesn't save the <strong>indexer.labels</strong> as only the model gets saved. How do, I save and use the indexer.labels from my training dataset on any new dataset. Can this be saved and retrived in mlflow ?</p>
<p>Apologies, if Iam sounding naïve here . But, getting back the original values in the new dataset is really getting me confused.</p>",0,0,2021-10-07 16:29:57.133000 UTC,,2021-10-08 07:06:50.887000 UTC,1,pyspark|databricks|random-forest|apache-spark-mllib|mlflow,115,2017-07-27 12:59:26.927000 UTC,2022-09-21 07:04:54.843000 UTC,,459,100,0,61,,,,,,['mlflow']
"When using a classification api for a AzureML model, how do I get a response indicating the percentage chance of each class","<p>When I query the endpoint that Azure provides, it just gives me the raw outcome. For instance, if the classifications were 'True' or 'False', the api would just return 'True' or 'False' instead of '23% True, 77% False', which is what I want.</p>",0,2,2020-05-04 16:28:12.070000 UTC,,,0,azure|azure-machine-learning-service,13,2019-09-16 22:50:39.720000 UTC,2020-05-06 15:23:03.937000 UTC,,1,0,0,3,,,,,,['azure-machine-learning-service']
Issue Registering Keras model / mlflow.artifacts,"<p>I'm first registering a keras model using <code>mlflow.keras.log_model()</code>.</p>
<p>Once it is registered, I get <code>code : null</code> in the description of the MLModel in Mlflow ui.</p>
<p>It seems like this prevents me from using it later in pyfunc that uses this model, as I have tried to do it for already deployed model and the only difference is this <code>code : null</code> thing, that prevents from deploying.</p>
<p>When trying to use the pyfunc, last error is <code>ModuleNotFoundError: No module named 'mlflow.artifacts'</code></p>
<pre><code> [ERROR] Exception in worker process
Traceback (most recent call last):
  File &quot;/opt/conda/lib/python3.6/site-packages/gunicorn/arbiter.py&quot;, line 583, in spawn_worker
    worker.init_process()
  File &quot;/opt/conda/lib/python3.6/site-packages/gunicorn/workers/base.py&quot;, line 119, in init_process
    self.load_wsgi()
  File &quot;/opt/conda/lib/python3.6/site-packages/gunicorn/workers/base.py&quot;, line 144, in load_wsgi
    self.wsgi = self.app.wsgi()
  File &quot;/opt/conda/lib/python3.6/site-packages/gunicorn/app/base.py&quot;, line 67, in wsgi
    self.callable = self.load()
  File &quot;/opt/conda/lib/python3.6/site-packages/gunicorn/app/wsgiapp.py&quot;, line 49, in load
    return self.load_wsgiapp()
  File &quot;/opt/conda/lib/python3.6/site-packages/gunicorn/app/wsgiapp.py&quot;, line 39, in load_wsgiapp
    return util.import_app(self.app_uri)
  File &quot;/opt/conda/lib/python3.6/site-packages/gunicorn/util.py&quot;, line 358, in import_app
    mod = importlib.import_module(module)
  File &quot;/opt/conda/lib/python3.6/importlib/__init__.py&quot;, line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 994, in _gcd_import
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 971, in _find_and_load
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 955, in _find_and_load_unlocked
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 665, in _load_unlocked
  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 678, in exec_module
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed
  File &quot;/opt/conda/lib/python3.6/site-packages/mlflow/pyfunc/scoring_server/wsgi.py&quot;, line 6, in &lt;module&gt;
    app = scoring_server.init(load_model(os.environ[scoring_server._SERVER_MODEL_PATH]))
  File &quot;/opt/conda/lib/python3.6/site-packages/mlflow/pyfunc/__init__.py&quot;, line 483, in load_model
    model_impl = importlib.import_module(conf[MAIN])._load_pyfunc(data_path)
  File &quot;/opt/conda/lib/python3.6/site-packages/mlflow/pyfunc/model.py&quot;, line 223, in _load_pyfunc
    python_model = cloudpickle.load(f)
  File &quot;/opt/conda/lib/python3.6/site-packages/cloudpickle/cloudpickle.py&quot;, line 562, in subimport
    __import__(name)
ModuleNotFoundError: No module named 'mlflow.artifacts'
[2022-07-26 10:43:32 +0000] [60] [INFO] Worker exiting (pid: 60)
[2022-07-26 10:43:32 +0000] [57] [INFO] Shutting down: Master
[2022-07-26 10:43:32 +0000] [57] [INFO] Reason: Worker failed to boot.
</code></pre>
<p>Any idea making this work ?</p>",0,0,2022-07-26 10:59:51.097000 UTC,,2022-07-27 11:38:46.737000 UTC,0,deployment|mlflow,43,2022-07-26 10:50:00.027000 UTC,2022-09-22 08:57:08.513000 UTC,,1,0,0,0,,,,,,['mlflow']
How to have my customized score file deployed on Azure with azure.mlflow sdk?,"<p>I have a customized score.py file which was generated within databricks but I didn't find a way to deploy it on a container.</p>

<p>I am using the mlflow.azureml, on my image creation I couldn't find how to specify the score.py in particular.</p>

<pre class=""lang-py prettyprint-override""><code>import mlflow.azureml

model_image, azure_model = mlflow.azureml.build_image(model_uri=model_uri, 
                                                      workspace=workspace,
                                                      model_name=""my_model"",
                                                      image_name=""image_name"",
                                                      description=""Predicts"",
                                                      synchronous=False)
</code></pre>

<p>Is there a way to specify the score.py using the lib?</p>",1,0,2019-09-22 15:10:44.150000 UTC,,,0,azure-machine-learning-service|mlflow,226,2014-04-03 02:59:11.190000 UTC,2021-03-13 21:24:17.030000 UTC,,85,1,0,3,,,,,,"['mlflow', 'azure-machine-learning-service']"
Snowflake auth issues using AWS SageMaker notebooks,"<p>We're using Python 3.x locally to connect to Snowflake, using the &quot;externalbrowser&quot; authentication method. This works quite nicely running code locally, where it automatically opens another browser tab and finishes authenticating that way.</p>
<pre><code>from sqlalchemy import create_engine
from snowflake.sqlalchemy import URL

engine = create_engine(URL(
account=&quot;MY_ACCOUNT&quot;,
user=&quot;MY_USERNAME&quot;,
authenticator=&quot;externalbrowser&quot;,
role=&quot;MY_ROLE&quot;,
warehouse=&quot;MY_WAREHOUSE&quot;
))
connection = engine.connect()
</code></pre>
<p>However running the same code from within an AWS SageMaker (Juypter) notebooks throws an error since it has no browser tab to open to complete the authentication. The error being returned is,</p>
<pre><code>DatabaseError: (snowflake.connector.errors.DatabaseError) 250008 (08001): Failed to connect to DB: &lt;MY_ACCOUNT&gt;.snowflakecomputing.com:443, Unable to open a browser in this environment.
(Background on this error at: https://sqlalche.me/e/14/4xp6)
</code></pre>
<p>We don't have any Okta authentication enabled or any other SAML (or OAuth) authentication options we can use instead. Is there any other way around this issue with SageMaker notebooks and externalbrowser authentication?</p>",1,0,2022-01-21 15:14:05.010000 UTC,,,1,python|authentication|jupyter-notebook|snowflake-cloud-data-platform|amazon-sagemaker,433,2022-01-21 15:02:40.657000 UTC,2022-07-21 09:10:49.827000 UTC,,11,0,0,0,,,,,,['amazon-sagemaker']
Unable to run experiment on Azure ML Studio after copying from different workspace,"<p>My simple experiment reads from an Azure Storage Table, Selects a few columns and writes to another Azure Storage Table. This experiment runs fine on the Workspace (Let's call it workspace1).</p>

<p>Now I need to move this experiment as is to another workspace(Call it WorkSpace2) using Powershell and need to be able to run the experiment. 
I am currently using this Library - <a href=""https://github.com/hning86/azuremlps"" rel=""nofollow noreferrer"">https://github.com/hning86/azuremlps</a> </p>

<p>Problem :</p>

<p>When I Copy the experiment using 'Copy-AmlExperiment' from WorkSpace 1 to WorkSpace 2, the experiment and all it's properties get copied except the Azure Table Account Key. 
Now, this experiment runs fine if I manually enter the account Key for the Import/Export Modules on studio.azureml.net</p>

<p>But I am unable to perform this via powershell. If I Export(Export-AmlExperimentGraph) the copied experiment from WorkSpace2 as a JSON and insert the AccountKey into the JSON file and Import(Import-AmlExperiment) it into WorkSpace 2. The experiment fails to run. </p>

<p>On PowerShell I get an ""Internal Server Error : 500"".</p>

<p>While running on studio.azureml.net, I get the notification as ""Your experiment cannot be run because it has been updated in another session. Please re-open this experiment to see the latest version.""</p>

<p>Is there anyway to move an experiment with external dependencies to another workspace and run it?</p>

<p>Edit : I think the problem is something to do with how the experiment handles the AccountKey. When I enter it manually, it's converted into a JSON array comprising of RecordKey and IndexInRecord. But when I upload the JSON experiment with the accountKey, it continues to remain the same and does not get resolved into RecordKey and IndexInRecord.</p>",3,0,2017-08-22 11:07:03.133000 UTC,,2017-08-22 11:17:50.630000 UTC,1,powershell|azure|machine-learning|azure-powershell|azure-machine-learning-studio,613,2017-08-09 12:10:34.107000 UTC,2018-07-24 08:17:17.513000 UTC,,33,0,0,16,,,,,,['azure-machine-learning-studio']
Copying Experiments from a MLFlow server to another MLFlow server,"<p>I have a user in a Linux machine and I run a mlflow server from this user. Artifacts are stored in local mlruns folder. Lets call this user as user A. Then I run another mlflow server from another Linux user and call this user as user B. I wanted to move older experiments that resides in mlruns directory of user A to mlflow that run in user B. I simply moved mlruns directory of user A to the home directory of user B and run mlflow from there again. When I accessed to mlflow UI by browser I saw that artifact location is configured correctly to mlruns folder of user B, but I couldn't see the experiments that moved from user A's mlruns directory. How can I see them in the UI too?</p>",2,0,2021-08-02 13:14:04.223000 UTC,,,1,mlflow,303,2018-08-02 10:29:26.067000 UTC,2022-09-14 12:43:17.120000 UTC,"İstanbul, Türkiye",275,167,2,85,,,,,,['mlflow']
How to download an artifact from MLFlow using REST?,"<p>I see the Python API:
<code>download_artifacts(run_id: str, path: str, dst_path: Optional[str] = None) → str</code> (<a href=""https://www.mlflow.org/docs/latest/python_api/mlflow.tracking.html#mlflow.tracking.MlflowClient.download_artifacts"" rel=""nofollow noreferrer"">here</a>), but I can't find the equivalent in REST.</p>",0,0,2021-11-08 11:13:52.513000 UTC,1.0,2021-11-09 10:44:51.217000 UTC,3,rest|databricks|mlflow,243,2009-06-14 12:54:00.077000 UTC,2022-09-23 21:20:51.750000 UTC,"New York, NY",13408,306,12,687,,,,,,['mlflow']
Can you recover IAM role?,"<p>I have accidentally delete iam role for the amazon sage maker studio, now i can't open it? Can i make another role and assign to it or can it be recovered?</p>
<p>I can't open sagemaker studio now.</p>",1,0,2021-07-28 12:25:26.573000 UTC,,2021-07-28 14:28:49.927000 UTC,0,amazon-web-services|amazon-sagemaker,65,2016-02-03 20:04:57.173000 UTC,2022-09-14 10:30:38.323000 UTC,,1,0,0,5,,,,,,['amazon-sagemaker']
Sagemaker endpoint with tensorflow container ignoring the inference.py file,"<p>I'm using a tensorflow model which is saved as follows:</p>
<pre><code>tf-model
    00000123
        assets
        variables
            variables.data-00000-of-00001
            variables.index
        keras_metadata.pb
        saved_model.pb
</code></pre>
<p>The tf model is getting picked up and it's working as an endpoint, and when I associate a predictor object inside of sagemaker with the endpoint and run it it returns what I expect.</p>
<p>However, I want to inference it with a POST json and I want a POST json back, the same as with sklearn or xgb or pytorch endpoints.</p>
<p>I tried to implement this on the inference.py which I'm passing as an entry point, but no matter what I try, the endpoint just seems to ignore the inference.py script.</p>
<p>I use the script almost exactly as is given at the end of this page: <a href=""https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/using_tf.html"" rel=""nofollow noreferrer"">https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/using_tf.html</a></p>
<p>I've tried both versions (input / output handler + just handler), I've tried leaving it in the sagemaker environment, I've tried packaging it up with the tar.gz file, I've put it in an s3 bucket and set up pointers and environment variables (via tensorflowmodel kwarg) to it, but no matter what I try it just ignores the inference.py.</p>
<p>I know it ignores it because I have made minor edits on the application/json part of the input handler and none of these edits show up, even when I change it so it only takes text/csv or something else the changes do not reflect.</p>
<p>What am I doing wrong? How can I get the tensorflow serverless environment to return a POST of the output instead of saving the output to s3 as per it's default behaviour?</p>",1,0,2022-05-11 21:53:21.950000 UTC,1.0,,0,python|tensorflow|amazon-sagemaker|endpoint,262,2019-05-20 00:02:06.777000 UTC,2022-08-25 09:34:49.220000 UTC,"London, UK",21,1,0,3,,,,,,['amazon-sagemaker']
Is it possible to trigger Sagemaker notebook from AWS Lambda function?,"<p>I am trying to trigger Sagemaker notebook or Sagemaker Studio notebook from AWS Lambda when data is available in S3 bucket. I want to know if this is possible and if yes, how?</p>
<p>All I want is once data is uploaded in S3, the lambda function should be able to spin up the Sagemaker notebook with a standard CPU cluster.</p>",1,0,2022-03-08 01:35:17.017000 UTC,,,0,amazon-web-services|aws-lambda|amazon-sagemaker,621,2020-06-29 19:21:19.090000 UTC,2022-09-14 18:02:24.907000 UTC,,82,14,0,8,,,,,,['amazon-sagemaker']
Partition by Rows equivalent in pandas (python,"<p>I am using Azure Machine Learning Studio and what to add a running total on my dataset. This includes a date column, and I want to sum all the rows (for a group) on or before the row date.</p>

<p>In SQL Server, I would use:</p>

<pre><code>    SELECT [t1].*,
SUM([t1].[Amount (Settlement CCY)) 
OVER (
  PARTITION BY [t1].[Contract Ref], [t1].[LOBCode], [t1].[Superline], [t1].[Occupation], [t1].[TransType], [t1].[SettCCY]
  ORDER BY     [t1].[Transaction Date] ASC
  ROWS BETWEEN UNBOUNDED PRECEDING
       AND     CURRENT ROW
)
FROM [t1]
GROUP BY [t1].[contract ref], [t1].[Transaction date], [t1].[LOBCode], [t1].[Superline], [t1].[Occupation], [t1].[TransType], [t1].[SettCCY]
</code></pre>

<p>but Azure Machine learning uses SQLite where the Over / Partition clauses aren't implemented.</p>

<p>I've tried an alternative in python/pandas:</p>

<pre><code>dataframe1 = dataframe1.assign(cumAMTscTD=dataframe1.groupby(['ContractRef', 'Basis', 'LOBCode', 'Superline', 'Occupation', 'TransType', 'SettCCY'])['AmtSettCCY'].transform('sum')).sort_values(['ContractRef','TransDate'])
</code></pre>

<p>but this sums up everything for the group, not just the those for the dates up toe current row. I assume therefore it doesn't cover the:</p>

<pre><code>ROWS BETWEEN UNBOUNDED PRECEDING
   AND     CURRENT ROW
</code></pre>

<p>How would I acheive this?</p>",1,2,2018-06-07 11:04:06.120000 UTC,0.0,2018-06-07 11:14:02.660000 UTC,0,python|sql|pandas|sqlite|azure-machine-learning-studio,744,2014-02-21 14:15:06.703000 UTC,2019-03-13 09:20:53.350000 UTC,,167,15,0,31,,,,,,['azure-machine-learning-studio']
How do I use a pickled model to deploy my Machine Learning model using AWS-Sagemaker after preprocessing the input,"<p>I have a pickled model (say an XGBoost Model - xgboost_model.sav).
I want to be able to get a json input (through an API invocation), perform some pre-processing (like missing value imputation, outlier treatment etc.), use the pickled file to return the predicted the outcome of the model(say a float denoting probability).
I want to create a sagemaker endpoint which can then be used to get the prediction(a probability) through a lambda function. (using boto3)</p>

<p>I have gone through the example notebooks provided my AWS Sagemaker but am not able to figure out how to execute the above steps and deploy my model.</p>

<p>Please help me with a step by step instruction possibly with some basic code.</p>",1,3,2019-06-17 06:06:31.330000 UTC,,,1,amazon-web-services|aws-lambda|amazon-sagemaker,1179,2019-06-07 06:20:43.373000 UTC,2022-01-04 14:50:13.933000 UTC,India,94,131,0,11,,,,,,['amazon-sagemaker']
Can you run Sagemaker Python SDK on Windows?,"<p>The <a href=""https://github.com/aws/sagemaker-python-sdk"" rel=""nofollow noreferrer"">documentation</a> says that sagemaker-python-sdk is only supported by Unix/Mac OS.</p>
<p>Is there a possibility to run sagemaker python libraries in Python.</p>
<p>I actually installed sagemaker-scikit-learn-extension on my windows Python environment however there is an error due to the library mlio missing(I installed it).</p>
<p>I assume this is due to me running Windows, but I don't know. Does anybody have any ideas if this libraries are runnable on Windows?</p>",0,2,2020-11-30 13:35:25.433000 UTC,,,0,python|amazon-web-services|amazon-sagemaker,73,2011-07-16 13:02:36.880000 UTC,2022-09-24 20:19:39.590000 UTC,Slovenia,14913,307,1,1093,,,,,,['amazon-sagemaker']
How to download folder from AzureML notebook folder to local or blob storage?,"<p>I saved file to the same directory using (./folder_name) when I use AzureML jupyter. Now how can I download to my local machine or blob storage?</p>
<p>The folder have a lot of files and sub-directory in it, which I scraped online. So it is not realistic to save one by one.</p>
<pre><code>file_path = &quot;./&quot;

for i in target_key_word:
    tem_str = i.replace(' ', '+')
    dir_name = file_path + i
    if not os.path.exists(dir_name):
        os.mkdir(dir_name)
    else:    
        print(&quot;Directory &quot; , dir_name ,  &quot; already exists&quot;)
</code></pre>",4,0,2020-11-10 16:10:35.707000 UTC,,2020-11-10 17:49:01.670000 UTC,1,python|azure|azure-storage|azure-blob-storage|azure-machine-learning-service,3310,2020-05-15 02:09:39.330000 UTC,2022-09-23 20:47:43.383000 UTC,,11,0,0,5,,,,,,['azure-machine-learning-service']
Package or namespace load failed for 'tcltk',"<p>I am trying to include the ""tcltk"" library under a R script in Azure Machine Learning Studio (R version 3.1.0). But I am always getting this error:</p>

<blockquote>
  <p>package or namespace load failed for 'tcltk'</p>
</blockquote>

<p>Any ideas please.</p>

<p>Thanks in advance.</p>

<p>Error message:</p>

<pre><code>Error 0063: The following error occurred during evaluation of R script:
---------- Start of error message from R ----------
package or namespace load failed for 'tcltk'

package or namespace load failed for 'tcltk'
----------- End of error message from R -----------
Start time: UTC 06/24/2019 11:27:02
End time: UTC 06/24/2019 11:27:08
</code></pre>",1,0,2019-06-24 11:27:55.537000 UTC,,2019-06-24 12:30:54.747000 UTC,1,azure-machine-learning-studio,126,2019-06-24 11:22:37.050000 UTC,2022-06-24 12:56:26.497000 UTC,,11,0,0,1,,,,,,['azure-machine-learning-studio']
Increasing processing power of Azure Machine Learning workspace,"<p>Is there a way to increase the processing power of the Azure ML? I've deployed a neural network on a huge dataset (8000+ retina images, and Azure is taking an impossible amount of time to run the programme. Is it possible to deploy the ML workspace from a Virtual Machine, so that I can leverage increased processing speeds? Help!!</p>",1,0,2016-10-14 07:04:17.303000 UTC,1.0,2017-11-08 21:19:17.023000 UTC,2,image-processing|virtual-machine|azure-machine-learning-studio|azure-dsvm,706,2016-10-14 06:55:13.303000 UTC,2016-11-15 08:02:55.893000 UTC,,21,0,0,2,,,,,,['azure-machine-learning-studio']
Loading trained model in to SageMaker Estimator,"<p>I've trained a custom model on sagemaker based on PyTorch estimator.<br />
Training has been completed, and I verified that the model artifacts have been saved into s3 location.</p>
<p>I want to load my trained model into my sagemaker notebooks so I can perform analysis/inference so on ...</p>
<p>I did as below but I am not sure if this is the right method to do this as it asks for instance type, and to my knowledge, If I were to load the already trained estimator, I would need to declare which type of computing instance I use once I start deploying the model for inference.</p>
<pre><code>estimator = PyTorch(
        model_data = ModelArtifact_S3_LOCATION,
        entry_point ='train.py',
        source_dir = 'code',
        role = role,
        framework_version = '1.5.0',
        py_version = 'py3',)
</code></pre>",1,0,2021-07-15 03:24:19.903000 UTC,,,0,amazon-web-services|amazon-sagemaker,628,2020-09-21 20:00:48.277000 UTC,2022-09-16 06:59:29.497000 UTC,"Seoul, South Korea",69,8,0,4,,,,,,['amazon-sagemaker']
How to set the number of cosmoDB item processed in micro-batch in Spark Structured streaming?,"<p>Basically, I'm using spark structured streaming to read sensor data (24 sensors with frequency 1s) from cosmo, doing some manip and calling a MLFlow classification model.</p>
<p>Thus, I need a batch of 24 input items (or a modulo of 24).</p>
<p>My code look like this so far :</p>
<pre><code>  &quot;spark.cosmos.accountEndpoint&quot; : cosmosEndpoint,
  &quot;spark.cosmos.accountKey&quot; : cosmosMasterKey,
  &quot;spark.cosmos.database&quot; : cosmosDatabaseName,
  &quot;spark.cosmos.container&quot; : cosmosContainerName,
  &quot;spark.cosmos.upsert&quot; : &quot;true&quot;
}

# Configure Catalog Api to be used
spark.conf.set(&quot;spark.sql.catalog.cosmosCatalog&quot;, &quot;com.azure.cosmos.spark.CosmosCatalog&quot;)
spark.conf.set(&quot;spark.sql.catalog.cosmosCatalog.spark.cosmos.accountEndpoint&quot;, cosmosEndpoint)
spark.conf.set(&quot;spark.sql.catalog.cosmosCatalog.spark.cosmos.accountKey&quot;, cosmosMasterKey)

# Initiate Cosmos Connection Config Object
changeFeedCfg = {
  &quot;spark.cosmos.accountEndpoint&quot;: cosmosEndpoint,
  &quot;spark.cosmos.accountKey&quot;: cosmosMasterKey,
  &quot;spark.cosmos.database&quot;: cosmosDatabaseName,
  &quot;spark.cosmos.container&quot;: cosmosContainerName,
  &quot;spark.cosmos.read.partitioning.strategy&quot;: &quot;Default&quot;,
  &quot;spark.cosmos.read.inferSchema.enabled&quot; : &quot;false&quot;,
  &quot;spark.cosmos.changeFeed.startFrom&quot; : &quot;Now&quot;,
  &quot;spark.cosmos.changeFeed.mode&quot; : &quot;Incremental&quot;,
  &quot;spark.cosmos.changeFeed.ItemCountPerTriggerHint&quot; : 24,
}

# Load model as a PysparkUDF
loaded_model = mlflow.pyfunc.spark_udf(spark, model_uri='runs:/*********/model', result_type='double')
literal_eval_udf = udf(ast.literal_eval, MapType(StringType(), StringType()))

fixedStream = spark.readStream.format(&quot;cosmos.oltp.changeFeed&quot;).options(**changeFeedCfg).load()

fixedStream = fixedStream.select('_rawBody').withColumn('temp', regexp_replace('_rawBody', ',&quot;_rid&quot;.*', '}')).drop('_rawBody')
fixedStream = fixedStream.withColumn(&quot;temp&quot;, map_values(literal_eval_udf(col(&quot;temp&quot;))))
keys = ['datetime', 'machine', 'id', 'factor', 'value', 'Sensor']
for k in range(len(keys)):
    fixedStream = fixedStream.withColumn(keys[k], fixedStream.temp[k])
fixedStream = fixedStream.select('factor','machine','Sensor','value')

def foreach_batch_function(df, epoch_id):
    df = df.groupBy('factor','machine').pivot(&quot;Sensor&quot;).agg(first(&quot;value&quot;))
    columns = list(df)
    df = df.withColumn('predictions', loaded_model(*columns)).collect()
    df.write.option(&quot;mergeSchema&quot;,&quot;true&quot;).format(&quot;delta&quot;).option(&quot;header&quot;, &quot;true&quot;).mode(&quot;append&quot;).saveAsTable(&quot;poc_industry.test_stream&quot;)
    
fixedStream.writeStream.foreachBatch(foreach_batch_function).start()
</code></pre>
<p>I have tried using with the read stream:</p>
<ul>
<li>ItemCountPerTriggerHint,</li>
<li>limit</li>
<li>maxItemCount</li>
</ul>
<p>I also tried slowing down the write with the trigger(processingTime='x seconds') option.</p>
<p>It runs without error, But none of it seems to have an effect on the batchDF size, indeed numInputRows seems to be varying randomly between (3 and 100).</p>
<p>As anyone achieve something like this?</p>",1,0,2022-07-14 15:19:11.507000 UTC,,,0,pyspark|azure-databricks|spark-structured-streaming|azure-cosmosdb-sqlapi|mlflow,37,2018-02-02 12:31:42.710000 UTC,2022-09-23 20:18:41.917000 UTC,,31,31,0,9,,,,,,['mlflow']
How to create a setuptools sdist .tar.gz package from a Conda project?,"<p>I have a PyTorch training project where dependencies are managed using <a href=""https://conda.io"" rel=""nofollow noreferrer"">conda</a>. I need to package my project as a <code>setuptools</code> <a href=""https://docs.python.org/3/distutils/sourcedist.html"" rel=""nofollow noreferrer"">software distribution</a> (sdist) in a <code>.tar.gz</code> file so that I can run it as a custom job in Google Cloud Platform's Vertex.AI Training service.</p>
<p>I've found an official guide that explains <a href=""https://docs.conda.io/projects/conda-build/en/latest/user-guide/recipes/build-without-recipe.html"" rel=""nofollow noreferrer"">how to generate a bdist package using Conda</a> but I don't think I can use that in Vertex.AI.</p>
<p><strong>Is there a Conda command for generating an sdist distribution? Or are there any other tools I can use to get this done automatically?</strong></p>
<p><em>Please note:</em> I know I can manually recreate the list of dependencies in my setuptools <code>setup.py</code> file but I would rather not do that because manually maintaining two copies of the same list could link to sync issues and errors.</p>",1,0,2022-03-14 09:43:02.287000 UTC,,2022-03-15 07:25:47.437000 UTC,1,python|conda|setuptools|google-cloud-vertex-ai|sdist,88,2008-11-02 09:34:07.787000 UTC,2022-09-21 09:19:31.780000 UTC,"Tel Aviv, Israel",15116,890,56,1182,,,,,,['google-cloud-vertex-ai']
What are the Azure ML output formats?,"<p>Does Azure ML only provide output through it's web services? 
Is it possible to feed the output to an Azure SQL database? 
Is it possible to feed the output to a Redshift database?</p>

<p>Essentially I am looking to know if I can integrate Azure ML Studio with our existing redshift analytics database.</p>",2,0,2017-01-11 04:16:07.760000 UTC,,,0,azure|amazon-redshift|forecasting|azure-machine-learning-studio,53,2015-11-17 06:37:34.387000 UTC,2022-08-17 22:04:19.137000 UTC,,182,160,3,33,,,,,,['azure-machine-learning-studio']
How send multiple request to AWS Sagemaker Endpoint on single invoke?,"<p>I've deployed a Deep learning model on SageMaker endpoint and can request/get answer using <code>sagemaker_client.invoke_endpoint</code>. But each <code>invoke_endpoint</code> accepts single body. How can I send multiple body to get multiple result on single request?</p>

<p>I've tried setting <code>body='{""instances"": [myData1, myData2]}'</code> but It recognizes as single string.</p>

<pre><code>def sagemaker_handler(doc):
    data = doc.encode(""UTF-8"")
    response = sagemaker_client.invoke_endpoint(EndpointName='myEndpoint',
                                                ContentType='application/json',
                                                Accept='application/json', Body=data)
return response
</code></pre>",2,2,2019-01-17 10:26:48.330000 UTC,,,1,python-3.x|amazon-web-services|amazon-sagemaker,4534,2014-05-22 19:34:18.190000 UTC,2022-07-18 18:31:30.377000 UTC,"Dhaka, Bangladesh",46,4,0,17,,,,,,['amazon-sagemaker']
How to create debug log in Amazon S3 bucket while running a code in Amazon Sagemaker,<p>I am having a very big loop. I wanted to run it over the Amazon Sage maker. For this I need to create debug log in Amazon S3 bucket. How to do it?</p>,1,1,2021-11-27 17:09:59.450000 UTC,,,1,debugging|amazon-s3|amazon-sagemaker,144,2020-04-24 10:34:21.987000 UTC,2022-08-13 06:30:57.293000 UTC,,95,39,0,30,,,,,,['amazon-sagemaker']
Is there a way to list out my datastores if I've deployed to a VNET?,"<p>I followed the instructions in the <a href=""https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-enable-virtual-network"" rel=""nofollow noreferrer"">MSFT Docs</a>, but now I can't list our my Datastores either via the SDK nor the Azure Machine Learning studio. </p>

<p>Instead, in the studio I see this:
<a href=""https://i.stack.imgur.com/im2oe.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/im2oe.png"" alt=""enter image description here""></a></p>

<p>Is there a way to make this work? Did I miss a step?</p>",2,0,2019-11-07 16:45:27.473000 UTC,,,0,azure-machine-learning-service,164,2018-06-19 20:17:41.717000 UTC,2022-09-21 12:31:28.057000 UTC,,392,8,4,39,,,,,,['azure-machine-learning-service']
how to create AzureML environment from a pip requirements file,<p>I am very comfortable with creating environment from a requirements file using pip. Could i create the same type of environment from requirements file for AzureML experiment?</p>,1,0,2020-12-05 16:41:49.167000 UTC,,,-1,python|azure|pip|azure-machine-learning-service|azureml-python-sdk,188,2017-01-19 06:38:11.853000 UTC,2022-09-20 02:56:14.233000 UTC,"Gurugram, Haryana, India",623,29,1,54,,,,,,['azure-machine-learning-service']
Customising model in AWS sagemaker,<p>I have a python script which I wrote using tensorflow python 3.6 AWS sagemaker jupyter notebook inside AWS sagemaker instance. I have to use sagemaker debugger for my Deep Learning model. I can see many links suggesting that first dockerise the algorithm image and then use it over sagemaker. Can anyone please suggest that is there any available alternative such that Tensorflow-1 docker image is available and I can include some other packages via pip in this image and then run my model on sagemaker ? I am using keras 2.3.0 with tensorflow 1.15 .Please guide and share necessary references.</p>,1,0,2020-10-22 07:16:02.160000 UTC,,2020-10-22 08:49:45.230000 UTC,0,python-3.x|docker|amazon-sagemaker|keras-2|tensorflow1.15,181,2016-09-14 05:43:44.143000 UTC,2022-08-16 12:07:05.640000 UTC,,348,24,0,64,,,,,,['amazon-sagemaker']
load csv and set parameters in jupyter notebook on Azure ML,"<p>I'm using a Python 3.4 Jupyter notebook to load a dataset in Azure ML which is stored in the cloud as a dataset in the Azure ML project environment. But using the default template created by Azure ML, I can't load the data due to a mixed datatypes error. </p>

<pre><code>from azureml import Workspace
import pandas as pd

ws = Workspace()
ds = ws.datasets['rossmann-train.csv']
df = ds.to_dataframe()
</code></pre>

<blockquote>
  <p>/home/nbuser/anaconda3_23/lib/python3.4/site-packages/IPython/kernel/<strong>main</strong>.py:6: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.</p>
</blockquote>

<p>In my local environment I just import the dataset as follows: </p>

<pre><code>df = pd.read_csv('train.csv',low_memory=False)
</code></pre>

<p>But I'm not sure how to do this in azure using the <code>ds</code> object. </p>

<pre><code>df = pd.read_csv(ds)
</code></pre>

<p>and</p>

<pre><code>pd.DataFrame.from_csv(ds)
</code></pre>

<p>raise the error:</p>

<blockquote>
  <p>OSError: Expected file path name or file-like object, got  type</p>
</blockquote>

<p>*edit: more info on the <code>ds</code> object: </p>

<pre><code>In  [1]: type(ds)
Out [1]: azureml.SourceDataset
In  [2]: print (ds)
Out [2]: rossmann-train.csv
</code></pre>",2,0,2017-06-28 10:30:39.977000 UTC,,2017-06-28 14:48:08.457000 UTC,2,python-3.x|pandas|jupyter-notebook|azure-machine-learning-studio,1515,2015-12-10 17:31:53.433000 UTC,2022-07-19 16:03:25.457000 UTC,,785,376,15,103,,,,,,['azure-machine-learning-studio']
Upgrade python modules in jupyter notebook,"<p>I just created an EC2 instance (in SageMaker) and ran the following commands in Jupyter Lab.</p>
<pre class=""lang-py prettyprint-override""><code>import fsspec
print(fsspec.__version__)
</code></pre>
<p>0.6.2</p>
<hr />
<p>But I need version 0.8.4, so I did:</p>
<pre class=""lang-py prettyprint-override""><code>!pip install fsspec==0.8.4
</code></pre>
<p>Requirement already satisfied: fsspec==0.8.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (0.8.4)</p>
<hr />
<p>This is confusing. So I did one more time:</p>
<pre class=""lang-py prettyprint-override""><code>import fsspec
print(fsspec.__version__)
</code></pre>
<p>Result: 0.6.2</p>
<hr />
<p>So, in this case I have both versions installed, but I need to use 0.8.4. How do I make Jupyter Lab use the version I want? (0.8.4)</p>
<p>Additionally, I also tried <code>!pip install tqdm==4.51</code>, in this case, the new version 4.51 actually installed, but when I go ahead and do <code>print(tqdm.__version__)</code>, I still get the old version 4.42. Same problem. Any help is appreciated.</p>",0,3,2020-11-24 04:21:56.297000 UTC,,2020-11-24 04:33:03.357000 UTC,0,python|pandas|jupyter-notebook|amazon-sagemaker,401,2017-01-23 05:07:59.830000 UTC,2022-09-24 02:49:58.733000 UTC,,962,184,7,108,,,,,,['amazon-sagemaker']
Visualizing decision jungle in Azure Machine Learning Studio,"<p>I have trained a decision jungle model on Azure Machine Learning, and  now I want to visualize the trees, to see if I can identify the root nodes that are the most determinant in the decision.</p>

<p>When I right-click and click Visualize on the Train Model, what is shown is the parameter set used for the training. How can I either visualize the jungle, or identify the features with highest information gain from this?</p>

<p>Thanks in advance!</p>",0,0,2018-11-13 03:55:48.593000 UTC,,,1,decision-tree|azure-machine-learning-studio|information-gain,88,2013-11-12 03:09:05.143000 UTC,2022-05-31 20:52:18.327000 UTC,"Milton, ON, Canada",179,174,0,30,,,,,,['azure-machine-learning-studio']
How to edit manifest when specifying multiple sources for one task with AWS Sagemaker ground truth,"<p>I would like to create a task to have one worker perform labeling of multiple sound sources with AWS Sagemaker ground truth.
I created a manifest file as follows, but I cannot specify multiple sound sources with source-ref.
How to create a manifest file?</p>

<p>dataset.manifest</p>

<pre><code> {""source-ref"":[""s3://sagemaker-sample/audio_01.wav"", ""s3://sagemaker-sample/audio_02.wav""]}
</code></pre>

<p>Error</p>

<blockquote>
  <p>ClientError: Manifest: s3://sagemaker-sample/dataset.manifest has
  invalid format at line number 0. Make sure that source or source-ref
  field contains a string value</p>
</blockquote>",1,2,2020-05-15 00:25:25.090000 UTC,,,0,json|amazon-web-services|amazon-sagemaker|crowdsourcing,470,2020-05-15 00:20:44.930000 UTC,2022-07-21 08:55:09.810000 UTC,,3,0,0,3,,,,,,['amazon-sagemaker']
Run a flask app within AWS Sagemaker using a custom model,"<p>I've searched for solutions at AWS and Stackoverflow but haven't got a satisfying answer. I have a Sagemaker notebook instance where I built a custom model (not sklearn/tf/torch) which would calculated a result csv file and save it in the notebook.</p>
<p>Now I need to serve predictions using the csv file. The input is id and output is the pre-calculated result from the csv. The sagemaker &quot;deploy custom model&quot; examples only include sklearn/tf/torch models.</p>
<p>Is there a way to create an model endpoint serving pre-calculated predictions from csv files?</p>",1,0,2021-10-06 17:31:25.360000 UTC,,,0,amazon-sagemaker,576,2017-10-16 22:59:49.703000 UTC,2022-09-23 01:03:23.527000 UTC,"SF, CA, United States",609,22,0,84,,,,,,['amazon-sagemaker']
Scored Labels dont match Scored Probabilities,"<p>I have a predictive web service. The scored labels field don't match the highest value for the specific scored probabilities columns. Is it more reliable to search through the different columns and pick the highest?</p>

<p>Like, I get a result that has a bunch of 
""Scored Probabilities for Class"" \""\"""": 0.6,
""Scored Probabilities for Class"" \""\"""": 3.09</p>

<p>But the scored labels show the one with 0.6, how is that possible?</p>",0,2,2018-03-09 20:57:57.973000 UTC,,,0,azure|azure-machine-learning-studio,122,2011-03-02 16:07:05.617000 UTC,2022-09-23 21:19:38.327000 UTC,Costa Rica,1483,647,13,219,,,,,,['azure-machine-learning-studio']
MLFLow: Install github-package dependency via pip and 1 building-job,"<p>I want to use MLFlow and I have to specify a Github python package as a pip dependency in the yaml-file.
The problem is, that I need to force pip to only use 1 job to build it (otherwise it would run out of memory).
How can I do this?</p>
<p>I tried already: mlflow run hello_ml -n 1
But n is no option. Nether j (job).</p>",0,0,2021-10-13 10:57:45.053000 UTC,,,0,python|pip|mlflow,26,2020-02-18 07:07:42.117000 UTC,2022-05-11 15:17:32.807000 UTC,,3,0,0,2,,,,,,['mlflow']
How to add dynamic training job name to SageMaker training job executed within an AWS StateMachine described by CDK?,"<p>I have a state machine created with CDK (TypeScript). In this state machine I have a map state in which I run multiple ML model trainings with SageMaker, amount depends on the output of previous step, we have 2-5 datasets. I'm struggling with setting the training job name dynamically based on the output of the previous step. If I use static name even with only 1 training, I can't run the state machine multiple times because the training job already exists.</p>
<p>This is what I have tried with CDK</p>
<pre><code>const training = new tasks.SageMakerCreateTrainingJob(
      this.stack,
      &quot;SageMaker training job&quot;,
      {
         trainingJobName: &quot;$.trainingJobName&quot;, // or static name like DemoTrainingJob
      ...
         InputDataConfig: [
           {
              channelName: &quot;train&quot;,
              dataSource: {
                s3DataSource: {
                  s3Location: tasks.S3Location.fromJsonExpression(&quot;$.input&quot;),
      ...
         hyperparameters: {
           &quot;max_depth.$&quot;: &quot;$.hyperparameters.max_depth&quot;,


</code></pre>
<p>s3DataSource and hyperparameters are shown only for reference as they work. Then, the previous step outputs trainingJobName like below (Data in TaskStateEntered for training job).</p>
<pre><code>  &quot;name&quot;: &quot;SageMaker training job&quot;,
  &quot;input&quot;: {
    &quot;output&quot;: &quot;s3://my-bucket/outputx&quot;,
    &quot;input&quot;: &quot;s3://my-bucket/inputx.csv&quot;,
    &quot;hyperparameters&quot;: {
      &quot;max_depth&quot;: &quot;5&quot;
    },
    &quot;trainingJobName&quot;: &quot;some-dynamic-name-YYYYMMDDHHiiSS&quot;
  },
  &quot;inputDetails&quot;: {
    &quot;truncated&quot;: false
  }
}
</code></pre>
<p>But as the input to SageMaker training (TaskScheduled) I see</p>
<pre><code> &quot;resourceType&quot;: &quot;sagemaker&quot;,
  &quot;resource&quot;: &quot;createTrainingJob&quot;,
  &quot;region&quot;: &quot;eu-west-1&quot;,
  &quot;parameters&quot;: {
    &quot;TrainingJobName&quot;: &quot;$.trainingJobName&quot;,
    &quot;RoleArn&quot;: ...
</code></pre>
<p>And TrainingJobName is not interpolated from the input. S3 data is correctly interpolated (though differently expressed) and hyperparameter value. Naturally the job fails, because $.trainingJobName is not valid training job name. Static would fail too if I had used it before, but that is not ok, because it prevents using the state machine more than once.</p>
<p>I have seen this
<a href=""https://github.com/aws-samples/amazon-sagemaker-cdk-examples/blob/master/cron-train/lambda-handler.py"" rel=""nofollow noreferrer"">https://github.com/aws-samples/amazon-sagemaker-cdk-examples/blob/master/cron-train/lambda-handler.py</a>
where TrainingJob is started from Lambda where dynamic training job name can be easily set, but this should be possible also within a state machine.</p>",1,0,2021-01-26 09:30:43.617000 UTC,,2021-02-03 15:20:48.483000 UTC,0,typescript|amazon-web-services|amazon-sagemaker|aws-cdk|aws-step-functions,597,2014-09-26 06:21:21.730000 UTC,2021-03-16 06:00:07.157000 UTC,,113,12,0,13,,,,,,['amazon-sagemaker']
What is the difference between deploying models in MLflow and Sagemaker?,"<p>I could do
<code>mlflow model serve -m &lt;RUN_ID&gt; --p 1234 --no-conda</code></p>
<p>and</p>
<p><code>mlflow sagemaker run-local -m &lt;MODEL_PATH&gt; -p 1234</code></p>
<p>Are they not the same anyway as both can do model serving so what's the hassle deploying it to Sagemaker?</p>
<p>I'm a beginner at this so if anyone can help me out with my understanding that will be great. Thank you so much in advance!</p>",1,0,2022-09-22 00:30:20.887000 UTC,,,0,rest|deployment|amazon-sagemaker|endpoint|mlflow,26,2017-02-23 06:31:44.143000 UTC,2022-09-24 07:48:55.500000 UTC,,1,0,0,4,,,,,,"['mlflow', 'amazon-sagemaker']"
Connecting DocumentDB with SageMaker (Studio),"<p>So, it has been more than a week that I've been trying to connect my DocumentDB with SageMaker Studio, and I haven't been able to.</p>
<p>Here is the scenario. I have a DocumentDB with which I want to connect using <code>pymongo</code> inside a notebook in SageMaker Studio. DocumentDB is restricted to VPC, so I've created my SageMaker Studio Domain in the same VPC as my DocumentDB. Yet, I keep getting time-out errors.</p>
<p>I've tried following this tutorial <a href=""https://aws.amazon.com/blogs/database/getting-started-with-amazon-documentdb-with-mongodb-compatibility-part-4-using-amazon-sagemaker-notebooks/"" rel=""nofollow noreferrer"">here</a>, yet, when I do, I still get an error and I'm not able to connect.</p>
<p>Has anyone done this before? BTW, I've already checked the security groups, and DocumentDB is habilitated for any inbound.</p>
<p>By the way, I'm not using the &quot;VPC Only&quot; configuration.</p>",0,2,2022-08-24 12:30:35.337000 UTC,,2022-08-24 12:38:59.087000 UTC,0,amazon-web-services|amazon-sagemaker|aws-documentdb,33,2015-03-28 12:43:08.720000 UTC,2022-09-23 13:59:26.683000 UTC,"Rio de Janeiro, RJ, Brasil",1467,186,0,47,,,,,,['amazon-sagemaker']
How to install python ta-lib library in Azure Machine Learning Juypyter Notebooks,"<p>SO I need to use this python library explained here:</p>
<p><a href=""https://medium.com/analytics-vidhya/recognizing-over-50-candlestick-patterns-with-python-4f02a1822cb5"" rel=""nofollow noreferrer"">https://medium.com/analytics-vidhya/recognizing-over-50-candlestick-patterns-with-python-4f02a1822cb5</a></p>
<p>However when I try to execute:</p>
<p>!pip install talib
import talib</p>
<p>I get lots of errors:</p>
<pre><code>Collecting talib
  Using cached talib-0.1.1.tar.gz (1.3 kB)
Building wheels for collected packages: talib
  Building wheel for talib (setup.py) ... error
  ERROR: Command errored out with exit status 1:
   command: /anaconda/envs/azureml_py36/bin/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'/tmp/pip-install-lof3_wuc/talib/setup.py'&quot;'&quot;'; __file__='&quot;'&quot;'/tmp/pip-install-lof3_wuc/talib/setup.py'&quot;'&quot;';f=getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(__file__);code=f.read().replace('&quot;'&quot;'\r\n'&quot;'&quot;', '&quot;'&quot;'\n'&quot;'&quot;');f.close();exec(compile(code, __file__, '&quot;'&quot;'exec'&quot;'&quot;'))' bdist_wheel -d /tmp/pip-wheel-1dvr6fd9
       cwd: /tmp/pip-install-lof3_wuc/talib/
  Complete output (29 lines):
  running bdist_wheel
  running build
  running build_py
  creating build
  creating build/lib
  creating build/lib/talib
  copying talib/__init__.py -&gt; build/lib/talib
  copying talib/cli.py -&gt; build/lib/talib
  installing to build/bdist.linux-x86_64/wheel
  running install
  Traceback (most recent call last):
    File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;
    File &quot;/tmp/pip-install-lof3_wuc/talib/setup.py&quot;, line 47, in &lt;module&gt;
      'talib = talib.cli:cli',
    File &quot;/anaconda/envs/azureml_py36/lib/python3.6/distutils/core.py&quot;, line 148, in setup
      dist.run_commands()
    File &quot;/anaconda/envs/azureml_py36/lib/python3.6/distutils/dist.py&quot;, line 955, in run_commands
      self.run_command(cmd)
    File &quot;/anaconda/envs/azureml_py36/lib/python3.6/distutils/dist.py&quot;, line 974, in run_command
      cmd_obj.run()
    File &quot;/anaconda/envs/azureml_py36/lib/python3.6/site-packages/wheel/bdist_wheel.py&quot;, line 326, in run
      self.run_command('install')
    File &quot;/anaconda/envs/azureml_py36/lib/python3.6/distutils/cmd.py&quot;, line 313, in run_command
      self.distribution.run_command(command)
    File &quot;/anaconda/envs/azureml_py36/lib/python3.6/distutils/dist.py&quot;, line 974, in run_command
      cmd_obj.run()
    File &quot;/tmp/pip-install-lof3_wuc/talib/setup.py&quot;, line 20, in run
      raise Exception(&quot;You probably meant to install and run ta-lib&quot;)
  Exception: You probably meant to install and run ta-lib
  ----------------------------------------
  ERROR: Failed building wheel for talib
  Running setup.py clean for talib
Failed to build talib
Installing collected packages: talib
    Running setup.py install for talib ... error
    ERROR: Command errored out with exit status 1:
     command: /anaconda/envs/azureml_py36/bin/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'/tmp/pip-install-lof3_wuc/talib/setup.py'&quot;'&quot;'; __file__='&quot;'&quot;'/tmp/pip-install-lof3_wuc/talib/setup.py'&quot;'&quot;';f=getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(__file__);code=f.read().replace('&quot;'&quot;'\r\n'&quot;'&quot;', '&quot;'&quot;'\n'&quot;'&quot;');f.close();exec(compile(code, __file__, '&quot;'&quot;'exec'&quot;'&quot;'))' install --record /tmp/pip-record-7fqjayvz/install-record.txt --single-version-externally-managed --compile --install-headers /anaconda/envs/azureml_py36/include/python3.6m/talib
         cwd: /tmp/pip-install-lof3_wuc/talib/
    Complete output (14 lines):
    running install
    Traceback (most recent call last):
      File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;
      File &quot;/tmp/pip-install-lof3_wuc/talib/setup.py&quot;, line 47, in &lt;module&gt;
        'talib = talib.cli:cli',
      File &quot;/anaconda/envs/azureml_py36/lib/python3.6/distutils/core.py&quot;, line 148, in setup
        dist.run_commands()
      File &quot;/anaconda/envs/azureml_py36/lib/python3.6/distutils/dist.py&quot;, line 955, in run_commands
        self.run_command(cmd)
      File &quot;/anaconda/envs/azureml_py36/lib/python3.6/distutils/dist.py&quot;, line 974, in run_command
        cmd_obj.run()
      File &quot;/tmp/pip-install-lof3_wuc/talib/setup.py&quot;, line 20, in run
        raise Exception(&quot;You probably meant to install and run ta-lib&quot;)
    Exception: You probably meant to install and run ta-lib
    ----------------------------------------
ERROR: Command errored out with exit status 1: /anaconda/envs/azureml_py36/bin/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'/tmp/pip-install-lof3_wuc/talib/setup.py'&quot;'&quot;'; __file__='&quot;'&quot;'/tmp/pip-install-lof3_wuc/talib/setup.py'&quot;'&quot;';f=getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(__file__);code=f.read().replace('&quot;'&quot;'\r\n'&quot;'&quot;', '&quot;'&quot;'\n'&quot;'&quot;');f.close();exec(compile(code, __file__, '&quot;'&quot;'exec'&quot;'&quot;'))' install --record /tmp/pip-record-7fqjayvz/install-record.txt --single-version-externally-managed --compile --install-headers /anaconda/envs/azureml_py36/include/python3.6m/talib Check the logs for full command output.
---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
&lt;ipython-input-56-ef2dd36041a5&gt; in &lt;module&gt;
      1 get_ipython().system('pip install talib')
----&gt; 2 import talib

ModuleNotFoundError: No module named 'talib'
</code></pre>",1,1,2020-11-05 20:42:15.850000 UTC,,,1,python|python-3.x|azure|pip|azure-machine-learning-studio,770,2011-04-05 19:05:03.093000 UTC,2022-09-16 12:42:27.473000 UTC,"Brussels, Bélgica",30340,1667,79,2937,,,,,,['azure-machine-learning-studio']
Install packages in Create R Model - AzureML,"<p>Is it currently possible to install packages in 'Create R Model'? Currently this is a huge limitation of AzureML.</p>

<p>I know it is possible to do it in 'Execute R Script' but in 'Execute R Script' you can't save the model.</p>",2,2,2016-11-10 09:55:03.003000 UTC,,,2,r|azure|machine-learning|r-package|azure-machine-learning-studio,269,2016-01-12 14:22:43.363000 UTC,2019-07-02 21:45:24.250000 UTC,,105,10,0,12,,,,,,['azure-machine-learning-studio']
How to solve programming error when storing pandas data frame to snowflake,"<p>I'm trying to use SQLAlchemy to store a data frame I created in sagemaker to snowflake. The code only works with certain columns. When I add other columns it gives me an error even though they have the same data type. In the following example, if I only upload TA_ID it works, yet if I upload Cluster_ID, the code throws me an error. </p>

<p>I checked SQLAlchemy website but didn't find much information on programming error. </p>

<h2>SQL codes used to create table</h2>

<pre><code>CREATE OR REPLACE TABLE test.m (
    TA_ID string,
     Cluster_ID string
)
</code></pre>

<h2>Python code</h2>

<pre><code>master2.to_sql(name='m', con=engine2, if_exists='append',  schema='test',index=False, index_label=None, chunksize=2000 )
</code></pre>

<p>ProgrammingError: </p>

<pre><code>(snowflake.connector.errors.ProgrammingError) 000904 (42000): SQL compilation error: error line 1 at position 29
invalid identifier '""Cluster_ID""' [SQL: 'INSERT INTO test.m (""TA_ID"", ""Cluster_ID"") VALUES (%(TA_ID)s, %(Cluster_ID)s)'] [parameters: ({'TA_ID': 'TA007', 'Cluster_ID': '0'}, {'TA_ID': 'TA007', 'Cluster_ID': '16'}, {'TA_ID': 'TA007', 'Cluster_ID': '40'}, {'TA_ID': 'TA007', 'Cluster_ID': '15'}, {'TA_ID': 'TA007', 'Cluster_ID': '29'}, {'TA_ID': 'TA007', 'Cluster_ID': '23'}, {'TA_ID': 'TA007', 'Cluster_ID': '9'}, {'TA_ID': 'TA007', 'Cluster_ID': '25'}, {'TA_ID': 'TA007', 'Cluster_ID': '42'}, {'TA_ID': 'TA007', 'Cluster_ID': '28'})] (Background on this error at: http://sqlalche.me/e/f405)
</code></pre>",1,1,2019-05-23 01:30:22.990000 UTC,,2019-05-23 04:54:03.900000 UTC,0,sql|pandas|sqlalchemy|amazon-sagemaker|snowflake-cloud-data-platform,934,2015-01-23 20:35:22.283000 UTC,2019-10-22 15:28:10.930000 UTC,,1,0,0,3,,,,,,['amazon-sagemaker']
Can I mount EFS in SageMaker processing job?,"<p>We are running some predictions using processing jobs in SageMaker. Before, we added some models into Docker image that was used by SageMaker processing job. Now, as we have more models (&gt;= 6Gb), it is quite a bad design to add them to Docker image.</p>
<p>At first, we thought to download huge models from S3, but then realised, that it will cost a fortune. Then, an idea came to my mind. What if we use EFS with all heavy models and connect it to the running processing job so that python code can reference and use them?</p>
<p>I did a search and couldn't find any working solution for this. UI in AWS doesn't allow to connect EFS, and I couldn't find any working solution how to connect to EFS from the docker container. All suggest to use Docker volumes, but there is no way to configure this when create a processing job.</p>
<p>Question is is there any way to mount EFS to processing job in SageMaker? If no, what are the alternatives to host and use heavy models in the processing jobs? Maybe there are some other alternatives?</p>
<p>Thank you for any input.</p>",1,0,2022-09-12 01:13:23.473000 UTC,1.0,,0,amazon-sagemaker|amazon-efs,36,2011-06-07 14:43:11.457000 UTC,2022-09-23 06:53:13.343000 UTC,"Sydney, Australia",537,16,0,49,,,,,,['amazon-sagemaker']
What are the pros and cons of using DVC and Pachyderm?,"<p>What are the pros and cons of using either of these?</p>

<p><a href=""https://github.com/iterative/dvc"" rel=""nofollow noreferrer"">https://github.com/iterative/dvc</a></p>

<p><a href=""https://github.com/pachyderm/pachyderm"" rel=""nofollow noreferrer"">https://github.com/pachyderm/pachyderm</a></p>",1,1,2019-07-04 06:12:59.043000 UTC,,,1,machine-learning|version-control|data-science|dvc|pachyderm,1635,2019-07-04 06:06:43.123000 UTC,2019-07-18 00:21:59.090000 UTC,,41,0,0,3,,,,,,"['dvc', 'pachyderm']"
Retrieve Sagemaker Model from Model Registry in Sagemaker Pipelines,"<p>I am implementing inference pipeline via AWS Sagemaker Pipelines with Python SDK. I have a Model Package Group in Model Registry and I want to use the latest approved model version from the package group for inference (I am going to use batch-transform inference). However, I don't know which Pipeline step to use to retrieve the latest approved model version. As a workaround, I tried to use from <code>sagemaker.workflow.lambda_step.LambdaStep</code> to retrieve model version ARN and then <code>sagemaker.ModelPackage</code> to define <code>sagemaker.workflow.steps.CreateModelStep </code>. The minimal working code is the following</p>
<pre><code>import sagemaker
from sagemaker.lambda_helper import Lambda
from sagemaker.workflow.lambda_step import (
    LambdaStep,
    LambdaOutput,
    LambdaOutputTypeEnum,
)
from sagemaker.workflow.pipeline import Pipeline

from sagemaker import ModelPackage
from sagemaker.workflow.steps import CreateModelStep
from sagemaker.inputs import CreateModelInput


role = sagemaker.get_execution_role()
sagemaker_sess = sagemaker.Session()

# create lambda function that retrieves latest approved model version ARN
function_name = f&quot;inference-pipeline-lambda-step&quot;
func = Lambda(
    function_name=function_name,
    execution_role_arn=role,
    script=&quot;get_model_arn.py&quot;,
    handler=&quot;get_model_arn.lambda_handler&quot;,
    timeout=600,
    memory_size=10240,
)
output_metric_value = LambdaOutput(output_name=&quot;model_package_arn&quot;, output_type=LambdaOutputTypeEnum.String)

# define Lambda step that retrieves latest approved model version ARN
step_get_model_arn = LambdaStep(
    name=&quot;GetModelARN&quot;,
    lambda_func=func,
    inputs={
    },
    outputs=[output_metric_value] 
)

# use output of the previous Lambda step to define a sagemaker Model
model = ModelPackage(
    role=role, 
    model_package_arn=step_get_model_arn.properties.Outputs['model_package_arn'], 
    sagemaker_session=sagemaker_sess
)

# define CreateModelStep so that the model can be later used in Transform step for batch-transform inference
inputs = CreateModelInput(
        instance_type='ml.m5.large',
    )

step_create_model = CreateModelStep(
    name=&quot;create-inference-model&quot;,
    model=model,
    inputs=inputs,
)

# Pipeline definition and creation/update
pipeline = Pipeline(
    name='well-logs-inference-pipeline',
    parameters=[],
    steps=[
        step_get_model_arn,
        step_create_model
    ],
)

pipeline.upsert(role_arn=role)
</code></pre>
<p>This gives an error</p>
<pre><code>TypeError: expected string or bytes-like object
</code></pre>
<p>As I understand it, the error happens in <code>model = ModelPackage(...)</code> expression. ModelPackage requires 'model_package_arn' to be a string, however, it is <code>sagemaker.workflow.properties.Properties</code> instead.</p>
<p>Is there a chance to retrieve model version from Model Package Group so that it can be later used in TransformStep?</p>
<p>The full traceback is here</p>
<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-6-63bdf0b9bf74&gt; in &lt;module&gt;
     65 )
     66 
---&gt; 67 pipeline.upsert(role_arn=role)

/opt/conda/lib/python3.7/site-packages/sagemaker/workflow/pipeline.py in upsert(self, role_arn, description, tags, parallelism_config)
    217         &quot;&quot;&quot;
    218         try:
--&gt; 219             response = self.create(role_arn, description, tags, parallelism_config)
    220         except ClientError as e:
    221             error = e.response[&quot;Error&quot;]

/opt/conda/lib/python3.7/site-packages/sagemaker/workflow/pipeline.py in create(self, role_arn, description, tags, parallelism_config)
    114         &quot;&quot;&quot;
    115         tags = _append_project_tags(tags)
--&gt; 116         kwargs = self._create_args(role_arn, description, parallelism_config)
    117         update_args(
    118             kwargs,

/opt/conda/lib/python3.7/site-packages/sagemaker/workflow/pipeline.py in _create_args(self, role_arn, description, parallelism_config)
    136             A keyword argument dict for calling create_pipeline.
    137         &quot;&quot;&quot;
--&gt; 138         pipeline_definition = self.definition()
    139         kwargs = dict(
    140             PipelineName=self.name,

/opt/conda/lib/python3.7/site-packages/sagemaker/workflow/pipeline.py in definition(self)
    299     def definition(self) -&gt; str:
    300         &quot;&quot;&quot;Converts a request structure to string representation for workflow service calls.&quot;&quot;&quot;
--&gt; 301         request_dict = self.to_request()
    302         request_dict[&quot;PipelineExperimentConfig&quot;] = interpolate(
    303             request_dict[&quot;PipelineExperimentConfig&quot;], {}, {}

/opt/conda/lib/python3.7/site-packages/sagemaker/workflow/pipeline.py in to_request(self)
     89             if self.pipeline_experiment_config is not None
     90             else None,
---&gt; 91             &quot;Steps&quot;: list_to_request(self.steps),
     92         }
     93 

/opt/conda/lib/python3.7/site-packages/sagemaker/workflow/utilities.py in list_to_request(entities)
     40     for entity in entities:
     41         if isinstance(entity, Entity):
---&gt; 42             request_dicts.append(entity.to_request())
     43         elif isinstance(entity, StepCollection):
     44             request_dicts.extend(entity.request_dicts())

/opt/conda/lib/python3.7/site-packages/sagemaker/workflow/steps.py in to_request(self)
    212     def to_request(self) -&gt; RequestType:
    213         &quot;&quot;&quot;Gets the request structure for `ConfigurableRetryStep`.&quot;&quot;&quot;
--&gt; 214         step_dict = super().to_request()
    215         if self.retry_policies:
    216             step_dict[&quot;RetryPolicies&quot;] = self._resolve_retry_policy(self.retry_policies)

/opt/conda/lib/python3.7/site-packages/sagemaker/workflow/steps.py in to_request(self)
    101             &quot;Name&quot;: self.name,
    102             &quot;Type&quot;: self.step_type.value,
--&gt; 103             &quot;Arguments&quot;: self.arguments,
    104         }
    105         if self.depends_on:

/opt/conda/lib/python3.7/site-packages/sagemaker/workflow/steps.py in arguments(self)
    411                 container_defs=self.model.prepare_container_def(
    412                     instance_type=self.inputs.instance_type,
--&gt; 413                     accelerator_type=self.inputs.accelerator_type,
    414                 ),
    415                 vpc_config=self.model.vpc_config,

/opt/conda/lib/python3.7/site-packages/sagemaker/model.py in prepare_container_def(self, instance_type, accelerator_type, serverless_inference_config)
    411         &quot;&quot;&quot;
    412         deploy_key_prefix = fw_utils.model_code_key_prefix(
--&gt; 413             self.key_prefix, self.name, self.image_uri
    414         )
    415         deploy_env = copy.deepcopy(self.env)

/opt/conda/lib/python3.7/site-packages/sagemaker/fw_utils.py in model_code_key_prefix(code_location_key_prefix, model_name, image)
    393         str: the key prefix to be used in uploading code
    394     &quot;&quot;&quot;
--&gt; 395     training_job_name = sagemaker.utils.name_from_image(image)
    396     return &quot;/&quot;.join(filter(None, [code_location_key_prefix, model_name or training_job_name]))
    397 

/opt/conda/lib/python3.7/site-packages/sagemaker/utils.py in name_from_image(image, max_length)
     58         max_length (int): Maximum length for the resulting string (default: 63).
     59     &quot;&quot;&quot;
---&gt; 60     return name_from_base(base_name_from_image(image), max_length=max_length)
     61 
     62 

/opt/conda/lib/python3.7/site-packages/sagemaker/utils.py in base_name_from_image(image)
    100         str: Algorithm name, as extracted from the image name.
    101     &quot;&quot;&quot;
--&gt; 102     m = re.match(&quot;^(.+/)?([^:/]+)(:[^:]+)?$&quot;, image)
    103     algo_name = m.group(2) if m else image
    104     return algo_name

/opt/conda/lib/python3.7/re.py in match(pattern, string, flags)
    173     &quot;&quot;&quot;Try to apply the pattern at the start of the string, returning
    174     a Match object, or None if no match was found.&quot;&quot;&quot;
--&gt; 175     return _compile(pattern, flags).match(string)
    176 
    177 def fullmatch(pattern, string, flags=0):

TypeError: expected string or bytes-like object
</code></pre>",0,0,2022-06-08 10:18:16.127000 UTC,,,0,amazon-web-services|amazon-sagemaker,48,2013-10-31 09:22:47.570000 UTC,2022-09-23 16:02:10.247000 UTC,,486,100,1,25,,,,,,['amazon-sagemaker']
SageMaker Studio domain creation fails due to KMS permissions,"<h1>Question</h1>
<p>Please help understand the cause and solution for the problem.</p>
<h1>Problem</h1>
<p>SageMaker Studio domain creation fails due to KMS permissions. The IAM Role specified to the SageMaker arn:aws:iam::316725000538:role/SageMaker has the permissions for KMS required as specified in <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/api-permissions-reference.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/api-permissions-reference.html</a>.</p>
<blockquote>
<p>Domain creation failed<br>
Unable to create Amazon EFS for domain 'd-1dq5c9rpkswy' because you don't have permissions to use the KMS key 'arn:aws:kms:us-east-2:316725000538:key/1e2dbf9d-daa0-408d-a290-1633b615c54f'. See <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/api-permissions-reference.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/api-permissions-reference.html</a> for required permissions for CreateDomain action.</p>
</blockquote>
<p><a href=""https://i.stack.imgur.com/AQ9iY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AQ9iY.png"" alt=""enter image description here"" /></a> tells the IAM permissions</p>
<h1>IAM Permission for CreateDomain action</h1>
<ul>
<li><a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/api-permissions-reference.html"" rel=""nofollow noreferrer"">Amazon SageMaker API Permissions: Actions, Permissions, and Resources Reference</a></li>
</ul>
<p><a href=""https://i.stack.imgur.com/vRPXW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vRPXW.png"" alt=""enter image description here"" /></a></p>
<p>The IAM permission required for the CreateDomain action have been attached to the IAM role.</p>
<p><a href=""https://i.stack.imgur.com/s0ROi.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/s0ROi.png"" alt=""enter image description here"" /></a></p>",2,0,2021-08-07 01:48:13.177000 UTC,,,0,amazon-web-services|amazon-sagemaker|amazon-kms,431,2014-11-22 09:22:35.470000 UTC,2022-09-24 22:13:03.237000 UTC,,14749,641,62,968,,,,,,['amazon-sagemaker']
"SageMaker Experiments: UnpicklingError: invalid load key, '\x1f'","<p>I am new to Data Science and have been trying to use SageMaker Experiments to create an extremely simple model. Using SageMaker Experiments, I trained a model using a CSV dataset. The model was output to S3, and I am now trying to load that model into a Jupyter Notebook and run a batch transform test on it.</p>
<p>However, I am getting an error when trying to use the following block of code provided in the SageMaker docs that is supposed to help me import the model. <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html"" rel=""nofollow noreferrer"">source</a></p>
<pre><code>import pickle as pkl
import tarfile
import xgboost

t = tarfile.open('model.tar.gz', 'r:gz')
t.extractall()

model = pkl.load(open(model_file_path, 'rb'))
</code></pre>
<p>The error is occurring at pkl.load:</p>
<pre><code> ---------------------------------------------------------------------------
UnpicklingError                           Traceback (most recent call last)
&lt;ipython-input-39-81639df86023&gt; in &lt;module&gt;
      6 t.extractall()
      7 
----&gt; 8 model = pkl.load(open(model_file_path, 'rb'))
      9 
     10 

UnpicklingError: invalid load key, '\x1f'.
</code></pre>
<p><strong>Note:</strong> model_file_path is 'model.tar.gz'</p>
<p>Because I am so new at this, I do not know if it is an error with what I have done, or if there is something about how SageMaker Experiments does something that I am missing. I have tried referencing other stackoverflow posts that contain this error, but they don't seem to directly apply (to my understanding) to my error. (<a href=""https://stackoverflow.com/questions/33049688/what-causes-the-error-pickle-unpicklingerror-invalid-load-key"">this</a> and <a href=""https://stackoverflow.com/questions/45121556/unable-to-load-cifar-10-dataset-invalid-load-key-x1f"">this</a>)</p>
<p>Any expert advice on SageMaker Experiments or the SageMaker v2 SDK would be extremely helpful to this newbie trying to break into the space. Thank you!</p>",1,0,2021-02-08 18:29:06.247000 UTC,,,1,python|machine-learning|pickle|xgboost|amazon-sagemaker,371,2017-12-10 03:01:42.863000 UTC,2021-04-16 02:16:59.697000 UTC,"Salt Lake City, UT, United States",11,0,0,9,,,,,,['amazon-sagemaker']
How does Azure Machine Learning (AML) decide what 'type' an input is in the training data as this is causing an error,"<p>We have trained a network in AML based on a large training data-set with many inputs. For one of the inputs all the training data supplied was an integer amount. Nowhere in our model do we specify the type of the input fields and this input is a continuous field as opposed to a discrete one. </p>

<p>However in use in production, the AML web service was called with this input supplied as a decimal instead of a real and it threw an error:</p>

<pre><code>Error Content: {""error"":{""code"":""BadArgument"",""message"":""Invalid argument provided."",""details"":[{""code"":""InputParseError"",""target"":""input1"",""message"":""Parsing of input vector failed.  Verify the input vector has the correct number of columns and data types.  Additional details: Input string was not in a correct format..""}]}}
</code></pre>

<p>If I edit the call to change the input from being a decimal to an integer it works perfectly. </p>

<p>Before we go change either the model or the calling code i would like to understand how/why this error is occurring?</p>

<p>I have done some research and cant seem to find anything that explains how AML decides the input ""type"" and whether it can be changed in the model.</p>",1,0,2018-10-25 03:30:12.870000 UTC,,,-1,azure-machine-learning-studio,296,2013-08-28 05:51:06.150000 UTC,2022-09-22 00:30:47.157000 UTC,"Auckland, New Zealand",397,310,6,37,,,,,,['azure-machine-learning-studio']
Extract feature importance from a mlflow 1.9 PyFuncModel model,"<p><strong>Top line</strong>: How can I extract feature importance from an xgboost model that has been saved in mlflow as a PyFuncModel?</p>
<p>Details:</p>
<ul>
<li>I've picked up model update responsibilities from a data scientist who has just left. They used mlflow to tune hyperparameters. I need to understand the feature importance from the model they have built. My attempts of using <code>shap</code> have failed (my code below).</li>
<li>I'm using mlflow 1.9.1. I can see that <code>mlflow.shap</code> exists in the current version, but this does not seem to appear in the version I'm on.</li>
</ul>
<pre><code>import mlflow
import shap

model = mlflow.pyfunc.load_model(model_load_details)  
print(f&quot;model {type(model)})&quot;) 
# model &lt;class 'mlflow.pyfunc.PyFuncModel'&gt;)

explainer = shap.Explainer(model)
</code></pre>
<p>... which returns the error message &quot;Exception: The passed model is not callable and cannot be analyzed directly with the given masker! Model: mlflow.pyfunc.loaded_model:&quot;</p>
<p><strong>My own thinking</strong>: Extract the parameter settings for the best model from mlflow, use these to retrain fresh xgboost model, then save as an xgboost flavor: From <a href=""https://www.mlflow.org/docs/latest/python_api/mlflow.xgboost.html"" rel=""nofollow noreferrer"">here</a>, then use <code>mlflow.xgboost.save_model()</code>. But, is there a better way?</p>",1,0,2022-01-13 12:56:18.843000 UTC,,,0,python|xgboost|mlflow,417,2013-05-20 14:56:31.847000 UTC,2022-09-05 16:59:42.370000 UTC,,607,8,0,60,,,,,,['mlflow']
SageMaker Estimator use_spot_instances causes Invalid MaxWaitTimeInSeconds,"<p><a href=""https://aws.amazon.com/blogs/aws/managed-spot-training-save-up-to-90-on-your-amazon-sagemaker-training-jobs/"" rel=""nofollow noreferrer"">Managed Spot Training: Save Up to 90% On Your Amazon SageMaker Training Jobs</a> says:</p>
<blockquote>
<p>Setting it up is extremely simple, as it should be when working with a fully-managed service:</p>
<ul>
<li>If you’re using the console, just switch the feature on.</li>
<li>If you’re working with the Amazon SageMaker SDK, <strong>just set the train_use_spot_instances to true</strong> in the Estimator constructor.</li>
</ul>
</blockquote>
<p><a href=""https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html#sagemaker.estimator.Estimator"" rel=""nofollow noreferrer"">SageMaker SDK sagemaker.estimator.Estimator</a> says:</p>
<blockquote>
<ul>
<li>use_spot_instances (bool) –
Specifies whether to use SageMaker Managed Spot instances for training. If enabled then the max_wait arg should also be set.</li>
<li>max_wait (int) –
Timeout in seconds waiting for spot training instances (default: None). After this amount of time Amazon SageMaker will stop waiting for Spot instances to become available (default: None).</li>
</ul>
</blockquote>
<p>As per the documentations, run below.</p>
<pre><code>from sagemaker.tensorflow import TensorFlow


estimator = TensorFlow(
    entry_point=&quot;fashion_mnist_training.py&quot;,
    source_dir=&quot;src&quot;,
    metric_definitions=metric_definitions,
    hyperparameters=hyperparameters,
    role=role,
    input_mode='File',
    framework_version=&quot;2.3.1&quot;,
    py_version=&quot;py37&quot;,
    instance_count=1,
    instance_type=&quot;ml.m5.xlarge&quot;,
    use_spot_instances=True,
    max_wait= 23 * 60 * 60, 
    base_job_name=base_job_name,
    checkpoint_s3_uri=checkpoint_s3_uri,
    model_dir=False  # To avoid duplicate 'model_dir' command line argument
)
</code></pre>
<p>However, error is caused.</p>
<pre><code>ClientError: An error occurred (ValidationException) when calling the CreateTrainingJob operation: Invalid MaxWaitTimeInSeconds. It must be present and be greater than or equal to MaxRuntimeInSeconds
</code></pre>",2,0,2021-09-03 09:21:17.280000 UTC,,,1,amazon-web-services|amazon-sagemaker|spot-instances,269,2014-11-22 09:22:35.470000 UTC,2022-09-24 22:13:03.237000 UTC,,14749,641,62,968,,,,,,['amazon-sagemaker']
AWS SageMaker EndPoint returns 415,"<p>I have trained a multiclass classification model on the wine quality dataset and I have deployed the model.
After deploying the model I got EndPoint URL like:</p>

<p><a href=""https://runtime.sagemaker.region.amazonaws.com/endpoints/experiment/invocations"" rel=""nofollow noreferrer"">https://runtime.sagemaker.region.amazonaws.com/endpoints/experiment/invocations</a></p>

<p>And I am invoking the URL after passing AWS credentials and body like:</p>

<p><code>{
  ""instances"": [7.4,0.7,0,1.9,0.076,11,34,0.9978,3.51,0.56,9.4]
}</code></p>

<p>But I am getting below error:</p>

<p><code>{
    ""ErrorCode"": ""CLIENT_ERROR_FROM_MODEL"",
    ""LogStreamArn"": """",
    ""OriginalMessage"": ""'application/json' is an unsupported content type."",
    ""OriginalStatusCode"": 415
}</code></p>

<p>I tried looking for the trace logs in the cloud watch but no traces there as well. Anyone could guide me on this?</p>

<p>I have trained a model using Sage Maker Studion.</p>",1,0,2019-12-27 11:57:00.647000 UTC,,2019-12-27 12:46:22.780000 UTC,0,amazon-web-services|amazon-sagemaker|awsdeploy,795,2014-06-12 07:28:21.253000 UTC,2022-09-24 14:09:54.573000 UTC,"Hyderabad, Telangana, India",962,116,239,283,,,,,,['amazon-sagemaker']
Runtime error using MLFlow and Spark on databricks,"<p>Here is some model I created:</p>
<pre><code>class SomeModel(mlflow.pyfunc.PythonModel):
    def predict(self, context, input):
        # do fancy ML stuff
        # log results
        pandas_df = pd.DataFrame(...insert predictions here...)
        spark_df = spark.createDataFrame(pandas_df)
        spark_df.write.saveAsTable('tablename', mode='append')
</code></pre>
<p>I'm trying to log my model in this manner by calling it later in my code:</p>
<pre><code>with mlflow.start_run(run_name=&quot;SomeModel_run&quot;):
    model = SomeModel()
    mlflow.pyfunc.log_model(&quot;somemodel&quot;, python_model=model)
</code></pre>
<p>Unfortunately it gives me this Error Message:</p>
<p><code>RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.</code></p>
<p>The error is caused because of the line <code>mlflow.pyfunc.log_model(&quot;somemodel&quot;, python_model=model)</code>, if I comment it out my model will make its predictions and log the results in my table.</p>
<p>Alternatively, removing the lines in my predict function where I call spark to create a dataframe and save the table, I am able to log my model.</p>
<p>How do I go about resolving this issue? I need my model to not only write to the table but also be logged</p>",0,8,2022-07-07 15:17:36.183000 UTC,1.0,,3,python|pyspark|apache-spark-sql|databricks|mlflow,93,2017-04-05 06:51:07.557000 UTC,2022-09-23 15:20:00.157000 UTC,,33,1,0,5,,,,,,['mlflow']
ImportError: cannot import name 'get_unsupported_framework_version_error',"<p>I am trying to learn AWS Machine Learning from <a href=""https://aws.amazon.com/getting-started/hands-on/build-train-deploy-monitor-machine-learning-model-sagemaker-studio/?sc_icampaign=Adoption_Campaign_2020_howmlisdone_gs.ho.tutstudio.console&amp;sc_ichannel=ha&amp;sc_icontent=awssm-5821_all-users&amp;sc_ioutcome=AIML_Digital_Marketing&amp;sc_iplace=console-sagemaker_standard&amp;trk=ha_a134p000004f0SDAAY%7Eha_awssm-5821_all-users&amp;trkCampaign=2020_howmlisdone_gs.ho.tutstudio"" rel=""nofollow noreferrer"">this AWS tutorial</a></p>
<p>I have not changed any code.</p>
<p>When I execute the line below</p>
<pre><code>from sagemaker.sklearn.processing import SKLearnProcessor
</code></pre>
<p>I get error:</p>
<pre><code>ImportError                               Traceback (most recent call last)
&lt;ipython-input-13-dd0123cd2e7d&gt; in &lt;module&gt;
----&gt; 1 from sagemaker.sklearn.processing import SKLearnProcessor
      2 
      3 sklearn_processor = SKLearnProcessor(framework_version='0.20.0',
      4                                      role=role,
      5                                      instance_type='ml.c4.xlarge',

~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/sklearn/__init__.py in &lt;module&gt;
     14 from __future__ import absolute_import
     15 
---&gt; 16 from sagemaker.sklearn.estimator import SKLearn  # noqa: F401
     17 from sagemaker.sklearn.model import SKLearnModel, SKLearnPredictor  # noqa: F401
     18 from sagemaker.sklearn.processing import SKLearnProcessor  # noqa: F401

~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/sklearn/estimator.py in &lt;module&gt;
     18 from sagemaker.estimator import Framework
     19 from sagemaker.fw_registry import default_framework_uri
---&gt; 20 from sagemaker.fw_utils import (
     21     framework_name_from_image,
     22     get_unsupported_framework_version_error,

ImportError: cannot import name 'get_unsupported_framework_version_error'
</code></pre>",1,0,2021-02-26 15:05:35.997000 UTC,,2021-02-26 15:15:21.153000 UTC,2,amazon-web-services|scikit-learn|amazon-sagemaker,768,2016-07-27 15:29:04.363000 UTC,2022-07-05 05:39:53.230000 UTC,,31,0,0,10,,,,,,['amazon-sagemaker']
AWS Sagemaker InvokeEndpoint: operation: Endpoint of account not found,"<p>I've been following this guide here: <a href=""https://aws.amazon.com/blogs/machine-learning/building-an-nlu-powered-search-application-with-amazon-sagemaker-and-the-amazon-es-knn-feature/"" rel=""nofollow noreferrer"">https://aws.amazon.com/blogs/machine-learning/building-an-nlu-powered-search-application-with-amazon-sagemaker-and-the-amazon-es-knn-feature/</a></p>
<p>I have successfully deployed the model from my notebook instance. I am also able to generate predictions by calling <code>predict()</code> method from <code>sagemaker.predictor</code>.</p>
<p>This is how I created and deployed the model</p>
<pre><code>class StringPredictor(Predictor):
    def __init__(self, endpoint_name, sagemaker_session):
        super(StringPredictor, self).__init__(endpoint_name, sagemaker_session, content_type='text/plain')

pytorch_model = PyTorchModel(model_data = inputs, 
                             role=role, 
                             entry_point ='inference.py',
                             source_dir = './code', 
                             framework_version = '1.3.1',
                             py_version='py3',
                             predictor_cls=StringPredictor)

predictor = pytorch_model.deploy(instance_type='ml.m5.large', initial_instance_count=4)
</code></pre>
<p>From the SageMaker dashboard, I can even see that my endpoint and the status is &quot;in-service&quot;</p>
<p>If I run <code>aws sagemaker list-endpoints</code> I can see my desired endpoint showing up correctly as well.
My issue is when I run this code (outside of sagemaker), I'm getting an error:</p>
<pre><code>import boto3
sm_runtime_client = boto3.client('sagemaker-runtime')
payload = &quot;somestring that is used here&quot;
response = sm_runtime_client.invoke_endpoint(EndpointName='pytorch-inference-xxxx',ContentType='text/plain',Body=payload)
</code></pre>
<p>This is the error thrown</p>
<pre><code>botocore.errorfactory.ValidationError: An error occurred (ValidationError) when calling the InvokeEndpoint operation: Endpoint pytorch-inference-xxxx of account xxxxxx not found.
</code></pre>
<p>This is quite strange as I'm able to see and run the endpoint just fine from sagemaker notebook and I am able to run the <code>predict()</code> method too.</p>
<p>I have verified the region, endpoint name and the account number.</p>",1,0,2021-08-13 18:54:01.387000 UTC,1.0,2021-08-14 03:36:20.000000 UTC,1,python|python-3.x|amazon-web-services|pytorch|amazon-sagemaker,767,2016-09-30 22:34:59.710000 UTC,2022-09-22 13:53:27.543000 UTC,,586,83,21,103,,,,,,['amazon-sagemaker']
Defining routes in MLflow serving,"<p>When we serve mlflow model we define different ports for each serving and to access these models we use IP:port/invocations</p>
<p>ex:</p>
<p>app 1 : IP:2020/invocations</p>
<p>app 2 : IP:2021/invocations</p>
<p>But I want to serve 2 mlflow models at same port with different routes.</p>
<p>ex:</p>
<p>app 1 : IP:2020/app1</p>
<p>app 2 : IP:2020/app2</p>
<p>How can I achieve it using MLflow.</p>",0,0,2021-12-29 07:32:38.347000 UTC,,2021-12-30 05:36:53.347000 UTC,0,python|routes|mlflow|serving,56,2017-09-04 04:11:52.457000 UTC,2022-09-21 08:05:37.620000 UTC,,334,105,8,24,,,,,,['mlflow']
Sagemaker ML - Load Tensorflow model endpoint directly inside a lambda function,"<p>I've already managed to create a lambda function that loads a model.pb from S3 and apply object detection to an input image (installed tensorflow 1.12)</p>

<p>Is it possible to load a Sagemaker model/endpoint-configuration inside a lambda function ? I mean install all packages needed inside the lambda, without deploying an endpoint/ec2-like instance.</p>

<p>I guess inference performance would drop, but the solution seems to be more cost effective and scalable ready.</p>",1,0,2019-10-16 09:44:14.073000 UTC,,,0,tensorflow|machine-learning|aws-lambda|amazon-sagemaker,117,2016-12-30 18:38:46.907000 UTC,2022-08-22 16:37:54.897000 UTC,,497,45,0,58,,,,,,['amazon-sagemaker']
How to deploy machine learning model saved as pickle file on AWS SageMaker,"<p>I have built an XGBoost Classifier and RandomForest Classifier model for the audio classification project. I want to deploy these models which are saved in pickle (.pkl) format on AWS Sagemaker. From what I have observed, there isn't a lot of resources available online. Can anyone guide me with the steps and if possible also provide the code? I already have the models built and I'm just left with deploying it on Sagemaker.</p>",1,0,2021-07-01 17:50:46.810000 UTC,,,0,machine-learning|deployment|amazon-sagemaker,1060,2021-05-10 11:54:56.993000 UTC,2021-09-22 10:05:26.647000 UTC,,11,0,0,27,,,,,,['amazon-sagemaker']
AzureML pass data between pipeline without saving it,"<p>I have made two scripts using PythonScriptStep where data_prep.py prepares a dataset by doing some data transformation which is thereafter sent to train.py for training an ML model in AzureML.</p>
<p>It is possible passing data between pipeline steps using PipelineData and OutputFileDatasetConfig, however these seem to save the data in azure blob.</p>
<p>Q: How can I send the data between the steps <strong>without</strong> saving the data anywhere?</p>",1,0,2021-08-19 06:39:43.933000 UTC,,,1,azure|azure-pipelines|azure-machine-learning-service,100,2021-08-19 06:33:39.277000 UTC,2022-09-23 11:34:32.393000 UTC,,21,0,0,3,,,,,,['azure-machine-learning-service']
Read the latest file from azure blob python,"<p>I have multiple files in an Azure blob. I want to read/download only the latest file.
How can I do it via python.</p>
<p>Note: I am using azure ML datastores to connect to the container in which the blob resides.</p>",1,0,2020-09-22 06:13:57.163000 UTC,,,0,azure-blob-storage|azure-machine-learning-service,1386,2019-05-16 13:52:24.133000 UTC,2022-08-16 06:46:30.150000 UTC,,61,0,0,5,,,,,,['azure-machine-learning-service']
YOLOv5 Evolution Results Not Reproducible wandb,"<p>I am running YOLOv5 in a sagemaker notebook.
The 10 epoch runs are using the following notebook script making use of the --evolve flag for hyperparameters.</p>
<pre><code>!export WANDB_RUN_GROUP=&quot;evolution&quot; &amp;&amp; python ./deepsea-yolov5/yolov5/train.py
--img=640
--data=./deepsea-yolov5/opt/ml/custom_config.yaml
--batch=2
--weights=yolov5s.pt
--cfg=./deepsea-yolov5/yolov5/models/yolov5s.yaml
--project=&quot;902005-vaa&quot;
--cache
--epochs=10
--evolve=30
</code></pre>
<p>Evolution runs only output one point on the graph at the end of 10 epochs and the outputted hyperparameters do not show reproducible results when running in a 50 epoch run. The blue 50 epoch line showcases using the optimal hyperparameters which should intersect with the highest 10 epoch run, but it doesn't reach anywhere close.
<a href=""https://i.stack.imgur.com/sW0KX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sW0KX.png"" alt=""enter image description here"" /></a></p>
<p>After finding the optimal hyperparameters I ran a 50 epoch run using those parameters using the following command.</p>
<pre><code>!export WANDB_RUN_GROUP=&quot;hyperparam&quot; &amp;&amp; python ./deepsea-yolov5/yolov5/train.py
--img=640
--data=./deepsea-yolov5/opt/ml/custom_config.yaml
--batch=2
--weights=yolov5s.pt
--cfg=./deepsea-yolov5/yolov5/models/yolov5s.yaml
--hyp=./deepsea-yolov5/opt/ml/input/data/hyp.scratch-low.yaml
--project=&quot;902005-vaa&quot;
--cache
--epochs=50
</code></pre>
<p>However as shown in the picture above, the runs do not intersect with the best-performing hyperparameter run.</p>",0,0,2022-07-13 21:33:50.047000 UTC,,,0,python|yolov5|wandb,63,2017-06-21 00:38:30.443000 UTC,2022-09-20 05:17:02.003000 UTC,"California, USA",11,0,0,10,,,,,,['wandb']
TesseractError running with Python3,"<p>I am running tesseract to automatically recognise text on my input image. However I get the following error</p>
<pre><code>TesseractError                            Traceback (most recent call last)
&lt;ipython-input-33-644ae6e68f14&gt; in &lt;module&gt;
     10 img = Image.open(filename)
     11 
---&gt; 12 text = pytesseract.image_to_string(img) # , config=custom_config

~/SageMaker/Image_processing/Matching_exercise/pytesseract.py in image_to_string(image, lang, config, nice, output_type, timeout)
    415         Output.DICT: lambda: {'text': run_and_get_output(*args)},
    416         Output.STRING: lambda: run_and_get_output(*args),
--&gt; 417     }[output_type]()
    418 
    419 

~/SageMaker/Image_processing/Matching_exercise/pytesseract.py in &lt;lambda&gt;()
    414         Output.BYTES: lambda: run_and_get_output(*(args + [True])),
    415         Output.DICT: lambda: {'text': run_and_get_output(*args)},
--&gt; 416         Output.STRING: lambda: run_and_get_output(*args),
    417     }[output_type]()
    418 

~/SageMaker/Image_processing/Matching_exercise/pytesseract.py in run_and_get_output(image, extension, lang, config, nice, timeout, return_bytes)
    282         }
    283 
--&gt; 284         run_tesseract(**kwargs)
    285         filename = kwargs['output_filename_base'] + extsep + extension
    286         with open(filename, 'rb') as output_file:

~/SageMaker/Image_processing/Matching_exercise/pytesseract.py in run_tesseract(input_filename, output_filename_base, extension, lang, config, nice, timeout)
    258     with timeout_manager(proc, timeout) as error_string:
    259         if proc.returncode:
--&gt; 260             raise TesseractError(proc.returncode, get_errors(error_string))
    261 
    262 

TesseractError: (-11, 'TessdataManager loaded -1140457472 types of tesseract data files. TessdataManager: seek to offset 127828764786688 - start of tessdatatype 0 (config))')
</code></pre>
<p>Can anyone help me please, I googled it and found nothing.</p>
<p>Is this error caused by the environment I am using. I am using AWS Sagemaker Jupyter-Notebook, I installed tesseract-ocr, and I called it out using the following code.</p>
<pre><code>img = cv2.imread('my_image.jpg')
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

filename = &quot;{}.png&quot;.format(os.getpid())
cv2.imwrite(filename, gray)

pytesseract.tesseract_cmd = '/usr/local/bin/tesseract' 
# custom_config = r'--oem 3 --psm 6'

img = Image.open(filename)

text = pytesseract.image_to_string(img)  # config=custom_config
</code></pre>
<p>Thank you!</p>",0,0,2021-10-29 10:30:42.110000 UTC,,,0,python-3.x|amazon-ec2|text|tesseract|amazon-sagemaker,55,2019-05-03 02:30:17.100000 UTC,2022-04-28 07:49:17.123000 UTC,,237,13,0,28,,,,,,['amazon-sagemaker']
Insufficient privileges for accessing data in S3,"<p>I am following the tutorial on <a href=""https://docs.aws.amazon.com/personalize/latest/dg/getting-started-console.html"" rel=""nofollow noreferrer"">Getting Started (Console) - Amazon Personalize</a> of the recommendation engine on Amazon SageMaker. 
When importing <code>User-item</code> interaction data, I got the following error:</p>

<blockquote>
  <p>There was an error with your dataset import</p>
  
  <p>Insufficient privileges for accessing data in S3. Please look at <a href=""https://docs.aws.amazon.com/personalize/latest/dg/getting-started.html#gs-upload-to-bucket"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/personalize/latest/dg/getting-started.html#gs-upload-to-bucket</a> and fix the bucket policy on <code>recommendation123</code>.</p>
</blockquote>

<p>I have tried different bucket policies but none of them is allowing to import the data.</p>

<p>The user-item interaction data flag should change from failed to active.</p>",1,0,2019-10-25 07:19:55.500000 UTC,1.0,2019-10-25 22:21:38.087000 UTC,3,amazon-web-services|amazon-s3|amazon-sagemaker,2249,2017-03-15 07:20:22.800000 UTC,2021-01-20 09:19:45.903000 UTC,"Indore, Madhya Pradesh, India",188,8,1,64,,,,,,['amazon-sagemaker']
"Azure ML Workspace missing secrets of associated keyvault, no way to access its datastores","<p>by mistake I deleted the secrets from my keyvault, which were associated to my default Azure Machine Learning datastores (workspacefilestore, workspaceartifactstore and workspaceblobstore).
As a result, when I click on one of these datastores (or a dataset depending on them), I get the error &quot;<strong>Please make sure that you are passing valid secret names and that the keyvault https://mauromikv00.v.</strong>&quot;, where mauromikv00 (without &quot;.v.&quot;) is my associated keyvault.
<a href=""https://i.stack.imgur.com/SNnBl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SNnBl.png"" alt=""enter image description here"" /></a>
I tried to recover the deleted secrets, but they aren't in the recycle bin anymore.</p>
<p>If I were able to click on the datastore, I would be able to use the &quot;updated authentication&quot; button to re-associate the blob storage to the keyvault. But the error above happens as soon as I click on the datastore, preventing me from updating its authentication credentials.</p>",1,0,2022-05-18 19:22:49.123000 UTC,,,0,azure|azure-machine-learning-studio|azure-machine-learning-service|azuremlsdk,267,2019-03-27 15:20:30.127000 UTC,2022-09-23 23:22:00.843000 UTC,,21,2,0,15,,,,,,"['azure-machine-learning-studio', 'azure-machine-learning-service']"
Azure machine learning repeated string,<p>I have a column in a dataset has many strings what i need a model that  scores or evaluate or get the percentage of how many times  the strings are repeated compared to each other.</p>,1,0,2015-05-29 02:44:43.573000 UTC,,2015-06-02 05:53:29.490000 UTC,-4,azure|machine-learning|azure-machine-learning-studio,107,2015-03-10 23:51:56.217000 UTC,2015-05-30 00:13:53.570000 UTC,,1,0,0,4,,,,,,['azure-machine-learning-studio']
Do the underlying resources that get used by AWS SageMaker have to be tagged independently (since tagging is only permitted at the domain level)?,"<p>SageMaker has many components that do not show up in the resource tag editor. So the individual components of SageMaker end up showing up as non-allocated costs.</p>
<p>So tags can be placed at the SageMaker domain level, BUT... does this also mean that tags will propagate to the resources spun up within that domain (jobs, instances, volumes, etc.)?</p>
<p>In other words, do the underlying resources that get used by SageMaker have to be tagged independently?</p>",1,0,2022-01-11 22:04:22.413000 UTC,,,0,amazon-web-services|amazon-sagemaker|tagging|aws-resource-group,266,2012-08-31 20:08:40.090000 UTC,2022-09-25 04:17:41.297000 UTC,,11650,6318,21,977,,,,,,['amazon-sagemaker']
"What is pytesseract.pytesseract.tesseract_cmd, and How to set it up on SageMaker?","<p><strong>Error Says:</strong></p>

<blockquote>
  <p>PermissionError: [Errno 13] Permission denied: '/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/'</p>
</blockquote>

<p>when setting up pytesseract.pytesseract.tesseract_cmd from PyTesseract Module, I gave the location of where the module is installed.</p>

<ol>
<li>What is this tesseract_cmd?</li>
<li>How to set it up on AWS SageMaker?</li>
<li>Is there any other way to extract text from images other than PyTesseract?</li>
<li>Why is it showing me PermissionError when I have the access?</li>
</ol>",0,1,2020-03-12 13:35:57.883000 UTC,,2020-03-12 17:34:44.387000 UTC,2,error-handling|amazon-sagemaker|python-tesseract,213,2019-08-18 09:56:18.310000 UTC,2020-05-27 14:03:38.673000 UTC,"Mumbai, Maharashtra, India",31,0,0,7,,,,,,['amazon-sagemaker']
"R wont assign a value to a variable, but doesn't throw an error","<p>When I run this code I want it to change the rows in the data set that dont have a probability over 60% to be changed to unassigned. I am accessing the variable the correct way because you can see it printed in the output, the issue is with assigning the variable.</p>
<pre><code>rowsCat &lt;- array(get('Email Category', envir = as.environment(dataset2) , mode = &quot;any&quot;, inherits = TRUE))
rowsCapCallProb &lt;- array(get('Scored Probabilities for Class &quot;Capital Call&quot;', envir = as.environment(dataset1) , mode = &quot;any&quot;, inherits = TRUE))
rowsDistroProb &lt;- array(get('Scored Probabilities for Class &quot;distribution&quot;', envir = as.environment(dataset1) , mode = &quot;any&quot;, inherits = TRUE))
rowsFinnancialProb &lt;- array(get('Scored Probabilities for Class &quot;Finnancial Document&quot;', envir = as.environment(dataset1) , mode = &quot;any&quot;, inherits = TRUE))
    
data.set = rbind(dataset1)

checkProb &lt;- function(count) {
  print(&quot;===================================================================&quot;)
  print(paste(&quot;Evaluating Probabilities on Row: &quot;, count))
   
  
  #Check to see if Probabilities are over 60%
  if(rowsCapCallProb[[count]] &gt; .60 | rowsDistroProb[[count]] &gt; .60 | rowsFinnancialProb[[count]] &gt; .60){
    print(paste(&quot;A probable Score was located for Row: &quot;,count))
  } else {
    print(paste(&quot;Attempting to assign 'Unassigned' to Row: &quot;,count, &quot; : &quot;, data.set$ScoredLables[[count]]))
    
    #\/ Issue is here in the assignment \/
    data.set$ScoredLables[[count]] &lt;- &quot;Unassigned&quot;    
  }
  
  print(&quot;===================================================================&quot;)
}


for (rowCat in rowsCat) {
    checkProb(count)
        
    count = count+1
}
</code></pre>
<p><a href=""https://i.stack.imgur.com/BH1Tp.png"" rel=""nofollow noreferrer"">Output Dataset</a>
<a href=""https://i.stack.imgur.com/p4upK.png"" rel=""nofollow noreferrer"">Basic Output</a></p>",1,1,2019-12-09 19:33:42.547000 UTC,,2020-06-20 09:12:55.060000 UTC,0,r|azure-machine-learning-studio,792,2019-11-06 20:39:47.957000 UTC,2021-07-12 20:32:50.557000 UTC,,19,0,0,4,,,,,,['azure-machine-learning-studio']
How to deploy Preprocessing Code with Vertex AI Endpoint,"<p>I have deployed a model in Vertex AI Endpoint and I am able to get the prediction by passing preprocessed features vector.</p>
<p>I want also to deploy preprocess code alongside the model.
How to wrap or deploy preprocessing logic.</p>",1,0,2021-09-10 05:56:51.090000 UTC,1.0,,-1,google-cloud-vertex-ai,324,2017-01-03 19:47:17.570000 UTC,2022-02-22 06:30:56.963000 UTC,"Bangalore, Karnataka, India",72,0,0,6,,,,,,['google-cloud-vertex-ai']
Inconsistent keras model.summary() output shapes on AWS SageMaker and EC2,"<p>I have the following model in a jupyter notebook:</p>
<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras import layers



physical_devices = tf.config.list_physical_devices('GPU')
tf.config.experimental.set_memory_growth(physical_devices[0], True)

SIZE = (549, 549)
SHUFFLE = False 
BATCH = 32
EPOCHS = 20

train_datagen =  DataGenerator(train_files, batch_size=BATCH, dim=SIZE, n_channels=1, shuffle=SHUFFLE)
test_datagen =  DataGenerator(test_files, batch_size=BATCH, dim=SIZE, n_channels=1, shuffle=SHUFFLE)


inp = layers.Input(shape=(*SIZE, 1))

x = layers.Conv2D(filters=549, kernel_size=(5,5), padding=&quot;same&quot;, activation=&quot;relu&quot;)(inp)
x = layers.BatchNormalization()(x)


x = layers.Conv2D(filters=549, kernel_size=(3, 3), padding=&quot;same&quot;, activation=&quot;relu&quot;)(x)
x = layers.BatchNormalization()(x)


x = layers.Conv2D(filters=549, kernel_size=(1, 1), padding=&quot;same&quot;, activation=&quot;relu&quot;)(x)
x = layers.BatchNormalization()(x)

x = layers.Conv2D(filters=549, kernel_size=(3, 3), padding=&quot;same&quot;, activation=&quot;sigmoid&quot;)(x)

model = Model(inp, x)

model.compile(loss=tf.keras.losses.binary_crossentropy, optimizer=Adam())

model.summary()
</code></pre>
<p>Sagemaker and EC2 are running tensorflow 2.7.1. The EC2 instance is p3.2xlarge with Deep Learning AMI GPU TensorFlow 2.7.0 (Amazon Linux 2) 20220607. The SageMaker notebook is using ml.p3.2xlarge and I am using the conda_tensorflow2_p38 kernel. The notebook is in an FSx Lustre file system that is mounted to both SageMaker and EC2 so it is definitely the same code running on both machines.</p>
<p>nvidia-smi output on SageMaker:</p>
<pre><code>+-----------------------------------------------------------------------------+
| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla V100-SXM2...  On   | 00000000:00:1E.0 Off |                    0 |
| N/A   37C    P0    24W / 300W |      0MiB / 16384MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
</code></pre>
<p>nvidia-smi output on EC2:</p>
<pre><code>+-----------------------------------------------------------------------------+
| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla V100-SXM2...  On   | 00000000:00:1E.0 Off |                    0 |
| N/A   42C    P0    51W / 300W |   2460MiB / 16384MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A     11802      C   /bin/python3.8                    537MiB |
|    0   N/A  N/A     26391      C   python3.8                        1921MiB |
+-----------------------------------------------------------------------------+
</code></pre>
<p>The model.summary() output on SageMaker is (this is what I want it to be):</p>
<pre class=""lang-py prettyprint-override""><code>Model: &quot;model&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 549, 549, 1)]     0         
                                                                 
 conv2d (Conv2D)             (None, 549, 549, 1)       7535574   
                                                                 
 batch_normalization (BatchN  (None, 549, 549, 1)      4         
 ormalization)                                                   
                                                                 
 conv2d_1 (Conv2D)           (None, 549, 549, 1)       2713158   
                                                                 
 batch_normalization_1 (Batc  (None, 549, 549, 1)      4         
 hNormalization)                                                 
                                                                 
 conv2d_2 (Conv2D)           (None, 549, 549, 1)       301950    
                                                                 
 batch_normalization_2 (Batc  (None, 549, 549, 1)      4         
 hNormalization)                                                 
                                                                 
 conv2d_3 (Conv2D)           (None, 549, 549, 1)       2713158   
                                                                 
=================================================================
Total params: 13,263,852
Trainable params: 13,263,846
Non-trainable params: 6

</code></pre>
<p>The model.summary() output on EC2 is (notice the shape change):</p>
<pre class=""lang-py prettyprint-override""><code>
Model: &quot;model&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 549, 549, 1)]     0         
                                                                 
 conv2d (Conv2D)             (None, 549, 549, 549)     14274     
                                                                 
 batch_normalization (BatchN  (None, 549, 549, 549)    2196      
 ormalization)                                                   
                                                                 
 conv2d_1 (Conv2D)           (None, 549, 549, 549)     2713158   
                                                                 
 batch_normalization_1 (Batc  (None, 549, 549, 549)    2196      
 hNormalization)                                                 
                                                                 
 conv2d_2 (Conv2D)           (None, 549, 549, 549)     301950    
                                                                 
 batch_normalization_2 (Batc  (None, 549, 549, 549)    2196      
 hNormalization)                                                 
                                                                 
 conv2d_3 (Conv2D)           (None, 549, 549, 549)     2713158   
                                                                 
=================================================================
Total params: 5,749,128
Trainable params: 5,745,834
Non-trainable params: 3,294
_________________________________________________________________
</code></pre>
<p>One other thing that is interesting, if I change my model on the EC2 instance to:</p>
<pre class=""lang-py prettyprint-override""><code>inp = layers.Input(shape=(*SIZE, 1))

x = layers.Conv2D(filters=1, kernel_size=(5,5), padding=&quot;same&quot;, activation=&quot;relu&quot;)(inp)
x = layers.BatchNormalization()(x)


x = layers.Conv2D(filters=1, kernel_size=(3, 3), padding=&quot;same&quot;, activation=&quot;relu&quot;)(x)
x = layers.BatchNormalization()(x)


x = layers.Conv2D(filters=1, kernel_size=(1, 1), padding=&quot;same&quot;, activation=&quot;relu&quot;)(x)
x = layers.BatchNormalization()(x)

x = layers.Conv2D(filters=1, kernel_size=(3, 3), padding=&quot;same&quot;, activation=&quot;sigmoid&quot;)(x)

model = Model(inp, x)

model.compile(loss=tf.keras.losses.binary_crossentropy, optimizer=Adam())
</code></pre>
<p>My model.summary() output becomes:</p>
<pre class=""lang-py prettyprint-override""><code>Model: &quot;model_2&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_3 (InputLayer)        [(None, 549, 549, 1)]     0         
                                                                 
 conv2d_8 (Conv2D)           (None, 549, 549, 1)       26        
                                                                 
 batch_normalization_6 (Batc  (None, 549, 549, 1)      4         
 hNormalization)                                                 
                                                                 
 conv2d_9 (Conv2D)           (None, 549, 549, 1)       10        
                                                                 
 batch_normalization_7 (Batc  (None, 549, 549, 1)      4         
 hNormalization)                                                 
                                                                 
 conv2d_10 (Conv2D)          (None, 549, 549, 1)       2         
                                                                 
 batch_normalization_8 (Batc  (None, 549, 549, 1)      4         
 hNormalization)                                                 
                                                                 
 conv2d_11 (Conv2D)          (None, 549, 549, 1)       10        
                                                                 
=================================================================
Total params: 60
Trainable params: 54
Non-trainable params: 6
_________________________________________________________________
</code></pre>
<p>In the last model the shape is correct but the trainable parameters is very low.</p>
<p>Any ideas as to why the output shape is different and why this is happening with the filters?</p>",0,0,2022-06-17 02:45:17.067000 UTC,,2022-06-17 04:19:15.147000 UTC,0,python|tensorflow|keras|amazon-ec2|amazon-sagemaker,39,2020-10-02 00:29:37.080000 UTC,2022-09-02 11:23:54.887000 UTC,,131,21,0,19,,,,,,['amazon-sagemaker']
Access databricks secrets in pyspark/python job,"<p>Databricks secrets can be accessed within notebooks using dbutils, however since dbutils is not available outside notebooks how can one access secrets in pyspark/python jobs, especially if they are run using mlflow.</p>

<p>I have already tried <a href=""https://stackoverflow.com/questions/51885332/how-to-load-databricks-package-dbutils-in-pyspark?rq=1"">How to load databricks package dbutils in pyspark</a></p>

<p>which does not work for remote jobs or mlflow project runs.</p>",1,0,2020-06-08 21:40:46.997000 UTC,,,1,databricks|azure-databricks|mlflow,1369,2017-06-15 11:22:56.727000 UTC,2021-03-07 17:34:22.217000 UTC,"Mumbai, Maharashtra, India",481,16,0,49,,,,,,['mlflow']
ValueError while deploying tensorflow model to Amazon SageMaker,"<p>I want to deploy my trained tensorflow model to the amazon sagemaker, I am following the official guide here: <a href=""https://aws.amazon.com/blogs/machine-learning/deploy-trained-keras-or-tensorflow-models-using-amazon-sagemaker/"" rel=""nofollow noreferrer"">https://aws.amazon.com/blogs/machine-learning/deploy-trained-keras-or-tensorflow-models-using-amazon-sagemaker/</a> to deploy my model using jupyter notebook.</p>

<p>But when I try to use code:</p>

<pre class=""lang-py prettyprint-override""><code>predictor = sagemaker_model.deploy(initial_instance_count=1, instance_type='ml.t2.medium')
</code></pre>

<p>It gives me the following error message:</p>

<blockquote>
  <p>ValueError: Error hosting endpoint sagemaker-tensorflow-2019-08-07-22-57-59-547: Failed Reason:  The image '520713654638.dkr.ecr.us-west-1.amazonaws.com/sagemaker-tensorflow:1.12-cpu-py3 ' does not exist.</p>
</blockquote>

<p>I think the tutorial does not tell me to create an image, and I do not know what to do.</p>

<pre><code>import boto3, re
from sagemaker import get_execution_role

role = get_execution_role()

# make a tar ball of the model data files
import tarfile
with tarfile.open('model.tar.gz', mode='w:gz') as archive:
    archive.add('export', recursive=True)

# create a new s3 bucket and upload the tarball to it
import sagemaker

sagemaker_session = sagemaker.Session()
inputs = sagemaker_session.upload_data(path='model.tar.gz', key_prefix='model')

from sagemaker.tensorflow.model import TensorFlowModel
sagemaker_model = TensorFlowModel(model_data = 's3://' + sagemaker_session.default_bucket() + '/model/model.tar.gz',
                                  role = role,
                                  framework_version = '1.12',
                                  entry_point = 'train.py',
                                  py_version='py3')

%%time
#here I fail to deploy the model and get the error message
predictor = sagemaker_model.deploy(initial_instance_count=1,
                                   instance_type='ml.m4.xlarge')

</code></pre>",1,1,2019-08-07 23:45:12.390000 UTC,,2019-08-08 00:58:16.773000 UTC,0,python|amazon-web-services|tensorflow|amazon-s3|amazon-sagemaker,1364,2019-08-02 20:36:37.783000 UTC,2019-08-11 23:00:25.980000 UTC,,1,0,0,0,,,,,,['amazon-sagemaker']
Store scaler with mlflow keras-model,"<p>We are looking into using mlflow to handle our keras models, and we would also like to use mlflow to deploy the models in e.g azure. But the models require some simple preprocessing of the data, in our case the use of a minmax scaler. For the deployed models to answer correctly they must apply the scaler on the input (and inverse on the output). I have not found any way to include the scaling in the persisted/deployed models. Have I overlooked something, or is it not possible?</p>",1,0,2018-09-17 16:26:20.813000 UTC,,,4,python|keras|mlflow,486,2013-03-06 13:18:07.873000 UTC,2022-09-24 20:35:16.157000 UTC,,199,13,0,95,,,,,,['mlflow']
Can we add our own environmental labels in MLflow?,"<p>There are certain default inbuilt system tags in mlflow to set certain values. But is possible to set our own environment labels in mlflow like training, testing etc.?</p>",1,1,2022-08-10 17:59:52.420000 UTC,,,0,environment-variables|mlflow,27,2022-08-10 17:24:01.717000 UTC,2022-09-23 07:38:08.560000 UTC,Rajapalayam,1,0,0,0,,,,,,['mlflow']
Xgboost Amazon Sagemaker grid search alternative,"<p>I am using Amazon Sagemaker to run a xgboost model in order to bet the best hyperparameter combination. I have to use the sagemaker implementation and not the notebook alternative to test if it runs faster than a gridsearch.
My problem is how can I make this work in a loop. Any Ideas? 
My understanding is that I have to code numerous jobs with different combinations.
I tried this as a test:</p>

<pre><code>for i in range (1,3):
    for j in range (13,15):
        job_name = 'regression' + '-'+str(i) +""-""+str(j)+""-"" +strftime(""%Y-%m-%d-%H-%M-%S"", gmtime())

        job_name_params = copy.deepcopy(parameters_xgboost)
        job_name_params['TrainingJobName'] = job_name
        job_name_params['OutputDataConfig']['S3OutputPath']= "".....""
        job_name_params['HyperParameters']['objective'] = ""reg:linear""
        job_name_params['HyperParameters']['silent'] = ""0""
        job_name_params['HyperParameters']['max_depth'] = str(i)
        job_name_params['HyperParameters']['min_child_weight'] = str(j)
        job_name_params['HyperParameters']['eta'] = ""0.01""
        job_name_params['HyperParameters']['num_round'] = ""1000""
        job_name_params['HyperParameters']['subsample'] = ""0.5""
        job_name_params['HyperParameters']['colsample_bytree'] = ""0.5""

        sm = boto3.Session().client('.....')


        sm.create_training_job(**job_name_params)
        sm.get_waiter('training_job_completed_or_stopped').wait(TrainingJobName=job_name)
        status = sm.describe_training_job(TrainingJobName=job_name)['TrainingJobStatus']
        print(""Training job ended with status: "" + status)
</code></pre>

<p>parameters_xgboost is how Sagemaker reads basic info and list of hyper params.</p>

<p>The good thing is that it works. The bad this is that this trains the models one at a time. I would like all of these combinations to run a the same time.
How can I do that?</p>",1,0,2018-05-25 12:03:05.780000 UTC,,,2,python-3.x|amazon-sagemaker,667,2015-05-26 22:53:10.120000 UTC,2019-06-03 07:51:39.510000 UTC,,455,7,0,58,,,,,,['amazon-sagemaker']
is it possible to use service principal for authentication to access ml_client?,"<p>The documentation <a href=""https://docs.microsoft.com/en-us/azure/machine-learning/tutorial-pipeline-python-sdk"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/machine-learning/tutorial-pipeline-python-sdk</a> shows that you can use DefaultAzureCredential and InteractiveBrowserCredential for
ml_client. is there a way I can use service principal instead?</p>
<pre><code>#code from docs
# Handle to the workspace
from azure.ai.ml import MLClient

# Authentication package
from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential

try:
    credential = DefaultAzureCredential()
    # Check if given credential can get token successfully.
    credential.get_token(&quot;https://management.azure.com/.default&quot;)
except Exception as ex:
    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work
    credential = InteractiveBrowserCredential()

# Get a handle to the workspace
ml_client = MLClient(
    credential=credential,
    subscription_id=&quot;&lt;SUBSCRIPTION_ID&gt;&quot;,
    resource_group_name=&quot;&lt;RESOURCE_GROUP&gt;&quot;,
    workspace_name=&quot;&lt;AML_WORKSPACE_NAME&gt;&quot;,
)
</code></pre>",0,0,2022-07-27 15:26:01.283000 UTC,,,1,python|azure|azure-machine-learning-service|azureml-python-sdk|azuremlsdk,129,2020-01-14 14:47:36.570000 UTC,2022-09-21 12:27:07.623000 UTC,,129,7,0,17,,,,,,['azure-machine-learning-service']
Can't log MLflow artifacts to S3 with docker-based tracking server,"<p>I'm trying to set up a simple MLflow tracking server with docker that uses a mysql backend store and S3 bucket for artifact storage.  I'm using a simple docker-compose file to set this up on a server and supplying all of the credentials through a .env file.  When I try to run the sklearn_elasticnet_wine example from the mlflow repo here: <a href=""https://github.com/mlflow/mlflow/tree/master/examples/sklearn_elasticnet_wine"" rel=""nofollow noreferrer"">https://github.com/mlflow/mlflow/tree/master/examples/sklearn_elasticnet_wine</a> using<code>TRACKING_URI = &quot;http://localhost:5005</code> from the machine hosting my tracking server, the run fails with the following error: <code>botocore.exceptions.NoCredentialsError: Unable to locate credentials</code>.  I've verified that my environment variables are correct and available in my mlflow_server container. The runs show up in my backend store so the run only seems to be failing at the artifact logging step.  I'm not sure why this isn't working.  I've seen a examples of how to set up a tracking server online, including: <a href=""https://towardsdatascience.com/deploy-mlflow-with-docker-compose-8059f16b6039"" rel=""nofollow noreferrer"">https://towardsdatascience.com/deploy-mlflow-with-docker-compose-8059f16b6039</a>.  Some use minio also but others just specify their s3 location as I have.  I'm not sure what I'm doing wrong at this point. Do I need to explicitly set the ARTIFACT_URI as well?  Should I be using Minio?  Eventually, I'll be logging runs to the server from another machine, hence the nginx container.  I'm pretty new to all of this so I'm hoping it's something really obvious and easy to fix but so far the Google has failed me.  TIA.</p>
<pre><code>version: '3'

services:
  app: 
    restart: always
    build: ./mlflow
    image: mlflow_server
    container_name: mlflow_server
    expose:
      - 5001
    ports:
      - &quot;5001:5001&quot;
    networks:
      - internal 
    environment:
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - AWS_DEFAULT_REGION=${AWS_DEFAULT_REGION}
      - AWS_S3_BUCKET=${AWS_S3_BUCKET}
      - DB_USER=${DB_USER}
      - DB_PASSWORD=${DB_PASSWORD}
      - DB_PORT=${DB_PORT}
      - DB_NAME=${DB_NAME}
    command: &gt;
      mlflow server 
      --backend-store-uri mysql+pymysql://${DB_USER}:${DB_PASSWORD}@${DB_HOST}:${DB_PORT}/${DB_NAME} 
      --default-artifact-root s3://${AWS_S3_BUCKET}/mlruns/
      --host 0.0.0.0 
      --port 5001

  nginx: 
    restart: always
    build: ./nginx
    image: mlflow_nginx
    container_name: mlflow_nginx
    ports:
      - &quot;5005:80&quot; 
    networks:
      - internal 
    depends_on:
      - app

networks:
  internal:
    driver: bridge
</code></pre>",1,0,2022-05-11 18:40:36.513000 UTC,,2022-05-11 19:22:39.877000 UTC,2,docker|amazon-s3|docker-compose|boto3|mlflow,620,2021-12-16 00:24:08.310000 UTC,2022-09-23 00:00:12.383000 UTC,,51,0,0,3,,,,,,['mlflow']
AWS Kinesis real time video processing using sagemaker,"<p>I am working on a POC for video face recognition using AWS kinesis, sagemaker.
I have set up the environment to send my webcam video to kinesis. I am also working on a custom docker implementation of my face recognition code to be deployed in sagemaker. </p>

<p>I am stuck with few things here and need some help:
I will be using python to build my applications. </p>

<ol>
<li>How do i send kinesis video frames as input to sagemaker model as the input will be frame.</li>
<li>Once the above is done the custom sagemaker code will return the face detected as well as the bounding box. I can call this sagemaker end point from my custom chalice flask app to get the output but I am not sure how to get the frame back so that i can show it in my web app built using chalice-deployed in aws as well. The web app will stream back the video with bounding box.</li>
</ol>

<p>I do not want to use amazon rekognition as well as HLS because they will incur a  lot of cost.</p>

<p>Can anyone help me with the above problems or suggest a better approach to solve it. Language i can use is python.</p>",1,0,2019-08-28 11:39:02.900000 UTC,,2019-09-05 08:12:29.677000 UTC,0,python|video-streaming|face-recognition|amazon-kinesis|amazon-sagemaker,829,2016-04-15 12:35:40.773000 UTC,2022-08-01 16:20:32.270000 UTC,,13,0,0,7,,,,,,['amazon-sagemaker']
"can i use mlflow.set_tracking_uri() to Store artifacts in GCP bucket? if Yes, then how?",<p>I'm trying to implement above things in my project.. I have read some of blogs and mlflow documentation but unable to solve the issue. Can you please guide me about the same? Thanks</p>,0,0,2022-08-29 17:43:38.523000 UTC,,,0,mlflow,13,2021-12-21 07:44:00.790000 UTC,2022-09-23 08:03:09.817000 UTC,,175,2,0,6,,,,,,['mlflow']
Getting error: ModelError: Received server error (500) error when hitting sage maker endpoint,"<pre><code>`const callSageMaker = async () =&gt; { 

const client = new SageMakerRuntimeClient();
 let data = '0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0';
    const params = {
        EndpointName: process.env.SAGEMAKER_EP_NAME,
        ContentType: &quot;text/plain&quot;,
        Body:'bytes',
        data,
    };
    const command = new InvokeEndpointCommand(params);
    try {
        const data = await client.send(command);
        return data;
    } catch (error) {
        // error handling.
        console.log('error catch',{ error });
        return error;
    }
}`
</code></pre>
<p>I am hitting this sageMaker endpoint but getting this error</p>
<p>` {
error: ModelError: Received server error (500) from primary with message &quot;&lt;!DOCTYPE HTML PUBLIC &quot;-//W3C//DTD HTML 3.2 Final//EN&quot;&gt;</p>
<p>500 Internal Server Error</p>
  <h1>Internal Server Error</h1>
  <p>The server encountered an internal error and was unable to complete your request. Either the server is overloaded or there is an error in the application.</p>
    {  
    '$fault': 'client',
    '$metadata': {
      httpStatusCode: 424,
      extendedRequestId: undefined,
      cfId: undefined,
      attempts: 1,
      totalRetryDelay: 0
    },
    OriginalStatusCode: 500,
    OriginalMessage: '\n' +
      '500 Internal Server Error\n' +
      '<h1>Internal Server Error</h1>\n' +
      '<p>The server encountered an internal error and was unable to complete your request. Either the server is overloaded or there is an error in the application.</p>\n',
    ErrorCode: 'INTERNAL_FAILURE_FROM_MODEL'
  }
}`",1,0,2022-03-25 17:20:00.377000 UTC,,,0,amazon-sagemaker|aws-sdk-js|aws-sdk-nodejs,634,2019-08-05 12:34:53.547000 UTC,2022-09-23 11:17:02.027000 UTC,,5,6,0,3,,,,,,['amazon-sagemaker']
Difference between Sagemaker notebook instance and Training job submission,"<p>I am getting an error with a Sagemaker training job with the error message &quot;OverflowError: signed integer is greater than maximum&quot;. This is an image identification problem with code written in keras and tensorflow. The input for this is a large npy file stored in an s3 bucket.
The code works fine when run the Sagemaker notebook cells but errors out when submitted as a Training job using boto3 request.
I am using the same role in both places. what could be the cause for this error? I am using ml.g4dn.16xlarge instance in both cases</p>",2,0,2022-07-11 21:04:44.807000 UTC,,2022-07-11 21:13:35.380000 UTC,0,numpy|tensorflow|keras|conv-neural-network|amazon-sagemaker,52,2015-07-29 06:58:52.453000 UTC,2022-07-29 19:34:07.360000 UTC,,57,1,0,17,,,,,,['amazon-sagemaker']
How can dvc pipeline recognize when to use encoding pipeline while new data added for the modeling?,"<p>I have created separate pipelines for feature encoding and feature scaling in DVC.
Now, when I will input new data from my flask API, how these DVC pipelines will automatically run and encode and scale data for modelling?</p>",0,3,2022-05-31 17:24:05.533000 UTC,,,0,machine-learning|pipeline|mlops|dvc,25,2022-05-31 17:05:18.443000 UTC,2022-09-24 10:05:19.797000 UTC,,9,0,0,0,,,,,,['dvc']
Predictions into the future Azure Machine Learning Studio Designer,"<p>I am currently developing an automated mechanism where I use the Azure Machine Learning Designer (AMLD). During development i used an 80/20 Split to test the efficency of my predictions.
Now i want to go live but I've missed the point where i can actually predict into the future.</p>
<p>I currently get a prediction for the last 20% of my data so i can compare them to the actual data. How do i change it so that the prediction actually starts at the end of my data?</p>
<p>A part of my prediction process is attached:</p>
<p><a href=""https://i.stack.imgur.com/eh7Rv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eh7Rv.png"" alt=""enter image description here"" /></a></p>",1,5,2022-04-26 09:30:32.403000 UTC,,,0,prediction|forecasting|azure-machine-learning-studio,39,2020-06-15 13:06:57.127000 UTC,2022-09-22 19:00:57.357000 UTC,Germany,13,0,0,4,,,,,,['azure-machine-learning-studio']
How can I get Docker image tag info for azureml base images?,"<p>I am currently using the <strong>mcr.microsoft.com/azureml/base:latest</strong> docker image.</p>

<p>To access the list of all tags from <strong>mcr.microsoft.com/azureml/base</strong> I do a GET request to
<a href=""http://mcr.microsoft.com/v2/azureml/base/tags/list"" rel=""nofollow noreferrer"">http://mcr.microsoft.com/v2/azureml/base/tags/list</a> according to instructions from <a href=""https://stackoverflow.com/a/50326027/4039897"">here</a>, and what I see is.</p>

<pre><code>{
  ""name"": ""azureml/base"",
  ""tags"": [
    ""0.1"",
    ""0.1.1"",
    ""0.1.2"",
    ""0.1.3"",
    ""0.1.4"",
    ""0.1.5"",
    ""0.2.0"",
    ""0.2.1"",
    ""0.2.2"",
    ""0.2.3"",
    ""0.2.4"",
    ""0.4.0"",
    ""intelmpi"",
    ""intelmpi2018.3-ubuntu16.04"",
    ""latest"",
    ""openmpi"",
    ""openmpi3.1.2-ubuntu16.04""
  ]
}
</code></pre>

<p>I see that there are different version tags (check below), however I would like to know what the numbered tags mean, if they are supported and how frequently all tags get updated.</p>

<p>Thanks!</p>",2,0,2019-08-15 06:07:12.183000 UTC,0.0,2020-05-03 10:55:42.047000 UTC,1,azure-machine-learning-service,358,2014-09-14 14:48:11.817000 UTC,2022-03-01 18:30:27.267000 UTC,Brazil,300,38,1,30,,,,,,['azure-machine-learning-service']
Is it possible to perform local dev on a CPU-only machine on HF/sagemaker?,"<p>I'm trying to dev locally on <code>sagemaker.huggingface.HuggingFace</code> before moving to sagemaker for actual training. I set up a</p>
<p><code>HF_estimator = HuggingFace(entry_point='train.py', instance_type='local' ...)</code></p>
<p>And called <code>HF_estimator.fit()</code></p>
<p>In <code>train.py</code> im simply printing and exiting to see if it will work. However I ran into this:</p>
<pre><code>ValueError: Unsupported processor: cpu. You may need to upgrade your SDK version (pip install -U sagemaker) for newer processors. Supported processor(s): gpu.
</code></pre>
<p>Is it possible to bypass this for local development?</p>",1,0,2022-06-12 14:13:13.093000 UTC,,,0,amazon-sagemaker|huggingface-transformers|huggingface,133,2013-05-23 22:24:43.580000 UTC,2022-09-24 14:44:15.853000 UTC,"Portland, OR",5626,211,1,872,,,,,,['amazon-sagemaker']
How do I specify and install the latest version of PyTorch via Conda in AWS Sagemaker?,"<p>I'm attempting to use a recent version of PyTorch (1.7.0) in a Conda environment on Sagemaker by specifying the package version in an environment.yml file. However, I'm getting a ResolvePackageNotFound error. Note that I'm just working in a Jupyter notebook with a kernel corresponding to this Conda environment. I'm not using a deep learning image.</p>
<p>Steps to reproduce:</p>
<p>Save the code below as a .yml file, navigate to where this .yml file is stored, and run <code>conda env create environment.yml</code>.</p>
<pre><code>name: test_env
dependencies:
  - numpy
  - pandas
  - pytorch&gt;=1.7.0
  - torchvision
  - scipy
  - ipykernel
  - torchvision
</code></pre>
<p>I've tried this on instances of various types (ml.p2.xlarge, ml.p3.2xlarge, and ml.p3.8xlarge) and have gotten the same error each time. I tried it with conda version 4.8.3 and 4.9.2 as well. If I specify <code>pytorch&gt;=1.5.0</code>, I'm able to create the environment successfully.</p>
<p>Does anyone have any idea why I can't create an environment successfully with more recent versions of PyTorch? Based on <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/nbi-add-external.html"" rel=""nofollow noreferrer"">this documentation</a>, I wonder if Sagemaker has preinstalled certain versions of PyTorch, and something is going awry when I attempt to use a more recent version.</p>",1,0,2020-12-10 23:46:26.450000 UTC,,2020-12-11 15:49:27.053000 UTC,2,python|pytorch|conda|amazon-sagemaker,1248,2020-12-10 23:35:40.513000 UTC,2021-04-02 00:02:50.423000 UTC,,21,0,0,4,,,,,,['amazon-sagemaker']
Real Time Inference Pipeline without Designer,"<p>i am currently getting into Azure Machine Learning. I am trying out the learning path for Data Scientists. In that learning path, the Designer is introduced, where Pipelines are being published to be consumed as a real time inference pipeline.</p>
<p>Since I dont want to use the Designer all the time I want to do the same in python. All the tutorials on Microsoft Learn only show how to deploy a single model as a service (e.g. <a href=""https://docs.microsoft.com/en-us/learn/modules/register-and-deploy-model-with-amls/"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/learn/modules/register-and-deploy-model-with-amls/</a>). In those tutorials pipelines are only created to train models, but not for predictions on incoming data (<a href=""https://docs.microsoft.com/en-us/learn/modules/create-pipelines-in-aml/"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/learn/modules/create-pipelines-in-aml/</a>). An entry script is used to load incoming data into the pretrained model. For me it is not clear how to implement pipeline steps inside this entry script. I checked online, but I couldnt find any explanation how to do this in a satisfying way. Is there a tutorial of any sort out there to do this?</p>
<p>I am thinking about those pipeline steps, because I would like to do some preprocessing to incoming data using the same scaler I used for training my model. In my eyes loading the training database every time new data is coming in to fit a scaler to the training dataset, seems way of an overload for (near-)real-time models.</p>
<p>I am guessing there is an easy way to do all this, but using the resources I found online, I couldnt come up with a suitable solution for this.</p>
<p>Best regards and Thank you in advance!</p>",2,0,2021-03-29 08:22:16.123000 UTC,,2021-03-30 07:08:31.887000 UTC,3,python|azure|azure-machine-learning-studio|azure-machine-learning-service,393,2021-03-29 07:56:12.580000 UTC,2022-01-11 08:51:48.023000 UTC,,31,1,0,1,,,,,,"['azure-machine-learning-studio', 'azure-machine-learning-service']"
Evaluate Model Node in Azure ML Studio does not take all the rows of the dataset in confusion matrix,"<p>I have <a href=""https://archive.ics.uci.edu/ml/datasets/APS+Failure+at+Scania+Trucks"" rel=""nofollow noreferrer"">this dataset</a> in which the positive class consists of component failures for a specific component of the APS system.
I am doing Predictive Maintenance using Microsoft Azure Machine Learning Studio.
As you can see from the pictures below, I am using 4 algorithm: Logistic Regression, Random Forest, Decision Tree and SVM. And you can see that the Output dataset in the score model node consists of 16k rows. However, when I see the output of the Evaluate Model, in the confusion matrix there are only 160 observations for the Logistic Regression, and the correct number, 16k for Random Forest. I have the same problem, only 160 observations in the models of Decision Tree and SVM. And the same problem is repeated in other experiments for example after feature selection, normalization etc.: some evaluate model does not use all the rows of the test dataset, and some other node does it.
How can I fix this problem? Because I am interested in the real number of false positive and false negatives.
<a href=""https://i.stack.imgur.com/PRkoQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PRkoQ.png"" alt=""Experiments"" /></a>
<a href=""https://i.stack.imgur.com/ln4LO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ln4LO.png"" alt=""16k rows"" /></a>
<a href=""https://i.stack.imgur.com/A1gH0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/A1gH0.png"" alt=""wrong confusion matrix"" /></a>
<a href=""https://i.stack.imgur.com/BkGi6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BkGi6.png"" alt=""correct confusion matrix"" /></a></p>",1,0,2021-01-13 16:19:14.090000 UTC,,,1,azure|classification|prediction|confusion-matrix|azure-machine-learning-studio,154,2019-06-02 14:04:11.307000 UTC,2022-09-24 15:00:18.637000 UTC,"Catania, CT, Italia",192,29,0,120,,,,,,['azure-machine-learning-studio']
How to input fsx for lustre to Amazon Sagemaker?,"<p>I am trying to set up Amazon sagemaker reading our dataset from our AWS Fsx for Lustre file system.</p>
<p>We are using the Sagemaker API, and previously we were reading our dataset from s3 which worked fine:</p>
<pre><code>estimator = TensorFlow(
   entry_point='model_script.py',  
   image_uri='some-repo:some-tag', 
   instance_type='ml.m4.10xlarge',
   instance_count=1,
   role=role,
   framework_version='2.0.0',
   py_version='py3',
   subnets=[&quot;subnet-1&quot;],
   security_group_ids=[&quot;sg-1&quot;, &quot;sg-2&quot;],
   debugger_hook_config=False,
  )
estimator.fit({
    'training': f&quot;s3://bucket_name/data/{hyperparameters['dataset']}/&quot;}
)
</code></pre>
<p>But now that I'm changing the input data source to Fsx Lustre file system, I'm getting an error that the file input should be s3:// or file://. I was following these <a href=""https://sagemaker.readthedocs.io/en/stable/overview.html"" rel=""nofollow noreferrer"">docs (fsx lustre)</a>:</p>
<pre><code>estimator = TensorFlow(
   entry_point='model_script.py',  
#    image_uri='some-docker:some-tag', 
   instance_type='ml.m4.10xlarge',
   instance_count=1,
   role=role,
   framework_version='2.0.0',
   py_version='py3',
   subnets=[&quot;subnet-1&quot;],
   security_group_ids=[&quot;sg-1&quot;, &quot;sg-2&quot;],
   debugger_hook_config=False,
  )
fsx_data_folder = FileSystemInput(file_system_id='fs-1',
                                    file_system_type='FSxLustre',
                                    directory_path='/fsx/data',
                                    file_system_access_mode='ro')
estimator.fit(f&quot;{fsx_data_folder}/{hyperparameters['dataset']}/&quot;)
</code></pre>
<p>Throws the following error:</p>
<pre><code>ValueError: URI input &lt;sagemaker.inputs.FileSystemInput object at 0x0000016A6C7F0788&gt;/dataset_name/ must be a valid S3 or FILE URI: must start with &quot;s3://&quot; or &quot;file://&quot;
</code></pre>
<p>Does anyone understand what I am doing wrong? Thanks in advance!</p>",1,0,2020-12-10 16:20:17.073000 UTC,,2022-03-21 23:48:16.650000 UTC,2,amazon-web-services|tensorflow|amazon-ec2|amazon-sagemaker|amazon-fsx,592,2020-06-03 17:23:09.660000 UTC,2021-12-06 14:07:17.903000 UTC,,53,3,0,6,,,,,,['amazon-sagemaker']
How to create model from best hyperparameter tuning job in Sagemaker?,"<p>I'm using AWS SageMaker to run hyperparameter tuning to optimize an XGBoost model. I'm able to do so using the example code below. My question is: How can I then take the best hyperparameter tuning job and create a model via code? I'm aware that in the AWS console I can see the best job and click on ""Create model"", but I want to be able to do so using Python code. </p>

<pre><code>sess = sagemaker.Session()
s3_input_train = sagemaker.s3_input(s3_data='s3://{}/{}/train'.format(bucket, prefix), content_type='csv')
s3_input_validation = sagemaker.s3_input(s3_data='s3://{}/{}/valid/'.format(bucket, prefix), content_type='csv')
xgb_cont = get_image_uri(region, 'xgboost', repo_version='0.90-1')
xgb = sagemaker.estimator.Estimator(xgb_cont, role, train_instance_count=1, train_instance_type='ml.m4.4xlarge',
                                    output_path='s3://{}/{}/output'.format(bucket, prefix), sagemaker_session=sess)
xgb.set_hyperparameters(eval_metric='rmse', objective='reg:squarederror', num_round=100)
hyperparameter_ranges = {'eta': ContinuousParameter(0, 1), 'min_child_weight': ContinuousParameter(1, 10),
                        'alpha': ContinuousParameter(0, 2), 'max_depth': IntegerParameter(1, 10)}
tuner = HyperparameterTuner(xgb, 
                            objective_metric_name='validation:rmse', 
                            objective_type='Minimize',
                            hyperparameter_ranges=hyperparameter_ranges, 
                            max_jobs=20, max_parallel_jobs=10)
tuner.fit({'train': s3_input_train, 'validation': s3_input_validation})
</code></pre>",1,1,2020-03-06 16:07:22.693000 UTC,,2020-03-06 17:10:17.390000 UTC,0,amazon-web-services|amazon-sagemaker|hyperparameters,575,2015-05-18 17:28:25.067000 UTC,2022-09-23 18:30:26.047000 UTC,,4631,424,5,333,,,,,,['amazon-sagemaker']
Error when loading ML model from the remote MLflow instance,"<p>I tried to load a model from the remote MLflow instance, using <code>load_model</code> function:</p>
<pre><code>import mlflow

model = mlflow.pyfunc.load_model(&quot;http://remote_IP_address:5000/runs:/&lt;run_id&gt;/model&quot;)
</code></pre>
<p>I found the run_id by using the REST API:</p>
<pre><code>import requests

requests.get(&quot;http://remote_IP_address:5000/api/2.0/preview/mlflow/runs/search&quot;,params={&quot;experiment_ids&quot;:[0,1]})
</code></pre>
<p>But I am receiving an error:</p>
<pre><code>ValueError: not enough values to unpack (expected 2, got 1)
</code></pre>
<p>I suppose the error is in the URI that I am using. Can you tell me the correct way to access the remote Mlflow instance and load the model?</p>
<p>p.s.
I also tried:</p>
<pre><code>mlflow.pyfunc.load_model(&quot;http://remote_Ip_address:5000/models:/&lt;model_name&gt;/production&quot;)
</code></pre>
<p>but I received the same error.</p>
<p>Thank you in advance!</p>",1,0,2022-04-08 10:44:42.147000 UTC,,2022-04-08 10:52:44.790000 UTC,0,python|jupyter-notebook|mlflow,286,2018-06-07 09:58:08.027000 UTC,2022-09-22 14:54:01.063000 UTC,,499,167,1,59,,,,,,['mlflow']
Managing data drift when using w2vec embeddings on VertexAI,"<p>So I am looking into moving my models from GCP's AI Platform to Vertex AI, my main motivation for it being the fact that Vertex AI has automatic email notifications when your data skews or drifts (<a href=""https://cloud.google.com/vertex-ai/docs/model-monitoring/using-model-monitoring"" rel=""nofollow noreferrer"">https://cloud.google.com/vertex-ai/docs/model-monitoring/using-model-monitoring</a>).</p>
<p>So if you start receiving dodgy data that doesn't resemble the training set, they send you an email telling you which features (columns) of the data you are trying to predict are drifting away from your training data.</p>
<p>However, I am unsure how this would work in my case since my data is text data that has been encoded using word2vec embeddings. Therefore, my dataset has 300 columns but I don't know what feature each of the columns refers to.</p>
<p>Is this sort of data drift analysis still useful in my particular case?</p>
<p>Thank you</p>",1,2,2022-05-20 06:47:31.863000 UTC,,,1,machine-learning|google-cloud-platform|word2vec|google-cloud-vertex-ai,133,2021-02-03 09:38:25.200000 UTC,2022-09-22 13:24:14.067000 UTC,"London, Reino Unido",57,8,0,14,,,,,,['google-cloud-vertex-ai']
MLFlow Projects can't find conda executable,"<p>I am following the tutorial on MLFlow website. I was able to run the train.py and mlflow ui worked fine. Packaging the project tries to use env variable  MLFLOW_CONDA_HOME but can't find conda.
I have tried setting the variable to the path of anaconda3/condabin but it doesn't seem to find my executable. This is the error I get:
 ERROR mlflow.cli: === Could not find Conda executable at /anaconda3/condabin\bin/conda. Ensure Conda is installed as per the inst
ructions at <a href=""https://conda.io/docs/user-guide/install/index.html"" rel=""nofollow noreferrer"">https://conda.io/docs/user-guide/install/index.html</a>. You can also configure MLflow to look for a specific Conda executable by setting the MLFLOW_CONDA_HOME environment variable
 to the path of the Conda executable ===</p>

<p>Adding \bin/conda at the end of my path seems to be the problem, I am not sure why mlflow is doing it. I even tried setting it to my python.exe in my conda env, but no luck. I can't find bin/conda folder in my Anaconda folder anywhere.</p>",7,2,2019-12-14 00:47:58.197000 UTC,1.0,,2,python|anaconda|conda|mlflow,3001,2013-03-25 16:26:12.147000 UTC,2020-08-11 23:36:17.443000 UTC,,31,0,0,7,,,,,,['mlflow']
YoloV5 killed at first epoch,"<p>I'm using a virtual machine on Windows 10 with this config:</p>
<pre><code>Memory 7.8 GiB
Processor Intel® Core™ i5-6600K CPU @ 3.50GHz × 3
Graphics llvmpipe (LLVM 11.0.0, 256 bits)
Disk Capcity 80.5 GB
OS Ubuntu 20.10 64 Bit
Virtualization Oracle
</code></pre>
<p>I installed docker for Ubuntu as described in <a href=""https://docs.docker.com/engine/install/ubuntu/"" rel=""nofollow noreferrer"">the official documentation</a>.<br>
I pulled the docker image as described on the <a href=""https://github.com/ultralytics/yolov5/wiki/Docker-Quickstart"" rel=""nofollow noreferrer"">yolo github section for docker</a>.<br>
Since I have no NVIDIA GPU I could not install a driver or CUDA.
I pulled the aquarium from <a href=""https://public.roboflow.com/object-detection/aquarium"" rel=""nofollow noreferrer"">roboflow</a> and installed it on a folde aquarium.
I ran this command to start the image and have my aquarium folder mounted</p>
<pre><code>sudo docker run --ipc=host -it -v &quot;$(pwd)&quot;/Desktop/yolo/aquarium:/usr/src/app/aquarium ultralytics/yolov5:latest
</code></pre>
<p>And was greeted with this banner</p>
<blockquote>
<h1>=============
== PyTorch ==</h1>
<p>NVIDIA Release 21.03 (build 21060478) PyTorch Version 1.9.0a0+df837d0</p>
<p>Container image Copyright (c) 2021, NVIDIA CORPORATION.  All rights
reserved.</p>
<p>Copyright (c) 2014-2021 Facebook Inc. Copyright (c) 2011-2014 Idiap
Research Institute (Ronan Collobert) Copyright (c) 2012-2014 Deepmind
Technologies    (Koray Kavukcuoglu) Copyright (c) 2011-2012 NEC
Laboratories America (Koray Kavukcuoglu) Copyright (c) 2011-2013 NYU<br />
(Clement Farabet) Copyright (c) 2006-2010 NEC Laboratories America
(Ronan Collobert, Leon Bottou, Iain Melvin, Jason Weston) Copyright
(c) 2006      Idiap Research Institute (Samy Bengio) Copyright (c)
2001-2004 Idiap Research Institute (Ronan Collobert, Samy Bengio,
Johnny Mariethoz) Copyright (c) 2015      Google Inc. Copyright (c)
2015      Yangqing Jia Copyright (c) 2013-2016 The Caffe contributors
All rights reserved.</p>
<p>NVIDIA Deep Learning Profiler (dlprof) Copyright (c) 2021, NVIDIA
CORPORATION.  All rights reserved.</p>
<p>Various files include modifications (c) NVIDIA CORPORATION.  All
rights reserved.</p>
<p>This container image and its contents are governed by the NVIDIA Deep
Learning Container License. By pulling and using the container, you
accept the terms and conditions of this license:
<a href=""https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license"" rel=""nofollow noreferrer"">https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license</a></p>
<p>WARNING: The NVIDIA Driver was not detected.  GPU functionality will
not be available.    Use 'nvidia-docker run' to start this container;
see    <a href=""https://github.com/NVIDIA/nvidia-docker/wiki/nvidia-docker"" rel=""nofollow noreferrer"">https://github.com/NVIDIA/nvidia-docker/wiki/nvidia-docker</a> .</p>
<p>NOTE: MOFED driver for multi-node communication was not detected.
Multi-node communication performance may be reduced.</p>
</blockquote>
<p>So no error there.<br>
I installed pip and with pip wandb I added wandb. I used <code>wandb login</code> and set my API key.<br><br>
I ran following command:</p>
<pre><code># python train.py --img 640 --batch 16 --epochs 10 --data ./aquarium/data.yaml --weights yolov5s.pt --project ip5 --name aquarium5 --nosave --cache
</code></pre>
<p>And received this output:</p>
<pre><code>github: skipping check (Docker image)
YOLOv5  v5.0-14-g238583b torch 1.9.0a0+df837d0 CPU

Namespace(adam=False, artifact_alias='latest', batch_size=16, bbox_interval=-1, bucket='', cache_images=True, cfg='', data='./aquarium/data.yaml', device='', entity=None, epochs=10, evolve=False, exist_ok=False, global_rank=-1, hyp='data/hyp.scratch.yaml', image_weights=False, img_size=[640, 640], label_smoothing=0.0, linear_lr=False, local_rank=-1, multi_scale=False, name='aquarium5', noautoanchor=False, nosave=True, notest=False, project='ip5', quad=False, rect=False, resume=False, save_dir='ip5/aquarium5', save_period=-1, single_cls=False, sync_bn=False, total_batch_size=16, upload_dataset=False, weights='yolov5s.pt', workers=8, world_size=1)
tensorboard: Start with 'tensorboard --logdir ip5', view at http://localhost:6006/
hyperparameters: lr0=0.01, lrf=0.2, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0
wandb: Currently logged in as: pebs (use `wandb login --relogin` to force relogin)
wandb: Tracking run with wandb version 0.10.26
wandb: Syncing run aquarium5
wandb: ⭐️ View project at https://wandb.ai/pebs/ip5
wandb:  View run at https://wandb.ai/pebs/ip5/runs/1c2j80ii
wandb: Run data is saved locally in /usr/src/app/wandb/run-20210419_102642-1c2j80ii
wandb: Run `wandb offline` to turn off syncing.

Overriding model.yaml nc=80 with nc=7

                 from  n    params  module                                  arguments                     
  0                -1  1      3520  models.common.Focus                     [3, 32, 3]                    
  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                
  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   
  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               
  4                -1  1    156928  models.common.C3                        [128, 128, 3]                 
  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              
  6                -1  1    625152  models.common.C3                        [256, 256, 3]                 
  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              
  8                -1  1    656896  models.common.SPP                       [512, 512, [5, 9, 13]]        
  9                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          
 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              
 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          
 12           [-1, 6]  1         0  models.common.Concat                    [1]                           
 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          
 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              
 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          
 16           [-1, 4]  1         0  models.common.Concat                    [1]                           
 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          
 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              
 19          [-1, 14]  1         0  models.common.Concat                    [1]                           
 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          
 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              
 22          [-1, 10]  1         0  models.common.Concat                    [1]                           
 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          
 24      [17, 20, 23]  1     32364  models.yolo.Detect                      [7, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]
[W NNPACK.cpp:80] Could not initialize NNPACK! Reason: Unsupported hardware.
Model Summary: 283 layers, 7079724 parameters, 7079724 gradients, 16.4 GFLOPS

Transferred 356/362 items from yolov5s.pt
Scaled weight_decay = 0.0005
Optimizer groups: 62 .bias, 62 conv.weight, 59 other
train: Scanning '/usr/src/app/aquarium/train/labels.cache' images and labels... 448 found, 0 missing, 1 empty, 0 corrupted: 100%|█| 448/448 [00:00&lt;?, ?
train: Caching images (0.4GB): 100%|████████████████████████████████████████████████████████████████████████████████| 448/448 [00:01&lt;00:00, 313.77it/s]
val: Scanning '/usr/src/app/aquarium/valid/labels.cache' images and labels... 127 found, 0 missing, 0 empty, 0 corrupted: 100%|█| 127/127 [00:00&lt;?, ?it
val: Caching images (0.1GB): 100%|██████████████████████████████████████████████████████████████████████████████████| 127/127 [00:00&lt;00:00, 141.31it/s]
Plotting labels... 

autoanchor: Analyzing anchors... anchors/target = 5.17, Best Possible Recall (BPR) = 0.9997
Image sizes 640 train, 640 test
Using 3 dataloader workers
Logging results to ip5/aquarium5
Starting training for 10 epochs...

     Epoch   gpu_mem       box       obj       cls     total    labels  img_size
  0%|                                                                                                                           | 0/28 [00:00&lt;?, ?it/s]Killed
root@cf40a6498016:~# /opt/conda/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 6 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
</code></pre>
<p>From this output I would think that there were 0 epochs completed.<br>
My data.yaml contains this code:</p>
<pre><code>train: /usr/src/app/aquarium/train/images
val: /usr/src/app/aquarium/valid/images

nc: 7
names: ['fish', 'jellyfish', 'penguin', 'puffin', 'shark', 'starfish', 'stingray']
</code></pre>
<p><a href=""https://wandb.ai/"" rel=""nofollow noreferrer"">wandb.ai</a> does not display any metrics, but I have the files config.yaml, requirements.txt, wandb-metadata.json and wandb-summary.json.</p>
<p>Why am I not getting any output?<br>
Has there in fact be no training at all?<br>
If there was a training, how can I use my model?</p>",2,1,2021-04-19 10:49:10.100000 UTC,,,2,python|docker|pytorch|yolov5|wandb,1939,2015-05-06 16:48:35.943000 UTC,2022-08-26 04:23:47.213000 UTC,Switzerland,2006,255,4,230,,,,,,['wandb']
AWS Sagemaker Ground Truth - Validating Annotations and Removing Bad Labelers,"<p>I've been working on named entity recognition labeling jobs for mechanical turk workers to label a named entity dataset that I've compiled. However, when running tests on Ground Truth, the results were not adequate, as the Turk workers were either completely missing the labels, or intentionally labeling poorly and quickly to complete tasks quickly. Normally, when setting up a mechanical turk task on the standard mechanical turk interface, I can require qualifications or request to only have mechanical turk master workers on the project. However, this doesn't seem to be available when operating from Sagemaker and using Ground Truth to create a labeling job. How can I improve the performance of the turkers, or perhaps stop a turk worker from completing my job if their performance is too low? <a href=""https://i.stack.imgur.com/Tl1Po.png"" rel=""nofollow noreferrer"">An example bad response from mechanical turk workers, where they label named entities as always Person on the first character. Probably botted turk accounts.</a></p>",0,1,2022-08-09 18:20:48.553000 UTC,,,0,amazon-web-services|amazon-sagemaker|amazon-ground-truth,21,2021-08-04 17:11:25.680000 UTC,2022-09-24 05:01:04.740000 UTC,,1,0,0,4,,,,,,['amazon-sagemaker']
Vertex AI scheduled executor container updates,"<p>I have a Vertex AI scheduled executor which runs custom jobs. Runs are going fine and trains the model, but we often update helper classes in our project (our notebook instance has a GIT connection). These classes need to be updated in the Docker container as well. Is there a way to automatically point the schedule to the latest docker container in the artifact registry?</p>",0,0,2022-09-16 09:46:56.923000 UTC,,,0,docker|google-cloud-platform|google-cloud-vertex-ai,10,2020-04-21 08:20:19.480000 UTC,2022-09-21 12:47:49.243000 UTC,,25,0,0,3,,,,,,['google-cloud-vertex-ai']
Azure: how to schedule AzureML scripts taking 1-5 days to complete?,"<p>I have AzureML scripts that take about 1-5 days to complete. All input data comes from Azure SQL and is stored to Azure SQL. The input data is below 10GB.</p>

<p><strong>What are preferred methods to schedule such scripts inside Azure?</strong></p>",1,0,2017-03-13 16:30:37.617000 UTC,1.0,2017-03-13 17:07:19.323000 UTC,1,azure|cron|azure-sql-database|scheduling|azure-machine-learning-studio,140,2009-08-27 11:33:59.053000 UTC,2022-05-27 10:56:29.307000 UTC,,48616,1240,41,3348,,,,,,['azure-machine-learning-studio']
How does one invert an encoded prediction in Keras for model serving?,"<p>I have a Keras model in which i have successfully added a <code>StringLookUp</code> pre-processing step as part of the model definition. This is generally a good practice because i can then feed it the raw data to get back a prediction.</p>
<p>I am feeding the model string words that are mapped to an integer. The Y values are also string words that have been mapped to an integer.</p>
<p>Here is the implementation of the encoder and decoders:</p>
<pre><code>#generate the encoder and decoders
encoder = tf.keras.layers.StringLookup(vocabulary=vocab, )
decoder = tf.keras.layers.StringLookup(vocabulary=vocab, output_mode=&quot;int&quot;, invert=True)
</code></pre>
<p>Here is the some of the code that makes the inference model</p>
<pre><code># For inference, you can export a model that accepts strings as input
inputs = Input(shape=(6,), dtype=&quot;string&quot;)
x = encoder(inputs)
outputs = keras_model(x)
inference_model = Model(inputs, outputs)

inference_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])  
inference_model.summary()
</code></pre>
<p>The <code>encoder</code> above is just a function that implements <code>tf.keras.layers.StringLookup</code></p>
<p>Now, inside the notebook, I can easily convert the predictions back to the Original String representations by using a <code>decoder</code> which implements the reverse of <code>StringLookUp</code>.</p>
<p><em><strong>Here's my problem</strong></em>
While this works fine inside the notebook, this isn't very practical for deploying the model as a REST API because the calling program has no way of knowing how the encoded integer maps back to the original string representation.</p>
<p><em><strong>So the question is what strategy should I use to implement the keras predict so that it returns the original string which I can then serialize using mlflow &amp; cloudpickle to deploy it as a servable model in databricks</strong></em></p>
<p>Any guidance would be very much appreciated. I've seen a lot of example of Keras, but none that show how to do enact this kind of behavior for model deployment.</p>",1,1,2022-03-25 05:35:26.013000 UTC,,,0,tensorflow|keras|deep-learning|mlflow,176,2015-03-27 21:44:36.943000 UTC,2022-09-24 05:46:47.850000 UTC,,61,0,0,18,,,,,,['mlflow']
how to save mlflow metrics and paramters to an s3 bucket without a server?,"<p>I want to save the parameters and metrics gotten from mlflow into an s3 bucket. Usually I get these from setting the <code>tracking_uri</code> in mlflow and that saves it on a server but I can't have a server in this case(was told no) and just want to store my parameters and metrics on the s3 bucket in the same manner as it would using the <code>tracking_uri</code>.</p>
<p>I can store the artifacts on the s3 bucket without issue but not the params/metrics.</p>
<p>Here is some code:</p>
<pre class=""lang-py prettyprint-override""><code>def mlflow_testing():
    
    tracking_uri =  &quot;s3://bucket_name/mlflow/&quot;,
    experiment_name = &quot;test&quot;,
    artifact_uri= &quot;s3://bucket_name/mlflow/&quot;
    
    mlflow.set_tracking_uri(tracking_uri)
    mlflow.create_experiment(experiment_name, artifact_uri)
    mlflow.set_experiment(experiment_name)
    
    with mlflow.start_run() as run:
        mlflow.log_param(&quot;test1&quot;, 0)
        mlflow.log_metric(&quot;test2&quot;, 1)
    
        with open(&quot;test.txt&quot;, &quot;w&quot;) as f:
            f.write(&quot;this is an artifact&quot;)
    
        mlflow.log_artifact(&quot;test.txt&quot;)
        mlflow.end_run()
</code></pre>
<p>This is capable of storing the artifact text file on the s3 bucket(so long as I make the uri a local path like <code>local_data/mlflow</code> instead of the s3 bucket).</p>
<p>Setting the s3 bucket for the <code>tracking_uri</code> results in this error:</p>
<pre><code>mlflow.tracking.registry.UnsupportedModelRegistryStoreURIException:
Model registry functionality is unavailable; got unsupported URI
's3://bucket_location/mlflow/' for model registry data storage.
Supported URI schemes are: ['', 'file', 'databricks', 'http', 'https',
'postgresql', 'mysql', 'sqlite', 'mssql']. See
https://www.mlflow.org/docs/latest/tracking.html#storage for how to
run an MLflow server against one of the supported backend storage
locations.
</code></pre>
<p>Does anyone have advice on getting around this without setting up a server? I just want those metrics and params.</p>",1,1,2022-05-14 08:41:29.357000 UTC,1.0,2022-05-14 14:14:43.397000 UTC,2,python|amazon-s3|amazon-sagemaker|mlflow,818,2021-03-17 15:21:32.347000 UTC,2022-07-14 10:53:41.117000 UTC,,21,0,0,12,,,,,,"['mlflow', 'amazon-sagemaker']"
How to create a serverless endpoint in aws sagemaker?,"<p>I am recreating an endpoint currently working in SageMaker for inference by a serverless endpoint. I am using one of the AWS maintained baseimages: <code>763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-inference:1.5-cpu-py3</code>.</p>
<p>Everything works when I choose non-serverless (i.e. provisioned option for endpoint configuration), but when I try to create one with the serverless option, it fails. The error messages from the logs in cloud watch are below, starting with python and log4j error at the end.</p>
<blockquote>
<p>OpenBLAS WARNING - could not determine the L2 cache size on this
system, assuming 256k. python: can't open file
'/usr/local/bin/deep_learning_container.py': [Errno 13] Permission
denied. Warning: MMS is using non-default JVM parameters:
-XX:-UseContainerSupport Failed to reap children process, log4j:ERROR setFile(null,true) call failed. java.io.FileNotFoundException:
logs/mms_log.log (No such file or directory)</p>
</blockquote>
<p>I have set memory to the maximum allowed memory size of 6GB for the serverless option. Why am I getting this error?</p>",1,1,2022-06-15 12:42:13.290000 UTC,,2022-06-15 12:52:51.370000 UTC,1,python|amazon-sagemaker|aws-serverless,53,2019-06-26 10:46:24.430000 UTC,2022-09-24 19:31:46.023000 UTC,,11,0,0,1,,,,,,['amazon-sagemaker']
Custom python model : succeed to load but fail to predict/serve,"<p>I have a custom python model, which basically sets up several perturbations of a scikit-learn estimator. I do succeed in running the project with <code>mlflow run project_directory</code> CLI, saving the model with a <code>save_model()</code> statement. It appears on the dashboard with <code>mlflow ui</code>. I can even load the saved model within my <code>main.py</code> script and predict on a pandas.DataFrame without any problem.</p>

<p>My problem comes when I try to <code>mlflow models serve -m project/models/run_id</code> of <code>mlflow models predict -m project/models/run_id -i data.json</code>. I get the following error : </p>

<p><code>ModuleNotFoundError: No module named 'multi_model'</code></p>

<p>In the MLflow documentation, there is no example of a custom model served, so I can't figure out how to solve this dependency problem. Here is my project tree :</p>

<pre><code>project/
├── MLproject
├── __init__.py
├── conda.yaml
├── loader.py
├── main.py
├── models
│   └── 0ef267b0c9784a118290fa1ff579adbe
│       ├── MLmodel
│       ├── conda.yaml
│       └── python_model.pkl
├── multi_model.py
</code></pre>

<p><code>multi_model.py</code> :</p>

<pre><code>import numpy as np
from mlflow.pyfunc import PythonModel
from sklearn.base import clone

class MultiModel(PythonModel):

    def __init__(self, estimator=None, n=10):
        self.n = n
        self.estimator = estimator

    def fit(self, X, y=None):
        self.estimators = []
        for i in range(self.n):
            e = clone(self.estimator)
            e.set_params(random_state=i)
            X_bootstrap = X.sample(frac=1, replace=True, random_state=i)
            y_bootstrap = y.sample(frac=1, replace=True, random_state=i)
            e.fit(X_bootstrap, y_bootstrap)
            self.estimators.append(e)
        return self

    def predict(self, context, X):
        return np.stack([
            np.maximum(0, self.estimators[i].predict(X))
            for i in range(self.n)], axis=1
        )
</code></pre>

<p><code>main.py</code> :</p>

<pre><code>import os
import click
from sklearn.ensemble import RandomForestRegressor
import mlflow.pyfunc
import multi_model

@click(...) # define the click options according to MLproject file
def run(next_week, window_size, nfold):
    train = loader.load(start_week, current_week)
    x_train, y_train = train.drop(columns=['target']), train['target']

    model = multi_model.MultiModel(RandomForestRegressor())

    with mlflow.start_run() as run:
        model.fit(x_train, y_train)
        model_path = os.path.join('models', run.info.run_id)
        mlflow.pyfunc.save_model(
            path=model_path, 
            python_model=model,
        )

if __name__ == '__main__':
    run()
</code></pre>",2,1,2019-10-02 09:53:55.003000 UTC,2.0,,7,mlflow,4972,2015-10-02 08:45:49.683000 UTC,2020-01-20 15:50:37.903000 UTC,,247,69,0,7,,,,,,['mlflow']
Trigger Lambda Function upon Manual Approval of SageMaker Model,"<p>We use SageMaker to train our ML model. After training, we need to examine the metric and give it a manual approval. Upon approval, we need to copy the model artifects to another AWS account. If we have a Lambda function to copy the artifects, how can we automatically trigger the lambda function upon the model approval?</p>",1,0,2022-07-01 01:45:55.060000 UTC,,,0,amazon-web-services|amazon-sagemaker,59,2011-08-25 14:24:40.603000 UTC,2022-08-26 02:14:19.280000 UTC,,1590,17,1,72,,,,,,['amazon-sagemaker']
onnx model in ubuntu 18.04 not running,"<p>I am trying to implement custom vision solution using C#, Azure custom vision, and ONNX Model. My API code is running perfect on windows OS, but when I am trying to run same code on Ubuntu 18.04, getting below error.</p>
<p>I have download trained ONNX model from Azure ml.</p>
<pre><code>Unhandled exception. System.Reflection.TargetInvocationException: Exception has been thrown by the target of an invocation.
 ---&gt; System.Reflection.TargetInvocationException: Exception has been thrown by the target of an invocation.
 ---&gt; System.DllNotFoundException: Unable to load shared library 'api-ms-win-core-com-l1-1-0.dll' or one of its dependencies. In order to help diagnose loading problems, consider setting the LD_DEBUG environment variable: libapi-ms-win-core-com-l1-1-0.dll: cannot open shared object file: No such file or directory
   at WinRT.Platform.CoIncrementMTAUsage(IntPtr* cookie)
   at WinRT.WinrtModule..ctor()
   --- End of inner exception stack trace ---
   at System.RuntimeTypeHandle.CreateInstance(RuntimeType type, Boolean publicOnly, Boolean wrapExceptions, Boolean&amp; canBeCached, RuntimeMethodHandleInternal&amp; ctor, Boolean&amp; hasNoDefaultCtor)
   at System.RuntimeType.CreateInstanceDefaultCtorSlow(Boolean publicOnly, Boolean wrapExceptions, Boolean fillCache)
   at System.RuntimeType.CreateInstanceDefaultCtor(Boolean publicOnly, Boolean skipCheckThis, Boolean fillCache, Boolean wrapExceptions)
   at System.Activator.CreateInstance[T]()
   at System.LazyHelper.CreateViaDefaultConstructor[T]()
   at System.Lazy`1.CreateViaDefaultConstructor()
   at System.Lazy`1.ViaConstructor()
   at System.Lazy`1.ExecutionAndPublication(LazyHelper executionAndPublication, Boolean useDefaultConstructor)
   at System.Lazy`1.CreateValue()
   at System.Lazy`1.get_Value()
   at WinRT.WinrtModule.get_Instance()
   at WinRT.WinrtModule.GetActivationFactory(String runtimeClassId)
   at WinRT.BaseActivationFactory..ctor(String typeNamespace, String typeFullName)
   at Windows.Storage.StorageFile._IStorageFileStatics..ctor()
   --- End of inner exception stack trace ---
   at System.RuntimeTypeHandle.CreateInstance(RuntimeType type, Boolean publicOnly, Boolean wrapExceptions, Boolean&amp; canBeCached, RuntimeMethodHandleInternal&amp; ctor, Boolean&amp; hasNoDefaultCtor)
   at System.RuntimeType.CreateInstanceDefaultCtorSlow(Boolean publicOnly, Boolean wrapExceptions, Boolean fillCache)
   at System.RuntimeType.CreateInstanceDefaultCtor(Boolean publicOnly, Boolean skipCheckThis, Boolean fillCache, Boolean wrapExceptions)
   at System.Activator.CreateInstance[T]()
   at WinRT.WeakLazy`1.get_Value()
   at Windows.Storage.StorageFile._IStorageFileStatics.get_Instance()
   at Windows.Storage.StorageFile.GetFileFromPathAsync(String path)
   at CustomVisionAPI.Controllers.HomeController.Run() in /home/aaa/bbb/CutomVision/WebApplication1/Controllers/HomeController.cs:line 44
   at System.Threading.Tasks.Task.&lt;&gt;c.&lt;ThrowAsync&gt;b__139_1(Object state)
   at System.Threading.QueueUserWorkItemCallback.&lt;&gt;c.&lt;.cctor&gt;b__6_0(QueueUserWorkItemCallback quwi)
   at System.Threading.ExecutionContext.RunForThreadPoolUnsafe[TState](ExecutionContext executionContext, Action`1 callback, TState&amp; state)
   at System.Threading.QueueUserWorkItemCallback.Execute()
   at System.Threading.ThreadPoolWorkQueue.Dispatch()
</code></pre>",1,0,2021-03-28 06:32:08.993000 UTC,1.0,2021-03-29 00:58:49.367000 UTC,0,azure-machine-learning-service|onnx|microsoft-custom-vision|onnxruntime,76,2021-02-28 05:50:21.440000 UTC,2021-05-24 06:10:13.313000 UTC,"Anand, Gujarat, India",11,0,0,8,,,,,,['azure-machine-learning-service']
Time out: Access Azure blob storage from within an Azure ML experiment,"<p>I would like to access azure blob storage from within an Azure ML experiment. I completely followed the answer from 
<a href=""https://stackoverflow.com/questions/35246826/access-azure-blog-storage-from-within-an-azure-ml-experiment"">Access Azure blog storage from within an Azure ML experiment</a>
however, there was some error.</p>

<p>My code is</p>

<pre><code>from azure.storage.blob import BlobService

def azureml_main(dataframe1 = None, dataframe2 = None):
   dataframe1.to_csv(""output.csv"", index=True)
   account_name = ""acount_name""
   account_key=""rJfqPEFcbgpS...SKZrBs5J2eOq0IJYrc2Vg==""
   CONTAINER_NAME = ""CONTAINER_NAME""

   blob_service = BlobService(account_name, account_key, protocol='http')
   blob_service.put_block_blob_from_path(CONTAINER_NAME,""output"",""output.csv"")
</code></pre>

<p>The error log is blow </p>

<pre><code>[Critical]     Error: Error 0085: The following error occurred during script 
evaluation, please view the output log for more information:
---------- Start of error message from Python interpreter ----------
Caught exception while executing function: Traceback (most recent call last):
File ""C:\pyhome\lib\site-
packages\requests\packages\urllib3\connectionpool.py"", line 559, in urlopen
body=body, headers=headers)
File ""C:\pyhome\lib\site-
packages\requests\packages\urllib3\connectionpool.py"", line 353, in 
_make_request
conn.request(method, url, **httplib_request_kw)
File ""C:\pyhome\lib\http\client.py"", line 1083, in request
self._send_request(method, url, body, headers)
File ""C:\pyhome\lib\http\client.py"", line 1128, in _send_request
self.endheaders(body)
File ""C:\pyhome\lib\http\client.py"", line 1079, in endheaders
self._send_output(message_body)
File ""C:\pyhome\lib\http\client.py"", line 911, in _send_output
self.send(msg)
File ""C:\pyhome\lib\http\client.py"", line 885, in send
self.sock.sendall(data)
socket.timeout: timed out
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
File ""C:\pyhome\lib\site-packages\requests\adapters.py"", line 376, in send
timeout=timeout
File ""C:\pyhome\lib\site-
packages\requests\packages\urllib3\connectionpool.py"", line 609, in urlopen
_stacktrace=sys.exc_info()[2])
File ""C:\pyhome\lib\site-packages\requests\packages\urllib3\util\retry.py"", 
line 247, in increment
raise six.reraise(type(error), error, _stacktrace)
File ""C:\pyhome\lib\site-packages\requests\packages\urllib3\packages\six.py"", 
line 309, in reraise
raise value.with_traceback(tb)
File ""C:\pyhome\lib\site-
packages\requests\packages\urllib3\connectionpool.py"", line 559, in urlopen
body=body, headers=headers)
File ""C:\pyhome\lib\site-
packages\requests\packages\urllib3\connectionpool.py"", line 353, in 
_make_request
conn.request(method, url, **httplib_request_kw)
File ""C:\pyhome\lib\http\client.py"", line 1083, in request
self._send_request(method, url, body, headers)
File ""C:\pyhome\lib\http\client.py"", line 1128, in _send_request
self.endheaders(body)
File ""C:\pyhome\lib\http\client.py"", line 1079, in endheaders
self._send_output(message_body)
File ""C:\pyhome\lib\http\client.py"", line 911, in _send_output
self.send(msg)
File ""C:\pyhome\lib\http\client.py"", line 885, in send
self.sock.sendall(data)
requests.packages.urllib3.exceptions.ProtocolError: ('Connection aborted.', 
timeout('timed out',))
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
File ""C:\server\invokepy.py"", line 199, in batch
odfs = mod.azureml_main(*idfs)
File ""C:\temp\a24f27b62ef74991b262bafe5e685164.py"", line 39, in azureml_main
blob_service.put_block_blob_from_path(CONTAINER_NAME,""ppp"",""output.csv"")
File ""c:\temp\script bundle\azure\storage\blob\blobservice.py"", line 975, in 
put_block_blob_from_path
retry_wait)
File ""c:\temp\script bundle\azure\storage\blob\blobservice.py"", line 1072, in 
put_block_blob_from_file
x_ms_lease_id)
File ""c:\temp\script bundle\azure\storage\blob\blobservice.py"", line 883, in 
put_blob
self._perform_request(request)
File ""c:\temp\script bundle\azure\storage\storageclient.py"", line 171, in 
_perform_request
resp = self._filter(request)
File ""c:\temp\script bundle\azure\storage\storageclient.py"", line 160, in 
_perform_request_worker
return self._httpclient.perform_request(request)
File ""c:\temp\script bundle\azure\storage\_http\httpclient.py"", line 181, in 
perform_request
self.send_request_body(connection, request.body)
File ""c:\temp\script bundle\azure\storage\_http\httpclient.py"", line 143, in 
send_request_body
connection.send(request_body)
File ""c:\temp\script bundle\azure\storage\_http\requestsclient.py"", line 81, 
in send
self.response = self.session.request(self.method, self.uri, data=request_body, 
headers=self.headers, timeout=self.timeout)
File ""C:\pyhome\lib\site-packages\requests\sessions.py"", line 468, in request
resp = self.send(prep, **send_kwargs)
File ""C:\pyhome\lib\site-packages\requests\sessions.py"", line 576, in send
r = adapter.send(request, **kwargs)
File ""C:\pyhome\lib\site-packages\requests\adapters.py"", line 426, in send
raise ConnectionError(err, request=request)
requests.exceptions.ConnectionError: ('Connection aborted.', timeout('timed 
out',))
Process returned with non-zero exit code 1
</code></pre>

<p>Any help or thought is appreciated!</p>",1,1,2017-06-16 07:03:05.243000 UTC,,,1,python|azure|machine-learning|blob|azure-machine-learning-studio,1660,2017-06-16 06:45:45.243000 UTC,2022-09-23 12:05:18.207000 UTC,,49,7,0,13,,,,,,['azure-machine-learning-studio']
Vertex AI cost and usage tracking,"<p>Is there a way to track and monitor the usage of Vertex AI by roles/projects? We wanted to calculate the cost per user group and charge back that amount to the respective groups.</p>
<p>Thanks</p>",1,0,2022-04-15 21:28:53.897000 UTC,,2022-04-18 02:14:13.617000 UTC,0,google-cloud-platform|google-cloud-vertex-ai,586,2018-07-01 14:59:34.833000 UTC,2022-09-24 15:38:47.317000 UTC,,1043,17,6,212,,,,,,['google-cloud-vertex-ai']
Early Stopping and Callbacks with Keras when using SageMaker,"<p>I am using sagemaker to train a keras model. I need to implement early stoping approach when training the model. </p>

<p>Is there a way to pass callbacks such as EarlyStopping, Histories..etc. </p>

<p>In traditional way, we used to pass this as a parameter to keras's fit function:</p>

<pre><code>results = model.fit(train_x_trim, train_y_trim, 
                    validation_data=(test_x, test_y), 
                    epochs=FLAGS.epoch,  
                    verbose=0, 
                    callbacks=[tboard, checkpointer, early_stopping, history])
</code></pre>

<p>However, if using SageMaker, we need to call SageMaker's fit function instead which doesn't support callbacks. </p>

<pre><code>from sagemaker.tensorflow import TensorFlow 
iris_estimator = TensorFlow(entry_point='training_code.py', 
                            role=role, output_path=model_location, 
                            code_location=custom_code_upload_location, 
                            train_instance_count=1, 
                            train_instance_type='ml.c4.xlarge', 
                            training_steps=1000, 
                            evaluation_steps=100)
</code></pre>

<p>Any idea how to implement callbacks in SageMaker ? </p>",1,0,2018-11-26 17:21:42.240000 UTC,2.0,2018-11-26 18:14:41.430000 UTC,3,keras|amazon-sagemaker,557,2017-01-31 18:21:19.543000 UTC,2019-05-27 18:20:04.420000 UTC,Canada,71,2,0,3,,,,,,['amazon-sagemaker']
How to recieve metrics for Object Detection while using pytorch and Weights & Biases?,"<p>I have been training and fine tuning few models for detection task on a custom dataset,
I would like to plot relevant metrics such as mean Average Precision (taking into account the predicted bounding box location and the enclosed object's classification).</p>
<p>I'm using Pytorch and have started using <code>Weights &amp; Biases</code> (<a href=""https://youtu.be/G7GH0SeNBMA?list=PLD80i8An1OEGajeVo15ohAQYF1Ttle0lk"" rel=""nofollow noreferrer"">Weights &amp; Biases integrated with pytorch</a>)</p>
<p>For avoiding inventing the wheel, I have used some files from here:</p>
<p><a href=""https://github.com/pytorch/vision/tree/main/references/detection"" rel=""nofollow noreferrer"">https://github.com/pytorch/vision/tree/main/references/detection</a></p>
<p><code>engine.py</code> - holds the <strong>train_one_epoch()</strong> function</p>
<p><code>cocoeval.py</code> - holds the <strong>summarize()</strong> function</p>
<p>Now I would like to log those metrics to a <code>Weights &amp; Biases</code>,
so I'll we able to get more clear view and intuition about the fine-tuning phase,but I'm not sure where is the proper place to put the logger invocation.
can somebody please assist me?</p>
<pre><code>wandb.watch()
</code></pre>
<p>Thank you</p>",1,0,2022-08-28 10:07:59.513000 UTC,,2022-08-28 10:19:04.403000 UTC,0,image-processing|pytorch|torch|wandb,33,2010-10-18 20:28:58.400000 UTC,2022-09-24 15:25:41.413000 UTC,,610,289,1,130,,,,,,['wandb']
Azure Machine Learning Studio Conditional Training Data,"<p>I have built an <strong>Microsoft Azure ML Studio</strong> workspace predictive web service, and have a scernario where I need to be able to run the service with different training datasets. </p>

<p>I know I can setup multiple web services via Azure ML, each with a different training set attached, but I am trying to find a way to do it all within the same workspace and passing a <code>Web Input Parameter</code> as the input value to choose which training set to use.</p>

<p>I have found <a href=""https://blogs.technet.microsoft.com/machinelearning/2017/06/19/loading-a-trained-model-dynamically-in-an-azure-ml-web-service/"" rel=""nofollow noreferrer"">this</a> article, which describes <em>almost</em> my scenario. However, this article relies on the training dataset that is being pulled from the <code>Load Trained Data</code> module, as having a static endpoint (or blob storage location). I don't see any way to dynamically (or conditionally) change this location based on a <code>Web Input Parameter</code>. </p>

<p><strong>Basically, does Azure ML support a ""conditional training data"" loading?</strong></p>

<p>Or, might there be a way to combine training datasets, then filter based on the passed <code>Web Input Parameter</code>?</p>",2,0,2017-12-31 14:53:04.430000 UTC,,2017-12-31 17:13:45.440000 UTC,0,machine-learning|conditional|azure-machine-learning-studio,252,2010-03-29 23:52:29.850000 UTC,2022-09-19 19:01:00.847000 UTC,New York,11705,138,5,794,,,,,,['azure-machine-learning-studio']
Sagemaker - Distributed training,"<p>I can’t find documentation on the behavior of Sagemaker when distributed training is not explicitly specified.</p>
<p>Specifically,</p>
<ol>
<li>When SageMaker distributed data parallel is used via distribution=‘dataparallel’ , documents state that each instance processes different batches of data.</li>
</ol>
<pre><code>from sagemaker.tensorflow import TensorFlow

estimator = TensorFlow(
    role=role,
    py_version=&quot;py37&quot;,
    framework_version=&quot;2.4.1&quot;,
    # For training with multinode distributed training, set this count. Example: 2
    instance_count=4,
    instance_type=&quot;ml.p3.16xlarge&quot;,
    sagemaker_session=sagemaker_session,
    # Training using SMDataParallel Distributed Training Framework
    distribution={&quot;smdistributed&quot;: {&quot;dataparallel&quot;: {&quot;enabled&quot;: True}}},
)
</code></pre>
<ol start=""2"">
<li>I am not sure what happens when distribution parameter is not specified but instance_count &gt; 1 as below</li>
</ol>
<pre><code>estimator = TensorFlow(
    py_version=&quot;py3&quot;,
    entry_point=&quot;mnist.py&quot;,
    role=role,
    framework_version=&quot;1.12.0&quot;,
    instance_count=4,
    instance_type=&quot;ml.m4.xlarge&quot;,
)
</code></pre>
<p>Thanks!</p>",2,0,2021-12-15 19:42:26.150000 UTC,,,0,machine-learning|distributed-computing|amazon-sagemaker,137,2011-04-17 14:58:33.500000 UTC,2022-06-10 21:06:59.340000 UTC,"Washington, DC, United States",346,3,0,226,,,,,,['amazon-sagemaker']
Listing Notebook instances tags can takes ages,"<p>I am currently using the boto3 SDK from a Lambda function in order to retrieve various information about the Sagemaker Notebook Instances deployed in my account (almost 70 so not that many...)</p>

<p>One of the operations I am trying to perform is <strong>listing the tags for each instance.</strong></p>

<p>However, from time to time <strong>it takes ages to return the tags</strong> : my Lambda either gets stopped (I could increase the timeout but still...) or a <em>ThrottlingException</em> is raised from the <em>sagemaker.list_tags</em> function (which could be avoided by increasing the number of retry upon sagemaker boto3 client creation) : </p>

<pre><code>sagemaker = boto3.client(""sagemaker"", config=Config(retries = dict(max_attempts = 10)))
instances_dict = sagemaker.list_notebook_instances()

if not instances_dict['NotebookInstances']:
    return ""No Notebook Instances""

while instances_dict:
    for instance in instances_dict['NotebookInstances']:
        print instance['NotebookInstanceArn']
        start = time.time()
        tags_notebook_instance = sagemaker.list_tags(ResourceArn=instance['NotebookInstanceArn'])['Tags']
        print (time.time() - start)
    instances_dict = sagemaker.list_notebook_instances(NextToken=instances_dict['NextToken']) if 'NextToken' in instances_dict else None 
</code></pre>

<p>If you guys have any idea to avoid such delays :)</p>

<p>TY</p>",1,2,2019-03-21 11:15:53.350000 UTC,,,0,amazon-web-services|aws-lambda|boto3|amazon-sagemaker,308,2012-04-25 08:15:38.187000 UTC,2022-09-01 15:47:33.047000 UTC,,63,12,0,12,,,,,,['amazon-sagemaker']
Unable to register an ONNX model in azure machine learning service workspace,"<p>I was trying to register an ONNX model to Azure Machine Learning service workspace in two different ways, but I am getting errors I couldn't solve.</p>

<p><strong>First method: Via Jupyter Notebook and python Script</strong></p>

<pre><code>model = Model.register(model_path = MODEL_FILENAME,
                       model_name = ""MyONNXmodel"",
                       tags = {""onnx"":""V0""},
                       description = ""test"",
                       workspace = ws)
</code></pre>

<p>The error is : <strong>HttpOperationError: Operation returned an invalid status code 'Service invocation failed!Request: GET <a href=""https://cert-westeurope.experiments.azureml.net/rp/workspaces"" rel=""nofollow noreferrer"">https://cert-westeurope.experiments.azureml.net/rp/workspaces</a>'</strong></p>

<p><strong>Second method: Via Azure Portal</strong>
<a href=""https://i.stack.imgur.com/0BJrO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0BJrO.png"" alt=""Error: Create model &quot;MyONNXmodel&quot;: ajax error 413.""></a></p>

<p>Anyone can help please?</p>",1,2,2019-03-08 13:22:25.573000 UTC,1.0,2019-04-08 01:32:28.163000 UTC,1,azure|onnx|azure-machine-learning-service,244,2017-01-10 13:34:39.080000 UTC,2022-09-22 15:43:10.210000 UTC,"Sfax, Tunisie",54,44,0,55,,,,,,['azure-machine-learning-service']
use global variables in AWS Sagemaker script,"<p>After having correctly deployed our model, I need to invoke it via lambda function. The script features two cleaning function, the first one (cleaning()) gives us 5 variables: the cleaned dataset and 4 other variables (scaler, monthdummies, compadummies, parceldummies) that we need to use in the second cleaning function (cleaning_test()).</p>
<p>The reason behind this is that in the use case I'll have only one instance at a time to perform predictions on, not an entire dataset. This means that I pass the row to the first cleaning() function since some commands won't work. I can't also use a scaler and neither create dummy variables, so the aim is to import the scaler and some dummies used in the cleaning() function, since they come from the whole dataset, that I used to train the model.</p>
<p>Hence, in the input_fn() function, the input needs to be cleaned using the cleaning_test() function, that requires the scaler and the three lists of dummies from the cleaning() one.</p>
<p>When I train the model, the cleaning() function works fine, but after the deployment, if we invoke the endpoint, it raises the error that variable &quot;scaler&quot; is not defined.</p>
<p>Below is the script.py:
Note that the test is # since I've already tested it, so now I'm training on the whole dataset and I want to predict completely new instances</p>
<pre><code>def cleaning(data):
    some cleaning on data stored in s3
    return cleaned_data, scaler, monthdummies, compadummies, parceldummies

def cleaning_test(data, scaler, monthdummies, compadummies, parceldummies):
    cleaning on data without labels
    return cleaned_data

def model_fn(model_dir):
    clf = joblib.load(os.path.join(model_dir, &quot;model.joblib&quot;))
    return clf



def input_fn(request_body, request_content_type):
    if request_content_type == &quot;application/json&quot;:
        data = json.loads(request_body)
        df = pd.DataFrame(data, index = [0])
        input_data = cleaning_test(df, scaler, monthdummies, compadummies, parceldummies)
    else:
        pass
    return input_data

        
def predict_fn(input_data, model):
    return model.predict_proba(input_data)

if __name__ =='__main__':

    print('extracting arguments')
    parser = argparse.ArgumentParser()

    # hyperparameters sent by the client are passed as command-line arguments to the script.
    parser.add_argument('--n_estimators', type=int, default=10)
    parser.add_argument('--min-samples-leaf', type=int, default=3)

    # Data, model, and output directories
    parser.add_argument('--model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))
    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))
    #parser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))
    parser.add_argument('--train-file', type=str, default='fp_train.csv')
    #parser.add_argument('--test-file', type=str, default='fp_test.csv')

    args, _ = parser.parse_known_args()

    print('reading data')
    train_df = pd.read_csv(os.path.join(args.train, args.train_file))
    #test_df = pd.read_csv(os.path.join(args.test, args.test_file))
    
    
    print(&quot;cleaning&quot;)
    train_df, scaler, monthdummies, compadummies, parceldummies = cleaning(train_df)
    #test_df, scaler1, monthdummies1, compadummies1, parceldummies1 = cleaning(test_df)
    
    print(&quot;splitting&quot;)
    y = train_df.loc[:,&quot;event&quot;]
    X = train_df.loc[:, train_df.columns != 'event']

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)


    &quot;&quot;&quot;print('building training and testing datasets')
    X_train = train_df.loc[:, train_df.columns != 'event']
    X_test = test_df.loc[:, test_df.columns != 'event']
    y_train = train_df.loc[:,&quot;event&quot;]
    y_test = test_df.loc[:,&quot;event&quot;]&quot;&quot;&quot;
    
    print(X_train.columns)
    print(X_test.columns)
    


    
    # train
    print('training model')
    model = RandomForestClassifier(
        n_estimators=args.n_estimators,
        min_samples_leaf=args.min_samples_leaf,
        n_jobs=-1)
    
    model.fit(X_train, y_train)

    # print abs error
    print('validating model')
    proba = model.predict_proba(X_test)

    
    # persist model
    path = os.path.join(args.model_dir, &quot;model.joblib&quot;)
    joblib.dump(model, path)
    print('model persisted at ' + path)

</code></pre>
<p>That I run through:</p>
<pre><code>sklearn_estimator = SKLearn(
    entry_point='script.py',
    role = get_execution_role(),
    train_instance_count=1,
    train_instance_type='ml.c5.xlarge',
    framework_version='0.20.0',
    base_job_name='rf-scikit',
    hyperparameters = {'n_estimators': 15})

sklearn_estimator.fit({'train':trainpath})

sklearn_estimator.latest_training_job.wait(logs='None')
artifact = sm_boto3.describe_training_job(
    TrainingJobName=sklearn_estimator.latest_training_job.name)['ModelArtifacts']['S3ModelArtifacts']


predictor = sklearn_estimator.deploy(
    instance_type='ml.c5.large',
    initial_instance_count=1)
</code></pre>
<p>The question is, how can I &quot;store&quot; the variables given by the cleaning() function during the training process, in order to use them in the input_fn() function, making cleaning_test() work fine?</p>
<p>Thanks!</p>",0,0,2020-07-16 09:29:22.437000 UTC,,,3,amazon-web-services|amazon-s3|amazon-sagemaker,222,2018-11-02 10:55:54.730000 UTC,2022-09-23 12:09:30.577000 UTC,Italia,119,11,0,11,,,,,,['amazon-sagemaker']
Anyone know why sagemaker would not be able to reach the files.fast.ai server,"<p>I have setup fast.ai sagemaker jupyter notebooks. The notebook environment works fine however its not able to download files from files.fast.ai server. Its able to reach fast.ai server. (attach pings from terminal)</p>

<p>Anyone have a clue? </p>

<p><a href=""https://i.stack.imgur.com/ltOdy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ltOdy.png"" alt=""not able to reach files.fast.ai server""></a></p>

<p><a href=""https://i.stack.imgur.com/ER3jn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ER3jn.png"" alt=""enter image description here""></a></p>",1,0,2020-04-01 17:33:01.243000 UTC,,,0,amazon-sagemaker|fast-ai,49,2011-07-12 13:12:16.900000 UTC,2022-04-12 14:40:38.743000 UTC,"Middletown, CT, United States",2262,105,0,83,,,,,,['amazon-sagemaker']
Azure ML Recommendations,"<p>I want to use Azure ML to find related products using information from receipts from a store.</p>

<p>I got a file of reciepts:</p>

<pre><code>44366,136778
79619,88975
78861,78864
53395,78129,78786,79295,79353,79406,79408,79417,85829,136712
32340,33973
31897,32905
32476,32697,33202,33344,33879,34237,34422,48175,55486,55490,55498
17800
32476,32697,33202,33344,33879,34237,34422,48175,55490,55497,55498,55503
47098
136974
85832
</code></pre>

<p>Each row represent one receipt and each number is a product id.</p>

<p>Given a product id I want to get a list of similar products, i.e. products that was bought together by other customers.</p>

<p>Can anyone point me in the right direction of how do to this?</p>",2,0,2014-08-08 14:04:22.160000 UTC,1.0,2015-11-25 09:19:29.170000 UTC,1,azure|machine-learning|azure-machine-learning-studio,539,2009-08-31 06:36:18.963000 UTC,2022-09-23 12:44:03.057000 UTC,"Norrköping, Sweden",1259,113,4,62,,,,,,['azure-machine-learning-studio']
Supervised text scoring,"<p>I have a short text (subject line of an email) and I have a value which indicates the success it (people opening that email).</p>

<p>What is the best process and algorithm to feed the data into a machine learning experiment to generate a predictive experiment to score the future text?</p>",2,0,2016-12-20 10:39:28.733000 UTC,,,1,machine-learning|azure-machine-learning-studio,308,2012-03-11 14:06:52.863000 UTC,2022-09-24 18:47:11.083000 UTC,,5586,67,23,398,,,,,,['azure-machine-learning-studio']
How to input the correct shape for a Tensorflow Estimator?,"<p>I am trying to build a Tensorflow estimator to use on <code>SageMaker</code>. The main function trains and evaluates the estimator. Despite my best attempts, I keep getting the following error:</p>

<blockquote>
  <p>ValueError: Input 0 of layer inputs is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: [50, 41]</p>
</blockquote>

<pre><code>def keras_model_fn(hyperparameters):
    """"""keras_model_fn receives hyperparameters from the training job and returns a compiled keras model.
    The model will be transformed into a TensorFlow Estimator before training and it will be saved in a 
    TensorFlow Serving SavedModel at the end of training.

    Args:
        hyperparameters: The hyperparameters passed to the SageMaker TrainingJob that runs your TensorFlow 
                         training script.
    Returns: A compiled Keras model
    """"""
    model = tf.keras.models.Sequential()
    model.add(tf.keras.layers.LSTM(32, name='inputs', input_shape=( None, 41)))
    model.add(tf.keras.layers.Dense(11, activation='softmax', name='dense'))
    model.compile(loss='categorical_crossentropy',
                  optimizer='rmsprop',
                  metrics=['accuracy'])

    return model


def train_input_fn(training_dir=None, hyperparameters=None):
    # invokes _input_fn with training dataset
    dataset = tf.data.Dataset.from_tensors(({INPUT_TENSOR_NAME: x_train}, y_train))
    dataset = dataset.repeat()
    return dataset.make_one_shot_iterator().get_next()

def eval_input_fn(training_dir=None, hyperparameters=None):
    # invokes _input_fn with evaluation dataset

    dataset =  tf.data.Dataset.from_tensors(({INPUT_TENSOR_NAME: x_test}, y_test))
    return dataset.make_one_shot_iterator().get_next()

if __name__ == '__main__':
    print(x_train.shape, y_train.shape)
    tf.logging.set_verbosity(tf.logging.INFO)
    model = keras_model_fn(0)
    estimator = tf.keras.estimator.model_to_estimator(keras_model=model)
    train_spec = tf.estimator.TrainSpec(input_fn=train_input_fn, max_steps=1000)
    eval_spec = tf.estimator.EvalSpec(input_fn=eval_input_fn)
    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)
</code></pre>

<p>My inputs and output shapes are:</p>

<blockquote>
  <p>(52388, 50, 41) (52388, 11)</p>
</blockquote>",1,1,2019-01-04 20:34:49.440000 UTC,,2019-01-05 09:58:44.260000 UTC,0,python|tensorflow|amazon-sagemaker,753,2012-12-24 14:35:59.000000 UTC,2022-09-23 10:35:00.417000 UTC,Dubai - United Arab Emirates,436,28,0,73,,,,,,['amazon-sagemaker']
Run mlflow project on multiple remote servers?,"<p>Can <code>MLflow</code> be used to dispatch <strong>projects</strong> to multiple remote servers?(not aws,azure etc.) from a local tracking server?<br>
I have the following scenario-<Br>
Multiple servers, where I would like to dispatch the <code>mlflow</code> project to all with different parameters, and let them ""report"" back to the current <strong>tracking server:</strong></p>

<pre><code>for ip in servers_ips:
    start_remote_mlflow(entry_point=GITHUBPATH,tracking_server=this_server_ip,hparams)
</code></pre>

<p>I see one can dispatch <code>mlflow</code> projects to aws or azure by specifying the ip or the remote machine. Can it be done with desktops as well?</p>",1,0,2020-05-23 16:24:20.187000 UTC,,,2,python|remote-server|mlflow,221,2016-10-18 05:35:53.033000 UTC,2022-09-25 05:02:23.130000 UTC,Israel,2057,201,2,269,,,,,,['mlflow']
Google Cloud Platform - Vertex AI Image classification training fails with no specific error message,"<p>I'm doing an image classification task using Vertex AI and after about 3 hours of training it fails. The error message is nondescript. It says &quot;Training pipeline failed with error message: Internal error occurred. Please retry in a few minutes. If you still experience errors, contact Vertex AI.&quot;</p>
<p>It's done that for three of my models using the same image dataset (about 45k large). What could be the error here? How can I find out?</p>",0,4,2021-11-29 18:27:51.447000 UTC,,2021-11-29 21:56:51.490000 UTC,0,machine-learning|google-cloud-platform|computer-vision|google-cloud-vertex-ai,192,2014-02-18 22:34:13.817000 UTC,2022-09-22 15:59:48.447000 UTC,,955,20,0,94,,,,,,['google-cloud-vertex-ai']
"Azure Machine Learning Studio, Python script input format","<p>I am execute a python script in Azure ML studio. The python script will take a single string input, process the string and then return the processed result.</p>

<p>I am using ""Enter Data Manually"" as input(connect to ""Dataset1"" on ""Execute Python Script"" module), and the input format is CSV. This input is also my ""Web Service Input"". So, in the python script, I will get the input text like following,</p>

<pre><code>         input_text = dataframe1.iat[0, 0]
</code></pre>

<p>But, I found that I have to enter the text start from the 2nd row, like the follow picture. </p>

<p><a href=""https://i.stack.imgur.com/y6SUS.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/y6SUS.jpg"" alt=""enter image description here""></a></p>

<p>If I put the text in the first row, then the python script won't execute. From the error message, seems its' index out of bound error. But I did put data at the first row and first column, the only column. Why I still get this error? I am not very clear on the input scheme. Can someone explain? Thanks.</p>

<pre><code>[Error]         Caught exception while executing function: Traceback (most recent call last):
[Error]           File ""C:\server\invokepy.py"", line 199, in batch
[Error]             odfs = mod.azureml_main(*idfs)
[Error]           File ""C:\temp\b792c87679e1424fb3300c65b0231c07.py"", line 43, in azureml_main
[Error]             input_text = dataframe1.iat[0, 0]
[Error]           File ""C:\pyhome\lib\site-packages\pandas\core\indexing.py"", line 1610, in __getitem__
[Error]             return self.obj.get_value(*key, takeable=self._takeable)
[Information]         theano/typed_list/type.py                      2016-09-06 14:31:24         3870
[Information]         theano/typed_list/type.pyc                     2016-09-06 14:31:26         6025
[Information]         theano/typed_list/__init__.py                  2016-09-06 14:31:24           71
[Information]         theano/typed_list/__init__.pyc                 2016-09-06 14:31:26          287
[Information]         theano/updates.py                              2016-09-06 14:31:24         3405
[Information]         theano/updates.pyc                             2016-09-06 14:31:26         3317
[Information]         theano/version.py                              2016-09-06 14:31:24          208
[Information]         theano/version.pyc                             2016-09-06 14:31:26          380
[Information]         theano/__init__.py                             2016-09-06 14:31:24         6675
[Information]         theano/__init__.pyc                            2016-09-06 14:31:26         6689
[Information]         [ READING ] 0:00:00
[Information]         Input pandas.DataFrame #1:
[Information]         Empty DataFrame
[Information]         Columns: [Text start from here.]
[Information]         Index: []
[Error]           File ""C:\pyhome\lib\site-packages\pandas\core\frame.py"", line 1832, in get_value
[Error]             return _maybe_box_datetimelike(series._values[index])
[Error]         IndexError: index 0 is out of bounds for axis 0 with size 0
[Error]         Process returned with non-zero exit code 1
</code></pre>",0,2,2018-10-19 19:07:57.973000 UTC,,,1,python|csv|azure-machine-learning-studio,145,2012-05-18 17:27:03.537000 UTC,2022-09-23 21:14:32.923000 UTC,,581,51,0,49,,,,,,['azure-machine-learning-studio']
"AWS Sagemaker suddenly and inconsistently throws resource limit exceeded error (limit 0), service quota says 10?","<p>I am suddenly getting the resource limit exceeded error</p>
<pre><code>The account-level service limit 'ml.g4dn.xlarge for transform job usage' is 0 Instances, with current utilization of 0 Instances and a request delta of 1 Instances
</code></pre>
<p>However, I have been able to create training and batch jobs with this instance just fine before. When I check Service Quotas, under &quot;Running Dedicated g4dn Hosts&quot; the applied quota value is 1, but there is a closed quota increase request of 10. So why on earth would it say the limit is 0?</p>
<p>EDIT: I have also confirmed that it sometimes throws the error and sometimes fails. I.e. I had a ml.g4dn.2xlarge training job that ran successfully, then throws a 0 instance resource limit error shortly after. And if I try to clone the successful job it throws a resource 0 error.</p>",2,0,2022-03-29 01:04:41.023000 UTC,,2022-03-29 04:40:39.523000 UTC,1,amazon-web-services|amazon-sagemaker,377,2015-01-15 17:43:03.700000 UTC,2022-08-23 22:54:25.603000 UTC,,1387,51,1,153,,,,,,['amazon-sagemaker']
"trainer.train() in Kaggle: StdinNotImplementedError: getpass was called, but this frontend does not support input requests","<p>When saving a version in Kaggle, I get <strong>StdinNotImplementedError: getpass was called, but this frontend does not support input requests</strong> whenever I use the Transformers.Trainer class. The general code I use:</p>
<pre><code>from transformers import Trainer, TrainingArguments
training_args = TrainingArguments(params)
trainer = Trainer(params)
trainer.train()
</code></pre>
<p>And the specific cell I am running now:</p>
<pre><code>from transformers import Trainer, TrainingArguments,EarlyStoppingCallback

early_stopping = EarlyStoppingCallback()

training_args = TrainingArguments(
    output_dir=OUT_FINETUNED_MODEL_PATH,          
    num_train_epochs=20,              
    per_device_train_batch_size=16,  
    per_device_eval_batch_size=16,   
    warmup_steps=0,                
    weight_decay=0.01,               
    logging_dir='./logs',            
    logging_steps=100,
    evaluation_strategy=&quot;steps&quot;,
    eval_steps=100,
    load_best_model_at_end=True,
    metric_for_best_model=&quot;eval_loss&quot;,
    greater_is_better=False
    
)

trainer = Trainer(
    model=model,                         
    args=training_args,                  
    train_dataset=train_dataset,         
    eval_dataset=val_dataset,             
    callbacks=[early_stopping]

)

trainer.train()
</code></pre>
<p>When trainer.train() is called, I get the error below, which I do not get if I train with native PyTorch. I understood that the error arises since I am asked to input a password, but no password is asked when using native PyTorch code, nor when using the same code with trainer.train() on Google Colab.
Any solution would be ok, like:</p>
<ol>
<li>Avoid being asked the password.</li>
<li>Enable input requests when saving a notebook on Kaggle. After that, if I understood correctly, I would need to go to <a href=""https://wandb.ai/authorize"" rel=""nofollow noreferrer"">https://wandb.ai/authorize</a> (after having created an account) and copy the generated key to console. However, I do not understand why wandb should be necessary since I never explicitly used it so far.</li>
</ol>
<pre><code>wandb: You can find your API key in your browser here: https://wandb.ai/authorize
Traceback (most recent call last):
  File &quot;/opt/conda/lib/python3.7/site-packages/wandb/sdk/wandb_init.py&quot;, line 741, in init
    wi.setup(kwargs)
  File &quot;/opt/conda/lib/python3.7/site-packages/wandb/sdk/wandb_init.py&quot;, line 155, in setup
    wandb_login._login(anonymous=anonymous, force=force, _disable_warning=True)
  File &quot;/opt/conda/lib/python3.7/site-packages/wandb/sdk/wandb_login.py&quot;, line 210, in _login
    wlogin.prompt_api_key()
  File &quot;/opt/conda/lib/python3.7/site-packages/wandb/sdk/wandb_login.py&quot;, line 144, in prompt_api_key
    no_create=self._settings.force,
  File &quot;/opt/conda/lib/python3.7/site-packages/wandb/sdk/lib/apikey.py&quot;, line 135, in prompt_api_key
    key = input_callback(api_ask).strip()
  File &quot;/opt/conda/lib/python3.7/site-packages/ipykernel/kernelbase.py&quot;, line 825, in getpass
    &quot;getpass was called, but this frontend does not support input requests.&quot;
IPython.core.error.StdinNotImplementedError: getpass was called, but this frontend does not support input requests.
wandb: ERROR Abnormal program exit
---------------------------------------------------------------------------
StdinNotImplementedError                  Traceback (most recent call last)
/opt/conda/lib/python3.7/site-packages/wandb/sdk/wandb_init.py in init(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)
    740         wi = _WandbInit()
--&gt; 741         wi.setup(kwargs)
    742         except_exit = wi.settings._except_exit

/opt/conda/lib/python3.7/site-packages/wandb/sdk/wandb_init.py in setup(self, kwargs)
    154         if not settings._offline and not settings._noop:
--&gt; 155             wandb_login._login(anonymous=anonymous, force=force, _disable_warning=True)
    156 

/opt/conda/lib/python3.7/site-packages/wandb/sdk/wandb_login.py in _login(anonymous, key, relogin, host, force, _backend, _silent, _disable_warning)
    209     if not key:
--&gt; 210         wlogin.prompt_api_key()
    211 

/opt/conda/lib/python3.7/site-packages/wandb/sdk/wandb_login.py in prompt_api_key(self)
    143             no_offline=self._settings.force,
--&gt; 144             no_create=self._settings.force,
    145         )

/opt/conda/lib/python3.7/site-packages/wandb/sdk/lib/apikey.py in prompt_api_key(settings, api, input_callback, browser_callback, no_offline, no_create, local)
    134             )
--&gt; 135             key = input_callback(api_ask).strip()
    136         write_key(settings, key, api=api)

/opt/conda/lib/python3.7/site-packages/ipykernel/kernelbase.py in getpass(self, prompt, stream)
    824             raise StdinNotImplementedError(
--&gt; 825                 &quot;getpass was called, but this frontend does not support input requests.&quot;
    826             )

StdinNotImplementedError: getpass was called, but this frontend does not support input requests.

The above exception was the direct cause of the following exception:

Exception                                 Traceback (most recent call last)
&lt;ipython-input-82-4d1046ab80b8&gt; in &lt;module&gt;
     42     )
     43 
---&gt; 44     trainer.train()

/opt/conda/lib/python3.7/site-packages/transformers/trainer.py in train(self, resume_from_checkpoint, trial, **kwargs)
   1067         model.zero_grad()
   1068 
-&gt; 1069         self.control = self.callback_handler.on_train_begin(self.args, self.state, self.control)
   1070 
   1071         # Skip the first epochs_trained epochs to get the random state of the dataloader at the right point.

/opt/conda/lib/python3.7/site-packages/transformers/trainer_callback.py in on_train_begin(self, args, state, control)
    338     def on_train_begin(self, args: TrainingArguments, state: TrainerState, control: TrainerControl):
    339         control.should_training_stop = False
--&gt; 340         return self.call_event(&quot;on_train_begin&quot;, args, state, control)
    341 
    342     def on_train_end(self, args: TrainingArguments, state: TrainerState, control: TrainerControl):

/opt/conda/lib/python3.7/site-packages/transformers/trainer_callback.py in call_event(self, event, args, state, control, **kwargs)
    386                 train_dataloader=self.train_dataloader,
    387                 eval_dataloader=self.eval_dataloader,
--&gt; 388                 **kwargs,
    389             )
    390             # A Callback can skip the return of `control` if it doesn't change it.

/opt/conda/lib/python3.7/site-packages/transformers/integrations.py in on_train_begin(self, args, state, control, model, **kwargs)
    627             self._wandb.finish()
    628         if not self._initialized:
--&gt; 629             self.setup(args, state, model, **kwargs)
    630 
    631     def on_train_end(self, args, state, control, model=None, tokenizer=None, **kwargs):

/opt/conda/lib/python3.7/site-packages/transformers/integrations.py in setup(self, args, state, model, **kwargs)
    604                     project=os.getenv(&quot;WANDB_PROJECT&quot;, &quot;huggingface&quot;),
    605                     name=run_name,
--&gt; 606                     **init_args,
    607                 )
    608             # add config parameters (run may have been created manually)

/opt/conda/lib/python3.7/site-packages/wandb/sdk/wandb_init.py in init(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)
    779             if except_exit:
    780                 os._exit(-1)
--&gt; 781             six.raise_from(Exception(&quot;problem&quot;), error_seen)
    782     return run

/opt/conda/lib/python3.7/site-packages/six.py in raise_from(value, from_value)

Exception: problem
</code></pre>",1,0,2021-06-22 16:09:46.880000 UTC,,,2,huggingface-transformers|kaggle|getpass|wandb,440,2021-06-22 15:33:48.980000 UTC,2022-01-20 16:06:40.790000 UTC,,51,1,0,4,,,,,,['wandb']
How to load a AzureML model in an Azure Databricks compute?,"<p>I am trying to run a <code>DatabricksStep</code>. I have used <code>ServicePrincipalAuthentication</code> to authenticate the run:</p>
<pre><code>appId = dbutils.secrets.get(&lt;secret-scope-name&gt;, &lt;client-id&gt;)
tenant = dbutils.secrets.get(&lt;secret-scope-name&gt;, &lt;directory-id&gt;)
clientSecret = dbutils.secrets.get(&lt;secret-scope-name&gt;, &lt;client-secret&gt;)
subscription_id = dbutils.secrets.get(&lt;secret-scope-name&gt;, &lt;subscription-id&gt;)
resource_group = &lt;aml-rgp-name&gt;
workspace_name = &lt;aml-ws-name&gt;

svc_pr = ServicePrincipalAuthentication(
       tenant_id=tenant,
       service_principal_id=appId,
       service_principal_password=clientSecret)

ws = Workspace(
       subscription_id=subscription_id,
       resource_group=resource_group,
       workspace_name=workspace_name,
       auth=svc_pr
       )
</code></pre>
<p>The authentication is successful since running the following block of code gives the desired output:</p>
<pre><code>subscription_id = ws.subscription_id
resource_group = ws.resource_group
workspace_name = ws.name
workspace_region = ws.location
print(subscription_id, resource_group, workspace_name, workspace_region, sep='\n')
</code></pre>
<p>However, the following block of codes gives an error:</p>
<pre><code>model_name=&lt;registered-model-name&gt;
model_path = Model.get_model_path(model_name=model_name, _workspace=ws)
loaded_model = joblib.load(model_path)
print('model loaded!')
</code></pre>
<p>This is giving an error:</p>
<pre><code>UserErrorException:
    Message: 
Operation returned an invalid status code 'Forbidden'. The possible reason could be:
1. You are not authorized to access this resource, or directory listing denied.
2. you may not login your azure service, or use other subscription, you can check your
default account by running azure cli commend:
'az account list -o table'.
3. You have multiple objects/login session opened, please close all session and try again.
                
    InnerException None
    ErrorResponse 
{
    &quot;error&quot;: {
        &quot;message&quot;: &quot;\nOperation returned an invalid status code 'Forbidden'. The possible reason could be:\n1. You are not authorized to access this resource, or directory listing denied.\n2. you may not login your azure service, or use other subscription, you can check your\ndefault account by running azure cli commend:\n'az account list -o table'.\n3. You have multiple objects/login session opened, please close all session and try again.\n                &quot;,
        &quot;code&quot;: &quot;UserError&quot;
    }
}
</code></pre>
<p>The error is <code>Forbidden Error</code> even though I have authenticated using <code>ServicePrincipalAuthentication</code>.
How to resolve this error to run inference using an AML registered model in ADB?</p>",1,4,2020-12-17 14:59:00.290000 UTC,,,0,azure-databricks|azure-machine-learning-service,466,2020-10-03 12:46:02.437000 UTC,2022-09-21 15:27:45.773000 UTC,"Bengaluru, Karnataka, India",887,187,32,130,,,,,,['azure-machine-learning-service']
Trying to query Azure SQL Database with Azure ML / Docker Image,"<p>I wanted to do a realtime deployment of my model on azure, so I plan to create an image which firsts queries an ID in azure SQL db to get the required features, then predicts using my model and returns the predictions. The error I get from PyODBC library is that drivers are not installed</p>

<p>I tried it on the azure ML jupyter notebook to establish the connection and found that no drivers are being installed in the environment itself. After some research i found that i should create a docker image and deploy it there,  but i still met with the same results</p>

<pre><code>    driver= '{ODBC Driver 13 for SQL Server}'
    cnxn = pyodbc.connect('DRIVER='+driver+';SERVER='+server+';PORT=1433;DATABASE='+database+';UID='+username+';PWD='+ password+';Encrypt=yes'+';TrustServerCertificate=no'+';Connection Timeout=30;')
</code></pre>

<blockquote>
  <p>('01000', ""[01000] [unixODBC][Driver Manager]Can't open lib 'ODBC
  Driver 13 for SQL Server' : file not found (0) (SQLDriverConnect)"")</p>
</blockquote>

<p>i want a result to the query instead i get this message</p>",3,2,2019-10-08 10:59:56.097000 UTC,,2019-10-08 11:55:47.687000 UTC,1,sql-server|azure|azure-machine-learning-service,1319,2018-05-10 14:51:27.537000 UTC,2019-12-10 09:09:54.133000 UTC,"Mumbai, Maharashtra, India",55,1,0,13,,,,,,['azure-machine-learning-service']
AWS SageMaker fails loading PyTorch .pth weights,"<p>I want to deploy PyTorch model to AWS SageMaker endpoint and experience some issues. Below there is a code and logs. Basically, I have .pth file for model trained outside SageMaker (Azure) and I want to move it to AWS. <code>macro_model.tar.gz</code> contains model weights <code>macro_model.pth</code> file and inference code with <code>requirements.txt</code>.</p>
<p><strong>Expected behavior</strong></p>
<p>Model deployed to endpoint and generates predictions</p>
<p><strong>Issue</strong></p>
<p>SageMaker <code>model_fn</code> function doesn't see model weights.</p>
<p>Any ideas what can be wrong?</p>
<p>Deployment code in SageMaker Notebook:</p>
<pre><code>import boto3
import sagemaker
from sagemaker.pytorch import PyTorchModel
from sagemaker import get_execution_role
import json
import numpy as np

role = get_execution_role()
conn = boto3.client('s3')

client = boto3.client('sagemaker')
pytorch_model = PyTorchModel(model_data='s3://.../macro_model.tar.gz',
                             framework_version=&quot;1.7&quot;, py_version=&quot;py3&quot;,
                             role=role, entry_point='inference.py', source_dir='code')

predictor = pytorch_model.deploy(initial_instance_count=1,instance_type='ml.p2.xlarge', endpoint_name='...')
</code></pre>
<p><code>macro_model.tar.gz</code> structure:</p>
<pre><code>|   macro_model
|           |--macro_model.pth
|
|           code
|               |--inference.py
|               |--requirements.txt
|
</code></pre>
<p><code>model_fn</code> function implementation:</p>
<pre><code>def model_fn(model_dir):
    logging.info('Loading the model...')

    layers = [
        nn.Linear(512, 512),
        nn.ReLU(),
        nn.Dropout(0.3),
        nn.Linear(512, 2)
      ]
    
    logging.info('Layers initiated...')
    model = VideoRecog_Model1(layers,7)
    logging.info('Model initiated...')
    with open(os.path.join(model_dir, 'macro_model.pth'), 'rb') as f:
        model.load_state_dict(torch.load(f))
    
    
    model.to(device).eval()
    logging.info('Done loading model')
    return model
</code></pre>
<p>Logs:</p>
<pre><code>2021-06-27T12:24:56.241+02:00   Collecting opencv-python-headless Downloading opencv_python_headless-4.5.2.54-cp36-cp36m-manylinux2014_x86_64.whl (38.2 MB)

2021-06-27T12:24:57.242+02:00   Collecting moviepy==1.0.3 Downloading moviepy-1.0.3.tar.gz (388 kB)
    2021-06-27T12:24:57.242+02:00   Collecting av==8.0.3 Downloading av-8.0.3-cp36-cp36m-manylinux2010_x86_64.whl (37.2 MB)

2021-06-27T12:24:58.243+02:00   Collecting decorator&lt;5.0,&gt;=4.0.2 Downloading decorator-4.4.2-py2.py3-none-any.whl (9.2 kB)

2021-06-27T12:24:58.243+02:00   Requirement already satisfied: tqdm&lt;5.0,&gt;=4.11.2 in /opt/conda/lib/python3.6/site-packages (from moviepy==1.0.3-&gt;-r /opt/ml/model/code/requirements.txt (line 2)) (4.59.0)

2021-06-27T12:24:58.243+02:00   Requirement already satisfied: requests&lt;3.0,&gt;=2.8.1 in /opt/conda/lib/python3.6/site-packages (from moviepy==1.0.3-&gt;-r /opt/ml/model/code/requirements.txt (line 2)) (2.22.0)

2021-06-27T12:24:58.243+02:00   Collecting proglog&lt;=1.0.0 Downloading proglog-0.1.9.tar.gz (10 kB)
    
2021-06-27T12:24:59.244+02:00   Requirement already satisfied: numpy&gt;=1.17.3 in /opt/conda/lib/python3.6/site-packages (from moviepy==1.0.3-&gt;-r /opt/ml/model/code/requirements.txt (line 2)) (1.19.1)
    
2021-06-27T12:24:59.244+02:00   Collecting imageio&lt;3.0,&gt;=2.5 Downloading imageio-2.9.0-py3-none-any.whl (3.3 MB)
    
2021-06-27T12:24:59.244+02:00   Collecting imageio_ffmpeg&gt;=0.2.0 Downloading imageio_ffmpeg-0.4.4-py3-none-manylinux2010_x86_64.whl (26.9 MB)
    
2021-06-27T12:25:00.244+02:00   Requirement already satisfied: pillow in /opt/conda/lib/python3.6/site-packages (from imageio&lt;3.0,&gt;=2.5-&gt;moviepy==1.0.3-&gt;-r /opt/ml/model/code/requirements.txt (line 2)) (8.2.0)
    
2021-06-27T12:25:00.244+02:00   Requirement already satisfied: certifi&gt;=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests&lt;3.0,&gt;=2.8.1-&gt;moviepy==1.0.3-&gt;-r /opt/ml/model/code/requirements.txt (line 2)) (2020.12.5)
    
2021-06-27T12:25:00.244+02:00   Requirement already satisfied: idna&lt;2.9,&gt;=2.5 in /opt/conda/lib/python3.6/site-packages (from requests&lt;3.0,&gt;=2.8.1-&gt;moviepy==1.0.3-&gt;-r /opt/ml/model/code/requirements.txt (line 2)) (2.8)
    
2021-06-27T12:25:00.244+02:00   Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests&lt;3.0,&gt;=2.8.1-&gt;moviepy==1.0.3-&gt;-r /opt/ml/model/code/requirements.txt (line 2)) (1.25.11)
    
2021-06-27T12:25:00.244+02:00   Requirement already satisfied: chardet&lt;3.1.0,&gt;=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests&lt;3.0,&gt;=2.8.1-&gt;moviepy==1.0.3-&gt;-r /opt/ml/model/code/requirements.txt (line 2)) (3.0.4)
    
2021-06-27T12:25:00.244+02:00   Building wheels for collected packages: moviepy, proglog Building wheel for moviepy (setup.py): started Building wheel for moviepy (setup.py): finished with status 'done' Created wheel for moviepy: filename=moviepy-1.0.3-py3-none-any.whl size=110726 sha256=d90e44b117edbb3d061d7d3d800e6e687392ac6b06f9001f82559c4dd88f19c4 Stored in directory: /root/.cache/pip/wheels/be/dc/17/8b4d5a63bcd05dc44db7da57e193372ccd333617293f9deebe Building wheel for proglog (setup.py): started
    
2021-06-27T12:25:01.245+02:00   Building wheel for proglog (setup.py): finished with status 'done' Created wheel for proglog: filename=proglog-0.1.9-py3-none-any.whl size=6147 sha256=da1b510090c3b87cf4c564558a64b996bc1fdf34d6d15fd56c14c9c776f5b366 Stored in directory: /root/.cache/pip/wheels/e7/11/a0/7e65f734d33043735a557b1244569cca327353db9068158076
    
2021-06-27T12:25:01.245+02:00   Successfully built moviepy proglog
    
2021-06-27T12:25:01.245+02:00   Installing collected packages: proglog, imageio-ffmpeg, imageio, decorator, opencv-python-headless, moviepy, av
    
2021-06-27T12:25:02.246+02:00   Attempting uninstall: decorator Found existing installation: decorator 5.0.9 Uninstalling decorator-5.0.9:
    
2021-06-27T12:25:03.246+02:00   Successfully uninstalled decorator-5.0.9
    
2021-06-27T12:25:05.252+02:00   Successfully installed av-8.0.3 decorator-4.4.2 imageio-2.9.0 imageio-ffmpeg-0.4.4 moviepy-1.0.3 opencv-python-headless-4.5.2.54 proglog-0.1.9
    
2021-06-27T12:25:05.252+02:00   WARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv
    
2021-06-27T12:25:07.256+02:00   2021-06-27 10:25:06,537 [INFO ] main org.pytorch.serve.ModelServer -
    
2021-06-27T12:25:07.256+02:00   Torchserve version: 0.3.1
    
2021-06-27T12:25:07.256+02:00   TS Home: /opt/conda/lib/python3.6/site-packages
    
2021-06-27T12:25:07.256+02:00   Current directory: /

2021-06-27T12:25:07.256+02:00   Temp directory: /home/model-server/tmp
    
2021-06-27T12:25:07.257+02:00   Number of GPUs: 1
    
2021-06-27T12:25:07.257+02:00   Number of CPUs: 1
    
2021-06-27T12:25:07.257+02:00   Max heap size: 14097 M
    
2021-06-27T12:25:07.257+02:00   Python executable: /opt/conda/bin/python3.6
    
2021-06-27T12:25:07.257+02:00   Config file: /etc/sagemaker-ts.properties
    
2021-06-27T12:25:07.257+02:00   Inference address: http://0.0.0.0:8080
    
2021-06-27T12:25:07.257+02:00   Management address: http://0.0.0.0:8080
    
2021-06-27T12:25:07.257+02:00   Metrics address: http://127.0.0.1:8082
    
2021-06-27T12:25:07.257+02:00   Model Store: /.sagemaker/ts/models
    
2021-06-27T12:25:07.257+02:00   Initial Models: model.mar
    
2021-06-27T12:25:07.257+02:00   Log dir: /logs
    
2021-06-27T12:25:07.257+02:00   Metrics dir: /logs
    
2021-06-27T12:25:07.257+02:00   Netty threads: 0
    
2021-06-27T12:25:07.257+02:00   Netty client threads: 0
    
2021-06-27T12:25:07.257+02:00   Default workers per model: 1

2021-06-27T12:25:07.257+02:00   Blacklist Regex: N/A
    
2021-06-27T12:25:07.257+02:00   Maximum Response Size: 6553500
    
2021-06-27T12:25:07.258+02:00   Maximum Request Size: 6553500
    
2021-06-27T12:25:07.258+02:00   Prefer direct buffer: false
    
2021-06-27T12:25:07.258+02:00   Allowed Urls: [file://.*|http(s)?://.*]
    
2021-06-27T12:25:07.258+02:00   Custom python dependency for model allowed: false
    
2021-06-27T12:25:07.258+02:00   Metrics report format: prometheus
    
2021-06-27T12:25:07.258+02:00   Enable metrics API: true
    
2021-06-27T12:25:07.258+02:00   2021-06-27 10:25:06,597 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: model.mar
    
2021-06-27T12:25:09.259+02:00   2021-06-27 10:25:08,725 [INFO ] main org.pytorch.serve.archive.ModelArchive - eTag a6e950a7055442d88ff2f182fdef1da3
    
2021-06-27T12:25:09.259+02:00   2021-06-27 10:25:08,744 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model model loaded.
    
2021-06-27T12:25:09.259+02:00   2021-06-27 10:25:08,782 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
    
2021-06-27T12:25:09.259+02:00   2021-06-27 10:25:08,935 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
    
2021-06-27T12:25:09.259+02:00   2021-06-27 10:25:08,935 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
    
2021-06-27T12:25:09.259+02:00   2021-06-27 10:25:08,937 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
    
2021-06-27T12:25:09.259+02:00   2021-06-27 10:25:08,996 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.ts.sock.9000
    
2021-06-27T12:25:09.259+02:00   2021-06-27 10:25:08,999 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]59
    
2021-06-27T12:25:09.259+02:00   2021-06-27 10:25:08,999 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.
    
2021-06-27T12:25:09.259+02:00   2021-06-27 10:25:08,999 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.6.13
    
2021-06-27T12:25:09.259+02:00   2021-06-27 10:25:09,013 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000
    
2021-06-27T12:25:09.259+02:00   2021-06-27 10:25:09,039 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.ts.sock.9000.
    
2021-06-27T12:25:10.260+02:00   Model server started.
    
2021-06-27T12:25:10.260+02:00   2021-06-27 10:25:09,768 [INFO ] pool-2-thread-1 TS_METRICS - CPUUtilization.Percent:100.0|#Level:Host|#hostname:model.aws.local,timestamp:1624789509
    
2021-06-27T12:25:10.260+02:00   2021-06-27 10:25:09,776 [INFO ] pool-2-thread-1 TS_METRICS - DiskAvailable.Gigabytes:11.663764953613281|#Level:Host|#hostname:model.aws.local,timestamp:1624789509
    
2021-06-27T12:25:10.260+02:00   2021-06-27 10:25:09,777 [INFO ] pool-2-thread-1 TS_METRICS - DiskUsage.Gigabytes:12.690078735351562|#Level:Host|#hostname:model.aws.local,timestamp:1624789509
    
2021-06-27T12:25:10.260+02:00   2021-06-27 10:25:09,777 [INFO ] pool-2-thread-1 TS_METRICS - DiskUtilization.Percent:52.1|#Level:Host|#hostname:model.aws.local,timestamp:1624789509
    
2021-06-27T12:25:10.260+02:00   2021-06-27 10:25:09,778 [INFO ] pool-2-thread-1 TS_METRICS - MemoryAvailable.Megabytes:59581.45703125|#Level:Host|#hostname:model.aws.local,timestamp:1624789509
    
2021-06-27T12:25:10.260+02:00   2021-06-27 10:25:09,787 [INFO ] pool-2-thread-1 TS_METRICS - MemoryUsed.Megabytes:1238.21875|#Level:Host|#hostname:model.aws.local,timestamp:1624789509
    
2021-06-27T12:25:10.260+02:00   2021-06-27 10:25:09,788 [INFO ] pool-2-thread-1 TS_METRICS - MemoryUtilization.Percent:3.0|#Level:Host|#hostname:model.aws.local,timestamp:1624789509
    
2021-06-27T12:25:12.261+02:00   2021-06-27 10:25:11,779 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Generating new fontManager, this may take some time...
    
2021-06-27T12:25:13.262+02:00   2021-06-27 10:25:12,875 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Loading the model...
    
2021-06-27T12:25:13.262+02:00   2021-06-27 10:25:12,906 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Layers initiated...
    
2021-06-27T12:25:14.262+02:00   2021-06-27 10:25:13,871 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading: &quot;https://download.pytorch.org/models/r3d_18-b3b3357e.pth&quot; to /root/.cache/torch/hub/checkpoints/r3d_18-b3b3357e.pth
    
2021-06-27T12:25:14.262+02:00   2021-06-27 10:25:13,872 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -
    
2021-06-27T12:25:14.262+02:00   2021-06-27 10:25:13,972 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - 0%| | 0.00/127M [00:00&lt;?, ?B/s]
    
2021-06-27T12:25:14.262+02:00   2021-06-27 10:25:14,072 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - 6%|▌ | 7.30M/127M [00:00&lt;00:01, 76.5MB/s]
    
2021-06-27T12:25:14.262+02:00   2021-06-27 10:25:14,172 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - 16%|█▋ | 20.7M/127M [00:00&lt;00:00, 114MB/s]
    
2021-06-27T12:25:15.263+02:00   2021-06-27 10:25:14,272 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - 27%|██▋ | 34.0M/127M [00:00&lt;00:00, 126MB/s]
    
2021-06-27T12:25:15.263+02:00   2021-06-27 10:25:14,372 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - 38%|███▊ | 48.1M/127M [00:00&lt;00:00, 134MB/s]
    
2021-06-27T12:25:15.263+02:00   2021-06-27 10:25:14,472 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - 49%|████▉ | 62.1M/127M [00:00&lt;00:00, 139MB/s]
    
2021-06-27T12:25:15.263+02:00   2021-06-27 10:25:14,572 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - 60%|██████ | 76.8M/127M [00:00&lt;00:00, 144MB/s]
    
2021-06-27T12:25:15.263+02:00   2021-06-27 10:25:14,692 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - 72%|███████▏ | 91.9M/127M [00:00&lt;00:00, 149MB/s]
    
2021-06-27T12:25:15.263+02:00   2021-06-27 10:25:14,794 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - 83%|████████▎ | 106M/127M [00:00&lt;00:00, 140MB/s]
    
2021-06-27T12:25:15.263+02:00   2021-06-27 10:25:14,852 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - 94%|█████████▍| 120M/127M [00:00&lt;00:00, 139MB/s]
    
2021-06-27T12:25:15.263+02:00   2021-06-27 10:25:14,987 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Model initiated...
    
2021-06-27T12:25:15.263+02:00

2021-06-27 10:25:14,988 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Backend worker process died.
    
2021-06-27 10:25:14,988 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Backend worker process died.
    
2021-06-27T12:25:15.263+02:00   2021-06-27 10:25:14,988 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Traceback (most recent call last):
    
2021-06-27T12:25:15.263+02:00

2021-06-27 10:25:14,988 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File &quot;/opt/conda/lib/python3.6/site-packages/ts/model_service_worker.py&quot;, line 182, in &lt;module&gt;
    
2021-06-27 10:25:14,988 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - File &quot;/opt/conda/lib/python3.6/site-packages/ts/model_service_worker.py&quot;, line 182, in &lt;module&gt;
    
2021-06-27T12:25:15.263+02:00   2021-06-27 10:25:14,989 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - worker.run_server()
    
2021-06-27T12:25:15.263+02:00

2021-06-27 10:25:14,989 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File &quot;/opt/conda/lib/python3.6/site-packages/ts/model_service_worker.py&quot;, line 154, in run_server
    
2021-06-27 10:25:14,989 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - File &quot;/opt/conda/lib/python3.6/site-packages/ts/model_service_worker.py&quot;, line 154, in run_server
    
2021-06-27T12:25:15.263+02:00   2021-06-27 10:25:14,989 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - self.handle_connection(cl_socket)
    
2021-06-27T12:25:15.263+02:00

2021-06-27 10:25:14,989 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File &quot;/opt/conda/lib/python3.6/site-packages/ts/model_service_worker.py&quot;, line 116, in handle_connection
    
2021-06-27 10:25:14,989 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - File &quot;/opt/conda/lib/python3.6/site-packages/ts/model_service_worker.py&quot;, line 116, in handle_connection
    
2021-06-27T12:25:15.263+02:00

2021-06-27 10:25:14,989 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service, result, code = self.load_model(msg)
    
2021-06-27 10:25:14,989 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - service, result, code = self.load_model(msg)
    
2021-06-27T12:25:15.263+02:00

2021-06-27 10:25:14,989 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File &quot;/opt/conda/lib/python3.6/site-packages/ts/model_service_worker.py&quot;, line 89, in load_model
    
2021-06-27 10:25:14,989 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - File &quot;/opt/conda/lib/python3.6/site-packages/ts/model_service_worker.py&quot;, line 89, in load_model
    
2021-06-27T12:25:15.263+02:00

2021-06-27 10:25:14,990 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     service = model_loader.load(model_name, model_dir, handler, gpu, batch_size, envelope)
    
2021-06-27 10:25:14,990 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - service = model_loader.load(model_name, model_dir, handler, gpu, batch_size, envelope)
    
2021-06-27T12:25:15.263+02:00   2021-06-27 10:25:14,990 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - File &quot;/opt/conda/lib/python3.6/site-packages/ts/model_loader.py&quot;, line 104, in load
    
2021-06-27T12:25:15.263+02:00   2021-06-27 10:25:14,990 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - initialize_fn(service.context)
    
2021-06-27T12:25:15.263+02:00   2021-06-27 10:25:14,990 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - File &quot;/home/model-server/tmp/models/a6e950a7055442d88ff2f182fdef1da3/handler_service.py&quot;, line 51, in initialize
    
2021-06-27T12:25:15.264+02:00   2021-06-27 10:25:14,991 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - super().initialize(context)
    
2021-06-27T12:25:15.264+02:00   2021-06-27 10:25:14,991 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - File &quot;/opt/conda/lib/python3.6/site-packages/sagemaker_inference/default_handler_service.py&quot;, line 66, in initialize
    
2021-06-27T12:25:15.264+02:00   2021-06-27 10:25:14,991 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - self._service.validate_and_initialize(model_dir=model_dir)
    
2021-06-27T12:25:15.264+02:00   2021-06-27 10:25:14,991 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - File &quot;/opt/conda/lib/python3.6/site-packages/sagemaker_inference/transformer.py&quot;, line 158, in validate_and_initialize
    
2021-06-27T12:25:15.264+02:00   2021-06-27 10:25:14,991 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - self._model = self._model_fn(model_dir)
    
2021-06-27T12:25:15.264+02:00

2021-06-27 10:25:14,991 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   File &quot;/opt/ml/model/code/inference.py&quot;, line 105, in model_fn
    
2021-06-27 10:25:14,991 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - File &quot;/opt/ml/model/code/inference.py&quot;, line 105, in model_fn
    
2021-06-27T12:25:15.264+02:00

2021-06-27 10:25:14,992 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -     with open(os.path.join(model_dir, 'macro_model.pth'), 'rb') as f:
    
2021-06-27 10:25:14,992 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - with open(os.path.join(model_dir, 'macro_model.pth'), 'rb') as f:
    
2021-06-27T12:25:15.264+02:00

**2021-06-27 10:25:14,992 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - FileNotFoundError: [Errno 2] No such file or directory: '/home/model-server/tmp/models/a6e950a7055442d88ff2f182fdef1da3/macro_model.pth'**
    
**2021-06-27 10:25:14,992 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - FileNotFoundError: [Errno 2] No such file or directory: '/home/model-server/tmp/models/a6e950a7055442d88ff2f182fdef1da3/macro_model.pth'**
    
2021-06-27T12:25:15.264+02:00   2021-06-27 10:25:14,994 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - 100%|██████████| 127M/127M [00:00&lt;00:00, 136MB/s]
    
2021-06-27T12:25:15.264+02:00   2021-06-27 10:25:14,998 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
    
2021-06-27T12:25:15.264+02:00   2021-06-27 10:25:14,999 [WARN ] W-9000-model_1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.
</code></pre>",2,1,2021-06-27 10:59:55.807000 UTC,1.0,2021-06-27 12:11:09.140000 UTC,1,pytorch|endpoint|amazon-sagemaker,1535,2021-05-20 13:50:57.017000 UTC,2022-08-07 16:49:50.490000 UTC,,41,2,0,6,,,,,,['amazon-sagemaker']
package 'mlr' could not be loaded in azure machine learning,"<p>I am trying to use the <a href=""https://CRAN.R-project.org/package=unbalanced"" rel=""nofollow noreferrer"">unbalanced package in R</a> in an azure machine learning classification experiment.</p>

<p>The problem I'm running into is getting the package to be imported in R, specifically getting the mlr package dependency to be installed too.</p>

<p>I use the following R script to import the unbalanced package:</p>

<pre><code>install.packages(""src/colorspace_1.2-6.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/RColorBrewer_1.1-2.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/dichromat_2.0-0.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/munsell_0.4.3.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/labeling_0.3.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/tibble_1.2.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/DBI_0.5-1.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/BH_1.60.0-2.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/digest_0.6.10.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/gtable_0.2.0.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/scales_0.4.0.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/backports_1.0.3.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/chron_2.3-47.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/assertthat_0.1.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/jsonlite_1.1.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/magrittr_1.5.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/dplyr_0.5.0.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/lazyeval_0.2.0.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/htmltools_0.3.5.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/Rcpp_0.12.7.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/stringr_1.1.0.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/httpuv_1.3.3.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/mime_0.5.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/xtable_1.8-2.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/R6_2.1.3.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/sourcetools_0.1.5.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/BBmisc_1.10.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/ggplot2_2.1.0.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/ParamHelpers_1.9.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/stringi_1.1.1.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/checkmate_1.8.1.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/data.table_1.9.6.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/ggvis_0.4.3.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/parallelMap_1.3.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/plyr_1.8.4.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/reshape2_1.4.1.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/shiny_0.14.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/iterators_1.0.8.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/mlr_2.9.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/foreach_1.4.3.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/doParallel_1.0.10.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/FNN_1.1.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/RANN_2.5.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/unbalanced_2.0.zip"", lib = ""."", repos = NULL, verbose = TRUE)
library(unbalanced, lib.loc=""."", verbose=TRUE)
</code></pre>

<p>I have a zip archive of those zipped packages connected to the Execute R script module in Azure ML like this:</p>

<p><a href=""https://i.stack.imgur.com/9mvAQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9mvAQ.png"" alt=""enter image description here""></a></p>

<p>Also, the archive contains the mlr package and its dependencies, as the archive was created on the archived packages in</p>

<pre><code>C:\Users\my_user\AppData\Local\Temp\RtmpU3WpI6\downloaded_packages
</code></pre>

<p>I am using CRAN R 3.1.0 R version in Azure ML.</p>

<p>I get the following error:</p>

<pre><code>Error 0063: The following error occurred during evaluation of R script:
---------- Start of error message from R ----------
package 'mlr' could not be loaded
</code></pre>

<p>Changing to Microsoft R Open 3.2.2 doesn't work at all, too, as the error code says that the mlr package cannot be found.</p>

<p>The order I install those packages is the same error in which they are installed in R Studio whenever I perform a fresh</p>

<pre><code>install.packages(""unbalanced"")
</code></pre>

<p>I really need to use this package for my classification. Any ideas about solving this error?</p>

<p>Thank you!</p>",0,4,2016-09-16 08:52:17.750000 UTC,,,0,r|azure|rscript|azure-machine-learning-studio,314,2014-08-19 07:17:30.823000 UTC,2022-09-20 07:44:10.983000 UTC,"Bucharest, Romania",124,1,0,18,,,,,,['azure-machine-learning-studio']
deploy model as endpoint in databricks,"<p>after creating a simple keras model, I would like to deploy it as an endpoint for real-time inference in azure databricks. I created a simple cluster but unfortunately I ma not able to deploy the model itself. the deployment itself cannot be completed and the status is still yellow (pending)
<a href=""https://i.stack.imgur.com/RxbN6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RxbN6.png"" alt=""enter image description here"" /></a></p>
<p>can you please guide me where the error may be? what needs to be checked?  Thank you have a nice day</p>
<p>BR
Tomas</p>",0,1,2022-07-11 13:33:12.123000 UTC,,,0,python|databricks|mlflow,48,2020-08-24 21:06:06.307000 UTC,2022-07-27 08:33:48.143000 UTC,,21,0,0,4,,,,,,['mlflow']
"Set ""azureML.CLI Compatibility Mode"" in Visual Studio Code","<p>In <a href=""https://docs.microsoft.com/en-us/azure/machine-learning/how-to-debug-visual-studio-code"" rel=""nofollow noreferrer"">this</a> tutorial, it is asked to set the <code>azureML.CLI Compatibility Mode</code> setting to 1.0 for the Azure Machine Learning extension. However, when checking the settings of the extension, such option is not available.</p>",1,3,2022-07-28 01:41:59.603000 UTC,,2022-07-28 02:59:53.273000 UTC,0,azure|visual-studio-code|azure-machine-learning-service,62,2015-10-20 00:27:48.317000 UTC,2022-09-21 18:49:31.233000 UTC,"Vancouver, BC, Canada",2059,50,10,252,,,,,,['azure-machine-learning-service']
How do I invoke a data enrichment function before model.predict while serving the model in Databricks,"<p>In Databricks, I have used mlflow and got my model served through REST API. It works fine when all model features are provided. But my use case is that only a single feature (the primary key) will be provided by the consumer application, and my code has to lookup the other features from a database based on that key and then use the model.predict to return the prediction. I tried researching but understood that the REST endpoints will simply invoke the model.predict function. How can I make it invoke a data massaging function before predicting?</p>",1,0,2022-02-14 08:12:02.870000 UTC,,2022-02-14 10:52:46.403000 UTC,1,model|databricks|mlflow|serving,109,2018-07-09 13:16:02.080000 UTC,2022-07-06 10:09:41.380000 UTC,,11,0,0,1,,,,,,['mlflow']
Loading custom conda envs not working in SageMaker,"<p>I have installed <code>miniconda</code> on my AWS SageMaker persistent EBS instance. Here is my starting script:</p>
<pre class=""lang-sh prettyprint-override""><code>#!/bin/bash

set -e

# OVERVIEW
# This script installs a custom, persistent installation of conda on the Notebook Instance's EBS volume, and ensures
# that these custom environments are available as kernels in Jupyter.
# 
# The on-start script uses the custom conda environment created in the on-create script and uses the ipykernel package
# to add that as a kernel in Jupyter.
#
# For another example, see:
# https://docs.aws.amazon.com/sagemaker/latest/dg/nbi-add-external.html#nbi-isolated-environment

sudo -u ec2-user -i &lt;&lt;'EOF'
unset SUDO_UID
WORKING_DIR=/home/ec2-user/SageMaker/

for env in $WORKING_DIR/miniconda/envs/*; do
    BASENAME=$(basename &quot;$env&quot;)
    source &quot;$WORKING_DIR/miniconda/bin/activate&quot;
    source activate &quot;$BASENAME&quot;
    pip install ipykernel boto3
    python -m ipykernel install --user --name &quot;$BASENAME&quot; --display-name &quot;Custom ($BASENAME)&quot;
done
# Optionally, uncomment these lines to disable SageMaker-provided Conda functionality.
# echo &quot;c.EnvironmentKernelSpecManager.use_conda_directly = False&quot; &gt;&gt; /home/ec2-user/.jupyter/jupyter_notebook_config.py
# rm /home/ec2-user/.condarc
EOF

echo &quot;Restarting the Jupyter server..&quot;
restart jupyter-server
</code></pre>
<p>I use this in order to load my custom envs. However, when I access the JupyterLab interface, even if I see that the activated kernel is the Custom one, the only version of python running on my notebook kernel is <code>/home/ec2-user/anaconda3/envs/JupyterSystemEnv/bin/python</code>:</p>
<p><a href=""https://i.stack.imgur.com/yHOnG.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/yHOnG.png"" alt=""enter image description here"" /></a></p>
<p>I also inspected the CloudWatch logs, and I see this error log: <code>Could not find conda environment: [custom_env]</code>.</p>
<p>But, when I run the commands of the starting script within the JupyterLab terminal, conda succeeds in finding those envs. So the question is: what am I missing?</p>
<p>Thanks a lot.</p>",3,5,2021-08-23 12:34:50.767000 UTC,1.0,,10,python|amazon-web-services|conda|jupyter-lab|amazon-sagemaker,3163,2019-04-11 19:48:28.437000 UTC,2022-09-23 14:27:17.820000 UTC,Remote,309,1360,0,33,,,,,,['amazon-sagemaker']
PyTorch Lightning with Amazon SageMaker,"<p>We’re currently running using Pytorch Lightning for training outside of SageMaker. Looking to use SageMaker to leverage distributed training, checkpointing, model training optimization(training compiler) etc to accelerate training process and save costs. Whats the recommended way to migrate their PyTorch Lightning scripts to run on SageMaker?</p>",3,0,2022-09-10 14:09:35.707000 UTC,,,0,amazon-sagemaker,39,2014-10-07 08:13:42.830000 UTC,2022-09-23 14:45:05.230000 UTC,,26,0,0,3,,,,,,['amazon-sagemaker']
Deployed xgboost model fail with an error when calling prediction function,"<p>I am using Amazon SageMaker (SageMaker: 2.24.1) to build and deploy a model on the classical &quot;orders_with_predicted_value.csv&quot; dataset. Everything works fine with the training, and the model deploys well, but when I call the prediction method on the test data I get the following error:</p>
<blockquote>
<p>AttributeError: (&quot;'NoneType' object has no attribute 'Accept'&quot;,
'occurred at index 0')</p>
</blockquote>
<p>I opened the prediction function of SageMaker trying to solve this issue but to no effect.</p>
<p>Here's my code:
<img src=""https://i.stack.imgur.com/szL2n.jpg"" alt=""1"" /></p>",0,2,2021-02-23 16:25:08.987000 UTC,,2021-02-23 17:57:42.383000 UTC,0,python|amazon-s3|xgboost|amazon-sagemaker,123,2021-02-23 16:12:41.077000 UTC,2021-12-05 19:20:32.340000 UTC,"Strasbourg, France",1,0,0,2,,,,,,['amazon-sagemaker']
Register model version via MLflow Python API while carrying over input/output schemas,"<p>I am trying to use the mlflow Python API to register models to an mlflow server's model registry. And in fact this does work:</p>
<pre><code>model_version = client.create_model_version(
    source=run.info.artifact_uri,
    name='my_model',
    run_id=run.info.run_id)    
</code></pre>
<p>Meaning the newly registered model version appears in the server's web UI.</p>
<p>However, the input and output schemas that have been logged with the respective run are not carried over to the new model version (i.e. in the UI, the &quot;Schema&quot; tab under &quot;Registered Models &gt; my_model &gt; Version 2&quot; is empty). When I instead register the new model version via the web UI, the input and output schemas do appear there.</p>
<p>How can I get mlflow to carry over the schemas even when using the Python API?</p>
<p>Thanks!</p>",0,0,2022-04-05 12:13:17.570000 UTC,,2022-04-08 11:10:42.090000 UTC,0,mlflow|mlops,118,2020-03-31 09:43:43.903000 UTC,2022-09-22 18:39:23.323000 UTC,"Bochum, Germany",1,0,0,5,,,,,,['mlflow']
Copying/Migrating AzureML Service Experiments,"<p>I am using azure machine learning services. I have multiple experiments in my Azure Machine Learning Workspace. Now I want to copy or migrate my ml experiments on different subscription, so I can use my experiments on a different workspace subscription.</p>

<p>Is there any possible way to migrate or copy AzureML experiments to reuse ?</p>

<p>Thank you</p>

<p>Regards,</p>

<p>Ahmad</p>",1,0,2020-04-02 15:04:33.393000 UTC,,,1,azure-machine-learning-service,225,2020-01-24 07:14:02.550000 UTC,2020-04-30 14:52:33.557000 UTC,,11,0,0,13,,,,,,['azure-machine-learning-service']
Why is this SQLite query not letting me cast the integer and subtract?,"<p>I have multiple columns in Azure Machine Learning that each have an hour, year, day, minute, etc for a date. I need to convert this hour from UTC to EDT, and then make it a date string such as</p>

<blockquote>
  <p>YYYY/MM/DD HH:SS</p>
</blockquote>

<p>This way, I can do an inner join. I've tried using CAST, CONVERT, and other SQLite functions, but none of these combos work. Here is where I am now: </p>

<pre><code>select *
CAST([Col11] as int) -4 as EDTHour

([Col8] || '/' || [Col9] || '/' || [Col10] || ' ' || EDTHour|| ':' || [Col12]) as WeatherTime from t1

select 'Time Stamp' as secondTableTime from t2

SELECT *
FROM t1
INNER JOIN t2
ON t1.WeatherTime=t2.secondTableTime
</code></pre>

<p>However, It never lets me cast the varchar column Col11 to a integer or decimal. What am I missing? </p>",1,0,2016-03-17 14:27:47.397000 UTC,,2016-03-18 21:23:07.940000 UTC,0,sqlite|azure|timezone|cortana-intelligence|azure-machine-learning-studio,64,2014-01-07 15:20:27.810000 UTC,2020-02-03 17:09:21.847000 UTC,Texas,2420,480,3,296,,,,,,['azure-machine-learning-studio']
How to know training status on aws sagemaker if I lost internet connection and returned on the kernel,"<p>I was training in aws Sagemaker Jupyter Lab( p3.2xlarge instance) and It was LSTM network and suddenly internet connection got lost and When I returned to the kernel when I got internet connection back..I could not see any training epochs updated.</p>
<p>Does anyone know that can I access what is processing in the kernel by terminal or something so I will know the training status.</p>",1,0,2020-07-07 14:48:49.227000 UTC,,2020-07-07 14:56:04.540000 UTC,0,amazon-web-services|lstm|amazon-sagemaker,368,2018-07-26 07:55:25.750000 UTC,2022-09-23 13:58:07.923000 UTC,"Pune, Maharashtra, India",69,10,0,12,,,,,,['amazon-sagemaker']
Update Azure ML realtime endpoint,"<p>I'm generating a machine learning modell using Azure Auto ML. Is there a way to update my published real-time endpoint without deleting it first?
Thanks in advance.</p>",1,0,2020-11-17 10:12:51.983000 UTC,1.0,,1,azure|azure-machine-learning-studio|azure-machine-learning-service,227,2019-08-20 14:39:14.020000 UTC,2021-08-27 09:32:47.737000 UTC,,21,0,0,17,,,,,,"['azure-machine-learning-studio', 'azure-machine-learning-service']"
Mlflow authorization with spnego,"<p>I saw this topic about Kerberos authntication - <a href=""https://github.com/mlflow/mlflow/issues/2678"" rel=""nofollow noreferrer"">https://github.com/mlflow/mlflow/issues/2678</a> . It was in 2020 . Our team trying to do authentication with kerberos by spnego. We did spnego on nginx server and it is fine - and get code 200 when we do curl to mlflow http uri . BUT we can't do it with mlflow environment variable .</p>
<p>The question is - Does mlflow has some feature to make authentication with spnego or not? Or it has just these environment variables for authentication and such methods :</p>
<ul>
<li>MLFLOW_TRACKING_USERNAME and MLFLOW_TRACKING_PASSWORD - username and password to use with HTTP Basic authentication. To use Basic authentication, you must set both environment variables .</li>
<li>MLFLOW_TRACKING_TOKEN - token to use with HTTP Bearer authentication. Basic authentication takes precedence if set.</li>
<li>MLFLOW_TRACKING_INSECURE_TLS - If set to the literal true, MLflow does not verify the TLS connection, meaning it does not validate certificates or hostnames for https:// tracking URIs. This flag is not recommended for production environments. If this is set to true then MLFLOW_TRACKING_SERVER_CERT_PATH must not be set.</li>
<li>MLFLOW_TRACKING_SERVER_CERT_PATH - Path to a CA bundle to use. Sets the verify param of the requests.request function (see <a href=""https://requests.readthedocs.io/en/master/api/"" rel=""nofollow noreferrer"">https://requests.readthedocs.io/en/master/api/</a>). When you use a self-signed server certificate you can use this to verify it on client side. If this is set MLFLOW_TRACKING_INSECURE_TLS must not be set (false).</li>
<li>MLFLOW_TRACKING_CLIENT_CERT_PATH - Path to ssl client cert file (.pem). Sets the cert param of the requests.request function (see <a href=""https://requests.readthedocs.io/en/master/api/"" rel=""nofollow noreferrer"">https://requests.readthedocs.io/en/master/api/</a>). This can be used to use a (self-signed) client certificate.</li>
</ul>",1,0,2022-08-11 13:44:58.580000 UTC,,,1,authentication|kerberos|mlflow|spnego,25,2018-09-07 16:23:40.893000 UTC,2022-09-04 09:09:48.817000 UTC,,21,0,0,1,,,,,,['mlflow']
Returning DataFrame instead of Dictionary after using FB Prophet with multiple categories,"<p>I did a basic forecast with python using FB Prophet for multiple categories. The Dataset looks something like this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>timestamp</th>
<th>type</th>
<th>value</th>
</tr>
</thead>
<tbody>
<tr>
<td>2022-01-01</td>
<td>type A</td>
<td>12345</td>
</tr>
<tr>
<td>2022-01-01</td>
<td>type B</td>
<td>12345</td>
</tr>
<tr>
<td>2022-01-01</td>
<td>type C</td>
<td>12345</td>
</tr>
<tr>
<td>2022-02-01</td>
<td>type A</td>
<td>12345</td>
</tr>
<tr>
<td>2022-02-01</td>
<td>type B</td>
<td>12345</td>
</tr>
<tr>
<td>2022-02-01</td>
<td>type C</td>
<td>12345</td>
</tr>
</tbody>
</table>
</div>
<p>When using the following code:</p>
<pre><code>from distutils.command.clean import clean

import pandas as pd
import os

def azureml_main(df = None, df2 = None):

    os.system(f&quot;pip install pystan==2.19.1.1&quot;)
    os.system(f&quot;pip install prophet&quot;)

    from prophet import Prophet

    prediction = {}
    
    cleaned_df = df[[&quot;DATE_KJ&quot;, &quot;VALUE&quot;, &quot;KSTAR&quot;]]
    cleaned_df.rename(columns = {'DATE_KJ':'ds', 'VALUE':'y', 'KSTAR':'article'}, inplace = True)

    list_articles = cleaned_df.article.unique()

    for article in list_articles:
        article_df = cleaned_df.loc[cleaned_df['article'] == article]
        m = Prophet()
        m.fit(article_df)

        future = m.make_future_dataframe(periods=18, freq = &quot;M&quot;)
        forecast = m.predict(future)

        prediction[article] = forecast

    return prediction,
</code></pre>
<p>I get the error:</p>
<pre><code>Got exception when invoking script: 'TypeError: Unsupported return type. Failed to convert return value from type &lt;class 'dict'&gt; to type pd.DataFrame.'.
</code></pre>
<p>This is because I use it in AML Studio and the return value needs to be a pandas DataFrame, currently it is a dictionary. I tried converting it with <code>pandas.DataFrame.from_dict</code> but this didn't help.</p>
<p>I checked <a href=""https://stackoverflow.com/questions/18837262/convert-python-dict-into-a-dataframe"">this</a> thread but i don't get how to use it with my problem.</p>
<p>Anyone got a solution for my problem?</p>
<p>Thanks in advance!</p>",0,0,2022-05-04 12:54:05.860000 UTC,,2022-05-04 13:18:20.790000 UTC,0,python|pandas|dataframe|azure|azure-machine-learning-studio,59,2020-06-15 13:06:57.127000 UTC,2022-09-22 19:00:57.357000 UTC,Germany,13,0,0,4,,,,,,['azure-machine-learning-studio']
What framework does Sagemaker use to train a model?,"<p>Models trained on Sagemaker produced <code>model.tar.gz</code> files as output.<br />
I want to use these files for prediction on my local machine.</p>
<p>I'm not sure which framework is being used by Sgaemaker to train these models.</p>
<p>I guess I'd have to use the same framework to load the model artifacts for prediction.</p>",1,0,2021-12-27 07:38:08.077000 UTC,,,0,amazon-web-services|amazon-sagemaker,27,2019-03-04 12:06:56.487000 UTC,2022-09-23 07:48:34.613000 UTC,,735,167,15,106,,,,,,['amazon-sagemaker']
Does MLflow support distributed XGBoost model?,"<p>I  use single-instance xgboost model to train, which works fine with mlflow to log all the model-related parameters by using <code>mlflow.xgboost.autolog()</code>, but when i change to distributed xgboost version changing from python package to JVM package by including the xgboost4j.jar and xgboost4j-spark.jar files, and also include mlflow module into it, (<code>mlflow.xgboost.autolog()</code>). The mlflow cannot show all the parameters on the page. They are empty.
<img src=""https://i.stack.imgur.com/dt91a.png"" alt=""screenshot"" /></p>
<p>So I looked at the source code in the mlflow.xgboost, in line 271, <code>def autolog(importance_types=[&quot;weight&quot;]):</code> says it imports the xgboost package, which i think is the single-instance xgboost model, I wonder if it is support the distributed version? Or is there any other methods to solve the problem?</p>",0,0,2020-10-12 06:44:14.150000 UTC,,2020-10-12 08:50:46.443000 UTC,2,xgboost|mlflow,133,2019-08-25 23:46:06.673000 UTC,2020-11-12 10:03:15.497000 UTC,,21,0,0,0,,,,,,['mlflow']
Two-Class-Logistic VS Binary Logistic Regression,"<p>During a test project on Azure Machine Learning Studio I have some questions based on my understandings.
In my project (in R) I have used Binary Logistic Regression, but in AML I found two Logistic regression Two-Class and MultiClass. So in that case I have used two-class Logistic regression. Am I Right in this case?</p>
<p>In another case during running glm() in R tool it perform Logistic regression and after summary(loreg Eqn) it provides the each variable's co-efficient &amp; estimates.</p>
<p>From R I have the following output:</p>
<p><a href=""https://i.stack.imgur.com/vjcU2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vjcU2.png"" alt=""enter image description here"" /></a></p>
<p>From AML after right-clicking Train Model and visualize:</p>
<p><a href=""https://i.stack.imgur.com/YRgEA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YRgEA.png"" alt=""enter image description here"" /></a></p>
<p>The weight in the above Picture is the estimates, am I right (Dataset is diff)?</p>",1,0,2017-02-27 11:33:23.623000 UTC,,2022-06-27 13:28:22.183000 UTC,0,azure|machine-learning|azure-machine-learning-studio,650,2016-02-22 07:55:13.720000 UTC,2022-09-23 07:02:01.480000 UTC,"Madhyamgram, Kolkata, West Bengal, India",858,7,1,121,,,,,,['azure-machine-learning-studio']
"azure machine learning studio ""execute R script"" is unable to load ggplot2 library from scriptbundle.zip","<p>Rscript:</p>

<p>install.packages(""src/Rpackages/Rcpp.zip"", lib = ""."", repos = NULL, verbose = TRUE)<br>
library(Rcpp, lib.loc=""."", verbose=TRUE) 
install.packages(""src/Rpackages/ggplot2_2.2.1.zip"", lib = ""."", repos = NULL, verbose = TRUE)<br>
library(ggplot2, lib.loc=""."", verbose=TRUE) </p>

<p>Error :</p>

<p>requestId = 2719717d8d5b4a479547886c19b0bcb4 errorComponent=Module. taskStatusCode=400. {""Exception"":{""ErrorId"":""FailedToEvaluateRScript"",""ErrorCode"":""0063"",""ExceptionType"":""ModuleException"",""Message"":""Error 0063: The following error occurred during evaluation of R script:\r\n---------- Start of error message from R ----------\r\npackage or namespace load failed for 'ggplot2'\r\n\r\n\r\npackage or namespace load failed for 'ggplot2'\r\n----------- End of error message from R -----------""}}Error: Error 0063: The following error occurred during evaluation of R script:---------- Start of error message from R ----------package or namespace load failed for 'ggplot2'package or namespace load failed for 'ggplot2'----------- End of error message from R ----------- Process exited with error code -2</p>

<p><a href=""https://i.stack.imgur.com/JDbHN.png"" rel=""nofollow noreferrer"">azuremlstudio snapshot</a></p>",1,1,2017-03-22 23:34:01.677000 UTC,,,0,azure-machine-learning-studio,272,2015-11-06 21:52:07.553000 UTC,2017-10-05 19:18:14.970000 UTC,"Seattle, WA, United States",1,0,0,3,,,,,,['azure-machine-learning-studio']
Truncate Table in Azure SQL Database for Azure ML Experiment,"<p>In my Azure ML experiment I am using a writer to write data into a table in Azure SQL Database. However, I would like to truncate the data in that table before each insert. Is there any way that I can achieve this through the experiment itself? Any inbuilt module through which I can achieve this?</p>

<p>I know from sql using triggers I can achieve this. </p>",1,3,2015-12-08 16:53:15.153000 UTC,,2015-12-09 18:00:18.627000 UTC,2,tsql|azure-sql-database|azure-machine-learning-studio,436,2015-12-08 16:51:25.033000 UTC,2020-10-12 14:17:25.503000 UTC,,101,4,0,16,,,,,,['azure-machine-learning-studio']
How to label a text with multiple paragraphs in AWS Ground Truth?,"<p>I was trying setup a single label labeling task in AWS Groundtruth through the console. My goal is to match some users in social media and for each user I have several possible candidates out of which one should be selected (label). My CSV looks like this:</p>
<pre><code>firtname | lastname | candidates

Romeo      Montague      x
Juliet     Capulet       x
</code></pre>
<p>Instead of &quot;x&quot;, I would like to have something like this</p>
<pre><code>candidate_1
description_1
link_1

candidate_2
description_2
link_2

candidate_3
description_3
link_3
</code></pre>
<p>The human worked should then select whereas the correct label is candidate_1, candidate_2 or candidate_3 or none of the above.</p>
<p>I am aware Sagemaker ground truth does not accept new lines characters and that it renders it in HTML so I tried to input the following:</p>
<pre><code>candidate_1 &lt;br/&gt; description &lt;br/&gt; link &lt;br/&gt;&lt;br/&gt; candidate_2 &lt;br/&gt; description &lt;br/&gt; link &lt;br/&gt;&lt;br/&gt; candidate_3 &lt;br/&gt; description &lt;br/&gt; link &lt;br/&gt;&lt;br/&gt; 
</code></pre>
<p>unfortunately, when I take a look at the console, the input on the left does not render correctly:</p>
<p><a href=""https://i.stack.imgur.com/kfuiE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kfuiE.png"" alt=""enter image description here"" /></a></p>
<p>The line breaks within the div tag seem to be simply ignored by the UI.</p>
<p>I found <a href=""https://forums.aws.amazon.com/thread.jspa?threadID=302236"" rel=""nofollow noreferrer"">this post</a> which contains the answer but I am struggling to adapt to my concrete use case.</p>
<p>How can I change my csv so that the multiple paragraphs get rendered corrected?</p>",0,2,2021-05-26 09:16:33.420000 UTC,,2021-05-26 11:54:00.030000 UTC,1,amazon-web-services|text|amazon-sagemaker|amazon-ground-truth,161,2017-03-24 14:05:50.000000 UTC,2022-09-24 21:27:16.943000 UTC,"Zürich, Suïssa",921,75,10,147,,,,,,['amazon-sagemaker']
Get auto-generated OutputFileDatasetConfig destination,"<p>From the <code>OutputFileDatasetConfig</code> documentation for the <code>destination</code> class member,</p>
<blockquote>
<p>If set to None, we will copy the output to the workspaceblobstore datastore, under the path /dataset/{run-id}/{output-name}</p>
</blockquote>
<p>Given I have the handle to such <code>OutputFileDatasetConfig</code> with <code>destination</code> set to <code>None</code>, how can I get the generated <code>destination</code> without recomputing the default myself as this can be subject to change.</p>",2,0,2021-09-15 18:29:30.063000 UTC,,,1,azure-machine-learning-service|azureml-python-sdk,252,2012-01-26 14:27:40.553000 UTC,2022-09-24 16:26:41.580000 UTC,,802,288,0,91,,,,,,['azure-machine-learning-service']
Installing Torch on Amazon Linux,"<p>This took me an excruciatingly long time for me to figure out so I decided to save people the trouble.</p>

<p>Trying to install torch on an ec2 instance with Amazon Linux(redhat) failed with the following command:</p>

<pre><code>git clone https://github.com/torch/distro.git ~/torch --recursive
cd ~/torch; bash install-deps;
./install.sh
</code></pre>

<p>It gave me the error:</p>

<pre><code>OpenBLAS Failed to compile
</code></pre>",1,0,2018-10-23 01:10:24.407000 UTC,,,1,amazon-ec2|torch|amazon-sagemaker,478,2014-06-24 20:18:14.843000 UTC,2022-05-25 01:28:11.310000 UTC,"Cambridge, MA, United States",754,127,3,38,,,,,,['amazon-sagemaker']
How do I transform a list of items into groups to predict group ratings in azure machine learning?,"<p>I'm newbie to azure machine learning and I'm trying to build a model that rates groups of items.</p>

<p>My data is a file with a list of items with features (small list - less than 80 items) and I need to make groups (of diferent sizes - groups of 2, 3, 4,... 10 items, for all the possible combinations) so that the model rate those groups (rates from 1 to 10). I also have some group rates to train the model.</p>

<p>I don't know how to transform the items into groups. </p>

<p>Another thing is, I'm not sure which model is the best. From all I gather, I think that a multiclass classification is the most suitable for this problem. Is it?</p>

<p>Thank you in advance and sorry for any grammar error in my text. </p>",1,0,2017-03-21 15:17:36.700000 UTC,,,0,azure-machine-learning-studio,32,2017-03-21 15:13:53.573000 UTC,2017-04-18 11:52:40.780000 UTC,,1,0,0,2,,,,,,['azure-machine-learning-studio']
Batch prediction Input,"<p>I have a tensorflow model deployed on Vertex AI of Google Cloud. The model definition is:</p>
<pre><code>item_model = tf.keras.Sequential([
  tf.keras.layers.StringLookup(
      vocabulary=item_vocab, mask_token=None),
  tf.keras.layers.Embedding(len(item_vocab) + 1, embedding_dim)
])

user_model = tf.keras.Sequential([
  tf.keras.layers.StringLookup(
      vocabulary=user_vocab, mask_token=None),
  # We add an additional embedding to account for unknown tokens.
  tf.keras.layers.Embedding(len(user_vocab) + 1, embedding_dim)
])


class NCF_model(tf.keras.Model):
    def __init__(self,user_model, item_model):
        super(NCF_model, self).__init__()
        # define all layers in init
        
        self.user_model = user_model
        self.item_model  = item_model
        self.concat_layer   = tf.keras.layers.Concatenate()
        self.feed_forward_1 = tf.keras.layers.Dense(32,activation= 'relu')
        self.feed_forward_2 = tf.keras.layers.Dense(64,activation= 'relu')
        self.final = tf.keras.layers.Dense(1,activation= 'sigmoid')


    def call(self, inputs ,training=False):
        user_id , item_id = inputs[:,0], inputs[:,1]
        x = self.user_model(user_id)
        y = self.item_model(item_id)

        x = self.concat_layer([x,y])
        x = self.feed_forward_1(x)
        x = self.feed_forward_2(x)
        x = self.final(x)


        return x
</code></pre>
<p>The model has two string inputs and it outputs a probability value.
When I use the following input in the batch prediction file, I get an empty prediction file.
Sample of csv input file:</p>
<pre><code>userid,itemid
yuu,190767
yuu,364
yuu,154828
yuu,72998
yuu,130618
yuu,183979
yuu,588
</code></pre>
<p>When I use a jsonl file with the following input.</p>
<pre><code>{&quot;input&quot;:[&quot;yuu&quot;, &quot;190767&quot;]}
</code></pre>
<p>I get the following error.</p>
<pre><code>('Post request fails. Cannot get predictions. Error: Exceeded retries: Non-OK result 400 ({\n    &quot;error&quot;: &quot;Failed to process element: 0 key: input of \'instances\' list. Error: INVALID_ARGUMENT: JSON object: does not have named input: input&quot;\n}) from server, retry=3.', 1)
</code></pre>
<p>What seems to be going wrong with these inputs?</p>",1,1,2022-01-16 11:25:44.157000 UTC,,,0,google-cloud-platform|google-ai-platform|google-cloud-vertex-ai,323,2015-04-20 12:22:18.823000 UTC,2022-09-24 14:16:22.667000 UTC,"Gurugram, Haryana, India",363,26,0,32,,,,,,['google-cloud-vertex-ai']
"Trying to deploy machine learning model on kubernettes, getting failed with ModuleNotFoundError: No module named 'Cython' or 'setuptools_rust'","<p>Here is my environment yml file :-</p>
<pre><code># Conda environment specification. The dependencies defined in this file will
# be automatically provisioned for runs with userManagedDependencies=False.
# Details about the Conda environment file format:
# https://conda.io/docs/user-guide/tasks/manage-environments.html#create-env-file-manually


name: project_environment
dependencies:
  # The python interpreter version.

  # Currently Azure ML only supports 3.5.2 and later.

- python
- pip

- pip:
    # Required packages for AzureML execution, history, and data preparation.

  - azureml-defaults
  - scikit-learn

  - numpy
  - azureml-monitoring
  - cython
  - setuptools_rust
</code></pre>
<p>And it is failing on below code:</p>
<pre><code>-
deployment_config = AksWebservice.deploy_configuration(auth_enabled=False, collect_model_data=True, enable_app_insights=True, cpu_cores = 2, memory_gb = 2)
aks_target = AksCompute(ws,aks_name)
(On below line getting error)
service = Model.deploy(ws, service_name, [model], inference_config, deployment_config, aks_target)
service.wait_for_deployment(show_output = True)
</code></pre>",1,1,2021-07-19 23:27:10.410000 UTC,,2021-07-22 19:27:52.303000 UTC,-2,azure-machine-learning-studio|azure-machine-learning-service,151,2021-07-19 23:18:48.023000 UTC,2021-09-08 18:37:11.090000 UTC,,1,0,0,1,,,,,,"['azure-machine-learning-studio', 'azure-machine-learning-service']"
Error in deploying model on azure kubernetes service,"<p>I am deploying a model on Azure Machine Learning studio using azure kubernetes service</p>
<pre><code>env = Environment(name='ocr')
aks_name = 'ocr-compute-2'
# Create the cluster
aks_target = AksCompute(ws, aks_name)




env.python.conda_dependencies.add_pip_package('google-cloud-vision')
env.python.conda_dependencies.add_pip_package('Pillow')
env.python.conda_dependencies.add_pip_package('Flask == 2.2.2')

env.python.conda_dependencies.add_pip_package('azureml-defaults')


inference_config = InferenceConfig(environment=env, source_directory='./', entry_script='./run1.py')

deployment_config = AksWebservice.deploy_configuration(autoscale_enabled=True, 
                                                autoscale_target_utilization=20,
                                                autoscale_min_replicas=1,
                                                autoscale_max_replicas=4)
</code></pre>
<p>I am getting this error</p>
<pre><code>  &quot;statusCode&quot;: 400,
  &quot;message&quot;: &quot;Kubernetes Deployment failed&quot;,
  &quot;details&quot;: [
    {
      &quot;code&quot;: &quot;CrashLoopBackOff&quot;,
      &quot;message&quot;: &quot;Your container application crashed as it does not have AzureML serving stack.
Make sure you have 'azureml-defaults&gt;=1.0.45' package in your pip dependencies, it contains requirements for the AzureML serving stack.&quot;
    }
</code></pre>
<p>Will be great if I can know what I am missing here.</p>",1,3,2022-08-23 08:59:16.733000 UTC,,,1,python|kubernetes|azure-devops|azure-machine-learning-service|azure-container-instances,81,2019-09-07 18:22:12.003000 UTC,2022-08-28 17:07:56.487000 UTC,"Lahore, Pakistan",137,22,0,100,,,,,,['azure-machine-learning-service']
Text Mining with Chinese Characters in Azure Machine Learning or R Language,"<p>Did anyone know that Text Mining with <strong>Chinese Characters</strong> in Azure Machine Learning or R Language is supported or not?</p>

<p>thanks. </p>",1,0,2015-07-30 21:01:40.490000 UTC,,,0,r|text-mining|azure-machine-learning-studio,296,2014-09-13 16:36:17.787000 UTC,2019-09-21 07:22:07.513000 UTC,,427,30,0,45,,,,,,['azure-machine-learning-studio']
Is it possible to use MLFlow and H2o.ai sparkling water in a Scala based project?,"<p>I'm solving a Scala data science problem in Intellij using maven. I noticed that MLFlow spark (<a href=""https://mvnrepository.com/artifact/org.mlflow/mlflow-spark/1.5.0"" rel=""nofollow noreferrer"">https://mvnrepository.com/artifact/org.mlflow/mlflow-spark/1.5.0</a>) is dependent on scala 2.12 while h2o.ai sparkling water is dependent on scala 2.11 (<a href=""https://mvnrepository.com/artifact/ai.h2o/sparkling-water-core"" rel=""nofollow noreferrer"">https://mvnrepository.com/artifact/ai.h2o/sparkling-water-core</a>). Is there any way to use both of these together using Scala? </p>",1,1,2020-01-23 21:54:13.420000 UTC,,,0,scala|apache-spark|h2o|sparkling-water|mlflow,217,2020-01-23 21:45:47.657000 UTC,2022-08-31 03:39:00.247000 UTC,,1,0,0,2,,,,,,['mlflow']
Sagemaker endpoint inference with third-party libraries,"<p>I'm trying to create an endpoint with custom libraries, particularly trying to install <code>fastai</code>.
Going over this <a href=""https://course.fast.ai/deployment_amzn_sagemaker.html#setup-your-sagemaker-notebook-instance"" rel=""nofollow noreferrer"">tutorial</a> which is apparently incomplete. Endpoint deployment was failing at health check because of the missing library.
<br>This <a href=""https://sagemaker.readthedocs.io/en/stable/using_pytorch.html#using-third-party-libraries"" rel=""nofollow noreferrer"">doc</a> says I can pass a list of third-party libraries in <code>requirements.txt</code> file. The file should be in <code>code</code> folder inside the <code>model.tar.gz</code> archive. This actually lets me deploy the endpoint with no issues but still fails when running predictions: <code>Received server error (500) from model with message ""No module named 'fastai'""</code>.
<br>Using PyTorch 1.3.1 estimator.
<br>Contents of the <code>requirements.txt</code> is just <code>fastai</code>.
<br>Wondering what could have gone wrong.</p>",1,0,2020-02-05 21:41:15.580000 UTC,,,3,pytorch|endpoint|amazon-sagemaker|fast-ai,310,2014-10-03 14:59:22.923000 UTC,2022-09-23 18:31:23.337000 UTC,,131,14,0,13,,,,,,['amazon-sagemaker']
AWS SageMaker Processing job,"<p>I was able to run a simple python code in Notebook instance to read and write csv files from/to S3 bucket. Now I want to create the SageMaker processing job to run the same code without any input/output data configuration. I have downloaded the same code and pushed the image to ECR repository. How to run this code in processing job and it should be able to install 's3fs' module?I just want to run python code in processing jobs without giving any input/output algorithms/configuration. Used boto3 to read/write from s3 bucket. With the current code it's stuck in &quot;In Progress&quot;
<a href=""https://i.stack.imgur.com/Ir1XD.png"" rel=""nofollow noreferrer"">downloaded code in vs code</a></p>
<p><a href=""https://i.stack.imgur.com/eGa2c.png"" rel=""nofollow noreferrer"">downloaded code in vs code</a></p>
<pre><code>!pip install s3fs
import boto3
import pandas as pd
from io import StringIO
client = boto3.client('s3')
path = 's3://weatheranalysis/weatherset.csv'
df = pd.read_csv(path)
df.head()
filename = 'newdata.csv'
bucketName = 'weatheranalysis'

csv_buffer = StringIO()

df.to_csv(csv_buffer)

client = boto3.client('s3')

response = client.put_object(
ACL='private',
Body = csv_buffer.getvalue(),
Bucket =bucketName,
Key = filename
</code></pre>
<p>)</p>",1,0,2021-02-19 07:04:49.147000 UTC,,2021-02-19 07:10:10.660000 UTC,0,amazon-web-services|amazon-sagemaker,1249,2020-07-11 11:40:40.987000 UTC,2022-09-10 17:31:25.400000 UTC,"Hyderabad, Telangana, India",1,0,0,6,,,,,,['amazon-sagemaker']
How to resolve the AttributeError: 'NoneType' object has no attribute 'CONTENT_TYPE',"<p>I am working on a recommendation engine by using MXNET on Sagemaker by following a tutorial.</p>
<p>After executing the following cell I am getting the AttributeError: 'NoneType' object has no attribute 'CONTENT_TYPE'</p>
<pre><code>test_preds= []
for array in np.array_split(test_df[['customer_id', 
'product_id']].values,40):
    test_preds += 
predictor.predict(json.dumps({'customer_id':array[:,0].tolist(),
                               'product_id':array[:,1].tolist()}))
test_preds = np.array(test_preds)
print('MSE:', np.mean((test_df['star_rating'] -test_preds))**2)
</code></pre>
<p>The error shown:</p>
<pre><code>---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-38-a830dbc6ac84&gt; in &lt;module&gt;
      2 for array in np.array_split(test_df[['customer_id', 'product_id']].values,40):
      3     test_preds += predictor.predict(json.dumps({'customer_id':array[:,0].tolist(),
----&gt; 4                                    'product_id':array[:,1].tolist()}))
      5 test_preds = np.array(test_preds)
      6 print('MSE:', np.mean((test_df['star_rating'] -test_preds))**2)

~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/sagemaker/predictor.py in predict(self, data, initial_args, target_model, target_variant, inference_id)
    132 
    133         request_args = self._create_request_args(
--&gt; 134             data, initial_args, target_model, target_variant, inference_id
    135         )
    136         response = self.sagemaker_session.sagemaker_runtime_client.invoke_endpoint(**request_args)

~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/sagemaker/predictor.py in _create_request_args(self, data, initial_args, target_model, target_variant, inference_id)
    158 
    159         if &quot;ContentType&quot; not in args:
--&gt; 160             args[&quot;ContentType&quot;] = self.content_type
    161 
    162         if &quot;Accept&quot; not in args:

~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/sagemaker/predictor.py in content_type(self)
    511     def content_type(self):
    512         &quot;&quot;&quot;The MIME type of the data sent to the inference endpoint.&quot;&quot;&quot;
--&gt; 513         return self.serializer.CONTENT_TYPE
    514 
    515     @property

AttributeError: 'NoneType' object has no attribute 'CONTENT_TYPE'

  
</code></pre>
<p>**predictor is a production endpoint of the trained model</p>
<p>test_df is defined here</p>
<pre><code>df_titles = df[['customer_id','product_title', 'product_id', 
'star_rating']]

df = df[['customer_id', 'product_id', 'star_rating']]

customers = df['customer_id'].value_counts()
products = df['product_id'].value_counts()

# Filter long-tail
customers = customers[customers &gt;= 5]
products = products[products &gt;= 10]
reduced_df_titles = df_titles.merge(pd.DataFrame({'customer_id': 
customers.index})).merge(pd.DataFrame({'product_id': 
products.index}))

reduced_df = df.merge(pd.DataFrame({'customer_id': 
customers.index})).merge(pd.DataFrame({'product_id': 
products.index}))
customers = reduced_df['customer_id'].value_counts()
products = reduced_df['product_id'].value_counts()

# Number users and items
#Next, we'll number each user and item, giving them their own 
sequential index. This will allow us to hold
#the information in a sparse format where the sequential indices 
indicate the row and column in our ratings matrix.
customer_index = pd.DataFrame({'customer_id': customers.index, 
'user': 
np.arange(customers.shape[0])})
product_index = pd.DataFrame({'product_id': products.index, 'item': 
np.arange(products.shape[0])})

reduced_df = reduced_df.merge(customer_index).merge(product_index)
reduced_df_titles = 
reduced_df_titles.merge(customer_index).merge(product_index)

# Split train and test

train_df, test_df = train_test_split(reduced_df, test_size=0.2, 
                                                random_state=0)
</code></pre>",0,4,2021-07-28 15:54:58.440000 UTC,,2021-07-28 16:10:03.657000 UTC,0,python|amazon-sagemaker|recommendation-engine|mxnet,534,2021-04-24 18:20:09.973000 UTC,2022-03-09 05:53:00.933000 UTC,,43,2,0,3,,,,,,['amazon-sagemaker']
Tensorboard launched by Azure ML package doesn't work correctly,"<p>I want to access tfevent file created during training and stored in logs in Azure ML service. This tfevent file can be accessed and shown correctly on normal tensorboard so the file is not broken but when I use Azure ML's tensorboard library to access it, either nothing shows up on local tensorboard or get connection refused.</p>

<p>I first logged it into ./logs/tensorboard like Azure ML has ./logs/azureml but tensorboard launched by Azure ML's module says there is no file to show like this below on the browser.</p>

<pre><code>No dashboards are active for the current data set.
Probable causes:

You haven’t written any data to your event files.
TensorBoard can’t find your event files.
If you’re new to using TensorBoard, and want to find out how to add data and set up your event files, check out the README and perhaps the TensorBoard tutorial.
If you think TensorBoard is configured properly, please see the section of the README devoted to missing data problems and consider filing an issue on GitHub.

Last reload: Wed Aug 21 2019 *****
Data location: /tmp/tmpkfj7gswu
</code></pre>

<p>So I thought that saved location would not be recognized by AML and I changed the save location to ./logs then browser shows that ""This site can’t be reached. ****** refused to connect.""</p>

<p>My Azure ML Python SDK version is 1.0.57</p>

<p>1) How can I fix this?</p>

<p>2) Where should I save tfevent file for AML to recognize it? I couldn't find any information about it in the documentation here. 
<a href=""https://docs.microsoft.com/en-us/python/api/azureml-tensorboard/azureml.tensorboard.tensorboard?view=azure-ml-py"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/python/api/azureml-tensorboard/azureml.tensorboard.tensorboard?view=azure-ml-py</a></p>

<p>This is how I'm launching tensorboard through Azure ML.</p>

<pre class=""lang-py prettyprint-override""><code>if __name__ == ""__main__"":
    parser = argparse.ArgumentParser(
        description=f'This script is to lanuch TensorBoard with '
        f'accessing run history from machine learning '
        f'experiments that output Tensorboard logs')
    parser.add_argument('--experiment-name',
                        dest='experiment_name',
                        type=str,
                        help='experiment name in Azure ML')
    parser.add_argument('--run-id',
                        dest='run_id',
                        type=str,
                        help='The filename of merged json file.')

    args = parser.parse_args()

    logger = get_logger(__name__)
    logger.info(f'SDK Version: {VERSION}')

    workspace = get_workspace()
    experiment_name = args.experiment_name
    run_id = args.run_id
    experiment = get_experiment(experiment_name, workspace, logger)
    run = get_run(experiment, run_id)

    # The Tensorboard constructor takes an array of runs, so pass it in as a single-element array here
    tb = Tensorboard([run])

    # If successful, start() returns a string with the URI of the instance.
    url = tb.start()
    print(url)
</code></pre>",1,0,2019-08-21 10:08:18.297000 UTC,1.0,,0,python|azure|azure-machine-learning-service,687,2019-08-21 01:13:27.733000 UTC,2020-01-28 14:03:35.467000 UTC,,1,0,0,0,,,,,,['azure-machine-learning-service']
Underlying pipeline differences between Batch Endpoints & direct pipeline runs,"<p>I am looking at executing some inference jobs based on a published model, but I'm not completely clear on the differences between using a pipeline directly (python v1 sdk), or using batch endpoints (python v2 sdk or http rest). I do not have a requirement for an endpoint to be online at all times, as all jobs will execute and shutdown immediately. Most inference jobs are executing off of a single dataset at a time.</p>
<p>I've investigated batch inference jobs and the underlying pipeline logs/execution appears to match pipelines.</p>
<p>Are there any other runtime differences? Cost? Are direct pipelines being phased out?</p>
<p>Edit: I have the ability to interact with AML via Python &amp; HTTP REST, so both options are in play. I've found both <code>pipeline endpoints</code> and <code>batch endpoints</code> produce HTTP endpoints that can be consumed.</p>",0,0,2022-08-15 14:10:47.103000 UTC,,2022-08-15 14:19:51.973000 UTC,0,azure-machine-learning-service,75,2009-12-23 18:05:43.563000 UTC,2022-09-12 20:19:48.823000 UTC,"Milwaukee, WI",447,70,1,108,,,,,,['azure-machine-learning-service']
AWS Sagemaker BlazingText Multiple Training Files,"<p>Trying to find out if you can use multiple files for your dataset in Amazon Sagemaker BlazingText. </p>

<p>I am trying to use it in Text Classification mode. </p>

<p>It appears that it's not possible, certainly not in File mode, but wondering about whether Pipe mode supports it. I don't want to have all my training data in 1 file, because if it's generated by an EMR cluster I would need to combine it afterwards which is clunky. </p>

<p>Thanks!</p>",1,0,2019-06-12 20:02:43.120000 UTC,,2019-06-19 14:24:33.957000 UTC,3,machine-learning|amazon-sagemaker,568,2010-01-28 15:36:44.500000 UTC,2020-10-30 13:38:18.957000 UTC,Florida,924,35,4,139,,,,,,['amazon-sagemaker']
Azure Machine Learning - Predicting a win/lose/draw API,"<p>I'm experimenting with an existing experiment on the Azure Machine learning gallery.  Its called <a href=""https://gallery.azure.ai/Experiment/Beat-the-Bookie"" rel=""nofollow noreferrer"">beat the bookie</a>.  I want to take it to the next step and add a web service to it.  This particular experiment has a dataset for years of matches.  It has a small python script to calculate an ELO (like chess or most online games).  I'm struggling to create an input API with 2 inputs: homeTeam and awayTeam.  With an output of 1 value FTR (Full-Time Result) which is either W/D/L (Win, Draw or Lose).</p>

<p>The issue I have is that when I create the API it has too many required inputs.  Do I have to give them averaged data or how do I reduce the inputs to 2?</p>",0,3,2018-04-05 13:23:08.567000 UTC,,2018-05-11 17:05:46.293000 UTC,0,azure|azure-machine-learning-studio,64,2016-07-26 11:43:05.120000 UTC,2022-06-30 15:03:27.173000 UTC,Ireland,970,93,1,68,,,,,,['azure-machine-learning-studio']
TensorFlow estimator is getting incorrect download input path,"<p>I am trying to run a TensorFlow estimator in Sagemaker Studio. This has worked in the past but after saving an html file of my notebook, that path is now being appended to the directory I am providing to download the input data for the training session.</p>
<p>My code:</p>
<pre><code>    # Set base model name that is used to save and load the model. Append a timestamp to it for uniqueness if batch tuning.
#Model Definition File
model_file = &quot;Model.py&quot;
#Input data saved as .npy
input_name = &quot;inputdata.npy&quot;
#Label data saved as .npy
label_name = &quot;inputlabels.npy&quot;
#Test data saved as .npy
test_name = &quot;testdata.npy&quot;
#Test labels saved as .npy
test_labels =&quot;testlabels.npy&quot;
#Path for trained model
bucket_dir = &quot;s3://my-bucket&quot;
#Bucket for trained model
bucket = &quot;my-bucket&quot;
model_name = &quot;my_model&quot;

model_name = model_name + ct.strftime(&quot;-%m%d%y-%H%M%S&quot;)
load_model_dir = os.path.join(bucket_dir,model_name)
data_dest = &quot;/&quot; + model_name + &quot;/&quot;+ input_name
print(logfile_name)
print(model_name)
print(load_model_dir)
print(data_dest)

#Define hyperparameters for hyperparameter tuning and set default values
shared_hyperparameters = {
    'bucket': bucket,
    'model_name': model_name,
    'model_dir':model_name,
    'sm_model_dir':model_name,
    'logfile_name': logfile_name,
    'train_data': input_name,
    'learning_rate': .001,
    'epochs': 100,
    'train': bucket_dir,
    'test': bucket_dir,
    'train_labels':label_name,
    'test_data': test_name,
    'test_labels': test_labels
}
...

aws_estimator = TensorFlow(
    entry_point= model_file, #Model definition .py file
    bucket = bucket,
    role= role,
    instance_count=1,
    instance_type=&quot;ml.m5.2xlarge&quot;,
    framework_version=&quot;2.1.0&quot;,
    py_version=&quot;py3&quot;,
    distribution={&quot;parameter_server&quot;: {&quot;enabled&quot;: True}},
    hyperparameters = shared_hyperparameters,
    metric_definitions = metric_definitions,
    log=&quot;All&quot;,
    my_name = model_name,
    log_name = logfile_name,
    train_data = input_name,
    train_labels=label_name,
)
history = aws_estimator.fit(bucket_dir)
</code></pre>
<p>This results in the following error:</p>
<blockquote>
<p>UnexpectedStatusException: Error for Training job tensorflow-training-2021-11-19-22-51-12-360: Failed. Reason: ClientError: Data download failed:S3 key: s3://my-bucket/s3://my-bucket/model_dir/my-notebook.html has an illegal char sub-sequence '//' in it</p>
</blockquote>
<p>I am not sure why the path to the HTML file is being appended onto the bucket_dir when it wasn't before. I saw a similar problem on the AWS forums, but no helpful response was provided. I have tried printing out what the SM_CHANNEL_TRAINING environment variable is before and after training, and it is None.</p>",2,1,2021-11-22 21:18:13.647000 UTC,,2021-11-22 22:49:41.340000 UTC,2,python|amazon-web-services|tensorflow|amazon-s3|amazon-sagemaker,78,2021-11-22 20:58:55.537000 UTC,2021-12-23 19:50:43.057000 UTC,,19,2,0,1,,,,,,['amazon-sagemaker']
AWS - is it possible to pass the parameters from AWS lambda function to AWS sagemaker notebook,"<p>I am beginner to AWS console. I am facing issues while building Machine Learning pipeline.</p>
<p>Currently, The Lambda function does the job of getting the uploaded filename, username from front end and invoking the notebook instance.</p>
<p>Also, life cycle configuration at the instance will invoke the notebook for training. So the question is how to pass above variables variables to sagemaker notebook for training the machine learning model. Is it possible to achieve this? Thank you.</p>
<pre><code>#invoke command from lambda
client.start_notebook_instance(NotebookInstanceName='&lt;sagemaker_instance_name&gt;')
</code></pre>
<p>Lifecycle configuration under sagemaker instance:</p>
<pre><code>#!/bin/bash

set -e

ENVIRONMENT=JupyterSystemEnv
#JupyterSystemEnv
NOTEBOOK_FILE=/home/ec2-user/SageMaker/XGBoost_training.ipynb

source /home/ec2-user/anaconda3/bin/activate &quot;$ENVIRONMENT&quot;

nohup jupyter nbconvert --ExecutePreprocessor.timeout=-1 --ExecutePreprocessor.kernel_name=python3 --to notebook --execute &quot;$NOTEBOOK_FILE&quot; &amp;
</code></pre>",0,3,2020-09-11 15:18:25.523000 UTC,,,1,amazon-web-services|aws-lambda|amazon-sagemaker,464,2015-04-15 03:39:04.583000 UTC,2022-08-05 15:24:38.917000 UTC,Ireland,21,0,0,28,,,,,,['amazon-sagemaker']
Automate data preprocessing on AWS sagemaker,"<p>Is there a way to Automate the data preprocessing on Sagemaker on weekly basis 
the preprocessing can include many simple transformations over the data in MBs from S3. 
<p>My Idea of Automation is like Triggering a script or a notebook instance that could run weekly</p>",1,0,2018-11-26 18:09:46.887000 UTC,,,1,amazon-web-services|automation|etl|amazon-sagemaker,326,2018-06-05 14:58:09.650000 UTC,2020-06-29 00:35:24.063000 UTC,"Jersey City, NJ, USA",21,0,0,4,,,,,,['amazon-sagemaker']
How to integrate mlflow and airflow? Is there any way to connect to mlflow server from airflow,"<p>Lets say I have a ML model in mlflow server artifacts. I want to run this model from airflow Dag. Also after running in airflow, metric logs should be visible in mlflow.
How can I achieve this?
There are connections in airflow, I couldn't find any connection type for mlflow.</p>",1,0,2021-03-29 17:16:36.660000 UTC,,,2,airflow|mlflow,389,2017-05-31 04:12:26.490000 UTC,2022-09-24 08:59:44.060000 UTC,,838,89,2,33,,,,,,['mlflow']
Store input data on /opt/ml/ of container using AWS Step Functions,"<p>I am following <a href=""https://github.com/awslabs/amazon-sagemaker-examples/blob/master/step-functions-data-science-sdk/automate_model_retraining_workflow/automate_model_retraining_workflow.ipynb"" rel=""nofollow noreferrer"">this</a> tutorial to setup a re-training of the Prophet model on a Docker image that is stored on ECR.</p>

<p>When I get to the Training_Step, this setup is not able to put the input data to the /opt/ml/input/data folder on the docker container. I can pick it up from the S3 folder, but since the training is happening on the container and it needs the input data to be present in the /opt/ml... location, the AWS Step Function setup is failing at the Training_Step.</p>

<p>Any inputs on how to push the input data collected from S3 to the docker container (/opt/ml/input/data) would be greatly appreciated. Thanks!</p>",1,0,2020-06-12 11:09:25.550000 UTC,,,0,python-3.x|docker|amazon-sagemaker|aws-step-functions,295,2014-07-01 02:04:57.097000 UTC,2021-08-04 21:45:35.390000 UTC,,688,27,0,17,,,,,,['amazon-sagemaker']
Azure ML Web Service - Slow response time,"<p>I have developed a Machine learning model (Random Forest classification model) in Azure Machine Learning Studio &amp; deployed the same to Azure Container Instance (ACI) as a web service.</p>

<p>I then test the performance of the Webservice from within a notebook running in Azure ML studio with a 4 core, 8 GB ram back-end Azure compute. Using the <code>%%timeit</code> magic command in Jupyter notebook, I get an average speed of around 1.2 seconds/loop (for running inference on 1000 data points).</p>

<p>However, when I test the same webservice outside of Azure, in my local machine, the performance drops to ~5 seconds/loop (more than 4 times slower)</p>

<p>I am fairly new to web service deployment, so I am really not sure how to go about troubleshooting this (and could not find any helpful info on googling either).
Do let me know if there are any specific configuration or environment details required to answer this (I have mostly just followed the azure documentation for deployment,e.g. the tutorial <a href=""https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-azure-container-instance"" rel=""nofollow noreferrer"">here</a>) </p>",1,0,2020-05-23 20:01:56.550000 UTC,,,1,azure|azure-machine-learning-studio,328,2015-05-31 06:43:28.167000 UTC,2021-08-02 06:17:55.520000 UTC,,1409,5,0,70,,,,,,['azure-machine-learning-studio']
Azure: problems with scoring a decision forest model,"<p>I built a two-class decision forest model in the azure ML studio by splitting a dataset into two. </p>

<p>I was happy with the model scoring and evaluation so I brought in a new dataset (with same variables, same data class and same source) for scoring. </p>

<p>I then got an error that the variables in the new scoring dataset was not categorical and I should use 'edit metadata' to make it categorical (I'm not sure sure why). I did this and this immediately made the model evaluation after scoring to be very poor.</p>

<p>How can I solve this problem?</p>

<p>A solution I have in mind but cannot figure out to do it is to avoid using the 'edit metadata' function so I can get same results as I got from the split data evaluation.</p>

<p>Error message:</p>

<pre><code> Error: Error 1000: AFx Library library exception: Feature 'Age' is of type: 'Numeric' which is not implicitly convertible to type: 'Categorical'. Please use the Metadata editor to explicitly convert the type.
</code></pre>",1,0,2016-05-23 10:14:59.953000 UTC,,2016-07-10 11:08:26.750000 UTC,1,azure|decision-tree|prediction|cortana-intelligence|azure-machine-learning-studio,569,2012-10-29 18:22:48.770000 UTC,2022-09-21 14:01:44.350000 UTC,,823,16,4,114,,,,,,['azure-machine-learning-studio']
Vertex AI endpoint prediction error : ValueError: Unable to coerce value,"<p>I built and deployed an XGBoost regressor model on vertex AI and I  am trying to make some predictions using Vertex AI python SDK.
Here's my code:</p>
<pre><code>client = aiplatform.gapic.PredictionServiceClient.from_service_account_json(filename=&quot;filename.json&quot;,client_options=client_options)

endpoint = client.endpoint_path(project=project, location=location, endpoint=endpoint_id)
 
response = client.predict(endpoint=endpoint, instances=instances)
   

predictions = response.predictions
</code></pre>
<p>Here is the value of the variable instances :</p>
<pre><code>array([[4.8700000e+00, 3.6505380e+06, 2.0000000e+01, 2.0210000e+03,
    4.0000000e+00, 5.3000000e+01, 0.0000000e+00, 0.0000000e+00,
    1.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
    1.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
    0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
    0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
    0.0000000e+00, 0.0000000e+00, 1.0000000e+00, 0.0000000e+00,
    0.0000000e+00],
   [3.7100000e+00, 1.2678515e+07, 2.0000000e+01, 2.0210000e+03,
    4.0000000e+00, 5.3000000e+01, 0.0000000e+00, 0.0000000e+00,
    1.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
    1.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
    0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
    0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,
    0.0000000e+00, 0.0000000e+00, 1.0000000e+00, 0.0000000e+00,
    0.0000000e+00]])
</code></pre>
<p>(Here I am trying to make two predictions).
When doing this I am getting an error:</p>
<blockquote>
<p>Unable to coerce value</p>
</blockquote>
<p>I tried to look at the number of features allowed by the model and it is the same as the input here (29). I tried looking into the xgboost code and according to the code the input is correct.
So I am guessing it is an issue on vertex AI side. But I don't know how to fix it.</p>",0,2,2022-05-08 23:43:43.960000 UTC,,,1,python|machine-learning|google-cloud-platform|xgboost|google-cloud-vertex-ai,272,2022-05-08 23:24:49.280000 UTC,2022-08-03 20:52:55.127000 UTC,,11,0,0,3,,,,,,['google-cloud-vertex-ai']
How to serve custom MLflow model with Docker?,"<p>We have a project following essentially this
<a href=""https://github.com/mlflow/mlflow/tree/master/examples/docker"" rel=""nofollow noreferrer"">docker example</a> with the only difference that we created a custom model similar to <a href=""https://www.mlflow.org/docs/latest/models.html#custom-python-models"" rel=""nofollow noreferrer"">this</a> whose code lies in a directory called <code>forecast</code>. We succeeded in running the model with <code>mlflow run</code>. The problem arises when we try to serve the model. After doing </p>

<pre><code>mlflow models build-docker -m ""runs:/my-run-id/my-model"" -n ""my-image-name""
</code></pre>

<p>we fail running the container with</p>

<pre><code>docker run -p 5001:8080 ""my-image-name""
</code></pre>

<p>with the following error:</p>

<pre><code>ModuleNotFoundError: No module named 'forecast'
</code></pre>

<p>It seems that the docker image is not aware of the source code defining our custom model class.
With Conda environnement the problem does not arise thanks to the <code>code_path</code> argument in <code>mlflow.pyfunc.log_model</code>.</p>

<p>Our Dockerfile is very basic, with just <code>FROM continuumio/miniconda3:4.7.12, RUN pip install {model_dependencies}</code>.</p>

<p>How to let the docker image know about the source code for deserialising the model and run it?</p>",1,0,2020-01-23 14:52:13.580000 UTC,1.0,,3,docker|mlflow,2105,2015-04-16 17:17:00.943000 UTC,2022-09-10 21:14:50.857000 UTC,"Paris, France",41,5,0,20,,,,,,['mlflow']
Unknown Error when install Microsoft Machine Learning Server 9.4.7 and Microsoft R Client 3.5.2,"<p>I want to install Microsoft Machine Learning Server 9.4.7 and Microsoft R Client 3.5.2, and I download the exe file from <a href=""https://my.visualstudio.com/Downloads?q=Machine%20Learning%20Server%209.4.7%20for%20Windows&amp;pgroup="" rel=""nofollow noreferrer"">https://my.visualstudio.com/Downloads?q=Machine%20Learning%20Server%209.4.7%20for%20Windows&amp;pgroup=</a> and <a href=""https://my.visualstudio.com/Downloads?q=R%20client&amp;pgroup="" rel=""nofollow noreferrer"">https://my.visualstudio.com/Downloads?q=R%20client&amp;pgroup=</a>.</p>

<p>I followed the instructions to install the software but failed. (<a href=""https://docs.microsoft.com/en-us/machine-learning-server/install/machine-learning-server-windows-install"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/machine-learning-server/install/machine-learning-server-windows-install</a>)</p>

<p>Log file for Microsoft Machine Learning Server</p>

<pre><code>[4868:486C][2020-01-13T20:06:38]i001: Burn v3.10.1.2213, Windows v10.0 (Build 17763: Service Pack 0), path: K:\ServerSetup.exe
[4868:486C][2020-01-13T20:06:38]i000: Initializing string variable 'InstallFolder' to value '[ProgramFiles64Folder]Microsoft\ML Server'
[4868:486C][2020-01-13T20:06:38]i000: Initializing string variable 'CacheFolder' to value '[TempFolder]'
[4868:486C][2020-01-13T20:06:38]i000: Initializing string variable 'MediaFolder' to value '[TempFolder]'
[4868:486C][2020-01-13T20:06:38]i009: Command Line: ''
[4868:486C][2020-01-13T20:06:38]i000: Setting string variable 'WixBundleLog' to value 'C:\Users\lzw\AppData\Local\Temp\Microsoft_Machine_Learning_Server_20200113200638.log'
[4868:486C][2020-01-13T20:06:38]i000: Setting string variable 'WixBundleOriginalSource' to value 'K:\ServerSetup.exe'
[4868:486C][2020-01-13T20:06:38]i000: Setting string variable 'WixBundleOriginalSourceFolder' to value 'K:\'
[4868:486C][2020-01-13T20:06:38]i000: Setting string variable 'WixBundleName' to value 'Microsoft Machine Learning Server'
[4868:486C][2020-01-13T20:06:38]i000: Setting string variable 'WixBundleManufacturer' to value 'Microsoft'
[4868:486C][2020-01-13T20:06:38]i000: Loading managed bootstrapper application.
[4868:486C][2020-01-13T20:06:38]i000: Creating BA thread to run asynchronously.
[4868:486C][2020-01-13T20:06:43]i500: Shutting down, exit code: 0x0
[4868:486C][2020-01-13T20:06:43]i410: Variable: CacheFolder = C:\Users\lzw\AppData\Local\Temp\
[4868:486C][2020-01-13T20:06:43]i410: Variable: InstallFolder = C:\Program Files\Microsoft\ML Server
[4868:486C][2020-01-13T20:06:43]i410: Variable: MediaFolder = C:\Users\lzw\AppData\Local\Temp\
[4868:486C][2020-01-13T20:06:43]i410: Variable: ProgramFiles64Folder = C:\Program Files\
[4868:486C][2020-01-13T20:06:43]i410: Variable: TempFolder = C:\Users\lzw\AppData\Local\Temp\
[4868:486C][2020-01-13T20:06:43]i410: Variable: WixBundleAction = 5
[4868:486C][2020-01-13T20:06:43]i410: Variable: WixBundleElevated = 0
[4868:486C][2020-01-13T20:06:43]i410: Variable: WixBundleLog = C:\Users\lzw\AppData\Local\Temp\Microsoft_Machine_Learning_Server_20200113200638.log
[4868:486C][2020-01-13T20:06:43]i410: Variable: WixBundleManufacturer = Microsoft
[4868:486C][2020-01-13T20:06:43]i410: Variable: WixBundleName = Microsoft Machine Learning Server
[4868:486C][2020-01-13T20:06:43]i410: Variable: WixBundleOriginalSource = K:\ServerSetup.exe
[4868:486C][2020-01-13T20:06:43]i410: Variable: WixBundleOriginalSourceFolder = K:\
[4868:486C][2020-01-13T20:06:43]i410: Variable: WixBundleProviderKey = {efdbcc7a-24e7-4b05-a849-1568a9737a42}
[4868:486C][2020-01-13T20:06:43]i410: Variable: WixBundleTag = 
[4868:486C][2020-01-13T20:06:43]i410: Variable: WixBundleVersion = 9.4.7.0
[4868:486C][2020-01-13T20:06:43]i007: Exit code: 0x0, restarting: No
</code></pre>

<p>Log file for Microsoft R client</p>

<pre><code>[4498:5098][2020-01-13T20:24:09]i001: Burn v3.10.1.2213, Windows v10.0 (Build 17763: Service Pack 0), path: D:\360极速浏览器下载\RClientSetup.exe
[4498:5098][2020-01-13T20:24:09]i000: Initializing string variable 'InstallFolder' to value '[ProgramFiles64Folder]Microsoft\R Client'
[4498:5098][2020-01-13T20:24:09]i000: Initializing string variable 'CacheFolder' to value '[TempFolder]'
[4498:5098][2020-01-13T20:24:09]i000: Initializing string variable 'MediaFolder' to value '[TempFolder]'
[4498:5098][2020-01-13T20:24:09]i009: Command Line: ''
[4498:5098][2020-01-13T20:24:09]i000: Setting string variable 'WixBundleLog' to value 'C:\Users\lzw\AppData\Local\Temp\Microsoft_R_Client_20200113202409.log'
[4498:5098][2020-01-13T20:24:09]i000: Setting string variable 'WixBundleOriginalSource' to value 'D:\360极速浏览器下载\RClientSetup.exe'
[4498:5098][2020-01-13T20:24:09]i000: Setting string variable 'WixBundleOriginalSourceFolder' to value 'D:\360极速浏览器下载\'
[4498:5098][2020-01-13T20:24:09]i000: Setting string variable 'WixBundleName' to value 'Microsoft R Client'
[4498:5098][2020-01-13T20:24:09]i000: Setting string variable 'WixBundleManufacturer' to value 'Microsoft'
[4498:5098][2020-01-13T20:24:09]i000: Loading managed bootstrapper application.
[4498:5098][2020-01-13T20:24:09]i000: Creating BA thread to run asynchronously.
[4498:5098][2020-01-13T20:24:09]i500: Shutting down, exit code: 0x0
[4498:5098][2020-01-13T20:24:09]i410: Variable: CacheFolder = C:\Users\lzw\AppData\Local\Temp\
[4498:5098][2020-01-13T20:24:09]i410: Variable: InstallFolder = C:\Program Files\Microsoft\R Client
[4498:5098][2020-01-13T20:24:09]i410: Variable: MediaFolder = C:\Users\lzw\AppData\Local\Temp\
[4498:5098][2020-01-13T20:24:09]i410: Variable: ProgramFiles64Folder = C:\Program Files\
[4498:5098][2020-01-13T20:24:09]i410: Variable: TempFolder = C:\Users\lzw\AppData\Local\Temp\
[4498:5098][2020-01-13T20:24:09]i410: Variable: WixBundleAction = 5
[4498:5098][2020-01-13T20:24:09]i410: Variable: WixBundleElevated = 0
[4498:5098][2020-01-13T20:24:09]i410: Variable: WixBundleLog = C:\Users\lzw\AppData\Local\Temp\Microsoft_R_Client_20200113202409.log
[4498:5098][2020-01-13T20:24:09]i410: Variable: WixBundleManufacturer = Microsoft
[4498:5098][2020-01-13T20:24:09]i410: Variable: WixBundleName = Microsoft R Client
[4498:5098][2020-01-13T20:24:09]i410: Variable: WixBundleOriginalSource = D:\360极速浏览器下载\RClientSetup.exe
[4498:5098][2020-01-13T20:24:09]i410: Variable: WixBundleOriginalSourceFolder = D:\360极速浏览器下载\
[4498:5098][2020-01-13T20:24:09]i410: Variable: WixBundleProviderKey = {4657acfe-78d8-4b9a-bbd6-ef29b555d84e}
[4498:5098][2020-01-13T20:24:09]i410: Variable: WixBundleTag = 
[4498:5098][2020-01-13T20:24:09]i410: Variable: WixBundleVersion = 3.5.2.0
[4498:5098][2020-01-13T20:24:09]i007: Exit code: 0x0, restarting: No

</code></pre>",1,3,2020-01-13 12:35:17.917000 UTC,,2020-01-28 12:14:30.000000 UTC,1,r|azure-machine-learning-studio|microsoft-r|microsoft-machine-learning-server,179,2019-06-12 11:11:31.500000 UTC,2022-09-25 01:27:29.217000 UTC,"Beijing, 北京市中国",1299,136,8,177,,,,,,['azure-machine-learning-studio']
ML Flow autolog with catboost,"<p>I am trying to use auto logging of ML Flow with catboost - but looking at the UI of the experiment (in Databricks UI) I don't see any parameters or metrics logged.</p>
<p>My code is:</p>
<pre><code>mlflow.sklearn.autolog()
model_for_analytics = catboost.CatBoostRegressor(**cb_params)
model_for_analytics.fit(x_train, y_train, **cb_fit_params)```
</code></pre>",0,3,2022-06-22 12:16:59.683000 UTC,,,0,databricks|mlflow,119,2016-03-21 08:49:39.920000 UTC,2022-07-17 11:53:14.840000 UTC,,383,10,0,19,,,,,,['mlflow']
How to disable crash output in comet ml?,"<p>Every time my script is crashing I have such output:</p>

<pre><code>COMET INFO:     sys.cpu.percent.43       : (0.0, 0.0)
COMET INFO:     sys.cpu.percent.44       : (1.8, 1.8)
COMET INFO:     sys.cpu.percent.avg      : (6.579545454545454, 6.579545454545454)
COMET INFO:     sys.gpu.0.free_memory    : (34089664512.0, 34089664512.0)
COMET INFO:     sys.gpu.0.gpu_utilization: (0.0, 0.0)
COMET INFO:     sys.gpu.0.total_memory   : (34089730048.0, 34089730048.0)
COMET INFO:     sys.gpu.0.used_memory    : (65536.0, 65536.0)
COMET INFO:     sys.load.avg             : (39.42, 39.42)
COMET INFO:     sys.ram.total            : (1621711745024.0, 1621711745024.0)
COMET INFO:     sys.ram.used             : (78552326144.0, 78552326144.0)
COMET INFO:   Other [count]:
COMET INFO:     offline_experiment: True
</code></pre>

<p>How can I disable it?</p>

<p>In comet-ml docs I found but probably it's not what I look for:</p>

<blockquote>
  <p>by setting the environmental variable COMET_DISABLE_AUTO_LOGGING to 1</p>
</blockquote>

<pre><code>$ export COMET_DISABLE_AUTO_LOGGING=1                                                                                             
$ echo $COMET_DISABLE_AUTO_LOGGING 
1
</code></pre>

<p>But it didn't help me.</p>",0,1,2020-05-06 18:26:02.853000 UTC,,2020-05-06 20:16:33.240000 UTC,1,python|pytorch|comet|comet-ml,94,2015-11-07 21:26:50.417000 UTC,2022-09-12 12:22:12.847000 UTC,"Moscow, Россия",7659,395,3,776,,,,,,['comet-ml']
Vertex AI Predictions using a .NET Core webapi Custom Container,"<p>I'm getting some strange behavior out of Vertex AI when calling Predictions on a custom container that is a .NET core based application.</p>
<p>A little background.  I'm doing a small proof of concept to test the viability of using .NET core based Docker images as custom containers in Vertex AI.</p>
<p>I have created a simple .NET core webapi that mocks out the prediction endpoint by returning what vertex-ai expects as a prediction result from the endpoint.  The Controller code is below:</p>
<pre><code>[ApiController]
[Route(&quot;api/[controller]&quot;)]
[Produces(&quot;application/json&quot;)]
public class PredictionController : ControllerBase
{
    private readonly ILogger&lt;PredictionController&gt; logger;
    public PredictionController(ILogger&lt;PredictionController&gt; logger)
    {
        this.logger = logger;
    }

    [HttpPost]
    public Prediction PostPrediction(dynamic data) 
    {
        Prediction prediction = new Prediction();

        List&lt;double&gt; predictions = new List&lt;double&gt;();

        predictions.Add(0.82);

        prediction.Predictions = predictions;

        return prediction;
         
    }
}
</code></pre>
<p>The Dockerfile used to host the .NET core application is:</p>
<pre><code>FROM mcr.microsoft.com/dotnet/sdk:6.0-alpine as build
WORKDIR /app
COPY . .
RUN dotnet restore
RUN dotnet publish -o /app/published-app

FROM mcr.microsoft.com/dotnet/aspnet:6.0-alpine as runtime
WORKDIR /app
COPY --from=build /app/published-app /app
ENTRYPOINT [ &quot;dotnet&quot;, &quot;/app/dotnet-poc.dll&quot; ]
</code></pre>
<p>Here is an image of hitting the dockerized .NET core app endpoint with the expected vertex-ai input and the output of the endpoint.</p>
<p><a href=""https://i.stack.imgur.com/YlJAj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YlJAj.png"" alt=""docker prediction"" /></a></p>
<p>Following the standard documentation for deploying a model using a custom container for Vertex AI predictions, I have uploaded my docker image to Artifact Registry and imported the model using the custom container in Vertex AI.  I then deploy the model to an endpoint and run a prediction test...</p>
<p><a href=""https://i.stack.imgur.com/EK1y9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EK1y9.png"" alt=""enter image description here"" /></a></p>
<p>As you can see Vertex is returning a 502 without much detail.  I cannot see any issue with the return from the prediction endpoint that should cause any error.</p>
<p>Thanks for reading and looking forward to any answers that might shed light on the issue.</p>
<p>Other things to note:</p>
<p>Health endpoint is returning 200</p>
<p>Vertex AI Endpoint is healthy</p>",0,1,2022-03-22 15:51:39.573000 UTC,,,0,.net-core|google-cloud-vertex-ai,72,2011-11-15 04:07:02.043000 UTC,2022-09-23 19:05:45.940000 UTC,,174,19,0,40,,,,,,['google-cloud-vertex-ai']
Athena query in Sage maker notebook. How to get the location of output file for reusability?,"<p>My query worked:</p>
<pre><code>from pyathena import connect
import pandas as pd
conn = connect(s3_staging_dir='s3://alphabucket/query-results/myfolder/',region_name='us-east-1')

df = pd.read_sql(&quot;select * from mydbname.mytablename limit 8;&quot;, conn)
</code></pre>
<p>How ever.. next time when I run the notebook, I would like to avoid running the query again.</p>
<p>I am looking for API that would return me the result file.</p>
<p>eg:</p>
<pre><code>df = pd.read_sql(&quot;select * from mydbname.mytablename limit 8;&quot;, conn)
file = conn.last_query_output_location() # Hypothetical function Doesnt Work
print(file) # --&gt; s3://alphabucket/query-results/myfolder/2021/07/23/dfjj00772hh.csv
</code></pre>",1,0,2021-07-23 18:20:12.717000 UTC,,2021-07-24 05:38:39.440000 UTC,0,python|amazon-web-services|amazon-athena|amazon-sagemaker,481,2021-01-22 15:51:57.830000 UTC,2021-07-27 15:29:36.380000 UTC,,43,3,0,8,,,,,,['amazon-sagemaker']
XGBoost prediction always returning the same value - why?,"<p>I'm using SageMaker's built in XGBoost algorithm with the following training and validation sets:</p>

<p><a href=""https://files.fm/u/pm7n8zcm"" rel=""nofollow noreferrer"">https://files.fm/u/pm7n8zcm</a></p>

<p>When running the prediction model that comes out of the training with the above datasets always produces the exact same result.</p>

<p>Is there something obvious in the training or validation datasets that could explain this behavior? </p>

<p>Here is an example code snippet where I'm setting the Hyperparameters:</p>

<pre><code>{
                    {""max_depth"", ""1000""},
                    {""eta"", ""0.001""},
                    {""min_child_weight"", ""10""},
                    {""subsample"", ""0.7""},
                    {""silent"", ""0""},
                    {""objective"", ""reg:linear""},
                    {""num_round"", ""50""}
                }
</code></pre>

<p>And here is the source code: <a href=""https://github.com/paulfryer/continuous-training/blob/master/ContinuousTraining/StateMachine/Retrain.cs#L326"" rel=""nofollow noreferrer"">https://github.com/paulfryer/continuous-training/blob/master/ContinuousTraining/StateMachine/Retrain.cs#L326</a></p>

<p>It's not clear to me what hyper parameters might need to be adjusted.</p>

<p>This screenshot shows that I'm getting a result with 8 indexes:
<a href=""https://i.stack.imgur.com/jl3tD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jl3tD.png"" alt=""enter image description here""></a></p>

<p>But when I add the 11th one, it fails. This leads me to believe that I have to train the model with zero indexes instead of removing them. So I'll try that next.
<a href=""https://i.stack.imgur.com/goxIe.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/goxIe.png"" alt=""enter image description here""></a>
Update: retraining with zero values included doesn't seem to help. I'm still getting the same value every time. I noticed i can't send more than 10 values to the prediction endpoint or it will return an error: ""Unable to evaluate payload provided"". So at this point using the libsvm format has only added more problems.</p>",2,7,2018-04-13 20:09:01.817000 UTC,1.0,2018-04-18 20:27:23.113000 UTC,-1,machine-learning|xgboost|amazon-sagemaker,4414,2010-07-26 09:30:24.400000 UTC,2021-07-10 18:54:34.733000 UTC,United States,9098,216,5,471,,,,,,['amazon-sagemaker']
sagemaker deep forecaseting file parsing error,"<p>I am trying out Deep AR fore<a href=""https://aws.amazon.com/blogs/machine-learning/now-available-in-amazon-sagemaker-deepar-algorithm-for-more-accurate-time-series-forecasting/"" rel=""nofollow noreferrer"">Deep AR Forecasting</a>casting training algorithm. My training job keeps failing with the following error while parsing the jsonlines file:
row: 1) Failure reason
ClientError: Error when parsing json (source: /opt/ml/input/data/train/daily_call_vol_lines.json, row: 1)
I am attaching the file (json) with json lines format I tried with
Any help into why the parser fails on sagemaker side would help!
Pasting the file content:
{
    ""TimeStamp"": ""2017-07-01"",
    ""Number of Calls"": 14
}
{
    ""TimeStamp"": ""2017-07-02"",
    ""Number of Calls"": 62
}
{
    ""TimeStamp"": ""2017-07-03"",
    ""Number of Calls"": 972
}</p>",1,0,2018-08-01 21:23:10.927000 UTC,,,0,amazon-sagemaker|jsonlines,671,2016-02-09 00:43:38.703000 UTC,2022-09-12 20:20:04.923000 UTC,,43,0,0,38,,,,,,['amazon-sagemaker']
Not able to save pyspark iforest model using pyspark,"<p>Using iforest as described here : <a href=""https://github.com/titicaca/spark-iforest"" rel=""nofollow noreferrer"">https://github.com/titicaca/spark-iforest</a>
But model.save() is throwing exception.</p>

<p>Followed the code snippet mentioned under ""Python API"" section on mentioned git page.</p>

<p>from pyspark.ml.feature import VectorAssembler
import os
import tempfile
from pyspark_iforest.ml.iforest import *</p>

<p>col_1:integer
col_2:integer
col_3:integer</p>

<p>assembler = VectorAssembler(inputCols=in_cols, outputCol=""features"")
featurized = assembler.transform(df)</p>

<p>iforest = IForest(contamination=0.5, maxDepth=2)
model=iforest.fit(df)</p>

<p>model.save(""model_path"")
Exception:
scala.NotImplementedError: The default jsonEncode only supports string, vector and matrix. org.apache.spark.ml.param.Param must override jsonEncode for java.lang.Double.</p>

<p>Below is the output dataframe I'm getting after executing ""model.transform(df)"". model.save() should be able to save model-files.
col_1:integer
col_2:integer
col_3:integer
features:udt
anomalyScore:double
prediction:double</p>",1,1,2019-06-27 13:17:42.687000 UTC,,,0,machine-learning|pyspark|mlflow,372,2015-06-02 08:51:41.097000 UTC,2022-09-23 10:13:22.773000 UTC,India,779,1,0,132,,,,,,['mlflow']
AWS: What is the difference between Augmented Manifest File and Manifest File?,"<p>I am trying to learn how to use AWS Sagemaker, and I've stumbled upon a setting for input data to either be a Manifest File or and Augmented Manifest File. When creating a training job through the Sagemaker console, I have seen in the <strong>Input data configuration</strong> section that it is not possible to use this combination of settings:</p>

<pre><code>Input mode = File
S3 data type = AugmentedManifestFile
</code></pre>

<p>as this results in the error <code>Pipe mode is required for AugmentedManifestFile S3 data type.</code>. </p>

<p>In the <strong>Input data configuration</strong> section, I can also choose the attribute names for the AgumentedManifestFile, which I cannot for the ManifestFile option. Is this because all Manifest Files must follow the same template, whereas Augmented Manifest Files lets you decide the template?</p>

<p>Furthermore, I have also seen in <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/incremental-training.html"" rel=""noreferrer"">this tutorial</a> that the Manifest File can be used for incremental training, but not the Augmented Manifest File. I am trying to puzzle together the pieces, but I don't seem to get the whole picture of their difference.</p>",1,0,2019-08-15 08:57:05.767000 UTC,1.0,,5,amazon-web-services|manifest|amazon-sagemaker,1816,2017-07-23 13:00:53.407000 UTC,2022-09-23 11:26:27.713000 UTC,"Stockholm, Sweden",1870,125,4,129,,,,,,['amazon-sagemaker']
Azure Machine Learning Experiment Creation,"<p>I am new to create Experiments in Azure ML. I want to done a sample and small POC on Azure ML.</p>

<p>I have a data for the students consisting of StudentID, Student Name and Marks for Monthly Tests 1,2 and 3. I just to want to Predict data for the Final Monthly Test (i.e., Monthly Test 4).</p>

<p>I don't know how to create and what kind of Transformations to be used in Predicting the Data.</p>

<p>Anyone Please...</p>

<p>Thanks in Advance
Pradeep</p>",2,0,2016-08-22 07:16:53.390000 UTC,,,0,azure|machine-learning|azure-machine-learning-studio,79,2014-07-29 10:38:22.783000 UTC,2018-03-12 09:39:38.177000 UTC,"Chennai, Tamil Nadu, India",21,0,0,2,,,,,,['azure-machine-learning-studio']
Database communication link error occurded on Azure ML Service (Used Azure SQL Server DB),"<p>I have used Azure Machine Learning Service and python programming language.  On Azure Machine Learning Service I have one python script in that script there is one normal Select query. whenever I have deploy models on azure then that time I got ML endpoint.  whenever i am posting input data on ML endpoint then i got <strong>('08S01', '[08S01] [Microsoft][ODBC Driver 17 for SQL Server]Communication link failure (0) (SQLExecDirectW)')</strong> error.</p>

<p>Also, I have used azure SQL Server Database.</p>

<p>I am not able to understand why I got this error.  Is there any reason for this error?</p>",1,1,2020-05-14 19:58:55.830000 UTC,,,1,python|database|azure-sql-database|azure-machine-learning-service,1043,2019-07-23 11:12:07.640000 UTC,2021-07-09 12:30:12.430000 UTC,,157,11,0,46,,,,,,['azure-machine-learning-service']
how to structure input/formats for batch inference in sagemaker?,"<p>example provided in the aws documentation , <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html</a>, states that the input csv can be structured like a sample below. I noticed for batch jobs in sagemaker, it can accept json as well. how to structure the json, does each record need to in a single line as shown in a csv example or can it be multiline?</p>
<pre><code>Record1-Attribute1, Record1-Attribute2, Record1-Attribute3, ..., Record1-AttributeM
...
</code></pre>",1,0,2022-04-05 18:31:54.813000 UTC,,,0,amazon-web-services|amazon-sagemaker,247,2021-06-22 17:06:55.117000 UTC,2022-09-24 17:08:41.977000 UTC,,95,10,0,16,,,,,,['amazon-sagemaker']
How to provide Dockerfile for mlflow models build-docker,"<p>I have created a Model using pyfunc file with mlflow which usage a conda_env to install packages required for model.</p>
<pre><code>pip_env = {
    'pip': [
        'pandas==0.24.1',
        'python-dateutil==2.8.1',
        'fuzzywuzzy==0.7.0'
    ]
}
conda_env = {
    'channels': ['defaults'],
    'dependencies': [
        'python=3.7.0',
        'pip=20.2.3',
        pip_env
    ]
}
mlflow.pyfunc.save_model(path=model_path, python_model=gfeCleanPrediction(), artifacts=artifacts, conda_env=conda_env,code_path=code_path)
</code></pre>
<p>I need to use my own Dockerfile which will build some packages from source and install, Is there a way I can provide it while running below command :</p>
<pre><code>mlflow  models build-docker -m MODEL_FOLDER_V-1-0-1 -n my_model --install-mlflow
</code></pre>
<p>I can see mlflow provide a custom_setup_steps_hook parameter in /python3.7/site-packages/mlflow/models/docker_utils.py</p>
<pre><code>def _build_image(image_name, entrypoint, mlflow_home=None, custom_setup_steps_hook=None):
    &quot;&quot;&quot;
    :param custom_setup_steps_hook: (Optional) Single-argument function that takes the string path
           of a dockerfile context directory and returns a string containing Dockerfile commands to
           run during the image build step.
    &quot;&quot;&quot;
    mlflow_home = os.path.abspath(mlflow_home) if mlflow_home else None
    with TempDir() as tmp:
        cwd = tmp.path()
        install_mlflow = _get_mlflow_install_step(cwd, mlflow_home)
        custom_setup_steps = custom_setup_steps_hook(cwd) if custom_setup_steps_hook else &quot;&quot;
        with open(os.path.join(cwd, &quot;Dockerfile&quot;), &quot;w&quot;) as f:
            f.write(
                _DOCKERFILE_TEMPLATE.format(
                    install_mlflow=install_mlflow,
                    custom_setup_steps=custom_setup_steps,
                    entrypoint=entrypoint,
                )
            )
</code></pre>
<p>How to use custom_setup_steps_hook OR use my own Dockerfile in <strong>mlflow  models build-docker</strong>??</p>",0,0,2022-05-21 16:21:45.177000 UTC,,,0,docker|dockerfile|mlflow,190,2014-11-02 10:27:32.403000 UTC,2022-07-07 11:05:50.310000 UTC,,31,0,0,15,,,,,,['mlflow']
How to store artifacts on a server running MLflow,"<p>I define the following docker image:</p>

<pre><code>FROM python:3.6

RUN pip install --upgrade pip
RUN pip install --upgrade mlflow

ENTRYPOINT mlflow server --host 0.0.0.0 --file-store /mnt/mlruns/
</code></pre>

<p>and build an image called <code>mlflow-server</code>. Next, I start this server from a local machine:</p>

<pre><code>docker run --rm -it -p 5000:5000 -v ${PWD}/mlruns/:/mnt/mlruns mlflow-server
</code></pre>

<p>Next, I define the following function:</p>

<pre><code>def foo(x, with_af=False):
    mlflow.start_run()
    mlflow.log_param(""x"", x)
    print(x)
    if with_af:
        with open(str(x), 'wb') as fout:
            fout.write(os.urandom(1024))
        mlflow.log_artifact(str(x))
        mlflow.log_artifact('./foo.data')
    mlflow.end_run()
</code></pre>

<p>From the same directory I run <code>foo(10)</code> and the parameter is logged correctly. However, <code>foo(10, True)</code> yields the following error: <code>PermissionError: [Errno 13] Permission denied: '/mnt'</code>. Seems like <code>log_artifact</code> tries to save the file on the local file system directly.</p>

<p>Any idea what am I doing wrong?</p>",2,0,2018-09-14 11:41:42.767000 UTC,3.0,2019-07-02 14:06:18.733000 UTC,20,python|docker|mlflow,15702,2011-03-22 10:28:37.227000 UTC,2022-09-23 13:51:41.560000 UTC,,11410,2846,6,1782,,,,,,['mlflow']
Is it possible to import custom source files into Kubeflow components?,"<p>I know that Kubeflow only modifies the container with the specified libraries to be installed. But I want to use my custom module in the training Component section of the pipeline.</p>
<p>So let me clarify my case; I'm deploying a GCP Vertex AI pipeline which exists of preprocessing and training  steps. And there is also custom library that I created using some libraries like scikit. My main issue is that I want to re-use that library objects within my training step which looks like;</p>
<pre><code>    packages_to_install = [
        &quot;pandas&quot;,
        &quot;sklearn&quot;,
        &quot;mycustomlibrary?&quot;
    ],
)
def train_xgb_model(
    dataset: Input[Dataset],
    model_artifact: Output[Model]
):
    
    from MyCustomLibrary import XGBClassifier
    import pandas as pd
    
    data = pd.read_csv(dataset.path)

    model = XGBClassifier(
        objective=&quot;binary:logistic&quot;
    )
    model.fit(
        data.drop(columns=[&quot;target&quot;]),
        data.target,
    )

    score = model.score(
        data.drop(columns=[&quot;target&quot;]),
        data.target,
    )

    model_artifact.metadata[&quot;train_score&quot;] = float(score)
    model_artifact.metadata[&quot;framework&quot;] = &quot;XGBoost&quot;
    
    model.save_model(model_artifact.path)``` 
</code></pre>",0,0,2022-09-06 14:24:14.327000 UTC,,,1,kubeflow|google-cloud-vertex-ai|kubeflow-pipelines,31,2018-06-25 06:49:27.513000 UTC,2022-09-12 08:12:55.840000 UTC,"İstanbul, Türkiye",41,1,0,4,,,,,,['google-cloud-vertex-ai']
Trigger Jupyter Notebook in Azure ML workspace from ADF,"<p>How do I trigger a notebook in my Azure Machine Learning notebook workspace from Azure Data Factory</p>
<p>I want to run a notebook in my Azure ML workspace when there are changes to my Azure storage account.</p>",2,0,2020-09-13 00:12:01.710000 UTC,,,3,azure-data-factory|azure-data-factory-2|azure-machine-learning-service,1244,2018-09-18 19:45:24.587000 UTC,2022-09-14 17:20:52.613000 UTC,"Laurel, MD, USA",359,3,0,41,,,,,,['azure-machine-learning-service']
Add git repo in Sagemaker notebook insatnce using Terraform,"<p>I am creating my Sagemaker notebook instance using terraform. I want to add my git repository with Sagemaker notebook instance at the time of instance creation.</p>

<p>Please help me if there is any solution for the same.</p>",2,0,2019-09-18 06:59:35.363000 UTC,,2019-09-18 07:10:47.127000 UTC,0,amazon-sagemaker|terraform-provider-aws,640,2019-09-18 06:48:27.287000 UTC,2020-05-26 17:50:07.560000 UTC,,1,0,0,1,,,,,,['amazon-sagemaker']
ML-Flow installation in R,"<p>I have installed MLFLOW for R in my ubuntu environment. When I try to execute any command for mlflow I am getting below error</p>

<blockquote>
  <p>mlflow_ui()
  Error in rethrow_call(c_processx_exec, command, c(command, args), stdin,  :
    cannot start processx process (system error 2, No such file or directory) @unix/processx.c:573</p>
</blockquote>",2,2,2019-10-10 10:56:54.580000 UTC,,2019-10-10 12:02:24.507000 UTC,7,r|ubuntu|mlflow,888,2017-07-12 08:50:32.780000 UTC,2021-08-09 04:41:04.280000 UTC,,71,0,0,7,,,,,,['mlflow']
"Error in curl::curl_fetch_disk(url, x$path, handle = handle): ""URL using bad/illegal format or missing URL""","<p>I am trying to read a csv file using below code.</p>
<p>df = s3read_using(FUN= read.csv, bucket=&quot;my-bucket-name&quot;,object=&quot;object.csv&quot;)</p>
<p>While doing this I am getting the below error</p>
<p><em>Error in curl::curl_fetch_disk(url, x$path, handle = handle): &quot;URL using bad/illegal format or missing URL&quot;</em></p>",2,0,2022-01-09 08:09:49.307000 UTC,,,1,r|csv|amazon-sagemaker,216,2020-04-24 10:34:21.987000 UTC,2022-08-13 06:30:57.293000 UTC,,95,39,0,30,,,,,,['amazon-sagemaker']
Amazon Sagemaker Factorization Machine inconsistent prediction results,"<p>Amazon Sagemaker's Factorization Machine model's inference results differ based on input data format. I am receiving different prediction results depending whether the inference data is json or protobuf.</p>

<p>My JSON input data is sparse.
Protobuf RecordIO input data is also sparse.</p>

<p>I have assumed that no matter the input data format, the Factorization Machine's predictions should be stable?</p>",0,2,2019-10-02 08:28:21.500000 UTC,,,0,json|protocol-buffers|amazon-sagemaker,39,2016-10-30 15:31:36.890000 UTC,2021-11-19 06:16:54.973000 UTC,Bulgaria,329,23,0,30,,,,,,['amazon-sagemaker']
Sagemaker: How to customize tensorflow-serving arguments,"<p>I have long running requests that are currently timing out under sagemaker and so am trying to increase the timeout. This is normally accomplished with the <code>rest_api_timeout_in_ms</code> argument to <code>tensorflow_model_server</code>. In order to change this argument, I've created a custom image with the following docker file:</p>
<pre><code>FROM 763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-inference:2.5.1-cpu-py37-ubuntu18.04

RUN echo '#!/bin/bash \n\n' &gt; /usr/bin/tf_serving_entrypoint.sh \
 &amp;&amp; echo '/usr/bin/tensorflow_model_server --port=8500 --rest_api_port=8501 --rest_api_timeout_in_ms=600000 --model_name=${MODEL_NAME} --model_base_path=${MODEL_BASE_PATH}/${MODEL_NAME} &quot;$@&quot;' &gt;&gt; /usr/bin/tf_serving_entrypoint.sh \
 &amp;&amp; chmod +x /usr/bin/tf_serving_entrypoint.sh

CMD [&quot;/usr/bin/tf_serving_entrypoint.sh&quot;]
</code></pre>
<p>However, sagemaker does not appear to actually be using this entrypoint, as the cloudwatch logs show the following:</p>
<pre><code>INFO:__main__:tensorflow serving command: tensorflow_model_server --port=23000 --rest_api_port=23001 --model_config_file=/sagemaker/model-config.cfg --max_num_load_retries=0
</code></pre>
<p>My only conclusion is that somewhere deep within the bowels of boto3/sagemaker sdk, this command is being altered.</p>
<p>How do I modify this command to increase the timeout?</p>
<p>Here's my deployment code:</p>
<pre class=""lang-py prettyprint-override""><code>sagemaker_session = sagemaker.Session()
    model_data = sagemaker_session.upload_data(path=&quot;model.tar.gz&quot;, key_prefix=MODEL_NAME)

    s3_bucket = sagemaker_session.default_bucket()
    model_s3_key = &quot;s3://&quot; + sagemaker_session.default_bucket() + f&quot;/{MODEL_NAME}/model.tar.gz&quot;

    create_model_name = sagemaker_session.create_model(
        name=MODEL_NAME,
        role=SAGEMAKER_ROLE,
        container_defs={
            &quot;Image&quot;: IMAGE_URI,  # Path to my custom image
            &quot;ModelDataUrl&quot;: model_s3_key,
        },
    )

    sagemaker_client = boto3.client('sagemaker', region_name=AWS_REGION)

    create_endpoint_config_response = sagemaker_client.create_endpoint_config(
        EndpointConfigName=endpoint_config_name, 
        ProductionVariants=[
            {
                &quot;VariantName&quot;: &quot;variant1&quot;, # The name of the production variant.
                &quot;ModelName&quot;: MODEL_NAME,
                &quot;InstanceType&quot;: &quot;ml.m5.xlarge&quot;, # Specify the compute instance type.
                &quot;InitialInstanceCount&quot;: 1 # Number of instances to launch initially.
            }
        ]
    )
</code></pre>",0,0,2021-09-28 16:59:07.100000 UTC,,,0,python|tensorflow|amazon-sagemaker,67,2013-11-12 21:01:29.407000 UTC,2022-09-25 05:27:34.497000 UTC,,6281,406,10,283,,,,,,['amazon-sagemaker']
Slow running of code in AWS for the first time,"<p>I am running my code on SageMaker, which runs my code slowly for the first time, but runs much faster the second time around. I guess there's something getting stored in the cache. Few days back, it was running with the same speed all the time. What could be a possible solution for this? </p>

<pre class=""lang-py prettyprint-override""><code>count_hea = 0
count_pleth = 0
for subfile in sorted(os.listdir('physionet.org/files/mimic3wdb-matched/1.0/p00')):
    count_hea = 0
    if subfile.startswith('p'):
        for subsubfile in sorted(os.listdir(os.path.join('physionet.org/files/mimic3wdb-matched/1.0/p00/' , subfile))):
            if subsubfile.startswith('p') and count_hea == 0 and not subsubfile[:-4].endswith('n'):
                try:            
                    i = i + 1
                    print(subsubfile)
                    count_hea = count_hea + 1
                    strip = subsubfile[:-4]
                    record = wfdb.rdrecord('physionet.org/files/mimic3wdb-matched/1.0/p00/' + subfile + '/' + strip, channel_names = ['PLETH'], return_res = 16)
                    r = record.__dict__
                    print(r['sig_name'])
                    if r['sig_name'] != None:
                        if r['sig_name'][0] == 'PLETH':
                            count_pleth = count_pleth + 1
                            print(count_pleth)  
                except Exception:
                    pass
print(count_pleth)
</code></pre>",1,2,2020-06-10 13:45:11.163000 UTC,,2020-06-10 17:55:35.903000 UTC,0,amazon-web-services|caching|amazon-sagemaker,942,2020-06-08 04:27:40.687000 UTC,2020-12-11 14:13:57.083000 UTC,,11,0,0,1,,,,,,['amazon-sagemaker']
How to access all registered models when deploying a machine learning model in an Azure Container Instance?,"<p>I have built a continuous integration/deployment pipeline in Azure DevOps to train and deploy a machine learning model into a production environment. It uses Azure Machine Learning Services in Python to set everything up i.e. train the model, register it in a machine learning workspace and deploy it as a webservice. One requirement is that I need to use multiple models in the deployed webservice. There is no problem to include the models in the deployed webservice when looking at the workspace from the Azure portal. My problem lies in that I don't know how to access them without knowing the names of the models. </p>

<p>What normally happens looks like this:
score.py</p>

<pre><code>from azureml.core.model import Model
from sklearn.externals import joblib
import pandas
def init():
   global model
   model_path = Model.get_model_path('model_name')
   model = joblib.load(model_path)
def run(raw_data):
   data = pandas.DataFrame(json.loads(raw_data)['Inputs'])
   return do_prediction(data) # Use the model to make prediction

</code></pre>

<p>Then I also have a python script that creates an image with all needed models and deploys it as a webservice in Azure.</p>

<p>What I would like to use would look something like this (but it gives an error since I can't list the models).
score.py</p>

<pre><code>from azureml.core.model import Model
from sklearn.externals import joblib
import pandas
def init():
   model_list = []
   models = Model.list() # Gives an error since no workspace is provided.
   for model in models:
      model_list.append(joblib.load(model.name))
def run(raw_data):
   data = pandas.DataFrame(json.loads(raw_data)['Inputs'])
   return do_prediction(data) # Use the model to make prediction

</code></pre>",1,0,2019-07-03 11:38:42.657000 UTC,,,3,python|azure-devops|azure-machine-learning-service,2083,2014-12-01 12:52:37.793000 UTC,2019-09-18 11:05:37.527000 UTC,"Uppsala, Sverige",91,1,0,7,,,,,,['azure-machine-learning-service']
How do I set overcommit_memory on Google Cloud notebook?,"<p>I am using a GCP Vertex managed notebook and I get a memory error which I think can be fixed by:</p>
<pre><code>echo 1 &gt; /proc/sys/vm/overcommit_memory
</code></pre>
<p>but when I run this from a Jupyterlab terminal I am asked for a sudo password, which I do not know. What can I do?</p>",1,1,2022-05-25 13:38:16.420000 UTC,,,1,google-cloud-platform|memory-management|jupyter-lab|google-cloud-vertex-ai,76,2012-10-25 08:48:34.717000 UTC,2022-09-23 10:10:32.783000 UTC,,2564,304,8,451,,,,,,['google-cloud-vertex-ai']
How to log source name and version on a MLFlow server from Jupyer lab,"<p>My setup:</p>

<ol>
<li>I have a running remote MLFlow server.</li>
<li>I have a git tracked project consisting of a .ipynb notebook.</li>
</ol>

<p>What am I doing:</p>

<ol>
<li>Firing up a jupyer lab server from inside the git tracked project.</li>
<li><p>Running mlflow runs from the notebook. </p>

<pre><code>        mlflow.log_params(params)
        mlflow.log_metrics(metrics)
        mlflow.h2o.log_model(m, ""model"")```
</code></pre></li>
</ol>

<p>Now the issue is, as shown below in the image, MLFlow by default cannot track:</p>

<ul>
<li>The source name -> defaults to the ipython kernel name. </li>
<li>The source version.</li>
</ul>

<p><a href=""https://i.stack.imgur.com/eGeCJ.png"" rel=""nofollow noreferrer"">tracking server snapshot</a></p>

<ol>
<li>Is there a way to manually control the source name and the source version? I noticed the older mlflow versions exposed these two parameters as part of mlflow.start_run() method. This is not the case anymore.</li>
<li>Is there a specific setup I need to follow to enable source and version tracking from jupyter by default?</li>
</ol>",0,0,2020-05-03 06:42:42.917000 UTC,,,1,jupyter-lab|mlflow,66,2014-01-30 08:58:09.527000 UTC,2021-12-08 01:50:32.913000 UTC,"Auckland, New Zealand",11,0,0,2,,,,,,['mlflow']
Predicting locally with a model trained on Sagemaker,"<p>I have trained a model on AWS SageMaker by using the built-in algorithm Semantic Segmentation. This trained model named as model.tar.gz is stored on S3. So I want to download this file from S3 and then use it to make inference on my local PC without using AWS SageMaker.</p>
<p>Here are the three files:</p>
<ol>
<li><p><code>hyperparams.json</code>: includes the parameters for network architecture, data inputs, and training. Refer to Semantic Segmentation Hyperparameters.</p>
</li>
<li><p><code>model_algo-1</code></p>
</li>
<li><p><code>model_best.params</code></p>
</li>
</ol>
<p>My code:</p>
<pre class=""lang-py prettyprint-override""><code>import mxnet as mx
from mxnet import image
from gluoncv.data.transforms.presets.segmentation import test_transform
import gluoncv



img = image.imread('./bdd100k/validation/14df900d-c5c145cb.jpg')
img = test_transform(img, ctx)
img = img.astype('float32')


model = gluoncv.model_zoo.PSPNet(2)

# load the trained model
model.load_parameters('./model/model_best.params')
</code></pre>
<p><strong>Error:</strong></p>
<pre><code>AssertionError: Parameter 'head.psp.conv1.0.weight' is missing in file './model/model_best.params', which contains parameters: 'layer3.2.bn3.beta', 'layer3.0.conv3.weight', 'conv1.1.running_var', ..., 'layer2.2.bn3.running_mean', 'layer3.4.bn2.running_mean', 'layer4.2.bn3.beta', 'layer3.4.bn3.beta'. Set allow_missing=True to ignore missing parameters.
</code></pre>",1,1,2020-10-21 16:17:52.433000 UTC,,2021-09-18 13:29:22.037000 UTC,5,amazon-sagemaker|mxnet|gluon,229,2014-06-03 18:52:59.517000 UTC,2022-08-21 06:55:48.400000 UTC,India,4221,84,13,441,,,,,,['amazon-sagemaker']
Error - WebApp Implementation with Principal Component Analysis(PCA) - Azure ML Studio,"<p>After applying Principal Component Analysis PCA to my data set in order to achieve better model accuracy. The 13 features dimensions, I am reducing it to 10 features using PCA. Everything is fine till here.</p>
<p>After implementing the model in WebApp, it is building &amp; seems fine in the studio.</p>
<p>In the testing phase of model prediction, Instead of displaying 10 features as an input, the UI system is showing the original features which is 13 &amp; the output is showing 10 featu<a href=""https://i.stack.imgur.com/X2jbw.jpg"" rel=""nofollow noreferrer"">enter image description here</a>res which does not have any feature names for the newly generated features which are 10. And also prediction is not working at all after executing it.\</p>
<p>Attached are the screenshots, Please refer.</p>",1,0,2021-02-21 10:58:17.650000 UTC,,,-1,machine-learning|azure-machine-learning-studio|azure-machine-learning-service|azure-machine-learning-workbench,40,2019-05-24 15:55:49.560000 UTC,2022-03-24 11:57:04.927000 UTC,,21,0,0,3,,,,,,"['azure-machine-learning-workbench', 'azure-machine-learning-studio', 'azure-machine-learning-service']"
How can I monitor CPU usage in AWS SageMaker while in a notebook?,<p>It doesn't seem that there is a way to monitor the usage of resources while in a Jupyter Notebook. Normally I can see what percentage of my cpu I am using. This might just be installing jupyter-resource-usage but I haven't been able to resolve that package in a notebook without issues.</p>,1,1,2021-08-27 23:48:53.473000 UTC,,,0,amazon-web-services|jupyter-notebook|amazon-sagemaker,800,2021-04-06 05:59:33.160000 UTC,2021-11-18 20:11:17.237000 UTC,,1,0,0,1,,,,,,['amazon-sagemaker']
"Cannot install the ""ipywidgets"" Jupyter Lab Extension on AWS sagemaker","<p>To install Jupyter Lab Extension on AWS sagemaker, You need to follow <a href=""https://github.com/aws-samples/amazon-sagemaker-notebook-instance-lifecycle-config-samples/tree/master/scripts"" rel=""nofollow noreferrer"">https://github.com/aws-samples/amazon-sagemaker-notebook-instance-lifecycle-config-samples/tree/master/scripts</a>. And then create the lifecycle configuration accordingly.</p>

<p>I did it and this is my <code>on-start.sh</code> file.</p>

<pre class=""lang-sh prettyprint-override""><code>#!/bin/bash

set -e

# OVERVIEW
# This script installs a jupyterlab extension package in SageMaker Notebook Instance

sudo -u ec2-user -i &lt;&lt;'EOF'
# PARAMETERS
EXTENSION_NAME=@jupyter-widgets/jupyterlab-manager
source /home/ec2-user/anaconda3/bin/activate JupyterSystemEnv
jupyter labextension install $EXTENSION_NAME
source /home/ec2-user/anaconda3/bin/deactivate
EOF
</code></pre>

<p>Everything should went smooth except for this extension it raises an error.</p>

<p>This is the error log from cloud watch.</p>

<pre><code>/bin/bash: /tmp/OnStart_2019-06-26-23-3260vo0j6p: /bin/bash^M: bad interpreter: No such file or directory
</code></pre>

<p>This one is the error message shown in the sagemaker console.</p>

<pre><code>Failure reason
Notebook Instance Lifecycle Config 'arn:aws:sagemaker:ap-southeast-1:658055165324:notebook-instance-lifecycle-config/jupyter-widgets-for-jupyterlab-copy' for Notebook Instance 'arn:aws:sagemaker:ap-southeast-1:658055165324:notebook-instance/test' took longer than 5 minutes. Please check your CloudWatch logs for more details if your Notebook Instance has Internet access.
</code></pre>

<p>I had done several attempts to locate the bug in the script file and the setup file of <code>ipywidgets</code> concerning <a href=""https://stackoverflow.com/questions/14219092/bash-my-script-bin-bashm-bad-interpreter-no-such-file-or-directory"">the 'bad interpreter' error</a>. I cannot find any traces of error in both.</p>

<p>I tried to upgrade my instance to T2 largest instance just in case the error came from the timeout. </p>

<p>The weirdest thing is that I am able to install it via the terminal from the terminal on jupyterlab. I measured the total time it takes to install and found it is around <code>4 mins</code> just enough time(AWS should allow more time since this is only one extension install). Noted that this installation was performed under the T2 medium instance(the cheapest instance type you could get). If you install it this way to have to reboot the jupyter lab to make it work, then you reboot your instance and everything reverts back to the not-yet-install state. This suggests that there is no way to install jupyter lab extension rather than using the lifecycle cycle configurations which will lead you back to the error.</p>

<p>At this point, I gave up and use the jupyter notebook instead if I really want to use the <code>ipywidgets</code>.</p>

<hr>

<p>Normally, this should be raised as technical support on AWS, but I have the basic plan so I decided to file it in StackOverflow for others that might encounter the same thing.</p>",1,4,2019-06-30 01:15:35.720000 UTC,1.0,,4,bash|amazon-web-services|amazon-sagemaker|jupyter-lab|ipywidgets,2151,2014-08-23 03:37:42.060000 UTC,2022-09-22 06:06:57.590000 UTC,"Bangkok, ประเทศไทย",2620,240,10,134,,,,,,['amazon-sagemaker']
How do I take an already existing MLflow model on my local filesystem and log it to a remote tracking server?,"<p>Let's say I already have an existing MLflow model on my local system of the <code>mlflow.pyfunc</code> flavor.</p>
<p>The directory looks like this</p>
<pre><code>model/
  data/
  code/
  conda.yml
  MLmodel
</code></pre>
<p>Where <code>MLmodel</code> is something like</p>
<pre><code>flavors:
  python_function:
    code: code
    data: data
    env: conda.yml
    loader_module: loader # model/code/loader.py has the entrypoint
</code></pre>
<p>I now try and log this model to a remote tracking server using (I'm in the directory above <code>model/</code>, so <code>./model/data</code> works, etc)</p>
<pre><code>import mlflow
mlflow.set_tracking_uri(&quot;http://localhost:5000&quot;)
mlflow.pyfunc.log_model(
  &quot;my-model-artifact&quot;,
  registered_model_name=&quot;my-model&quot;, # same for all model versions,
  data_path=&quot;model/data&quot;,
  code_path=&quot;model/code&quot;,
  loader_module=&quot;model/code/loader&quot;
)
</code></pre>
<p>The tracking server ends up logging a nested MLflow model.. this is inside of the <code>./artifacts/my-model-artifact</code> directory on the tracking server</p>
<pre><code>./artifacts/my-model-artifact
  conda.yaml
  MLmodel # *not* my MLmodel, one newly generated by MLflow
  data/
  code/
</code></pre>
<p>Where <code>data</code> now points nested to my entire <code>model/data</code> directory and <code>code</code> points to a nested <code>model/code</code> directory.</p>
<p>It's like it doesn't understand that I already have this full artifact..</p>",0,1,2022-03-01 22:43:04.637000 UTC,,,0,model|mlflow,336,2015-03-10 05:37:19.877000 UTC,2022-09-23 19:26:30.290000 UTC,"Santa Cruz, CA",3256,81,0,164,,,,,,['mlflow']
Azure ML R script to choose labels with highest score in web service,<p>I have an Azure Machine Learning experiment with thousands of labels.  When I run the web service I get a table with two rows (label name and label probability) but thousands of columns.  I would like to select just the 5 labels with the highest probability (and preferably sorted descending).  Anyone now an Azure ML Studio component or R script that can do this please? Thanks!</p>,0,2,2018-04-29 14:55:13.367000 UTC,,2018-05-11 14:50:54.003000 UTC,0,r|azure|azure-machine-learning-studio|azure-machine-learning-workbench,53,2015-06-27 10:40:25.343000 UTC,2022-04-22 16:50:07.507000 UTC,London,168,15,0,32,,,,,,"['azure-machine-learning-workbench', 'azure-machine-learning-studio']"
Trains: reusing previous task id,<p>I am using <code>reuse_last_task_id=True</code> to overwrite an existing task (with same project and task name). But the experiment contains the torch model and therefore does not overwrite the existing task but creates a new one. How can I detach the model from the task?</p>,1,0,2020-11-01 19:49:39.110000 UTC,,2020-12-31 12:37:52.433000 UTC,1,trains|clearml,133,2014-09-11 05:09:09.420000 UTC,2022-09-25 00:10:01.540000 UTC,,187,21,0,32,,,,,,['clearml']
Using R to extract specific column in Azure ML,"<p>I have created a forecasting causal model in Azure ML studio that determines an organization's hiring needs for every month for 2015. Since it is a causal model, I individually forecast all parameters for 2015 and supply them to my model.</p>

<p>One such factor is previous 9 months Hire value. It means that if I am forecasting a hire value for Jan-2015, the previous 9 month hire values will be considered (Dec - April 2014).</p>

<p>The following is my parameter set:</p>

<pre><code>Year    Month   Factor A    Factor B    Factor C    Factor D    Prev. Month-1   Prev. Month-2   Prev. Month-3   Prev. Month-4   Prev. Month-5   Prev. Month-6   Prev. Month-7   Prev. Month-8   Prev. Month-9
</code></pre>

<p>Sample Input:</p>

<pre><code>Year    Month   Factor A    Factor B    Factor C    Factor D    Prev. Month-1   Prev. Month-2   Prev. Month-3   Prev. Month-4   Prev. Month-5   Prev. Month-6   Prev. Month-7   Prev. Month-8   Prev. Month-9
2015    1       2           4           6           8           10              11              12              13              14              15              16              17              18
</code></pre>

<p>Once I run the model, I get the Forecasted hire (Score Labels) as my output:</p>

<pre><code>Year    Month   Factor A    Factor B    Factor C    Factor D    Prev. Month-1   Prev. Month-2   Prev. Month-3   Prev. Month-4   Prev. Month-5   Prev. Month-6   Prev. Month-7   Prev. Month-8   Prev. Month-9   Score Labels
2015    1       2           4           6           8           10              11              12              13              14              15              16              17              18              19
</code></pre>

<p>For forecasting for February-2015, the forecasted value for January becomes Prev. Month-1 value. </p>

<pre><code>Year    Month   Factor A    Factor B    Factor C    Factor D    Prev. Month-1   Prev. Month-2   Prev. Month-3   Prev. Month-4   Prev. Month-5   Prev. Month-6   Prev. Month-7   Prev. Month-8   Prev. Month-9
2015    1       2           4           6           8           10              11              12              13              14              15              16              17              18
2015    2       3           5           7           9           19              10              11              12              13              14              15              16              17
</code></pre>

<p>This becomes a bit tedious as I have to repeat it 12 times - one for each month. Can someone suggest how do I solve it using R script? Here is what I have written so far:</p>

<pre><code>dataset &lt;- maml.mapInputPort(1) 

previous &lt;- 9
orig_names &lt;- names(dataset)
n_rows &lt;- dim(dataset)[1]

base &lt;- 9
for (i in 1:previous) {
  dataset[(i+1):n_rows,base+i] &lt;- dataset[1:(n_rows-i),base]
  dataset[1:i,base+i] &lt;- dataset[1:i,base+i-1]
}

a &lt;- -1:-previous
new_names &lt;- paste(""Prev. Month"",a)
names(dataset) &lt;- c(orig_names,new_names)

maml.mapOutputPort(""dataset"")
</code></pre>

<p>Thanks.</p>

<p><strong>Update 1</strong></p>

<p>I have managed to execute my model in a loop in Azure. For each iteration, I need to provide the input parameters (like for February). </p>

<p>Can someone help me on how to obtain the forecast under ""Score Labels"" and append that as a parameter under ""Prev. Month-1"" to my input for next month in R? </p>",0,2,2015-07-08 04:15:12.023000 UTC,,2016-03-11 22:29:17.553000 UTC,0,r|azure|dataframe|azure-machine-learning-studio,305,2015-01-31 04:53:22.700000 UTC,2022-01-29 02:50:58.853000 UTC,"San Jose, CA, USA",1221,53,1,163,,,,,,['azure-machine-learning-studio']
Sagemaker deployment with source_dir as S3 URI,"<p>I'm deploying Pytorch model saved as <code>.pth</code> to AWS Sagemaker. I specify custom <code>inference.py</code> file and my catalog looks like this:</p>
<pre><code>|   my_model
|           |--model.pth
|
|           code
|               |--inference.py
|               |--requirements.txt
|
</code></pre>
<p>That's completely in line with <a href=""https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html#bring-your-own-model"" rel=""nofollow noreferrer"">SDK doc</a>. So I put the archive to S3 bucket, after that I define my model as following:</p>
<pre><code>pytorch_model = PyTorchModel(model_data='s3://{bucket_name}/my_path/my-model.tar.gz',
                             framework_version=&quot;1.8.1&quot;, py_version=&quot;py3&quot;,
                             role=role, entry_point='inference.py')

</code></pre>
<p>Again, seems for me to be consistent with requirements. Still it returns error, claiming, that <code>inference.py</code> doesn't exist. I've tried to specify <code>source_dir</code> to be <code>code</code>. Still doesn't find. Then I tried to give it as path to my-model.tar.gz, as suggested in the <code>code</code><a href=""https://github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/model.py"" rel=""nofollow noreferrer"">here</a>. Deployment succeeds, but then in logs I see it's asking me to define <code>model_fn</code> func, which is defined inside <code>inference.py</code> file and works perfectly if the code stored with the notebook, which deploys the model.</p>
<p>What am I doing wrong? How should I store model artifacts in S3 bucket and how should I define <code>entry_point</code> and <code>source_dir</code> to deploy it successfully?</p>",2,0,2021-05-24 07:23:29.157000 UTC,2.0,,1,amazon-web-services|amazon-s3|deployment|amazon-sagemaker,325,2021-05-20 13:50:57.017000 UTC,2022-08-07 16:49:50.490000 UTC,,41,2,0,6,,,,,,['amazon-sagemaker']
Error converting string feature to numeric in Azure ML studio,"<p><code>QuotedPremium</code> column is a string feature so I need to convert it to numeric value in order to use algorithm. </p>

<p><a href=""https://i.stack.imgur.com/QUgm0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QUgm0.png"" alt=""enter image description here""></a></p>

<p>So, for that I am using Edit Metadata module, where I specify data type to be converted is <code>Floating Point</code>. </p>

<p>After I run it - I got an error:</p>

<pre><code>Could not convert type System.String to type System.Double, inner exception message: Input string was not in a correct format.
</code></pre>

<p><a href=""https://i.stack.imgur.com/vOEku.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vOEku.png"" alt=""enter image description here""></a></p>

<p>What am I missing here?</p>",1,3,2018-01-04 01:26:49.813000 UTC,,,1,azure|machine-learning|azure-machine-learning-studio,2072,2016-03-10 08:00:45.393000 UTC,2022-09-23 23:59:58.457000 UTC,"San Diego, CA, United States",4046,505,7,825,,,,,,['azure-machine-learning-studio']
How to track parameter and metrics from Vertex AI pipelines,"<p>We are using both Vertex AI training jobs and Kubeflow pipelines in Google Clouds  Vertex AI.</p>
<p>In training jobs we log parameters and metrics to Vertex AI Experiments through the python sdk.</p>
<p>Can Vertex AI Pipelines track metrics from the Kubeflow pipeline to Experiments? Or failing that is it possible to get the pipeline run id and log manually through the sdk using this id is the run id? Any other approaches for experiment tracking in Vertex AI Pipelines?</p>",1,0,2021-10-07 06:58:29.387000 UTC,,,0,google-cloud-ml|kubeflow-pipelines|google-cloud-vertex-ai,404,2019-01-08 10:22:10.440000 UTC,2021-11-16 22:08:35.060000 UTC,,111,0,0,10,,,,,,['google-cloud-vertex-ai']
Issues with deploying spark and mlflow to sagemaker,"<p>My goal is to deploy a spark/mlflow to sagemaker with the following command:</p>
<pre><code>    mlflow sagemaker deploy .. 
</code></pre>
<p>I've successfully pushed a image to EC2 with</p>
<pre><code>mlflow sagemaker build-and-push-container
</code></pre>
<p>I encounter errors when attempting to run mlflow sagemaker deploy:</p>
<pre><code>[error] 446#446: *69 upstream timed out (110: Connection timed out) while reading response header from upstream, client: 10.32.0.2, server: , request: &quot;GET /ping HTTP/1.1&quot;, upstream: &quot;http://127.0.0.1:8000/ping&quot;, host: &quot;model.aws.local:8080&quot;
java.io.IOException: Failed to connect to model.aws.local/172.17.0.2:34473
</code></pre>
<p>Therefore, I added the following as I thought I was mishandling pyspark in sagemaker:</p>
<pre><code>classpath = &quot;:&quot;.join(sagemaker_pyspark.classpath_jars()) 
spark = SparkSession.builder.config( &quot;spark.driver.extraClassPath&quot;, classpath ).appName('audit-risk-predictor-training').getOrCreate()          
</code></pre>
<p>However this outputted the following error:</p>
<pre><code>Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: org/apache/commons/configuration/Configuration
    at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.&lt;init&gt;(DefaultMetricsSystem.java:38)
    at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.&lt;clinit&gt;(DefaultMetricsSystem.java:36)
    at org.apache.hadoop.security.UserGroupInformation$UgiMetrics.create(UserGroupInformation.java:134)
    at org.apache.hadoop.security.UserGroupInformation.&lt;clinit&gt;(UserGroupInformation.java:254)
    at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2487)
    at scala.Option.getOrElse(Option.scala:189)
    at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2487)
    at org.apache.spark.SecurityManager.&lt;init&gt;(SecurityManager.scala:79)
    at org.apache.spark.deploy.SparkSubmit.secMgr$lzycompute$1(SparkSubmit.scala:368)
    at org.apache.spark.deploy.SparkSubmit.secMgr$1(SparkSubmit.scala:368)
    at org.apache.spark.deploy.SparkSubmit.$anonfun$prepareSubmitEnvironment$8(SparkSubmit.scala:376)
    at scala.Option.map(Option.scala:230)
    at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:376)
    at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:894)
    at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
    at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
    at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
    at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1039)
    at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1048)
    at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.lang.ClassNotFoundException: org.apache.commons.configuration.Configuration
    at java.net.URLClassLoader.findClass(URLClassLoader.java:387)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
    ... 20 more
---------------------------------------------------------------------------
Exception                                 Traceback (most recent call last)
Input In [7], in &lt;cell line: 3&gt;()
      1 # Create Spark Session
      2 classpath = &quot;:&quot;.join(sagemaker_pyspark.classpath_jars()) 
----&gt; 3 spark = SparkSession.builder.config( &quot;spark.driver.extraClassPath&quot;, classpath ).appName('audit-risk-predictor-training').getOrCreate()

File ~/.local/lib/python3.8/site-packages/pyspark/sql/session.py:228, in SparkSession.Builder.getOrCreate(self)
    226         sparkConf.set(key, value)
    227     # This SparkContext may be an existing one.
--&gt; 228     sc = SparkContext.getOrCreate(sparkConf)
    229 # Do not update `SparkConf` for existing `SparkContext`, as it's shared
    230 # by all sessions.
    231 session = SparkSession(sc)

File ~/.local/lib/python3.8/site-packages/pyspark/context.py:384, in SparkContext.getOrCreate(cls, conf)
    382 with SparkContext._lock:
    383     if SparkContext._active_spark_context is None:
--&gt; 384         SparkContext(conf=conf or SparkConf())
    385     return SparkContext._active_spark_context

File ~/.local/lib/python3.8/site-packages/pyspark/context.py:144, in SparkContext.__init__(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)
    139 if gateway is not None and gateway.gateway_parameters.auth_token is None:
    140     raise ValueError(
    141         &quot;You are trying to pass an insecure Py4j gateway to Spark. This&quot;
    142         &quot; is not allowed as it is a security risk.&quot;)
--&gt; 144 SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)
    145 try:
    146     self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,
    147                   conf, jsc, profiler_cls)

File ~/.local/lib/python3.8/site-packages/pyspark/context.py:331, in SparkContext._ensure_initialized(cls, instance, gateway, conf)
    329 with SparkContext._lock:
    330     if not SparkContext._gateway:
--&gt; 331         SparkContext._gateway = gateway or launch_gateway(conf)
    332         SparkContext._jvm = SparkContext._gateway.jvm
    334     if instance:

File ~/.local/lib/python3.8/site-packages/pyspark/java_gateway.py:108, in launch_gateway(conf, popen_kwargs)
    105     time.sleep(0.1)
    107 if not os.path.isfile(conn_info_file):
--&gt; 108     raise Exception(&quot;Java gateway process exited before sending its port number&quot;)
    110 with open(conn_info_file, &quot;rb&quot;) as info:
    111     gateway_port = read_int(info)

Exception: Java gateway process exited before sending its port number
</code></pre>
<p>Any insight on where I'm going wrong? Is spark capable of running in sagemaker?</p>",0,0,2022-07-17 22:00:01.223000 UTC,,,0,python|apache-spark|deployment|amazon-sagemaker|mlflow,59,2022-01-13 10:12:59.037000 UTC,2022-09-23 16:16:27.333000 UTC,,45,5,0,4,,,,,,"['mlflow', 'amazon-sagemaker']"
AWS sagemaker-container: How to create or pass the resourceconfig.json to framework for training?,"<p>I am trying to create a custom model/image/container for Amazon Sagemaker.
I had read all the basics tutorials, how to create an image with your requirements. Actually i have a properly set image which runs tensorflow , trains, deploy and serve the model locally.</p>

<p>The problems come when i am trying to run the container using sagemaker python SDK. more precisely, trying to use the framework module and Class to create my own custom estimator to run the custom image/container.</p>

<p>here i post the minimum code to explain my case:</p>

<blockquote>
  <p>File Structure:</p>
</blockquote>

<pre><code>.
├── Dockerfile
├── variables.env
├── requirements.txt
├── test_sagemaker.ipynb
├── src
|   ├── train
|   ├── serve
|   ├── predict.py
|   └── custom_code/my_model_functions
|
└── local_test
    ├── train_local.sh
    ├── serve_local.sh
    ├── predict.sh
    └── test_dir
        ├── model/model.pkl
        ├── output/output.txt
        └── input
            ├── data/data.pkl
            └── config
                ├── hyperparameters.json
                ├── inputdataconfig.json
                └── resourceconfig.json

</code></pre>

<blockquote>
  <p>dockerfile.</p>
</blockquote>

<pre><code>FROM ubuntu:16.04

MAINTAINER Amazon AI &lt;sage-learner@amazon.com&gt;

# Install python and other runtime dependencies
RUN apt-get update &amp;&amp; \
    apt-get -y install build-essential libatlas-dev git wget curl nginx jq &amp;&amp; \
    apt-get -y install python3-dev python3-setuptools

# Install pip
RUN cd /tmp &amp;&amp; \
    curl -O https://bootstrap.pypa.io/get-pip.py &amp;&amp; \
    python3 get-pip.py &amp;&amp; \
    rm get-pip.py

# Installing Requirements
COPY requirements.txt /requirements.txt
RUN pip3 install -r /requirements.txt

# Set SageMaker training environment variables
ENV SM_ENV_VARIABLES env_variables

COPY local_test/test_dir /opt/ml

# Set up the program in the image
COPY src /opt/program
WORKDIR /opt/program
</code></pre>

<blockquote>
  <p>Train</p>
</blockquote>

<pre><code>
from __future__ import absolute_import

import json, sys, logging, os, subprocess, time, traceback
from pprint import pprint

# Custom Code Functions
from custom_code.custom_estimator import CustomEstimator
from custom_code.custom_dataset import create_dataset

# Important Seagemaker Modules
import sagemaker_containers.beta.framework as framework
from sagemaker_containers import _env

logger = logging.getLogger(__name__)

def run_algorithm_mode():
    """"""Run training in algorithm mode, which does not require a user entry point. """"""

    train_config = os.environ.get('training_env_variables')
    model_path = os.environ.get(""model_path"")

    print(""Downloading Dataset"")
    train_dataset,  test_dataset = create_dataset(None)
    print(""Creating Model"")
    clf = CustomEstimator.create_model(train_config)
    print(""Starting Training"")
    clf = clf.train_model(train_dataset, test_dataset)
    print(""Saving Model"")
    module_name = 'classifier.pkl'
    CustomEstimator.save_model(clf, model_path)


def train(training_environment):
    """"""Run Custom Model training in either 'algorithm mode' or using a user supplied module in local SageMaker environment.
    The user supplied module and its dependencies are downloaded from S3.
    Training is invoked by calling a ""train"" function in the user supplied module.
    Args:
        training_environment: training environment object containing environment variables,
                               training arguments and hyperparameters
    """"""

    if training_environment.user_entry_point is not None:
        print(""Entry Point Receive"")
        framework.modules.run_module(training_environment.module_dir,
                                     training_environment.to_cmd_args(),
                                     training_environment.to_env_vars(),
                                     training_environment.module_name,
                                     capture_error=False)
        print_directories()
    else:
        logger.info(""Running Custom Model Sagemaker in 'algorithm mode'"")
        try:
            _env.write_env_vars(training_environment.to_env_vars())
        except Exception as error:
            print(error)
        run_algorithm_mode()

def main():
    train(framework.training_env())
    sys.exit(0)

if __name__ == '__main__':
    main()

</code></pre>

<blockquote>
  <p>test_sagemaker.ipynb</p>
</blockquote>

<p>I created this custom sagemaker estimator using the Framework class of the sagemaker estimator.</p>

<pre><code>import boto3
from sagemaker.estimator import Framework

class ScriptModeTensorFlow(Framework):
    """"""This class is temporary until the final version of Script Mode is released.
    """"""

    __framework_name__ = ""tensorflow-scriptmode""

    create_model = TensorFlow.create_model

    def __init__(
        self,
        entry_point,
        source_dir=None,
        hyperparameters=None,
        py_version=""py3"",
        image_name=None,
        **kwargs
    ):
        super(ScriptModeTensorFlow, self).__init__(
            entry_point, source_dir , hyperparameters, image_name=image_name, **kwargs
        )
        self.py_version = py_version
        self.image_name = None
        self.framework_version = '2.0.0'
        self.user_entry_point = entry_point
        print(self.user_entry_point)
</code></pre>

<p>Then create the estimator passing the <strong>entry_point</strong> and the images (all the others parameters the class needs to run.)</p>

<pre><code>estimator = ScriptModeTensorFlow(entry_point='training_script_path/train_model.py',
                       image_name='sagemaker-custom-image:latest',
                       source_dir='source_dir_path/input/config',
                       train_instance_type='local',      # Run in local mode
                       train_instance_count=1,
                       hyperparameters=hyperparameters,
                       py_version='py3',
                       role=role)
</code></pre>

<p>Finally, hitting training...</p>

<pre><code>estimator.fit({""train"": ""s3://s3-bucket-path/training_data""})
</code></pre>

<p>but I get the following error:</p>

<pre><code>Creating tmpm3ft7ijm_algo-1-mjqkd_1 ... 
Attaching to tmpm3ft7ijm_algo-1-mjqkd_12mdone
algo-1-mjqkd_1  | Reporting training FAILURE
algo-1-mjqkd_1  | framework error: 
algo-1-mjqkd_1  | Traceback (most recent call last):
algo-1-mjqkd_1  |   File ""/usr/local/lib/python3.6/dist-packages/sagemaker_containers/_trainer.py"", line 65, in train
algo-1-mjqkd_1  |     env = sagemaker_containers.training_env()
algo-1-mjqkd_1  |   File ""/usr/local/lib/python3.6/dist-packages/sagemaker_containers/__init__.py"", line 27, in training_env
algo-1-mjqkd_1  |     resource_config=_env.read_resource_config(),
algo-1-mjqkd_1  |   File ""/usr/local/lib/python3.6/dist-packages/sagemaker_containers/_env.py"", line 240, in read_resource_config
algo-1-mjqkd_1  |     return _read_json(resource_config_file_dir)
algo-1-mjqkd_1  |   File ""/usr/local/lib/python3.6/dist-packages/sagemaker_containers/_env.py"", line 192, in _read_json
algo-1-mjqkd_1  |     with open(path, ""r"") as f:
algo-1-mjqkd_1  | FileNotFoundError: [Errno 2] No such file or directory: '/opt/ml/input/config/resourceconfig.json'
algo-1-mjqkd_1  | 
algo-1-mjqkd_1  | [Errno 2] No such file or directory: '/opt/ml/input/config/resourceconfig.json'
algo-1-mjqkd_1  | Traceback (most recent call last):
algo-1-mjqkd_1  |   File ""/usr/local/bin/dockerd-entrypoint.py"", line 24, in &lt;module&gt;
algo-1-mjqkd_1  |     subprocess.check_call(shlex.split(' '.join(sys.argv[1:])))
algo-1-mjqkd_1  |   File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call
algo-1-mjqkd_1  |     raise CalledProcessError(retcode, cmd)
algo-1-mjqkd_1  | subprocess.CalledProcessError: Command '['train']' returned non-zero exit status 2.
tmpm3ft7ijm_algo-1-mjqkd_1 exited with code 1
Aborting on container exit...
</code></pre>

<p>At first glance the error seems obvious, the file  '/opt/ml/input/config/resourceconfig.json' is missing. The thing is I have no way of creating this file so that sagemaker framework can get the host for multiprocessing (whcih i don t need them yeet).
When I am creating the image 'sagemaker-custom-image:latest' following the folder structure show bellow, I already give the 'resoruceconfig.json' to the '/opt/ml/input/config/' folder inside the image.</p>

<pre><code>/opt/ml
├── input
│   ├── config
│   │   ├── hyperparameters.json
│   │   ├── inputdataconfig.json
│   │   └── resourceConfig.json
│   └── data
│       └── &lt;channel_name&gt;
│           └── &lt;input data&gt;
├── model
│   └── &lt;model files&gt;
└── output
    └── failure
</code></pre>

<p>Reading the documentation in AWS, when using sagemaker sdk to run your image, it says that all the files in the container in the folder 'opt/ml' may no longer be visibles during training.</p>

<blockquote>
  <p>/opt/ml and all sub-directories are reserved by Amazon SageMaker training. When building your algorithm’s docker image, please ensure you don't place any data required by your algorithm under them as the data may no longer be visible during training.<a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo-dockerfile.html"" rel=""nofollow noreferrer"">How Amazon SageMaker Runs Your Training Image</a></p>
</blockquote>

<p>This basically resumes my problem.</p>

<p>Yes, I know I can make use of the prebuilt estimators and images from sagemaker.</p>

<p>Yes, I know I can bypass the framwork library and run the image train from docker run.</p>

<p>But i have the need to implement a fully custom sagemaker sdk/image/container/model to use with entrypoint. I know is a bit ambitious.</p>

<p><strong>So to Reformulate my question</strong>: How do I get Sagemaker Framework or SDK to create inside the image the require resourceconfig.json file?</p>",2,0,2020-02-03 19:42:38.520000 UTC,1.0,,3,amazon-sagemaker,2596,2018-02-09 15:34:03.580000 UTC,2022-02-03 18:57:07.250000 UTC,"Santiago, Chile",118,4,0,11,,,,,,['amazon-sagemaker']
Error Code 429 on Vertex AI (Google Cloud Platform),"<p>I'm currently running jobs on Vertex AI and I encountered the following problem :</p>
<pre><code>&quot;error&quot;: {
    &quot;code&quot;: 429,
    &quot;message&quot;: &quot;The following quota metrics exceed quota limits: aiplatform.googleapis.com/custom_model_training_nvidia_p4_gpus&quot;,
    &quot;status&quot;: &quot;RESOURCE_EXHAUSTED&quot;
  }
</code></pre>
<p>Last Friday, I had this error, and Monday, it worked again. Since then, I ran 8 jobs and the error came back.</p>
<p>I read Google documentation on Quotas and checked Quotas on IAM and Admin, but I didn't really understand it. It didn't seem that I exceeded something.
Could someone explain to me how quotas work?</p>",0,0,2022-08-03 13:48:49.530000 UTC,1.0,2022-08-04 15:16:52.203000 UTC,2,google-cloud-platform|quota|google-cloud-vertex-ai,156,2022-08-03 13:25:26.597000 UTC,2022-09-20 14:46:43.420000 UTC,,21,0,0,1,,,,,,['google-cloud-vertex-ai']
How do I load the model artifact from AWS Sagemaker built-in container?,"<p>I'm using the linear-learner container from Sagemaker to train a model. The training has completed and the model artifact is saved in S3. I download it which is a .tar.gz file and there is the actual model file stored in it called model-algo-1 without format extension. I'm trying to load this model and inspect the model coefficients but not sure how to do so.</p>
<p>I tried pickle and joblib but they didn't work. Does anyone know how to load the model file trained from Sagemaker built-in container? Or is there any other way I can check the model coefficients? It's a logistic regression model.</p>
<p>Thanks</p>",0,0,2020-08-26 03:51:08.283000 UTC,,2020-08-26 16:56:35.780000 UTC,2,python|amazon-web-services|machine-learning|amazon-sagemaker,281,2020-05-29 05:17:00.280000 UTC,2022-02-09 17:12:29.170000 UTC,,23,0,0,7,,,,,,['amazon-sagemaker']
How to register a local model of 900mb in Azure Machine Learning Service with CLI or SDK,"<p>I have a 900mb model and a 9mb model. </p>

<p>I am using the latest version of the Azure ML CLI to register it in my workspace with this command:</p>

<pre><code>az ml model register -n ""rj-model"" --model-path ""models\model_v1.bin -t ""model-deployment\model.json""
</code></pre>

<p>The 9mb file uploads successfully however the 900 mb file times out with this error:</p>

<pre><code>{'Azure-cli-ml Version': '1.0.60.1', 'Error': AzureHttpError('Operation could not be completed within the specified time. ErrorCode: OperationTimedOut\n&lt;?xml version=""1.0"" encoding=""utf-8""?&gt;&lt;Error&gt;&lt;Code&gt;OperationTimedOut&lt;/Code&gt;&lt;Message&gt;Operation could not be completed within the specified time.\nRequestId:2822b750-801e-0061-65fe-71aa88000000\nTime:2019-09-23T11:03:52.5521518Z&lt;/Message&gt;&lt;/Error&gt;',)}
</code></pre>

<p>The UI has an even lower timeout threshold.</p>

<p>My thoughts on solving this:  </p>

<ul>
<li>I need to upload the model to blob storage and access it from the cloud, not locally. I checked the <a href=""https://docs.microsoft.com/en-us/cli/azure/ext/azure-cli-ml/ml/model?view=azure-cli-latest#ext-azure-cli-ml-az-ml-model-register"" rel=""nofollow noreferrer"">Model Register</a> method and it has an <strong>--asset-path</strong> property, however this is for an Experiment (I am not running experiments with AML - a Data Scientist is doing them in his own workflow and passing the models over to me for deployment). </li>
<li>The DataStore looks promising and I could mount my model on a blob (after uploading it with Storage Explorer) and access it - however I can't seem to do this with the CLI. The Python SDK does have a dataset property but not a datastore property and the <a href=""https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.model.model?view=azure-ml-py#register-workspace--model-path--model-name--tags-none--properties-none--description-none--datasets-none--model-framework-none--model-framework-version-none--child-paths-none-"" rel=""nofollow noreferrer"">Register method</a> does not seem to have a cloud option.</li>
</ul>

<p>So my question is: How can I register large models in Azure ML?</p>",0,3,2019-09-23 11:12:54.930000 UTC,1.0,,1,azure-machine-learning-service,375,2009-10-21 01:51:25.500000 UTC,2022-09-13 05:24:36.847000 UTC,"Sydney, Australia",4947,277,8,531,,,,,,['azure-machine-learning-service']
Azure ML & Pandas: How to convert String to DateTime,"<p>I've got a dataset at hand with a column of DateTime in String format, eg.</p>

<pre><code>a = 'Tue Sep 22 1998 00:00:00 GMT+0000 (Coordinated Universal Time)'
</code></pre>

<p>and a is just a value from the column.</p>

<p>If I use Metadata Editor in Azure Machine Learning Studio, it won't work and will complain that it can't do the conversion (from String to DateTime). I guess it's something to do with the format. So I'm trying the following:</p>

<pre><code>a = str(a)[:10]+','+str(a)[10:15]
#'Tue Sep 22, 1998'
</code></pre>

<p>Now .NET surely can do the conversion, I mean by method like Convert.ToDateTime(). However, when I visualized the output of the Python script, I found the String has been changed into 'Tue Sep 22, 1998 None,', which is quite weird. Anyone knows what's wrong with it? I'm attaching the excerpt of python code down below:</p>

<pre><code>def azureml_main(dataframe1 = None, dataframe2 = None):

  dataframe1['timestamp'] = dataframe1['timestamp'].apply(lambda a: str(a)[:10]+','+str(a)[10:15])

  return dataframe1,
</code></pre>",1,2,2015-03-21 07:26:20.947000 UTC,,2016-01-23 02:04:04.060000 UTC,3,python|pandas|machine-learning|azure-machine-learning-studio,2509,2012-06-21 12:42:46.043000 UTC,2022-09-22 02:44:08.533000 UTC,,1618,1035,5,185,,,,,,['azure-machine-learning-studio']
How do I set the name of my AWS SageMaker repo user?,"<p>In the GitHub repo for my AWS SageMaker project, commits are labelled as being created by the user ""EC2 Default User"".</p>

<p><a href=""https://i.stack.imgur.com/gHDbe.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gHDbe.png"" alt=""enter image description here""></a></p>

<p>How do I customize the name of this user in SageMaker so that it is used every time I start my Notebook Instance? </p>",3,2,2018-12-14 18:31:31.863000 UTC,2.0,2018-12-19 17:38:13.633000 UTC,3,git|github|amazon-sagemaker|aws-secrets-manager,2362,2011-03-12 19:54:30.313000 UTC,2022-09-21 19:06:38.420000 UTC,United States,41475,1198,107,1912,,,,,,['amazon-sagemaker']
Cannot access registered dataset in AMLS pipeline,"<p>I have a service principal for my AMLS workspace that has been granted a storage blob contributor role to ADLS Gen 2. ADLS Gen 2 is behind a vnet, but using the service principal, I was able to register it as a datastore and register a csv file in ADLS Gen 2 as a dataset in my AMLS workspace. I am using azureml.core version 1.16.0</p>
<p>Within my workspace, running</p>
<pre><code>data = ws.datasets.get(&quot;csv data&quot;)
data.take(5).to_pandas_dataframe()
</code></pre>
<p>works with no issue. I would like to use this csv data as input into an ML pipeline run with a PythonScriptStep with inputs = [data.as_named_input('data')] after running data = ws.datasets.get(&quot;csv data&quot;). However, when I run the code</p>
<pre><code>run = Run.get_context()
run.input_datasets['data'].to_pandas_dataframe()
</code></pre>
<p>in my pipeline script, it fails with an error</p>
<pre><code>StreamAccessException was caused by AuthenticationException.
'AdlsGen2-ReadHeaders' for '[REDACTED]' on storage failed with status code 'Forbidden' (This request is
not authorized to perform this operation.)
</code></pre>
<p>Where am I going wrong?</p>",0,1,2020-11-04 00:54:59.120000 UTC,,2020-11-04 02:04:44.397000 UTC,1,azure-machine-learning-service|vnet,185,2020-05-17 18:00:51.347000 UTC,2022-06-27 19:36:47.687000 UTC,,179,2,0,53,,,,,,['azure-machine-learning-service']
Use Neo4j in Azure Devops or Azure Machine learning,"<p>I have a project in python. My goal is to create an article recommendation with Neo4j.</p>
<p>Here is what I did:</p>
<ol>
<li>Web scraping articles</li>
<li>Clean the data</li>
<li>Insert the data into Neo4j</li>
<li>Use Neo4j algorithms (graph data science library)</li>
</ol>
<p>Everything is working, but only on my laptop.</p>
<p>I would like to migrate my project on Azure devops and azure machine learning in order to have a web app.</p>
<p>I have an Azure account and I have created Azure ML and Azure Devops for this project but I don't know how I can use Neo4j in azure devops and azure machine learning. Maybe with a VM or a container ?</p>",1,2,2020-09-11 08:30:49.167000 UTC,,2020-09-14 08:22:04.443000 UTC,3,python|azure|neo4j|azure-machine-learning-service,206,2020-05-04 19:50:13.210000 UTC,2021-11-25 09:41:09.417000 UTC,,197,34,0,82,,,,,,['azure-machine-learning-service']
predict() missing 1 required positional argument: 'X' while consuming a deployed web service through python in azure,"<p>I'm trying to consume a web service that I deployed and I get predict() missing 1 required positional argument: 'X' error when I try to consume it with the REST end point. Here is a link for reference about my previous questions in Microsoft:</p>
<p><a href=""https://docs.microsoft.com/en-us/answers/questions/286859/predict-missing-1-required-positional-argument-39x.html?childToView=300191#answer-300191"" rel=""nofollow noreferrer"">Micsoroft question</a></p>
<p><a href=""https://docs.microsoft.com/en-us/answers/questions/282967/error-while-consuming-the-deployed-web-service-thr.html"" rel=""nofollow noreferrer"">Follow up question</a></p>
<p>Here are my train.py and score.py</p>
<p>train.py</p>
<pre><code>df = pd.read_csv('prediction_data01.csv')
df = df[pd.notnull(df['DESCRIPTION'])]
df = df[pd.notnull(df['CUSTOMERCODE'])]
col = ['CUSTOMERCODE', 'DESCRIPTION']
df = df[col]
df.columns = ['CUSTOMERCODE', 'DESCRIPTION']
df['category_id'] = df['DESCRIPTION'].factorize()[0]

tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 4), stop_words='english')
features = tfidf.fit_transform(df.DESCRIPTION).toarray()
labels = df.category_id

df = df.applymap(str)
X_train, X_test, y_train, y_test = train_test_split(df['CUSTOMERCODE'], df['DESCRIPTION'], random_state=0)
count_vect = CountVectorizer()
X_train_counts = count_vect.fit_transform(X_train)
tfidf_transformer = TfidfTransformer()
X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)

clf = MultinomialNB().fit(X_train_tfidf, y_train)
os.makedirs(&quot;./outputs&quot;, exist_ok=True)
joblib.dump(clf, 'prediction-model.pickle')
</code></pre>
<p>Score.py</p>
<pre><code>def init():
    global model
    # AZUREML_MODEL_DIR is an environment variable created during deployment.
    # It is the path to the model folder (./azureml-models/$MODEL_NAME/$VERSION)
    # For multiple models, it points to the folder containing all deployed models (./azureml-models)
    model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), &quot;prediction-model.pickle&quot;)
    model = joblib.load(model_path)

def run(raw_data):
    data = np.array(json.loads(raw_data)['data'])
    # make prediction
    y_hat = model.predict(data)
    # you can return any data type as long as it is JSON-serializable
    return y_hat.tolist()
</code></pre>
<p>I have tested the model results locally and it's working fine. I predicted the results of the model and with the below code.</p>
<pre><code>clf = MultinomialNB().fit(X_train_tfidf, y_train)
with open(&quot;prediction.pickle&quot;, &quot;wb&quot;) as f:
pickle.dump(MultinomialNB, f)
print(clf.predict(count_vect.transform([&quot;18339&quot;])))
</code></pre>
<p>I'm able to predict successfully with the above code and also I'm able to predict with loading the saved model pickle file using the below code.</p>
<pre><code>pickle_in = open(&quot;prediction.pickle&quot;, &quot;rb&quot;)
Multinomial_model = pickle.load(pickle_in)
clf = Multinomial_model().fit(X_train_tfidf, y_train)
print(clf.predict(count_vect.transform([&quot;18339&quot;])))
</code></pre>
<p>I get this error -- fit() missing 1 required positional argument: 'y'-- when I do not use parenthesis in the above code fit method. I dont know if it helps.</p>
<pre><code>clf = Multinomial_model.fit(X_train_tfidf, y_train)
</code></pre>
<p>Any help is appreciated. Thanks in advance.</p>",0,0,2021-03-05 06:02:13.523000 UTC,,2021-03-05 08:13:39.700000 UTC,1,python|azure|machine-learning|scikit-learn|azure-machine-learning-service,264,2020-08-28 11:04:43.117000 UTC,2021-03-30 05:33:41.520000 UTC,"Madurai, Tamil Nadu, India",60,3,0,6,,,,,,['azure-machine-learning-service']
Xgboost Amazon Sagemaker predict_proba,"<p>I have trained an Xgboost multi-class classification algo for 5 classes using the Sagemaker framework. I then saved the model on S3. Now, when I load it, I only have the option of predict, that will return me the classes and not the probabilities of each class.</p>

<pre><code>pred = xgb_uploaded_model.predict(new_data)
</code></pre>

<p>The uploaded model has no option of predict_proba, that would have came in handy. 
Any ideas on hot to get probabilities from this a model saved like this? 
P.S. I don't want to use Endpoints.</p>",0,2,2018-06-07 15:03:08.867000 UTC,,,6,amazon-sagemaker,869,2015-05-26 22:53:10.120000 UTC,2019-06-03 07:51:39.510000 UTC,,455,7,0,58,,,,,,['amazon-sagemaker']
Azure ML failed to deploy endpoint,"<h2>Problem Description</h2>
<p>I am following <a href=""https://www.analyticsvidhya.com/blog/2022/02/deploy-your-ml-model-as-a-web-service-in-microsoft-azure-cloud/"" rel=""nofollow noreferrer"">this tutorial</a> to deploy an ML model to Azure ML as a web service.</p>
<p>The model has been uploaded successfully (Image 1), but it failed when I created the web service endpoint (Image 2).</p>
<p><img src=""https://i.stack.imgur.com/nQEDr.png"" alt=""Image 1: succeeded model upload"" /></p>
<p><img src=""https://i.stack.imgur.com/QJUEO.png"" alt=""Image 2: failed endpoint deployment"" /></p>
<p>I am new to cloud computing and still confused about how to troubleshoot this issue. The &quot;Deployment logs&quot; tab only contained short information as below.</p>
<pre><code>container &quot;predict&quot; in pod &quot;wk-caas-f62cbd87d3e3400a8fffc23a20f0744e-8c212518c5f6b7404584eb194515f3a1-pod&quot; is waiting to start: PodInitializing
</code></pre>
<p>Here are the model and source codes I used in the deployment:</p>
<ul>
<li><a href=""https://github.com/mari-bangkit/prediction-model/blob/main/regressor.pkl"" rel=""nofollow noreferrer"">Model</a></li>
<li><a href=""https://github.com/mari-bangkit/prediction-model/blob/main/scoring.py"" rel=""nofollow noreferrer"">Entry script</a></li>
<li><a href=""https://github.com/mari-bangkit/prediction-model/blob/main/conda_env.yml"" rel=""nofollow noreferrer"">Conda environment YAML</a></li>
<li><a href=""https://pastebin.com/nUy8YGyh"" rel=""nofollow noreferrer"">Deployment logs</a></li>
</ul>
<h2>Question</h2>
<p>Any advice to solve this issue?</p>",1,2,2022-06-07 03:38:30.807000 UTC,0.0,2022-06-07 05:16:09.707000 UTC,1,python|azure|machine-learning|conda|azure-machine-learning-service,211,2019-11-30 10:44:54.723000 UTC,2022-09-22 13:21:33.573000 UTC,,11,0,0,2,,,,,,['azure-machine-learning-service']
"""list index out of range"" error in AzureML inference schema","<p>I've deployed a model using AzureML's inference cluster. I recently found that some of the requests to the model's API endpoint resulted in a 404 HTTP error involving a missing swagger.json file.</p>
<p>So I followed <a href=""https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-advanced-entry-script"" rel=""nofollow noreferrer"">this guide</a> in order to auto-generate the swagger.json file. But now all the requests to the endpoint result in a &quot;list index out of range&quot; error and it's something to do with the <code>input_schema</code> decorator. I just can't seem to pinpoint what the problem is exactly.</p>
<p>Here is a minimal recreation of my scoring script:</p>
<pre><code>from inference_schema.schema_decorators import input_schema, output_schema
from inference_schema.parameter_types.standard_py_parameter_type import StandardPythonParameterType


def inference(args):
    # inference logic here
    return model_output


def init():
    global model
    model = get_model()


input_sample = StandardPythonParameterType({
    'input_1': 'some text',
    'input_2': 'some other text',
    'input_3': 'other text'
})

sample_global_parameters = StandardPythonParameterType(1.0)

output_sample = StandardPythonParameterType({
    'Results': {
        'text': 'some text',
        'model_output': [
             {
                 'entity_type': 'date',
                 'value': '05/04/2022'
             }
        ]
    }
})

@input_schema('Inputs', input_sample)
@input_schema('GlobalParameters', sample_global_parameters)
@output_schema(output_sample)

def run(Inputs, GlobalParameters):
    try:
        return inference(Inputs['input_1'], Inputs['input_2'], Inputs['input_3'])
    except Exception as e:
        error = str(e)
        return error

</code></pre>
<p>I've checked out <a href=""https://stackoverflow.com/questions/64315239/azure-ml-inference-schema-list-index-out-of-range-error"">this</a> and <a href=""https://docs.microsoft.com/en-us/answers/questions/442305/swagger-file-not-present-azure-machine-learning.html"" rel=""nofollow noreferrer"">this</a> question but it didn't seem to help.</p>
<p>I tried looking at the <a href=""https://github.com/Azure/InferenceSchema"" rel=""nofollow noreferrer"">code on GitHub</a> as well but I still can't triangulate on the exact problem.</p>
<p>I'm calling the API from Postman with the default headers (I'm not adding anything). The request body looks like this:</p>
<pre><code>{
    &quot;Inputs&quot;: {
        &quot;input_1&quot;: &quot;some text&quot;,
        &quot;input_2&quot;: &quot;some other text&quot;,
        &quot;input_3&quot;: &quot;different text&quot;
    },
    &quot;GlobalParameters&quot;: 1.0
}
</code></pre>
<p>This is the error message from the endpoint logs:</p>
<pre><code>2022-04-05 06:33:22,536 | root | ERROR | Encountered Exception: Traceback (most recent call last):
  File &quot;/var/azureml-server/synchronous/routes.py&quot;, line 65, in run_scoring
    response, time_taken_ms = invoke_user_with_timer(service_input, request_headers)
  File &quot;/var/azureml-server/synchronous/routes.py&quot;, line 110, in invoke_user_with_timer
    result, time_taken_ms = capture_time_taken(user_main.run)(**params)
  File &quot;/var/azureml-server/synchronous/routes.py&quot;, line 92, in timer
    result = func(*args, **kwargs)
  File &quot;/var/azureml-app/main.py&quot;, line 21, in run
    return_obj = driver_module.run(**arguments)
  File &quot;/azureml-envs/azureml_e63c7c0baf9bf3d861ce5992975a467b/lib/python3.7/site-packages/inference_schema/schema_decorators.py&quot;, line 61, in decorator_input
    return user_run(*args, **kwargs)
  File &quot;/azureml-envs/azureml_e63c7c0baf9bf3d861ce5992975a467b/lib/python3.7/site-packages/inference_schema/schema_decorators.py&quot;, line 55, in decorator_input
    args[param_position] = _deserialize_input_argument(args[param_position], param_type, param_name)
IndexError: list index out of range

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;/azureml-envs/azureml_e63c7c0baf9bf3d861ce5992975a467b/lib/python3.7/site-packages/flask/app.py&quot;, line 1832, in full_dispatch_request
    rv = self.dispatch_request()
  File &quot;/azureml-envs/azureml_e63c7c0baf9bf3d861ce5992975a467b/lib/python3.7/site-packages/flask/app.py&quot;, line 1818, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File &quot;/var/azureml-server/synchronous/routes.py&quot;, line 44, in score_realtime
    return run_scoring(service_input, request.headers, request.environ.get('REQUEST_ID', '00000000-0000-0000-0000-000000000000'))
  File &quot;/var/azureml-server/synchronous/routes.py&quot;, line 74, in run_scoring
    raise RunFunctionException(str(exc))
run_function_exception.RunFunctionException
</code></pre>",1,2,2022-04-05 06:40:03.770000 UTC,,,2,python|azure|azure-devops|azure-machine-learning-service,191,2022-04-04 10:46:28.377000 UTC,2022-07-22 12:18:06.723000 UTC,,31,0,0,2,,,,,,['azure-machine-learning-service']
Cannot use `gcloud auth print-identity-token` from within Vertex AI Custom Job,"<h3>Background</h3>
<ul>
<li>I want to use a service account, with a vertex AI custom job, and create an identity token.</li>
<li>I am attempting to notify a cloud run API once a vertex-AI custom job has finished.</li>
<li>I have created a service account with a <code>roles/run.invoker</code> role, and have confirmed that the service account can access the cloud run API.
<ul>
<li>I used credentials from the service account and created an identity token with audience set to the cloud run API URL</li>
</ul>
</li>
</ul>
<h3>Problem</h3>
<ul>
<li>When a vertex AI custom job is created using <a href=""https://cloud.google.com/sdk/gcloud/reference/ai/custom-jobs/create"" rel=""nofollow noreferrer""><code>gcloud ai custom-jobs create</code></a> or through the <a href=""https://pkg.go.dev/cloud.google.com/go/aiplatform"" rel=""nofollow noreferrer"">golang client library</a>, an identity token cannot be obtained for a custom service account.
<ul>
<li><code>gcloud auth print-identity-token</code> results in an error:
<ul>
<li><code>(gcloud.auth.print-identity-token) No identity token can be obtained from the current credentials.</code></li>
</ul>
</li>
<li>Using the <a href=""https://cloud.google.com/run/docs/authenticating/service-to-service#use_the_metadata_server"" rel=""nofollow noreferrer"">metadata server URL</a> directly gives a <code>Not Found</code> response.
<ul>
<li>Command:
<pre><code>curl \
  -s \
  --get \
  --data-urlencode &quot;audience=$CLOUD_RUN_API_URL&quot; \
  --data-urlencode &quot;format=full&quot; \
  -H &quot;Metadata-Flavor: Google&quot; \
  http://metadata/computeMetadata/v1/instance/service-accounts/default/identity
</code></pre>
</li>
<li>Response: <code>Not Found</code></li>
</ul>
</li>
</ul>
</li>
<li>I need some guidance on what settings / way of invoking the custom job will allow me to access the service account's identity token.</li>
</ul>
<hr />
<h3>Details</h3>
<ul>
<li>For the purposes of testing creating an identity token, the custom container is specified using the docker file below:</li>
</ul>
<pre><code>FROM alpine:3.6

RUN apk add --update python curl which bash
RUN curl -sSL https://sdk.cloud.google.com | bash -s -- --disable-prompts
ENV PATH $PATH:/root/google-cloud-sdk/bin

CMD [&quot;gcloud&quot;, &quot;auth&quot;, &quot;print-identity-token&quot;, &quot;--verbosity&quot;, &quot;debug&quot;]
</code></pre>
<ul>
<li><p>The following produce a working identity token:</p>
<ul>
<li>✅ Running this image locally</li>
<li>✅ Running this image using the &quot;Create&quot; button on the vertex AI GUI and setting the service account to my custom one, created for this use case.</li>
<li>✅ Running this image using <code>gcloud</code> with NO service account set (using vertex-ai default service account for the project)</li>
</ul>
</li>
<li><p>The following results in an error (the problem which prompted me to ask this question):</p>
<ul>
<li><p>❌ Running this image using <code>gcloud</code> with the following settings, I get an <strong>error</strong>:</p>
<ul>
<li>Command (image name/service account redacted):
<pre><code>gcloud ai custom-jobs create \
  --region=asia-northeast1 \
  --display-name=cli_identity_test \
  --worker-pool-spec=machine-type=n2-standard-4,replica-count=1,container-image-uri=&lt;IMAGE NAME&gt; \
  --service-account=&lt;SERVICE ACCOUNT EMAIL&gt;
</code></pre>
</li>
<li>Error:
<pre><code>DEBUG: (gcloud.auth.print-identity-token) No identity token can be obtained from the current credentials.                                                                
Traceback (most recent call last):                                                                                                                                       
  File &quot;/root/google-cloud-sdk/lib/googlecloudsdk/calliope/cli.py&quot;, line 987, in Execute                                                                                 
    resources = calliope_command.Run(cli=self, args=args)                                                                                                                
  File &quot;/root/google-cloud-sdk/lib/googlecloudsdk/calliope/backend.py&quot;, line 809, in Run                                                                                 
    resources = command_instance.Run(args)                                                                                                                               
  File &quot;/root/google-cloud-sdk/lib/googlecloudsdk/calliope/exceptions.py&quot;, line 129, in TryFunc                                                                          
    return func(*args, **kwargs)                                                                                                                                         
  File &quot;/root/google-cloud-sdk/lib/surface/auth/print_identity_token.py&quot;, line 130, in Run                                                                               
    credential = _Run(args)                                                                                                                                              
  File &quot;/root/google-cloud-sdk/lib/surface/auth/print_identity_token.py&quot;, line 78, in _Run                                                                               
    'No identity token can be obtained from the current credentials.')                                                                                                   
InvalidIdentityTokenError: No identity token can be obtained from the current credentials.                                                                               
ERROR: (gcloud.auth.print-identity-token) No identity token can be obtained from the current credentials.
</code></pre>
</li>
</ul>
</li>
<li><p>❌ Running the image with a custom service account, and using the vertex ai golang API client library to start the job</p>
</li>
</ul>
</li>
</ul>
<hr />
<h3>A note on roles</h3>
<ul>
<li>I have also tried extending the roles associated with my service account, but these do not fix the error:
<ul>
<li>to match the default vertex-ai service account (<code>roles/aiplatform.customCodeServiceAgent</code>)</li>
<li>Service Account Token Creator (<code>roles/iam.serviceAccountTokenCreator</code>)</li>
<li>Service Account User (<code>roles/iam.serviceAccountUser</code>)</li>
</ul>
</li>
</ul>",1,1,2022-07-04 08:02:27.750000 UTC,,,0,google-cloud-platform|gcloud|service-accounts|google-cloud-vertex-ai,243,2010-07-31 06:08:12.487000 UTC,2022-09-25 03:32:37.283000 UTC,"Tokyo, Japan",8173,150,10,394,,,,,,['google-cloud-vertex-ai']
"Why does saving CSV file in S3 work in notebook cell, but not in terminal?","<p>I am trying to save a csv file on s3 by using a buffer object. The strange thing is that all works perfectly fine if I do it in jupyter notebook cell. I can read it again using pandas without any issues. Below is the code snippet:</p>
<pre><code>csv_buffer = StringIO()
s3_resource = boto3.resource('s3')
df.to_csv(csv_buffer, index = False)
_ = s3_resource.meta.client.put_object(Body = csv_buffer.getvalue(), Bucket = 'bucket', Key = 'temp.csv')
</code></pre>
<p>But, when I try to execute the same code dynamically in my program through command line interface, the file is being saved, albeit when I try to read it using pandas, I am getting error as follows: UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte. As expected, when I download the file from s3, there are strange characters.</p>
<p>I have tried various solutions including new conda environment, using parquet instead of csv, adding ContentType parameter in put object. But nothing seems to work. Also, when I print the contents of csv buffer in my script, I can see the string contents as well. Any help would be much appreciated.</p>",0,3,2022-03-30 19:09:26.867000 UTC,,,1,python|amazon-s3|amazon-sagemaker,44,2022-03-30 18:54:29.280000 UTC,2022-09-23 07:53:28.643000 UTC,,11,0,0,0,,,,,,['amazon-sagemaker']
How to use packaged model tar.gz inside SageMaker Processing Job?,"<p>I am working on deploying a full ML pipeline for SageMaker and Airflow. I would like to separate training and processing part of the pipeline.</p>
<p>I have a question concerning the <code>SageMakerProcessingOperator</code>(<a href=""https://airflow.apache.org/docs/apache-airflow-providers-amazon/1.1.0/_modules/airflow/providers/amazon/aws/operators/sagemaker_processing.html"" rel=""nofollow noreferrer"">source_code</a>). This operator relies on <a href=""https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_processing_job"" rel=""nofollow noreferrer"">create_processing_job()</a> function. When using this operator, I would like to extend the base docker image used for processing in order to use an home-made script. Currently, the processing works fine when I push my container to aws ECR. However, I would prefer to use a part of the script stored inside my packaged model (with tar.gz format).</p>
<p>For training and registering the model, we can specify the image used to extend with <code>sagemaker_submit_directory</code> and <code>SAGEMAKER_PROGRAM</code> env variable (cf <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/prebuilt-containers-extend.html"" rel=""nofollow noreferrer"">aws_doc</a>). However it looks like it is not possible using the SageMakerProcessingOperator.
Below is a extract of the config used in the operator, with no success yet.</p>
<pre><code>&quot;Environment&quot;: {
    &quot;sagemaker_enable_cloudwatch_metrics&quot;: &quot;false&quot;,
    &quot;SAGEMAKER_CONTAINER_LOG_LEVEL&quot;: &quot;20&quot;,
    &quot;SAGEMAKER_REGION&quot;: f&quot;{self.region_name}&quot;,
    &quot;SAGEMAKER_SUBMIT_DIRECTORY&quot;: f&quot;{self.train_code_path}&quot;,
    &quot;SAGEMAKER_PROGRAM&quot;: f&quot;{self.processing_entry_point}&quot;,
    &quot;sagemaker_job_name&quot;: f&quot;{self.process_job_name}&quot;,
},
</code></pre>
<p>Did anyone manage to use these parameters for Sagemaker create_processing_job() ? Or is it only limited to AWS ECR ?</p>",1,0,2022-05-02 23:18:16.500000 UTC,,,0,amazon-web-services|tensorflow|airflow|amazon-sagemaker|mlops,211,2020-06-10 07:58:06.757000 UTC,2022-06-21 09:13:33.713000 UTC,"Paris, France",23,5,0,6,,,,,,['amazon-sagemaker']
How to get all metrics for a SageMaker Hyperparameter tuning job?,"<p>SageMaker does offer a HyperparameterTuningJobAnalytics object, but it only contains the final objective metric value. </p>

<p>Here is example code.</p>

<pre class=""lang-py prettyprint-override""><code>tuner = sagemaker.HyperparameterTuningJobAnalytics(tuning_job_name)

full_df = tuner.dataframe()
</code></pre>

<p>The dataframe it returns only contains the objective metric as the column FinalObjectiveValue.</p>

<p>If I have defined more than one metric for the tuning job, how do I get other metrics in SageMaker?</p>",1,0,2018-10-16 14:34:08.550000 UTC,,2018-10-17 17:25:31.177000 UTC,0,amazon-web-services|amazon-sagemaker|hyperparameters,754,2017-02-02 23:16:08.110000 UTC,2019-11-12 20:10:42.617000 UTC,"Cleveland, OH, United States",1,0,0,4,,,,,,['amazon-sagemaker']
Using InputPath/Output Path on Vertex?,"<p>TLDR, does Google Cloud Vertex support InputPath/OutputPath?</p>
<p>I have set up my environment on Google Cloud in the following way:</p>
<pre><code>export PROJECT_ID=&quot;xxx&quot;
export SERVICE_ACCOUNT_ID=&quot;xxx&quot;
export USER_EMAIL=&quot;xxx&quot;
export BUCKET_NAME=&quot;xxx&quot;
export FILE_NAME=&quot;xxx&quot;
export GOOGLE_APPLICATION_CREDENTIALS=&quot;xxx&quot;


gcloud iam service-accounts create $SERVICE_ACCOUNT_ID \
--description=&quot;Service principal for running vertex and creating pipelines/metadata&quot; \
--display-name=&quot;$SERVICE_ACCOUNT_ID&quot; \
--project ${PROJECT_ID}

gcloud projects add-iam-policy-binding ${PROJECT_ID} \
    --member=&quot;serviceAccount:$SERVICE_ACCOUNT_ID@$PROJECT_ID.iam.gserviceaccount.com&quot; \
    --role=roles/storage.objectAdmin

gcloud projects add-iam-policy-binding ${PROJECT_ID} \
    --member=&quot;serviceAccount:$SERVICE_ACCOUNT_ID@$PROJECT_ID.iam.gserviceaccount.com&quot; \
    --role=roles/aiplatform.user

gcloud projects add-iam-policy-binding ${PROJECT_ID} \
    --member=&quot;serviceAccount:$SERVICE_ACCOUNT_ID@$PROJECT_ID.iam.gserviceaccount.com&quot; \
    --role=roles/ml.admin

gcloud projects get-iam-policy $PROJECT_ID \
    --flatten=&quot;bindings[].members&quot; \
    --format='table(bindings.role)' \
    --filter=&quot;bindings.members:serviceAccount:$SERVICE_ACCOUNT_ID@$PROJECT_ID.iam.gserviceaccount.com&quot;

gcloud iam service-accounts add-iam-policy-binding \
    $SERVICE_ACCOUNT_ID@$PROJECT_ID.iam.gserviceaccount.com \
    --member=&quot;user:$USER_EMAIL&quot; \
    --role=&quot;roles/iam.serviceAccountUser&quot;
    --project ${PROJECT_ID}

gsutil mb -p $PROJECT_ID gs://$BUCKET_NAME

gsutil iam ch \
    serviceAccount:$SERVICE_ACCOUNT_ID@$PROJECT_ID.iam.gserviceaccount.com:roles/storage.objectCreator,objectViewer \
    gs://$BUCKET_NAME

https://cloud.google.com/docs/authentication/getting-started#auth-cloud-implicit-python
gcloud iam service-accounts keys create $FILE_NAME.json --iam-account=$SERVICE_ACCOUNT_ID@$PROJECT_ID.iam.gserviceaccount.com 
</code></pre>
<p>I then have the following three blocks of code:</p>
<pre class=""lang-py prettyprint-override""><code>import kfp.v2.components
from kfp.v2.dsl import InputPath
from kubernetes.client.models import V1EnvVar
from kubernetes import client, config
from typing import NamedTuple
from base64 import b64encode
import kfp.v2.dsl as dsl
import kubernetes
import json
import kfp
from google.cloud import aiplatform
import datetime
import pprint as pp
import requests

from step_1 import step_1_fn
from step_2 import step_2_fn

step_1_comp = kfp.v2.dsl.component(
    func=step_1_fn,
    base_image=&quot;library/python:3.10-slim-buster&quot;,
    packages_to_install=[
        &quot;dill&quot;,
    ],
)
step_2_comp = kfp.v2.dsl.component(
    func=step_2_fn,
    base_image=&quot;library/python:3.10-slim-buster&quot;,
    packages_to_install=[
        &quot;dill&quot;,
    ],
)


@kfp.dsl.pipeline(
    pipeline_root=&quot;gs://minimal_vertex_test_bucket&quot;,
    name=&quot;minimalcompile&quot;,
)
def root():
    step_1_exec = step_1_comp()
    step_2_exec = step_2_comp(input_context_path=step_1_exec.outputs[&quot;output_context_path&quot;])
</code></pre>
<pre class=""lang-py prettyprint-override""><code>import kfp
from kfp.v2.dsl import component, Artifact, Input, InputPath, Output, OutputPath, Dataset, Model
from typing import NamedTuple


def step_1_fn(
    output_context_path: OutputPath(str),
):
    import dill

    a = 10

    dill.dump_session(&quot;{output_context_path}&quot;)

</code></pre>
<pre class=""lang-py prettyprint-override""><code>import kfp
from kfp.v2.dsl import component, Artifact, Input, InputPath, Output, OutputPath, Dataset, Model
from typing import NamedTuple


def step_2_fn(
    input_context_path: InputPath(str),
    output_context_path: OutputPath(str),
    metadata_url: str = &quot;&quot;,
):
    from base64 import urlsafe_b64encode, urlsafe_b64decode
    import dill
    from pathlib import Path

    with Path(input_context_path).open(&quot;rb&quot;) as reader:
        input_context = reader.read()
    
    print(f&quot;A = {a}&quot;)

    dill.dump_session(&quot;{output_context_path}&quot;)
</code></pre>
<p>I use the following command to compile the above:</p>
<pre class=""lang-py prettyprint-override""><code>compiler.Compiler().compile(pipeline_func=root_module.root, package_path=&quot;.&quot;)
</code></pre>
<p>Which generates a correct <code>root.json</code> file.</p>
<p>However, when I upload this to Google Cloud Vertex, I get the following error at Step 2 -</p>
<pre><code>2022-06-13 16:41:34.796 PDT

workerpool0-0
[KFP Executor 2022-06-13 23:41:34,795 INFO]: Looking for component `step_2_fn` in --component_module_path `/tmp/tmp.cgPjtzQEt1/ephemeral_component.py`
2022-06-13 16:41:34.796 PDT

workerpool0-0
[KFP Executor 2022-06-13 23:41:34,795 INFO]: Loading KFP component &quot;step_2_fn&quot; from /tmp/tmp.cgPjtzQEt1/ephemeral_component.py (directory &quot;/tmp/tmp.cgPjtzQEt1&quot; and module name &quot;ephemeral_component&quot;)
2022-06-13 16:41:34.798 PDT

workerpool0-0
Traceback (most recent call last):
2022-06-13 16:41:34.798 PDT

workerpool0-0
 File &quot;/usr/local/lib/python3.10/runpy.py&quot;, line 196, in _run_module_as_main
2022-06-13 16:41:34.798 PDT

workerpool0-0
 return _run_code(code, main_globals, None,
2022-06-13 16:41:34.798 PDT

workerpool0-0
 File &quot;/usr/local/lib/python3.10/runpy.py&quot;, line 86, in _run_code
2022-06-13 16:41:34.798 PDT

workerpool0-0
 exec(code, run_globals)
2022-06-13 16:41:34.798 PDT

workerpool0-0
 File &quot;/usr/local/lib/python3.10/site-packages/kfp/v2/components/executor_main.py&quot;, line 104, in &lt;module&gt;
2022-06-13 16:41:34.798 PDT

workerpool0-0
 executor_main()
2022-06-13 16:41:34.798 PDT

workerpool0-0
 File &quot;/usr/local/lib/python3.10/site-packages/kfp/v2/components/executor_main.py&quot;, line 100, in executor_main
2022-06-13 16:41:34.798 PDT

workerpool0-0
 executor.execute()
2022-06-13 16:41:34.798 PDT

workerpool0-0
 File &quot;/usr/local/lib/python3.10/site-packages/kfp/v2/components/executor.py&quot;, line 307, in execute
2022-06-13 16:41:34.798 PDT

workerpool0-0
 func_kwargs[k] = self._get_input_artifact_path(k)
2022-06-13 16:41:34.798 PDT

workerpool0-0
 File &quot;/usr/local/lib/python3.10/site-packages/kfp/v2/components/executor.py&quot;, line 116, in _get_input_artifact_path
2022-06-13 16:41:34.799 PDT

workerpool0-0
 raise ValueError(
2022-06-13 16:41:34.799 PDT

workerpool0-0
ValueError: Failed to get input artifact path for artifact name input_context_path
2022-06-13 16:41:52.005 PDT
</code></pre>
<p>However, when I look at the output_context_path value of step 1, I see this:</p>
<pre><code>--executor_input; {&quot;outputs&quot;:{&quot;outputFile&quot;:&quot;/gcs/minimal_vertex_test_bucket/669070936339/minimalcompile-20220613043819/step-1-fn_3079644658226167808/executor_output.json&quot;,&quot;parameters&quot;:{&quot;output_context_path&quot;:{&quot;outputFile&quot;:&quot;/gcs/minimal_vertex_test_bucket/669070936339/minimalcompile-20220613043819/step-1-fn_3079644658226167808/output_context_path&quot;}}}}; --function_to_execute; step_1_fn
</code></pre>
<p>The input path for step 2, it says this:</p>
<pre><code>--executor_input; {&quot;inputs&quot;:{&quot;parameterValues&quot;:{&quot;input_context_path&quot;:&quot;&quot;,&quot;metadata_url&quot;:&quot;&quot;},&quot;parameters&quot;:{&quot;input_context_path&quot;:{&quot;stringValue&quot;:&quot;&quot;},&quot;metadata_url&quot;:{&quot;stringValue&quot;:&quot;&quot;}}},&quot;outputs&quot;:{&quot;outputFile&quot;:&quot;/gcs/minimal_vertex_test_bucket/669070936339/minimalcompile-20220613043819/step-2-fn_-6143727378628608000/executor_output.json&quot;,&quot;parameters&quot;:{&quot;output_context_path&quot;:{&quot;outputFile&quot;:&quot;/gcs/minimal_vertex_test_bucket/669070936339/minimalcompile-20220613043819/step-2-fn_-6143727378628608000/output_context_path&quot;}}}}; --function_to_execute; step_2_fn
</code></pre>
<p>This would imply that the &quot;outputFile&quot; from step 1 is not being read as the input value in step 2. (output_context_path has a value, input_context_path in step 2 is empty).</p>
<p>So, the question is, is InputPath/OutputPath supported? If not, what's the standard practice for writing a large file from one step and importing that large file into the second step?</p>",1,0,2022-06-13 23:55:43.070000 UTC,,,0,google-cloud-platform|gcloud|kubeflow|google-cloud-vertex-ai|kubeflow-pipelines,209,2008-09-02 20:31:47.747000 UTC,2022-09-24 20:51:12.950000 UTC,United States,6463,528,2,761,,,,,,['google-cloud-vertex-ai']
Tensorflow Session management issue | When session managed by other TF module,"<p>I am  using an external module for tensorflow work, getting session management issue when try to use it in flask application from pickle file, can someone please help me</p>

<blockquote>
  <p>ValueError: Tensor(""TensorSliceDataset:0"", shape=(), dtype=variant)
  must be from the same graph as Tensor(""Iterator:0"", shape=(),
  dtype=resource).</p>
</blockquote>

<p>Sample code structure</p>

<pre><code>from io import StringIO
import flask
import pandas as pd
import tensorrec as tr
import tensorflow as tf


class ScoringService(object):
    model = None                
    model_path = 'path for pickle file'
    @classmethod
    def get_model(cls):
        if cls.model == None:
            cls.model = tr.TensorRec.load_model(model_path)

        return cls.model

    @classmethod
    def get_reco(cls, model,use_ft, ite_ft):
            tf.reset_default_graph()
            predictions = model.predict(use_ft, ite_ft)
        return predictions

    @classmethod
    def predict(cls, input):
        clf = cls.get_model()
        n_reco_test = cls.get_reco(clf, input,use_ft, ite_ft)
        return n_reco_test

# The flask app for serving predictions
app = flask.Flask(__name__)

@app.route('/invocations', methods=['POST'])
def transformation():
    data = None

    # Convert from CSV to pandas
    if flask.request.content_type == 'text/csv':
        data = flask.request.data.decode('utf-8')
        s = StringIO(data)
        data = pd.read_csv(s, header=None)
    else:
        return flask.Response(response='This predictor only supports CSV data', status=415, mimetype='text/plain')
    # Do the prediction
    predictions = ScoringService.predict(data)
    return flask.Response(response=predictions, status=200,mimetype='text/csv')
</code></pre>

<p>more detail about error </p>

<pre><code>""POST /invocations HTTP/1.1"" 500 291 ""-"" ""AHC/2.0""
File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py"", line 350, in _apply_op_helper
g = ops._get_graph_from_inputs(_Flatten(keywords.values()))
File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 5637, in _get_graph_from_inputs
_assert_same_graph(original_graph_element, graph_element)
File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 5573, in _assert_same_graph
original_item)
</code></pre>",0,8,2018-12-06 04:42:53.607000 UTC,,,0,python|amazon-web-services|tensorflow|amazon-sagemaker,61,2017-07-30 08:26:08.107000 UTC,2022-09-20 14:45:30.870000 UTC,"Delhi, India",1370,94,1,125,,,,,,['amazon-sagemaker']
Azure - prediction over time,"<p>I am currently studying machine learning and predictive analysis so thought a good way to start would be using Azure ML Studio.
I did the Car Price Tutorial quite successfully however I now want to do something different.
I thought about using Currency Data; Price and Volume to try and predict the price the next day.
Everything has gone smoothly as I copied the Car Price tutorial. However when I come to test it the test wants all the variables to predict the new price, but I don't have any of &quot;tomorrows&quot; data. All I want to do is type tomorrows date and it will predict the price using yesterdays and before price and volume data.
Could you help me please? I am sure it is a small amendment but not sure what!
Thank you
Sam</p>",1,0,2020-07-07 07:33:58.817000 UTC,,2020-07-07 08:31:12.470000 UTC,1,azure|azure-machine-learning-studio|azure-machine-learning-service|predictive,42,2020-07-07 07:32:13.893000 UTC,2020-07-09 07:37:38.643000 UTC,,11,0,0,3,,,,,,"['azure-machine-learning-studio', 'azure-machine-learning-service']"
"error creating SageMaker project in terraform because service catalog product ""does not exist or access was denied""","<p>I have a product with id: prod-xxxxxxxxxxxx. I have checked that it exists in aws service catalog. However, when I try to create an aws_sagemaker_project using terraform:</p>
<pre><code>resource &quot;aws_sagemaker_project&quot; &quot;test-project&quot; {
  project_name = &quot;test-project&quot;

  service_catalog_provisioning_details {
    product_id = &quot;prod-xxxxxxxxxxxx&quot;
  }
}
</code></pre>
<p>I get the error: &quot;error creating SageMaker project: ValidationException: Product prod-xxxxxxxxxxxx does not exist or access was denied&quot;. How do I ensure that I can access this product?</p>
<p>Do I need a launch constraint for this product, and to grant access to the portfolio to end users as described here: <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-projects-templates-custom.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-projects-templates-custom.html</a>?</p>",1,2,2022-06-15 00:03:27.917000 UTC,,2022-06-15 15:53:39.393000 UTC,0,amazon-web-services|terraform|amazon-sagemaker|aws-service-catalog,189,2022-05-25 20:48:45.307000 UTC,2022-09-15 14:23:04.530000 UTC,,35,25,0,26,,,,,,['amazon-sagemaker']
"MemoryError in ""to_sql"" command in Jupyter Notebook","<p>I am working on a jupyter notebooks on AWS Sage Maker. I have performed text processing on a data with 5000 rows. I want to write this to another SQL query with the following code.</p>

<pre><code>conn=sqlite3.connect('final_2.sqlite')
c=conn.cursor()
conn.text_factory=str
final.to_sql('Reviews',conn,schema=None,if_exists='replace')
</code></pre>

<p>Its saving a 2.09 GB and its stopping. When I open this is file its not considering as file. Then I tried to write to a .csv file but still same issue.
When I downloaded and opened the csv I am getting the following error.</p>

<pre><code>Jupyter Notebook
current mode
File
Edit
View
Language
1
Error! Traceback (most recent call last):
2
  File ""/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/tornado/web.py"", line 1699, in _execute
3
    result = await result
4
  File ""/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/tornado/gen.py"", line 209, in wrapper
5
    yielded = next(result)
6
  File ""/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/notebook/services/contents/handlers.py"", line 112, in get
7
    path=path, type=type, format=format, content=content,
8
  File ""/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/notebook/services/contents/filemanager.py"", line 438, in get
9
    model = self._file_model(path, content=content, format=format)
10
  File ""/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/notebook/services/contents/filemanager.py"", line 365, in _file_model
11
    content, format = self._read_file(os_path, format)
12
  File ""/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/notebook/services/contents/fileio.py"", line 309, in _read_file
13
    bcontent = f.read()
14
MemoryError
15
​
16
Saving disabled.
17
See Console for more details.
</code></pre>

<p>I tried checking my free space in python and there is still about 30 GB free space.</p>

<p>Can someone please let me know what is the issue with this situation.Thank you !</p>",1,2,2019-11-30 05:17:34.947000 UTC,,2019-11-30 06:50:55.457000 UTC,0,python|amazon-web-services|amazon-ec2|jupyter-notebook|amazon-sagemaker,413,2019-08-10 10:11:07.937000 UTC,2021-09-28 15:47:43.140000 UTC,,137,8,0,63,,,,,,['amazon-sagemaker']
SageMaker Studio Lab - XGBoost Algortim - Am I Reading the Right Documentation?,"<p>Google has answers for SageMaker Studio, but I am at a loss for an answer on SageMaker Studio LAB...</p>
<p>I am reading the following on XGBoost - Am I in the right place for SM Studio LAB?</p>
<p><a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html</a></p>
<p><strong>Is there a better way for me to use XGBoost in LAB, vs what I am doing below by reading the doc above?</strong></p>
<p>In SageMaker Studio I would do the following to get the ECR container for the XGBoost algorithm:</p>
<pre><code>from sagemaker import image_uris
container = image_uris.retrieve('xgboost', boto3.Session().region_name, '1')
</code></pre>
<p>I made it a bit farther using the github example:</p>
<p><a href=""https://github.com/aws/studio-lab-examples/blob/main/connect-to-aws/Access_AWS_from_Studio_Lab.ipynb"" rel=""nofollow noreferrer"">https://github.com/aws/studio-lab-examples/blob/main/connect-to-aws/Access_AWS_from_Studio_Lab.ipynb</a></p>
<p>This works:</p>
<pre><code>from sagemaker import image_uris
from sagemaker.xgboost import XGBoost

# Create a training job name
job_name = 'ufo-xgboost-job-{}'.format(datetime.now().strftime(&quot;%Y%m%d%H%M%S&quot;))
print('Here is the job name {}'.format(job_name))

import sagemaker
import boto3
from sagemaker import image_uris
from sagemaker.session import Session
from sagemaker.inputs import TrainingInput
</code></pre>
<p>But this is giving me trouble:</p>
<pre><code>sess = sagemaker.Session()

xgb = sagemaker.estimator.Estimator(container,
                                    role, 
                                    instance_count=1, 
                                    instance_type='ml.m4.xlarge',
                                    output_path='model.tar.gz',
                                    sagemaker_session=sess)

xgb.set_hyperparameters(objective='multi:softmax',
                        num_class=3,
                        num_round=100)

data_channels = {
    'train': s3_input_train,
    'validation': s3_input_validation
}
xgb.fit(data_channels, job_name=job_name) 
</code></pre>
<p>With the following errors:</p>
<pre><code>ParsingError                              Traceback (most recent call last)
~/.conda/envs/default/lib/python3.9/site-packages/botocore/configloader.py in raw_config_parse(config_filename, parse_subsections)
    148         try:
--&gt; 149             cp.read([path])
    150         except (six.moves.configparser.Error, UnicodeDecodeError):

~/.conda/envs/default/lib/python3.9/configparser.py in read(self, filenames, encoding)
    696                 with open(filename, encoding=encoding) as fp:
--&gt; 697                     self._read(fp, filename)
    698             except OSError:

~/.conda/envs/default/lib/python3.9/configparser.py in _read(self, fp, fpname)
   1115         if e:
-&gt; 1116             raise e
   1117 

ParsingError: Source contains parsing errors: '/home/studio-lab-user/.aws/config'
    [line  5]: 'from sagemaker import image_uris\n'
    [line  6]: 'import boto3\n'

During handling of the above exception, another exception occurred:
</code></pre>
<p>truncated
error at bottom:</p>
<pre><code>ConfigParseError: Unable to parse config file: /home/studio-lab-user/.aws/config
</code></pre>",1,0,2022-01-20 03:58:37.550000 UTC,,2022-01-21 02:51:28.567000 UTC,0,xgboost|amazon-sagemaker|lab,93,2021-02-03 22:00:19.010000 UTC,2022-02-01 02:49:24.483000 UTC,"East Bay, CA, USA",1,0,0,3,,,,,,['amazon-sagemaker']
An issue with inference speed while using SageMaker Neo,"<p>I am a student studying Sage Maker Neo.
I am working on this tutorial.</p>
<p>Training and Serving with TensorFlow on Amazon SageMaker</p>
<p><a href=""https://github.com/aws/amazon-sagemaker-examples/blob/master/aws_sagemaker_studio/frameworks/tensorflow_mnist/tensorflow_mnist.ipynb"" rel=""nofollow noreferrer"">https://github.com/aws/amazon-sagemaker-examples/blob/master/aws_sagemaker_studio/frameworks/tensorflow_mnist/tensorflow_mnist.ipynb</a></p>
<p>What I'm curious about is that the inference speed is similar when using the c5 instance and when using the p2 instance.
Please let me know what I am missing.</p>",1,1,2021-12-15 06:04:25.040000 UTC,,2021-12-15 15:28:13.587000 UTC,0,amazon-sagemaker,41,2021-12-15 06:01:21.413000 UTC,2022-05-25 14:13:48.157000 UTC,,1,0,0,2,,,,,,['amazon-sagemaker']
"How to define current time zone in Azure ML for strptime function, unknown timezone 'localtime'","<p>All of these dates that I’ve manipulated in Execute R module in Azure Machine Learning write out as blank in the output – that is, these date columns exist, but there is no value in those columns. </p>

<p>The source variables which contain date information that I’m reading into the data frame have two different date formats.  They are as follows:</p>

<pre><code>usage$Date1=c(‘8/6/2015’   ‘8/20/2015’  ‘7/9/2015’)
usage$Date2=c(‘4/16/2015 0:00’,  ‘7/1/2015 0:00’, ‘7/1/2015 0:00’) 
</code></pre>

<p>I inspected the log file in AML, and AML can't find the local time zone. 
The log file warnings specifically:
    [ModuleOutput] 1: In strptime(x, format, tz = tz) :
    [ModuleOutput] unable to identify current timezone 'C':
    [ModuleOutput] please set environment variable 'TZ' [ModuleOutput]
    [ModuleOutput] 2: In strptime(x, format, tz = tz) : unknown timezone 'localtime'</p>

<p>I referred to another answer regarding setting default time zone for strptime here</p>

<p><a href=""https://stackoverflow.com/questions/4047188/unknown-timezone-name-in-r-strptime-as-posixct"">unknown timezone name in R strptime/as.POSIXct</a></p>

<p>I changed my code to explicitly define the global environment time variable.</p>

<pre><code>Sys.setenv(TZ='GMT')


####Data frame usage cleanup, format and labeling
usage&lt;-as.data.frame(usage)
usage$Date1&lt;-as.character(usage$Date1)
usage$Date1&lt;-as.POSIXct(usage$Date1, ""%m/%d/%Y"",tz=""GMT"")
usage$Date1&lt;-format(usage$Date1, ""%m/%d/%Y"")
usage$Date1&lt;-as.Date(usage$Date1, ""%m/%d/%Y"")
usage&lt;-as.data.frame(usage)

usage$Date2&lt;- as.POSIXct(usage$Date2, ""%m/%d/%Y"",tz=""GMT"")
usage$Date2&lt;- format(usage$Date2,""%m/%d/%Y"")
usage$Date2&lt;-as.Date(usage$Date2, ""%m/%d/%Y"")
usage&lt;-as.data.frame(usage)
</code></pre>

<p>The problem persists -as a result AzureML does not write these variables out, rather writing out these columns as blanks.<br>
(This code works in R studio, where I presume the local time is taken from the system.)</p>

<p>After reading two blog posts on this problem, it seems that Azure ML doesn't support some date time formats:</p>

<p><a href=""http://blogs.msdn.com/b/andreasderuiter/archive/2015/02/03/troubleshooting-error-1000-rpackage-library-exception-failed-to-convert-robject-to-dataset-when-running-r-scripts-in-azure-ml.aspx"" rel=""nofollow noreferrer"">http://blogs.msdn.com/b/andreasderuiter/archive/2015/02/03/troubleshooting-error-1000-rpackage-library-exception-failed-to-convert-robject-to-dataset-when-running-r-scripts-in-azure-ml.aspx</a></p>

<p><a href=""http://www.mikelanzetta.com/2015/01/data-cleaning-with-azureml-and-r-dates/"" rel=""nofollow noreferrer"">http://www.mikelanzetta.com/2015/01/data-cleaning-with-azureml-and-r-dates/</a></p>

<p>So I tried to convert to POSIXct before sending it to the output stream, which I've done as follows:
    tenantusage$Date1 = as.POSIXct(tenantusage$Date1 , ""%m/%d/%Y"",tz = ""EST5EDT"");
    tenantusage$Date2 = as.POSIXct(tenantusage$Date2 , ""%m/%d/%Y"",tz = ""EST5EDT"");</p>

<p>But encounter the same problem.  The information in these variables refuses to write out to the output.  Date1 and Date2 columns are blank.</p>

<p>Please advise!</p>

<p>thanks</p>",1,1,2015-10-29 00:43:41.807000 UTC,1.0,2017-05-23 12:13:57.147000 UTC,7,r|datetime|azure-machine-learning-studio,1738,2015-10-29 00:28:31.507000 UTC,2017-05-24 18:50:05.633000 UTC,,71,3,0,6,,,,,,['azure-machine-learning-studio']
pythonnet on Sagemaker cannot import clr,"<p>I cannot import clr from SageMaker with this error.
What kind of tweaks make this successful ?</p>
<p>From Sagemaker console :</p>
<pre><code>source activate python3    
conda install pythonnet
</code></pre>
<p>This seems to finely install pythonnet without error.</p>
<p>In python code :</p>
<pre><code>import clr
</code></pre>
<p>This produce this error :</p>
<pre><code>Traceback (most recent call last):
  File &quot;test_clr.py&quot;, line 1, in &lt;module&gt;
    import clr
ImportError: System.TypeInitializationException: The type initializer for 'Sys' threw an exception. ---&gt; System.DllNotFoundException: /home/ec2-user/anaconda3/envs/python3/lib/../lib/libmono-native.so assembly:&lt;unknown assembly&gt; type:&lt;unknown type&gt; member:(null)
  at (wrapper managed-to-native) Interop+Sys.LChflagsCanSetHiddenFlag()
  at Interop+Sys..cctor () [0x00000] in &lt;aa5dff9b31c64fce86559bbbf6cd364f&gt;:0 
   --- End of inner exception stack trace ---
  at Interop.GetRandomBytes (System.Byte* buffer, System.Int32 length) [0x00000] in &lt;aa5dff9b31c64fce86559bbbf6cd364f&gt;:0 
  at System.Guid.FastNewGuidArray () [0x00020] in &lt;aa5dff9b31c64fce86559bbbf6cd364f&gt;:0 
  at System.Reflection.Emit.ModuleBuilder..ctor (System.Reflection.Emit.AssemblyBuilder assb, System.String name, System.String fullyqname, System.Boolean emitSymbolInfo, System.Boolean transient) [0x00035] in &lt;aa5dff9b31c64fce86559bbbf6cd364f&gt;:0 
  at System.Reflection.Emit.AssemblyBuilder.DefineDynamicModule (System.String name, System.String fileName, System.Boolean emitSymbolInfo, System.Boolean transient) [0x0005b] in &lt;aa5dff9b31c64fce86559bbbf6cd364f&gt;:0 
  at System.Reflection.Emit.AssemblyBuilder.DefineDynamicModule (System.String name) [0x00000] in &lt;aa5dff9b31c64fce86559bbbf6cd364f&gt;:0 
  at Python.Runtime.CodeGenerator..ctor () [0x0002b] in &lt;0e10ac2b10a44c1baa160aa337220b6a&gt;:0 
  at Python.Runtime.DelegateManager..ctor () [0x00061] in &lt;0e10ac2b10a44c1baa160aa337220b6a&gt;:0 
  at Python.Runtime.PythonEngine.Initialize (System.Collections.Generic.IEnumerable`1[T] args, System.Boolean setSysArgv, System.Boolean initSigs) [0x0000a] in &lt;0e10ac2b10a44c1baa160aa337220b6a&gt;:0 
  at Python.Runtime.PythonEngine.Initialize (System.Boolean setSysArgv, System.Boolean initSigs) [0x00005] in &lt;0e10ac2b10a44c1baa160aa337220b6a&gt;:0 
  at Python.Runtime.PythonEngine.InitExt () [0x00000] in &lt;0e10ac2b10a44c1baa160aa337220b6a&gt;:0 
</code></pre>
<p>Environment :</p>
<ul>
<li>Amazon Linux AMI release 2018.03</li>
<li>Mono JIT compiler version 6.12.0.90 (tarball Fri Mar  5 04:37:13 UTC 2021)</li>
<li>Python 3.6.13</li>
<li>conda 4.8.4</li>
<li>pythonnet 2.4.0</li>
</ul>
<p>Similar symptoms :</p>
<p><a href=""https://github.com/pythonnet/pythonnet/issues/1034"" rel=""nofollow noreferrer"">System.DllNotFoundException when trying to import clr, despite referenced assembly existing #1034</a> : I confirmed libmono-native.so library there in the right directory.</p>",0,0,2021-04-22 11:33:42.547000 UTC,,,1,mono|amazon-sagemaker|python.net,288,2012-10-05 12:17:18.773000 UTC,2022-08-10 05:13:32.573000 UTC,Tokyo,81,85,0,15,,,,,,['amazon-sagemaker']
Can I use Pillow and numpy in a AWS Lambda function?,"<p>I'm trying to use a lambda function in order to do a sagemaker prediction with an image and I need to transform from </p>

<pre><code>s3 = boto3.resource('s3') 
bucket = s3.Bucket(bucket)
object = bucket.Object(key)
response = object.get()
file_stream = response['Body']
</code></pre>

<p>to a proper format to use in </p>

<pre><code>response = client.invoke_endpoint(EndpointName=endpoint_name, Body=json.dumps(data))
</code></pre>

<p>I did some test on a jupyter notebook and I found this commands do the job:</p>

<pre><code>im = Image.open(file_stream)
im_array = np.array(im)
data = img_data.tolist()
</code></pre>

<p>but is required PIL Image and NumPy. </p>

<p>Is there any chance to import these libraries in a lambda function or any other approach?</p>

<p>Thank you,
Samuele.</p>",1,1,2020-05-21 16:00:05.353000 UTC,,2020-05-21 23:14:12.970000 UTC,1,python|amazon-web-services|aws-lambda|amazon-sagemaker,763,2013-08-21 09:45:58.527000 UTC,2020-06-03 16:29:01.123000 UTC,,11,0,0,7,,,,,,['amazon-sagemaker']
Vertex AI Endpoint creation failes,"<p>When I try to deploy a google auto ml trained model to a google vertex ai endpoint (which worked 3 days ago) I suddely get an error message:</p>
<p>Hello Vertex AI Customer,</p>
<p>Due to an error, Vertex AI was unable to create endpoint &quot;test2&quot;.
Additional Details:
Operation State: Failed with errors
Resource Name:
projects//######/locations/europe-west4/endpoints//#####
Error Messages: Machine type temporarily unavailable, please deploy with a
different machine type or retry</p>
<p>To view the error on Cloud Console, go back to your endpoint using
<a href=""https://console.cloud.google.com/vertex-ai/locations/europe-west4/models//#####/versions/1/deploy?authuser=1&amp;folder=&amp;organizationId=&amp;project=/######"" rel=""nofollow noreferrer"">https://console.cloud.google.com/vertex-ai/locations/europe-west4/models//#####/versions/1/deploy?authuser=1&amp;folder=&amp;organizationId=&amp;project=/######</a></p>
<p>Sincerely,
The Google Cloud AI TeamHello Vertex AI Customer,</p>
<p>Does anyone have an idea why the endpoint deployment suddely does not work anymore?</p>",0,1,2022-05-30 19:25:23.073000 UTC,,,0,google-cloud-platform|google-cloud-vertex-ai,217,2022-05-30 19:20:29.847000 UTC,2022-08-04 16:31:25.180000 UTC,,1,0,0,0,,,,,,['google-cloud-vertex-ai']
Train model after load in MLFlow,"<p>My goal is to store an empty model into MLFlow Registry and then load it for training.</p>
<p>I have a register_model.py which looks like this:</p>
<pre><code>if __name__ == &quot;__main__&quot;:

    remote_server_uri = &quot;http://127.0.0.1:5000&quot;
    mlflow.set_tracking_uri(remote_server_uri)

    # Load and compile Keras model
    model = tf.keras.applications.MobileNetV2((32, 32, 3), classes=10, weights=None)
    model.compile(&quot;adam&quot;, &quot;sparse_categorical_crossentropy&quot;, metrics=[&quot;accuracy&quot;])


    # Load CIFAR-10 dataset
    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()

    epochs = 1
    batch_size = 32
    mlflow.tensorflow.autolog()
    model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size)

    tf.keras.models.save_model(model, &quot;models/tensorflow&quot;)

    mlflow.tensorflow.log_model(
        tf_saved_model_dir='models/tensorflow',
        tf_meta_graph_tags=None,
        tf_signature_def_key='serving_default',
        artifact_path=&quot;saved/models/tensorflow&quot;,
        registered_model_name=&quot;tensorflow-MobileNetV2-32inputs&quot;
    )
</code></pre>
<p>Then I'm trying to load it using:</p>
<pre><code>if __name__ == &quot;__main__&quot;:

    remote_server_uri = &quot;http://127.0.0.1:5000&quot;
    mlflow.set_tracking_uri(remote_server_uri)


    model_name = &quot;tensorflow-MobileNetV2-32inputs&quot;
    model_version = 1

    model = mlflow.tensorflow.load_model(
        model_uri=f&quot;models:/{model_name}/{model_version}&quot;
    )
</code></pre>
<p>I would expect my model to be a able to do things like 'model.fit()' and 'model.predict()' but Im always getting:</p>
<pre><code>AttributeError: '_WrapperFunction' object has no attribute 'fit'
</code></pre>
<p>So my question is: it is possible to save a tensorflow/keras model and to load the architecture to be trained/retrained and even modified through mlflow? Load the model, add a new layer, store the model either as a new version or as a new model itself, for example.</p>
<p>Thanks in advance!</p>",0,0,2022-03-24 11:27:48.757000 UTC,,,0,python|tensorflow|mlflow,116,2012-05-19 11:47:02.397000 UTC,2022-09-20 08:15:38.867000 UTC,"Málaga, Spain",1176,61,7,80,,,,,,['mlflow']
"How can I run Google Cloud's ""AI Notebooks"" on a schedule automatically?","<p>Notebooks in the Google Cloud Platform has been great for Python development in the cloud, but the last missing piece is just running existing notebooks on a schedule. There's a million different tools (Airflow, Papermill, Google Cloud Jobs, Google Cloud Scheduler, Google Cloud Cron Jobs), and as someone not as familiar with Cloud, it's really easy to get lost. Any suggestion? Thanks guys!</p>",3,0,2021-07-29 02:25:24.810000 UTC,,2022-06-03 08:39:21.700000 UTC,3,python|google-cloud-platform|google-cloud-vertex-ai,1317,2018-04-09 01:06:01.717000 UTC,2022-01-21 07:39:34.450000 UTC,,143,8,0,36,,,,,,['google-cloud-vertex-ai']
RuntimeError: as_numpy_iterator() is not supported while tracing functions on tensorflow 2.x dataset on sagemaker,"<p>I am preparing a <a href=""https://pypi.org/project/sagemaker-tensorflow/"" rel=""noreferrer"">sagemaker PIPE mode dataset</a> to train a time series model on <code>SageMaker</code> with <code>PIPE</code> mode. The <code>PipeModeDataset</code> is a <code>TensorFlow Dataset</code> for reading <code>SageMaker</code> Pipe Mode channels. I am using an augmented manifest file which contains image location on <code>S3</code> and the label each line. My model accept batches of images (512 x 512 x 1) with single label per batch as input. I thought of using the window function to bundle the images read from pipe. Please refer to following partial code for dataset generation.</p>
<pre><code>def _input_fn(channel):
    &quot;&quot;&quot;Returns a Dataset for reading from a SageMaker PipeMode channel.&quot;&quot;&quot;
    features = {
        'image-ref': tf.io.FixedLenFeature([], tf.string),
        'label': tf.io.FixedLenFeature([3], tf.int64),
    }
    
    def parse(record):
        parsed = tf.io.parse_single_example(record, features)
        image = tf.io.decode_png(parsed['image-ref'], channels=1, dtype=tf.uint8)
        image = tf.reshape(image, [512, 512, 1])
        label = parsed['label']
        return (image, label)

    ds = PipeModeDataset(channel, record_format='TFRecord', benchmark=True, benchmark_records_interval=100)
    ds = ds.map(parse)
    
    print (&quot;PipeModeDataset print0 = &quot; + str(ds))
    ds = ds.window(16, shift=1, drop_remainder=True)
    print (&quot;PipeModeDataset print1 = &quot; + str(ds))
    
    def window_func(window, label):
        window = window.batch(16, drop_remainder=True)
        label = label.batch(16, drop_remainder=True)
        
        print (&quot;window batch is = &quot; + str(window))
        print (&quot;label batch is = &quot; + str(label))
        
        window_np = np.stack(list(window.as_numpy_iterator()))
        label_np = np.stack(list(label.as_numpy_iterator())) # TODO: only get the last label
        
        return tf.data.Dataset.from_tensor_slices((window_np, label_np))
    
    ds = ds.flat_map(lambda window, label: window_func(window, label))
    ....
    ....
</code></pre>
<p>Getting following error at the moment. How to fix this? Recommend better ways if there is any.</p>
<pre><code>PipeModeDataset print0 = &lt;MapDataset shapes: ((512, 512, 1), (3,)), types: (tf.uint8, tf.int64)&gt;
PipeModeDataset print1 = &lt;WindowDataset shapes: (DatasetSpec(TensorSpec(shape=(512, 512, 1), dtype=tf.uint8, name=None), TensorShape([])), DatasetSpec(TensorSpec(shape=(3,), dtype=tf.int64, name=None), TensorShape([]))), types: (DatasetSpec(TensorSpec(shape=(512, 512, 1), dtype=tf.uint8, name=None), TensorShape([])), DatasetSpec(TensorSpec(shape=(3,), dtype=tf.int64, name=None), TensorShape([])))&gt;
window batch is = &lt;BatchDataset shapes: (16, 512, 512, 1), types: tf.uint8&gt;
label batch is = &lt;BatchDataset shapes: (16, 3), types: tf.int64&gt;

RuntimeError: in user code:

    /opt/ml/code/train_on_pipemode.py:104 None  *
        ds = ds.flat_map(lambda window, label: window_func(window, label))
    /opt/ml/code/train_on_pipemode.py:96 window_func  *
        window_np = np.stack(list(window.as_numpy_iterator()))
    /usr/local/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py:518 as_numpy_iterator  **
        raise RuntimeError(&quot;as_numpy_iterator() is not supported while tracing &quot;

    RuntimeError: as_numpy_iterator() is not supported while tracing functions
</code></pre>
<p><a href=""https://stackoverflow.com/questions/60045971/runtimeerror-as-numpy-iterator-is-not-supported-while-tracing-functions"">This answer</a> says to enable eager execution, but it is enabled in my case when I printed <code>tf.executing_eagerly()</code>. I am training on <code>tensorflow 2.x</code>.</p>
<pre><code>Tensorflow version: 2.3.1
Eager execution: True
</code></pre>",0,0,2020-12-02 18:08:30.877000 UTC,,,5,tensorflow|runtime-error|tensorflow2.0|tensorflow-datasets|amazon-sagemaker,322,2014-02-28 08:27:04.047000 UTC,2022-09-15 13:51:41.977000 UTC,Sri Lanka,879,1666,6,138,,,,,,['amazon-sagemaker']
Set up environment varialbe during deployment for Azure Container Instance,"<p>I am deploying my machine learning model to an azure container instance through a pipeline using the code snipplet below:</p>
<pre><code>from azureml.core.webservice import AciWebservice

deployment_config = AciWebservice.deploy_configuration(
    cpu_cores=0.5, memory_gb=1, auth_enabled=True
)
</code></pre>
<p>And the problem is that I have a secret environment which i want to use in <code>score.py</code> but since I cannot pass this value or cannot set an environment variable, i am unable to use it.</p>
<p>How to overcome this issue?</p>",1,0,2021-08-09 12:32:16.007000 UTC,,,0,azure-devops|azure-machine-learning-studio|azure-container-instances,257,2021-03-24 10:00:57.473000 UTC,2022-09-10 14:40:32.643000 UTC,,21,0,0,9,,,,,,['azure-machine-learning-studio']
AzureML Authentication for registering datasets using azureml.core.dataset,"<p>I am trying to register a data set programmatically using azurecli authentication</p>
<h2>What I tried</h2>
<pre><code>authentication = AzureCliAuthentication()

workspace = Workspace.from_config( &quot;config.json&quot;), auth=authentication)
store = Datastore.get(workspace, datastore_name)
path = [(store, filePath)]
dataset = Dataset.Tabular.from_delimited_files(path=path)

</code></pre>
<p>I am logged in using <code>azure-cli</code></p>
<pre><code>az login
</code></pre>
<p>I am currently the owner of the datastore_name which is a gen2 datalake instance in the same subscription / region as the Azure ML workspace</p>
<h2>Question</h2>
<ul>
<li><p><strong>I get an interactive login</strong>  each time I get to the <code>Dataset.Tabular.from_</code> line. How can make it use the azure cli creds ?</p>
</li>
<li><p>I plan to use the same python script as part of CI/CD pipeline azurecli task to register the datasets across multiple workspaces</p>
</li>
</ul>",2,0,2022-07-06 14:12:46.073000 UTC,,2022-07-06 14:18:15.970000 UTC,2,azure-machine-learning-service|azureml-python-sdk,432,2010-04-12 17:27:26.887000 UTC,2022-09-24 02:56:17.550000 UTC,United States,9826,1723,15,1238,,,,,,['azure-machine-learning-service']
MLFlow trying to launch non-existent python,"<p>Launching <code>mlflow ui</code> from a brand new conda environment, called, say B, results in:</p>
<blockquote>
<p>Fatal error in launcher: Unable to create process using '&quot;C:\Users\user_name\anaconda3\envs\A\python.exe&quot; etc</p>
</blockquote>
<p>MLFLow was orgiinally installed in the environment A, but it no longer exists! Where can I find this setting and how should I fix it?</p>",0,0,2022-05-15 00:43:28.793000 UTC,,,0,python-3.x|mlflow,16,2012-09-20 20:09:44.713000 UTC,2022-09-05 01:40:27.663000 UTC,,93,10,0,14,,,,,,['mlflow']
Send data and get result from Azure Machine Learning Studio by Ionic mobile app,"<p>I have a classification model in Azure Machine Learning Studio. I need to send data to this model and get the predicted result by Ionic mobile app.
I have model API. I need request and response sample code.</p>",1,0,2019-04-22 06:42:28.760000 UTC,,2019-04-22 06:51:26.127000 UTC,1,ionic-framework|azure-mobile-services|azure-machine-learning-studio,95,2016-11-28 21:11:01.947000 UTC,2022-08-19 08:15:13.457000 UTC,,15,0,0,5,,,,,,['azure-machine-learning-studio']
Sagemaker - additional endpoint,"<p>I created my own model with scikit learn. Apart from standard endpoints, '/ping' and '/invocations' I added a third one, '/estimates' (it calls predict_probabilities()). </p>

<p>When I run locally container all endpoints work correctly. When I deploy this as endpoint on Sagemaker '/invocations' works correctly. With the same auth token (generated by Postman) calling '/estimates' gives me an error:</p>

<pre><code>&lt;AccessDeniedException&gt;
  &lt;Message&gt;Unable to determine service/operation name to be authorized&lt;/Message&gt;
&lt;/AccessDeniedException&gt;
</code></pre>

<p>I can't even see attempts to call '/estimates' in cloudwatch. 
Is there any way to create 3rd endpoint or should I create another container for this and serve probabilities from '/invocations'? </p>",2,0,2019-01-28 13:41:58.010000 UTC,,,1,python|amazon-web-services|docker|scikit-learn|amazon-sagemaker,576,2014-08-11 20:32:11.150000 UTC,2022-09-23 14:14:27.703000 UTC,Den Haag,1039,41,0,104,,,,,,['amazon-sagemaker']
No space left on device [Amazon SageMaker],"<p>I am working on training my model in P2.xlarge instance. When I download a dataset, I get the following error: ""Exception during downloading or extracting: [Errno 28] No space left on device""\
I checked that P2.xlarge has 61GiB storage, translating to 64GB. I hardly have 5GB worth of data in my instance. Could you please let me know how to proceed?</p>",3,1,2018-07-12 03:18:08.853000 UTC,2.0,,6,amazon-sagemaker,11293,2015-10-29 09:52:03.787000 UTC,2018-12-14 20:10:25.873000 UTC,,91,0,0,9,,,,,,['amazon-sagemaker']
"Lifetime cycle config is longer than 5 minutes, nohup command not working","<p>I am trying to install a number of dependencies for my jupyter notebook, but would like them to be permanent to save me 20 minutes every time I restart the notebook. I have opted for using a lifecycle configuration, however, my script takes longer than 5 minutes to run. I found this article (<a href=""https://aws.amazon.com/premiumsupport/knowledge-center/sagemaker-lifecycle-script-timeout/"" rel=""nofollow noreferrer"">https://aws.amazon.com/premiumsupport/knowledge-center/sagemaker-lifecycle-script-timeout/</a>) to help resolve this problem, however, my notebook is still failing to start with the following error:</p>
<pre><code>Notebook Instance Lifecycle Config 'arn:aws:sagemaker:eu-west-2:347285168835:notebook-instance-lifecycle-config/nbs-aap-dev-dsar' for Notebook Instance 'arn:aws:sagemaker:eu-west-2:347285168835:notebook-instance/nbs-aap-dev-dsar' took longer than 5 minutes. Please check your CloudWatch logs for more details if your Notebook Instance has Internet access.
</code></pre>
<p>Here is the script I am trying to run:</p>
<pre><code>sudo nohup yum install wget &amp;
sudo yum install autoconf &amp;
sudo yum install automake &amp;
sudo yum install libtool &amp;
sudo yum install jpeg &amp;
sudo yum install tiff &amp;
sudo yum install libpng &amp;
sudo yum install tiff2png &amp;
sudo yum install libtiff &amp;
sudo yum install autoconf aclocal automake &amp;
sudo yum install libtool &amp;
sudo yum -y install libjpeg-devel libpng-devel libpng-devel libtiff-devel zlib-devel &amp;
sudo yum install gcc gcc-c++ make &amp;
sudo wget https://github.com/DanBloomberg/leptonica/releases/download/1.82.0/leptonica-1.82.0.tar.gz &amp;
sudo tar xzvf leptonica-1.82.0.tar.gz &amp;
cd leptonica-1.82.0 &amp;
sudo ./configure --prefix=/usr/local/ &amp;
sudo make &amp;
sudo make install &amp;
sudo wget https://codeload.github.com/tesseract-ocr/tesseract/tar.gz/4.1.1 &amp;
sudo tar -zxvf 4.1.1 &amp;
cd tesseract-4.1.1 &amp;
sudo ./autogen.sh &amp;
sudo cp /home/ec2-user/leptonica-1.82.0/lept.pc /usr/lib64/pkgconfig/. &amp;
sudo LIBLEPT_HEADERSDIR=/usr/local/lib ./configure --prefix=/usr/local/ --with-extra-libraries=/usr/local/lib &amp;
sudo make &amp;
sudo make install &amp;
export LD_LIBRARY_PATH=/usr/local/lib &amp;
sudo ldconfig &amp;
sudo wget https://github.com/tesseract-ocr/tessdata_best/raw/main/eng.traineddata &amp;
sudo mv -v eng.traineddata /usr/local/share/tessdata/eng.traineddata &amp;
sudo wget https://github.com/ArtifexSoftware/ghostpdl-downloads/releases/download/gs9550/ghostpdl-9.55.0.tar.gz &amp;
sudo tar -zxvf ghostpdl-9.55.0.tar.gz &amp;
cd ghostpdl-9.55.0 &amp;
sudo ./configure --prefix=/usr/local/ &amp;
sudo make &amp;
sudo make install &amp;
sudo yum -y install poppler-utils &amp;
sudo wget https://github.com/qpdf/qpdf/releases/download/release-qpdf-10.1.0/qpdf-10.1.0.tar.gz &amp;
sudo tar xzvf qpdf-10.1.0.tar.gz &amp;
cd qpdf-10.1.0 &amp;
sudo ./configure --prefix=/usr/local/ &amp;
sudo make &amp;
sudo make install
</code></pre>",1,0,2021-12-07 11:13:36.513000 UTC,,2021-12-07 11:57:18.030000 UTC,3,amazon-web-services|jupyter-notebook|amazon-sagemaker|yum,739,2021-07-01 11:02:10.027000 UTC,2022-08-24 14:07:05.850000 UTC,,33,0,0,5,,,,,,['amazon-sagemaker']
AWS SageMaker training script: how to pass custom user parameters,"<p>I am training a classifier using Scikit-learn with the SageMaker Python SDK.<br>
The overall process involves three sequential phases:  </p>

<ol>
<li>hyper-parameters tuning job using training and validation data sets   </li>
<li>training job with the best hyper-parameters founded in 1. and the
whole data set (training + validation from 1.)  </li>
<li>training a calibrated model with the provided the ""prefit"" model from 2. and an additional data set for calibration.</li>
</ol>

<p>The reason I need to split the process is to save the un-calibrated model created at step 2.</p>

<p>For each of this step I prepare a training script as explained in: <a href=""https://sagemaker.readthedocs.io/en/stable/using_sklearn.html#prepare-a-scikit-learn-training-script"" rel=""nofollow noreferrer"">https://sagemaker.readthedocs.io/en/stable/using_sklearn.html#prepare-a-scikit-learn-training-script</a></p>

<p>The three scripts are very similar and to avoid code redundancy I would like to use a single script with additional logic inside for the three situation. More precisely, I would like to pass additional custom parameters to the <code>.fit</code> methods of the <code>sagemaker.tuner.HyperparameterTuner</code> and <code>sagemaker.sklearn.estimator.SKLearn</code> objects in order to be able to action the logic in the script depending on the usage (phase 1. ,2. or 3.).   </p>

<p>I already tried by hacking the <code>SM_CHANNEL_XXX</code><br>
<code>parser.add_argument('--myparam', type=str, default=os.environ.get('SM_CHANNEL_MYPRAM'))</code>
while invoking <code>.fit(inputs={'train': ..., 'test': ..., 'myparam': myvalue})</code> but it expect a valid s3 URI.</p>

<p>Any idea on how to pass extra custom parameters to the training scripts? </p>",1,0,2020-01-08 15:40:29.577000 UTC,,,1,python|scikit-learn|amazon-sagemaker,2478,2014-07-30 10:25:22.230000 UTC,2022-09-22 19:20:13.617000 UTC,Belgium,1714,133,6,307,,,,,,['amazon-sagemaker']
InternalServerError: We encountered an internal error. Please try again,"<p>I encountered an error: &quot;Internal Server Error: We encountered an internal error. Please try again.&quot; when I run following script.</p>
<p><strong>This error occurs suddenly after completed some tasks. Suddenly.</strong></p>
<pre class=""lang-py prettyprint-override""><code>from sagemaker.network import NetworkConfig
from sagemaker.processing import ProcessingInput, ProcessingOutput, Processor

processor = Processor(
    role=****,
    image_uri=****,
    instance_count=1,
    instance_type=&quot;m5.large&quot;,
    network_config=NetworkConfig(security_group_ids=[****], subnets=[****])
)
processor.run(
    inputs=[
        ProcessingInput(***),
    ],
    outputs=[
        ProcessingOutput(
            source=&quot;****&quot;,
            destination=&quot;****&quot;,
            s3_upload_mode=&quot;Continuous&quot;,
        )
    ]
)
</code></pre>
<p>Stack trace is followings.</p>
<pre><code>  File &quot;run_sagemaker.py&quot;, line 44, in process2
    processor.run(
  File &quot;/home/lubuntu/.miniconda/envs/sagemaker/lib/python3.8/site-packages/sagemaker/processing.py&quot;, line 165, in run
    self.latest_job.wait(logs=logs)
  File &quot;/home/lubuntu/.miniconda/envs/sagemaker/lib/python3.8/site-packages/sagemaker/processing.py&quot;, line 731, in wait
    self.sagemaker_session.logs_for_processing_job(self.job_name, wait=True)
  File &quot;/home/lubuntu/.miniconda/envs/sagemaker/lib/python3.8/site-packages/sagemaker/session.py&quot;, line 3167, in logs_for_processing_job
    self._check_job_status(job_name, description, &quot;ProcessingJobStatus&quot;)
  File &quot;/home/lubuntu/.miniconda/envs/sagemaker/lib/python3.8/site-packages/sagemaker/session.py&quot;, line 2666, in _check_job_status
    raise exceptions.UnexpectedStatusException(
sagemaker.exceptions.UnexpectedStatusException: Error for Processing job ****: Failed. Reason: InternalServerError: We encountered an internal error.  Please try again.
</code></pre>
<p>But when I set s3_upload_mode=&quot;EndOfJob&quot;, this error didn't occur.</p>
<p>My pc environment is</p>
<ul>
<li>lubuntu 20.04 LTS (work on VMWare Workstation Player)</li>
<li>python 3.8.3</li>
<li>sagemaker 1.69.0</li>
<li>AWS region is Ohio (us-east-2)</li>
</ul>
<p>What is wrong for me?</p>
<p>Please lend me your wisdom.</p>",2,1,2020-07-19 04:37:54.070000 UTC,,,4,python|amazon-sagemaker,5438,2020-07-08 06:47:49.687000 UTC,2021-03-06 18:23:32.210000 UTC,,41,0,0,2,,,,,,['amazon-sagemaker']
How to create AzureML environment for an experiment from conda specification file,<p>I want to use my existing specification file of conda environment for creating AzureML experiment environment. How could we do that?</p>,1,0,2020-12-05 16:56:50.483000 UTC,,,-1,python|azure|conda|azure-machine-learning-service|azureml-python-sdk,163,2017-01-19 06:38:11.853000 UTC,2022-09-20 02:56:14.233000 UTC,"Gurugram, Haryana, India",623,29,1,54,,,,,,['azure-machine-learning-service']
How to run an Object Detection prediction on a MobileNet SSD model hosted on SageMaker,"<p>I trained a MobileNet v1 SSD model using AWS SageMaker and it trained perfectly.  I have an export/Servo/xxx/saved_graph (a tflite model derived from the checkpoints worked perfectly on an IoT device.)    The model deployed on SageMaker as an endpoint without a problem.   My problem is, I don't know how to feed images into the model endpoint.  (Which presumably is the same as asking how do I feed data into a served model?)   </p>

<p>When I query the model (below), I see the input signature is 'serialized_example'.   So, I'm assuming I need a serialized example.   That's where I run out of talent.  </p>

<p>I need help understanding how to convert jpg file into a serialized example.  Or how can I use a tfrecord.  </p>

<pre><code>saved_model_cli show --dir {MODEL_PATH} --tag_set serve --signature_def tensorflow/serving/predict
</code></pre>

<p>yields (I omitted the outputs):</p>

<pre><code>  inputs['serialized_example'] tensor_info:
      dtype: DT_STRING
      shape: ()
      name: tf_example:0
</code></pre>

<p>I have a Dataset but don't know how to get the right class -</p>

<pre><code>raw_dataset = tf.data.TFRecordDataset(tfrecord_file_list)
example_decoder = TfExampleDecoder()
decoded_dataset = raw_dataset.map(example_decoder.decode)
</code></pre>

<p>if I enumerate across a decoded dataset, how can I feed the data into my model?</p>

<pre><code>for n, e in enumerate(decoded_dataset.take(4)):
   what do I do with e - which looks like:

&lt;DatasetV1Adapter shapes: {image: (?, ?, 3), source_id: (), key: (), filename: (), groundtruth_boxes: (?, 4), groundtruth_area: (?,), groundtruth_is_crowd: (?,), groundtruth_difficult: (?,), groundtruth_group_of: (?,), groundtruth_weights: (?,), groundtruth_classes: (?,), groundtruth_image_classes: (?,), original_image_spatial_shape: (2,)}, types: {image: tf.uint8, source_id: tf.string, key: tf.string, filename: tf.string, groundtruth_boxes: tf.float32, groundtruth_area: tf.float32, groundtruth_is_crowd: tf.bool, groundtruth_difficult: tf.int64, groundtruth_group_of: tf.int64, groundtruth_weights: tf.float32, groundtruth_classes: tf.int64, groundtruth_image_classes: tf.int64, original_image_spatial_shape: tf.int32}&gt;
</code></pre>",0,1,2019-08-13 20:43:14.413000 UTC,1.0,,1,tensorflow|object-detection|amazon-sagemaker|mobilenet,429,2016-01-11 20:03:25.717000 UTC,2022-06-10 16:43:54.400000 UTC,,360,4,0,24,,,,,,['amazon-sagemaker']
Azure ML webservice columns returned no as expected,"<p>I have created an experiment to help categorise a description, this all works fine. However it does not tell me the weightings. When on the studio website I click test on the experiment and call the service I get back a JSON blob including lots of useful data such as the column names and weightings. When I actually use the web service from my app using C# the returned json does not include this information?</p>

<p>Any reason for this?</p>

<p>thanks
Andy</p>",1,0,2017-02-03 12:10:20.837000 UTC,,,1,azure-machine-learning-studio,94,2011-03-18 10:47:07.763000 UTC,2021-11-24 21:38:12.780000 UTC,"East Midlands, UK",1627,59,8,150,,,,,,['azure-machine-learning-studio']
POST request fails with large data send to model deployed on Azure Container,"<p><strong>Summary</strong></p>

<p>I have a PyTorch model deployed on an Azure Container instance via the Azure Machine Learning Service SDK. The model takes (large) images for classification in standard numpy formatting.</p>

<p>It seems, I'm hitting a HTTP request size limit on the server side. Requests to the model succeeds with PNG images of a size in the 8-9mb range and fails with images of the 15mb+ size. Specifically, it fails with 413 Request Entity Too Large.</p>

<p>I assume, the limit is set in Nginx in the Docker image being build, as part of the deployment process. My question: <em>Given that the issue is due to the HTTP request size limit, is there any way to increase this limit in the azureml API?</em></p>

<p><strong>Deployment process</strong></p>

<p>The deployment process succeeds as expected.</p>

<pre class=""lang-py prettyprint-override""><code>from azureml.core import Workspace
from azureml.core.model import InferenceConfig, Model
from azureml.core.webservice import AciWebservice, Webservice
from azureml.exceptions import WebserviceException
from pathlib import Path

PATH = Path('/data/home/azureuser/my_project')

ws = Workspace.from_config()
model = ws.models['my_pytorch_model']

inference_config = InferenceConfig(source_directory=PATH/'src',
                                   runtime='python',
                                   entry_script='deployment/scoring/scoring.py',
                                   conda_file='deployment/environment/env.yml')

deployment_config = AciWebservice.deploy_configuration(cpu_cores=2, memory_gb=4)
aci_service_name = 'azure-model'

try:
    service = Webservice(ws, name=aci_service_name)
    if service:
        service.delete()
except WebserviceException as e:
    print()

service = Model.deploy(ws, aci_service_name, [model], inference_config, deployment_config)

service.wait_for_deployment(True)
print(service.state)
</code></pre>

<p><strong>Testing via <code>requests</code></strong></p>

<p>A simple test using requests:</p>

<pre class=""lang-py prettyprint-override""><code>import os
import json
import numpy as np
import requests
from PIL import Image as PilImage

test_data = np.array(PilImage.open(PATH/'src/deployment/test/test_image.png')).tolist()
test_sample = json.dumps({'raw_data': 
    test_data
})
test_sample_encoded = bytes(test_sample, encoding='utf8')

headers = {
    'Content-Type': 'application/json'
}

response = requests.post(
    service.scoring_uri,
    data=test_sample_encoded,
    headers=headers,
    verify=True,
    timeout=10
)
</code></pre>

<p>Produces the following error in <code>requests</code> for larger files:</p>

<pre class=""lang-py prettyprint-override""><code>ConnectionError: ('Connection aborted.', BrokenPipeError(32, 'Broken pipe'))
</code></pre>

<p>Which I guess is a known error in requests, when a connection is closed from the server before data upload is completed.</p>

<p><strong>Testing via <code>pycurl</code></strong></p>

<p>Using the curl wrapper, I get a more interpretable response.</p>

<pre class=""lang-py prettyprint-override""><code>import pycurl
from io import BytesIO

c = pycurl.Curl()
b = BytesIO()

c.setopt(c.URL, service.scoring_uri)
c.setopt(c.POST, True)
c.setopt(c.HTTPHEADER,['Content-Type: application/json'])
c.setopt(pycurl.WRITEFUNCTION, b.write)
c.setopt(c.POSTFIELDS, test_sample)
c.setopt(c.VERBOSE, True)
c.perform()

out = b.getvalue()

b.close()
c.close()

print(out)

</code></pre>

<p>For large files, this yields the following error:</p>

<pre class=""lang-html prettyprint-override""><code>&lt;html&gt;
    &lt;head&gt;
        &lt;title&gt;
            413 Request Entity Too Large
        &lt;/title&gt;
    &lt;/head&gt;
    &lt;body bgcolor=""white""&gt;
        &lt;center&gt;
            &lt;h1&gt;
                413 Request Entity Too Large
            &lt;/h1&gt;
        &lt;/center&gt;
        &lt;hr&gt;
        &lt;center&gt;
                nginx/1.10.3 (Ubuntu)
        &lt;/center&gt;
    &lt;/body&gt;
&lt;/html&gt;
</code></pre>

<p>Leading me to believe this is an issue in the Nginx configuration. Specifically, I guess that client_max_body_size is set to 10mb.</p>

<p><strong>Question summarised</strong></p>

<p>Given that I am indeed hitting an issue with the Nginx configuration, can I change it somehow? If not using the Azure Machine Learning Service SDK, then maybe by overwriting the <code>/etc/nginx/nginx.conf</code> file?</p>",2,0,2019-10-02 11:52:14.097000 UTC,,,1,azure-machine-learning-service,574,2013-08-22 07:20:04.257000 UTC,2019-10-15 07:09:47.553000 UTC,,26,0,0,2,,,,,,['azure-machine-learning-service']
How to deploy pre-trained model using sagemaker?,"<p>I have a pre-trained model that will translate text from English to Marathi. You can find it here...</p>

<p>git clone <a href=""https://github.com/shantanuo/Word-Level-Eng-Mar-NMT.git"" rel=""nofollow noreferrer"">https://github.com/shantanuo/Word-Level-Eng-Mar-NMT.git</a></p>

<p>Clone and Run the notebook. I am looking for a way to deploy it so that users can use it as an API</p>

<p>The guidelines for deploying the model can be found here...
<a href=""https://gitlab.com/shantanuo/dlnotebooks/blob/master/sagemaker/01-Image-classification-transfer-learning-cifar10.ipynb"" rel=""nofollow noreferrer"">https://gitlab.com/shantanuo/dlnotebooks/blob/master/sagemaker/01-Image-classification-transfer-learning-cifar10.ipynb</a></p>

<p>I will like to know the steps to follow in order to deploy the model.
Is it possible to create an android app for this?</p>",2,0,2019-03-03 04:50:49.577000 UTC,1.0,2019-03-05 04:09:34.220000 UTC,3,android|amazon-sagemaker,2124,2009-07-16 03:00:08.253000 UTC,2022-09-24 07:20:30.793000 UTC,"Mumbai, India",30594,2740,25,4010,,,,,,['amazon-sagemaker']
unicode error during model inference in Sagemaker notebook,"<p>I am doing inference on a model trained in the sagemaker notebook. I am getting Unicode error while passing the input.</p>
<p>Before deploying, I tried the following and it worked - process the text with input_fn and then pass its output to predict_fn for prediction. But I am facing issue when I use the deploy fn of the sagemaker endpoint.  How can I resolve this.</p>
<pre><code>input_text = &quot;BACKGROUND: COVID-19 is associated with pulmonary embolism (PE) in adults.&quot;
deployment.predict(json.dumps({&quot;data&quot;:input_text}))
</code></pre>
<p>Error
<code>Traceback (most recent call last):   File &quot;/miniconda3/lib/python3.7/site-packages/sagemaker_containers/_functions.py&quot;, line 93, in wrapper     return fn(*args, **kwargs)   File &quot;/opt/ml/code/train_nmf.py&quot;, line 311, in input_fn     input_data = json.loads(serialized_input_data)   File &quot;/miniconda3/lib/python3.7/json/__init__.py&quot;, line 343, in loads     s = s.decode(detect_encoding(s), 'surrogatepass')</code></p>
<p>Training in Sagemaker Notebook</p>
<pre><code>from sagemaker.sklearn.estimator import SKLearn

script_path = 'train_nmf.py'

sklearn = SKLearn(
    entry_point=script_path,
    instance_type=&quot;ml.m4.xlarge&quot;,
    framework_version=&quot;0.23-1&quot;,
    py_version=&quot;py3&quot;,
    role=role,
    sagemaker_session=sagemaker_session,
    output_path=output_data_uri,
    code_location=training_desc_uri,
    source_dir='/home/ec2-user/SageMaker/src')
</code></pre>
<p>Train NMF Code</p>
<pre><code>import os
import numpy as np
import pandas as pd
import joblib
import json
CONTENT_TYPE_JSON = &quot;application/json&quot; 

def process_text(text):
    text = [each.lower() for each in text]
    return text

def model_fn(model_dir):
    # SageMaker automatically load the model.tar.gz from the S3 and 
    # mount the folders inside the docker container. The  'model_dir'
    # points to the root of the extracted tar.gz file.
    model = joblib.load(os.path.join(model_dir, &quot;nmf_model.pkl&quot;))
    return model

def predict_fn(input_data, model):
    # Do your inference
    predicted_topics = model.transform(input_data)
    return predicted_topics

def input_fn(serialized_input_data, model_dir, content_type=CONTENT_TYPE_JSON):
    input_data = json.loads(serialized_input_data)
    input_text_processed = pd.Series(input_data).apply(process_text)
    tf_idf_model = joblib.load(os.path.join(model_dir, &quot;tf_idf.pkl&quot;))
    processed_sample_text = tf_idf_model.transform(input_text_processed)
    return processed_sample_text

def output_fn(prediction_output, model_dir, accept=CONTENT_TYPE_JSON):
    if accept == CONTENT_TYPE_JSON:
        topic_keywords = joblib.load(
            os.path.join(model_dir, &quot;topic_keywords.pkl&quot;)
        )
        pred_dominant_topic = np.argmax(prediction_output, axis=1)
        pred_df = pd.DataFrame(prediction_output, columns=topic_keywords)
        pred_df[&quot;dominant_topic&quot;] = pred_dominant_topic
        return json.dumps(pred_df.to_dict(&quot;records&quot;)), accept
    raise Exception('Unsupported content type')
    
</code></pre>",0,2,2022-03-30 12:17:25.433000 UTC,,,0,python-3.x|scikit-learn|amazon-sagemaker|topic-modeling,82,2020-01-11 08:29:02.730000 UTC,2022-09-23 19:27:27.437000 UTC,"Bangalore, Karnataka, India",1,0,0,2,,,,,,['amazon-sagemaker']
Parquet vs. RecordIO,"<p>I am learning about AWS these days. So, I am sorry if this question is too basic. I've read a bunch of open and closed questions on the benefits of Parquet over CSV (answered: <a href=""https://stackoverflow.com/questions/36822224/what-are-the-pros-and-cons-of-parquet-format-compared-to-other-formats"">What are the pros and cons of parquet format compared to other formats?</a>), and RecordIO-protobuf in terms of file vs. pipe mode (e.g. unanswered <a href=""https://stackoverflow.com/questions/62009495/what-makes-recordio-attractive"">What makes RecordIO attractive</a>). However, I haven't seen any comparison between RecordIO-protobuf and Parquet.</p>
<p>Here's what I could gather from my research:</p>
<ul>
<li>Parquet is a columnar format, but RecordIO-protobuf is used for serialization.</li>
<li>Not all SageMaker algorithms support Parquet. Most SageMaker algorithms work best in RecordIO-protobuf format. (<a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/cdf-training.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/cdf-training.html</a>)</li>
</ul>
<p>Other than above two differences what are the pros and cons of using Parquet vs. recordIO format? Moreover, searching for &quot;Parquet vs. RecordIO&quot; gave me zero Google results, which makes me think that I am comparing apples with oranges.</p>
<p>I'd appreciate any thoughts.</p>",1,1,2021-02-27 23:45:34.623000 UTC,1.0,,2,amazon-web-services|hadoop|parquet|amazon-sagemaker,1654,2015-12-24 04:17:07.517000 UTC,2022-03-09 21:32:39.987000 UTC,,3880,148,7,490,,,,,,['amazon-sagemaker']
"Azure DevOps Build : run command ""OnCancel""","<p>I use Azure DevOps to schedule jobs on Azure Batch AI. Launching of jobs works great, I have python code that does the same.</p>

<p>What I am trying to achieve is that all jobs in the Batch AI experiment should be terminated when the build is cancelled. Currently, cancelling the build doesn't affect the run status of the Batch AI jobs.</p>

<p>Hence, is there a sort of ""OnCancel"" event to hook on to in the build to run a command (which will be python code to terminate all jobs) ?</p>",1,2,2018-11-06 21:12:10.987000 UTC,,2018-11-06 23:49:33.107000 UTC,1,azure|azure-devops|azure-machine-learning-studio,440,2011-06-19 17:47:04.643000 UTC,2022-09-23 00:45:08.543000 UTC,,1876,165,2,201,,,,,,['azure-machine-learning-studio']
AWS sagemaker - import libraries from other sagemaker notebooks in thr same folder,"<p>Started using AWS sage maker recently. I have a couple of notebooks that hold the code for few functions that i would like to use in another notebook of sagemaker. I tried importing these functions with import but didn t work. Any suggestions. The modules are in the same folder as the other notebook </p>

<p>I tried mane combinations of the following without success</p>

<pre><code>From feutures import *
</code></pre>

<p>How can i add external functions to my project? Does the function need to be in a   <code>.py</code> format or   <code>.ipnb</code> is also fine? </p>",1,0,2018-11-29 15:49:22.607000 UTC,,2018-11-30 01:25:44.803000 UTC,-1,amazon-web-services|import|amazon-sagemaker,589,2016-09-23 13:31:59.333000 UTC,2019-07-22 09:12:36.567000 UTC,,1,0,0,0,,,,,,['amazon-sagemaker']
Where can i find some good azure mlops examples?,"<p>I am working on an azure mlops end to end implementation and I am looking for some recent frameworks that can help me on my journey. So far, I am seeing things that are 2 years old. The mlops examples have got to show the training, registering and inferencing components.</p>",0,1,2022-08-18 18:09:53.837000 UTC,,,0,azure-machine-learning-service|mlops|azureml-python-sdk|azuremlsdk|azure-ml-pipelines,55,2022-08-18 18:06:40.810000 UTC,2022-09-20 08:42:59.847000 UTC,,1,0,0,2,,,,,,['azure-machine-learning-service']
Change default OS from SageMaker Notebook Instance,"<p>When creating a Notebook instance, I don't understand how to change the default OS image.
For example, I would have:</p>
<pre><code>sh-4.2$ cat /etc/os-release
NAME=&quot;Amazon Linux AMI&quot;
VERSION=&quot;2018.03&quot;
ID=&quot;amzn&quot;
ID_LIKE=&quot;rhel fedora&quot;
VERSION_ID=&quot;2018.03&quot;
PRETTY_NAME=&quot;Amazon Linux AMI 2018.03&quot;
ANSI_COLOR=&quot;0;33&quot;
CPE_NAME=&quot;cpe:/o:amazon:linux:2018.03:ga&quot;
HOME_URL=&quot;http://aws.amazon.com/amazon-linux-ami/&quot;
You have new mail in /var/spool/mail/ec2-user
</code></pre>
<p>But on SageMaker Studio, the default image is different when creating an instance:</p>
<pre><code>NAME=&quot;Ubuntu&quot;
VERSION=&quot;20.04.3 LTS (Focal Fossa)&quot;
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME=&quot;Ubuntu 20.04.3 LTS&quot;
VERSION_ID=&quot;20.04&quot;
HOME_URL=&quot;https://www.ubuntu.com/&quot;
SUPPORT_URL=&quot;https://help.ubuntu.com/&quot;
BUG_REPORT_URL=&quot;https://bugs.launchpad.net/ubuntu/&quot;
PRIVACY_POLICY_URL=&quot;https://www.ubuntu.com/legal/terms-and-policies/privacy-policy&quot;
VERSION_CODENAME=focal
UBUNTU_CODENAME=focal
</code></pre>
<p>How can I use ubuntu image on SageMaker Notebook instance instead of &quot;Amazon Linux AMI&quot;?</p>
<p>Thanks</p>
<p>Related: <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/nbi-al2.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/nbi-al2.html</a></p>",1,1,2021-12-29 04:55:36.977000 UTC,1.0,,1,amazon-web-services|jupyter-notebook|amazon-sagemaker,502,2017-12-13 12:56:50.217000 UTC,2022-09-22 06:20:22.683000 UTC,,409,25,0,54,,,,,,['amazon-sagemaker']
How do I parse in the arguments to my Python file when running the Sagemaker pipeline's ProcessingStep?,"<p>I read from this <a href=""https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html#sagemaker.workflow.steps.ProcessingStep"" rel=""nofollow noreferrer"">documentation</a> that the ProcessingStep can accept job arguments.</p>
<p>I currently have a python script contaning a function to be executed via ProcessingStep that requires arguments to be parsed in. I am not sure how I can extract the arguments from the 'Job arguments' such that I can call the function in the python script with the arguments.</p>
<p>Here is an example of code snippet from my python script:</p>
<pre><code>def params(input_params):
    details = {&quot;database&quot;: input_params[0],
         &quot;table&quot;: input_params[1], 
         &quot;catalog&quot;: input_params[2], 
         &quot;earliestday&quot;: int(input_params[3]), 
         &quot;latestday&quot;: int(input_params[4]),
         &quot;s3bucket&quot;: input_params[5], 
         &quot;bucketpath&quot;: input_params[6]}
    return details

output_params = params(input_params) #this is where I'm not sure how I can extract the argument from the job arguments in the ProcessingStep to call my function here
</code></pre>
<p>Here's how my processingstep code looks like:</p>
<pre><code>step_params = ProcessingStep(
    name=&quot;StateParams&quot;,
    processor=sklearn_processor, 
    outputs = [processing_output],
    job_arguments = [&quot;ABC&quot;, &quot;SESSION_123&quot;, &quot;AwsDataCatalog&quot;, &quot;5&quot;, &quot;7&quot;, &quot;mybucket&quot;, &quot;bucket2/tmp/athena_sagemaker&quot;],   #This is the job argument I input which I hope will be parsed into my python file function
    code = &quot;params.py&quot;,
)
</code></pre>
<p>Would greatly appreciate if any of you can advice me on how I can go about using the job arguments in the ProcessingStep to successfully call the function in the python script, thanks!</p>",1,0,2021-09-11 08:04:05.253000 UTC,,,0,amazon-web-services|amazon-sagemaker,551,2021-06-09 07:10:46.127000 UTC,2022-09-08 12:58:09.290000 UTC,,23,12,0,8,,,,,,['amazon-sagemaker']
getting token for Azure ML,"<p>I'm quiet new to Azure. I have deployed a model using on Azure ML studio to a webservice which need token to be authenticated. I can get the token using Python SDK. but I need to get the token using postman. for this purpose I register and App in Azure Active Directory to get the access token using it but this token is not a valid token for Azure ML and when I use this token to call my web service it will give &quot;Unauthorized, invalid AAD token specified&quot;. Does anyone have any suggestion about this problem?</p>",2,0,2022-03-09 13:26:22.370000 UTC,0.0,,0,azure-active-directory|azure-machine-learning-service,293,2022-03-09 13:16:11.260000 UTC,2022-04-19 13:22:29.093000 UTC,,1,0,0,0,,,,,,['azure-machine-learning-service']
Error in deploying PyTorch model using SageMaker Pipeline and RegisterModel,"<p>Can anyone provide an example for deploying a pytorch model using <strong>SageMaker Pipeline</strong>?</p>
<p>I've used the MLOps template (MLOps template for model building, traing and deployment) of SageMaker Studio to build a MLOps project.</p>
<p>The template is using sagemaker pipelines to build a pipeline for preprocessing and training and registering the model.
And deployment script is implemented in the YAML file and employing CloudFormation to run. The deployment script will be triggered automatically when the model is registered.</p>
<p>The template is using xgboost model to train the data and deploy the model. I want to use Pytorch and deploy it.
I successfully replaced the pytorch with xgboost and successfully preprocessed the data, trained the model and registered the model. But I didn't use inference.py in my model. So I get error for the model deployment.</p>
<p><strong>The error log in updating the endpoint is:</strong></p>
<pre><code>FileNotFoundError: [Errno 2] No such file or directory: '/opt/ml/model/code/inference.py'
</code></pre>
<p>I tried to find example of using inference.py for pytorch model, but I couldn't find any example which uses <strong>sagemaker pipelines</strong> and <strong>RegisterModel</strong>.</p>
<p>Any help would be appreciated.</p>
<p>Below you can see a part of the pipeline for training and registering the model.</p>
<pre><code>from sagemaker.pytorch.estimator import PyTorch
from sagemaker.workflow.pipeline import Pipeline
from sagemaker.workflow.steps import (
    ProcessingStep,
    TrainingStep,
)
from sagemaker.workflow.step_collections import RegisterModel

pytorch_estimator = PyTorch(entry_point= os.path.join(BASE_DIR, 'train.py'),
                            instance_type= &quot;ml.m5.xlarge&quot;,
                            instance_count=1,
                            role=role,
                            framework_version='1.8.0',
                            py_version='py3',
                            hyperparameters = {'epochs': 5, 'batch-size': 64, 'learning-rate': 0.1})

step_train = TrainingStep(
        name=&quot;TrainModel&quot;,
        estimator=pytorch_estimator,

        inputs={
                &quot;train&quot;: sagemaker.TrainingInput(
                            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[
                            &quot;train_data&quot;
                            ].S3Output.S3Uri,
                            content_type=&quot;text/csv&quot;,
                        ),
                &quot;dev&quot;: sagemaker.TrainingInput(
                            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[
                            &quot;dev_data&quot;
                            ].S3Output.S3Uri,
                            content_type=&quot;text/csv&quot;
                        ),
                &quot;test&quot;: sagemaker.TrainingInput(
                            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[
                            &quot;test_data&quot;
                            ].S3Output.S3Uri,
                            content_type=&quot;text/csv&quot;
                        ),
        },
)
step_register = RegisterModel(
            name=&quot;RegisterModel&quot;,
            estimator=pytorch_estimator,
            model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,
            content_types=[&quot;text/csv&quot;],
            response_types=[&quot;text/csv&quot;],
            inference_instances=[&quot;ml.t2.medium&quot;, &quot;ml.m5.large&quot;],
            transform_instances=[&quot;ml.m5.large&quot;],
            model_package_group_name=model_package_group_name,
            approval_status=model_approval_status,
        )
    
pipeline = Pipeline(
            name=pipeline_name,
            parameters=[
                processing_instance_type,
                processing_instance_count,
                training_instance_type,
                model_approval_status,
                input_data,
            ],
            steps=[step_process, step_train, step_register],
            sagemaker_session=sagemaker_session,
        )
</code></pre>",1,0,2021-10-11 08:35:57.330000 UTC,,2021-10-11 11:58:26.383000 UTC,0,pytorch|amazon-cloudformation|amazon-sagemaker|endpoint|mlops,258,2015-12-23 16:48:13.150000 UTC,2022-09-17 08:32:34.823000 UTC,Finland,398,21,1,28,,,,,,['amazon-sagemaker']
The feature statistics under datasets in Google Vertex AI shows inconsistent results. Has anyone had a similar problem?,"<p>I created a dataset in Google Vertex AI that contains numerous features and after clicking on &quot;Generate Statistics&quot; inside the dataset, I can see some basic stats about each feature, and when I click on each feature a pie chart of % distribution of each value and a histogram show up.</p>
<p>Here is my question. For one of my numerical features, I have lots of zeros, specifically 652 zeros and my whole dataset contains 81K distinct values of that feature. The pie chart shows me that my dataset contains %83 percent zero values.</p>
<p>How is it possible? When I calculate the percentages I get that the dataset has only %0.80 percent zeros. (652/81K)*100=0.80%</p>
<p>Is it a reporting problem, a formatting problem? Has anyone had any issues with the stats in Vertex AI datasets?</p>
<p>Note: I don't have such problems with my other numerical features, I have the problem with only one feature containing a large number of zeros.</p>",0,0,2022-04-04 08:58:28.463000 UTC,,,0,google-cloud-platform|google-cloud-ml|google-cloud-vertex-ai,63,2017-09-20 13:24:49.350000 UTC,2022-09-23 08:25:17.930000 UTC,"İstanbul, Turkey",1,0,0,0,,,,,,['google-cloud-vertex-ai']
Unpickling error when running fairseq on AML using multiple GPUs,"<p>I am trying to run fairseq translation task on AML using 4 GPUs (P100)and it fails with the following error:</p>

<blockquote>
  <p>-- Process 2 terminated with the following error: Traceback (most recent call last):   File
  ""/azureml-envs/azureml_8ef3d311fd9072540e3352d9621cca49/lib/python3.6/site-packages/fairseq/distributed_utils.py"",
  line 174, in all_gather_list
      result.append(pickle.loads(bytes(out_buffer[2 : size + 2].tolist())))
  _pickle.UnpicklingError: invalid load key, '\xad'.</p>
  
  <p>During handling of the above exception, another exception occurred:</p>
  
  <p>Traceback (most recent call last):   File
  ""/azureml-envs/azureml_8ef3d311fd9072540e3352d9621cca49/lib/python3.6/site-packages/torch/multiprocessing/spawn.py"",
  line 19, in _wrap
      fn(i, *args)   File ""/mnt/batch/tasks/shared/LS_root/jobs/nlx-ml-neuralrewrite/azureml/pytorch-fairseq_1568826205_6846ecb6/mounts/workspacefilestore/azureml/pytorch-fairseq_1568826205_6846ecb6/train.py"",
  line 272, in distributed_main
      main(args, init_distributed=True)   File ""/mnt/batch/tasks/shared/LS_root/jobs/nlx-ml-neuralrewrite/azureml/pytorch-fairseq_1568826205_6846ecb6/mounts/workspacefilestore/azureml/pytorch-fairseq_1568826205_6846ecb6/train.py"",
  line 82, in main
      train(args, trainer, task, epoch_itr)   File ""/mnt/batch/tasks/shared/LS_root/jobs/nlx-ml-neuralrewrite/azureml/pytorch-fairseq_1568826205_6846ecb6/mounts/workspacefilestore/azureml/pytorch-fairseq_1568826205_6846ecb6/train.py"",
  line 123, in train
      log_output = trainer.train_step(samples)   File ""/azureml-envs/azureml_8ef3d311fd9072540e3352d9621cca49/lib/python3.6/site-packages/fairseq/trainer.py"",
  line 305, in train_step
      [logging_outputs, sample_sizes, ooms, self._prev_grad_norm],   File
  ""/azureml-envs/azureml_8ef3d311fd9072540e3352d9621cca49/lib/python3.6/site-packages/fairseq/distributed_utils.py"",
  line 178, in all_gather_list
      'Unable to unpickle data from other workers. all_gather_list requires all ' Exception: Unable to unpickle data from other workers.
  all_gather_list requires all workers to enter the function together,
  so this error usually indicates that the workers have fallen out of
  sync somehow. Workers can fall out of sync if one of them runs out of
  memory, or if there are other conditions in your training script that
  can cause one worker to finish an epoch while other workers are still
  iterating over their portions of the data.</p>
  
  <p> 2019-09-18
  17:28:44,727|azureml.WorkerPool|DEBUG|[STOP]</p>
  
  <p>Error occurred: User program failed with Exception: </p>
  
  <p>-- Process 2 terminated with the following error: Traceback (most recent call last):   File
  ""/azureml-envs/azureml_8ef3d311fd9072540e3352d9621cca49/lib/python3.6/site-packages/fairseq/distributed_utils.py"",
  line 174, in all_gather_list
      result.append(pickle.loads(bytes(out_buffer[2 : size + 2].tolist())))
  _pickle.UnpicklingError: invalid load key, '\xad'.</p>
  
  <p>During handling of the above exception, another exception occurred:</p>
  
  <p>Traceback (most recent call last):   File
  ""/azureml-envs/azureml_8ef3d311fd9072540e3352d9621cca49/lib/python3.6/site-packages/torch/multiprocessing/spawn.py"",
  line 19, in _wrap
      fn(i, *args)   File ""/mnt/batch/tasks/shared/LS_root/jobs/nlx-ml-neuralrewrite/azureml/pytorch-fairseq_1568826205_6846ecb6/mounts/workspacefilestore/azureml/pytorch-fairseq_1568826205_6846ecb6/train.py"",
  line 272, in distributed_main
      main(args, init_distributed=True)   File ""/mnt/batch/tasks/shared/LS_root/jobs/nlx-ml-neuralrewrite/azureml/pytorch-fairseq_1568826205_6846ecb6/mounts/workspacefilestore/azureml/pytorch-fairseq_1568826205_6846ecb6/train.py"",
  line 82, in main
      train(args, trainer, task, epoch_itr)   File ""/mnt/batch/tasks/shared/LS_root/jobs/nlx-ml-neuralrewrite/azureml/pytorch-fairseq_1568826205_6846ecb6/mounts/workspacefilestore/azureml/pytorch-fairseq_1568826205_6846ecb6/train.py"",
  line 123, in train
      log_output = trainer.train_step(samples)   File ""/azureml-envs/azureml_8ef3d311fd9072540e3352d9621cca49/lib/python3.6/site-packages/fairseq/trainer.py"",
  line 305, in train_step
      [logging_outputs, sample_sizes, ooms, self._prev_grad_norm],   File
  ""/azureml-envs/azureml_8ef3d311fd9072540e3352d9621cca49/lib/python3.6/site-packages/fairseq/distributed_utils.py"",
  line 178, in all_gather_list
      'Unable to unpickle data from other workers. all_gather_list requires all ' Exception: Unable to unpickle data from other workers.
  all_gather_list requires all workers to enter the function together,
  so this error usually indicates that the workers have fallen out of
  sync somehow. Workers can fall out of sync if one of them runs out of
  memory, or if there are other conditions in your training script that
  can cause one worker to finish an epoch while other workers are still
  iterating over their portions of the data.</p>
</blockquote>

<p>The same code with same param runs fine on a single local GPU. How do I resolve this issue?</p>",0,4,2019-09-18 18:01:47.947000 UTC,,,0,azure-machine-learning-service,216,2019-09-18 17:54:14.257000 UTC,2019-11-15 22:49:28.580000 UTC,,1,0,0,4,,,,,,['azure-machine-learning-service']
How to deploy our own TensorFlow Object Detection Model in amazon Sagemaker?,"<p>I have my own trained TF Object Detection model. If I try to deploy/implement the same model in AWS Sagemaker. It was not working.
I have tried TensorFlowModel() in Sagemaker. But there is an argument called entrypoint- how to create that .py file for prediction?</p>",1,0,2020-04-23 03:24:32.687000 UTC,1.0,,1,amazon-web-services|tensorflow|amazon-sagemaker|object-detection-api,408,2020-03-31 13:43:15.660000 UTC,2020-07-21 12:08:11.893000 UTC,,11,0,0,1,,,,,,['amazon-sagemaker']
Response From AWS SageMaker InvokeEndpoint Returning NULL Body (Guzzle Stream) (AWS SDK for PHP),"<p>I am using the AWS PHP SDK to retrieve a prediction from a SageMaker Model.  The response comes back as successful, however, the body is a Null or Guzzle stream.  I know it's getting the prediction successfully since the content-length value is exactly the same as what I get when using the AWS CLI to run the prediction (which works fine).  Below is my code as well as the $result object.  I have also tested adding the &quot;accept&quot; field to the InvokeEndpoint with various MIME types with no success.  If anyone has run into similar issues please let me know!</p>
<p>Any help is greatly appreciated!</p>
<p><strong>Code</strong></p>
<pre><code>use Aws\SageMakerRuntime\SageMakerRuntimeClient;

// Build Client
$client = new Aws\SageMakerRuntime\SageMakerRuntimeClient([
                        'region' =&gt; 'us-east-1',
                        'version' =&gt; &quot;latest&quot;,
                        'credentials' =&gt; [
                            'key'    =&gt; 'XXXXXXXXXXXXXX',
                            'secret' =&gt; 'XXXXXXXXXXXXXX',
                        ]
]);

// Invoke Endpoint
$result = $client-&gt;invokeEndpoint([
    'Body' =&gt; '{&quot;instances&quot;: [{&quot;in0&quot;:[13127,Sunrise Tells The Story,Midland,605,1/16/22,50,2197,9329,2071,53,574,39,483,122]}]}',
    'ContentType' =&gt; 'text/csv',
    'EndpointName' =&gt; 'Sagemaker-Before-10-24-21'
]);
</code></pre>
<p><strong>Result</strong></p>
<pre><code>object(Aws\Result)#124 (2) {
  [&quot;data&quot;:&quot;Aws\Result&quot;:private]=&gt;
  array(5) {
    [&quot;Body&quot;]=&gt;
    object(GuzzleHttp\Psr7\Stream)#119 (7) {
      [&quot;stream&quot;:&quot;GuzzleHttp\Psr7\Stream&quot;:private]=&gt;
      resource(9) of type (stream)
      [&quot;size&quot;:&quot;GuzzleHttp\Psr7\Stream&quot;:private]=&gt;
      NULL
      [&quot;seekable&quot;:&quot;GuzzleHttp\Psr7\Stream&quot;:private]=&gt;
      bool(true)
      [&quot;readable&quot;:&quot;GuzzleHttp\Psr7\Stream&quot;:private]=&gt;
      bool(true)
      [&quot;writable&quot;:&quot;GuzzleHttp\Psr7\Stream&quot;:private]=&gt;
      bool(true)
      [&quot;uri&quot;:&quot;GuzzleHttp\Psr7\Stream&quot;:private]=&gt;
      string(10) &quot;php://temp&quot;
      [&quot;customMetadata&quot;:&quot;GuzzleHttp\Psr7\Stream&quot;:private]=&gt;
      array(0) {
      }
    }
    [&quot;ContentType&quot;]=&gt;
    string(23) &quot;text/csv; charset=utf-8&quot;
    [&quot;InvokedProductionVariant&quot;]=&gt;
    string(17) &quot;AllTrafficVariant&quot;
    [&quot;CustomAttributes&quot;]=&gt;
    string(0) &quot;&quot;
    [&quot;@metadata&quot;]=&gt;
    array(4) {
      [&quot;statusCode&quot;]=&gt;
      int(200)
      [&quot;effectiveUri&quot;]=&gt;
      string(105) &quot;https://runtime.sagemaker.us-east-1.amazonaws.com/endpoints/Before-10-24-21/invocations&quot;
      [&quot;headers&quot;]=&gt;
      array(5) {
        [&quot;x-amzn-requestid&quot;]=&gt;
        string(36) &quot;e523ea41-0bd9-4bcd-9a80-8b5e3f710bbf&quot;
        [&quot;x-amzn-invoked-production-variant&quot;]=&gt;
        string(17) &quot;AllTrafficVariant&quot;
        [&quot;date&quot;]=&gt;
        string(29) &quot;Mon, 24 Jan 2022 15:22:54 GMT&quot;
        [&quot;content-type&quot;]=&gt;
        string(23) &quot;text/csv; charset=utf-8&quot;
        [&quot;content-length&quot;]=&gt;
        string(2) &quot;18&quot;
      }
      [&quot;transferStats&quot;]=&gt;
      array(1) {
        [&quot;http&quot;]=&gt;
        array(1) {
          [0]=&gt;
          array(0) {
          }
        }
      }
    }
  }
  [&quot;monitoringEvents&quot;:&quot;Aws\Result&quot;:private]=&gt;
  array(0) {
  }
}
</code></pre>",0,0,2022-01-24 15:37:21.057000 UTC,,,0,php|amazon-web-services|guzzle|amazon-sagemaker|aws-php-sdk,173,2018-09-27 05:14:54.107000 UTC,2022-07-03 21:58:29.533000 UTC,,1,0,0,1,,,,,,['amazon-sagemaker']
SageMaker update endpoint hangs and fails,"<p>We have a weekly job in Databricks that calls the following code to update a live SageMaker endpoint. The endpoint is hosted on 7 m4x.large instances and serves a fairly high amount of trafic (100s of requests per second).</p>
<pre><code>import mlflow.sagemaker as mfs
logged_model = 'runs:/' + run_id + '/model'
image_ecr_url = &quot;882336743852.dkr.ecr.us-west-2.amazonaws.com/mlflow-pyfunc:1.18.0&quot;
mfs.deploy(app_name='modelName', model_uri=logged_model, image_url=image_ecr_url, region_name=region, instance_count=DEFAULT_INSTANCE_COUNT, mode=&quot;replace&quot;)

</code></pre>
<p>We have recently noticed that many times the job fails with the following error:</p>
<pre><code>&quot;ClientError: An error occurred (ValidationException) when calling the UpdateEndpoint operation: Cannot update in-progress endpoint&quot;
</code></pre>
<p>Furthermore we see a huge spike in 4XX errors from the endpoint. Based on the documentation there should be no service downtime with updates... even if the update fails, we would not expect downtime to production traffic.</p>
<p>Do you have any ideas what is going on here?</p>",0,1,2021-11-01 19:59:19.330000 UTC,,,0,amazon-web-services|amazon-sagemaker,526,2010-10-30 00:41:43.343000 UTC,2022-09-24 14:39:10.563000 UTC,,4599,25,3,141,,,,,,['amazon-sagemaker']
Unable to list repositories in sagemaker project due to stack not existing?,"<p>The following is the error message I get:</p>
<pre><code>Error listing project repositories. Error message: Error listing resources in a stack. Error message: Stack with id SC-xxxxxxxxxxxx-pp-xxxxxxxxxxxxx does not exist. 
</code></pre>
<p>I checked, and a stack was created with the code commit repository but with a different id. I'm wondering where this stack with different id is coming from. Is this a permissions issue?</p>
<p>Edit: This is a custom sagemaker project created from this cloud formation template(which is a service catalog product).</p>
<pre><code>---
Parameters:
  SageMakerProjectId:
    Description: &quot;Service-generated id of the project&quot;
    NoEcho: true
    Type: String
  SageMakerProjectName:
    AllowedPattern: &quot;^[a-zA-Z](-*[a-zA-Z0-9])*&quot;
    Description: &quot;Name of the project&quot;
    MaxLength: 32
    MinLength: 1
    NoEcho: true
    Type: String
Resources:
  ModelBuildCodeCommitRepository:
    Properties:
      Code:
        BranchName: main
        S3:
          Bucket: sagemaker-servicecatalog-seedcode-us-west-2
          Key: toolchain/image-build-model-building-workflow-v1.0.zip
      RepositoryDescription:
        ? &quot;Fn::Sub&quot;
        : &quot;SageMaker Model building workflow infrastructure as code for the Project ${SageMakerProjectName}&quot;
      RepositoryName:
        ? &quot;Fn::Sub&quot;
        : &quot;${SageMakerProjectName}-${SageMakerProjectId}-modelbuild&quot;
    Type: &quot;AWS::CodeCommit::Repository&quot;
</code></pre>",0,3,2022-06-27 21:35:10.350000 UTC,0.0,2022-06-28 19:00:22.387000 UTC,1,amazon-web-services|amazon-sagemaker,36,2022-05-25 20:48:45.307000 UTC,2022-09-15 14:23:04.530000 UTC,,35,25,0,26,,,,,,['amazon-sagemaker']
"python script that runs inside a docker image, doesn't use the usual PYTHONPATH","<p>I'm creating a docker image using the following Dockerfile:</p>
<pre><code>FROM python:3.7
RUN apt-get update &amp;&amp; pip install sagemaker boto3 numpy sagemaker-training

# Copies the training code inside the container
COPY cv.py /opt/ml/code/train.py
COPY scikit_learn_iris.py /opt/ml/code/scikit_learn_iris.py

# Defines train.py as script entrypoint
ENV SAGEMAKER_PROGRAM train.py

# Install custom packages specified in requirements.txts
COPY requirements.txt requirements.txt
RUN pip install -r requirements.txt

ENV PYTHONPATH &quot;/usr/local/lib/python3.7/site-packages&quot;
</code></pre>
<p>In the requirements file, I have added <code>lightgbm</code> library and it installs it successfully inside the docker image. When sagemaker runs starts to run <code>scikit_learn_iris.py</code> cause it can't import <code>lightgbm</code>: <code>ModuleNotFoundError: No module named 'lightgbm'</code>. I'm printing the sys path and PYTHONPATH at the start of <code>scikit_learn_iris.py</code> script and it shows the following results :</p>
<pre><code>sys.path = ['/opt/ml/code', '/opt/ml/code', '/miniconda3/bin', '/miniconda3/lib/python37.zip', '/miniconda3/lib/python3.7', '/miniconda3/lib/python3.7/lib-dynload', '/miniconda3/lib/python3.7/site-packages']

PYTHONPATH = ['/opt/ml/code', '/miniconda3/bin', '/miniconda3/lib/python37.zip', '/miniconda3/lib/python3.7', '/miniconda3/lib/python3.7/lib-dynload', '/miniconda3/lib/python3.7/site-packages']
</code></pre>
<p>why the script is using <code>/miniconda3/...</code> to find the libraries? Even tough I'm setting <code>PYTHONPATH</code> env variable in the Dockerfile? How do I make it understand to look in the correct path?! This path <code>/miniconda3/</code> doesn't even exists in the docker image when I checked (using <code>docker run -it IMAGE_NAME bash</code>)</p>",0,1,2022-09-03 13:55:32.087000 UTC,,,0,python|docker|pip|amazon-sagemaker,42,2014-06-05 15:58:58.270000 UTC,2022-09-23 09:57:39.850000 UTC,,5014,499,1,244,,,,,,['amazon-sagemaker']
Multiple lines on same plot with incremental logging - wandb,"<p>I am using weights &amp; biases (wandb). I want to group multiple plots into one while using incremental logging, any way to do that?</p>
<p>Say we have 10 metrics, I can add them to the project incrementally, gradually building 10 graphs:</p>
<pre><code>import wandb
import math

N_STEPS = 100

wandb.init(project=&quot;someProject&quot;, name=&quot;testMultipleLines&quot;)
for epoch in range(N_STEPS):
    log = {}
    log['main_metric'] = epoch / N_STEPS  # some main metric

    # some other metrics I want to have all on 1 plot
    other_metrics = {}
    for j in range(10):
        other_metrics[f'metric_{j}'] = math.sin(j * math.pi * (epoch / N_STEPS))
    log['other_metrics'] = other_metrics

    wandb.log(log)
</code></pre>
<p>This by default gets presented on the wandb interface as 11 different plots. How can they be grouped through the API (without using the web interface) such that <code>main_metric</code> is on one figure and all <code>other_metrics</code> are bunched together on a second figure?</p>",1,1,2022-04-20 18:42:51.990000 UTC,,,6,python|machine-learning|plot|wandb,275,2016-03-06 08:33:36.340000 UTC,2022-09-23 03:53:28.120000 UTC,,667,100,1,22,,,,,,['wandb']
Weird color scheme in Azure ML Studio notebooks,"<p>I am getting this weird color scheme in Azure ML Studio. The code runs fine, but the excessive use of red makes it look like I have errors on every line. Anyone know how to change the theme?</p>
<p><a href=""https://i.stack.imgur.com/zIiR4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zIiR4.png"" alt=""Printscreen"" /></a></p>",1,0,2022-06-28 08:58:07.043000 UTC,,,0,azure|jupyter-notebook|azure-machine-learning-service|ml-studio,46,2016-05-21 16:01:40.890000 UTC,2022-08-24 14:21:04.223000 UTC,,646,29,4,26,,,,,,['azure-machine-learning-service']
How can I change the version of my model in Azure Machine Learning service?,"<p>When registering a model into Azure Machine Learning, it keeps increments the version number. Can i set the version number to a specific number? Or even stop it from increments every time? </p>

<p><a href=""https://i.stack.imgur.com/EB4wN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EB4wN.png"" alt=""enter image description here""></a></p>",1,0,2020-01-28 13:42:24.563000 UTC,,,0,azure-machine-learning-service,124,2017-02-07 16:37:56.780000 UTC,2021-03-11 21:08:08.630000 UTC,,51,0,0,22,,,,,,['azure-machine-learning-service']
AWS sagemaker error - AttributeError: 'NoneType' object has no attribute 'startswith',"<p>As per this - <a href=""https://stackoverflow.com/questions/56255154/how-to-use-a-pretrained-model-from-s3-to-predict-some-data"">How to use a pretrained model from s3 to predict some data?</a> , I was trying to use an existing model to create an endpoint, but I was facing the following error -</p>
<pre><code>    Traceback (most recent call last):
  File &quot;/miniconda3/lib/python3.7/site-packages/gunicorn/workers/base_async.py&quot;, line 55, in handle
    self.handle_request(listener_name, req, client, addr)
  File &quot;/miniconda3/lib/python3.7/site-packages/gunicorn/workers/ggevent.py&quot;, line 143, in handle_request
    super().handle_request(listener_name, req, sock, addr)
  File &quot;/miniconda3/lib/python3.7/site-packages/gunicorn/workers/base_async.py&quot;, line 106, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
  File &quot;/miniconda3/lib/python3.7/site-packages/sagemaker_sklearn_container/serving.py&quot;, line 124, in main
    serving_env.module_dir)
  File &quot;/miniconda3/lib/python3.7/site-packages/sagemaker_sklearn_container/serving.py&quot;, line 101, in import_module
    user_module = importlib.import_module(module_name)
  File &quot;/miniconda3/lib/python3.7/importlib/__init__.py&quot;, line 118, in import_module
    if name.startswith('.'):
</code></pre>
<p>As per <a href=""https://stackoverflow.com/questions/58050712/problem-deploying-the-best-estimator-gotten-with-sagemaker-estimator-estimator"">Problem deploying the best estimator gotten with sagemaker.estimator.Estimator (w/ sklearn custom image)</a>, <a href=""https://forums.aws.amazon.com/thread.jspa?threadID=313838"" rel=""nofollow noreferrer"">https://forums.aws.amazon.com/thread.jspa?threadID=313838</a> , I am using the correct env variables (along with <code>SAGEMAKER_DEFAULT_INVOCATIONS_ACCEPT</code>, <code>SAGEMAKER_PROGRAM</code>, and <code>SAGEMAKER_SUBMIT_DIRECTORY</code>), but somehow the health checks are failing while creation of the endpoint.</p>
<p>I tried the similar thing via AWS console and it is working surprisingly.
Is there a work around for this to do it via code?</p>
<p>My code snippet:</p>
<pre><code>trainedmodel = sagemaker.model.Model(
model_data='s3://my-bucket/my-key/output/model.tar.gz',
image='my-image',
env={&quot;SAGEMAKER_DEFAULT_INVOCATIONS_ACCEPT&quot;: &quot;text/csv&quot;, 
     &quot;SAGEMAKER_USE_NGINX&quot;: &quot;True&quot;, 
     &quot;SAGEMAKER_WORKER_CLASS_TYPE&quot;: &quot;gevent&quot;, 
     &quot;SAGEMAKER_KEEP_ALIVE_SEC&quot;: &quot;60&quot;, 
     &quot;SAGEMAKER_CONTAINER_LOG_LEVEL&quot;: &quot;20&quot;,
     &quot;SAGEMAKER_ENABLE_CLOUDWATCH_METRICS&quot;: &quot;false&quot;,
     &quot;SAGEMAKER_PROGRAM&quot;: &quot;my-script.py&quot;,
     &quot;SAGEMAKER_REGION&quot;: &quot;us-east-1&quot;,
     &quot;SAGEMAKER_SUBMIT_DIRECTORY&quot;: &quot;s3://my-bucket/my-key/source/sourcedir.tar.gz&quot;
    },
role=role)

trainedmodel.deploy(initial_instance_count=1, instance_type='ml.c4.xlarge', endpoint_name = 'my-endpoint')
</code></pre>",4,0,2020-06-20 17:45:55.037000 UTC,,2021-11-17 13:38:44.900000 UTC,4,python|docker|amazon-s3|boto3|amazon-sagemaker,4086,2018-08-17 04:50:03.700000 UTC,2021-11-18 12:17:19.553000 UTC,"Faridabad, Haryana, India",99,4,0,28,,,,,,['amazon-sagemaker']
VIM (or other Plugin) installation in SageMaker Jupyter(Lab),<p>Can one install Jupyter/JupyterLab plugins on SageMaker? I don't see any options to add plugins either in JupyterLab or the SageMaker interface. Would love to have at least the VIM plugin installed.</p>,2,0,2019-01-10 18:24:17.967000 UTC,2.0,2019-01-11 09:14:19.713000 UTC,2,amazon-sagemaker,1635,2014-07-14 03:33:39.730000 UTC,2022-09-23 14:53:07.687000 UTC,"Modena, Province of Modena, Italy",724,362,1,44,,,,,,['amazon-sagemaker']
Filter mlflow runs by commit ID,"<p>When using the UI of MlFlow, is it possible to filter/search the runs using the (git) commit ID? I manage to search by parameters but it doesn't seem like there's a way to filter by the commit ID.</p>

<p><a href=""https://i.stack.imgur.com/npkFO.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/npkFO.png"" alt=""enter image description here""></a></p>",2,0,2019-01-17 10:22:40.897000 UTC,,2019-01-18 07:23:50.323000 UTC,4,machine-learning|version-control|mlflow,982,2011-03-22 10:28:37.227000 UTC,2022-09-23 13:51:41.560000 UTC,,11410,2846,6,1782,,,,,,['mlflow']
Linear regression model with integration of MLFlow,"<p>Does anybody has a link to sample Linear Regression code integrated with MLFlow and explaining all three concepts of MLFlow i.e. Tracking, Project and Model? </p>

<p>I'm particularly looking for a demo link to the same.</p>",1,1,2019-05-27 14:54:33.863000 UTC,,,1,python|machine-learning|linear-regression|azure-databricks|mlflow,204,2016-01-27 15:07:06.643000 UTC,2020-11-09 18:39:49.130000 UTC,,329,8,0,28,,,,,,['mlflow']
Standardized filenames when passing folders between steps in pipeline architecture?,"<p>I am using AzureML pipelines, where the interface between pipeline steps is through a folder or file.</p>
<p>When I am passing data into the pipeline, I point directly to a single file. No problem at all. Very useful when passing in configuration files which all live in the same folder on my local computer.</p>
<p>However, when passing data between different steps of the pipeline, I can't provide the next step with a file path. All the steps get is a path to some folder that they can write to. Then that same path is passed to the next step.</p>
<p>The problem comes when the following step is then supposed to load something from the folder.
Which filename is it supposed to try to load?</p>
<p><strong>Approaches I've considered:</strong></p>
<ul>
<li>Use a standardized filename for everything. Problem is that I want to be able to run the steps locally too, independant of any pipeline. This makes very for a very poor UX for that use case.</li>
<li>Check if the path is to a file, if it isn't, check all the files in the folder. If there is only one file, then use it. Otherwise throw an exception. This is maybe the most elegant solution from a UX perspective, but it sounds overengineered to me. We also don't structurally share any code between the steps at the moment, so either we will have repetition or we will need to find some way to share code, which is non-trivial.</li>
<li>Allow custom filenames to be passed in optionally, otherwise use a standard filename. This helpes with the UX, but often the filenames are supposed to be defined by the configuration files being passed in, so while we could do some bash scripting to get the filename into the command, it feels like a sub-par solution.</li>
</ul>
<p>Ultimately it feels like none of the solutions I have come up with are any good.
It feels like we are making things more difficult for ourselves in the future if we assume some default filename. F.x. we work with multiple file types, so it would need to omit an extension.
But any way to do it without default filenames would also cause maintainence headache down the line, or incurr substantial upfront cost.</p>
<p>The question is am I missing something? Any potential traps, better solutions, etc. would be appreciated. It definately feels like I am somewhat under- and/or overthinking this.</p>",0,0,2022-07-26 15:14:48.323000 UTC,,,0,pipeline|azure-machine-learning-service,31,2019-11-26 13:52:01.893000 UTC,2022-08-12 11:15:50.807000 UTC,Iceland,33,4,0,3,,,,,,['azure-machine-learning-service']
Automl object detection local filesystem,"<p>I have tried to create a azure automl model to find an object in the image.</p>
<p>According to the tutorial it is required that you specify the labels in adataframe where on of the columns are mounted to a aml datastore.</p>
<p>Question: Is it possible to link it to a local repository instead eg in a compute?</p>
<p>Link:  <a href=""https://docs.microsoft.com/en-us/azure/machine-learning/tutorial-auto-train-image-models"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/machine-learning/tutorial-auto-train-image-models</a></p>
<p>I tried to use os path but it did not work.</p>",0,1,2022-03-21 19:00:54.553000 UTC,,,0,azure|object-detection|azure-machine-learning-service|azure-auto-ml,21,2018-05-13 14:47:24.170000 UTC,2022-06-12 12:08:47.330000 UTC,"Pakis, Indonesia",101,1,0,0,,,,,,['azure-machine-learning-service']
schedule Azure machine learning compute instances,"<p>I want to schedule azure machine learning compute instances so that I can stop them during off-hours like weekends, azure automation solution with runbook seems to be working with VMs in general but not with azure ML. The solution could be either a script or ML pipeline.</p>",2,0,2020-05-08 07:34:59.370000 UTC,1.0,2020-05-09 09:44:15.823000 UTC,5,azure-machine-learning-service|azure-vm,711,2020-05-08 07:31:40.490000 UTC,2020-10-09 05:51:27.747000 UTC,,51,0,0,3,,,,,,['azure-machine-learning-service']
control over Vertex AI annotation,"<p>In the Google Vertex AI platform, the configuration of labelling the data set is decided by the vertex AI itself, ie. for example the color which is given to the bounding box while labelling is decided by vertex ai, what if I want to assign the color as my preference? I couldn't see any option to change the annotation color and also tried to feed externally labelled files to the vertex ai and it shows an error as there is no field named color;</p>
<pre><code>{
  &quot;imageGcsUri&quot;: &quot;gs://cloud-ai-platform-5100f6e6-d2e6-4966-869e-ba22a09ef85a/bg_MAX_0002.JPG&quot;,
  &quot;boundingBoxAnnotations&quot;: [
    {
      &quot;displayName&quot;: &quot;abc&quot;,
      &quot;xMin&quot;: 0.07510431154381085,
      &quot;xMax&quot;: 0.34492350486787204,
      &quot;yMin&quot;: 0.1022964509394572,
      &quot;yMax&quot;: 0.4384133611691023,
      &quot;annotationResourceLabels&quot;: {
        &quot;aiplatform.googleapis.com/annotation_set_name&quot;: &quot;3765893295830466560&quot;
      }
    },
    {
      &quot;displayName&quot;: &quot;pqr&quot;,
      &quot;xMin&quot;: 0.6801112656467315,
      &quot;xMax&quot;: 0.9318497913769124,
      &quot;yMin&quot;: 0.1503131524008351,
      &quot;yMax&quot;: 0.6367432150313153,
      &quot;annotationResourceLabels&quot;: {
        &quot;aiplatform.googleapis.com/annotation_set_name&quot;: &quot;3765893295830466560&quot;
      }
    },
    {
      &quot;displayName&quot;: &quot;xyz&quot;,
      &quot;xMin&quot;: 0.15438108484005564,
      &quot;xMax&quot;: 0.6620305980528511,
      &quot;yMin&quot;: 0.605427974947808,
      &quot;yMax&quot;: 0.906054279749478,
      &quot;annotationResourceLabels&quot;: {
        &quot;aiplatform.googleapis.com/annotation_set_name&quot;: &quot;3765893295830466560&quot;
      }
    }
  ],
  &quot;dataItemResourceLabels&quot;: {
    
  }
}
</code></pre>
<p>Given above is an example label generated by vertex ai and it doesn't contain any color information even though I've labelled in multiple colors.
So my question is how can we get control over the color feature of the labelling system in vertex AI?</p>",0,0,2022-02-16 06:52:00.227000 UTC,,2022-07-08 11:13:04.877000 UTC,0,machine-learning|google-cloud-platform|annotations|artificial-intelligence|google-cloud-vertex-ai,173,2021-07-13 12:34:08.183000 UTC,2022-03-07 05:23:22.500000 UTC,India,1,0,0,2,,,,,,['google-cloud-vertex-ai']
issues with passing command line args to custom training job vertex AI,"<p>I am trying to run basic custom training job</p>
<pre><code>job = aiplatform.CustomContainerTrainingJob(
 display_name='testjob-name',
 container_uri='gcr.io/prj-id/image-name:latest',
 project=project_id,
 credentials= credentials,
 staging_bucket= 'stage-bucket'
)
</code></pre>
<p>using below code to run the job</p>
<pre><code>job.run(
        args=['--data_dir', '/gcs/bucket/folder',
        '--model_dir', '/gcs/bucket/model',
        '--configs', 'internal-config.yml'],
        replica_count=1,
        sync=True
    )

</code></pre>
<p>Training job is getting exited with code 127 when I am passing args with job.run(). kindly help, what is the correct way to send command line arguments to custom training python script.
There isn’t much coming up in logs either.</p>
<p>Thanks in advance</p>",0,0,2022-09-23 16:02:25.487000 UTC,,,0,google-cloud-platform|google-cloud-vertex-ai|custom-training,22,2015-06-05 12:51:43.627000 UTC,2022-09-25 05:29:10.830000 UTC,,130,49,0,42,,,,,,['google-cloud-vertex-ai']
How to return name of AWS Sagemaker notebook instance within python script?,"<p>Bit of an edge case ask, basically I'm trying to write a general purpose script that can be set up as a cronjob on AWS Sagemaker that will notify users if an instance is still active. <strong>If possible, I'd like to be able to call name of the AWS Sagemaker instance (different from the name of the active jupyter notebook) within the cronjob.</strong></p>

<p>Haven't been able to find anything in the boto3 or sagemaker documentation specific to this. I know that the instance name is contained within the URL path, but similarly haven't found a way to reference back to this. I'm expecting this isn't an intended functionality so will probably just go ahead with setting values manually in a config file, but if anyone has any creative solutions I'd appreciate the input!</p>",1,2,2020-01-28 19:51:20.940000 UTC,,2020-01-29 17:57:04.603000 UTC,0,python|amazon-web-services|amazon-sagemaker,926,2016-03-03 06:30:06.313000 UTC,2022-09-23 17:07:37.883000 UTC,,757,83,1,80,,,,,,['amazon-sagemaker']
How do you reference the models repository in Azure Machine Learning from within side a python script step?,"<p>I know there's a $MODEL_VERSION variable when you create a scoring script using AKS but how about for a script task (example python script task) but I can't find documentation on how to deserialize a model into object from within script step running on a Linux AML computer cluster.</p>
<p>Is there a way to use models I've published to models tab in Workspace (say name is my model) from within a python script step?</p>
<p>For example in this code snippet:</p>
<pre><code>import job lib
model = joblib.load(file_path + &quot;mymodel&quot;)
</code></pre>
<p>I'm looking for what relative or absolute NIX path to use for file_path during a run where mymodel has already been published to the Workspace.</p>",1,0,2021-07-05 00:33:42.417000 UTC,,,1,azure-machine-learning-studio|azure-machine-learning-service,362,2021-07-05 00:28:29.467000 UTC,2021-08-17 03:11:59.333000 UTC,,11,0,0,0,,,,,,"['azure-machine-learning-studio', 'azure-machine-learning-service']"
AWS SageMaker stop running without error/warning output,"<p>I am trying to do hyperparameter tuning of xgboost I installed using <code>pip install xgboost</code> directly in SageMaker notebook instance. Below is my code to do the hyperparameter tuning:</p>

<pre><code>def train_xgb(X_train, y_train):
    """"""
    train an xgboosting model, optimize hyperparameters via grid search and return predicted results
    :param df: input dataframe containing all train/val/test data
    :return: trained model, predicted probability for y = 1 and predicted y label(0 for nopickup, 1 for pickup)
    """"""
    # grid search to find the best hyperparameter

    model = XGBClassifier()
    learning_rate =  [0.01] #0.001, 0.01, 0.1]
    max_depth = [15, 30, 45]
    n_estimators = [150, 200, 300]
    reg_alpha = [0.01] #[0.01, 0.1, 0.5]
    reg_lambda = [5] #[0.1, 0.5, 5]
    min_child_weight = [0.5] #[0.01, 0.1, 0.5]
    subsample = [0.5] #, 0.7, 0.9]
    colsample_bytree = [0.7] #[0.7, 0.5, 0.9]
    scale_pos_weight= [2]
    param = dict(learning_rate=learning_rate, n_estimators=n_estimators, max_depth=max_depth, reg_alpha=reg_alpha,
                      reg_lambda=reg_lambda, min_child_weight=min_child_weight, subsample=subsample,
                      colsample_bytree=colsample_bytree, scale_pos_weight=scale_pos_weight)

    grid_search = GridSearchCV(model, param_grid = param, scoring='roc_auc', cv=StratifiedKFold(10))
    print('Time before grid search: ', datetime.now())
    result = grid_search.fit(X_train, y_train)
    print('Time after grid search: ', datetime.now())
    # summarize results
    print(""Best %f using %s"" % (result.best_score_, result.best_params_))

    # for initial scan
    # model = XGBClassifier(colsample_bytree=0.9, learning_rate=0.01, max_depth=15, min_child_weight=0.5, n_estimators=50,
    # reg_alpha=0.01, reg_lambda=5, subsample=0.7, scale_pos_weight=2)

    best_xgb = result.best_estimator_
    best_xgb.fit(X_train, y_train)

    return best_xgb
</code></pre>

<p>My training data is about 2 million rows and 160 columns, with the csv file of a size of around 0.5M KB (not big at all). The size of engineered dataframe is 1.5GB. The notebook instance I chose is ml.c5.18xlarge. However, the code will run at the <code>grid_search.fit(X_train, y_train)</code> step for about 2hrs (I set 2 parameters and three values for each, so in total only 9 combinations) and stop without any warning/error output. </p>

<p>This seems to be a rare case as few people came across it when I asked around. However, no output makes it challenging to debug. I wonder 1) is it normal to take so long to run? 2) any hint why this happens and how to debug? Many thanks in advance!</p>",0,3,2019-08-16 21:36:06.453000 UTC,,2019-08-16 23:58:25.993000 UTC,0,python|xgboost|amazon-sagemaker|hyperparameters,447,2014-05-25 15:30:58.507000 UTC,2022-07-24 23:13:53.413000 UTC,"Seattle, WA, United States",1021,92,0,120,,,,,,['amazon-sagemaker']
azure ml metrics using log or parent.log()?,"<p>At the moment I am using log to track ml experiment metrics in azure. An Example of the output.</p>
<pre><code>Run 1  mse=0.3
Run 2  mse=0.2
run 3  mse=0.1
</code></pre>
<p>However, I want one mse value that summarises the entire pipeline would  parent_run.log allow me to do this?</p>
<p>Research material used
<a href=""https://docs.microsoft.com/en-us/azure/machine-learning/how-to-track-designer-experiments"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/machine-learning/how-to-track-designer-experiments</a></p>",1,0,2020-12-03 22:52:19.897000 UTC,,,3,azure-machine-learning-studio|azure-machine-learning-service,532,2020-01-14 14:47:36.570000 UTC,2022-09-21 12:27:07.623000 UTC,,129,7,0,17,,,,,,"['azure-machine-learning-studio', 'azure-machine-learning-service']"
Wandb line plots only show bar charts after refresh,"<p>My weights and biases (wandb) panels (e.g. for loss) shortly show line plots (x: steps, y: loss), then refresh (showing a spinner for some time) and then only show bar charts.</p>
<p>Editing such panels either shows (a) &quot;Select runs that logged eval/loss
to visualize data in this line chart.&quot; on the left or (b) &quot;Showing a bar chart instead of a line chart because all logged values are length one.&quot; on the right.</p>
<p>Does that mean that (a) a value <code>eval/loss</code> is not found for the runs, or (b) only one value is given per run? How can I change this? But why was there a real line plot shown for about a second, before the panel refreshes? Where there values dropped? Why?</p>
<p>Panels shortly show line plots: <a href=""https://i.stack.imgur.com/MaMtL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MaMtL.png"" alt=""enter image description here"" /></a></p>
<p>Then panels refresh and only show bar charts: <a href=""https://i.stack.imgur.com/5oTvP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5oTvP.png"" alt=""enter image description here"" /></a></p>
<h2>Code</h2>
<p>I am using huggingface transformers <code>TrainingArguments</code> with argument <code>report_to=&quot;wandb&quot;</code> (but with the default <code>logging_steps</code> of 500). I am doing 10-fold cross validation without any explicit <code>wandb.log</code> call within the cross validation loop. I do all of this in the <code>train()</code> function, which has as last command <code>wandb.finish()</code>. <code>train()</code> is called via <code>wandb.agent(sweep_id, train)</code> as I am using all of this within a large sweep.</p>",0,0,2022-03-25 12:53:53.057000 UTC,,,0,machine-learning|pytorch|huggingface-transformers|wandb,231,2011-05-12 10:27:49.933000 UTC,2022-09-21 09:38:10.530000 UTC,"Karlsruhe, Germany",6763,937,28,709,,,,,,['wandb']
Exporting AutoGluOn models to be used in another sagemaker instance / desktop,"<p>I have been using AutoGluOn on SageMaker to train a model. I would like to migrate the fitted predictor to another instance and save the model to a local machine.</p>
<p>The predictor.load('model_folder/') is relevant only if the model files are already there.
Downloading the entire folder seems unfeasible as it is +500GB.</p>
<p>What is the best way to export the predictor with all the relevant files to a single file/folder? Is there a better way than just then just downloading the entire model folder?</p>
<p>Thanks.</p>",0,1,2021-12-20 18:24:15.990000 UTC,,,0,python|deep-learning|amazon-sagemaker|gluon,119,2020-10-09 22:19:20.150000 UTC,2021-12-20 23:13:44.673000 UTC,,1,0,0,2,,,,,,['amazon-sagemaker']
Nodejs example for pachyderm,"<p>I am new to Pachyderm.</p>

<p>I have a pipeline to extract, transform and then save in the db.
Everything is already written in nodejs, docekrized.
Now, I would like to move and use pachyderm.</p>

<p>I tried following the python examples they provided, but creating this new pipeline always fails and the job never starts.</p>

<p>All my code does is take the <code>/pfs/data</code> and copy it to <code>/pfs/out</code>. </p>

<p>Here is my pipeline definition</p>

<pre><code>{
    ""pipeline"": {
        ""name"": ""copy""
    },
    ""transform"": {
        ""cmd"": [""npm"", ""start""],
        ""image"": ""simple-node-docker""
    },
    ""input"": {
        ""pfs"": {
            ""repo"": ""data"",
            ""glob"": ""/*""
        }
    }
}
</code></pre>

<p>All that happens is that the pipeline fails and the job never starts.</p>

<p>Is there a way to debug on why the pipeline is failing?
Is there something special about my docker image that needs to happen?</p>",1,0,2019-05-25 20:45:13.660000 UTC,,,0,node.js|pachyderm,80,2014-06-01 13:18:45.057000 UTC,2022-09-24 21:07:11.220000 UTC,,71,9,0,2,,,,,,['pachyderm']
Using SageMaker Debugger and Debugger Profiler with custom containers (BYOC),"<p>I see here: <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/debugger-bring-your-own-container.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/debugger-bring-your-own-container.html</a> that is is possible to user the <strong>SageMaker Debugger</strong> and the <strong>Debugger profiler</strong> with <strong>custom containers</strong>, but it seems to only be usable if you are using the SageMaker estimator.</p>
<p>Can SageMaker Debugger and the profiler be used with custom containers when the SageMaker estimator is not being used? We technically use the SageMaker estimator, but that estimator is just a wrapper for a Java application. Can the SageMaker Debugger and the profiler be used in that context?</p>",0,0,2022-09-11 00:11:17.257000 UTC,,2022-09-11 00:37:19.080000 UTC,0,amazon-web-services|deep-learning|containers|amazon-sagemaker|amazon-sagemaker-debugger,23,2014-01-16 15:43:59.673000 UTC,2022-09-25 03:22:08.463000 UTC,Singapore,5854,155,70,794,,,,,,['amazon-sagemaker']
deploy web service for registered R model in Azure ML,"<p>I have an absolute nightmare to use <a href=""https://github.com/Azure/azureml-sdk-for-r"" rel=""nofollow noreferrer"">azureml-sdk-for-r</a>. So I try to achieve everything via the UI (<a href=""https://ml.azure.com/"" rel=""nofollow noreferrer"">https://ml.azure.com/</a>). I trained a model locally like so in R 4.0.5</p>
<pre><code>library(datasets)
library(caret)

data(iris)

setwd(&quot;C:/Data&quot;)

index &lt;- createDataPartition(iris$Species, p=0.80, list=FALSE)
testset &lt;- iris[-index,]
trainset &lt;- iris[index,]

model = train(Species ~ ., 
                  data=trainset, 
                  method=&quot;rpart&quot;, 
                  trControl = trainControl(method = &quot;cv&quot;))

saveRDS(model, &quot;model.rds&quot;)
</code></pre>
<p>I deployed/registered it via the UI, no issue. The &quot;scoring script&quot; I try to use to remove the model dependency is as follows (the only dependency is really jsonlite).</p>
<pre><code>library(jsonlite)

init &lt;- function()
{
  message(&quot;model is loaded&quot;)
  
  function(data)
  {
    prediction_data &lt;- as.data.frame(fromJSON(data))
    return('{&quot;result&quot;: &quot;Hello world&quot;}')
  }
}
</code></pre>
<p>I use the following yml file as my conda dependency file for this screen:</p>
<p><a href=""https://i.stack.imgur.com/ZFv3i.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZFv3i.png"" alt=""enter image description here"" /></a></p>
<pre><code>name: scoring_environment
channels:
  - defaults
dependencies:
  - r-base=4.0.5
  #- r-essentials=4.0.5
  # whatever other dependencies you have
  - jsonlite=1.7.2 
</code></pre>
<p>But get immediately this:</p>
<p><a href=""https://i.stack.imgur.com/Dz7Fd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Dz7Fd.png"" alt=""enter image description here"" /></a></p>
<p>How can I debug what's going on? Is the conda dependency file wrong? As it stand, Azure ML is absolutely useless for me as an R user with locally trained models )-:</p>
<p>PS:</p>
<p>I also try to deploy this locally like so:</p>
<pre><code>library(azuremlsdk)

interactive_auth &lt;- interactive_login_authentication(tenant_id=&quot;296bf094-bdb4-488f-8ebd-92b2dd1464c2&quot;)

ws &lt;- get_workspace(
        name = &quot;xxx&quot;, 
        subscription_id = &quot;xxx&quot;, 
        resource_group =&quot;xxx&quot;, 
        auth = interactive_auth
)

model &lt;- get_model(ws, name = &quot;iris&quot;)

r_env &lt;- r_environment(name = &quot;r_env&quot;)

# Create inference config
inference_config &lt;- inference_config(
  entry_script = &quot;score1.R&quot;,
  source_directory = &quot;.&quot;,
  environment = r_env)

local_deployment_config &lt;- local_webservice_deployment_config()

service &lt;- deploy_model(ws, 
                        'rservice-local', 
                        list(model), 
                        inference_config, 
                        local_deployment_config)
# Wait for deployment
wait_for_deployment(service, show_output = TRUE)

# Show the port of local service
message(service$port)
</code></pre>
<p>It downloads registred model to my local machine. So this bit works but then there is this error:</p>
<pre><code>/azureml-envs/azureml_da3e97fcb51801118b8e80207f3e01ad/lib/python3.6/site-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning:  cannot open file '/var/azureml-app/iris/score1.R': No such file or directory
</code></pre>
<p>So I tried to deliberately created a relative folder:</p>
<p>/var/azureml-app/iris/</p>
<p>where the above script lives and place score1.r (see above) there. Still same error. I am lost!</p>",0,1,2021-05-22 10:42:55.607000 UTC,1.0,2021-05-22 10:57:22.873000 UTC,3,r|azure-machine-learning-studio|azure-machine-learning-service|azuremlsdk,163,2010-03-01 10:53:04.443000 UTC,2022-09-24 18:56:19.313000 UTC,Somewhere,15705,2171,91,2150,,,,,,"['azure-machine-learning-studio', 'azure-machine-learning-service']"
Use labeled data from Amazon Sagemaker on local machine without further dependency on AWS,"<p>Is there a possibility to train a network on a segmented semantic segmentation dataset, by downloading the data from Amazon and using it on your local machine, without any dependencies on AWS? And how would this be done?</p>
<p>Thanks in advance</p>",1,0,2020-07-14 13:31:17.210000 UTC,,,0,amazon-web-services|tensorflow|amazon-sagemaker,21,2018-02-17 20:51:10.837000 UTC,2022-05-02 11:04:34.277000 UTC,,65,10,0,38,,,,,,['amazon-sagemaker']
ML.NET Detect anomalies with gaps/missings in data,"<p>I have done some tests with <a href=""https://docs.microsoft.com/en-us/dotnet/api/microsoft.ml.timeseriescatalog.detectiidspike?view=ml-dotnet"" rel=""nofollow noreferrer"">DetectSpike</a> and <a href=""https://docs.microsoft.com/en-us/dotnet/api/microsoft.ml.timeseriescatalog.detectanomalybysrcnn?view=ml-dotnet"" rel=""nofollow noreferrer"">DetectAnomaly</a> but despite differences and whole pipeline building - both methods require input of 1-dimensional float[]</p>
<p>Its fine to find anomalies and spikes but I was asked to find anomalies &quot;normal&quot;, and anomalies with gaps. (this can be one scenario no need to separate this by kind)</p>
<p>Example 1: WebService receives about 30-100 requests/min, then theres break/gap and no requests for few minutes.</p>
<p>Example 2: I am receiving one bill invoice with random value each month, and theres two months without invoices.
<a href=""https://i.stack.imgur.com/8DJ8n.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8DJ8n.png"" alt=""enter image description here"" /></a>
I guess, that I am not able to do this without Date/Time dimension/column.
Any ideas?</p>",1,3,2022-07-26 11:11:23.307000 UTC,,,0,c#|.net|machine-learning|azure-machine-learning-service|ml.net,38,2012-11-17 10:58:22.690000 UTC,2022-09-23 11:46:14.293000 UTC,"Cracow, Poland",887,184,0,111,,,,,,['azure-machine-learning-service']
How to schedule repeated runs of a custom training job in Vertex AI,"<p>I have packaged my training code as a python package and then am able to run it as a custom training job on Vertex AI. Now, I wanted to be able to schedule this job to run, say every 2 weeks, and re-train the model. The Scheduling settings in the CustomJoBSpec allow only 2 fields, &quot;timeout&quot; and &quot;restartJobOnWorkerRestart&quot; so it's not possible using the scheduling settings in the CustomJobSpec. One way to achieve this I could think of was to create a Vertex AI pipeline with a single step using the &quot;CustomPythonPackageTrainingJobRunOp&quot; Google Cloud Pipeline Component and then scheduling the pipeline to run as I see fit. Are there better alternatives to achieve this?</p>
<p><strong>Edit:</strong></p>
<p>I was able to schedule the custom training job using Cloud Scheduler, but I found using the create_schedule_from_job_spec method in the AIPlatformClient very easy to use in the Vertex AI pipeline. The steps I took to schedule the custom job using Cloud Scheduler in gcp are as follows, <a href=""https://cloud.google.com/scheduler/docs/http-target-auth#setting_up_the_service_account"" rel=""nofollow noreferrer"">link</a> to google docs:</p>
<ol>
<li>Set target type to HTTP</li>
<li>For the url to specify the custom job, I followed <a href=""https://cloud.google.com/vertex-ai/docs/training/create-custom-job#curl"" rel=""nofollow noreferrer"">this</a> link to get the url</li>
<li>For the authentication, under Auth header, I selected the &quot;Add OAauth token&quot;</li>
</ol>
<p>You also need to have a &quot;Cloud Scheduler service account&quot; with  a &quot;Cloud Scheduler Service Agent role granted to it&quot; in your project. Although the docs ay this should have been set up automatically if you enabled the Cloud Scheduler API after March 19, 2019, this was not the case for me and had to add the service account with the role manually.</p>",1,6,2021-08-15 16:12:43.683000 UTC,,2021-08-16 21:06:01.557000 UTC,2,google-cloud-platform|google-cloud-ai|google-cloud-ai-platform-pipelines|google-cloud-vertex-ai,2669,2016-08-15 20:29:46.790000 UTC,2022-09-24 22:22:50.413000 UTC,,700,17,1,90,,,,,,['google-cloud-vertex-ai']
Getting this weird error when trying to run DVC pull,"<p>I am new to using DVC and just exploring it. I am trying to pull data from s3 that was pushed by another person on my team. But I am getting this error:</p>

<pre><code>WARNING: Some of the cache files do not exist neither locally nor on remote. Missing cache files:
name: head_test_file.csv, md5: 45db668193ba44228d61115b1d0304fe
WARNING: Cache '45db668193ba44228d61115b1d0304fe' not found. File 'head_test_file.csv' won't be created.
No changes.
ERROR: failed to pull data from the cloud - Checkout failed for following targets:
head_test_file.csv
Did you forget to fetch?
</code></pre>",2,1,2020-03-26 05:39:46.813000 UTC,,,3,dvc,7063,2017-04-11 13:31:59.307000 UTC,2022-03-29 20:52:48.753000 UTC,,1756,82,5,199,,,,,,['dvc']
"Exporting Keras model to protobuf with a (None, 2) ouptut shape","<p>I have a Keras model I'm trying to export to ProtoBuf</p>

<p>The final couple of layers look like this:</p>

<pre><code>features (Dense)                (None, 128)          49280       concatenate_1[0][0]              
__________________________________________________________________________________________________
gaze_target (Dense)             (None, 2)            258         features[0][0]      
</code></pre>

<p>I try exporting it like this:</p>

<pre><code>sess = K.get_session()

constant_graph = graph_util.convert_variables_to_constants(sess, sess.graph.as_graph_def(), 'gaze_target')
graph_io.write_graph(constant_graph, 'export', 'output.pb', as_text=False)
</code></pre>

<p>This errors with this:</p>

<pre><code>~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/framework/graph_util_impl.py in extract_sub_graph(graph_def, dest_nodes)
    191 
    192   if isinstance(dest_nodes, six.string_types):
--&gt; 193     raise TypeError(""dest_nodes must be a list."")
    194 
    195   name_to_input_name, name_to_node, name_to_seq_num = _extract_graph_summary(

TypeError: dest_nodes must be a list.
</code></pre>

<p>How do I export this model to ProtoBuf? (Ultimately for use on SageMaker)</p>",1,0,2020-01-24 04:22:43.730000 UTC,,,0,python|tensorflow|keras|protocol-buffers|amazon-sagemaker,126,2010-02-24 23:42:07.860000 UTC,2022-09-23 07:54:51.263000 UTC,,1337,61,20,190,,,,,,['amazon-sagemaker']
"""dvc add -external S3://mybucket/data.csv"" is failing with access error even after giving correct remote cache configurations","<p>I'm using dvc and connecting to remote S3 for data track and also setting remote dvc cache in same remote S3.
Following is configure file,</p>
<pre><code>[core]
    remote = s3remote
[cache]
    s3 = s3cache
[‘remote “s3remote”’]
    url = S3://dvc-example
    endpointurl = http://localhost:9000/
    access_key_id = user
    secret_access_key = password
    use_ssl = false
[‘remote “s3cache”’]
    url = s3://dvc-example/cache
    endpointurl = http://localhost:9000/
    access_key_id = user
    secret_access_key = password
    use_ssl = false
</code></pre>
<p>I'm able to push and pull from remote repository to local using s3remote.</p>
<p>But when I try to add external data by configuring cache(s3cache), am getting error.</p>
<p>Both s3cache, s3remote has same credentials, then why is it failing when I add external data in dvc?</p>",1,2,2021-04-15 08:19:28.363000 UTC,,2021-04-15 14:42:01.960000 UTC,4,git|amazon-web-services|machine-learning|amazon-s3|dvc,352,2017-05-31 04:12:26.490000 UTC,2022-09-24 08:59:44.060000 UTC,,838,89,2,33,,,,,,['dvc']
How to install some R packages in Azure ML,"<p>I need to install some R packages in AzureML, but I think that I´m not doing this by the correct way:</p>
<pre><code>install.packages(c (
  
  &quot;timetk&quot;,
  &quot;modeltime&quot;,
  &quot;modeltime.ensemble&quot;,
  &quot;quantmod&quot;,
  &quot;rvest&quot;,
  &quot;tidyverse&quot;,
  &quot;gtrendsR&quot;,
  &quot;xlsx&quot; 
))

</code></pre>
<p>Then when I call the librarys after installing I have the following issue message:</p>
<pre><code>Error in library(timetk): There is no package called `timetk´...
 
</code></pre>
<p>I readed this post here (<a href=""https://stackoverflow.com/questions/43176442/install-r-packages-in-azure-ml"">Install R Packages in Azure ML</a>) in stackoverflow but I confess that I can´t go ahead to solve this...</p>",1,3,2021-12-13 11:12:41.760000 UTC,,2021-12-31 19:16:07.763000 UTC,0,r|azure|installation|package|azure-machine-learning-service,114,2021-10-02 02:22:30.553000 UTC,2022-09-24 19:45:10.530000 UTC,"Curitiba, PR, Brasil",103,11,0,48,,,,,,['azure-machine-learning-service']
"""It is a distutils installed project ..."" when calling install_mlflow()","<p>When <code>install_mlflow()</code> is called to install mlflow for R, the following error is encountered.</p>
<blockquote>
<p>Attempting uninstall: certifi
Found existing installation: certifi 2018.4.16
ERROR: Cannot uninstall 'certifi'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.</p>
</blockquote>
<p><strong>Note:</strong> The above is using <code>miniconda</code> installed using <code>install_miniconda()</code> command.</p>
<hr />
<p>P.S. Posting question &amp; answer for everyone's benefit (I spend 2 days on this).</p>",1,0,2021-11-19 04:23:40.763000 UTC,,2021-11-19 11:15:45.143000 UTC,0,r|rstudio|mlflow,95,2013-03-10 07:04:03.940000 UTC,2022-09-25 01:34:00.087000 UTC,Maldives,3381,698,7,289,,,,,,['mlflow']
How do you install modules within sagemaker training jobs?,"<p>I don't think I'm asking this question right but I have jupyter notebook that launches a Tensorflow training job with a python training script I wrote.</p>

<p>That training script requires certain modules. Seems my sagemaker training job is failing because some of the modules don't exist.</p>

<p>How can I ensure that my training job script has all the modules it needs?</p>

<p><strong>Edit</strong></p>

<p>An example of one of these modules is <code>keras</code>.</p>

<p>The odd thing is, I can <code>import keras</code> in the jupyter notebook, but when that import statement is in my training script then I get the <code>No module named keras</code> error</p>",6,0,2018-11-29 02:12:12.190000 UTC,3.0,2018-11-29 02:19:05.090000 UTC,9,tensorflow|amazon-sagemaker,8822,2011-10-21 21:58:08.810000 UTC,2022-09-17 00:51:12.053000 UTC,,4966,744,11,304,,,,,,['amazon-sagemaker']
How long does it take for a Sagemaker endpoint configuration's training job to finish so you can create the endpoint?,"<p>I'm new to Sagemaker but have been waiting a few hours for a Sagemaker training job to complete so that I can create the endpoint... The Sagemaker console shows a Create endpoint button, but when I press it, it doesn't work. The end point configuration still has a spinning icon for ""Training job"" </p>

<p>How long does it typically take for a Sagemaker endpoint to spin up? </p>",1,0,2020-04-12 01:12:39.430000 UTC,,,1,amazon-sagemaker,1364,2010-06-03 07:13:00.447000 UTC,2022-09-25 04:32:13.383000 UTC,"San Francisco, CA",18781,965,37,1916,,,,,,['amazon-sagemaker']
Azure Machine learning python can't open file,"<p>I chose to use Python 3.8.1 Azure ML in Azure Machine learning studio, but when i run the command
<code>!python train.py</code>, it uses python Anconda 3.6.9, when i downloaded python 3.8 and run the command <code>!python38 train.py</code> in the same dir  as before, the response was <code>python3.8: can't open file</code> .
Any idea?
Also Python 3 in azure, is always busy, without anything running from my side.
Thank you.</p>",2,2,2021-05-19 19:51:28.223000 UTC,,,1,python|azure|azure-machine-learning-studio,171,2020-12-30 20:25:46.650000 UTC,2021-06-24 19:02:37.960000 UTC,,23,0,0,3,,,,,,['azure-machine-learning-studio']
Upload tensorboard logs from cloud storage to vertex ai - tensorboard,"<p>I created a pipeline with vertex ai and added the code for creating and storing my tensorboard logs in cloud storage. The next step in the instructions here <a href=""https://cloud.google.com/vertex-ai/docs/experiments/tensorboard-overview#getting_started"" rel=""nofollow noreferrer"">https://cloud.google.com/vertex-ai/docs/experiments/tensorboard-overview#getting_started</a> is to use the tb-gcp-uploader command to upload the logs to the tensboard experiment page. But I'm getting this message &quot;'tb-gcp-uploader' is not recognized as an internal or external command&quot;. Any thoughts?</p>",1,0,2021-11-12 09:49:08.300000 UTC,,,0,google-cloud-ml|google-cloud-vertex-ai,263,2021-11-12 09:20:00.180000 UTC,2022-09-19 12:56:58.140000 UTC,,1,0,0,3,,,,,,['google-cloud-vertex-ai']
sagemaker invoke_endpoint signature_def feature prep,"<p>I have a sagemaker tensorflow model using a custom estimator, similar to the abalone.py sagemaker tensorflow example, using build_raw_serving_input_receiver_fn in the serving_input_fn:</p>

<pre><code>def serving_input_fn(params):
    tensor = tf.placeholder(tf.float32, shape=[1, NUM_FEATURES])
    return build_raw_serving_input_receiver_fn({INPUT_TENSOR_NAME: tensor})()
</code></pre>

<p>Predictions are being request from java-script using json:</p>

<pre><code>  response = @client.invoke_endpoint(
    endpoint_name: @name,
    content_type: ""application/json"",
    accept: ""application/json"",
    body: values.to_json
    )
</code></pre>

<p>Everything fine so far. Now I want to add some feature engineering (scaling transformations on the features using a scaler derived from the training data). Following the pattern of the answer for <a href=""https://stackoverflow.com/questions/46474658/data-normalization-with-tensorflow-tf-transform"">Data Normalization with tensorflow tf-transform
</a> I've now got serving_input_fn like this:</p>

<pre><code>def serving_input_fn(params):
    feature_placeholders = {
        'f1': tf.placeholder(tf.float32, [None]),
        'f2': tf.placeholder(tf.float32, [None]),
        'f3': tf.placeholder(tf.float32, [None]),
    }
    features = {
        key: tf.expand_dims(tensor, -1)
        for key, tensor in feature_placeholders.items()
    }
    return tf.estimator.export.ServingInputReceiver(add_engineering(features), feature_placeholders)
</code></pre>

<p>From saved_model_cli show --dir . --all I can see the input signature has changed:</p>

<pre><code>signature_def['serving_default']:
The given SavedModel SignatureDef contains the following input(s):
inputs['f1'] tensor_info:
    dtype: DT_FLOAT
    shape: (-1)
    name: Placeholder_1:0
inputs['f2'] tensor_info:
    dtype: DT_FLOAT
    shape: (-1)
    name: Placeholder_2:0
inputs['f3'] tensor_info:
    dtype: DT_FLOAT
    shape: (-1)
    name: Placeholder:0
</code></pre>

<p>How do I prepare features for prediction from this new model? In python I've been unsuccessfully trying things like</p>

<pre><code>requests = [{'f1':[0.1], 'f2':[0.1], 'f3':[0.2]}]
predictor.predict(requests)
</code></pre>

<p>also need to send prediction requests from java-script.</p>",2,0,2018-03-26 12:31:38.880000 UTC,,2018-03-26 14:59:23.033000 UTC,0,tensorflow|tensorflow-serving|amazon-sagemaker,780,2018-03-22 09:26:18.233000 UTC,2019-06-24 16:11:50.700000 UTC,,1,0,0,1,,,,,,['amazon-sagemaker']
How can I add a health check to a Sagemaker Endpoint?,"<p>My sagemaker endpoint has a /ping and according to AWS Cloudwatch it gets pinged about every 5 seconds:</p>

<pre><code>10.32.0.1 - - [01/Feb/2018:08:08:35 +0000] ""GET /ping HTTP/1.1"" 200 1 ""-"" ""AHC/2.0""
</code></pre>

<p>However, I don't see what would happen if this ping would fail. Where can I configure the health check?</p>",2,0,2018-02-01 08:11:30.400000 UTC,,,5,amazon-web-services|amazon-sagemaker,3461,2011-01-04 15:39:58.400000 UTC,2022-09-24 15:06:20.150000 UTC,"München, Deutschland",112558,2933,350,21355,,,,,,['amazon-sagemaker']
How to safely shutdown mlflow ui?,"<p>After running <code>mlflow ui</code> on a remote server, I'm unable to reopen the <code>mlflow ui</code> again.<br>
A workaround is to kill all my processes in the server using <code>pkill -u MyUserName</code>.<br>
Otherwise I get the following error:  </p>

<pre><code>[INFO] Starting gunicorn 20.0.4  
[ERROR] Connection in use: ('127.0.0.1', 5000)
[ERROR] Retrying in 1 second.  
...
Running the mlflow server failed. Please see ther logs above for details.
</code></pre>

<p>I understand the error but I don't understand:<br>
1. What is the correct way to shutdown <code>mlflow ui</code><br>
2. How can I identify the <code>mlflow ui</code> process in order to only kill that process and not use the <code>pkill</code>  </p>

<p>Currently I close the browser or use ctrl+C </p>",5,0,2020-03-04 17:00:47.020000 UTC,2.0,2021-08-31 18:18:00.327000 UTC,6,python|r|machine-learning|mlflow,9850,2016-01-20 20:19:09.903000 UTC,2022-09-11 19:40:57.297000 UTC,Israel,1153,113,14,168,,,,,,['mlflow']
Amazon SageMaker deploying from model artifacts - what object do we load from archive?,"<p>I am following this <a href=""https://github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/tensorflow/deploying_python.rst"" rel=""nofollow noreferrer"">tutorial</a>. It shows how to deploy directly from model artifacts:</p>

<pre><code>from sagemaker.tensorflow import TensorFlowModel

tf_model = TensorFlowModel(model_data='s3://mybucket/model.tar.gz',
                           role='MySageMakerRole',
                           entry_point='entry.py',
                           name='model_name')

predictor = tf_model.deploy(initial_instance_count=1, instance_type='ml.c4.xlarge')
</code></pre>

<p>I am curious what kind of object is stored in <code>s3://mybucket/model.tar.gz</code>? Is it tf.estimator.EstimatorSpec or tf.estimator.Estimator or something else? Is there example how to build and store this object?</p>",1,0,2019-05-07 17:43:41.293000 UTC,,,0,python|tensorflow|amazon-sagemaker,96,2012-09-26 16:34:10.370000 UTC,2022-09-25 01:21:08.267000 UTC,,6498,1266,2,988,,,,,,['amazon-sagemaker']
Converting Mechanical Turk bounding boxes to JSON for object detection,"<p>I am looking to have several 1000 images annotated with bounding boxes on MTurk. When reading the following tutorial (<a href=""https://blog.mturk.com/tutorial-annotating-images-with-bounding-boxes-using-amazon-mechanical-turk-42ab71e5068a"" rel=""nofollow noreferrer"">https://blog.mturk.com/tutorial-annotating-images-with-bounding-boxes-using-amazon-mechanical-turk-42ab71e5068a</a>), it seems like the output of a task will look like the following:</p>

<pre><code>The Worker with ID A39ECJ12CY7TE9 gave the answer 
[{'left': 369, 'top': 47, 'width': 151, 'height': 160, 'label': 'stop sign'}]
</code></pre>

<p>For my object detection model in Sagemaker, I need a dataset of training images in jpg in one folder, and the correct annotations in json format for each image in another folder. An example of a proper json file: <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/object-detection.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/object-detection.html</a></p>

<p>So the question is, how can I use MTurk to get a dataset in the format I need? Is there a way to easily convert their output to JSON files? I have no experience with MTurk whatsoever, so any suggestions are appreciated!</p>",1,0,2018-10-23 12:35:01.557000 UTC,,,1,amazon-web-services|object-detection|bounding-box|mechanicalturk|amazon-sagemaker,478,2017-03-18 21:45:08.190000 UTC,2022-04-01 20:48:19.743000 UTC,,509,5,0,84,,,,,,['amazon-sagemaker']
Vertex AI automatic retraining,"<p>I’m trying to create a Vertex AI endpoint with Monitoring enabled that can trigger a Vertex AI pipeline execution when one of the deployed models drops its performance.
However, Vertex AI does not provide any built-in feature to do it.
Is there a method to capture the alert thrown by Vertex AI Monitoring and trigger the Pipeline?</p>",0,3,2022-09-02 14:41:11.993000 UTC,1.0,,2,python|google-cloud-platform|google-cloud-vertex-ai|mlops,48,2018-11-30 14:55:35.083000 UTC,2022-09-23 15:08:04.723000 UTC,,21,1,0,1,,,,,,['google-cloud-vertex-ai']
Error while loading DSTNEE i sagemaker terminal,"<p>I got this error can someone help?
This error I got while building docker image in the sagemaker terminal , i tried to resolve by changing ubuntu and the cuda version ut still error is persistent can someone tell the reason of error?</p>
<pre><code>In file included from NcExcptionWrap.h:2:0,
                 from NNNetwork.cpp:14:
/usr/include/ncException.h:26:7: note: candidate: netCDF::exceptions::NcException::NcException(const string&amp;, const string&amp;, const char*, int)
       NcException(const std::string&amp; exceptionName,const std::string&amp; complaint,const char* fileName,int lineNumber);
       ^
/usr/include/ncException.h:26:7: note:   candidate expects 4 arguments, 3 provided
/usr/include/ncException.h:24:11: note: candidate: netCDF::exceptions::NcException::NcException(const netCDF::exceptions::NcException&amp;)
     class NcException : public std::exception {
           ^
/usr/include/ncException.h:24:11: note:   candidate expects 1 argument, 3 provided
make[1]: *** [/opt/amazon/dsstne/build/tmp/engine/cpp/NNNetwork.o] Error 1
</code></pre>",0,0,2022-06-08 05:42:38.430000 UTC,,,0,amazon-sagemaker,12,2022-06-08 05:32:00.957000 UTC,2022-06-08 09:58:08.520000 UTC,,1,0,0,0,,,,,,['amazon-sagemaker']
Azure ML: ''Sequential' object has no attribute '_distribution_strategy'',"<p>I want to create an experiment with my pretrained and saved model on azure ml. When using this model, I get this error:</p>
<pre><code>Got exception when invoking script: ''Sequential' object has no attribute '_distribution_strategy''.
</code></pre>
<p>It probably has something to do with the installed libraries, however, I don't know how to fix it.
Here is the code:</p>
<pre><code>    import pandas as pd
    import numpy as np
    import sys
    import pickle
    import io
    import os
    os.system(f&quot;pip install keras&quot;)
    os.system(f&quot;pip install tensorflow=2.2&quot;)
    
    import keras
    
    from sklearn.feature_extraction.text import TfidfVectorizer
    
    def azureml_main(df, dataframe2 = None):
    
    
        sys.path.insert(0,&quot;./Script Bundle&quot;)
        tfidf, clf = pickle.load(open(&quot;./Script Bundle/model.pkl&quot;, 'rb'))
    
        f = tfidf.transform(df.description)
       
        y_pred = clf.predict_classes(f.toarray()) ### this line causes the error
        y_probability = np.around(clf.predict_proba(f.toarray()),4)
        df_pred = pd.DataFrame(y_pred).transpose()
        df_probability = pd.DataFrame(y_probability).transpose()
            
        df_all = pd.concat([df_pred.transpose(),df_probability.transpose()],axis=1)
    
        return df_all,
</code></pre>
<p>Hope someone can help.</p>",0,1,2020-09-15 06:54:45.043000 UTC,,,2,python|tensorflow|keras|azure-machine-learning-service,294,2020-09-07 12:54:31.573000 UTC,2020-09-17 08:48:58.150000 UTC,,21,0,0,2,,,,,,['azure-machine-learning-service']
Plotly shows blank graphs in AWS Sagemaker JupyterLab,"<p>Background: I am new to the Python world and am using Plotly for creating basic graphs in Python. I am using AWS Sagemaker's JupyterLab for creating the python scripts.</p>

<p>Issue: I have been trying to run the basic codes mentioned on Plotly's website however even those are returning blank graphs. </p>

<p>Issue Resolution Tried by myself: </p>

<ol>
<li><p>pip installed plotly version 4.6.0</p></li>
<li><p>Steps mentioned on <a href=""https://plotly.com/python/getting-started/"" rel=""noreferrer"">https://plotly.com/python/getting-started/</a> for JupyterLab support have already been executed</p></li>
</ol>

<p>Code Example:</p>

<pre><code>import plotly.graph_objects as go
fig = go.Figure(data=go.Bar(y=[2, 3, 1]))
fig.show()
</code></pre>",3,2,2020-04-14 18:13:47.007000 UTC,,2020-04-14 18:24:41.800000 UTC,10,machine-learning|plotly|amazon-sagemaker|jupyter-lab|plotly-python,4590,2020-04-14 17:59:49.970000 UTC,2020-09-01 05:41:06.707000 UTC,,101,0,0,2,,,,,,['amazon-sagemaker']
How to restore code of pyfunc class in MlFlow?,"<p>I have lost a code of my pyfunc class which I uploaded to MlFlow. Model is working on a production but via MLFlow UI I can't get code view of model class. For some R models I can get code class using load_model. After call this method I get the whole code view of class which is uploaded to MlFlow. Unfortunately, it doesnt work the same way in python. All files are inside hdfs directory of MLFlow.</p>
<pre><code>import mlflow
logged_model = '&quot;file:///var/mm/mlflow/artifacts/***/*******/artifacts/my_model_name'

# Load model as a PyFuncModel.
loaded_model = mlflow.pyfunc.load_model(logged_model)
</code></pre>
<p>After invoking to loaded_model I get only some not important infos about model:</p>
<pre><code>mlflow.pyfunc.loaded_model:
  artifact_path: my_model_name
  flavor: mlflow.pyfunc.model
  run_id: *****
</code></pre>
<p>How can I check the code of the class which was uploaded to MLFlow earlier?</p>",1,0,2022-08-15 21:07:53.730000 UTC,,2022-08-16 07:14:09.490000 UTC,0,python|mlflow,31,2021-04-03 20:09:07.677000 UTC,2022-09-23 14:47:28.920000 UTC,,7,0,0,6,,,,,,['mlflow']
How to run your own python code on amazon sagemaker,"<p>I have a python code which uses keras and tensorflow backend. My system doesn't support training this model due to low memory space. I want to take use of Amazon sagemaker.</p>
<p>However all the tutorials I find are about deploying your model in docker containers. My model isn't trained and I want to train it on Amazon Sagemaker.</p>
<p>Is there a way to do this?</p>
<p><strong>EDIT :</strong> Also can I make a script of my python code and run on it on AWS sagemaker?</p>",1,0,2020-10-22 11:10:11.953000 UTC,,2020-10-22 11:28:31.040000 UTC,0,python|amazon-sagemaker,955,2020-10-21 11:03:38.153000 UTC,2020-11-23 18:53:17.957000 UTC,,3,0,0,3,,,,,,['amazon-sagemaker']
MLFlow Webhook calling Azure DevOps pipeline - retrieve body,"<p>I am using the MLFlow Webhooks , mentioned <a href=""https://docs.microsoft.com/en-us/azure/databricks/applications/mlflow/model-registry-webhooks"" rel=""nofollow noreferrer"">here</a>. I am using that to queue an Azure Devops Pipeline.</p>
<p>However, I can't seem to to find a way to retrieve the payload variables inside my pipeline.</p>
<p>E.g. during transition of models, according to the document, such a payload is passed</p>
<pre><code>POST
/your/endpoint/for/event/model-versions/stage-transition
--data {
  &quot;event&quot;: &quot;MODEL_VERSION_TRANSITIONED_STAGE&quot;,
  &quot;webhook_id&quot;: &quot;c5596721253c4b429368cf6f4341b88a&quot;,
  &quot;event_timestamp&quot;: 1589859029343,
  &quot;model_name&quot;: &quot;Airline_Delay_SparkML&quot;,
  &quot;version&quot;: &quot;8&quot;,
  &quot;to_stage&quot;: &quot;Production&quot;,
  &quot;from_stage&quot;: &quot;None&quot;,
  &quot;text&quot;: &quot;Registered model 'someModel' version 8 transitioned from None to Production.&quot;
}
</code></pre>
<p>My webhook is created like this:</p>
<pre><code>mlflow_webhook_triggerDevOps={
  &quot;events&quot;: [&quot;TRANSITION_REQUEST_CREATED&quot;, &quot;REGISTERED_MODEL_CREATED&quot;],
  &quot;description&quot;: &quot;Integration with Azure DevOps&quot;,
  &quot;status&quot;: &quot;ACTIVE&quot;,
  &quot;http_url_spec&quot;: {
                    &quot;url&quot;: &quot;https://dev.azure.com/orgname/ProjectName/_apis/build/builds?definitionId=742&amp;api-version=6.0&quot;,
                    &quot;authorization&quot;: &quot;Basic &quot; + base64_message
                    }
 }

mlflow_createwebhook=requests.post('https://databricksurl/api/2.0/mlflow/registry-webhooks/create', headers=header, proxies=proxies, json=mlflow_webhook_body)
</code></pre>
<p>How do I then retrieve the payload variable e.g. model_name, inside my pipeline definition in Azure Devops?.</p>
<p>I looked at <a href=""https://stackoverflow.com/questions/50838651/vsts-use-api-to-set-build-parameters-at-queue-time"">this post</a>, but I can't seem to see any payload information (like mentioned above) under the Network-payload tab (or I am not using properly).</p>
<p>Right now, I can trigger the pipeline, but can't seem to find a way to retrieve the payload.</p>
<p>Is it possible? Am I missing something?</p>",0,3,2022-05-14 09:55:21.283000 UTC,,,1,azure-devops|databricks|webhooks|azure-databricks|mlflow,125,2015-04-10 08:31:54.763000 UTC,2022-09-24 09:37:37.383000 UTC,,596,53,1,80,,,,,,['mlflow']
Batch transform sparse matrix with AWS SageMaker Python SDK,"<p>I have successfully trained a Scikit-Learn LSVC model with AWS SageMaker.<br>
I want to make batch prediction (aka. batch transform) on a relatively big dataset which is a scipy sparse matrix with shape 252772 x 185128. (The number of features is high because there is one-hot-encoding of bag-of-words and ngrams features).   </p>

<p>I struggle because of:   </p>

<ul>
<li><p>the size of the data   </p></li>
<li><p>the format of the data</p></li>
</ul>

<p>I did several experiments to check what was going on:    </p>

<h3>1. predict locally on sample sparse matrix data</h3>

<p><strong>It works</strong><br>
Deserialize the model artifact locally on a SageMaker notebook and predict on a sample of the sparse matrix.<br>
This was just to check that the model can predict on this kind of data.</p>

<h3>2. Batch Transform on a sample csv data</h3>

<p><strong>It works</strong><br>
Launch a Batch Transform Job on SageMaker and request to transform a small sample in dense csv format : it works but does not scale, obviously.<br>
The code is:  </p>

<pre class=""lang-py prettyprint-override""><code>sklearn_model = SKLearnModel(
    model_data=model_artifact_location_on_s3,
    entry_point='my_script.py',
    role=role,
    sagemaker_session=sagemaker_session)

transformer = sklearn_model.transformer(
   instance_count=1, 
   instance_type='ml.m4.xlarge', 
   max_payload=100)

transformer.transform(
   data=batch_data, 
   content_type='text/csv',
   split_type=None)   

print('Waiting for transform job: ' + transformer.latest_transform_job.job_name)
transformer.wait()

</code></pre>

<p>where:  </p>

<ul>
<li>'my_script.py' implements a simple <code>model_fn</code> to deserialize the model artifact: </li>
</ul>

<pre class=""lang-py prettyprint-override""><code>def model_fn(model_dir):
    clf = joblib.load(os.path.join(model_dir, ""model.joblib""))
    return clf
</code></pre>

<ul>
<li><code>batch_data</code> is the s3 path for the csv file.  </li>
</ul>

<h3>3. Batch Transform of a sample dense numpy dataset.</h3>

<p><strong>It works</strong><br>
I prepared a sample of the data and saved it to s3 in Numpy <code>.npy</code> format. According to <a href=""https://sagemaker.readthedocs.io/en/stable/using_sklearn.html#sagemaker-scikit-learn-model-server"" rel=""noreferrer"">this documentation</a>, SageMaker Scikit-learn model server can deserialize NPY-formatted data (along with JSON and CSV data).<br>
The only difference with the previous experiment (2) is the argument <code>content_type='application/x-npy'</code> in <code>transformer.transform(...)</code>.   </p>

<p>This solution does not scale and we would like to pass a Scipy sparse matrix: </p>

<h3>4. Batch Transform of a big sparse matrix.</h3>

<p><strong>Here is the problem</strong><br>
SageMaker Python SDK does not support sparse matrix format out of the box.<br>
Following this:  </p>

<ul>
<li><a href=""https://aws.amazon.com/blogs/machine-learning/build-a-movie-recommender-with-factorization-machines-on-amazon-sagemaker/"" rel=""noreferrer"">https://aws.amazon.com/blogs/machine-learning/build-a-movie-recommender-with-factorization-machines-on-amazon-sagemaker/</a>    </li>
<li><a href=""https://stackoverflow.com/questions/55479366/errors-running-sagemaker-batch-transformation-with-lda-model"">Errors running Sagemaker Batch Transformation with LDA model</a>  </li>
</ul>

<p>I used <code>write_spmatrix_to_sparse_tensor</code> to write the data to protobuf format on s3. The function I used is:  </p>

<pre class=""lang-py prettyprint-override""><code>def write_protobuf(X_sparse, bucket, prefix, obj):
    """"""Write sparse matrix to protobuf format at location bucket/prefix/obj.""""""
    buf = io.BytesIO()
    write_spmatrix_to_sparse_tensor(file=buf, array=X_sparse, labels=None)
    buf.seek(0)
    key = '{}/{}'.format(prefix, obj)
    boto3.resource('s3').Bucket(bucket).Object(key).upload_fileobj(buf)
    return 's3://{}/{}'.format(bucket, key)
</code></pre>

<p>Then the code used for launching the batch transform job is:</p>

<pre class=""lang-py prettyprint-override""><code>sklearn_model = SKLearnModel(
    model_data=model_artifact_location_on_s3,
    entry_point='my_script.py',
    role=role,
    sagemaker_session=sagemaker_session)

transformer = sklearn_model.transformer(
   instance_count=1, 
   instance_type='ml.m4.xlarge', 
   max_payload=100)

transformer.transform(
   data=batch_data, 
   content_type='application/x-recordio-protobuf',
   split_type='RecordIO')   

print('Waiting for transform job: ' + transformer.latest_transform_job.job_name)
transformer.wait()
</code></pre>

<p>I get the following error:</p>

<pre><code>sagemaker_containers._errors.ClientError: Content type application/x-recordio-protobuf is not supported by this framework.
</code></pre>

<p><strong>Questions:</strong><br>
(Reference doc for Transformer: <a href=""https://sagemaker.readthedocs.io/en/stable/transformer.html"" rel=""noreferrer"">https://sagemaker.readthedocs.io/en/stable/transformer.html</a>) </p>

<ul>
<li>If <code>content_type='application/x-recordio-protobuf'</code> is not allowed, what should I use?</li>
<li>Is <code>split_type='RecordIO'</code> the proper setting in this context?   </li>
<li>Should I provide an <code>input_fn</code> function in my script to deserialize the data?</li>
<li>Is there another better approach to tackle this problem?</li>
</ul>",0,1,2019-10-16 09:56:46.713000 UTC,,,5,python|amazon-web-services|scikit-learn|sparse-matrix|amazon-sagemaker,834,2014-07-30 10:25:22.230000 UTC,2022-09-22 19:20:13.617000 UTC,Belgium,1714,133,6,307,,,,,,['amazon-sagemaker']
Authenticate Custom Training Job in Vertex AI with Service Account,"<p>I am trying to run a Custom Training Job to deploy my model in Vertex AI directly from a Jupyterlab. This Jupyterlab is instantiated from a Vertex AI Managed Notebook where I already specified the service account.</p>
<p>My aim is to deploy the training script that I specify to the method <code>CustomTrainingJob</code> directly from the cells of my notebook. This would be equivalent to pushing an image that contains my script to <strong>container registry</strong> and deploying the Training Job manually from the UI of Vertex AI (in this way, by specifying the service account, I was able to corectly deploy the training job). However, I need everything to be executed from the same notebook.</p>
<p>In order to specify the credentials to the <code>CustomTrainingJob</code> of aiplatform, I execute the following cell, where all variables are correctly set:</p>
<pre class=""lang-python prettyprint-override""><code>import google.auth
from google.cloud import aiplatform
from google.auth import impersonated_credentials

source_credentials = google.auth.default()
target_credentials = impersonated_credentials.Credentials(
source_credentials=source_credentials,
target_principal='SERVICE_ACCOUNT.iam.gserviceaccount.com',
target_scopes = ['https://www.googleapis.com/auth/cloud-platform'])

aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_NAME)

job = aiplatform.CustomTrainingJob(
    display_name=JOB_NAME,
    script_path=SCRIPT_PATH,
    container_uri=MODEL_TRAINING_IMAGE,
    credentials=target_credentials
)
</code></pre>
<p>When after the <code>job.run()</code> command is executed it seems that the credentials are not correctly set. In particular, the following error is returned:</p>
<pre class=""lang-python prettyprint-override""><code>/opt/conda/lib/python3.7/site-packages/google/auth/impersonated_credentials.py in _update_token(self, request)
    254 
    255         # Refresh our source credentials if it is not valid.
--&gt; 256         if not self._source_credentials.valid:
    257             self._source_credentials.refresh(request)
    258 

AttributeError: 'tuple' object has no attribute 'valid'
</code></pre>
<p>I also tried different ways to configure the credentials of my service account but none of them seem to work. In this case it looks like the tuple that contains the source credentials is missing the 'valid' attribute, even if the method <code>google.auth.default()</code> only returns two values.</p>",0,0,2022-05-20 09:20:32.043000 UTC,,,0,google-cloud-platform|service-accounts|google-cloud-vertex-ai,149,2022-03-24 16:46:24.823000 UTC,2022-09-24 21:22:42.907000 UTC,,23,5,0,2,,,,,,['google-cloud-vertex-ai']
How to specify a name for the output file of a SageMaker Batch Transform job?,"<p>I have a Batch Transform job set up in AWS SageMaker. Currently this uses some input data and a pre-trained model. The orchestration of the job is being done using the <code>boto3</code> python library from within a lambda.</p>

<p>Something I am having difficulty with is a good way to specify the name of the output file, in our case a <code>predictions.csv</code>. Ideally we would like to add a timestamp to this name.</p>

<p>First thing I tried was to apply a filename via a parameter to the <code>pandas.to_csv()</code> function. However making only this change SageMaker then fails with the following error:</p>

<blockquote>
  <p>TypeError: The view function did not return a valid response. The function either returned None or ended without a return statement.</p>
</blockquote>

<p>This is a pretty weird error, especially given the code change that causes it.</p>

<p>I have also tried applying a filename to the <code>output_path</code> parameter which is part of the <a href=""https://sagemaker.readthedocs.io/en/stable/transformer.html"" rel=""nofollow noreferrer"">SageMaker transformer</a> object. This is intended to only specify the S3 folder path and adding a filename at the end just causes a weirdly named s3 folder (e.g. <code>output/stillafolder.csv/predictions.csv</code>).</p>

<p>The only way in which I have found that allows me to change the output filename, is to change the input filename, as a behaviour I have observed (although I have not found any documentation on this) is that the output filename will by default match the input filename. </p>

<p>This isn't great for my current purposes though so any advice would be much appreciated!</p>",1,0,2019-11-05 15:58:34.403000 UTC,0.0,,2,python|pandas|amazon-sagemaker,1639,2015-09-22 14:32:49.407000 UTC,2022-09-04 14:53:27.090000 UTC,Belfast,1682,165,4,164,,,,,,['amazon-sagemaker']
Sagemaker not using GPU even though it is deployed in GPU instance,"<p>I was able to successfully deploy my object detection model into <strong>Sagemaker</strong> and get predictions from it but turns out the <strong>GPU</strong> is not being utilized when prediction are to be done.</p>

<p>I deployed my custom model using the code:</p>

<pre class=""lang-py prettyprint-override""><code>predictor = model.deploy(initial_instance_count=1, instance_type='ml.p2.xlarge')
</code></pre>

<p>Then, I get predictions by:</p>

<pre><code>results = predictor.predict(imageData)
</code></pre>

<p>This process takes around ~15+ seconds and when I check the utilization graph in endpoint dashboard I get it as:</p>

<p><a href=""https://i.stack.imgur.com/w8xKP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/w8xKP.png"" alt=""enter image description here""></a> </p>

<p>The GPU memory is being used but no peaks under utilizations. Can anyone guide me on what's happening here?</p>

<p><strong>Some more info:</strong></p>

<ul>
<li>Image used: 763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-inference:1.14-gpu</li>
<li>Instance used: ml.p2.xlarge</li>
</ul>

<blockquote>
  <p>Let me know if I need to provide some more info.</p>
</blockquote>",0,4,2019-12-20 07:46:22.453000 UTC,1.0,,2,python|amazon-web-services|tensorflow|amazon-sagemaker,900,2016-01-06 15:34:21.897000 UTC,2022-09-25 04:15:13.330000 UTC,"Kathmandu, Nepal",1991,925,54,869,,,,,,['amazon-sagemaker']
AWS Sagemaker ClientError: imread read blank (None) image for file: /opt/ml/input/data/train/image_directory/key_of_first_image.jpg,"<p>I am following this <a href=""https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/imageclassification_caltech/Image-classification-lst-format.ipynb"" rel=""nofollow noreferrer"">tutorial</a> on my own dataset, which is on S3 - both image files and .lst files.
The structure of the S3-bucket is:</p>

<pre><code>s3://{bucket_name}/image-classification/train -&gt; in here I have 2 'directories' with images of my 2 classes

s3://{bucket_name}/image-classification/train_lst

s3://{bucket_name}/image-classification/validation -&gt; in here I have 2 'directories' with images of my 2 classes

s3://{bucket_name}/image-classification/validation_lst
</code></pre>

<p>I have setup the IAM-role for Sagemaker to have access to the bucket, the bucket has 'sagemaker' in it's name.
When I finally run the script (almost identically as in the tutorial, I only need a session with a profile_name to access Sagemaker), it gives me the error above and ends with a failed job status. I can't find any solution on this. All idea's are welcome.</p>

<p>Thanks in advance.</p>",2,1,2019-11-22 10:51:22.120000 UTC,,2019-11-22 11:23:25.357000 UTC,2,amazon-web-services|amazon-sagemaker,630,2019-11-22 10:38:25.887000 UTC,2021-04-30 09:44:01.787000 UTC,,21,0,0,6,,,,,,['amazon-sagemaker']
Error on Spark MLFlow Model Registery using DataBricks after upgrade: cannot load trained XGBoost model,"<p>I had some code for training and then using XGBoost models on a Databricks environment. As my runtime version got deprecated, I upgraded it, but I quickly noticed I could not load my trained models anymore. The reason seems to be a change in the naming of functions in Sparkdl:</p>
<pre><code>Error loading metadata: Expected class name sparkdl.xgboost.xgboost_core.XgboostClassifierModel but found class name sparkdl.xgboost.xgboost.XgboostClassifierModel
</code></pre>
<p>Would anyone have advise on how to fix this issue? Maybe modify the metadata?</p>",0,0,2022-02-28 09:03:38.097000 UTC,,,1,pyspark|xgboost|azure-databricks|mlflow,111,2017-06-24 15:53:38.333000 UTC,2022-09-08 11:29:05.553000 UTC,,11,0,0,2,,,,,,['mlflow']
Microsoft Cognitive Services Speech SDK debugging problem,"<p>I'm trying to translate a bunch of audio files, which are stored on a local drive, into text.
Now when i do this on my local machine, it works fine. No problem. But when i try executing the same thing on my remote machine (it has the files stored on it, so I don't have to copy to my local machine and back), the service just returns empty files.</p>
<p>The code that I'm using is identical:</p>
<pre><code>import azure.cognitiveservices.speech as speechsdk
import time

import glob
import logging

# ...

# Acquire the logger for a library (azure.storage.blob in this example)
# logger = logging.getLogger('azure.cognitiveservices.speech')
logger = logging.getLogger('azure')

# Set the desired logging level
# logger.setLevel(logging.DEBUG)
logger.setLevel(logging.DEBUG)
handler = logging.FileHandler(filename=&quot;azure.log&quot;)
logger.addHandler(handler)

root = 'my/root/dir'
audio_root = root + '/audio'
translation_root = root + '/transcription'
list_of_wav_files = glob.glob(audio_root + '/*.wav')
list_of_text_files = glob.glob(translation_root + '/*.txt')
list_of_wav_files = [x[:-4].split('\\')[1] for x in list_of_wav_files]
list_of_text_files = [x[:-4].split('\\')[1] for x in list_of_text_files]

list_of_files_to_convert = set(list_of_wav_files) - set(list_of_text_files)

# Creates an instance of a speech config with specified subscription key and service region.
# Replace with your own subscription key and region identifier from here: https://aka.ms/speech/sdkregion
speech_key, service_region = &quot;key&quot;, &quot;westeurope&quot;

def speech_recognize_continuous_from_file(filename, custom=False):
    &quot;&quot;&quot;performs continuous speech recognition with input from an audio file&quot;&quot;&quot;
    # &lt;SpeechContinuousRecognitionWithFile&gt;
    speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=service_region)
    audio_config = speechsdk.audio.AudioConfig(filename=audio_root + '/' + filename + '.wav')
    if custom:
        speech_config.endpoint_id = &quot;my_custom_endpoint_id&quot;

    speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config, language=&quot;de-DE&quot;)

    if custom:
        outFile = open(translation_root + '/' + filename + '_custom_model.txt', 'w')
    else: 
         outFile = open(translation_root + '/' + filename + '.txt', 'w')
    
    done = False

    def stop_cb(evt):
        &quot;&quot;&quot;callback that signals to stop continuous recognition upon receiving an event `evt`&quot;&quot;&quot;
        print('CLOSING on {}'.format(evt))
        nonlocal done
        done = True
        outFile.close()

    # Connect callbacks to the events fired by the speech recognizer
    speech_recognizer.recognizing.connect(lambda evt: print('RECOGNIZING: {}'.format(evt)))
    speech_recognizer.recognized.connect(lambda evt: outFile.write('{} \n'.format(evt.result.text)))
    speech_recognizer.recognized.connect(lambda evt: print('RECOGNIZED: {}'.format(evt.result.text)))
    speech_recognizer.session_started.connect(lambda evt: print('SESSION STARTED: {}'.format(evt)))
    speech_recognizer.session_stopped.connect(lambda evt: print('SESSION STOPPED {}'.format(evt)))
    speech_recognizer.canceled.connect(lambda evt: print('CANCELED {}'.format(evt)))
    # stop continuous recognition on either session stopped or canceled events
    speech_recognizer.session_stopped.connect(stop_cb)
    speech_recognizer.canceled.connect(stop_cb)

    # Start continuous speech recognition
    speech_recognizer.start_continuous_recognition()
    while not done:
        time.sleep(.5)

    speech_recognizer.stop_continuous_recognition()
    # &lt;/SpeechContinuousRecognitionWithFile&gt;


for file in list_of_files_to_convert:
    print('Converting: ' + file)
    speech_recognize_continuous_from_file(file,  False)
    # speech_recognize_continuous_from_file(file, True)
</code></pre>
<p>Now when i execute on my local machine, this is the log, that i get:</p>
<pre><code>Converting: ID_1
SESSION STARTED: SessionEventArgs(session_id=123)
RECOGNIZING: SpeechRecognitionEventArgs(session_id=123, result=SpeechRecognitionResult(result_id=1212, text=&quot;sometext...&quot;, reason=ResultReason.RecognizingSpeech))
</code></pre>
<p>And so on, until it translated the file.</p>
<p>But when i execute the same code on the other machine, i get the following:</p>
<pre><code>Converting: ID_1
SESSION STARTED: SessionEventArgs(session_id=1234)
CANCELED SpeechRecognitionCanceledEventArgs(session_id=1234, result=SpeechRecognitionResult(result_id=112, text=&quot;&quot;, reason=ResultReason.Canceled))
CLOSING on SpeechRecognitionCanceledEventArgs(session_id=1234, result=SpeechRecognitionResult(result_id=112, text=&quot;&quot;, reason=ResultReason.Canceled))
SESSION STOPPED SessionEventArgs(session_id=1234)
CLOSING on SessionEventArgs(session_id=1234)
CANCELED SpeechRecognitionCanceledEventArgs(session_id=1234, result=SpeechRecognitionResult(result_id=13, text=&quot;&quot;, reason=ResultReason.Canceled))
CLOSING on SpeechRecognitionCanceledEventArgs(session_id=1234, result=SpeechRecognitionResult(result_id=13, text=&quot;&quot;, reason=ResultReason.Canceled))
</code></pre>
<p>The log file is also empty.
So i don't know how to debug this problem. My guess is, that the remote machine has a certain port, that needs to be opened. But i can't tell which one, because I don't know which port is being used by the script.
Although if i'm using the requests package, i can successfully call http and https adresses.</p>
<p>As a final note, because that might also lead to the issue:
The local machine is running Windows 10 and the remote Machine is running Windows Server 2016.</p>
<p>Any ideas on why this issue is occurring and how to better debug it, are very welcome.</p>",0,2,2021-07-16 12:29:56.020000 UTC,,,1,python-3.x|azure|debugging|logging|azure-machine-learning-service,201,2021-04-07 10:59:18.553000 UTC,2022-09-08 13:51:32.780000 UTC,,13,0,0,2,,,,,,['azure-machine-learning-service']
How to run data bricck notebook with mlflow in azure data factory pipeline?,"<p>My colleagues and I are facing an issue when trying to run my databricks notebook in Azure Data Factory and the error is coming from MLFlow.</p>
<p>The command that is failing is the following:</p>
<pre><code># Take the parent notebook path to use as path for the experiment
context = json.loads(dbutils.notebook.entry_point.getDbutils().notebook().getContext().toJson())
nb_base_path = context['extraContext']['notebook_path'][:-len(&quot;00_training_and_validation&quot;)]

experiment_path = nb_base_path + 'trainings'
mlflow.set_experiment(experiment_path)
experiment = mlflow.get_experiment_by_name(experiment_path)
experiment_id = experiment.experiment_id

run = mlflow.start_run(experiment_id=experiment_id, run_name=f&quot;run_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}&quot;)
</code></pre>
<p>And the error that is throwing is:</p>
<p>An exception was thrown from a UDF: 'mlflow.exceptions.RestException: INVALID_PARAMETER_VALUE: No experiment ID was specified. An experiment ID must be specified in Databricks Jobs and when logging to the MLflow server from outside the Databricks workspace. If using the Python fluent API, you can set an active experiment under which to create runs by calling mlflow.set_experiment(&quot;/path/to/experiment/in/workspace&quot;) at the start of your program.', from , line 32.</p>
<p>The pipeline just runs the notebook from ADF, it does not have any other step and the cluster we are using is type 7.3 ML.</p>
<p>Could you please help us?</p>
<p>Thank you in advance!</p>",1,0,2022-02-03 09:23:02.260000 UTC,,,0,azure-data-factory|databricks|mlflow,392,2020-04-08 07:25:08.360000 UTC,2022-02-03 09:43:32.287000 UTC,"Madrid, Spain",35,1,0,10,,,,,,['mlflow']
Install gym atari in Amazon SageMaker,"<p>I am trying to install Gym Atari environments on a Amazon SageMaker instance. I run the following script in a jupyter notebook:</p>
<pre><code>!python -m pip install --upgrade pip
!pip install gym[atari]
import urllib.request
urllib.request.urlretrieve('http://www.atarimania.com/roms/Roms.rar', 'Roms.rar')
urllib.request.urlretrieve('https://www.rarlab.com/rar/rarlinux-x64-6.0.2.tar.gz', 'rarlinux-x64-6.0.2.tar.gz')
!tar -xvzf rarlinux-x64-6.0.2.tar.gz
%cd rar
!make
!sudo make install
%cd ..
!unrar x Roms.rar
!mkdir rars
!mv HC\ ROMS.zip rars
!mv ROMS.zip rars
!python -m atari_py.import_roms rars
</code></pre>
<p>The script upgrades pip, install gym with the Atari dependencies, fetch the Roms, install unrar to extract the file Roms.rar and then run the command <code>python -m atari_py.import_roms rars</code>.</p>
<p>An issue arise when I run <code>python -m atari_py.import_roms rars</code>. The output is the following:</p>
<pre><code>rar/
rar/unrar
rar/acknow.txt
rar/whatsnew.txt
rar/order.htm
rar/readme.txt
rar/rar.txt
rar/makefile
rar/default.sfx
rar/rar
rar/rarfiles.lst
rar/license.txt
/home/ec2-user/SageMaker/rar
mkdir -p /usr/local/bin
mkdir -p /usr/local/lib
cp rar unrar /usr/local/bin
cp: cannot create regular file ‘/usr/local/bin/rar’: Permission denied
cp: cannot create regular file ‘/usr/local/bin/unrar’: Permission denied
make: *** [install] Error 1
mkdir -p /usr/local/bin
mkdir -p /usr/local/lib
cp rar unrar /usr/local/bin
cp rarfiles.lst /etc
cp default.sfx /usr/local/lib
/home/ec2-user/SageMaker

UNRAR 6.02 freeware      Copyright (c) 1993-2021 Alexander Roshal


Extracting from Roms.rar

Extracting  HC ROMS.zip                                                 36  OK 
Extracting  ROMS.zip                                                    7 99  OK 
All OK
copying adventure.bin from HC ROMS/BY ALPHABET (PAL)/A-G/Adventure (PAL).bin to /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p36/lib/python3.6/site-packages/atari_py/atari_roms/adventure.bin
Traceback (most recent call last):
  File &quot;/home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p36/lib/python3.6/runpy.py&quot;, line 193, in _run_module_as_main
    &quot;__main__&quot;, mod_spec)
  File &quot;/home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p36/lib/python3.6/runpy.py&quot;, line 85, in _run_code
    exec(code, run_globals)
  File &quot;/home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p36/lib/python3.6/site-packages/atari_py/import_roms.py&quot;, line 93, in &lt;module&gt;
    main()
  File &quot;/home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p36/lib/python3.6/site-packages/atari_py/import_roms.py&quot;, line 89, in main
    import_roms(args.dirpath)
  File &quot;/home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p36/lib/python3.6/site-packages/atari_py/import_roms.py&quot;, line 78, in import_roms
    _check_zipfile(f, save_if_matches)
  File &quot;/home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p36/lib/python3.6/site-packages/atari_py/import_roms.py&quot;, line 23, in _check_zipfile
    process_f(innerf)
  File &quot;/home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p36/lib/python3.6/site-packages/atari_py/import_roms.py&quot;, line 67, in save_if_matches
    f.seek(0)
io.UnsupportedOperation: seek
</code></pre>
<p>I did some research but found nothing. I would appreciate some help, thanks!</p>",1,1,2021-07-23 13:09:39.903000 UTC,,,0,python|reinforcement-learning|amazon-sagemaker|openai-gym,148,2018-10-15 21:39:49.447000 UTC,2022-08-16 13:04:04.833000 UTC,,21,0,0,8,,,,,,['amazon-sagemaker']
Sagemaker - batch transform] Internal server error : 500,"<p>I am trying to do a batch transform on a training dataset in an S3 bucket. I have followed this link:
<a href=""https://github.com/aws-samples/quicksight-sagemaker-integration-blog"" rel=""nofollow noreferrer"">https://github.com/aws-samples/quicksight-sagemaker-integration-blog</a></p>
<p>The training data on which transformation is being applied is of ~35 MB.
I am getting these errors:</p>
<ol>
<li>Bad HTTP status received from algorithm: 500</li>
<li>The server encountered an internal error and was unable to complete your request. Either the server is overloaded or there is an error in the application.</li>
</ol>
<p>Process followed:</p>
<pre><code>1. s3_input_train = sagemaker.TrainingInput(s3_data='s3://{}/{}/rawtrain/'.format(bucket, prefix), content_type='csv')

2. from sagemaker.sklearn.estimator import SKLearn
sagemaker_session = sagemaker.Session()
script_path = 'preprocessing.py'
sklearn_preprocessor = SKLearn(
entry_point=script_path,
role=role,
train_instance_type=&quot;ml.c4.xlarge&quot;,
framework_version='0.20.0',
py_version = 'py3',
sagemaker_session=sagemaker_session)
sklearn_preprocessor.fit({'train': s3_input_train})

3. transform_train_output_path = 's3://{}/{}/{}/'.format(bucket, prefix, 'transformtrain-train-output')
scikit_learn_inferencee_model = sklearn_preprocessor.create_model(env={'TRANSFORM_MODE': 'feature-transform'})
transformer_train = scikit_learn_inferencee_model.transformer(
instance_count=1,
assemble_with = 'Line',
output_path = transform_train_output_path,
accept = 'text/csv',
strategy = &quot;MultiRecord&quot;,
max_payload =40,
instance_type='ml.m4.xlarge')

4. Preprocess training input
transformer_train.transform(s3_input_train.config['DataSource']['S3DataSource']['S3Uri'], 
                            content_type='text/csv',
                            split_type = &quot;Line&quot;)
print('Waiting for transform job: ' + transformer_train.latest_transform_job.job_name)
transformer_train.wait()
preprocessed_train_path = transformer_train.output_path + transformer_train.latest_transform_job.job_name

preprocessing.py

from __future__ import print_function

import time
import sys
from io import StringIO
import os
import shutil

import argparse
import csv
import json
import numpy as np
import pandas as pd
import logging

from sklearn.compose import ColumnTransformer
from sklearn.externals import joblib
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import Binarizer, StandardScaler, OneHotEncoder

from sagemaker_containers.beta.framework import (
    content_types, encoders, env, modules, transformer, worker)

# Specifying the column names here.
feature_columns_names = [
    'A',
    'B',
    'C',
    'D',
    'E',
    'F',
    'G',
    'H',
    'I',
    'J',
    'K'
] 

label_column = 'ab'

feature_columns_dtype = {
    'A' :  str,
    'B' :  np.float64,
    'C' :  np.float64,
    'D' :  str,
    &quot;E&quot; :  np.float64,
    'F' :  str,
    'G' :  str,
    'H' :  np.float64,
    'I' :  str,
    'J' :  str,
    'K':  str,
}

label_column_dtype = {'ab': np.int32}  

def merge_two_dicts(x, y):
    z = x.copy()   # start with x's keys and values
    z.update(y)    # modifies z with y's keys and values &amp; returns None
    return z

def _is_inverse_label_transform():
    &quot;&quot;&quot;Returns True if if it's running in inverse label transform.&quot;&quot;&quot;
    return os.getenv('TRANSFORM_MODE') == 'inverse-label-transform'

def _is_feature_transform():
    &quot;&quot;&quot;Returns True if it's running in feature transform mode.&quot;&quot;&quot;
    return os.getenv('TRANSFORM_MODE') == 'feature-transform'


if __name__ == '__main__':

    parser = argparse.ArgumentParser()

    # Sagemaker specific arguments. Defaults are set in the environment variables.
    parser.add_argument('--output-data-dir', type=str, default=os.environ['SM_OUTPUT_DATA_DIR'])
    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])
    parser.add_argument('--train', type=str, default=os.environ['SM_CHANNEL_TRAIN'])


    args = parser.parse_args()

    input_files = [ os.path.join(args.train, file) for file in os.listdir(args.train) ]
    if len(input_files) == 0:
        raise ValueError(('There are no files in {}.\n' +
                          'This usually indicates that the channel ({}) was incorrectly specified,\n' +
                          'the data specification in S3 was incorrectly specified or the role specified\n' +
                          'does not have permission to access the data.').format(args.train, &quot;train&quot;))

    raw_data = [ pd.read_csv(
        file, 
        header=None,
        names=feature_columns_names + [label_column],
        dtype=merge_two_dicts(feature_columns_dtype, label_column_dtype)) for file in input_files ]
    concat_data = pd.concat(raw_data)
    
    numeric_features = list([
    'B',
    'C',
    'E',
    'H'
    ])


    numeric_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='median')),
        ('scaler', StandardScaler())])

    categorical_features = list(['A','D','F','G','I','J','K'])
    categorical_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
        ('onehot', OneHotEncoder(handle_unknown='ignore'))])

    preprocessor = ColumnTransformer(
        transformers=[
            ('num', numeric_transformer, numeric_features),
            ('cat', categorical_transformer, categorical_features)],
        remainder=&quot;drop&quot;)

    preprocessor.fit(concat_data)

    joblib.dump(preprocessor, os.path.join(args.model_dir, &quot;model.joblib&quot;))

    print(&quot;saved model!&quot;)
    
    
def input_fn(input_data, request_content_type):
    &quot;&quot;&quot;Parse input data payload
    
    We currently only take csv input. Since we need to process both labelled
    and unlabelled data we first determine whether the label column is present
    by looking at how many columns were provided.
    &quot;&quot;&quot;
    
    
    content_type = request_content_type.lower(
    ) if request_content_type else &quot;text/csv&quot;
    content_type = content_type.split(&quot;;&quot;)[0].strip()
    
    
    if isinstance(input_data, str):
        str_buffer = input_data
    else:
        str_buffer = str(input_data,'utf-8')
    

    if _is_feature_transform():
        if content_type == 'text/csv':
            # Read the raw input data as CSV.
            df = pd.read_csv(StringIO(input_data),  header=None)
            if len(df.columns) == len(feature_columns_names) + 1:
                # This is a labelled example, includes the  label
                df.columns = feature_columns_names + [label_column]
            elif len(df.columns) == len(feature_columns_names):
                # This is an unlabelled example.
                df.columns = feature_columns_names
            return df
        else:
            raise ValueError(&quot;{} not supported by script!&quot;.format(content_type))
    
    
    if _is_inverse_label_transform():
        if (content_type == 'text/csv' or content_type == 'text/csv; charset=utf-8'):
            # Read the raw input data as CSV.
            df = pd.read_csv(StringIO(str_buffer),  header=None)
            if len(df.columns) == len(feature_columns_names) + 1:
            # This is a labelled example, includes the ring label
               df.columns = feature_columns_names + [label_column]
            elif len(df.columns) == len(feature_columns_names):
            # This is an unlabelled example.
               df.columns = feature_columns_names
            return df
        else:
            raise ValueError(&quot;{} not supported by script!&quot;.format(content_type))
            
            
def output_fn(prediction, accept):
    &quot;&quot;&quot;Format prediction output
    
    The default accept/content-type between containers for serial inference is JSON.
    We also want to set the ContentType or mimetype as the same value as accept so the next
    container can read the response payload correctly.
    &quot;&quot;&quot;
    
    accept = 'text/csv'
    if type(prediction) is not np.ndarray:
        prediction=prediction.toarray()
    
   
    if accept == &quot;application/json&quot;:
        instances = []
        for row in prediction.tolist():
            instances.append({&quot;features&quot;: row})

        json_output = {&quot;instances&quot;: instances}

        return worker.Response(json.dumps(json_output), mimetype=accept)
    elif accept == 'text/csv':
        return worker.Response(encoders.encode(prediction, accept), mimetype=accept)
    else:
        raise RuntimeException(&quot;{} accept type is not supported by this script.&quot;.format(accept))


def predict_fn(input_data, model):
    &quot;&quot;&quot;Preprocess input data
    
    We implement this because the default predict_fn uses .predict(), but our model is a preprocessor
    so we want to use .transform().

    The output is returned in the following order:
    
        rest of features either one hot encoded or standardized
    &quot;&quot;&quot;

    
    if _is_feature_transform():
        features = model.transform(input_data)


        if label_column in input_data:
            # Return the label (as the first column) and the set of features.
            return np.insert(features.toarray(), 0, pd.get_dummies(input_data[label_column])['True.'], axis=1)
        else:
            # Return only the set of features
            return features
    
    if _is_inverse_label_transform():
        features = input_data.iloc[:,0]&gt;0.5
        features = features.values
        return features
    

def model_fn(model_dir):
    &quot;&quot;&quot;Deserialize fitted model
    &quot;&quot;&quot;
    if _is_feature_transform():
        preprocessor = joblib.load(os.path.join(model_dir, &quot;model.joblib&quot;))
        return preprocessor
</code></pre>
<p>Please help.</p>",1,0,2021-06-30 10:00:57.630000 UTC,,,0,amazon-sagemaker,596,2018-03-09 07:03:25.870000 UTC,2022-09-23 17:17:54.453000 UTC,"Cebu, Philippines",1,0,0,16,,,,,,['amazon-sagemaker']
Retrieve model endpoint given a Model Package Group in SageMaker?,"<p>Given a Model Package Group name, and a Model Package, I want to implement an API that returns me the endpoint for the latest version of that Model Package.</p>
<p>What's the API in Java SDK that for a given Model Package and a specific version I get the correspondent endpoint?</p>
<p><strong>Clarification update:</strong> the model package (with its version) have already been deployed. The purpose of the API is to retrieve the <em>already existing</em> endpoint, and I don't know how to do this in Java.</p>
<p>In Python it would be something like:</p>
<pre><code>model_artifact_summary = list(Artifact.list(source_uri=model_package_arn))[0]
model_artifact = ModelArtifact.load(artifact_arn=model_artifact_summary.artifact_arn)
endpoints = model_artifact.endpoint_contexts()
</code></pre>
<p>But I can't figure out the Java equivalent.</p>",1,0,2022-03-17 11:00:23.397000 UTC,,2022-03-18 09:52:32.247000 UTC,1,amazon-web-services|amazon-sagemaker,259,2015-01-21 21:45:58.933000 UTC,2022-09-23 08:24:22.427000 UTC,Italy,6098,1155,8,853,,,,,,['amazon-sagemaker']
How can I load and deploy a pre-trained AWS Sagemaker XGBoost model on local machine?,"<p>I've trained a Sagemaker XGBoost model and downloaded the model.tar.gz file from S3 onto my local machine. How can I load this model for deploying it using flask?</p>

<p>I've tried using pickle to load the unzipped model file but it doesn't seem to work.</p>

<pre><code>import sagemaker
import boto3
import os
import pickle

with open('xgboost-model', 'r') as inp:
   cls.model = pkl.load(inp)
</code></pre>

<p>Traceback (most recent call last):
  File """", line 2, in 
  File ""C:\Anaconda3\lib\encodings\cp1252.py"", line 23, in decode
    return codecs.charmap_decode(input,self.errors,decoding_table)[0]
UnicodeDecodeError: 'charmap' codec can't decode byte 0x81 in position 969: character maps to </p>",1,0,2019-01-10 00:03:40.773000 UTC,1.0,,4,machine-learning|xgboost|amazon-sagemaker,3577,2012-06-04 21:07:02.563000 UTC,2021-01-22 16:23:41.947000 UTC,,355,4,0,44,,,,,,['amazon-sagemaker']
How can I install the R kernel on an AWS SageMaker notebook instance without internet access?,"<p>I am trying to install the R kernel on an AWS SageMaker notebook instance that has no internet access. AWS provides documentation on how to install the R kernel on a notebook instance with internet access <a href=""https://github.com/awslabs/amazon-sagemaker-examples/blob/master/advanced_functionality/install_r_kernel/install_r_kernel.ipynb"" rel=""nofollow noreferrer"">here</a>. However, I am trying to install the R kernel on a notebook instance without internet access. Their <a href=""https://github.com/awslabs/amazon-sagemaker-examples/blob/master/advanced_functionality/install_r_kernel/install_r_kernel.ipynb"" rel=""nofollow noreferrer"">documentation</a> states that r-essentials must be installed. I've tried downloading/installing the r-essentials tar.bz2 archive from the <a href=""https://anaconda.org/r/r-essentials/files"" rel=""nofollow noreferrer"">Anaconda package repository</a>. However, it seems like the r-essentials is simply a metapackage and does not contain the actual packages that are to be installed. </p>

<p>Installing r-essentials using the tar.bz2 archive:
<a href=""https://i.stack.imgur.com/qzkEk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qzkEk.png"" alt=""install r-essentials via tar.bz2 archive""></a></p>

<p>Checking if the R kernel exists in the notebook dropdown:
<a href=""https://i.stack.imgur.com/pcjxB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pcjxB.png"" alt=""Notebook drop down""></a></p>

<p>Is there a workaround for this? The goal is to get the R kernel on the notebook instance. Thanks in advance!</p>",1,0,2019-03-10 17:17:18.303000 UTC,,,0,r|amazon-web-services|jupyter-notebook|conda|amazon-sagemaker,939,2018-04-19 04:36:51.863000 UTC,2021-08-12 14:14:37.360000 UTC,,355,5,0,80,,,,,,['amazon-sagemaker']
Can't read from tfrecords in S3 from notebook instance,"<p>I try to read from tfrecords in S3 from a Sage Maker notebook instance following instructions here: <a href=""https://www.tensorflow.org/versions/master/deploy/s3"" rel=""nofollow noreferrer"">https://www.tensorflow.org/versions/master/deploy/s3</a></p>

<pre><code>import tensorflow as tf
import os
os.environ['AWS_ACCESS_KEY_ID'] = '&lt;my-key&gt;'
os.environ['AWS_SECRET_ACCESS_KEY'] = '&lt;my-secret&gt;'

from tensorflow.python.lib.io import file_io
print(file_io.stat('s3://&lt;my-bucket&gt;/data/DEMO-mnist/train.tfrecords'))
</code></pre>

<p>The above code fails with the error:</p>

<pre><code>---------------------------------------------------------------------------
NotFoundError                             Traceback (most recent call last)
&lt;ipython-input-7-770c0aef6d7b&gt; in &lt;module&gt;()
      1 from tensorflow.python.lib.io import file_io
----&gt; 2 print(file_io.stat('s3://&lt;my-bucket&gt;/data/DEMO-mnist/train.tfrecords'))

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/lib/io/file_io.py in stat(filename)
    551   with errors.raise_exception_on_not_ok_status() as status:
    552     pywrap_tensorflow.Stat(compat.as_bytes(filename), file_statistics, status)
--&gt; 553     return file_statistics

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py in __exit__(self, type_arg, value_arg, traceback_arg)
    517             None, None,
    518             compat.as_text(c_api.TF_Message(self.status.status)),
--&gt; 519             c_api.TF_GetCode(self.status.status))
    520     # Delete the underlying status object from memory otherwise it stays alive
    521     # as there is a reference to status from this from the traceback due to

NotFoundError: Object s3://&lt;my-bucket&gt;/data/DEMO-mnist/train.tfrecords does not exist
</code></pre>

<p>However the same code works fine if I run from a regular EC2 instance without using SageMaker.</p>

<p>IAM role used for the notebook instance has full S3 access.</p>",1,1,2018-07-14 16:31:33.973000 UTC,,2018-07-31 09:54:28.423000 UTC,0,amazon-web-services|tensorflow|amazon-s3|amazon-sagemaker,814,2009-06-17 05:10:17.663000 UTC,2022-04-22 22:18:06.087000 UTC,"San Francisco Bay Area, CA, United States",1417,73,1,192,,,,,,['amazon-sagemaker']
How to send numpy array to sagemaker endpoint using lambda function,"<p>How to invoke sagemaker endpoint with input data type <code>numpy.ndarray</code>.
I have deployed a sagemaker model and trying to hit it using lambda function.
But I am unable to figure out how to do it. I am getting server error.</p>

<p>One row of the Input data. 
The total data set has <code>shape=(91,5,12)</code>.
The below is only one row of Input data.</p>

<pre><code>array([[[0.30440741, 0.30209799, 0.33520652, 0.41558442, 0.69096432,
         0.69611016, 0.25153326, 0.98333333, 0.82352941, 0.77187154,
         0.7664042 , 0.74468085],
        [0.30894981, 0.33151662, 0.22907725, 0.46753247, 0.69437367,
         0.70410559, 0.29259044, 0.9       , 0.80882353, 0.79401993,
         0.89501312, 0.86997636],
        [0.33511896, 0.34338939, 0.24065546, 0.48051948, 0.70384005,
         0.71058715, 0.31031288, 0.86666667, 0.89705882, 0.82724252,
         0.92650919, 0.89125296],
        [0.34617355, 0.36150251, 0.23726854, 0.54545455, 0.71368726,
         0.71703244, 0.30228356, 0.85      , 0.86764706, 0.86157254,
         0.97112861, 0.94089835],
        [0.36269508, 0.35923332, 0.40285461, 0.62337662, 0.73325475,
         0.7274392 , 0.26241391, 0.85      , 0.82352941, 0.89922481,
         0.9343832 , 0.90780142]]])
</code></pre>

<p>I am using the following code but unable to invoke the endpoint</p>

<pre><code>import boto3
def lambda_handler(event, context):
    # The SageMaker runtime is what allows us to invoke the endpoint that we've created.
    runtime = boto3.Session().client('sagemaker-runtime')

    endpoint = 'sagemaker-tensorflow-2019-04-22-07-16-51-717'

    print('givendata ', event['body'])
    # data = numpy.array([numpy.array(xi) for xi in event['body']])
    data = event['body']
    print('numpy array ', data)

    # Now we use the SageMaker runtime to invoke our endpoint, sending the review we were given
    response = runtime.invoke_endpoint(EndpointName = endpoint,# The name of the endpoint we created
                                       ContentType = 'application/json',                 # The data format that is expected
                                       Body = data) # The actual review

    # The response is an HTTP response whose body contains the result of our inference
    result = response['Body'].read().decode('utf-8')

    print('response', result)

    # Round the result so that our web app only gets '1' or '0' as a response.
    result = round(float(result))

    return {
        'statusCode' : 200,
        'headers' : { 'Content-Type' : 'text/plain', 'Access-Control-Allow-Origin' : '*' },
        'body' : str(result)
    }
</code></pre>

<p>I am unable to figure out what should be written in place of ContentType.
Because I am not aware of MIME type in case of <code>numpy.ndarray</code>.</p>",2,0,2019-04-22 07:54:51.650000 UTC,3.0,2019-04-22 09:23:42.110000 UTC,4,numpy|aws-lambda|python-3.6|numpy-ndarray|amazon-sagemaker,5236,2016-12-13 16:19:15.277000 UTC,2022-09-20 10:40:10.673000 UTC,,114,1,0,17,,,,,,['amazon-sagemaker']
Sagemaker Error for HyperParameterTuning job,"<p>I checked the aws forums and here at SO and I can't find a solution to this error or an explanation to what it is.</p>

<p>The full error is:</p>

<pre><code>UnexpectedStatusException: Error for HyperParameterTuning job sagemaker-xgboost-200330-1544: Failed. Reason: No objective metrics found after running 5 training jobs. Please ensure that the custom algorithm is emitting the objective metric as defined by the regular expression provided.

</code></pre>

<p>and it is respective to the following code:</p>

<pre><code>xgbt = sagemaker.estimator.Estimator(container, #name of training container
                                    role, # IAM role to use
                                    train_instance_count = 1, # number of instances yo use for training
                                    train_instance_type = 'ml.m4.xlarge', #type of virtual machine to use
                                    output_path = 's3://{}/{}/output'.format(session.default_bucket(),
                                                                            prefix),
                                    sagemaker_session = session) #current sagemaker session


xgbt.set_hyperparameters(
    max_depth = 5,
    eta = 0.1,
    eval_metric='auc',
    objective='binary:logistic',
    early_stopping_rounds=500,
    rate_drop=0.1,
    colsample_bytree=0.8,
    subsample=0.75,
    min_child_weight=0,
    num_round = 500)


xgbt.fit({'train': s3_input_train})

</code></pre>

<p>and then the hyperparameter tuning is where it croaks:</p>

<pre><code>from sagemaker.tuner import IntegerParameter, ContinuousParameter, HyperparameterTuner

xgb_hyperparameter_tuner = HyperparameterTuner(estimator = xgbt, # The estimator object to use as the basis for the training jobs.
                                               objective_metric_name = 'validation:auc', # The metric used to compare trained models.
                                               objective_type = 'Maximize', # Whether we wish to minimize or maximize the metric.
                                               max_jobs = 20, # The total number of models to train
                                               max_parallel_jobs = 3, # The number of models to train in parallel
                                               hyperparameter_ranges = {
                                                    'max_depth': IntegerParameter(3, 12),
                                                    'eta'      : ContinuousParameter(0.01, 0.5),
                                                    'min_child_weight': IntegerParameter(2, 8),
                                                    'subsample': ContinuousParameter(0.5, 0.9),
                                                    'gamma': ContinuousParameter(0, 10),
                                               })

xgb_hyperparameter_tuner.fit({'train': s3_input_train})

xgb_hyperparameter_tuner.wait()

</code></pre>

<p>I think I have the metrics correctly defined so I don't know what it wants from me.</p>

<p>Thank you so much for checking this out.</p>",0,2,2020-03-30 15:59:05.573000 UTC,,,0,python-3.x|xgboost|amazon-sagemaker,1313,2018-02-06 15:55:07.093000 UTC,2022-03-31 15:50:16.033000 UTC,,331,14,0,73,,,,,,['amazon-sagemaker']
How to mock async operation in flink with completable future,"<p>I'm trying to test my <code>RichAsyncFunction</code> impl, but I have encountered the following issues. The main flow is to invoke an API call on the client, but I am getting a serialization error when I mock the response to be a completable future and test the step function with.</p>
<pre><code>DataStream&lt;MyObject&gt; resultStream = AsyncDataStream.unorderedWait(stream,
        new MyAsyncStep(), timeout, TimeUnit.MILLISECONDS, capacity);
</code></pre>
<p>I know the completable future itself is non-serializable, but <code>MyAsyncStep</code> works fine when I run it in the IDE or flink cluster and failed when I run the test case. In the <code>asyncInvoke</code> methods I have</p>
<pre><code>List&lt;CompletableFuture&gt; completableFutureList = getCompletableFutureList(client, input);
</code></pre>
<p>Anyone knows how to mock the <code>getCompletableFutureList</code>? currently it gave me <code>getCompletableFutureList not serializable. The object probably contains or references non serializable fields.</code></p>",0,1,2021-11-01 19:25:17.227000 UTC,,,0,java|apache-flink|flink-streaming|amazon-sagemaker|completable-future,130,2015-11-07 21:57:38.500000 UTC,2022-09-23 23:36:02.777000 UTC,,190,19,1,45,,,,,,['amazon-sagemaker']
"Can we use data directly from RDS or df as a data source for training job in Sagemaker, rather than pulling it from from s3 or EFS?","<p>I am using Sagemaker platform for model development and deployment. Data is read from RDS tables and then spitted to train and test df.
To create the training job in Sagemaker, I found that it takes data source only as s3 and EFS. For that I need to keep train and test data back to s3, which is repeating the data storing process in RDS and s3.
I would want to directly pass the df from RDS as a parameter in tarining job code. Is there any way we can pass df in fit method</p>
<pre><code>    image=&quot;581132636225.dkr.ecr.ap-south-1.amazonaws.com/sagemaker-ols-model:latest&quot;
    model_output_folder = &quot;model-output&quot;
    print(image)
    tree = sagemaker.estimator.Estimator(
        image,
        role,
        1,
        &quot;ml.c4.2xlarge&quot;,
        output_path=&quot;s3://{}/{}&quot;.format(sess.default_bucket(), model_output_folder),
        sagemaker_session=sess,
    )

**tree.fit({'train': &quot;s3_path_having_test_data&quot;}, wait=True)**
</code></pre>",1,2,2022-01-04 11:35:23.900000 UTC,,2022-01-04 17:22:12.063000 UTC,2,python|amazon-web-services|data-science|amazon-sagemaker,160,2018-12-27 10:32:44.167000 UTC,2022-02-02 07:38:56.800000 UTC,"Bangalore, Karnataka, India",21,0,0,7,,,,,,['amazon-sagemaker']
Use an Azure ML compute cluster to run Kedro + Mlflow pipeline,"<p>I want to use an Azure Machine Learning compute cluster as a compute target to run a Kedro pipeline integrated with Mlflow.</p>
<p>Here's the code snippet (hooks.py) that integrates experiment tracking using Mlflow and Azure ML as backend/artifact stores.</p>
<pre><code>&quot;&quot;&quot;Project hooks.&quot;&quot;&quot;
from typing import Any, Dict, Iterable, Optional
import git
import os
import mlflow
import mlflow.sklearn
from kedro.config import ConfigLoader
from kedro.framework.hooks import hook_impl
from kedro.io import DataCatalog
from kedro.pipeline.node import Node
from kedro.versioning import Journal
from azureml.core import Workspace
from azureml.core.experiment import Experiment

class ProjectHooks:
    @hook_impl
    def register_config_loader(
        self,
        conf_paths: Iterable[str],
        env: str,
        extra_params: Dict[str, Any],
    ) -&gt; ConfigLoader:
        return ConfigLoader(conf_paths)

    @hook_impl
    def register_catalog(
        self,
        catalog: Optional[Dict[str, Dict[str, Any]]],
        credentials: Dict[str, Dict[str, Any]],
        load_versions: Dict[str, str],
        save_version: str,
        journal: Journal,
    ) -&gt; DataCatalog:
        return DataCatalog.from_config(
            catalog, credentials, load_versions, save_version, journal
        )


class ModelTrackingHooks:
    &quot;&quot;&quot;Namespace for grouping all model-tracking hooks with MLflow together.&quot;&quot;&quot;

    @hook_impl
    def before_pipeline_run(self, run_params: Dict[str, Any]) -&gt; None:
        &quot;&quot;&quot;Hook implementation to start an MLflow run
        with the same run_id as the Kedro pipeline run.
        &quot;&quot;&quot;

        # Get Azure workspace
        ws = Workspace.get(name=workspace_name,
                           subscription_id=subscription_id,
                           resource_group=resource_group)

        # Set tracking uri
        mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())

        # Create an Azure ML experiment in the workspace
        experiment = Experiment(workspace=ws, name='kedro-mlflow-experiment')
        mlflow.set_experiment(experiment.name)

        mlflow.start_run(run_name=run_params[&quot;run_id&quot;])
        mlflow.log_params(run_params)

    @hook_impl
    def after_node_run(
        self, node: Node, outputs: Dict[str, Any], inputs: Dict[str, Any]
    ) -&gt; None:
        &quot;&quot;&quot;Hook implementation to add model tracking after some node runs.
        In this example, we will:
        * Log the parameters after the data splitting node runs.
        * Log the model after the model training node runs.
        * Log the model's metrics after the model evaluating node runs.
        &quot;&quot;&quot;
        if node._func_name == &quot;function_name&quot;:
            mlflow.log_metrics(...)

    @hook_impl
    def after_pipeline_run(self) -&gt; None:
        &quot;&quot;&quot;Hook implementation to end the MLflow run
        after the Kedro pipeline finishes.
        &quot;&quot;&quot;
        mlflow.end_run()
</code></pre>
<p>This works well on a <strong>compute instance</strong> that I created in my Azure ML workspace, simply by doing the following :</p>
<ol>
<li><code>git clone</code> the source code into the Azure ML compute instance</li>
<li>Do a <code>kedro run</code> in the compute instance Terminal</li>
</ol>
<p>That's ok but what I really want is to use <strong>compute clusters</strong> to deal with hyperparameter tuning and other heavy workloads... I Just want to mention here that I still want to git clone to the compute instance and submit the run to the compute cluster from within the compute instance (but if anyone has a better approach, please feel free to share).</p>
<p>I know of two ways (listed below) to specify a compute cluster as a compute target in Azure ML but both require to pass a <code>script</code> parameter.</p>
<ol>
<li>Pure Azure ML <code>ScriptRunConfig()</code> method to submit experiments by specifying <code>script</code> and <code>compute_target</code> parameters. See <a href=""https://docs.microsoft.com/en-us/azure/machine-learning/how-to-log-view-metrics"" rel=""nofollow noreferrer"">Submit remote run with Azure Ml</a></li>
<li>Mlflow integration with Azure ML : that requires to add an MLproject file to the project folder. See <a href=""https://docs.microsoft.com/en-us/azure/machine-learning/how-to-train-mlflow-projects"" rel=""nofollow noreferrer"">Submit an mlflow project run</a>.</li>
</ol>
<p>I tried for quite some time now to figure out how to do that within the Kedro structure but without success. So my question here, what's the best way to push experiment runs in a Kedro Pipeline to Azure ML compute clusters?</p>
<p>Thank you in advance for your help !</p>",0,1,2021-12-13 17:56:35.970000 UTC,1.0,2021-12-14 08:40:44.520000 UTC,1,python|azure|mlflow|azure-machine-learning-service|kedro,271,2020-04-10 11:23:52.390000 UTC,2022-08-12 18:19:53.330000 UTC,,127,8,0,20,,,,,,"['mlflow', 'azure-machine-learning-service']"
AWS Sagemaker Kernel appears to have died and restarts,"<p>I am getting a kernel error while trying to retrieve the data from an API that includes 100 pages. The data size is huge but the code runs well when executed on Google Colab or on local machine.</p>

<p>The error I see in a window is-</p>

<p><strong>Kernel Restarting
The kernel appears to have died. It will restart automatically.</strong></p>

<p>I am using an ml.m5.xlarge machine with a memory allocation of 1000GB and there are no pre-saved datasets in the instance. Also, the expected data size is around 60 GB split into multiple datasets of 4 GB each.</p>

<p>Can anyone help?</p>",1,0,2020-03-21 07:12:39.840000 UTC,,,1,python-3.x|amazon-sagemaker,1026,2019-10-04 04:37:35.220000 UTC,2020-04-19 18:01:54.867000 UTC,"Mumbai, Maharashtra, India",11,0,0,2,,,,,,['amazon-sagemaker']
Batch Prediction with scikit-learn using jsonl in Vertex AI,"<p>I have a scikit-learn model successfully trained and loaded onto Vertex AI, but I can't seem to do batch prediction with jsonl. I've tried using these formats with jsonl:</p>
<pre><code>{&quot;dense_input&quot;: [1, 2, 3, ...]}
{&quot;dense_input&quot;: [4, 5, 6, ...]}
</code></pre>
<p>and</p>
<pre><code>{&quot;val_1&quot;: 1, &quot;val_2&quot;: 2, ...}
{&quot;val_1&quot;: 4, &quot;val_2&quot;: 5, ...}
</code></pre>
<p>but I get this error for both:</p>
<blockquote>
<p>('Post request fails. Cannot get predictions. Error: Predictions are not in the response. Got: {&quot;error&quot;: &quot;Prediction failed: Exception during sklearn prediction: float() argument must be a string or a number, not 'dict'&quot;}.', 2)</p>
</blockquote>
<p>I've tried batch prediction using a CSV file and it works fine, but I'm having difficulty with the jsonl file. Does anyone know what's the problem? Thanks!</p>",0,0,2022-07-18 15:56:45.463000 UTC,,,1,json|google-cloud-platform|scikit-learn|google-cloud-ml|google-cloud-vertex-ai,73,2022-07-18 15:32:34.550000 UTC,2022-09-18 18:25:28.147000 UTC,,11,0,0,2,,,,,,['google-cloud-vertex-ai']
Security Group settings for using sagemaker notebooks in private subnet,"<p>I am new to sagemaker, and am hoping to use sagemaker in a VPC with a private subnet, so data accessed from s3 is not exposed to public internet.</p>
<p>I have created a vpc with a private subnet (no internet or nat gateway), and have attached a vpc s3 gateway endpoint - with this, can I apply the subnet's <strong>default</strong> security group settings to the sagemaker notebook instances? ..or are some additional configurations to this required?</p>
<p>Also, I'm hoping to keep internet access for the sagemaker notebook instance, so I can still download python packages (but just wanting to ensure data read from s3 using the private subnet is all okay with its default security group)</p>
<p>Thank you</p>",0,2,2021-11-29 02:25:15.770000 UTC,,2021-11-29 09:21:59.560000 UTC,0,amazon-web-services|amazon-vpc|amazon-sagemaker|aws-security-group|private-subnet,919,2017-10-24 09:51:42.423000 UTC,2022-03-27 07:45:15.880000 UTC,,89,7,0,21,,,,,,['amazon-sagemaker']
Model is missing inputs ['_automl_sample_weight_ac91'],"<p>I am running Azure Databricks AutoML using 12.2 cluster that claims handling the imbalanced dataset. Although, it is not in my input, this cluster adds extra columns as _automl_sample_weight_ac91, _automl_split_col_636d. It drops_automl_split_col_636d column before training, but keeps the _automl_sample_weight_ac91 column. After registering the model, I am trying to predict very separate dataset than the one I used on training...
Apparently, registered automl model adds _automl_sample_weight_ac91 as column, but my very separate test data frame  has not sample weight column, thus gives the following error during prediction:</p>
<p>PythonException: 'mlflow.exceptions. MlflowException: Model is missing inputs ['_automl_sample_weight_ac91'].'. Full traceback below:</p>
<p>How can I solve this error related to _automl_sample_weight_ac91)?</p>
<p><a href=""https://i.stack.imgur.com/CJm9Q.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>Thanks</p>",0,0,2022-09-12 16:17:00.610000 UTC,,2022-09-12 16:19:30.557000 UTC,0,mlflow|automl,14,2020-10-19 14:59:15.243000 UTC,2022-09-12 20:18:40.110000 UTC,"Charlotte, NC, USA",1,0,0,2,,,,,,['mlflow']
NVMe SSD disk on AWS Sagemaker Notebook Instances,"<p>Sagemake Studio added support for instances with attached high-performance SSDs. Among others, the M5d instance features an attached SSD. (See, for instance here <a href=""https://aws.amazon.com/about-aws/whats-new/2021/08/amazon-sagemaker-supports-m5d-r5-p3dn-g4dn-instances-sagemaker-notebook-instances/"" rel=""nofollow noreferrer"">AWS Blog: Amazon SageMaker now supports M5d, R5, P3dn, and G4dn instances for SageMaker Notebook Instances</a>)</p>
<p>My question is, how do I make use of the SSD?</p>
<p>The block device is not mounted when I start a Notebook with a Kernel on an M5d instance.
Below is a listing of the mount command form inside the notebook. I can see it when executing <code>lsblk</code>. (output is also below). However, there is no way I can format or mount the disk as the environment is restricted by Linux capabilities.</p>
<pre><code>!mount

overlay on / type overlay (rw,relatime,lowerdir=/var/lib/dataroot/data-root/200010.1001/overlay2/l/JGWGMF4ZZ6PHEWK6JM4VICT7E4:/var/lib/dataroot/data-root/200010.1001/overlay2/l/66JWDG6GVXACNLSFJJBEOVZUO4:/var/lib/dataroot/data-root/200010.1001/overlay2/l/3U4YS24OOOKWXXZ7GZ3N4YOJZU:/var/lib/dataroot/data-root/200010.1001/overlay2/l/XOM2F2QYQYJMEUDYRFWJF637G2:/var/lib/dataroot/data-root/200010.1001/overlay2/l/4SSLKXEKJRW2M3MDUY6GRWQZOR:/var/lib/dataroot/data-root/200010.1001/overlay2/l/MDSB4ZKIAUXBZ3U5S4YM4DDOLQ:/var/lib/dataroot/data-root/200010.1001/overlay2/l/NE42TLPPST4P26YJZXL7KQYD5V:/var/lib/dataroot/data-root/200010.1001/overlay2/l/ZQA5ST7XAT673MNTWDL4YWBHLL:/var/lib/dataroot/data-root/200010.1001/overlay2/l/UZYHAWYHUINESI25LGXT5V2RHY:/var/lib/dataroot/data-root/200010.1001/overlay2/l/QECAGRX3FEW5FAJXK5DACNEFKA:/var/lib/dataroot/data-root/200010.1001/overlay2/l/IBZBCWE3M7DNAUXEOAHQUCM5HG:/var/lib/dataroot/data-root/200010.1001/overlay2/l/VQMIS7KW6ZB5VOQNKQE3J2RF4J:/var/lib/dataroot/data-root/200010.1001/overlay2/l/UOWDHH53G2E442U6NNUMBITSRJ,upperdir=/var/lib/dataroot/data-root/200010.1001/overlay2/9b5b78d144189e2437ee3a1716d2215d6a101155c2b35f9e8c0c3b1004b627fd/diff,workdir=/var/lib/dataroot/data-root/200010.1001/overlay2/9b5b78d144189e2437ee3a1716d2215d6a101155c2b35f9e8c0c3b1004b627fd/work)
proc on /proc type proc (rw,nosuid,nodev,noexec,relatime)
tmpfs on /dev type tmpfs (rw,nosuid,size=65536k,mode=755,uid=200010,gid=1001)
devpts on /dev/pts type devpts (rw,nosuid,noexec,relatime,gid=10000004,mode=620,ptmxmode=666)
sysfs on /sys type sysfs (ro,nosuid,nodev,noexec,relatime)
tmpfs on /sys/fs/cgroup type tmpfs (rw,nosuid,nodev,noexec,relatime,mode=755,uid=200010,gid=1001)
cgroup on /sys/fs/cgroup/systemd type cgroup (rw,nosuid,nodev,noexec,relatime,xattr,release_agent=/usr/lib/systemd/systemd-cgroups-agent,name=systemd)
cgroup on /sys/fs/cgroup/blkio type cgroup (rw,nosuid,nodev,noexec,relatime,blkio)
cgroup on /sys/fs/cgroup/net_cls,net_prio type cgroup (rw,nosuid,nodev,noexec,relatime,net_cls,net_prio)
cgroup on /sys/fs/cgroup/cpuset type cgroup (rw,nosuid,nodev,noexec,relatime,cpuset)
cgroup on /sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,memory)
cgroup on /sys/fs/cgroup/perf_event type cgroup (rw,nosuid,nodev,noexec,relatime,perf_event)
cgroup on /sys/fs/cgroup/cpu,cpuacct type cgroup (rw,nosuid,nodev,noexec,relatime,cpu,cpuacct)
cgroup on /sys/fs/cgroup/freezer type cgroup (rw,nosuid,nodev,noexec,relatime,freezer)
cgroup on /sys/fs/cgroup/hugetlb type cgroup (rw,nosuid,nodev,noexec,relatime,hugetlb)
cgroup on /sys/fs/cgroup/pids type cgroup (rw,nosuid,nodev,noexec,relatime,pids)
cgroup on /sys/fs/cgroup/devices type cgroup (rw,nosuid,nodev,noexec,relatime,devices)
mqueue on /dev/mqueue type mqueue (rw,nosuid,nodev,noexec,relatime)
shm on /dev/shm type tmpfs (rw,nosuid,nodev,noexec,relatime,size=65536k,uid=200010,gid=1001)
127.0.0.1:/200010 on /root type nfs4 (rw,relatime,vers=4.1,rsize=1048576,wsize=1048576,namlen=255,hard,noresvport,proto=tcp,port=20486,timeo=600,retrans=2,sec=sys,clientaddr=127.0.0.1,local_lock=none,addr=127.0.0.1)
/dev/nvme0n1p1 on /opt/.sagemakerinternal type xfs (rw,noatime,attr2,inode64,usrquota,prjquota)
/dev/nvme0n1p1 on /etc/resolv.conf type xfs (rw,noatime,attr2,inode64,usrquota,prjquota)
/dev/nvme0n1p1 on /etc/hostname type xfs (rw,noatime,attr2,inode64,usrquota,prjquota)
/dev/nvme0n1p1 on /etc/hosts type xfs (rw,noatime,attr2,inode64,usrquota,prjquota)
/dev/nvme0n1p1 on /var/log/studio type xfs (rw,noatime,attr2,inode64,usrquota,prjquota)
/dev/nvme0n1p1 on /var/log/apps type xfs (rw,noatime,attr2,inode64,usrquota,prjquota)
/dev/nvme0n1p1 on /opt/ml/metadata/resource-metadata.json type xfs (ro,noatime,attr2,inode64,usrquota,prjquota)
devtmpfs on /dev/null type devtmpfs (rw,nosuid,size=16042360k,nr_inodes=4010590,mode=755)
devtmpfs on /dev/random type devtmpfs (rw,nosuid,size=16042360k,nr_inodes=4010590,mode=755)
devtmpfs on /dev/full type devtmpfs (rw,nosuid,size=16042360k,nr_inodes=4010590,mode=755)
devtmpfs on /dev/tty type devtmpfs (rw,nosuid,size=16042360k,nr_inodes=4010590,mode=755)
devtmpfs on /dev/zero type devtmpfs (rw,nosuid,size=16042360k,nr_inodes=4010590,mode=755)
devtmpfs on /dev/urandom type devtmpfs (rw,nosuid,size=16042360k,nr_inodes=4010590,mode=755)
proc on /proc/bus type proc (ro,nosuid,nodev,noexec,relatime)
proc on /proc/fs type proc (ro,nosuid,nodev,noexec,relatime)
proc on /proc/irq type proc (ro,nosuid,nodev,noexec,relatime)
proc on /proc/sys type proc (ro,nosuid,nodev,noexec,relatime)
proc on /proc/sysrq-trigger type proc (ro,nosuid,nodev,noexec,relatime)
tmpfs on /proc/acpi type tmpfs (ro,relatime,uid=200010,gid=1001)
devtmpfs on /proc/kcore type devtmpfs (rw,nosuid,size=16042360k,nr_inodes=4010590,mode=755)
devtmpfs on /proc/keys type devtmpfs (rw,nosuid,size=16042360k,nr_inodes=4010590,mode=755)
devtmpfs on /proc/latency_stats type devtmpfs (rw,nosuid,size=16042360k,nr_inodes=4010590,mode=755)
devtmpfs on /proc/timer_list type devtmpfs (rw,nosuid,size=16042360k,nr_inodes=4010590,mode=755)
devtmpfs on /proc/sched_debug type devtmpfs (rw,nosuid,size=16042360k,nr_inodes=4010590,mode=755)
tmpfs on /sys/firmware type tmpfs (ro,relatime,uid=200010,gid=1001)
</code></pre>
<pre><code>!lsblk

NAME          MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT
nvme1n1       259:0    0 279.4G  0 disk 
nvme0n1       259:1    0   124G  0 disk 
├─nvme0n1p1   259:2    0   124G  0 part /opt/ml/metadata/resource-metadata.json
└─nvme0n1p128 259:3    0     1M  0 part 
</code></pre>",0,1,2022-08-27 21:28:17.233000 UTC,,2022-08-27 22:56:19.970000 UTC,0,amazon-sagemaker,43,2016-11-18 12:14:15.223000 UTC,2022-09-16 21:21:44.413000 UTC,,101,24,0,13,,,,,,['amazon-sagemaker']
multi-model endpoints on sagemaker,"<p>I know, one can have multiple models on a single endpoint(Multi-Model endpoints). But, can I have these multiple models of different use cases and different datasets from the s3 bucket? because we mention model artifacts while invoking for prediction. can I have?</p>",1,0,2020-05-27 17:50:57.737000 UTC,,,0,amazon-sagemaker|amazon-machine-learning|multi-model-forms,1043,2020-05-15 14:03:49.927000 UTC,2020-08-13 15:05:22.840000 UTC,,1,0,0,0,,,,,,['amazon-sagemaker']
AWS EventBridge triggers SageMaker pipeline successfully but says failure,"<p>I have a SageMaker pipeline set up to transform audio data and run a model training script.  We are using custom containers for both parts if that matters.  The pipeline runs end to end when I test it in SageMaker with no issues.</p>
<p>I set up an EventBridge rule to trigger the pipeline overnight at regular intervals.  This triggers the pipeline successfully, but in my invocations dashboard it appears that the jobs fail.</p>
<p>I'm using the default event bus.  The execution role has all Read, Write, and List on my pipeline's ARN.</p>
<p>While the pipelines appear to run successfully, it would be nice if I could rely on the dashboards in the EventBridge console.  Is there anywhere else I should be looking to debug?</p>
<p><a href=""https://i.stack.imgur.com/uP4Lt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uP4Lt.png"" alt=""Failed Invocations"" /></a></p>
<p><a href=""https://i.stack.imgur.com/SsKDI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SsKDI.png"" alt=""SageMaker Pipelines are Successful"" /></a></p>",0,2,2022-09-06 14:53:50.593000 UTC,,,0,amazon-web-services|amazon-sagemaker|aws-event-bridge,20,2013-08-16 20:23:45.670000 UTC,2022-09-23 21:13:36.407000 UTC,,1105,658,2,222,,,,,,['amazon-sagemaker']
What AWS quotas do I need for SageMaker Asynchronous Inference Endpoints to be able to scale?,"<p>I want to deploy a model using an Asynchronous Inference endpoint which will auto-scale. However, I cannot find the information about what quotas are required for this to work without running out of resources.</p>
<p>Does scaling require some specific type of quotas, so that multiple jobs can be executed in parallel on different instances of the inference container?</p>
<p>It really isn't clear in the documentation whether quotas apply to Asynchronous Inference endpoints or not. Clearly, they apply to real-time inference endpoints, but Asynchronous Inference documentation does not seem to mention about it at all...</p>",1,0,2022-08-08 12:42:56.253000 UTC,,2022-08-08 13:11:27.583000 UTC,0,amazon-sagemaker,48,2013-02-12 18:29:06.977000 UTC,2022-09-24 21:22:40.153000 UTC,London,571,523,12,58,,,,,,['amazon-sagemaker']
Vertex AI custom job: cannot launch it via python,"<p>Trying to follow the example <a href=""https://cloud.google.com/vertex-ai/docs/training/create-custom-job#create_custom_job-python"" rel=""nofollow noreferrer"">here</a> to launch a custom job in vertex ai.</p>
<p>This is the code I am using</p>
<pre><code>from google.cloud import aiplatform


def create_custom_job_sample(
    project: str,
    display_name: str,
    container_image_uri: str,
    location: str = &quot;us-central1&quot;,
    api_endpoint: str = &quot;us-central1-aiplatform.googleapis.com&quot;,
):
    # The AI Platform services require regional API endpoints.
    client_options = {&quot;api_endpoint&quot;: api_endpoint}
    # Initialize client that will be used to create and send requests.
    # This client only needs to be created once, and can be reused for multiple requests.
    client = aiplatform.gapic.JobServiceClient(client_options=client_options)
    custom_job = {
        &quot;display_name&quot;: display_name,
        &quot;job_spec&quot;: {
            &quot;worker_pool_specs&quot;: [
                {
                    &quot;machine_spec&quot;: {
                        &quot;machine_type&quot;: &quot;n1-standard-4&quot;
                    },
                    &quot;replica_count&quot;: 1,
                    &quot;container_spec&quot;: {
                        &quot;image_uri&quot;: container_image_uri,
                        &quot;command&quot;: [],
                        &quot;args&quot;: [],
                    },
                }
            ]
        },
    }
    parent = f&quot;projects/{project}/locations/{location}&quot;
    response = client.create_custom_job(parent=parent, custom_job=custom_job)
    print(&quot;response:&quot;, response)

create_custom_job_sample(
    &quot;MY_PROJECT&quot;,
    &quot;job-123&quot;,
    &quot;europe-west1-docker.pkg.dev/&lt;MYPROJECT&gt;/&lt;MY_IMAGE_URI&gt;&quot;,
    &quot;europe-west1&quot;,
    &quot;eu-west1-aiplatform.googleapis.com&quot;
)
</code></pre>
<p>However I get an error that start with</p>
<pre><code>E0728 15:00:53.742356000 4417760704 hpack_parser.cc:1234]              Error parsing metadata: error=invalid value key=content-type value=text/html; charset=UTF-8
</code></pre>
<p>and ends with
<code>google.api_core.exceptions.Unknown: None Stream removed </code></p>
<p>Don't understand what the problem is. The job starts correctly from the terminal with</p>
<pre><code>gcloud ai custom-jobs create \
  --region=europe-west1 \
  --display-name=test-job-1 \
  --worker-pool-spec=machine-type=n1-standard-4,replica-count=1,executor-image-uri=&lt;MY_IMAGE_URI&gt;,local-package-path=.,script=myfolder/myscript.py
</code></pre>
<p>Could someone help me?</p>",0,3,2022-07-28 14:26:29.777000 UTC,,,0,python|google-cloud-platform|google-cloud-vertex-ai,160,2017-01-27 15:07:04.880000 UTC,2022-09-22 15:19:15.020000 UTC,,1991,468,27,107,,,,,,['google-cloud-vertex-ai']
Delete and recreate the registry for an azure machine learning workspace,"<p>Our azure machine learning workspace container registry has grown extremely large (4Tb) and has many obsolete entries. I would like to delete the registry and simply create a new one. We do not need any entries from the old one.</p>
<p>If I delete the current registry, create a new one, how do I attach it to the workspace?  I dont want to create a new workspace.</p>",1,0,2021-05-15 23:43:11.180000 UTC,,,2,azure-machine-learning-service,172,2019-08-14 14:48:38.450000 UTC,2022-09-25 02:16:40.673000 UTC,,113,3,0,6,,,,,,['azure-machine-learning-service']
How to upload a folder on SageMaker Notebooks,"<p>I am trying to upload a folder on SageMaker. However, I cannot select any folders, I need to go through the files and upload them one by one. Is there any way to upload the whole directory from local computer to SageMaker?</p>",1,1,2022-07-07 21:04:49.773000 UTC,,2022-07-10 10:27:24.760000 UTC,0,amazon-web-services|server|directory|upload|amazon-sagemaker,75,2018-02-25 07:22:56.310000 UTC,2022-08-29 16:07:10.467000 UTC,"Auburn, Al",447,10,0,26,,,,,,['amazon-sagemaker']
ModuleNotFoundError: No module named 'spacy' even though spacy and python are in the same path,"<p>I am following the spaCy installation guideline inside my AWS SageMaker notebook</p>
<pre><code>pip install -U pip setuptools wheel
pip install -U spacy
python -m spacy download en_core_web_sm
</code></pre>
<p>When I do <code>import spacy</code> I get error</p>
<pre><code>ModuleNotFoundError: No module named 'spacy' 
</code></pre>
<p>I made sure my python and spacy path are same</p>
<p>What am I missing?
<a href=""https://i.stack.imgur.com/pDbOF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pDbOF.png"" alt=""enter image description here"" /></a></p>",1,3,2021-10-26 00:01:13.503000 UTC,,2022-08-31 03:13:56.693000 UTC,1,python|spacy|amazon-sagemaker,1136,2012-06-12 04:14:13.300000 UTC,2022-08-13 03:10:50.523000 UTC,,2448,445,5,337,,,,,,['amazon-sagemaker']
How to use a .joblib model from Amazon Sagemaker in a local environment?,"<p>I created a model in AWS using Sagemaker. I downloaded model.joblib to my machine. I am trying to use it to make some predictions. I can load the file:</p>
<pre><code>import joblib
import mlio
import sklearn
filename=r&quot;C:\Users\benki\Downloads\model.tar\model.joblib&quot;
loaded_model = joblib.load(filename)
</code></pre>
<p>However, I am not sure where to go from here. I've tried to score and predict 'loaded_model' but I only get error messages explaining that 'loaded_model' does not have these attributes. 'loaded_model' is the type: sagemaker_sklearn_extension.externals.automl_transformer.AutoMLTransformer</p>
<p>In AWS from a Sagemaker Jupyter Notebook instance, I can make predictions with the following:</p>
<pre><code>endpoint_name = &quot;My_Model&quot;  
#print(f&quot;Note: Invoking Endpoint: {endpoint_name}&quot;)
content_type = &quot;text/csv&quot;                                        
accept = &quot;text/csv&quot;  

# create for loop to create results, one at a time
predict=[]
for sample in payloads:
    response = client.invoke_endpoint(
    EndpointName=endpoint_name,
    ContentType=content_type,
    Body=sample
    )
    #print('inference complete')
    inference = (response['Body'].read().decode('ascii'))
    predict.append((sample,inference))
</code></pre>
<p>How do I engage this joblib model?</p>",1,0,2021-06-02 17:55:59.433000 UTC,,2021-06-02 18:03:22.487000 UTC,0,python|machine-learning|amazon-sagemaker|joblib,397,2016-02-25 16:12:31.837000 UTC,2022-09-23 07:50:53.863000 UTC,,351,63,0,173,,,,,,['amazon-sagemaker']
ValueError: Cannot Convert String to Float With Pandas and Amazon Sagemaker,"<p>I'm trying to deploy a simple ML model on SageMaker to get the hang of it, and I am not having any luck because I get the following error:  </p>

<pre><code>ValueError: could not convert string to float: '6.320000000000000097e-03 1.800000000000000000e+01 2.310000000000000053e+00 0.000000000000000000e+00 5.380000000000000338e-01 6.575000000000000178e+00 6.520000000000000284e+01 4.089999999999999858e+00 1.000000000000000000e+00 2.960000000000000000e+02 1.530000000000000071e+01 3.968999999999999773e+02 4.980000000000000426e+00 2.400000000000000000e+01'
</code></pre>

<p>This is the first row of my dataframe.  </p>

<p>This is the code in my notebook that I'm using right now:</p>

<pre><code>from sagemaker import get_execution_role, Session
from sagemaker.sklearn.estimator import SKLearn
work_dir = 'data'
session  = Session()
role     = get_execution_role()
train_input = session.upload_data('data')
script      = 'boston_housing_prep.py'

model = SKLearn(
entry_point         = script,
train_instance_type = 'ml.c4.xlarge',
role                = role,
sagemaker_session   = session,
hyperparameters     = {'alpha': 10}
)

model.fit({'train': train_input})
</code></pre>

<p>My script for boston_housing_prep.py looks like this:</p>

<pre><code>import argparse
import pandas as pd
import os

from sklearn.linear_model import Ridge
from sklearn.externals import joblib
from sklearn.preprocessing import StandardScaler
import numpy as np

if __name__ == '__main__':
    parser = argparse.ArgumentParser()

    parser.add_argument('--alpha', type=int, default=1)

    parser.add_argument('--output-data-dir', type=str, default=os.environ['SM_OUTPUT_DATA_DIR'])
    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])
    parser.add_argument('--train', type=str, default=os.environ['SM_CHANNEL_TRAIN'])

    args = parser.parse_args()
    input_files = [ os.path.join(args.train, file) for file in os.listdir(args.train) ]
    if len(input_files) == 0:
        raise ValueError(('There are no files in {}.\n' +
                      'This usually indicates that the channel ({}) was incorrectly specified,\n' +
                      'the data specification in S3 was incorrectly specified or the role specified\n' +
                      'does not have permission to access the data.').format(args.train, ""train""))
    raw_data = [ pd.read_csv(file, header=None, engine=""python"") for file in input_files ]
    df       = pd.concat(raw_data)

    y_train = df.iloc[:, -1]
    X_train = df.iloc[:, :5]

    scaler  = StandardScaler()
    X_train = scaler.fit_transform(X_train)

    alpha = args.alpha

    clf = Ridge(alpha=alpha)
    clf = clf.fit(X_train, y_train)

    joblib.dump(clf, os.path.join(args.model_dir, ""model.joblib""))

def model_fn(model_dir):
    clf = joblib.load(os.path.join(model_dir, ""model.joblib""))
    return clf
</code></pre>

<p>The line that's giving the problem is this one: </p>

<pre><code>X_train = scaler.fit_transform(X_train)
</code></pre>

<p>I tried <code>df = df.astype(np.float) </code> after I loaded in the df, but that didn't work either.</p>

<p>This file loads in without a problem when I'm not in SageMaker.</p>",0,4,2019-06-10 08:05:10.920000 UTC,,2019-06-10 10:13:03.330000 UTC,0,pandas|numpy|scikit-learn|amazon-sagemaker,336,2014-09-15 23:32:32.337000 UTC,2022-09-23 22:20:44.790000 UTC,"New York, NY, United States",3257,451,0,319,,,,,,['amazon-sagemaker']
Defining Metrics on SageMaker to CloudWatch,"<p>From AWS Sagemaker Documentation, In order to track metrics in cloudwatch for custom ml algorithms (non-builtin), I read that I have to define my estimaotr as below.</p>
<p>But I am not sure how to alter my training script so that the metric definitions declared inside my estimators can pick up these values.</p>
<pre><code>estimator =
                Estimator(image_name=ImageName,
                role='SageMakerRole', 
                instance_count=1,
                instance_type='ml.c4.xlarge',
                k=10,
                sagemaker_session=sagemaker_session,
                metric_definitions=[
                   {'Name': 'train:error', 'Regex': 'Train_error=(.*?);'},
                   {'Name': 'validation:error', 'Regex': 'Valid_error=(.*?);'}
                ]
            )
</code></pre>
<p>In my training code, I have</p>
<pre><code>    for epoch in range(1, args.epochs + 1):
        total_loss = 0
        model.train()
        for step, batch in enumerate(train_loader):
            b_input_ids = batch[0].to(device)
            b_input_mask = batch[1].to(device)
            b_labels = batch[2].to(device)
            model.zero_grad()

            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)
            loss = outputs[0]

            total_loss += loss.item()
            loss.backward() # Computes the gradients
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # Clip for error prevention
            # modified based on their gradients, the learning rate, etc.
            optimizer.step() # Back Prop
logger.info(&quot;Average training loss: %f\n&quot;, total_loss / len(train_loader))
</code></pre>
<p>Here, I want the train:error to pick up <code>total_loss / len(train_loader)</code> but I am not sure how to assign this.</p>",1,0,2021-07-21 13:43:45.787000 UTC,,,1,python|pytorch|amazon-cloudwatch|amazon-sagemaker,1420,2020-09-21 20:00:48.277000 UTC,2022-09-16 06:59:29.497000 UTC,"Seoul, South Korea",69,8,0,4,,,,,,['amazon-sagemaker']
AzureML SDK problems with pip-installed dependencies in environment file,"<p>i am try to run a Python Script in AzureML SDK. However in the runs log file, it prints the same error over and over again:</p>
<blockquote>
<p>Warning: you have pip-installed dependencies in your environment file, but you do not list pip itself as one of your conda dependencies.  Conda may not use the correct pip to install your packages, and they may end up in the wrong place.  Please add an explicit pip dependency.  I'm adding one for you, but still nagging you.</p>
</blockquote>
<p>The defined conda environment looks like this:</p>
<pre><code>    {
    &quot;databricks&quot;: {
        &quot;eggLibraries&quot;: [],
        &quot;jarLibraries&quot;: [],
        &quot;mavenLibraries&quot;: [],
        &quot;pypiLibraries&quot;: [],
        &quot;rcranLibraries&quot;: []
    },
    &quot;docker&quot;: {
        &quot;arguments&quot;: [],
        &quot;baseDockerfile&quot;: null,
        &quot;baseImage&quot;: &quot;mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20210301.v1&quot;,
        &quot;baseImageRegistry&quot;: {
            &quot;address&quot;: null,
            &quot;password&quot;: null,
            &quot;registryIdentity&quot;: null,
            &quot;username&quot;: null
        },
        &quot;enabled&quot;: false,
        &quot;platform&quot;: {
            &quot;architecture&quot;: &quot;amd64&quot;,
            &quot;os&quot;: &quot;Linux&quot;
        },
        &quot;sharedVolumes&quot;: true,
        &quot;shmSize&quot;: null
    },
    &quot;environmentVariables&quot;: {
        &quot;EXAMPLE_ENV_VAR&quot;: &quot;EXAMPLE_VALUE&quot;
    },
    &quot;inferencingStackVersion&quot;: null,
    &quot;name&quot;: &quot;MyEnvironment04&quot;,
    &quot;python&quot;: {
        &quot;baseCondaEnvironment&quot;: null,
        &quot;condaDependencies&quot;: {
            &quot;channels&quot;: [
                &quot;anaconda&quot;,
                &quot;conda-forge&quot;
            ],
            &quot;dependencies&quot;: [
                &quot;python=3.6.2&quot;,
                {
                    &quot;pip&quot;: [
                        &quot;pip=20.2.40&quot;
                    ]
                },
                &quot;scikit-learn&quot;
            ],
            &quot;name&quot;: &quot;azureml_815589d460c271a1415198e7283fa9e9&quot;
        },
        &quot;condaDependenciesFile&quot;: null,
        &quot;interpreterPath&quot;: &quot;python&quot;,
        &quot;userManagedDependencies&quot;: false
    },
    &quot;r&quot;: null,
    &quot;spark&quot;: {
        &quot;packages&quot;: [],
        &quot;precachePackages&quot;: true,
        &quot;repositories&quot;: []
    },
    &quot;version&quot;: &quot;1&quot;
}
</code></pre>
<p>I am assuming that the definition for my pip package in my environment is wrong.</p>
<pre><code># Create the dependencies object
myenv_dep = CondaDependencies.create(conda_packages=['scikit-learn'], pip_packages=['pip=20.2.40'])
myenv.python.conda_dependencies = myenv_dep
</code></pre>
<p>Please let me know if i need to provide further information.</p>
<p>Thank you!</p>",1,0,2021-06-07 08:00:59.247000 UTC,,,0,python|pip|azure-machine-learning-service|azureml-python-sdk,332,2020-07-30 11:58:46.463000 UTC,2022-02-11 10:24:32.690000 UTC,,1,0,0,2,,,,,,['azure-machine-learning-service']
Error when trying to create a multi-layer neural network for two-class classification,"<p>I'm currently trying to create a two layer neural network using the following Net# code:</p>

<pre><code>input Data auto;
hidden H1 1600 from Data all;
hidden H2 1600 from H1 all;
output Result auto from H2;
</code></pre>

<p>For some reason when I run the experiment, I get this error:</p>

<pre><code>requestId = cbc3ec794f0a48799b66f8a53b6cc1f3 errorComponent=Module. taskStatusCode=400. {
    ""Exception"":{            
        ""ErrorId"":""LibraryException"",
        ""ErrorCode"":""1000"",
        ""ExceptionType"":""ModuleException"",
        ""Message"":""Error 1000: TLC library exception: Exception of type 'Microsoft.Numerics.AFxLibraryException' was thrown."",
        ""Exception"":{
            ""Library"":""TLC"",
            ""ExceptionType"":""LibraryException"",
            ""Message"":""Exception of type 'Microsoft.Numerics.AFxLibraryException' was thrown.""
        }
    }
}
Error: Error 1000: TLC library exception: Exception of type 'Microsoft.Numerics.AFxLibraryException' was thrown. Process exited with error code -2
</code></pre>

<p>I don't get this error when I use the normal single layer builder interface that is initially presented. Any help would be much appreciated in diagnosing this.</p>",0,2,2016-11-27 00:53:17.687000 UTC,,,2,azure-machine-learning-studio,363,2012-01-08 23:14:47.693000 UTC,2022-09-21 15:56:17.997000 UTC,"Lincoln, NE, USA",2126,156,12,326,,,,,,['azure-machine-learning-studio']
How to Integrate Pycharm and git with azure machile learning service (workspace),"<p>I want to create a machine learning pipeline using python with PyCharm and run everything in azure machine learning service workspace. Then I want to integrate my pycharm script in a way when I edit and save my script, it runs a new experiment in Azure ML workspace.</p>

<p>I have check all the tutorials on using Azure ML service using python sdk, however, every time it is via notebooks but not with pycharm.</p>",1,0,2019-08-23 09:52:48.013000 UTC,,,4,python|azure|pycharm|azure-machine-learning-service,1205,2019-08-23 09:45:28.437000 UTC,2019-09-18 13:18:50.727000 UTC,,41,0,0,2,,,,,,['azure-machine-learning-service']
Sagemaker pytorch inference stops at model call on gpu,"<p>I deployed a pytorch model using sagemaker and can successfully query it on a CPU. Deploying it on a GPU leads to a InternalServerError client-side though. Checking the CloudWatch Logs shows that the request is received, preprocessing finishes and the call to the model is started. I can also see log from the metric collector about the prediction time. At that point there are no further logs though. The print statement I put right after the model call is never reached.</p>
<p>It is possible that there is an error happening which doesn't make it to CloudWatch. I have noticed that sagemaker seems to not show stack traces fully. Unfortunately I have already set the log_level to DEBUG without success.</p>
<p>I'm running on a sagemaker docker container - <code>pytorch-inference:1.10-gpu-py38</code> on a <code>ml.g4dn.xlarge</code> instance. The model itself is compiled using torchscript.trace. I am using a custom transform function which you can see below as well as the CloudWatch Logs (the log continues as the client retries 4x).</p>
<p>If anyone has any idea what is happening here it would be very much appreciated!</p>
<pre><code>import io
import torch
import os, sys
import json
import logging
from PIL import Image

from sagemaker_inference import (
    content_types,
    decoder,
    encoder,
    errors,
    utils,
)
from MyDetrFeatureExtractor import MyDetrFeatureExtractor


INFERENCE_ACCELERATOR_PRESENT_ENV = &quot;SAGEMAKER_INFERENCE_ACCELERATOR_PRESENT&quot;
IMG_WIDTH = 800
IMG_HEIGHT = 1131
MODEL_FILE = &quot;model.pt&quot;
THRESHOLD = 0.2
feature_extractor = MyDetrFeatureExtractor.from_pretrained(
            &quot;facebook/detr-resnet-50&quot;, size=(IMG_WIDTH, IMG_HEIGHT))
index_to_name = json.load(open('/opt/ml/model/code/id2label.json', 'r'))

logger = logging.getLogger(&quot;sagemaker-inference&quot;)
# logger.addHandler(logging.StreamHandler(sys.stdout))


def model_fn(model_dir):
    logger.info(f&quot;Trying to load model from {model_dir}/{MODEL_FILE}.&quot;)
    device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
    model = torch.jit.load(f&quot;{model_dir}/{MODEL_FILE}&quot;, map_location=torch.device(device))
    model = model.to(device)
    return model


def preprocess(images):
    logger.info(&quot;Preprocessing image...&quot;)
    try:
        encoding = feature_extractor(images=images, return_tensors=&quot;pt&quot;)
        pixel_values = encoding[&quot;pixel_values&quot;]
    except Exception as e:
        logger.error(&quot;Preprocessing Failed.&quot;)
        logger.error(e)
    return pixel_values


def load_fn(input_data, content_type):
    &quot;&quot;&quot;A default input_fn that can handle JSON, CSV and NPZ formats.
    Args:
        input_data: the request payload serialized in the content_type format
        content_type: the request content_type
    Returns: input_data deserialized into torch.FloatTensor or torch.cuda.FloatTensor,
        depending if cuda is available.
    &quot;&quot;&quot;
    device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
    
    if content_type == &quot;application/x-image&quot;:
        if isinstance(input_data, str):
            # if the image is a string of bytesarray.
            print(&quot;Found string of bytesarray. Translating to Image.&quot;)
            image = base64.b64decode(input_data)
        elif isinstance(input_data, (bytearray, bytes)):
            # If the image is sent as bytesarray
            print(&quot;Found bytesarray. Translating to Image.&quot;)
            image = Image.open(io.BytesIO(input_data))
    else:
        err_msg = f&quot;Type [{content_type}] not support this type yet&quot;
        logger.error(err_msg)
        raise ValueError(err_msg)

    # image = Image.from_array(np_array)
    size = image.size
    image_sizes_orig = [[size[1], size[0]]]
    logger.info(f&quot;Image of size {size} loaded. Start Preprocessing.&quot;)
    tensor = preprocess(image)
    return tensor.to(device), torch.tensor(image_sizes_orig)


def inference_fn(data, model):
    &quot;&quot;&quot;A default predict_fn for PyTorch. Calls a model on data deserialized in input_fn.
    Runs prediction on GPU if cuda is available.
    Args:
        data: input data (torch.Tensor) for prediction deserialized by input_fn
        model: PyTorch model loaded in memory by model_fn
    Returns: a prediction
    &quot;&quot;&quot;
    with torch.no_grad():
        if os.getenv(INFERENCE_ACCELERATOR_PRESENT_ENV) == &quot;true&quot;:
            device = torch.device(&quot;cpu&quot;)
            model = model.to(device)
            input_data = data.to(device)
            model.eval()
            with torch.jit.optimized_execution(True, {&quot;target_device&quot;: &quot;eia:0&quot;}):
                output = model(input_data)
        else:
            device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
            logger.info(f&quot;Running predictions on {device}.&quot;)
            model = model.to(device)
            input_data = data.to(device)
            model.eval()
            logger.info(&quot;Compute predictions.&quot;)
            output = model(input_data)
            logger.info(&quot;Finished actual inference&quot;)

    return output


def postprocess(output, img_sizes_orig):
    logger.info(&quot;Postprocessing image...&quot;)
    try:
        results_all = feature_extractor.post_process(output, img_sizes_orig, use_dict=False)
        results = []
        for res_per_img in results_all:
            scores_per_img = res_per_img['scores'].detach().numpy()
            # keep only predictions with confidence &gt;= threshold
            keep = scores_per_img &gt; THRESHOLD
            labels_per_img = list(map(
                index_to_name.get, 
                res_per_img['labels'][keep].detach().numpy().astype(str)
            ))
            bboxes_per_img = res_per_img['boxes'][keep].detach().numpy()
            scores_per_img = scores_per_img[keep]
            out = [{
                'bbox': list(map(int, bbox)),
                'score': score.astype(float),
                'label': label
            } for score, label, bbox in 
            zip(scores_per_img, labels_per_img, bboxes_per_img)]
            logger.info(f&quot;Appending {out}.&quot;)
            results.append(out)
    except Exception as e:
        logger.error(&quot;Postprocessing Failed.&quot;)
        logger.error(e)
    
    return results


def create_output(prediction, accept):
        &quot;&quot;&quot;A default output_fn for PyTorch. Serializes predictions from predict_fn to JSON, CSV or NPY format.
        Args:
            prediction: a prediction result from predict_fn
            accept: type which the output data needs to be serialized
        Returns: output data serialized
        &quot;&quot;&quot;
        if type(prediction) == torch.Tensor:
            prediction = prediction.detach().cpu().numpy().tolist()

        for content_type in utils.parse_accept(accept):
            if content_type in encoder.SUPPORTED_CONTENT_TYPES:
                encoded_prediction = encoder.encode(prediction, content_type)
                if content_type == content_types.CSV:
                    encoded_prediction = encoded_prediction.encode(&quot;utf-8&quot;)
                if content_type == content_types.JSON:
                    encoded_prediction = encoded_prediction.encode(&quot;utf-8&quot;)
                return encoded_prediction, accept

        raise errors.UnsupportedFormatError(accept)



def transform_fn(model, request_body, content_type, accept_type):
    logger.info(&quot;Received Request.&quot;)
    images, image_sizes = load_fn(request_body, content_type)
    logger.info(&quot;Starting Inference.&quot;)
    output = inference_fn(images, model)
    logger.info(&quot;Postprocessing.&quot;)
    results = postprocess(output, image_sizes)
    logger.info(results)
    return create_output(results, accept_type)
</code></pre>
<p>and the logs...</p>
<pre><code>Requirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from -r /opt/ml/model/code/requirements.txt (line 1)) (1.22.2)
Requirement already satisfied: Pillow in /opt/conda/lib/python3.8/site-packages (from -r /opt/ml/model/code/requirements.txt (line 2)) (9.1.1)
Collecting nvgpu
  Downloading nvgpu-0.9.0-py2.py3-none-any.whl (9.4 kB)
Collecting transformers==4.17
  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.8/3.8 MB 50.7 MB/s eta 0:00:00
Requirement already satisfied: packaging&gt;=20.0 in /opt/conda/lib/python3.8/site-packages (from transformers==4.17-&gt;-r /opt/ml/model/code/requirements.txt (line 4)) (20.4)
Collecting regex!=2019.12.17
  Downloading regex-2022.7.9-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (765 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 765.0/765.0 kB 23.8 MB/s eta 0:00:00
Collecting filelock
  Downloading filelock-3.7.1-py3-none-any.whl (10 kB)
Collecting huggingface-hub&lt;1.0,&gt;=0.1.0
  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 101.5/101.5 kB 25.5 MB/s eta 0:00:00
Requirement already satisfied: tqdm&gt;=4.27 in /opt/conda/lib/python3.8/site-packages (from transformers==4.17-&gt;-r /opt/ml/model/code/requirements.txt (line 4)) (4.64.0)
Requirement already satisfied: pyyaml&gt;=5.1 in /opt/conda/lib/python3.8/site-packages (from transformers==4.17-&gt;-r /opt/ml/model/code/requirements.txt (line 4)) (5.4.1)
Collecting tokenizers!=0.11.3,&gt;=0.11.1
  Downloading tokenizers-0.12.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.6/6.6 MB 111.5 MB/s eta 0:00:00
Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from transformers==4.17-&gt;-r /opt/ml/model/code/requirements.txt (line 4)) (2.27.1)
Collecting sacremoses
  Downloading sacremoses-0.0.53.tar.gz (880 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 880.6/880.6 kB 90.0 MB/s eta 0:00:00
  Preparing metadata (setup.py): started
  Preparing metadata (setup.py): finished with status 'done'
Collecting pynvml
  Downloading pynvml-11.4.1-py3-none-any.whl (46 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 47.0/47.0 kB 15.6 MB/s eta 0:00:00
Requirement already satisfied: psutil in /opt/conda/lib/python3.8/site-packages (from nvgpu-&gt;-r /opt/ml/model/code/requirements.txt (line 3)) (5.9.0)
Requirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from nvgpu-&gt;-r /opt/ml/model/code/requirements.txt (line 3)) (1.4.2)
Collecting flask-restful
  Downloading Flask_RESTful-0.3.9-py2.py3-none-any.whl (25 kB)
Collecting tabulate
  Downloading tabulate-0.8.10-py3-none-any.whl (29 kB)
Collecting termcolor
  Downloading termcolor-1.1.0.tar.gz (3.9 kB)
  Preparing metadata (setup.py): started
  Preparing metadata (setup.py): finished with status 'done'
Collecting arrow
  Downloading arrow-1.2.2-py3-none-any.whl (64 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 64.0/64.0 kB 19.0 MB/s eta 0:00:00
Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from nvgpu-&gt;-r /opt/ml/model/code/requirements.txt (line 3)) (1.16.0)
Collecting flask
  Downloading Flask-2.1.3-py3-none-any.whl (95 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 95.6/95.6 kB 29.0 MB/s eta 0:00:00
Collecting ansi2html
  Downloading ansi2html-1.8.0-py3-none-any.whl (16 kB)
Collecting packaging&gt;=20.0
  Downloading packaging-21.3-py3-none-any.whl (40 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.8/40.8 kB 13.5 MB/s eta 0:00:00
Requirement already satisfied: typing-extensions&gt;=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub&lt;1.0,&gt;=0.1.0-&gt;transformers==4.17-&gt;-r /opt/ml/model/code/requirements.txt (line 4)) (4.2.0)
Requirement already satisfied: pyparsing!=3.0.5,&gt;=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging&gt;=20.0-&gt;transformers==4.17-&gt;-r /opt/ml/model/code/requirements.txt (line 4)) (3.0.9)
Requirement already satisfied: python-dateutil&gt;=2.7.0 in /opt/conda/lib/python3.8/site-packages (from arrow-&gt;nvgpu-&gt;-r /opt/ml/model/code/requirements.txt (line 3)) (2.8.2)
Collecting itsdangerous&gt;=2.0
  Downloading itsdangerous-2.1.2-py3-none-any.whl (15 kB)
Requirement already satisfied: click&gt;=8.0 in /opt/conda/lib/python3.8/site-packages (from flask-&gt;nvgpu-&gt;-r /opt/ml/model/code/requirements.txt (line 3)) (8.1.3)
Collecting Jinja2&gt;=3.0
  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 133.1/133.1 kB 37.2 MB/s eta 0:00:00
Collecting Werkzeug&gt;=2.0
  Downloading Werkzeug-2.1.2-py3-none-any.whl (224 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 224.9/224.9 kB 50.7 MB/s eta 0:00:00
Collecting importlib-metadata&gt;=3.6.0
  Downloading importlib_metadata-4.12.0-py3-none-any.whl (21 kB)
Requirement already satisfied: pytz in /opt/conda/lib/python3.8/site-packages (from flask-restful-&gt;nvgpu-&gt;-r /opt/ml/model/code/requirements.txt (line 3)) (2022.1)
Collecting aniso8601&gt;=0.82
  Downloading aniso8601-9.0.1-py2.py3-none-any.whl (52 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 52.8/52.8 kB 17.9 MB/s eta 0:00:00
Requirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests-&gt;transformers==4.17-&gt;-r /opt/ml/model/code/requirements.txt (line 4)) (1.26.9)
Requirement already satisfied: certifi&gt;=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests-&gt;transformers==4.17-&gt;-r /opt/ml/model/code/requirements.txt (line 4)) (2022.5.18.1)
Requirement already satisfied: idna&lt;4,&gt;=2.5 in /opt/conda/lib/python3.8/site-packages (from requests-&gt;transformers==4.17-&gt;-r /opt/ml/model/code/requirements.txt (line 4)) (3.3)
Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests-&gt;transformers==4.17-&gt;-r /opt/ml/model/code/requirements.txt (line 4)) (2.0.12)
Requirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (from sacremoses-&gt;transformers==4.17-&gt;-r /opt/ml/model/code/requirements.txt (line 4)) (1.1.0)
Collecting zipp&gt;=0.5
  Downloading zipp-3.8.1-py3-none-any.whl (5.6 kB)
Collecting MarkupSafe&gt;=2.0
  Downloading MarkupSafe-2.1.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)
Building wheels for collected packages: sacremoses, termcolor
  Building wheel for sacremoses (setup.py): started
  Building wheel for sacremoses (setup.py): finished with status 'done'
  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895241 sha256=a3bb167ffae5506dddf61987611fcdfc0b8204913917be57bf7567f41240501c
  Stored in directory: /root/.cache/pip/wheels/82/ab/9b/c15899bf659ba74f623ac776e861cf2eb8608c1825ddec66a4
  Building wheel for termcolor (setup.py): started
  Building wheel for termcolor (setup.py): finished with status 'done'
  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4832 sha256=f2b732eca48c5b5b44b0b23a29ba7130b890cb8b7df31955e7d7f34c7caeeb16
  Stored in directory: /root/.cache/pip/wheels/a0/16/9c/5473df82468f958445479c59e784896fa24f4a5fc024b0f501
Successfully built sacremoses termcolor
Installing collected packages: tokenizers, termcolor, aniso8601, zipp, Werkzeug, tabulate, regex, pynvml, packaging, MarkupSafe, itsdangerous, filelock, ansi2html, sacremoses, Jinja2, importlib-metadata, huggingface-hub, arrow, transformers, flask, flask-restful, nvgpu
  Attempting uninstall: packaging
    Found existing installation: packaging 20.4
    Uninstalling packaging-20.4:
      Successfully uninstalled packaging-20.4
Successfully installed Jinja2-3.1.2 MarkupSafe-2.1.1 Werkzeug-2.1.2 aniso8601-9.0.1 ansi2html-1.8.0 arrow-1.2.2 filelock-3.7.1 flask-2.1.3 flask-restful-0.3.9 huggingface-hub-0.8.1 importlib-metadata-4.12.0 itsdangerous-2.1.2 nvgpu-0.9.0 packaging-21.3 pynvml-11.4.1 regex-2022.7.9 sacremoses-0.0.53 tabulate-0.8.10 termcolor-1.1.0 tokenizers-0.12.1 transformers-4.17.0 zipp-3.8.1
WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv
WARNING: There was an error checking the latest version of pip.
WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.
2022-07-22T11:10:02,627 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-07-22T11:10:02,696 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.5.3
TS Home: /opt/conda/lib/python3.8/site-packages
Current directory: /
Temp directory: /home/model-server/tmp
Number of GPUs: 1
Number of CPUs: 1
Max heap size: 3234 M
Python executable: /opt/conda/bin/python3.8
Config file: /etc/sagemaker-ts.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8080
Metrics address: http://127.0.0.1:8082
Model Store: /.sagemaker/ts/models
Initial Models: model=/opt/ml/model
Log dir: /logs
Metrics dir: /logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /.sagemaker/ts/models
Model config: 
{
    &quot;model&quot;: {
        &quot;1.0&quot;: {
            &quot;defaultVersion&quot;: true,
            &quot;marName&quot;: &quot;model.mar&quot;,
            &quot;minWorkers&quot;: 1,
            &quot;maxWorkers&quot;: 1,
            &quot;batchSize&quot;: 1,
            &quot;maxBatchDelay&quot;: 10000,
            &quot;responseTimeout&quot;: 60
        }
    }
}
2022-07-22T11:10:02,703 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-07-22T11:10:02,706 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: /opt/ml/model
2022-07-22T11:10:02,709 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive version is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher
2022-07-22T11:10:02,710 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive createdOn is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher
2022-07-22T11:10:02,712 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model model loaded.
2022-07-22T11:10:02,722 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2022-07-22T11:10:02,797 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2022-07-22T11:10:02,797 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2022-07-22T11:10:02,800 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
Model server started.
2022-07-22T11:10:03,018 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-07-22T11:10:03,544 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:container-0.local,timestamp:1658488203
2022-07-22T11:10:03,545 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:26.050277709960938|#Level:Host|#hostname:container-0.local,timestamp:1658488203
2022-07-22T11:10:03,545 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:25.937984466552734|#Level:Host|#hostname:container-0.local,timestamp:1658488203
2022-07-22T11:10:03,545 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:49.9|#Level:Host|#hostname:container-0.local,timestamp:1658488203
2022-07-22T11:10:03,546 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:0.0|#Level:Host,device_id:0|#hostname:container-0.local,timestamp:1658488203
2022-07-22T11:10:03,546 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:0|#Level:Host,device_id:0|#hostname:container-0.local,timestamp:1658488203
2022-07-22T11:10:03,546 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:0|#Level:Host,device_id:0|#hostname:container-0.local,timestamp:1658488203
2022-07-22T11:10:03,547 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:13904.71875|#Level:Host|#hostname:container-0.local,timestamp:1658488203
2022-07-22T11:10:03,547 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:1511.390625|#Level:Host|#hostname:container-0.local,timestamp:1658488203
2022-07-22T11:10:03,547 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:11.7|#Level:Host|#hostname:container-0.local,timestamp:1658488203
2022-07-22T11:10:03,814 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000
2022-07-22T11:10:03,815 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - [PID]60
2022-07-22T11:10:03,815 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started.
2022-07-22T11:10:03,815 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.10
2022-07-22T11:10:03,821 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000
2022-07-22T11:10:03,830 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.
2022-07-22T11:10:03,832 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1658488203832
2022-07-22T11:10:03,902 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1
2022-07-22T11:10:04,735 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - 
2022-07-22T11:10:04,736 [WARN ] W-9000-model_1.0-stderr MODEL_LOG - Downloading:   0%|          | 0.00/274 [00:00&lt;?, ?B/s]
2022-07-22T11:10:04,737 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Trying to load model from /opt/ml/model/model.pt.
2022-07-22T11:10:05,938 [INFO ] pool-2-thread-2 ACCESS_LOG - /169.254.178.2:40592 &quot;GET /ping HTTP/1.1&quot; 200 6
2022-07-22T11:10:05,939 [INFO ] pool-2-thread-2 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:container-0.local,timestamp:null
2022-07-22T11:10:08,126 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 4223
2022-07-22T11:10:08,127 [INFO ] W-9000-model_1.0 TS_METRICS - W-9000-model_1.0.ms:5410|#Level:Host|#hostname:container-0.local,timestamp:1658488208
2022-07-22T11:10:08,127 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:72|#Level:Host|#hostname:container-0.local,timestamp:null
2022-07-22T11:10:10,861 [INFO ] pool-2-thread-2 ACCESS_LOG - /169.254.178.2:40592 &quot;GET /ping HTTP/1.1&quot; 200 1
2022-07-22T11:10:10,861 [INFO ] pool-2-thread-2 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:container-0.local,timestamp:null
2022-07-22T11:12:03,445 [INFO ] pool-3-thread-2 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:container-0.local,timestamp:1658488323
2022-07-22T11:12:03,447 [INFO ] pool-3-thread-2 TS_METRICS - DiskAvailable.Gigabytes:26.09253692626953|#Level:Host|#hostname:container-0.local,timestamp:1658488323
2022-07-22T11:12:03,447 [INFO ] pool-3-thread-2 TS_METRICS - DiskUsage.Gigabytes:25.89572525024414|#Level:Host|#hostname:container-0.local,timestamp:1658488323
2022-07-22T11:12:03,448 [INFO ] pool-3-thread-2 TS_METRICS - DiskUtilization.Percent:49.8|#Level:Host|#hostname:container-0.local,timestamp:1658488323
2022-07-22T11:12:03,449 [INFO ] pool-3-thread-2 TS_METRICS - GPUMemoryUtilization.Percent:5.731683102786419|#Level:Host,device_id:0|#hostname:container-0.local,timestamp:1658488323
2022-07-22T11:12:03,449 [INFO ] pool-3-thread-2 TS_METRICS - GPUMemoryUsed.Megabytes:866|#Level:Host,device_id:0|#hostname:container-0.local,timestamp:1658488323
2022-07-22T11:12:03,449 [INFO ] pool-3-thread-2 TS_METRICS - GPUUtilization.Percent:0|#Level:Host,device_id:0|#hostname:container-0.local,timestamp:1658488323
2022-07-22T11:12:03,449 [INFO ] pool-3-thread-2 TS_METRICS - MemoryAvailable.Megabytes:12352.65625|#Level:Host|#hostname:container-0.local,timestamp:1658488323
2022-07-22T11:12:03,449 [INFO ] pool-3-thread-2 TS_METRICS - MemoryUsed.Megabytes:3051.94140625|#Level:Host|#hostname:container-0.local,timestamp:1658488323
2022-07-22T11:12:03,450 [INFO ] pool-3-thread-2 TS_METRICS - MemoryUtilization.Percent:21.5|#Level:Host|#hostname:container-0.local,timestamp:1658488323
2022-07-22T11:12:05,859 [INFO ] pool-2-thread-2 ACCESS_LOG - /169.254.178.2:40592 &quot;GET /ping HTTP/1.1&quot; 200 0
2022-07-22T11:12:05,860 [INFO ] pool-2-thread-2 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:container-0.local,timestamp:null
2022-07-22T11:12:10,860 [INFO ] pool-2-thread-2 ACCESS_LOG - /169.254.178.2:40592 &quot;GET /ping HTTP/1.1&quot; 200 0
2022-07-22T11:12:10,860 [INFO ] pool-2-thread-2 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:container-0.local,timestamp:null
2022-07-22T11:12:15,860 [INFO ] pool-2-thread-2 ACCESS_LOG - /169.254.178.2:40592 &quot;GET /ping HTTP/1.1&quot; 200 1
2022-07-22T11:12:15,860 [INFO ] pool-2-thread-2 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:container-0.local,timestamp:null
2022-07-22T11:12:20,193 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1658488340193
2022-07-22T11:12:20,195 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend received inference at: 1658488340
2022-07-22T11:12:20,196 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Received Request.
2022-07-22T11:12:20,205 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Found bytesarray. Translating to Image.
2022-07-22T11:12:20,206 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Image of size (1654, 2339) loaded. Start Preprocessing.
2022-07-22T11:12:20,206 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Preprocessing image...
2022-07-22T11:12:20,342 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Starting Inference.
2022-07-22T11:12:20,343 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Running predictions on cuda.
2022-07-22T11:12:20,349 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Compute predictions.
2022-07-22T11:12:20,869 [INFO ] pool-2-thread-2 ACCESS_LOG - /169.254.178.2:40608 &quot;GET /ping HTTP/1.1&quot; 200 0
2022-07-22T11:12:20,870 [INFO ] pool-2-thread-2 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:container-0.local,timestamp:null
2022-07-22T11:12:22,119 [INFO ] W-9000-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:1923.3|#ModelName:model,Level:Model|#hostname:container-0.local,requestID:f49f15ab-aed4-4ecf-80e2-22910f5d578e,timestamp:1658488342
2022-07-22T11:12:22,120 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1925
2022-07-22T11:12:22,121 [INFO ] W-9000-model_1.0 ACCESS_LOG - /169.254.178.2:40592 &quot;POST /invocations HTTP/1.1&quot; 500 1940
2022-07-22T11:12:22,122 [INFO ] W-9000-model_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:container-0.local,timestamp:null
2022-07-22T11:12:22,122 [INFO ] W-9000-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:container-0.local,timestamp:null
2022-07-22T11:12:22,122 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:4|#Level:Host|#hostname:container-0.local,timestamp:null
2022-07-22T11:12:22,172 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1658488342171
2022-07-22T11:12:22,177 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend received inference at: 1658488342
</code></pre>",1,0,2022-07-22 12:10:44.407000 UTC,,,-1,pytorch|amazon-sagemaker|torchscript|torchserve,44,2020-07-02 07:57:09.090000 UTC,2022-09-23 09:01:56.670000 UTC,,109,2,0,1,,,,,,['amazon-sagemaker']
Naming a Sagemaker Processing job using Sagemaker Pipelines ProcessingStep,"<p>I am running a Sagemaker Pipeline with the current processor:</p>
<pre><code>from sagemaker.sklearn.processing import SKLearnProcessor


framework_version = &quot;0.23-1&quot;

sklearn_processor = SKLearnProcessor(
    framework_version=framework_version,
    instance_type=processing_instance_type,
    instance_count=processing_instance_count,
    base_job_name=&quot;pre-processing-job-name&quot;,
    role=role
)
</code></pre>
<p>and the processing step is:</p>
<pre><code>from sagemaker.processing import ProcessingInput, ProcessingOutput
from sagemaker.workflow.steps import ProcessingStep


step_process = ProcessingStep(
    name=&quot;AbaloneProcess&quot;,
    processor=sklearn_processor,
    inputs=[
        ProcessingInput(source=input_data, destination=&quot;/opt/ml/processing/input&quot;),
    ],
    outputs=[
        ProcessingOutput(output_name=&quot;train&quot;, source=&quot;/opt/ml/processing/train&quot;),
        ProcessingOutput(output_name=&quot;validation&quot;, source=&quot;/opt/ml/processing/validation&quot;),
        ProcessingOutput(output_name=&quot;test&quot;, source=&quot;/opt/ml/processing/test&quot;),
    ],
    code=&quot;abalone/preprocessing.py&quot;,
)
</code></pre>
<p>It looks like the base_job_name does nothing, because the processing job that is created is <code>pipelines-o6e2jn38g05j-AbaloneProcess-nc2OlXF8jA</code>.</p>
<p>I want the processing job name to be defined manually. Does Sagemaker pipelines support this? I seem to be going around in circles.</p>",1,0,2021-12-24 02:45:35.627000 UTC,,2021-12-24 07:52:29.443000 UTC,2,amazon-web-services|machine-learning|data-science|amazon-sagemaker,417,2021-12-24 02:35:43.617000 UTC,2022-02-01 20:03:06.273000 UTC,,21,0,0,3,,,,,,['amazon-sagemaker']
How to get own failure info in case of failed SageMaker Training Job?,"<p><strong>My main purpose:</strong> Easy way to collect information about different failure scenarios in SageMaker TrainingJob.</p>
<p><strong>What do I use currently?</strong></p>
<p>Sagemaker SKLearn Estimator(TrainingJobs are inside)</p>
<p><strong>Where will my model train?</strong></p>
<p>Different datasets. So, I need control and collect all information about all training processes and their final statuses on different datasets.</p>
<p><strong>Which failure scenarios do I have?</strong></p>
<p>There are plenty of them. I have create my own python Errors for them.
For example: 1. There are labels only for one class. 2. Too small dataset(by my own criterions) 3. Missing data for crucial columns 4. e.t.c.</p>
<p><strong>Where am I stuck?</strong></p>
<p>After failed training I can't get own errors from training job response. All of them are <code>ExecuteUserScriptError</code> I can't pass my own info in <code>FailureReason</code> or <code>ErrorMessage</code>(always it's empty). I see which error was raised in CloudWatchLogs and TrainingJob Failure Traceback(from SagemakerNotebook). So, bad solution is parse CloudWatchLogs or <code>/opt/ml/output/failure</code> in case of failure with Lambda function.</p>
<p><strong>Question: How to provide my own <code>ErrorMessage</code> or <code>FailureReason</code> in TrainingJob description?</strong></p>",0,1,2022-05-17 09:33:37.410000 UTC,,,1,python|amazon-web-services|monitoring|amazon-sagemaker,152,2022-05-17 08:56:09.633000 UTC,2022-09-13 09:12:48.457000 UTC,,11,0,0,0,,,,,,['amazon-sagemaker']
Machine Learning Suggestions,<p>I have data of a lot of students who got selected by some colleges based on their marks. Iam new to machine Learning. Can I have some suggestions how can I add Azure Machine Learning for predicting the colleges that they can get based on their marks</p>,8,0,2015-11-18 18:31:03.857000 UTC,1.0,2016-02-01 04:21:37.667000 UTC,7,azure-machine-learning-studio,851,2013-06-30 07:02:12.520000 UTC,2022-01-11 18:27:32.863000 UTC,Hyderabad,562,27,2,75,,,,,,['azure-machine-learning-studio']
Missing required parameter error while using Amazon SageMaker Ground Truth through AWS Console,"<p>We are hitting the following error when attempting to submit a single label text labeling job through SageMaker Ground Truth via the AWS Console: <code>MissingRequiredParameter: Missing required key 'PreHumanTaskLambdaArn' in params.HumanTaskConfig</code></p>
<p>Input text files are present in S3, and clicking &quot;Complete data setup&quot; creates the manifest with no issues.</p>
<p>Amazon Mechanical Turk workers are selected as the Worker Type, with only the &quot;Price per task&quot; changed.</p>
<p>The task type is set to Text Classification (Single Label).</p>
<p>When setting up the &quot;Text classification labeling tool (Single Label)&quot;, the preview shows the task exactly as we would expect to see it.</p>",0,0,2022-05-10 08:13:26.250000 UTC,,,0,amazon-web-services|amazon-sagemaker|mechanicalturk,92,2019-10-24 15:01:34.950000 UTC,2022-05-23 11:24:16.923000 UTC,"Leeds, UK",1,0,0,1,,,,,,['amazon-sagemaker']
How to pass a DataPath PipelineParameter from AzureDatafactory to AzureMachineLearningExecutePipeline Activity?,"<BR>
<ul>
<li><p>I am trying to read a file from a Blob Storage, load to pandas and write it to a BlobStorage</p>
</li>
<li><p>I have an Azure Machine Learning Pipeline with a PythonScriptStep that takes 2 PipelineParameters and are DataPaths as below.</p>
<pre class=""lang-py prettyprint-override""><code>from azureml.core import Datastore
from azureml.data.datapath import DataPath, DataPathComputeBinding, DataReference
from azureml.pipeline.core import PipelineParameter

datastore = Datastore(ws, &quot;SampleStore&quot;)
in_raw_path_default = 'somefolder/raw/alerts/2020/08/03/default_in.csv'
in_cleaned_path_default= 'somefolder/cleaned/alerts/2020/08/03/default_out.csv'

in_raw_datapath = DataPath(datastore=datastore, path_on_datastore=in_raw_path_default)
in_raw_path_pipelineparam = PipelineParameter(name=&quot;inrawpath&quot;, default_value=in_raw_datapath)
raw_datapath_input = (in_raw_path_pipelineparam, DataPathComputeBinding(mode='mount'))

in_cleaned_datapath = DataPath(datastore=datastore, path_on_datastore=in_cleaned_path_default)
in_cleaned_path_pipelineparam = PipelineParameter(name=&quot;incleanedpath&quot;, default_value=in_cleaned_datapath)
cleaned_datapath_input = (in_cleaned_path_pipelineparam, DataPathComputeBinding(mode='mount'))

from azureml.pipeline.steps import PythonScriptStep

source_directory = script_folder + '/pipeline_Steps'
dataprep_step = PythonScriptStep(
    script_name=&quot;SimpleTest.py&quot;, 
    arguments=[&quot;--input_data&quot;, raw_datapath_input, &quot;--cleaned_data&quot;, cleaned_datapath_input],
    inputs=[raw_datapath_input, cleaned_datapath_input],    
    compute_target=default_compute, 
    source_directory=source_directory,
    runconfig=run_config,
    allow_reuse=True
)

from azureml.pipeline.core import Pipeline
pipeline_test = Pipeline(workspace=ws, steps=[dataprep_step])

test_raw_path = DataPath(datastore=datastore, path_on_datastore='samplefolder/raw/alerts/2017/05/31/test.csv')
test_cleaned_path = DataPath(datastore=datastore, path_on_datastore='samplefolder/cleaned/alerts/2020/09/03')
pipeline_run_msalerts = Experiment(ws, 'SampleExperiment').submit(pipeline_test, pipeline_parameters={&quot;inrawpath&quot;  : test_raw_path,
                                                                                                        &quot;incleanedpath&quot; : test_cleaned_path})```

</code></pre>
</li>
</ul>
<p>This is the Script Used(SimpleTest.py):<BR></p>
<pre class=""lang-py prettyprint-override""><code>import os
import sys
import argparse
import pathlib
import azureml.core
import pandas as pd

parser = argparse.ArgumentParser(&quot;datapreponly&quot;)
parser.add_argument(&quot;--input_data&quot;, type=str)
parser.add_argument(&quot;--cleaned_data&quot;, type=str)

args = parser.parse_args()

print(&quot;Argument 1: %s&quot; % args.input_data)
print(&quot;Argument 2: %s&quot; % args.cleaned_data)

testDf = pd.read_csv(args.input_data, error_bad_lines=False)
print('Total Data Shape' + str(testDf.shape))

if not (args.cleaned_data is None):
    output_path = args.cleaned_data
    os.makedirs(output_path, exist_ok=True)
    outdatapath = output_path + '/alert.csv'    
    testDf.to_csv(outdatapath, index=False)
</code></pre>
<p><strong>Triggering this AzureMLPipeline from AzureDataFactory :</strong><BR>
The above code works fine by executing the ML pipeline in AzureMLWorkspace/PipelineSDK. I am trying to trigger the AzureMLpipeline from AzureDataFactory(AzureMachineLearningExecutePipeline) activity as follows<BR></p>
<p><img src=""https://i.stack.imgur.com/Jo0dD.png"" alt=""enter image description here"" /></p>
<p>Tried a debug run as follows by passing 2 string input paths<BR>
rawdatapath = &quot;samplefolder/raw/alerts/2017/05/31/test.csv&quot;<BR>
cleaneddatapath = &quot;samplefolder/raw/cleaned/2020/09/03/&quot;</p>
<p><img src=""https://i.stack.imgur.com/BByYd.png"" alt=""enter image description here"" /></p>
<pre><code>Current directory:  /mnt/batch/tasks/shared/LS_root/jobs/myazuremlworkspace/azureml/d8ee11ea-5838-46e5-a8ce-da2fbff5aade/mounts/workspaceblobstore/azureml/d8ee11ea-5838-46e5-a8ce-da2fbff5aade
Preparing to call script [ SimpleTest.py ] 
with arguments:
 ['--input_data', '/mnt/batch/tasks/shared/LS_root/jobs/myazuremlworkspace/azureml/d8ee11ea-5838-46e5-a8ce-da2fbff5aade/mounts/SampleStore/somefolder/raw/alerts/2020/08/03/default_in.csv',
 '--cleaned_data', '/mnt/batch/tasks/shared/LS_root/jobs/myazuremlworkspace/azureml/d8ee11ea-5838-46e5-a8ce-da2fbff5aade/mounts/SampleStore/somefolder/cleaned/alerts/2020/08/03/default_out.csv']
After variable expansion, calling script [ SimpleTest.py ] with arguments:
 ['--input_data', '/mnt/batch/tasks/shared/LS_root/jobs/myazuremlworkspace/azureml/d8ee11ea-5838-46e5-a8ce-da2fbff5aade/mounts/SampleStore/somefolder/raw/alerts/2020/08/03/default_in.csv',
 '--cleaned_data', '/mnt/batch/tasks/shared/LS_root/jobs/myazuremlworkspace/azureml/d8ee11ea-5838-46e5-a8ce-da2fbff5aade/mounts/SampleStore/somefolder/cleaned/alerts/2020/08/03/default_out.csv']

Script type = None
Argument 1: /mnt/batch/tasks/shared/LS_root/jobs/myazuremlworkspace/azureml/d8ee11ea-5838-46e5-a8ce-da2fbff5aade/mounts/SampleStore/somefolder/raw/alerts/2020/08/03/default_in.csv
Argument 2: /mnt/batch/tasks/shared/LS_root/jobs/myazuremlworkspace/azureml/d8ee11ea-5838-46e5-a8ce-da2fbff5aade/mounts/SampleStore/somefolder/cleaned/alerts/2020/08/03/default_out.csv
.......................
FileNotFoundError: [Errno 2] No such file or directory: '/mnt/batch/tasks/shared/LS_root/jobs/myazuremlworkspace/azureml/d8ee11ea-5838-46e5-a8ce-da2fbff5aade/mounts/SampleStore/somefolder/raw/alerts/2020/08/03/default_in.csv'
</code></pre>
<p>It shows that the default path is taken instead of the pipeline parameter(<em>No such File or directory error is less important as the main point is the default path is taken instead of the pipeline parameters</em>). I doubt its because of pass the pipelineparameter as a string instead of a datapath.<BR>
<BR><br />
<strong>FINALLY THE QUESTION</strong> :  How to pass a datapath to an AzureMLPipelineActivity from Azure Data Factory?</p>
<p><BR>Thanks.</p>",3,0,2020-09-03 19:14:41.487000 UTC,,2020-09-08 10:51:54.143000 UTC,3,azure-data-factory|azure-data-factory-2|azure-machine-learning-studio|azure-machine-learning-service,1758,2019-11-28 11:07:18.203000 UTC,2022-09-23 21:26:15.933000 UTC,,350,19,1,34,,,,,,"['azure-machine-learning-studio', 'azure-machine-learning-service']"
"dvc.api.read() raises an ""UnicodeDecodeError""","<p>I am trying to acess a DICOM file [image saved in the Digital Imaging and Communications in Medicine (DICOM) format]:</p>
<pre><code>import dvc.api

path = 'dir/image.dcm'
remote = 'remote_name'
repo = 'git_repo'
mode = 'r'

data = dvc.api.read(path = path, remote = remote, repo = repo, mode = mode)
</code></pre>
<p>When I run the previous code, and after the &quot;downloading progress bar&quot; is complete, I get the following error:</p>
<pre><code>Traceback (most recent call last): File &quot;draft.py&quot;, line 7, in &lt;module&gt; mode ='r') File &quot;C:\Users\lbrandao\anaconda3\envs\my_env\lib\site-packages\dvc\api.py&quot;, line 91, in read return fd.read() File &quot;C:\Users\lbrandao\anaconda3\envs\my_env\lib\encodings\cp1252.py&quot;, line 23, in decode return codecs.charmap_decode(input,self.errors,decoding_table)[0] UnicodeDecodeError: 'charmap' codec can't decode byte 0x81 in position 764: character maps to &lt;undefined&gt;
</code></pre>
<p>I tried to overcome this issue by using the encoding argument:</p>
<pre><code>data = dvc.api.read(path = path, remote = remote, repo = repo, mode = mode, encoding='ANSI')
</code></pre>
<p>Since, when I open a DICOM file using for example Notepad++, this is the encoding specified. However, it raises the error:</p>
<pre><code>Exception ignored in: &lt;bound method Pool.__del__ of &lt;dvc.fs.pool.Pool object at 0x0000021D1347A160&gt;&gt; Traceback (most recent call last): File &quot;C:\Users\lbrandao\anaconda3\envs\my_env\lib\site-packages\dvc\fs\pool.py&quot;, line 42, in __del__ File &quot;C:\Users\lbrandao\anaconda3\envs\my_env\lib\site-packages\dvc\fs\pool.py&quot;, line 46, in close File &quot;C:\Users\lbrandao\anaconda3\envs\my_env\lib\site-packages\dvc\fs\ssh\connection.py&quot;, line 71, in close File &quot;C:\Users\lbrandao\anaconda3\envs\my_env\lib\site-packages\paramiko\sftp_client.py&quot;, line 194, in close File &quot;C:\Users\lbrandao\anaconda3\envs\my_env\lib\site-packages\paramiko\sftp_client.py&quot;, line 185, in _log File &quot;C:\Users\lbrandao\anaconda3\envs\my_env\lib\site-packages\paramiko\sftp.py&quot;, line 158, in _log File &quot;C:\Users\lbrandao\anaconda3\envs\my_env\lib\logging\__init__.py&quot;, line 1372, in log File &quot;C:\Users\lbrandao\anaconda3\envs\my_env\lib\logging\__init__.py&quot;, line 1441, in _log File &quot;C:\Users\lbrandao\anaconda3\envs\my_env\lib\logging\__init__.py&quot;, line 1411, in makeRecord TypeError: 'NoneType' object is not callable
</code></pre>
<p>I also tried <code>encoding = 'utf-8'</code>, but the &quot;UnicodeDecodeError&quot; continues to appear:</p>
<pre><code>Traceback (most recent call last): File &quot;draft.py&quot;, line 7, in &lt;module&gt; mode ='r', encoding='utf-8') File &quot;C:\Users\lbrandao\anaconda3\envs\ccab_env_dev\lib\site-packages\dvc\api.py&quot;, line 91, in read return fd.read() File &quot;C:\Users\lbrandao\anaconda3\envs\ccab_env_dev\lib\codecs.py&quot;, line 321, in decode (result, consumed) = self._buffer_decode(data, self.errors, final) UnicodeDecodeError: 'utf-8' codec can't decode byte 0xd0 in position 140: invalid continuation byte
</code></pre>
<p>Can anyone please help? Thanks.</p>",1,4,2021-08-11 13:32:25.447000 UTC,,2021-08-11 15:09:10.723000 UTC,1,python|encoding|dvc,186,2019-09-30 10:15:09.623000 UTC,2022-09-19 14:01:39.763000 UTC,,95,13,0,12,,,,,,['dvc']
Run AutoML_execution failed with exception - Azure ML Studio JupyterLab,"<p>I was trying to do a time series experiment as per this article - <a href=""https://docs.microsoft.com/en-in/azure/machine-learning/how-to-auto-train-forecast"" rel=""nofollow noreferrer"">Auto-train a time-series forecast model</a></p>

<p>At first run, I was using the following code block.</p>

<pre><code>time_series_settings = {
""time_column_name"": ""day_datetime"",
""grain_column_names"": [""store""],
""max_horizon"": ""auto"",
""target_lags"": ""auto"",
""target_rolling_window_size"": ""auto"",
""preprocess"": True,
</code></pre>

<p>}</p>

<p>Then, I decided to push it to limits. started changing the horizon to higher units. I ran into a limit based on the training data, which was fine. I simply had to use a lower horizon value. Then, that error stopped and I started getting an error of this format. </p>

<pre><code>Run AutoML_6f4dff67-59e2-46b3-ba56-54f35d3813a6_27 failed with exception
</code></pre>

<p>I thought, it was an issue with the VM size, so I switched back to the default values, from the original notebook which ran successfully half an hour ago. that also had stopped working. </p>

<p>So, what to do? That error wont go away.</p>",1,0,2020-03-07 16:27:00.747000 UTC,,,0,azure|azure-machine-learning-studio|azure-machine-learning-service|azure-machine-learning-workbench,119,2015-09-15 16:27:17.953000 UTC,2022-09-24 06:49:58.907000 UTC,"Bangalore, Karnataka, India",2272,1340,67,516,,,,,,"['azure-machine-learning-workbench', 'azure-machine-learning-studio', 'azure-machine-learning-service']"
Wandb login permission denied python virtual environment,"<p>I am using a remote Slurm cluster from my university and want to get access to my wandb profile. I run my py project in a virtual env with <code>python 3.7.4</code>, where I could install successfully <code>wandb</code>.</p>
<p>However, when I try to login from command line <code>python -m wandb login</code> I get this error</p>
<pre><code>(374_env1) [userX@peregrine ~]$ python -m wandb login
Traceback (most recent call last):
File &quot;/software/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/runpy.py&quot;, line 193, in _run_module_as_main
&quot;__main__&quot;, mod_spec)
File &quot;/software/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/runpy.py&quot;, line 85, in _run_code
exec(code, run_globals)
File &quot;/data/userX/.envs/374_env1/lib/python3.7/site-packages/wandb/__main__.py&quot;, line 1, in &lt;module&gt;
from wandb.cli import cli
File &quot;/data/userX/.envs/374_env1/lib/python3.7/site-packages/wandb/cli/cli.py&quot;, line 53, in &lt;module&gt;
datefmt=&quot;%Y-%m-%d %H:%M:%S&quot;,
File &quot;/software/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/logging/__init__.py&quot;, line 1895, in basicConfig
h = FileHandler(filename, mode)
File &quot;/software/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/logging/__init__.py&quot;, line 1087, in __init__
StreamHandler.__init__(self, self._open())
File &quot;/software/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/logging/__init__.py&quot;, line 1116, in _open
return open(self.baseFilename, self.mode, encoding=self.encoding)
PermissionError: [Errno 13] Permission denied: '/local/tmp/debug-cli.log'
</code></pre>
<p>I tried also to login outside of the venv but still permission denied. Is it because as a user from the cluster I don't have enough <code>-m</code> permissions? I also tried <code>python -u wandb login</code> but still permission denied. ps. I don't have <code>sudo</code> permission.
Any insights?</p>",0,0,2022-03-16 18:20:02.023000 UTC,,2022-03-16 18:35:44.270000 UTC,0,python-3.x|virtualenv|wandb,337,2013-04-21 14:40:47.053000 UTC,2022-08-24 09:41:25.140000 UTC,,642,126,0,116,,,,,,['wandb']
How to access Mlflow running on fargate (ECS) with only VPN in/outbound rules from sagemaker notebook instance?,"<p><strong>Context:</strong></p>
<p>I have deployed Mlflow on ECS(Fargate) using terraform using this public <a href=""https://github.com/Glovo/terraform-aws-mlflow"" rel=""nofollow noreferrer"">git-repo</a>. After deploying Mlflow which was publicly accessible using the link, I made some changes in the security group and changed in/outbound rule to the only company VPN ips, now that link is only accessible under the VPN.</p>
<p><strong>Question:</strong></p>
<p>Now I have Sagemake notebook instance and want to access that link inside the notebook and the notebook is running on AWS internet(outside Company-VPN) and I'm not able to access that link. What could be the possible solution?</p>
<p>I don't want to open access of Mlflow-link publicaly to accessible form anywhere on the internet.</p>
<p><strong>Running this code on notebook:</strong></p>
<pre><code>!pip install mlflow
import mlflow
mlflow.set_tracking_uri(&quot;http://mlflow-mlp-xyz-xyz.eu-west-1.elb.amazonaws.com/&quot;)
mlflow.get_experiment_by_name('mlpmlflowlogger')
current_experiment=dict(mlflow.get_experiment_by_name('mlpmlflowlogger'))
print(current_experiment)
experiment_id=current_experiment['experiment_id']
print(experiment_id)
</code></pre>",0,1,2022-09-01 13:31:23.460000 UTC,,,0,amazon-web-services|amazon-ecs|amazon-vpc|aws-security-group|mlflow,31,2017-11-29 13:03:57.980000 UTC,2022-09-20 13:40:47.487000 UTC,"Deggendorf, Germany",813,87,1,115,,,,,,['mlflow']
How to run sagemaker processing and training job?,"<p>I have created a processing and training job in sagemaker from console that is of left side of the panel of sagemaker, that processing job has no option to run it from console. Can someone tell how I can run that training job? Do I need a sagemaker notebook to run it? Any ideas?</p>
<p><img src=""https://i.stack.imgur.com/Hm0rs.png"" alt="""" /></p>",1,1,2021-10-11 16:42:42.330000 UTC,,2021-10-11 19:08:00.030000 UTC,0,amazon-web-services|machine-learning|amazon-sagemaker,124,2018-04-06 02:05:12.400000 UTC,2021-10-11 16:36:09.603000 UTC,,1,0,0,0,,,,,,['amazon-sagemaker']
Vertex AI deploy custom model to an endpoint,"<p>After I run</p>
<pre><code>gcloud beta ai endpoints deploy-model (ENDPOINT : --region=REGION) --display-name=DISPLAY_NAME --model=MODEL [--accelerator=[count=COUNT],[type=TYPE]] [--deployed-model-id=DEPLOYED_MODEL_ID] [--disable-container-logging] [--enable-access-logging] [--machine-type=MACHINE_TYPE] [--max-replica-count=MAX_REPLICA_COUNT] [--min-replica-count=MIN_REPLICA_COUNT] [--service-account=SERVICE_ACCOUNT] [--traffic-split=[DEPLOYED_MODEL_ID=VALUE,…]] [GCLOUD_WIDE_FLAG …]
</code></pre>
<p>I got error &quot;(gcloud.beta.ai.endpoints.deploy-model) INVALID_ARGUMENT: AUTOMATIC_RESOURCES is not one of the supported deployment resources types for Model projects/.../locations/us-central1/models/...</p>
<p>What does this mean?</p>",1,0,2022-07-15 00:56:05.487000 UTC,,,0,google-cloud-vertex-ai,63,2021-04-01 22:58:50.923000 UTC,2022-07-20 08:28:55.803000 UTC,,1,0,0,2,,,,,,['google-cloud-vertex-ai']
Convert sagemaker model (MXNet) to ONNX: infer_shape error,"<h1>Working</h1>

<p>I'm working on sagemaker jupyter notebook (environement: <code>anaconda3/envs/mxnet_p36/lib/python3.6</code>).</p>

<p>I run <strong>successfully</strong> this tutorial: <a href=""https://github.com/onnx/tutorials/blob/master/tutorials/MXNetONNXExport.ipynb"" rel=""nofollow noreferrer"">https://github.com/onnx/tutorials/blob/master/tutorials/MXNetONNXExport.ipynb</a></p>

<hr>

<h1>Not working</h1>

<p>Then, on the same evironement, I tried to apply the same process to files generated by a sagemaker training job. So, I used as input the <strong>S3 model artifact</strong> files, changing some lines of the tutorial code to meet my needs.
I used built in object detection SSD VGG-16 network with hyperparameter image_shape: 300.</p>

<pre><code>sym = './model_algo_1-symbol.json'
params = './model_algo_1-0000.params'
input_shape = (1,3,300,300)
</code></pre>

<p>And <code>verbose=True</code> as last parameter in <code>export_model()</code> method:</p>

<pre><code>converted_model_path = onnx_mxnet.export_model(sym, params, [input_shape], np.float32, onnx_file, True)
</code></pre>

<p>When I run the code I got <strong>this error</strong> (<strong>verbose output</strong> at the end of the post):</p>

<pre><code>MXNetError: Error in operator multibox_target: [14:36:32] src/operator/contrib/./multibox_target-inl.h:224: Check failed: lshape.ndim() == 3 (-1 vs. 3) : Label should be [batch, num_labels, label_width] tensor
</code></pre>

<hr>

<h1>Question</h1>

<p>I was not able to find any solution so far:</p>

<ul>
<li>maybe the <code>input_shape = (1,3,300,300)</code> is wrong, but I'm not able to
find it out;</li>
<li>maybe the model contains some unexpected layer or so;</li>
</ul>

<p>Does anybody knows a way to fix this problem or a workaround to use the model on a local machine? 
<br>(I mean without having to deploy to aws)</p>

<p><hr>
The <strong>verbose output</strong>:</p>

<pre><code>  infer_shape error. Arguments:
  data: (1, 3, 300, 300)
  conv3_2_weight: (256, 256, 3, 3)
  fc7_bias: (1024,)
  multi_feat_3_conv_1x1_conv_weight: (128, 512, 1, 1)
  conv4_1_bias: (512,)
  conv5_3_bias: (512,)
  relu4_3_cls_pred_conv_bias: (16,)
  multi_feat_2_conv_3x3_relu_cls_pred_conv_weight: (24, 512, 3, 3)
  relu4_3_loc_pred_conv_bias: (16,)
  relu7_cls_pred_conv_weight: (24, 1024, 3, 3)
  conv3_3_bias: (256,)
  multi_feat_5_conv_3x3_relu_cls_pred_conv_weight: (16, 256, 3, 3)
  conv4_3_weight: (512, 512, 3, 3)
  conv1_2_bias: (64,)
  multi_feat_2_conv_3x3_relu_cls_pred_conv_bias: (24,)
  multi_feat_4_conv_3x3_conv_weight: (256, 128, 3, 3)
  conv4_1_weight: (512, 256, 3, 3)
  relu4_3_scale: (1, 512, 1, 1)
  multi_feat_4_conv_3x3_conv_bias: (256,)
  multi_feat_5_conv_3x3_relu_cls_pred_conv_bias: (16,)
  conv2_2_weight: (128, 128, 3, 3)
  multi_feat_3_conv_3x3_relu_loc_pred_conv_weight: (24, 256, 3, 3)
  multi_feat_5_conv_3x3_conv_bias: (256,)
  conv5_1_bias: (512,)
  multi_feat_3_conv_3x3_conv_bias: (256,)
  conv2_1_bias: (128,)
  conv5_2_weight: (512, 512, 3, 3)
  multi_feat_5_conv_3x3_relu_loc_pred_conv_weight: (16, 256, 3, 3)
  multi_feat_4_conv_3x3_relu_loc_pred_conv_weight: (16, 256, 3, 3)
  multi_feat_2_conv_3x3_conv_weight: (512, 256, 3, 3)
  multi_feat_2_conv_1x1_conv_bias: (256,)
  multi_feat_2_conv_1x1_conv_weight: (256, 1024, 1, 1)
  conv4_3_bias: (512,)
  relu7_cls_pred_conv_bias: (24,)
  fc6_bias: (1024,)
  conv2_1_weight: (128, 64, 3, 3)
  multi_feat_2_conv_3x3_conv_bias: (512,)
  multi_feat_2_conv_3x3_relu_loc_pred_conv_weight: (24, 512, 3, 3)
  multi_feat_5_conv_1x1_conv_bias: (128,)
  relu7_loc_pred_conv_bias: (24,)
  multi_feat_3_conv_3x3_relu_loc_pred_conv_bias: (24,)
  conv3_3_weight: (256, 256, 3, 3)
  conv1_2_weight: (64, 64, 3, 3)
  multi_feat_2_conv_3x3_relu_loc_pred_conv_bias: (24,)
  conv1_1_bias: (64,)
  multi_feat_4_conv_3x3_relu_cls_pred_conv_bias: (16,)
  conv4_2_weight: (512, 512, 3, 3)
  conv5_3_weight: (512, 512, 3, 3)
  relu7_loc_pred_conv_weight: (24, 1024, 3, 3)
  multi_feat_3_conv_3x3_conv_weight: (256, 128, 3, 3)
  conv3_1_weight: (256, 128, 3, 3)
  multi_feat_4_conv_3x3_relu_cls_pred_conv_weight: (16, 256, 3, 3)
  relu4_3_loc_pred_conv_weight: (16, 512, 3, 3)
  multi_feat_5_conv_3x3_conv_weight: (256, 128, 3, 3)
  fc7_weight: (1024, 1024, 1, 1)
  conv4_2_bias: (512,)
  multi_feat_3_conv_3x3_relu_cls_pred_conv_weight: (24, 256, 3, 3)
  multi_feat_3_conv_3x3_relu_cls_pred_conv_bias: (24,)
  conv2_2_bias: (128,)
  conv5_1_weight: (512, 512, 3, 3)
  multi_feat_3_conv_1x1_conv_bias: (128,)
  multi_feat_4_conv_3x3_relu_loc_pred_conv_bias: (16,)
  conv1_1_weight: (64, 3, 3, 3)
  multi_feat_4_conv_1x1_conv_bias: (128,)
  conv3_1_bias: (256,)
  multi_feat_5_conv_3x3_relu_loc_pred_conv_bias: (16,)
  multi_feat_4_conv_1x1_conv_weight: (128, 256, 1, 1)
  fc6_weight: (1024, 512, 3, 3)
  multi_feat_5_conv_1x1_conv_weight: (128, 256, 1, 1)
  conv3_2_bias: (256,)
  conv5_2_bias: (512,)
  relu4_3_cls_pred_conv_weight: (16, 512, 3, 3)
</code></pre>",0,0,2020-01-02 15:02:22.223000 UTC,,,2,python|amazon-sagemaker|mxnet|onnx,271,2015-08-18 11:23:52.727000 UTC,2022-09-24 06:22:49.123000 UTC,,10705,2035,2,781,,,,,,['amazon-sagemaker']
How can I use the predictive experiment of Azure ML on my website,"<p>I used Azure ML studio and I obtained a predictive experiment with an API key.</p>

<p><strong>My question is:</strong><br>
How can I embed this API key or the text obtained or the predictive experiment on a website or on my portfolio?<br>
So that others can use the predictive experiment from the website...</p>",0,0,2018-04-25 14:00:56.743000 UTC,,2019-02-19 08:59:34.003000 UTC,1,azure|azure-machine-learning-studio,28,2018-04-25 13:40:32.880000 UTC,2018-05-02 07:50:37.023000 UTC,,11,0,0,2,,,,,,['azure-machine-learning-studio']
How to deploy the model using sagemaker which has no model artifacts,<p>I want to deploy the model which has no artifacts(model zip file) using sagemaker and use its endpoint in my application to get the results. Can someone help me in doing this</p>,1,0,2021-01-27 14:27:28.960000 UTC,,,1,amazon-web-services|amazon-ec2|endpoint|amazon-sagemaker|amazon-ecr,277,2018-05-16 11:06:48.480000 UTC,2022-09-09 14:17:43.800000 UTC,,103,4,0,40,,,,,,['amazon-sagemaker']
How to fix 'incorrect artifact/model path on HDFS showing on MLflow server',"<p>I run a mlflow server with the following command using mlflow, version 1.2.0</p>

<pre><code>mlflow server --host myhost -p myport --backend-store-uri mysql://user@localhost/mlflow --default-artifact-root hdfs://myhost/user/myid/mlflow_test
</code></pre>

<p>I run the experiment from MLflow tutorial quickstart <a href=""https://www.mlflow.org/docs/latest/quickstart.html"" rel=""nofollow noreferrer"">https://www.mlflow.org/docs/latest/quickstart.html</a></p>

<p>the command:</p>

<pre><code>mlflow run sklearn_elasticnet_wine -P alpha=0.5 --no-conda
</code></pre>

<p>the code to log the model is </p>

<pre><code>mlflow.sklearn.log_model(lr, ""model"")
</code></pre>

<p>in </p>

<pre><code>https://github.com/mlflow/mlflow/blob/master/examples/sklearn_elasticnet_wine/train.py
</code></pre>

<p>I visit the server by webbrowser myhost: myport and check the run I ran.</p>

<p>I successfully get the ran info by myhost: <code>myport/#/experiments/0/runs/run_id</code></p>

<p>in this page, i found that the first layer (model directory) path is correct. that is, <code>run_id/artifacts/model</code>
<a href=""https://i.stack.imgur.com/bJZWv.png"" rel=""nofollow noreferrer"">correct path</a></p>

<p>but once I click the MLmodel file under model folder, the path get wrong:
I expect to see        <code>run_id/artifacts/model/MLmodel</code>
but actually it was    <code>run_id/artifacts/MLmodel</code>
<a href=""https://i.stack.imgur.com/WIhVR.png"" rel=""nofollow noreferrer"">wrong path</a></p>",0,0,2019-08-19 07:47:14.727000 UTC,,2019-08-19 09:52:29.487000 UTC,2,mlflow,812,2019-08-19 07:38:46.393000 UTC,2019-12-27 04:02:09.053000 UTC,,21,0,0,3,,,,,,['mlflow']
How to plot on kedro mlflow ui x1=array/list/dict and y1=array/list/dict?,"<p>I am new to kedro, and I don't know if I am asking the right question here.</p>
<p>Is it possible on kedro mlflow ui to plot x and y lists?</p>
<p>I am running kedro pipeline with mlflow. I have catalog.yaml which I log metrics and artifacts.</p>
<p>The end goal is:</p>
<p>kedro run 1  # generate x1[1,2,3,4] and y1=[1,2,2,2] these numbers are just examples</p>
<p>kedro run 2  # generate x2[1,2,3,4] and y2=[3,1,2,1] these numbers are just examples</p>
<p>kedro run 3  # generate x2[1,2,3,4] and y2=[1,3,3,3] these numbers are just examples</p>
<p>then kedro mlflow ui</p>
<p>select run1, run2, and run3 then click compare.</p>
<p>on scatter plot ---&gt; able to select x1, x2, and x3 and for y axis able to select y1, y2, and y3</p>
<p>then I should be able to see plot with three lines.</p>
<p>Something like this:
<a href=""https://i.stack.imgur.com/Pfg9C.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Pfg9C.png"" alt=""enter image description here"" /></a></p>
<p>thank you for your help.</p>",1,0,2022-04-01 23:26:28.277000 UTC,,,0,python|machine-learning|pipeline|mlflow|kedro,44,2014-06-23 03:13:07.580000 UTC,2022-09-22 00:35:55.773000 UTC,,21,0,0,29,,,,,,['mlflow']
MLFlow RestException: RESOURCE_ALREADY_EXISTS error when starting runs,"<p>Previously was using ML FLow with Databricks on Azure Machine Learning to register and track model Hyperparameter tuning with both SKLearn and Stats model models from start of September with no issues. But since about 23rd October, I started getting these kinds of errors:</p>
<p>RestException: RESOURCE_ALREADY_EXISTS: Failed to create AML experiment for experiment id=863468136127724, name=/my-experiment3, artifactLocation=dbfs:/databricks/mlflow-tracking/863468136127724. There is an existing AML experiment with id=c74bdea3-e382-4cdf-868a-ee1421de078e and name='/adb/5909321886823418/863468136127724/my-experiment3' and artifactLocation='' that is not compatible.</p>
<p><a href=""https://i.stack.imgur.com/n1qmL.png"" rel=""nofollow noreferrer"">Even when running a newly created experiment, it will throw this error</a></p>
<p>We recently have updated to ml flow v1.21.0  but it doesn't seem to be a bug as there is nothing on the ML Flow github that is similar, just wondering if anyone has encountered anything similar as I run out of ideas of things to look for the issue.</p>",1,3,2021-11-08 20:22:30.083000 UTC,1.0,,1,mlflow,618,2021-11-08 20:13:38.383000 UTC,2021-12-15 19:26:40.927000 UTC,,11,0,0,1,,,,,,['mlflow']
Ho do I call a webservice deployed in Azure Machine Learning Service?,"<p>I'm trying Azure Machine Learning Service for the first time. I worked on examples given by MS. I was able to develop in Python and deploy as web services. But I'm not able to use it outside the notebook or any application. Do I need API of that web service? If yes, where can I find it?</p>

<p>I have not got anything to try on. I googled for different methods, but couldn't find relevant link. So I didn't try much there.</p>",1,0,2019-04-04 11:48:47.737000 UTC,,2020-12-23 01:23:12.400000 UTC,0,python-3.x|azure-machine-learning-service,163,2018-12-20 07:13:19.703000 UTC,2021-02-01 05:49:25.620000 UTC,,13,0,0,10,,,,,,['azure-machine-learning-service']
"R error in Azure Machine Learning, not in R Studio","<p>I'm facing a most annoying behavior where a R script works juts fine in R Studio and generates an error in Azure ML.</p>

<p>I first I thought it was about inputs and outputs difference, but as you can see in the script below, I removed dependencies to input and outputs.</p>

<p>The error is generated by the call to <code>chartr</code>: ""old"" is longer than ""new"".</p>

<p>Any input is appreciated.</p>

<pre><code>accented_characters &lt;- list('Š'='S', 'š'='s', 'Ž'='Z', 'ž'='z', 'À'='A', 'Á'='A', 'Â'='A', 'Ã'='A', 'Ä'='A', 'Å'='A', 'Æ'='A', 'Ç'='C', 'È'='E', 'É'='E',
                        'Ê'='E', 'Ë'='E', 'Ì'='I', 'Í'='I', 'Î'='I', 'Ï'='I', 'Ñ'='N', 'Ò'='O', 'Ó'='O', 'Ô'='O', 'Õ'='O', 'Ö'='O', 'Ø'='O', 'Ù'='U',
                        'Ú'='U', 'Û'='U', 'Ü'='U', 'Ý'='Y', 'Þ'='B', 'ß'='Ss', 'à'='a', 'á'='a', 'â'='a', 'ã'='a', 'ä'='a', 'å'='a', 'æ'='a', 'ç'='c',
                        'è'='e', 'é'='e', 'ê'='e', 'ë'='e', 'ì'='i', 'í'='i', 'î'='i', 'ï'='i', 'ð'='o', 'ñ'='n', 'ò'='o', 'ó'='o', 'ô'='o', 'õ'='o',
                        'ö'='o', 'ø'='o', 'ù'='u', 'ú'='u', 'û'='u', 'ý'='y', 'ý'='y', 'þ'='b', 'ÿ'='y' )

input &lt;- data.frame(text = c(""some piZzaé pizZa word a to : here $"",""or there € with 28'89.5""))
stop_words &lt;- data.frame(international = c('pizza'))

stop_words &lt;- as.character(stop_words$international)
stop_words &lt;- gsub(""^\\s+|\\s+$"", """", stop_words) # trim
stop_words &lt;- tolower(stop_words) # lowercase

input &lt;- as.character(input$text)
input &lt;- gsub(""[[:space:]]+"", ' ', input) # remove multiple spaces
input &lt;- gsub(""[1-9!\""#$€%&amp;'()*+,./:;&lt;=&gt;@}~^_|`\\?\\[\\{]+"", '', input) # remove punctuation, numbers and some others. Note, does not remove closing bracket, can't figure out why
input &lt;- chartr(paste(names(accented_characters), collapse = ''),
            paste(accented_characters, collapse = ''), input) # remove accents
input &lt;- tolower(input) # lowercase everything         
input &lt;- gsub(""\\b[a-z]{1,2}\\b"", '', input) #remove too short words
input &lt;- gsub(paste(stop_words, ""|""), '', input) # remove stop words

input &lt;- data.frame(input) # set as data.frame class
</code></pre>",1,2,2015-08-05 09:46:52.707000 UTC,,,1,r|azure|azure-machine-learning-studio,453,2012-07-30 15:08:34.857000 UTC,2022-06-10 20:51:40.947000 UTC,"Paris, France",2742,347,8,291,,,,,,['azure-machine-learning-studio']
"In Azure Machine Learning, where are log files stored?","<p>Every time I submit and execute a Run in Azure Machine Learning, I end up getting logs. I can see the logs by going to the Run page and clicking on 'Outputs+logs' tab (see the image).
Here I am curious about where the logs actually are. I have an Azure Blob storage but can't find the logs there. Where are they?</p>
<p><a href=""https://i.stack.imgur.com/MYHwt.png"" rel=""nofollow noreferrer"">enter image description here</a></p>",2,0,2021-07-03 15:25:02.243000 UTC,,,2,azure|machine-learning|azure-machine-learning-studio|azure-machine-learning-service,537,2021-01-13 12:52:21.460000 UTC,2022-09-14 07:35:58.047000 UTC,"Seoul, South Korea",31,0,0,4,,,,,,"['azure-machine-learning-studio', 'azure-machine-learning-service']"
Azure Machine Learning Studio - Predict multiple values,"<p>I am trying to build a model using the designer in Azure Machine Learning Studio that will need to predict multiple values simultaneously.</p>
<p>The scenario I am working with is there are a set of codes assigned to an order. When the order is audited, the codes may be adjusted (either added, removed, changed or a combination of all). What this results in is a before and after set of codes assigned to an order.</p>
<p>What I want to do is feed the before and after codes plus other characteristics of the order into a model so going forward it is possible to provide suggestions to the auditor on potential changes to 'before' codes to create the 'after' codes.</p>
<p>I want to understand if this can be done using the designer or if it would need to be done using code. If it can be done with the designer, how would it be done?</p>
<p>With the experimentation I have done so far, I can only find ways for it to predict a single value, not a collection or values.</p>",0,0,2022-02-03 06:36:10.150000 UTC,,2022-02-03 13:45:12.543000 UTC,0,design-patterns|azure-machine-learning-studio,48,2019-06-07 03:58:24.440000 UTC,2022-02-03 21:34:33.547000 UTC,"Melbourne VIC, Australia",1,0,0,0,,,,,,['azure-machine-learning-studio']
How to create MLOps vertex ai pipeline with custom sklearn code?,"<p>I'm trying to build MLOps pipeline using vertex ai but failing to deploy it</p>
<pre><code>@dsl.pipeline(
    # Default pipeline root. You can override it when submitting the pipeline.
    pipeline_root=PIPELINE_ROOT,
    # A name for the pipeline. Use to determine the pipeline Context.
    name=&quot;pipeline-test-1&quot;,
)
def pipeline(
serving_container_image_uri: str = &quot;us-docker.pkg.dev/cloud-aiplatform/prediction/tf2-cpu.2-3:latest&quot;
):
    dataset_op = get_data()
    train_op = train_xgb_model(dataset_op.outputs[&quot;dataset_train&quot;])
    train_knn = knn_model(dataset_op.outputs[&quot;dataset_train&quot;])
    
    eval_op = eval_model(
        test_set=dataset_op.outputs[&quot;dataset_test&quot;],
        xgb_model=train_op.outputs[&quot;model_artifact&quot;],
        knn_model=train_knn.outputs['best_model_artifact']
    )
    
    endpoint_op = gcc_aip.ModelDeployOp(
    project=PROJECT_ID,
    model=eval_op.outputs[&quot;model_artifacts&quot;],
    machine_type=&quot;n1-standard-4&quot;,
    )
    
    #endpoint_op.after(eval_op)
    
compiler.Compiler().compile(pipeline_func=pipeline,
        package_path='xgb_pipe.json')
</code></pre>
<p>gcc_aip.ModelDeployOp is throwing error that correct model id or name should be pass</p>",0,0,2021-08-01 22:23:54.787000 UTC,1.0,,1,mlops|google-cloud-vertex-ai,154,2015-11-25 08:05:38.667000 UTC,2022-06-15 12:39:30.893000 UTC,,73,3,0,55,,,,,,['google-cloud-vertex-ai']
"mlflow cannot load model , no module optimizer_experimental","<p>I am trying to load a model using mlflow.</p>
<pre><code>model = mlflow.keras.load_model(model_path, custom_objects=custom_objects)
</code></pre>
<p>An Error occured:</p>
<p>ModuleNotFoundError: No module named 'keras.optimizers.optimizer_experimental'; 'keras.optimizers' is not a package</p>
<p>Please any ideas?</p>
<p>I am using these versions:
mlflow                        1.24.0
tensorflow                    2.8.2
keras                         2.8.0</p>",0,0,2022-06-20 15:20:46.573000 UTC,,,0,python|tensorflow|keras|mlflow,69,2020-08-11 13:26:47.053000 UTC,2022-09-23 15:21:54.383000 UTC,,21,1,0,8,,,,,,['mlflow']
AWS Sagemaker: Is there a way to look at at the coefficients of the regressors of a 'linear learner - binary_classifier' model?,"<p>I'm interested in getting a little more information on the model that's being built. Namely, what are the coefficients of the regressors that it is using for the prediction?</p>

<p>The loss function is automatically set to 'logistic'. I am assuming this means it is building a typical logistic regression model.</p>",1,0,2018-10-02 20:35:59.400000 UTC,,,0,amazon-web-services|amazon-sagemaker,422,2018-10-02 20:27:28.187000 UTC,2021-06-24 15:20:06.333000 UTC,"Boulder, CO, USA",1,0,0,3,,,,,,['amazon-sagemaker']
Microsoft Azure Bot Framework Emulator is not sending message,"<p>On my Windows 10, I installed <a href=""https://docs.microsoft.com/en-us/azure/bot-service/bot-service-debug-emulator?view=azure-bot-service-3.0"" rel=""nofollow noreferrer"">Microsoft Azure Bot Framework Emulator</a> from the <a href=""https://github.com/Microsoft/BotFramework-Emulator/releases"" rel=""nofollow noreferrer"">here</a> (I had used <code>botframework-emulator-setup-4.0.15-alpha.exe</code> from the GitHub link that Microsoft Article provided).</p>

<p>But when I run the Emulator, configure a bot and send a text message it says <code>couldn't send</code> (as highlighted in snapshot 3 below:</p>

<p>Step 1: Install Emulator
<a href=""https://i.stack.imgur.com/IXN0m.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IXN0m.png"" alt=""enter image description here""></a>
Step2: Create a new bot Configuration
<a href=""https://i.stack.imgur.com/zaFYh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zaFYh.png"" alt=""enter image description here""></a>
Step3: Send message
<a href=""https://i.stack.imgur.com/O1U8C.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/O1U8C.png"" alt=""enter image description here""></a></p>",2,0,2018-05-16 19:48:44.817000 UTC,,,0,azure|azure-machine-learning-studio|azure-bot-service,967,2012-02-25 04:28:19.340000 UTC,2022-09-24 17:06:32.277000 UTC,,19815,2703,22,2272,,,,,,['azure-machine-learning-studio']
"Azureml : error ""The SSL connection could not be established, see inner exception."" while creating Tabular Dataset from Azure Blob Storage file","<p>I have a new error using Azure ML maybe due to the Ubuntu upgrade to 22.04 which I did yesterday.</p>
<p>I have a workspace azureml created through the portal and I can access it whitout any issue with python SDK</p>
<pre><code>from azureml.core import Workspace
ws = Workspace.from_config(&quot;config/config.json&quot;)
ws.get_details()
</code></pre>
<p>output</p>
<pre><code>{'id': '/subscriptions/XXXXX/resourceGroups/gr_louis/providers/Microsoft.MachineLearningServices/workspaces/azml_lk',
 'name': 'azml_lk',
 'identity': {'principal_id': 'XXXXX',
  'tenant_id': 'XXXXX',
  'type': 'SystemAssigned'},
 'location': 'westeurope',
 'type': 'Microsoft.MachineLearningServices/workspaces',
 'tags': {},
 'sku': 'Basic',
 'workspaceid': 'XXXXX',
 'sdkTelemetryAppInsightsKey': 'XXXXX',
 'description': '',
 'friendlyName': 'azml_lk',
 'keyVault': '/subscriptions/XXXXX/resourceGroups/gr_louis/providers/Microsoft.Keyvault/vaults/azmllkXXXXX',
 'applicationInsights': '/subscriptions/XXXXX/resourceGroups/gr_louis/providers/Microsoft.insights/components/azmllkXXXXX',
 'storageAccount': '/subscriptions/XXXXX/resourceGroups/gr_louis/providers/Microsoft.Storage/storageAccounts/azmllkXXXXX',
 'hbiWorkspace': False,
 'provisioningState': 'Succeeded',
 'discoveryUrl': 'https://westeurope.api.azureml.ms/discovery',
 'notebookInfo': {'fqdn': 'ml-azmllk-westeurope-XXXXX.westeurope.notebooks.azure.net',
  'resource_id': 'XXXXX'},
 'v1LegacyMode': False}
</code></pre>
<p>I then use this workspace <code>ws</code> to upload a file (or a directory) to Azure Blob Storage like so</p>
<pre><code>from azureml.core import Dataset

ds = ws.get_default_datastore()

Dataset.File.upload_directory(
    src_dir=&quot;./data&quot;,
    target=ds,
    pattern=&quot;*dataset1.csv&quot;,
    overwrite=True,
    show_progress=True
)
</code></pre>
<p>which again works fine and outputs</p>
<pre><code>Validating arguments.
Arguments validated.
Uploading file to /
Filtering files with pattern matching *dataset1.csv
Uploading an estimated of 1 files
Uploading ./data/dataset1.csv
Uploaded ./data/dataset1.csv, 1 files out of an estimated total of 1
Uploaded 1 files
Creating new dataset

{
  &quot;source&quot;: [
    &quot;('workspaceblobstore', '//')&quot;
  ],
  &quot;definition&quot;: [
    &quot;GetDatastoreFiles&quot;
  ]
}
</code></pre>
<p>My file is indeed uploaded to Blob Storage and I can see it either on azure portal or on azure ml studio (ml.azure.com).
<a href=""https://i.stack.imgur.com/EM634.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EM634.png"" alt=""uploaded_file"" /></a></p>
<p>The error comes up when I try to create a Tabular dataset from the uploaded file. The following code doesn't work :</p>
<pre><code>from azureml.core import Dataset

data1 = Dataset.Tabular.from_delimited_files(
    path=[(ds, &quot;dataset1.csv&quot;)]
)
</code></pre>
<p>and it gives me the error :</p>
<pre><code>ExecutionError: 
Error Code: ScriptExecution.DatastoreResolution.Unexpected
Failed Step: XXXXXX
Error Message: ScriptExecutionException was caused by DatastoreResolutionException.
  DatastoreResolutionException was caused by UnexpectedException.
    Unexpected failure making request to fetching info for Datastore 'workspaceblobstore' in subscription: 'XXXXXX', resource group: 'gr_louis', workspace: 'azml_lk'. Using base service url: https://westeurope.experiments.azureml.net. HResult: 0x80131501.
      The SSL connection could not be established, see inner exception.
| session_id=XXXXXX
</code></pre>
<p>After some research, I assumed it might be due to openssl version (which now is 1.1.1) but I am not sure and I surely don't know how to fix it...any ideas ?</p>",1,2,2022-08-30 12:40:08.273000 UTC,,,-1,ssl|openssl|azure-blob-storage|azure-machine-learning-service|azureml-python-sdk,91,2020-02-03 13:10:46.663000 UTC,2022-09-22 15:41:25.667000 UTC,France,33,6,0,10,,,,,,['azure-machine-learning-service']
Jupyter kernel dying when reading parquet files from multiple folders into sagemaker,"<p>I am trying to read a very large amount of data from s3 parquet files into my SageMaker notebook instance. I am not sure how much data is too much data for the jupyter notebook to handle, so when I try a few files at once by providing a folder with many parquet files at once, the kernel dies. Is there anyway to proceed, given that I need to load even more data to train ML models later?</p>

<p>So far I tried a couple of different methods suggested here in StackOverflow, but since I am extremely new to AWS environment it has been hard to work things out by myself. Right now, the following code is what I am running:</p>

<pre class=""lang-py prettyprint-override""><code>
import s3fs
import pandas as pd
import boto3
import pyarrow.parquet as pq

# Creating an S3 Filesystem (Only required when using S3)

s3 = s3fs.S3FileSystem()
s3_path = ""s3://my_bucket_name""
directory = 'path/to/folder'
print(f'{s3_path}/{directory}')

# Loading Files (S3)

data = pq.ParquetDataset(f'{s3_path}/{directory}', filesystem = s3, validate_schema=False).read_pandas().to_pandas()

</code></pre>

<p>I had a bunch of errors trying to read parquet files like these but now it seems to be working when I try to read just one file. How should I proceed?</p>",0,3,2019-11-01 17:57:02.793000 UTC,,,0,python|amazon-s3|amazon-sagemaker|pyarrow,1298,2019-11-01 17:31:59.350000 UTC,2022-09-10 09:52:35.997000 UTC,"São Paulo, SP, Brasil",471,80,8,26,,,,,,['amazon-sagemaker']
Deploy/convert trained AWS SageMaker MXNet Model to iOS devices,"<p>I trained a MXnet SSD resnet-50 model with SageMaker Object Detection Algorithm and want to use it on iOS devices. Therefore I need to convert it to the Apple CoreML format. I tried with <a href=""https://github.com/apache/incubator-mxnet/tree/master/tools/coreml"" rel=""nofollow noreferrer"">mxnet-to-coreml</a>. </p>

<p>Maybe there are better ways to do it?
Does anyone know a convenient way to achieve this task? </p>

<p>My model consists of two files:</p>

<ul>
<li>resnet50_ssd_model-symbol.json</li>
<li>resnet50_ssd_model-0000.params</li>
</ul>

<p>Before converting the model, I set it to deploy-state by using ""deploy.py"" provided by <a href=""https://github.com/apache/incubator-mxnet/tree/master/example/ssd"" rel=""nofollow noreferrer"">MXnet</a>. </p>

<p>The <a href=""https://github.com/apache/incubator-mxnet/tree/master/tools/coreml"" rel=""nofollow noreferrer"">mxnet-to-coreml</a> converter fails with the following error:</p>

<blockquote>
  <p>raise TypeError(""MXNet layer of type %s is not supported."" % layer)
      TypeError: MXNet layer of type _copy is not supported.</p>
</blockquote>

<p>I invoked the converter script like this:</p>

<pre><code>python mxnet_coreml_converter.py --model-prefix='ssd_resnet50_512' -- 
epoch=0 --input-shape='{""data"":""3, 512, 512""}' --mode=classifier --pre- 
processing-arguments='{""image_input_names"":""data""}' --output- 
file=""resnet50.mlmodel""
</code></pre>",1,0,2019-08-06 08:29:27.663000 UTC,1.0,,1,object-detection|amazon-sagemaker|mxnet|coreml|coremltools,192,2013-11-16 17:19:00.603000 UTC,2022-09-23 13:22:47.613000 UTC,,667,181,0,20,,,,,,['amazon-sagemaker']
make clearml agent do not install envs for every task,"<p>I want make my clearml agent do not install python envs for every task, it take too long.</p>
<p>I tried setup config like: package_manager.system_size_packages=true, but it doesn't work.</p>
<p>clearml agent won't install python envs anymore</p>",1,0,2022-03-28 08:01:10.133000 UTC,,,-1,clearml,121,2022-03-28 07:56:51.643000 UTC,2022-03-29 11:40:36.717000 UTC,,1,0,0,0,,,,,,['clearml']
AMT Crowd Entity Annotation,"<p>I'm creating a task on AMT using the Crowd HTML Elements that let reviewers  label each word in a text block. Here is how the code looks:</p>
<pre><code>&lt;script src=&quot;https://assets.crowd.aws/crowd-html-elements.js&quot;&gt;&lt;/script&gt;


    
    &lt;crowd-entity-annotation
        name=&quot;crowd-entity-annotation&quot;
        header=&quot;Watch the video and label parts of the text below to indicate whether each word/phrase is correctly reflected in the video.&quot;
        labels=&quot;[{'label': 'included in the video', 'shortDisplayName': 'corr', 'fullDisplayName': 'correct'}, {'label': 'not included in the video', 'shortDisplayName': 'incorr', 'fullDisplayName': 'incorrect'}]&quot;
        text=&quot;This is a test block of text.&quot;
    &gt;
        &lt;full-instructions header=&quot;Do the text and the video match&quot;&gt;
            Apply labels to words or phrases based on whether they are correctly reflected in the video or not. 

            &lt;ol&gt;
                &lt;li&gt;In this task you will read a text and watch a video.&lt;/li&gt;
                &lt;li&gt;&lt;strong&gt;Read&lt;/strong&gt; the text and watch the video carefully and determine words/phrases in the text the are correcly reflected in the video.&lt;/li&gt;
                
                
            &lt;/ol&gt;
        &lt;/full-instructions&gt;

        &lt;short-instructions&gt;
            Apply labels to words or phrases based on whether they are correctly reflected in the video or not. 

            &lt;ol&gt;
                &lt;li&gt;In this task you will read a text and watch a video that depicts what is described in the text.&lt;/li&gt;
                &lt;li&gt;&lt;strong&gt;Read&lt;/strong&gt; the text and watch the video carefully and determine words/phrases in the text the are correcly reflected in the video.&lt;/li&gt;
                
            &lt;/ol&gt;
            
        &lt;/short-instructions&gt;


        &lt;div id=&quot;additionalQuestions&quot; style=&quot;margin-top: 20px&quot;&gt;
                
                &lt;video width=&quot;400&quot; height=&quot;400&quot; controls&gt;
                         &lt;source src=&quot;${video}&quot; type=&quot;video/mp4&quot;&gt;
                &lt;/video&gt;
                &lt;p&gt;&lt;/p&gt;
                &lt;h4&gt;Overall how would you rate the alignment between the video and text description?&lt;/h4&gt;
                &lt;crowd-radio-group&gt;
                    &lt;crowd-radio-button name=&quot;very poor&quot; value=&quot;1&quot;&gt;very poor&lt;/crowd-radio-button&gt;
                    &lt;crowd-radio-button name=&quot;poor&quot; value=&quot;2&quot;&gt;poor&lt;/crowd-radio-button&gt;
                    &lt;crowd-radio-button name=&quot;average&quot; value=&quot;3&quot;&gt;average&lt;/crowd-radio-button&gt;
                    &lt;crowd-radio-button name=&quot;good&quot; value=&quot;4&quot;&gt;good&lt;/crowd-radio-button&gt;
                    &lt;crowd-radio-button name=&quot;excellent&quot; value=&quot;5&quot;&gt;excellent&lt;/crowd-radio-button&gt;
                &lt;/crowd-radio-group&gt;
            &lt;/div&gt;

       
 
&lt;/crowd-entity-annotation&gt;


&lt;script&gt;

  document.addEventListener('all-crowd-elements-ready', () =&gt; {
    document
      .querySelector('crowd-entity-annotation')
      .shadowRoot
      .querySelector('crowd-form')
      .form
      .appendChild(additionalQuestions);
  });
&lt;/script&gt;
</code></pre>
<p>Right now the job can be submitted if at least one character in the text is labeled. I want to ensure that the reviewer selects a label for each word/character in the text before submitting the job.
What I am trying is to use <code>onsubmit</code> and a validation function in the script section to check whether the length of the labeled entities added together is equal to the total length of the text block but I can't access the output of the crowd entity annotation. This is what I tried but failed:</p>
<pre><code>const answers = document.querySelector('crowd-entity-annotation').value.entities 
</code></pre>
<p>I appreciate any help.</p>",0,0,2022-04-20 23:44:34.417000 UTC,,,0,amazon|amazon-sagemaker|mechanicalturk,100,2022-04-20 23:08:26.817000 UTC,2022-09-22 18:02:40.387000 UTC,,1,0,0,3,,,,,,['amazon-sagemaker']
Azure ML time series model inference error during data input (python),"<p>In the Azure ML Studio I prepared a model with AutoML for time series forecasting. The data have some rare gaps in all data sets.
I am using the following code to call for a deployed Azure AutoML model as a web service:</p>
<pre><code>import requests
import json
import pandas as pd

# URL for the web service
scoring_uri = 'http://xxxxxx-xxxxxx-xxxxx-xxxx.xxxxx.azurecontainer.io/score'
    
# Two sets of data to score, so we get two results back
new_data = pd.DataFrame([
            ['2020-10-04 19:30:00',1.29281,1.29334,1.29334,1.29334,1],
            ['2020-10-04 19:45:00',1.29334,1.29294,1.29294,1.29294,1],
            ['2020-10-04 21:00:00',1.29294,1.29217,1.29334,1.29163,34],
            ['2020-10-04 21:15:00',1.29217,1.29257,1.29301,1.29115,195]],
            columns=['1','2','3','4','5','6']        
)
# Convert to JSON string
input_data = json.dumps({'data': new_data.to_dict(orient='records')})

# Set the content type
headers = {'Content-Type': 'application/json'}
    
# Make the request and display the response
resp = requests.post(scoring_uri, input_data, headers=headers)
print(resp.text)
</code></pre>
<p>I am getting an error:</p>
<pre><code>{\&quot;error\&quot;: \&quot;DataException:\\n\\tMessage: No y values were provided. We expected non-null target values as prediction context because there is a gap between train and test and the forecaster depends on previous values of target. If it is expected, please run forecast() with ignore_data_errors=True. In this case the values in the gap will be imputed.\\n\\tInnerException: None\\n\\tErrorResponse \\n{\\n
</code></pre>
<p>I tried to add &quot;ignore_data_errors=True&quot; to different parts of the code without a success, hence, getting another error:</p>
<pre><code>TypeError: __init__() got an unexpected keyword argument 'ignore_data_errors'
</code></pre>
<p>I would very much appreciate any help as I am stuck at this.</p>",1,0,2021-01-07 04:26:10.233000 UTC,,2021-01-19 02:36:49.627000 UTC,1,python|azure|automl|azure-machine-learning-service,167,2020-10-28 09:40:54.057000 UTC,2022-09-14 12:38:08.697000 UTC,"Perth WA, Australia",41,33,0,9,,,,,,['azure-machine-learning-service']
Semantic segmentation with amazon AWS and S3 instance,"<p>This is probably a easy question but I have been stuck now for a time.
I want to train an FCN an Amazon AWS. For this I want to use the procedure used in this example ( <a href=""https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/semantic_segmentation_pascalvoc/semantic_segmentation_pascalvoc.ipynb"" rel=""nofollow noreferrer"">https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/semantic_segmentation_pascalvoc/semantic_segmentation_pascalvoc.ipynb</a> ) with my own datase.</p>

<p>In contrast to that procedure I have my train and annotation images (as .png) saved in one S3 bucket with four folders (Training, TrainingAnnotation, Validation, ValidationAnnotaion).The files in the folder for Training and Annotation have the same name.</p>

<p>I trained my model with followong code:</p>

<pre><code>%%time
import sagemaker
from sagemaker import get_execution_role

role = get_execution_role()
print(role)

bucket = sess.default_bucket()  
prefix = 'semantic-segmentation'
print(bucket)

from sagemaker.amazon.amazon_estimator import get_image_uri
training_image = get_image_uri(sess.boto_region_name, 'semantic-segmentation', repo_version=""latest"")
print (training_image)

s3_output_location = 's3://{}/{}/output'.format(bucket, prefix)
print(s3_output_location)

# Create the sagemaker estimator object.
ss_model = sagemaker.estimator.Estimator(training_image,
                                         role, 
                                         train_instance_count = 1, 
                                         train_instance_type = 'ml.p2.xlarge',
                                         train_volume_size = 50,
                                         train_max_run = 360000,
                                         output_path = s3_output_location,
                                         base_job_name = 'ss-notebook-demo',
                                         sagemaker_session = sess)
num_training_samples=5400
# Setup hyperparameters 
ss_model.set_hyperparameters(backbone='resnet-50', 
                             algorithm='fcn',                   
                             use_pretrained_model='True', 
                             crop_size=248, .                             
                             num_classes=4, 
                             epochs=10, 
                             learning_rate=0.0001,                             
                             optimizer='rmsprop', 'adam', 'rmsprop', 'nag', 'adagrad'.
                             lr_scheduler='poly', 'cosine' and 'step'.                           
                             mini_batch_size=16, 
                             validation_mini_batch_size=16,
                             early_stopping=True, 
                             early_stopping_patience=2, 
                             early_stopping_min_epochs=10,    
                             num_training_samples=num_training_samples) 
# Create full bucket names

bucket1 = 'imagelabel1' 
train_channel = 'Training'
validation_channel = 'Validation'
train_annotation_channel = 'TrainingAnnotation'
validation_annotation_channel =  'ValidataionAnnotation'


s3_train_data = 's3://{}/{}'.format(bucket1, train_channel)
s3_validation_data = 's3://{}/{}'.format(bucket1, validation_channel)
s3_train_annotation = 's3://{}/{}'.format(bucket1, train_annotation_channel)
s3_validation_annotation  = 's3://{}/{}'.format(bucket1, validation_annotation_channel)



distribution = 'FullyReplicated'
# Create sagemaker s3_input objects
train_data = sagemaker.session.s3_input(s3_train_data, distribution=distribution, 
                                        content_type='image/png', s3_data_type='S3Prefix')
validation_data = sagemaker.session.s3_input(s3_validation_data, distribution=distribution, 
                                        content_type='image/png', s3_data_type='S3Prefix')
train_annotation = sagemaker.session.s3_input(s3_train_annotation, distribution=distribution, 
                                        content_type='image/png', s3_data_type='S3Prefix')
validation_annotation = sagemaker.session.s3_input(s3_validation_annotation, distribution=distribution, 
                                        content_type='image/png', s3_data_type='S3Prefix')

data_channels = {'train': train_data, 
                 'validation': validation_data,
                 'train_annotation': train_annotation, 
                 'validation_annotation':validation_annotation}
s3://imagelabel1/Training
ss_model.fit(inputs=data_channels, logs=True)
</code></pre>

<p>The Errror Message is:</p>

<p>ValueError: Error for Training job ss-notebook-demo-2019-07-15-06-42-25-784: Failed Reason: ClientError: train channel is empty.</p>

<p>Does someone know what is wrong in this Code?</p>

<p>Thank you </p>

<p>Simon</p>",1,0,2019-07-14 13:22:22.100000 UTC,,2019-07-15 18:06:18.413000 UTC,0,amazon-web-services|amazon-s3|amazon-sagemaker|semantic-segmentation,204,2019-07-14 13:06:48.897000 UTC,2022-09-15 18:46:01.810000 UTC,Munich,11,0,0,0,,,,,,['amazon-sagemaker']
"Mlflow ""load_model"" goes in deadlock","<p>Trying to load a model from past run in mlflow, in jupyterlab, never finishes. After waiting for hours, interrupting the run throws the below state.</p>
<p><a href=""https://i.stack.imgur.com/NgxDL.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NgxDL.jpg"" alt=""enter image description here"" /></a></p>
<pre><code>---------------------------------------------------------------------------
KeyboardInterrupt                         Traceback (most recent call last)
Input In [21], in &lt;cell line: 2&gt;()
      1 logged_model = 'runs:/7f6932baef144fa69847ba11ef66f8e6/model/'
----&gt; 2 loaded_model = mlflow.tensorflow.load_model(logged_model)

File ~/.conda/envs/tensorflow/lib/python3.8/site-packages/mlflow/tensorflow/__init__.py:397, in load_model(model_uri, dst_path)
    360 def load_model(model_uri, dst_path=None):
    361     &quot;&quot;&quot;
    362     Load an MLflow model that contains the TensorFlow flavor from the specified path.
    363 
   (...)
    395                                 for _, output_signature in signature_definition.outputs.items()]
    396     &quot;&quot;&quot;
--&gt; 397     local_model_path = _download_artifact_from_uri(artifact_uri=model_uri, output_path=dst_path)
    398     flavor_conf = _get_flavor_configuration(local_model_path, FLAVOR_NAME)
    399     _add_code_from_conf_to_system_path(local_model_path, flavor_conf)

File ~/.conda/envs/tensorflow/lib/python3.8/site-packages/mlflow/tracking/artifact_utils.py:95, in _download_artifact_from_uri(artifact_uri, output_path)
     92     parsed_uri = parsed_uri._replace(path=posixpath.dirname(parsed_uri.path))
     93     root_uri = prefix + urllib.parse.urlunparse(parsed_uri)
---&gt; 95 return get_artifact_repository(artifact_uri=root_uri).download_artifacts(
     96     artifact_path=artifact_path, dst_path=output_path
     97 )

File ~/.conda/envs/tensorflow/lib/python3.8/site-packages/mlflow/store/artifact/runs_artifact_repo.py:125, in RunsArtifactRepository.download_artifacts(self, artifact_path, dst_path)
    110 def download_artifacts(self, artifact_path, dst_path=None):
    111     &quot;&quot;&quot;
    112     Download an artifact file or directory to a local directory if applicable, and return a
    113     local path for it.
   (...)
    123     :return: Absolute path of the local filesystem location containing the desired artifacts.
    124     &quot;&quot;&quot;
--&gt; 125     return self.repo.download_artifacts(artifact_path, dst_path)

File ~/.conda/envs/tensorflow/lib/python3.8/site-packages/mlflow/store/artifact/artifact_repo.py:242, in ArtifactRepository.download_artifacts(self, artifact_path, dst_path)
    240 # Check if the artifacts points to a directory
    241 if self._is_directory(artifact_path):
--&gt; 242     dst_local_path, inflight_downloads = async_download_artifact_dir(
    243         src_artifact_dir_path=artifact_path, dst_local_dir_path=dst_path
    244     )
    245 else:
    246     inflight_downloads = async_download_artifact(
    247         src_artifact_path=artifact_path, dst_local_dir_path=dst_path
    248     )

File ~/.conda/envs/tensorflow/lib/python3.8/site-packages/mlflow/store/artifact/artifact_repo.py:208, in ArtifactRepository.download_artifacts.&lt;locals&gt;.async_download_artifact_dir(src_artifact_dir_path, dst_local_dir_path)
    206 for file_info in dir_content:
    207     if file_info.is_dir:
--&gt; 208         inflight_downloads += async_download_artifact_dir(
    209             src_artifact_dir_path=file_info.path,
    210             dst_local_dir_path=dst_local_dir_path,
    211         )[2]
    212     else:
    213         inflight_downloads += async_download_artifact(
    214             src_artifact_path=file_info.path,
    215             dst_local_dir_path=dst_local_dir_path,
    216         )

File ~/.conda/envs/tensorflow/lib/python3.8/site-packages/mlflow/store/artifact/artifact_repo.py:199, in ArtifactRepository.download_artifacts.&lt;locals&gt;.async_download_artifact_dir(src_artifact_dir_path, dst_local_dir_path)
    195 local_dir = os.path.join(dst_local_dir_path, src_artifact_dir_path)
    196 inflight_downloads = []
    197 dir_content = [  # prevent infinite loop, sometimes the dir is recursively included
    198     file_info
--&gt; 199     for file_info in self.list_artifacts(src_artifact_dir_path)
    200     if file_info.path != &quot;.&quot; and file_info.path != src_artifact_dir_path
    201 ]
    202 if not dir_content:  # empty dir
    203     if not os.path.exists(local_dir):

File ~/.conda/envs/tensorflow/lib/python3.8/site-packages/mlflow/store/artifact/sftp_artifact_repo.py:94, in SFTPArtifactRepository.list_artifacts(self, path)
     92 artifact_dir = self.path
     93 list_dir = posixpath.join(artifact_dir, path) if path else artifact_dir
---&gt; 94 if not self.sftp.isdir(list_dir):
     95     return []
     96 artifact_files = self.sftp.listdir(list_dir)

File ~/.conda/envs/tensorflow/lib/python3.8/site-packages/pysftp/__init__.py:652, in Connection.isdir(self, remotepath)
    650 self._sftp_connect()
    651 try:
--&gt; 652     result = S_ISDIR(self._sftp.stat(remotepath).st_mode)
    653 except IOError:     # no such file
    654     result = False

File ~/.conda/envs/tensorflow/lib/python3.8/site-packages/paramiko/sftp_client.py:493, in SFTPClient.stat(self, path)
    491 path = self._adjust_cwd(path)
    492 self._log(DEBUG, &quot;stat({!r})&quot;.format(path))
--&gt; 493 t, msg = self._request(CMD_STAT, path)
    494 if t != CMD_ATTRS:
    495     raise SFTPError(&quot;Expected attributes&quot;)

File ~/.conda/envs/tensorflow/lib/python3.8/site-packages/paramiko/sftp_client.py:822, in SFTPClient._request(self, t, *arg)
    820 def _request(self, t, *arg):
    821     num = self._async_request(type(None), t, *arg)
--&gt; 822     return self._read_response(num)

File ~/.conda/envs/tensorflow/lib/python3.8/site-packages/paramiko/sftp_client.py:852, in SFTPClient._read_response(self, waitfor)
    850 while True:
    851     try:
--&gt; 852         t, data = self._read_packet()
    853     except EOFError as e:
    854         raise SSHException(&quot;Server connection dropped: {}&quot;.format(e))

File ~/.conda/envs/tensorflow/lib/python3.8/site-packages/paramiko/sftp.py:201, in BaseSFTP._read_packet(self)
    200 def _read_packet(self):
--&gt; 201     x = self._read_all(4)
    202     # most sftp servers won't accept packets larger than about 32k, so
    203     # anything with the high byte set (&gt; 16MB) is just garbage.
    204     if byte_ord(x[0]):

File ~/.conda/envs/tensorflow/lib/python3.8/site-packages/paramiko/sftp.py:185, in BaseSFTP._read_all(self, n)
    183             break
    184 else:
--&gt; 185     x = self.sock.recv(n)
    187 if len(x) == 0:
    188     raise EOFError()

File ~/.conda/envs/tensorflow/lib/python3.8/site-packages/paramiko/channel.py:699, in Channel.recv(self, nbytes)
    686 &quot;&quot;&quot;
    687 Receive data from the channel.  The return value is a string
    688 representing the data received.  The maximum amount of data to be
   (...)
    696     if no data is ready before the timeout set by `settimeout`.
    697 &quot;&quot;&quot;
    698 try:
--&gt; 699     out = self.in_buffer.read(nbytes, self.timeout)
    700 except PipeTimeout:
    701     raise socket.timeout()

File ~/.conda/envs/tensorflow/lib/python3.8/site-packages/paramiko/buffered_pipe.py:160, in BufferedPipe.read(self, nbytes, timeout)
    158 while (len(self._buffer) == 0) and not self._closed:
    159     then = time.time()
--&gt; 160     self._cv.wait(timeout)
    161     if timeout is not None:
    162         timeout -= time.time() - then

File ~/.conda/envs/tensorflow/lib/python3.8/threading.py:302, in Condition.wait(self, timeout)
    300 try:    # restore state no matter what (e.g., KeyboardInterrupt)
    301     if timeout is None:
--&gt; 302         waiter.acquire()
    303         gotit = True
    304     else:

KeyboardInterrupt:
</code></pre>
<p>The mlflow tracking server is working properly for all the other operations. I am able to log params, metrics and artifacts. But I am not able to load a model or retrive any of the artifacts.</p>
<p>Update:</p>
<p>Looks like a bug as per <a href=""https://github.com/mlflow/mlflow/issues/5656"" rel=""nofollow noreferrer"">https://github.com/mlflow/mlflow/issues/5656</a>.</p>",1,0,2022-04-27 22:08:48.723000 UTC,,2022-04-27 22:28:50.080000 UTC,0,python|paramiko|mlflow,229,2016-11-15 12:10:53.253000 UTC,2022-09-23 07:04:48.820000 UTC,,40,31,0,12,,,,,,['mlflow']
Sagemaker Endpoint returns error when trying to make a prediction in the Sagemaker Notebook Instances,"<p>I've deployed an inference pipeline endpoint on sagemaker, but now when I try to make a prediction call but I get an error message and to the best of my knowledge I am following the examples shown here:
<a href=""https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/scikit_learn_inference_pipeline/Inference%20Pipeline%20with%20Scikit-learn%20and%20Linear%20Learner.ipynb"" rel=""nofollow noreferrer"">https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/scikit_learn_inference_pipeline/Inference%20Pipeline%20with%20Scikit-learn%20and%20Linear%20Learner.ipynb</a></p>
<p>The code to make an endpoint call is here:</p>
<pre><code>from sagemaker.predictor import json_serializer, csv_serializer, json_deserializer, RealTimePredictor
from sagemaker.content_types import CONTENT_TYPE_CSV, CONTENT_TYPE_JSON

predictor = RealTimePredictor(
    endpoint='Boston-inf-pipeline-July09-endpoint',
    sagemaker_session=sagemaker_session,
    serializer=csv_serializer,
    content_type=CONTENT_TYPE_CSV,
    accept=CONTENT_TYPE_CSV)


from sklearn.pipeline import Pipeline
from sklearn.preprocessing import Binarizer, StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer

column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']

dff = pd.read_csv('housing.csv',delimiter=r&quot;\s+&quot;, names=column_names)

dff.drop('MEDV',axis=1,inplace=True)

 
#String
x = '0.00632,18.0,2.31,0,0.538,6.575,65.2,4.09,1,296.0,15.3,396.9,4.98'

#DataFrame
y= dff.head(1)

#Array
z = np.array([0.00632,18.0,2.31,0,0.538,6.575,65.2,4.09,1,296.0,15.3,396.9,4.98])

print(predictor.predict(x))**strong text**
</code></pre>
<p>and here is the error message:</p>
<p><a href=""https://i.stack.imgur.com/6mAQD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6mAQD.png"" alt=""enter image description here"" /></a></p>
<p>and here is what is says on Cloud Watch</p>
<p><a href=""https://i.stack.imgur.com/aLY2r.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/aLY2r.png"" alt=""enter image description here"" /></a></p>",1,0,2020-07-16 16:00:18.970000 UTC,,2020-07-16 17:55:24.870000 UTC,1,python|amazon-web-services|machine-learning|amazon-sagemaker,300,2020-06-24 04:34:57.527000 UTC,2020-08-13 05:48:23.647000 UTC,,11,0,0,7,,,,,,['amazon-sagemaker']
Separators in data file for azure ml studio,"<p>I have csv file news.csv with such data:</p>

<pre><code>ID \t TITLE \t URL \t PUBLISHER \t CATEGORY \t STORY \t HOSTNAME \t TIMESTAMP
</code></pre>

<p>But Azure ML studio experiments dont see Separators \t and when I try to select column I cant do it. How to fix it?</p>",1,1,2019-03-31 17:00:28.587000 UTC,,,1,azure|azure-machine-learning-studio,109,2019-03-22 07:26:07.403000 UTC,2019-05-06 15:32:30.967000 UTC,,67,2,0,13,,,,,,['azure-machine-learning-studio']
"Retrieving the cpu, gpu specs on a deployed Deep Learning VM","<p>I already deployed a Deep Learning VM by configuring cpu and gpu. I would like to retrieve that information.</p>
<p>Is there a way to find out what gpu and cpu specification my deployed DeepLearning VM has?</p>",1,0,2022-07-05 05:13:13.507000 UTC,,2022-07-06 00:47:02.413000 UTC,0,google-cloud-platform|deep-learning|google-cloud-vertex-ai|google-ai-platform|google-dl-platform,61,2013-05-26 03:45:19.747000 UTC,2022-09-22 01:29:51.950000 UTC,,311,6,0,35,,,,,,['google-cloud-vertex-ai']
How to set-up MLFlow artifact sftp store on a remote Linux machine?,"<p>My goal is to configure ML Flow to run on a remote Linux server, with logs stored in the PostgreSQL database and artifacts in /home/aw/mlfow/mllogs, where &quot;aw&quot; is my user name with root privileges.</p>
<p>When I run my simple python code (see section below):</p>
<ol>
<li><p>when it's last line (&quot;mlflow.log_artifact(&quot;features.txt&quot;)) is commented out: I get no errors in Python, but when I enter log details via browser, in the artifact section I get the warning: <code>Unable to list artifacts stored undersftp://mlflow_user@194.39.141.27:~/mlflow/mlruns/0/d1eb9ce83b6b4ede96a9ea5203c097da/artifacts</code></p>
</li>
<li><p>in case the last line is active, python compiler returns a long list of errors, ending with <code>ValueError: Port could not be cast to integer value as '~'</code>
I tried running the server from the CLI in many different ways, each time changing the --default-artifact-root parameter (see section below). Interestingly, the printout of the mlflow.get_artifact_uri() variable from the Python code remains the same, despite changes in the CLI server parameters: it always shows up as
<code>sftp://mlflow_user@94.39.141.27:~/mlflow/mlruns/0/c613c110839946a3adc198377cc82c0c/artifacts</code></p>
</li>
</ol>
<p>So, it looks like there is a problem of setting this parameter up during MLFlow server run from CLI. Maybe it's cached somewhere? Linux server reboot doesn't help.</p>
<p>Code to reproduce issue
Server CLI run (credentials are just examples):</p>
<p>option 1 (based on <a href=""https://towardsdatascience.com/setup-mlflow-in-production-d72aecde7fef"" rel=""nofollow noreferrer"">https://towardsdatascience.com/setup-mlflow-in-production-d72aecde7fef</a>):
<code>mlflow server --backend-store-uri postgresql://mlflow_user:mlflow321@localhost/mlflow_db --default-artifact-root sftp://mlflow_user@194.39.141.27:~/mlflow/mlruns -h 0.0.0.0 -p 8000&amp;</code></p>
<p>options 2.1 and 2.2 (aw is my user name on the machine, with root privileges):
<code>mlflow server --backend-store-uri postgresql://mlflow_user:mlflow321@localhost/mlflow_db --default-artifact-root sftp://aw:aw_pass@194.39.141.27:~/mlflow/mlruns -h 0.0.0.0 -p 8000&amp;</code></p>
<p><code>mlflow server --backend-store-uri postgresql://mlflow_user:mlflow321@localhost/mlflow_db --default-artifact-root sftp://aw:@194.39.141.27:~/home/aw/mlflow/mlruns -h 0.0.0.0 -p 8000&amp;</code></p>
<p>option 3:
<code>mlflow server --backend-store-uri postgresql://mlflow_user:mlflow321@localhost/mlflow_db --default-artifact-root sftp://mlflow_user:mlflow_pass#@194.39.141.27:~/mlflow/mlruns -h 0.0.0.0 -p 8000&amp;</code></p>
<p>Python code:</p>
<pre><code>import mlflow

if __name__ == &quot;__main__&quot;:
    mlflow.set_tracking_uri(&quot;http://194.39.141.27:8000&quot;) #hostname IP here is just an example

    features = &quot;rooms, zipcode, median_price, school_rating, transport&quot;
    with open(&quot;features.txt&quot;, 'w') as f:
        f.write(features)

    with mlflow.start_run():

        tracking_uri = mlflow.get_tracking_uri()
        artifact_uri = mlflow.get_artifact_uri()
        print(&quot;Tracking uri: {}&quot;.format(tracking_uri))
        print(&quot;Artifact uri: {}&quot;.format(artifact_uri))

        mlflow.log_artifact(&quot;features.txt&quot;)
</code></pre>
<p>Other info / logs
Python compiler log:</p>
<pre><code>File &quot;1.py&quot;, line 18, in
mlflow.log_artifact(&quot;features.txt&quot;)
File &quot;/home/aw/anaconda3/lib/python3.8/site-packages/mlflow/tracking/fluent.py&quot;, line 544, in log_artifact
MlflowClient().log_artifact(run_id, local_path, artifact_path)
File &quot;/home/aw/anaconda3/lib/python3.8/site-packages/mlflow/tracking/client.py&quot;, line 903, in log_artifact
self._tracking_client.log_artifact(run_id, local_path, artifact_path)
File &quot;/home/aw/anaconda3/lib/python3.8/site-packages/mlflow/tracking/_tracking_service/client.py&quot;, line 271, in log_artifact
artifact_repo = self._get_artifact_repo(run_id)
File &quot;/home/aw/anaconda3/lib/python3.8/site-packages/mlflow/tracking/_tracking_service/client.py&quot;, line 262, in _get_artifact_repo
return get_artifact_repository(artifact_uri)
File &quot;/home/aw/anaconda3/lib/python3.8/site-packages/mlflow/store/artifact/artifact_repository_registry.py&quot;, line 102, in get_artifact_repository
return _artifact_repository_registry.get_artifact_repository(artifact_uri)
File &quot;/home/aw/anaconda3/lib/python3.8/site-packages/mlflow/store/artifact/artifact_repository_registry.py&quot;, line 71, in get_artifact_repository
return repository(artifact_uri)
File &quot;/home/aw/anaconda3/lib/python3.8/site-packages/mlflow/store/artifact/sftp_artifact_repo.py&quot;, line 32, in init
&quot;port&quot;: parsed.port,
File &quot;/home/aw/anaconda3/lib/python3.8/urllib/parse.py&quot;, line 174, in port
raise ValueError(message) from None
ValueError: Port could not be cast to integer value as '~'
</code></pre>
<p>To conclude: My main goal is to store MLFLow artifacts on a remote Linux server. I would really appreciate any help on how to do this?</p>",0,0,2021-05-08 16:53:10.340000 UTC,,2021-05-08 17:00:11.583000 UTC,3,python|mlflow,498,2019-10-22 18:02:58.057000 UTC,2022-09-17 11:08:32.793000 UTC,"Lublin, Polska",95,4,0,14,,,,,,['mlflow']
Conda/Pip Environment creation fails with azureml & adlfs/opencensus-ext-azure,"<p>I am trying to create a local development environment using conda with <code>azureml</code> libraries. Following environment.yml file works fine.</p>
<pre><code>name: cortixml_azure_env
channels:
  - conda-forge
  - defaults
dependencies:
  - python=3.8.3
  - pandas
  - numpy
  - flake8
  - black
  - pip
  - pip:
    - pyarrow
    - pytest
    - rope
    - dask[dataframe,distributed]
    - azure-storage-blob
    - opencensus-ext-azure
    - azureml-core
    - azureml-pipeline-steps
    - azureml-pipeline-core
    - azureml-pipeline
    - azureml-mlflow
    - scikit-learn
    - lightgbm
    - xgboost
</code></pre>
<p>But the moment, I add <a href=""https://pypi.org/project/adlfs/0.0.9/"" rel=""nofollow noreferrer"">adlfs</a> under pip installable, it get stuck at &quot;Installing pip dependencies:&quot; for hours and finally fails. This happens for <code>opencensus-ext-azure</code> as well.</p>
<p>Any suggestions?</p>",1,1,2021-11-03 07:54:16.040000 UTC,1.0,,1,python|dask|azure-machine-learning-service|opencensus|azureml-python-sdk,379,2010-07-30 15:52:19.753000 UTC,2022-09-23 12:22:17.867000 UTC,"Bangalore, India",4265,315,11,403,,,,,,['azure-machine-learning-service']
How can we convert Notebooks under Azure Machine Learning into Azure Web Service?,"<p>I have written a Jupyter notebook in Azure Machine Learning which executes without issue. Now, I have to publish the code as a web service. &quot;<a href=""https://medium.com/microsoftazure/from-jupyter-notebook-to-azure-web-app-in-5-easy-steps-2783f8fd847d"" rel=""nofollow noreferrer"">From Jupyter Notebook to Azure Web App in 5 Easy Steps</a>&quot; describes converting a notebook into webservice using Docker and Azure Container Registry</p>
<p>Is there any alternative simple method within azure machine learning to convert the notebook into a web service? This can be done in the designer section:</p>
<p><img src=""https://i.stack.imgur.com/lMt8S.jpg"" alt=""see screenshot"" />.</p>",1,1,2021-01-04 15:13:24.643000 UTC,,2021-01-17 05:13:14.107000 UTC,0,azure-web-app-service|azure-machine-learning-service,301,2020-12-11 07:29:57.970000 UTC,2021-12-09 11:49:49.713000 UTC,,1,0,0,2,,,,,,['azure-machine-learning-service']
How do I use the GPU in a AWS Sagemaker ml.t3.2xlarge instance?,"<p>I have a notebook ml.t3.2xlarge instance on AWS Sagemaker and I want to train a neural network using pytorch.</p>
<p>The following command returns &quot;False&quot;:</p>
<pre><code>torch.cuda.is_available()
</code></pre>
<p>Is there something I need to do to activate the GPU?</p>
<p>When I run the following command:</p>
<pre><code>! nvidia-smi 
</code></pre>
<p>then it says &quot;NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.&quot;.</p>
<p>I was assuming all the notebook instance types had GPUs, since they're used for training neural networks.</p>
<p>Is it possible to use the GPU in this type of notebook instance and how would I do that?</p>",1,0,2021-11-24 16:29:15.263000 UTC,,,1,python|amazon-web-services|pytorch|amazon-sagemaker,714,2019-09-11 09:47:50.110000 UTC,2022-06-15 17:10:01.733000 UTC,,11,0,0,1,,,,,,['amazon-sagemaker']
Issue with numpy dependency when building docker image,"<p>I am trying to create a Sagemaker endpoint for model inference using the Build your own algorithm container (<a href=""https://sagemaker-examples.readthedocs.io/en/latest/advanced_functionality/scikit_bring_your_own/scikit_bring_your_own.html"" rel=""nofollow noreferrer"">https://sagemaker-examples.readthedocs.io/en/latest/advanced_functionality/scikit_bring_your_own/scikit_bring_your_own.html</a>) but am having an issue when installing Numpy in the creation of the image.</p>
<p>We've already previously have gotten it to work with our old model, but the new vowpal wabbit model requires numpy, scikit-learn, pandas and vowpal wabbit library which is causing it to fail in the docker build. I'm not sure if we should continue using this container or should migrate to a python one or sagemaker one, but would need to support nginx.</p>
<p>#EDIT: Forgot to mention that when I build it locally, it is created successfully but when fails through Cloudformation.</p>
<p>Dockerfile here:</p>
<pre><code># This is a Python 3 image that uses the nginx, gunicorn, flask stack
# for serving inferences in a stable way.
FROM ubuntu:18.04

# Retrieves information about what packages can be installed
RUN apt-get -y update &amp;&amp; \
    apt-get install -y --no-install-recommends \
        wget \
        python3-pip \
        python3.8 \
        python3-setuptools \
        nginx \
        ca-certificates &amp;&amp; \
    rm -rf /var/lib/apt/lists/*

# Set python 3.8 as default
RUN update-alternatives --install /usr/bin/python python /usr/bin/python3.8 1
RUN update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.8 1

# Get all python packages without excess cache created by pip.
COPY requirements.txt .
RUN pip3 install --upgrade pip setuptools wheel
RUN pip3 --no-cache-dir install -r requirements.txt

# Set some environment variables. PYTHONUNBUFFERED keeps Python from buffering our standard
# model_output stream, which means that logs can be delivered to the user quickly. PYTHONDONTWRITEBYTECODE
# keeps Python from writing the .pyc files which are unnecessary in this case. We also update
# PATH so that the train and serve programs are found when the container is invoked.
ENV PYTHONUNBUFFERED=TRUE
ENV PYTHONDONTWRITEBYTECODE=TRUE
ENV PATH=&quot;/opt/program:${PATH}&quot;
ENV PYTHONPATH /model_contents

# Set up the program in the image
COPY bandit/ /opt/program/
WORKDIR /opt/program/

# create directories for storing model and vectorizer
RUN mkdir model &amp;&amp; mkdir vectorizer

# Give permissions to run scripts
RUN chmod +x /opt/program/serve &amp;&amp; chmod +x /opt/program/train
</code></pre>
<p>requirements.txt here:</p>
<pre><code>sagemaker==2.25.1
typing-extensions==3.7.4.3
numpy==1.20.1
boto3==1.17.12
awscli==1.19.12
python-dotenv==0.15.0
flask==1.1.2
scikit-learn==1.0.0
pandas==1.3.5
vowpalwabbit==8.11.0
</code></pre>
<p>Full traceback here:</p>
<pre><code>Running setup.py install for numpy: started

    Running setup.py install for numpy: finished with status 'error'

    Complete output from command /usr/bin/python3 -u -c &quot;import setuptools, tokenize;__file__='/tmp/pip-build-cd653krx/numpy/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))&quot; install --record /tmp/pip-q3eo46tw-record/install-record.txt --single-version-externally-managed --compile:

    Running from numpy source directory.

    Note: if you need reliable uninstall behavior, then install

    with pip instead of using `setup.py install`:

      - `pip install .`       (from a git repo or downloaded source

                               release)

      - `pip install numpy`   (last NumPy release on PyPi)

    Cythonizing sources

    Processing numpy/random/_bounded_integers.pxd.in

    Processing numpy/random/_bounded_integers.pyx.in

    Traceback (most recent call last):

      File &quot;/tmp/pip-build-cd653krx/numpy/tools/cythonize.py&quot;, line 53, in process_pyx

        import Cython

    ModuleNotFoundError: No module named 'Cython'

    The above exception was the direct cause of the following exception:

    Traceback (most recent call last):

      File &quot;/tmp/pip-build-cd653krx/numpy/tools/cythonize.py&quot;, line 234, in &lt;module&gt;

        main()

      File &quot;/tmp/pip-build-cd653krx/numpy/tools/cythonize.py&quot;, line 230, in main

        find_process_files(root_dir)

      File &quot;/tmp/pip-build-cd653krx/numpy/tools/cythonize.py&quot;, line 221, in find_process_files

        process(root_dir, fromfile, tofile, function, hash_db)

      File &quot;/tmp/pip-build-cd653krx/numpy/tools/cythonize.py&quot;, line 187, in process

        processor_function(fromfile, tofile)

      File &quot;/tmp/pip-build-cd653krx/numpy/tools/cythonize.py&quot;, line 90, in process_tempita_pyx

        process_pyx(pyxfile, tofile)

      File &quot;/tmp/pip-build-cd653krx/numpy/tools/cythonize.py&quot;, line 60, in process_pyx

        raise OSError(msg) from e

    OSError: Cython needs to be installed in Python as a module

    Traceback (most recent call last):

      File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;

      File &quot;/tmp/pip-build-cd653krx/numpy/setup.py&quot;, line 450, in &lt;module&gt;

        setup_package()

      File &quot;/tmp/pip-build-cd653krx/numpy/setup.py&quot;, line 432, in setup_package

        generate_cython()

      File &quot;/tmp/pip-build-cd653krx/numpy/setup.py&quot;, line 237, in generate_cython

        raise RuntimeError(&quot;Running cythonize failed!&quot;)

    RuntimeError: Running cythonize failed!
</code></pre>",2,3,2022-01-05 11:27:40.977000 UTC,,2022-01-05 12:06:30.527000 UTC,0,python|docker|numpy|amazon-sagemaker|vowpalwabbit,383,2018-05-14 09:05:19.837000 UTC,2022-07-01 15:18:50.603000 UTC,"Madrid, Spain",33,1,0,11,,,,,,['amazon-sagemaker']
AWS SageMaker Training Job not saving model output,"<p>I'm running a training job on SageMaker. The job doesn't fully complete and hits the MaxRuntimeInSeconds stopping condition. When the job is stopping, documentation says the artifact will still be saved. I've attached the status progression of my training job below. It looks like the training job finished correctly. However the output S3 folder is empty. Any ideas on what is going wrong here? The training data is located in the same bucket so it should have everything it needs.</p>

<p><a href=""https://i.stack.imgur.com/5KtEW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5KtEW.png"" alt=""status progression""></a></p>",2,0,2020-02-10 16:06:31.953000 UTC,,,0,amazon-sagemaker,973,2013-09-03 23:51:41.493000 UTC,2022-09-21 22:16:17.837000 UTC,,1652,135,11,84,,,,,,['amazon-sagemaker']
Sagemaker: Specifying custom entry point gives not found error,"<p>I am trying to deploy my object detection model that was trained using tensorflow to sagemaker. I was able to deploy it without specifying any entry points during model creation but it turns out doing that will only work for small sizes images (Sagemaker has limit of 5MB). The code I used for this is as:</p>

<pre class=""lang-py prettyprint-override""><code>from sagemaker.tensorflow.serving import Model

# Initialize model ...
model = Model(
    model_data= s3_path_for_model,
    role=sagemaker_role,
    framework_version=""1.14"",
    env=env)

# Deploy model ...
predictor = model.deploy(initial_instance_count=1,
                         instance_type='ml.t2.medium')


# Test using an image ...
import cv2
import numpy as np

image_content = cv2.imread(""PATH_TO_IMAGE"",
                           1).astype('uint8').tolist()
body = {""instances"": [{""inputs"": image_content}]}

# Works fine for small images ...
# I get predictions perfectly with this ...
results = predictor.predict(body)

</code></pre>

<p>So, I googled around and found that I need to pass an <code>entry_point</code> for <code>Model()</code> in order to predict for larger images. Something like:</p>

<pre class=""lang-py prettyprint-override""><code>model = Model(
        entry_point=""inference.py"",
        dependencies=[""requirements.txt""],
        model_data=  s3_path_for_model,
        role=sagemaker_role,
        framework_version=""1.14"",
        env=env
)
</code></pre>

<p>But doing this gives <strong>FileNotFoundError: [Errno 2] No such file or directory: 'inference.py'</strong>. A little help here please. I am using <code>sagemaker-python-sdk</code>.
My folder structure is as:</p>

<pre><code>model
    |__ 001
          |__saved_model.pb
          |__variables
                |__&lt;contents here&gt;

    |__ code
          |__inference.py
          |__requirements.txt

</code></pre>

<blockquote>
  <p>Note: I have also tried, ./code/inference.py and /code/inference.py.</p>
</blockquote>",1,2,2019-12-17 11:18:15.533000 UTC,,,1,python|amazon-web-services|tensorflow|tensorflow-serving|amazon-sagemaker,786,2016-01-06 15:34:21.897000 UTC,2022-09-25 04:15:13.330000 UTC,"Kathmandu, Nepal",1991,925,54,869,,,,,,['amazon-sagemaker']
Use ffprobe on Sagemaker Jupyter Notebook: /bin/sh: 1: ffprobe: not found,"<p>I need to use <code>FFprobe</code> to get the metadata(the orientation of the video) of videos on SageMaker Studio. Even I installed <code>FFmpeg</code> and <code>FFprobe</code> via <code>pip install</code>, the notebook doesn't recognize the packages.</p>
<p><code>/bin/sh: 1: ffprobe: not found</code></p>
<p>It is tricky since I've already tried to install them on System terminal and it worked, but it still doesn't work on the Image terminal or Jupyter Notebook.</p>
<p>Is that because of the dependencies? I've checked other ways to get the metadata but couldn't find one working. I could get simple metadata via <code>hachoir-metadata</code> but it didn't give the orientation. Any advice is welcomed. Thanks.</p>",0,0,2020-07-02 21:15:16.073000 UTC,,2020-07-02 21:23:47.370000 UTC,1,amazon-web-services|ffmpeg|jupyter-notebook|amazon-sagemaker|ffprobe,190,2014-10-15 11:59:57.410000 UTC,2022-09-22 13:03:11.200000 UTC,"New York, NY, USA",39,49,0,11,,,,,,['amazon-sagemaker']
Inaccurate column separation on string data containing commas in CSV file in Azure Machine Learning Studio,"<p>I'm using first 100 of lines from <a href=""https://www.kaggle.com/wcukierski/enron-email-dataset/home"" rel=""nofollow noreferrer"">The Enron Email Dataset</a> for my experiment in Azure ML Studio, however the Saved Dataset object is being populated with odd 4.8K lines instead of 100. That must be due to <a href=""https://docs.microsoft.com/en-us/azure/machine-learning/studio-module-reference/convert-to-csv#inaccurate-column-separation-on-string-data-containing-commas"" rel=""nofollow noreferrer"">""Inaccurate column separation on string data containing commas""</a> issue, which I understand. </p>

<p>However, using same dataset in the Python project locally and/or in Azure ML Jupyter notebook (same imported dataset from ML Studio - not separately imported to Jupyter notebook) the number of lines is being read correctly and the further logic does also work fine. </p>

<p>Jupyter example:</p>

<pre><code>from azureml import Workspace
ws = Workspace()
ds = ws.datasets['The Enron Email Dataset (Minimal)']
emails_df = ds.to_dataframe()
</code></pre>

<p>Local example: </p>

<pre><code>import pandas as pd
emails_df = pd.read_csv('C:/enron-email-dataset/emails.csv', nrows=100)
</code></pre>

<p>And here is how dataset visualisation looks like in Azure ML Studio 
<a href=""https://i.stack.imgur.com/zBBZY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zBBZY.png"" alt=""enter image description here""></a></p>

<p>It's clear that it get's messed up after it gets moved from saved datasets to an experiment, but my question is - what would be the best way to work around it? Calling dataset from Azure BLOB Storage inside my Python code perhaps? </p>

<p><strong>EDIT 1:</strong> Removing commas from CSV file didn't helped either. The dataset is working fine in Jupyter and locally, but it appears messed up when added to the experiment. </p>

<p><strong>EDIT 2:</strong> Removing carriage returns resulted in dataset being visualised correctly in the experiment, however that broke the further text pre-processing logic in Python code (<a href=""https://docs.python.org/3/library/email.parser.html"" rel=""nofollow noreferrer"">email.parser</a>). Which I could amend making it to work across different environments. That's probably as good as it can get.
<a href=""https://i.stack.imgur.com/mDMq8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mDMq8.png"" alt=""enter image description here""></a></p>",0,0,2018-12-04 13:43:36.747000 UTC,,2018-12-06 09:51:24.493000 UTC,3,python-3.x|azure-machine-learning-studio|kaggle|email-parsing,307,2011-12-13 00:34:37.697000 UTC,2022-09-23 13:19:02.707000 UTC,"Nottingham, United Kingdom",277,110,0,58,,,,,,['azure-machine-learning-studio']
How to serve daily precomputed predictions in aws sagemaker?,"<p>I'm trying to use Sagemaker to serve precomputed predictions. The predictions are in the following format in a python dictionary.</p>

<pre><code>customer_group prediction
1              50
2              60
3              25
4              30
...
</code></pre>

<p>Currently the docker <a href=""https://github.com/awslabs/amazon-sagemaker-examples/blob/master/advanced_functionality/scikit_bring_your_own/container/decision_trees/predictor.py"" rel=""nofollow noreferrer"">serve API code</a> goes to s3 and downloads the data daily.</p>

<p>The problem is that downloading the data blocks the api from responding to the Sagemaker <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-inference-code.html#your-algorithms-inference-algo-ping-requests"" rel=""nofollow noreferrer"">health endpoint calls</a>.</p>

<p>This a case study of <a href=""https://aws.amazon.com/solutions/case-studies/zappos-case-study/"" rel=""nofollow noreferrer"">how zappos did it</a> using Amazon DynamoDB. However, is there a way to do it in Sagemaker? </p>

<p>Where and how can I add the s3 download function to avoid interrupting the health check?</p>

<p>Could this work? -> <a href=""https://github.com/seomoz/s3po"" rel=""nofollow noreferrer"">https://github.com/seomoz/s3po</a>
<a href=""https://blog.miguelgrinberg.com/post/the-flask-mega-tutorial-part-x-email-support"" rel=""nofollow noreferrer"">https://blog.miguelgrinberg.com/post/the-flask-mega-tutorial-part-x-email-support</a></p>

<pre><code>app = flask.Flask(__name__)

@app.route('/ping', methods=['GET'])
def ping():
    """"""Determine if the container is working and healthy. In this sample container, we declare
    it healthy if we can load the model successfully.""""""
    health = ScoringService.get_model() is not None  # You can insert a health check here

    status = 200 if health else 404
    return flask.Response(response='\n', status=status, mimetype='application/json')

@app.route('/invocations', methods=['POST'])
def transformation():
    """"""Do an inference on a single batch of data. In this sample server, we take data as CSV, convert
    it to a pandas data frame for internal use and then convert the predictions back to CSV (which really
    just means one prediction per line, since there's a single column.
    """"""
    data = None

    # Convert from CSV to pandas
    if flask.request.content_type == 'text/csv':
        data = flask.request.data.decode('utf-8')
        s = StringIO.StringIO(data)
        data = pd.read_csv(s, header=None)
    else:
        return flask.Response(response='This predictor only supports CSV data', status=415, mimetype='text/plain')

    print('Invoked with {} records'.format(data.shape[0]))

    # Do the prediction
    predictions = ScoringService.predict(data)

    # Convert from numpy back to CSV
    out = StringIO.StringIO()
    pd.DataFrame({'results':predictions}).to_csv(out, header=False, index=False)
    result = out.getvalue()

    return flask.Response(response=result, status=200, mimetype='text/csv')
</code></pre>",1,0,2020-05-02 21:53:43.127000 UTC,1.0,2020-05-03 11:33:04.957000 UTC,4,python|amazon-web-services|amazon-s3|amazon-sagemaker,361,2016-07-14 00:43:23.417000 UTC,2022-09-14 08:41:23.240000 UTC,,163,152,0,44,,,,,,['amazon-sagemaker']
Print message from inside AWS SageMaker endpoint,"<p>I've set up an endpoint for a model in AWS SageMaker. I set up my own container and am trying to debug some path related errors. To do this I need to print some information so I know what's going on.</p>
<p>Is there a way to print a message either to the terminal or to AWS CloudWatch from inside the endpoint.</p>
<p>Update:</p>
<p>I'm now trying to get logger to work, I'm doing the following:</p>
<pre><code>import logging
logger = logging.getLogger()
logger.setLevel(logging.DEBUG)
logger.debug(&quot;message&quot;)
</code></pre>
<p>I optimally want this to show up in CloudWatch but currently it's not.</p>",0,5,2022-07-20 21:50:42.883000 UTC,,2022-07-21 14:42:04.420000 UTC,0,python|logging|amazon-cloudwatch|amazon-sagemaker|endpoint,104,2022-07-05 19:52:34.840000 UTC,2022-09-21 21:49:37.727000 UTC,,15,5,0,7,,,,,,['amazon-sagemaker']
Databricks notebook integrated mlflow artifact location and retention,"<ol>
<li><p>Currently by default in notebook run, it will create an experiment ID, but the Artifact Location would point to something under dbfs:/databricks/mlflow/{experiment id}. If there is a way we may change this in default experiment creation? We like to manage the storage outside databricks.</p></li>
<li><p>How long is default TTL for experiment runs and metrics? Is it configurable and how?</p></li>
</ol>",1,1,2019-05-01 21:44:58.277000 UTC,,,1,azure-databricks|mlflow,587,2016-03-18 04:43:36.763000 UTC,2019-12-07 00:33:14.047000 UTC,,21,0,0,3,,,,,,['mlflow']
What sagemaker algorithm can be trained by the minimal instance?,"<p>I am following <a href=""https://github.com/mtm12/SageMakerDemo"" rel=""nofollow noreferrer"">this example</a> on how to train a machine learning model in Amazon-sagemaker. The problem is ml.t2.medium instance fail to satisfy the constraint of K-means algorithm. I can't use the instances that satisfy the constraints of the algorithm(I will ask support for allocation of this resource). My question is there any algorithm from sagemaker that one can train on the MNIST dataset with the minimal instance?</p>",1,0,2020-07-19 09:46:55.870000 UTC,,,0,amazon-web-services|jupyter-notebook|instance|amazon-sagemaker,150,2011-07-16 13:02:36.880000 UTC,2022-09-24 20:19:39.590000 UTC,Slovenia,14913,307,1,1093,,,,,,['amazon-sagemaker']
Hosting spacy models using sagemaker with endpoint,"<p>I'm new to sagemaker, My requirement is i have spacy model(where there is no training required),Now i want to deploy the model using sagemaker with endpoint, so that i can use that endpoint in my project. I'm following this <a href=""https://medium.com/analytics-vidhya/how-to-run-spacy-on-aws-sagemaker-custom-containers-quick-and-easy-884a6898ed48"" rel=""nofollow noreferrer"">link</a>, but i didn't figure out on how to use this model as an endpoint. Can someone help me in getting this done.</p>",0,0,2021-01-25 18:07:01.320000 UTC,,,1,amazon-sagemaker|amazon-ecr,221,2018-05-16 11:06:48.480000 UTC,2022-09-09 14:17:43.800000 UTC,,103,4,0,40,,,,,,['amazon-sagemaker']
Installing private python wheel from a storage account,"<p>I use a shared storage account across all our AML workspaces. I register this storage account on each workspace and use it to share data and libraries when targeting AML compute. To install a private python wheel, we currently have to download the package locally and use the Environment.add_private_pip_wheel to upload it and retrieve the remote url of the package. I would like to know if there’s a way to pass a reference to the package in the shared storage account instead of having to move it to the default workspace storage account.</p>

<p>I tried to pass the full blob url (e.g. <a href=""https://mystorageaccount.blob.core.windows.net/mypackage.whl"" rel=""nofollow noreferrer"">https://mystorageaccount.blob.core.windows.net/mypackage.whl</a>)  in the pip_packages dependency parameter to the TensorFlow estimator class but the experiment execution fails with a 404 error : The specified resource does not exist for url <a href=""https://mystorageaccount.blob.core.windows.net/mypackage.whl"" rel=""nofollow noreferrer"">https://mystorageaccount.blob.core.windows.net/mypackage.whl</a></p>

<pre class=""lang-py prettyprint-override""><code>from azureml.train.dnn import TensorFlow
from azureml.core.runconfig import MpiConfiguration

project_folder = ""./myproject""

dependencies = [""https://mystorageaccount.blob.core.windows.net/mypackage.whl""]

script_params={
  ""--data_dir"":data_dir.as_mount(),
  ""--output_dir"":output_data.as_mount(),
  ""--do_train"":""True"",
  ""--do_eval"":""False"",
  ""--do_predict"":""False"",
}

mpiConfig = MpiConfiguration()
mpiConfig.process_count_per_node = 1
estimator= TensorFlow(source_directory=project_folder,
                      compute_target=compute_target,
                      script_params=script_params,
                      entry_script='./train_classifier.py',
                      node_count=1,
                      distributed_training=mpiConfig,
                      pip_packages=dependencies,
                      framework_version='1.13',
                      use_gpu=True)
</code></pre>

<p>Since the storage account has been registered in the workspace, I expect to be able to install the private package directly (using the code above) from the shared storage account into AML compute without having to download it locally and then calling Environment.add_private_pip_wheel function.</p>

<p>Instead, I get the following error and the experiment fails:</p>

<p>(from -r /azureml-environment-setup/condaenv.kc80np3o.requirements.txt (line 1)) because of HTTP error 404 Client Error: The specified resource does not exist. for url: <a href=""https://mystorageaccount.blob.core.windows.net/mypackage.whl"" rel=""nofollow noreferrer"">https://mystorageaccount.blob.core.windows.net/mypackage.whl</a> ... </p>

<p>CondaValueError: pip returned an error</p>",1,0,2019-08-15 00:58:32.547000 UTC,1.0,2019-08-15 02:37:37.033000 UTC,2,python|azure-machine-learning-service,449,2019-08-15 00:34:17.487000 UTC,2020-03-11 17:45:04.523000 UTC,,21,0,0,0,,,,,,['azure-machine-learning-service']
"RuntimeError: Input tensor at index 3 has invalid shape [2, 2, 16, 128, 64] but expected [2, 4, 16, 128, 64]","<p>Runtime error while finetuning a pretrained <a href=""https://huggingface.co/gpt2-medium"" rel=""nofollow noreferrer"">GPT2-medium</a> model using <a href=""https://huggingface.co/transformers/"" rel=""nofollow noreferrer"">Huggingface</a> library in SageMaker - <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/notebooks-available-instance-types.html"" rel=""nofollow noreferrer"">ml.p3.8xlarge</a> instance.</p>
<p>The <code>finetuning_gpt2_script.py</code> contains the below,</p>
<p>Libraries:</p>
<pre><code>from transformers import Trainer, TrainingArguments
from transformers import EarlyStoppingCallback
from transformers import GPT2LMHeadModel, GPT2Tokenizer
from transformers import TextDataset,DataCollatorForLanguageModeling
</code></pre>
<p>Pretrained Models:</p>
<pre><code>gpt2_model = GPT2LMHeadModel.from_pretrained(&quot;gpt2-medium&quot;)
gpt2_tokenizer = GPT2Tokenizer.from_pretrained(&quot;gpt2-medium&quot;)
</code></pre>
<p>Train and Test Data Construction:</p>
<pre><code>train_dataset = TextDataset(
          tokenizer=gpt2_tokenizer,
          file_path=train_path,
          block_size=128)
    
test_dataset = TextDataset(
          tokenizer=gpt2_tokenizer,
          file_path=test_path,
          block_size=128)
    
data_collator = DataCollatorForLanguageModeling(
        tokenizer=gpt2_tokenizer, mlm=False,
    )
</code></pre>
<p><code>train_path</code> &amp; <code>test_path</code> are unstructured text data file of size 1.45 Million and 200K lines of data</p>
<p>Training arguments:</p>
<pre><code>training_args = TrainingArguments(
        output_dir=&quot;./gpt2-finetuned-models&quot;, #The output directory
        overwrite_output_dir=True, #overwrite the content of the output directory
        num_train_epochs=1, # number of training epochs
        per_device_train_batch_size=8, # batch size for training #32
        per_device_eval_batch_size=8,  # batch size for evaluation #64
        save_steps=100, # after # steps model is saved
        warmup_steps=500,# number of warmup steps for learning rate scheduler
        prediction_loss_only=True,
        metric_for_best_model = &quot;eval_loss&quot;,
        load_best_model_at_end = True,
        evaluation_strategy=&quot;epoch&quot;,
    )
</code></pre>
<p><code>training_args</code> are the training arguments constructed to train the model.</p>
<p>Trainer:</p>
<pre><code>trainer = Trainer(
        model=gpt2_model,
        args=training_args,
        data_collator=data_collator,
        train_dataset=train_dataset,
        eval_dataset=test_dataset,
        callbacks = [early_stop_callback],
    )
early_stop_callback = EarlyStoppingCallback(early_stopping_patience  = 3)
</code></pre>
<p>Training:</p>
<pre><code>trainer.train()
trainer.save_model(model_path)
</code></pre>
<p>Here, the training is done for only 1 epoch in 4 GPUS using <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/notebooks-available-instance-types.html"" rel=""nofollow noreferrer"">ml.p3.8xlarge</a> instance.</p>
<p>The training is done by torch-distribution like below,</p>
<pre><code>python -m torch.distributed.launch finetuning_gpt2_script.py
</code></pre>
<p>While training at the end of the epoch, observed the below error,</p>
<p><code>RuntimeError: Input tensor at index 3 has invalid shape [2, 2, 16, 128, 64] but expected [2, 4, 16, 128, 64]</code></p>
<ol>
<li>Is the <code>RuntimeError</code> because of the way the <code>train_dataset</code> and <code>test_dataset</code>constructed using <code>TextData</code> ?</li>
<li>Am I doing wrong in the <code>torch-distribution</code> ?</li>
</ol>",1,0,2021-01-21 06:11:18.000000 UTC,,,0,python|pytorch|amazon-sagemaker|huggingface-transformers|gpt-2,702,2012-11-02 09:36:14.987000 UTC,2022-09-23 13:20:41.533000 UTC,"Cyberjaya, Malaysia",870,359,1,176,,,,,,['amazon-sagemaker']
Not able to set up gcc in aws sagemaker notebook,"<p>I am working in a jupyter notebook in aws sagemaker and want to use prophet for time-series forecast. I am using the conda_python3 kernel. According to the installation instruction from <a href=""https://facebook.github.io/prophet/docs/installation.html#python"" rel=""nofollow noreferrer"">https://facebook.github.io/prophet/docs/installation.html#python</a> I need to set up gcc before installing prophet, but when I try '%conda install gcc', I get the error message that the environment is inconsistent.</p>
<p>Does anyone know how to solve this issue?</p>
<p><strong>Error message:</strong></p>
<p>Collecting package metadata (current_repodata.json): done
Solving environment: /
The environment is inconsistent, please check the package plan carefully
The following packages are causing the inconsistency:</p>
<ul>
<li>conda-forge/noarch::nbclient==0.5.2=pyhd8ed1ab_0</li>
<li>conda-forge/linux-64::matplotlib==3.3.4=py36h5fab9bb_0</li>
<li>conda-forge/noarch::qdarkstyle==2.8.1=pyhd8ed1ab_2</li>
<li>conda-forge/linux-64::scikit-image==0.16.2=py36hb3f55d8_0</li>
<li>conda-forge/noarch::python-language-server==0.36.2=pyhd8ed1ab_0</li>
<li>conda-forge/linux-64::widgetsnbextension==3.5.1=py36h5fab9bb_4</li>
<li>conda-forge/noarch::flake8==3.8.4=py_0</li>
<li>conda-forge/noarch::ipywidgets==7.6.3=pyhd3deb0d_0</li>
<li>conda-forge/noarch::typing-extensions==3.7.4.3=0</li>
<li>conda-forge/noarch::path.py==12.5.0=0</li>
<li>conda-forge/noarch::dask==2021.2.0=pyhd8ed1ab_0</li>
<li>conda-forge/noarch::nbformat==5.1.2=pyhd8ed1ab_1</li>
<li>conda-forge/linux-64::path==15.1.2=py36h5fab9bb_0</li>
<li>conda-forge/linux-64::nbconvert==6.0.7=py36h5fab9bb_3</li>
<li>conda-forge/linux-64::distributed==2021.2.0=py36h5fab9bb_0</li>
<li>conda-forge/noarch::anaconda-client==1.7.2=py_0</li>
<li>conda-forge/noarch::aioitertools==0.7.1=pyhd8ed1ab_0</li>
<li>conda-forge/linux-64::matplotlib-base==3.3.4=py36hd391965_0</li>
<li>conda-forge/linux-64::pluggy==0.13.1=py36h5fab9bb_4</li>
<li>conda-forge/noarch::black==20.8b1=py_1</li>
<li>conda-forge/linux-64::blaze==0.11.3=py36_0</li>
<li>conda-forge/noarch::pyls-spyder==0.3.2=pyhd8ed1ab_0</li>
<li>conda-forge/noarch::odo==0.5.1=py_1</li>
<li>conda-forge/linux-64::keyring==22.0.1=py36h5fab9bb_0</li>
<li>conda-forge/noarch::anaconda-project==0.9.1=pyhd8ed1ab_0</li>
<li>conda-forge/noarch::importlib_metadata==3.7.0=hd8ed1ab_0</li>
<li>conda-forge/linux-64::jupyter==1.0.0=py36h5fab9bb_6</li>
<li>conda-forge/noarch::jupyterlab_server==2.3.0=pyhd8ed1ab_0</li>
<li>conda-forge/noarch::seaborn-base==0.11.1=pyhd8ed1ab_1</li>
<li>conda-forge/noarch::imageio==2.9.0=py_0</li>
<li>conda-forge/noarch::numpydoc==1.1.0=py_1</li>
<li>conda-forge/linux-64::yarl==1.6.3=py36h8f6f2f9_1</li>
<li>conda-forge/noarch::jsonschema==3.2.0=py_2</li>
<li>conda-forge/noarch::flask==1.1.2=pyh9f0ad1d_0</li>
<li>conda-forge/noarch::seaborn==0.11.1=hd8ed1ab_1</li>
<li>conda-forge/noarch::helpdev==0.7.1=pyhd8ed1ab_0</li>
<li>conda-forge/linux-64::nb_conda==2.2.1=py36h5fab9bb_4</li>
<li>conda-forge/noarch::nbclassic==0.2.6=pyhd8ed1ab_0</li>
<li>conda-forge/noarch::sphinx==3.5.1=pyhd8ed1ab_0</li>
<li>conda-forge/noarch::jupyterlab_launcher==0.13.1=py_2</li>
<li>conda-forge/linux-64::spyder==4.2.0=py36h5fab9bb_0</li>
<li>conda-forge/linux-64::importlib-metadata==3.7.0=py36h5fab9bb_0</li>
<li>conda-forge/linux-64::pytest==6.2.2=py36h5fab9bb_0</li>
<li>conda-forge/noarch::pyls-black==0.4.6=pyh9f0ad1d_0
/ ^C
failed with initial frozen solve. Retrying with flexible solve.</li>
</ul>
<p>CondaError: KeyboardInterrupt</p>
<p>Note: you may need to restart the kernel to use updated packages.</p>",1,0,2022-06-11 19:54:45.090000 UTC,,2022-06-11 20:02:33.333000 UTC,0,conda|amazon-sagemaker|facebook-prophet,138,2022-06-11 19:38:40.180000 UTC,2022-08-08 19:11:29.863000 UTC,,1,0,0,0,,,,,,['amazon-sagemaker']
How to perform Real Time Object Detection with trained AWS model,"<p>After successfully training object detection model with AWS SageMaker, how do I use this model to perform real time object detection on RTSP video?</p>",2,0,2021-02-20 13:09:03.790000 UTC,,,2,amazon-web-services|object-detection|amazon-sagemaker,162,2020-08-10 08:14:11.990000 UTC,2022-09-21 21:49:57.863000 UTC,,139,11,0,46,,,,,,['amazon-sagemaker']
Sagemaker: Problem with elastic inference when deploying,"<p>When executing the deploy code to <strong>sagemaker</strong> using <strong>sagemaker-python-sdk</strong> I get error as :</p>

<pre><code>UnexpectedStatusException: Error hosting endpoint tensorflow-inference-eia-XXXX-XX-XX-XX-XX-XX-XXX: 
Failed. Reason: The image '763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-inference-eia:1.14 
-gpu' does not exist..
</code></pre>

<p>The code that I am using to deploy is as:</p>

<pre><code>predictor = model.deploy(initial_instance_count=1,
                         instance_type='ml.p2.xlarge', accelerator_type='ml.eia1.medium')
</code></pre>

<p>If I remove the <code>accelerator_type</code> parameter then the endpoint gets deployed with no errors. Any idea on why this happens? Sagemaker seems to be referring to the image that doesn't exist. How do I fix this?</p>

<p>Also, I made sure that the version is supported from here: <a href=""https://github.com/aws/sagemaker-python-sdk#tensorflow-sagemaker-estimators"" rel=""nofollow noreferrer"">https://github.com/aws/sagemaker-python-sdk#tensorflow-sagemaker-estimators</a>'. I am on TensorFlow: 1.14.</p>

<blockquote>
  <p><strong>Edit:</strong>
  Turns out, this works:</p>
</blockquote>

<pre><code>predictor = model.deploy(initial_instance_count=1,
                         instance_type='ml.m4.xlarge', accelerator_type='ml.eia1.medium')
</code></pre>

<p>So, I am guessing that elastic inference is not available for GPU instances? </p>

<blockquote>
  <p>Note: None of the instances that I deploy my endpoint to is using GPU. (Please suggest some ideas if you are familiar or have made it work.)</p>
</blockquote>",1,6,2019-12-23 10:03:47.373000 UTC,,2019-12-24 16:19:17.377000 UTC,0,python|amazon-web-services|tensorflow|deployment|amazon-sagemaker,259,2016-01-06 15:34:21.897000 UTC,2022-09-25 04:15:13.330000 UTC,"Kathmandu, Nepal",1991,925,54,869,,,,,,['amazon-sagemaker']
Can't make API calls to model endpoint deployed on AWS Sagemaker,"<p>We have trained a Nasnet model, and deployed the model as an endpoint on AWS Sagemaker successfully. When loading in the model locally, predictions can be made, but I'm not sure the format to pass in images when calling the API endpoint.</p>

<p>For reference, when loaded using tf.keras.load_model, the model's input is as follows:</p>

<pre><code>[&lt;tf.Tensor 'input_2:0' shape=(None, 331, 331, 3) dtype=float32&gt;]
</code></pre>

<p>In addition, here is the function used to build the model, containing the model's predict_signature_def</p>

<pre><code>def build(loaded_model, export_dir):
    build = builder.SavedModelBuilder(export_dir)
    print(""INPUT FORMAT:"")
    print(loaded_model.input)
    signature = predict_signature_def(inputs={""inputs"": loaded_model.input}, outputs={""score"": loaded_model.output})
    with K.get_session() as sess:
        # Save the meta graph and variables
        build.add_meta_graph_and_variables(
            sess=sess, tags=[tag_constants.SERVING], signature_def_map={""serving_default"": signature})
        build.save()
</code></pre>

<p>We've tried making API calls to the endpoint in the following format:</p>

<pre><code>
    client = boto3.client('runtime.sagemaker',
    region_name='us-east-1',
    aws_access_key_id='ACCESS_KEY',
    aws_secret_access_key='SECRET_KEY')


    with open(""kitchen.jpg"", ""rb"") as image:
        f = image.read()
        b = bytearray(f)


    response = client.invoke_endpoint(EndpointName='ENDPOINT_NAME_HERE',
    Body=b)
</code></pre>

<p>We tried multiple passing multiple formats for an image in the body (bytearray, base64, numpy array). However, we keep getting the same error from AWS:</p>

<pre><code>Received client error (415) from model with message ""{""error"": ""Unsupported Media Type: Unknown""}"".
</code></pre>

<p>Does anyone know what the proper image input format should be, or have any suggestions? Thanks</p>",0,4,2020-05-26 21:47:09.087000 UTC,,,2,python|image|amazon-web-services|tensorflow|amazon-sagemaker,487,2020-04-27 18:02:04.443000 UTC,2022-09-22 01:11:51.867000 UTC,,125,10,0,20,,,,,,['amazon-sagemaker']
how to use existing conda environment as a AzureML environment,<p>I have created a Azure Compute target and using the notebooks from it. I want to create a conda environment using Notebook Terminal and install my own packages and use it in AzureML experiment environment.</p>,2,1,2020-12-05 16:34:15.300000 UTC,,,1,python|azure|azure-machine-learning-service|azureml-python-sdk,969,2017-01-19 06:38:11.853000 UTC,2022-09-20 02:56:14.233000 UTC,"Gurugram, Haryana, India",623,29,1,54,,,,,,['azure-machine-learning-service']
How to use kfp Artifact with sklearn?,"<p>I'm trying to develop a custom pipeline with kubeflow pipeline (kfp) components inside Vertex AI (Google Cloud Platform). The steps of the pipeline are:</p>
<ol>
<li>read data from a big query table</li>
<li>create a pandas <code>DataFrame</code></li>
<li>use the <code>DataFrame</code> to train a K-Means model</li>
<li>deploy the model to an endpoint</li>
</ol>
<p>Here there is the code of the step 2. I had to use <code>Output[Artifact]</code> as output because <code>pd.DataFrame</code> type that I found <a href=""https://stackoverflow.com/questions/43890844/pythonic-type-hints-with-pandas"">here</a> did not work.</p>
<pre class=""lang-py prettyprint-override""><code>@component(base_image=&quot;python:3.9&quot;, packages_to_install=[&quot;google-cloud-bigquery&quot;,&quot;pandas&quot;,&quot;pyarrow&quot;])
def create_dataframe(
    project: str,
    region: str,
    destination_dataset: str,
    destination_table_name: str,
    df: Output[Artifact],
):
    
    from google.cloud import bigquery
    
    client = bigquery.Client(project=project, location=region)
    dataset_ref = bigquery.DatasetReference(project, destination_dataset)
    table_ref = dataset_ref.table(destination_table_name)
    table = client.get_table(table_ref)

    df = client.list_rows(table).to_dataframe()
</code></pre>
<p>Here the code of the step 3:</p>
<pre class=""lang-py prettyprint-override""><code>@component(base_image=&quot;python:3.9&quot;, packages_to_install=['sklearn'])
def kmeans_training(
        dataset: Input[Artifact],
        model: Output[Model],
        num_clusters: int,
):
    from sklearn.cluster import KMeans
    model = KMeans(num_clusters, random_state=220417)
    model.fit(dataset)
</code></pre>
<p>The run of the pipeline is stopped due to the following error:</p>
<pre class=""lang-py prettyprint-override""><code>TypeError: float() argument must be a string or a number, not 'Artifact'
</code></pre>
<p>Is it possible to convert Artifact to <code>numpy array</code> or <code>Dataframe</code>?</p>",1,0,2021-11-15 16:10:03.783000 UTC,1.0,,0,python|google-cloud-platform|google-cloud-ml|google-cloud-vertex-ai|kfp,339,2021-03-24 12:34:53.617000 UTC,2022-09-22 15:26:12.103000 UTC,"Alatri, Frosinone, FR",67,3,0,33,,,,,,['google-cloud-vertex-ai']
How to add more tables in mlflow?,"<p>I want to add more tables in mlflow.</p>

<p>As mlflow provide facility to do machine learning experiments but I just want to add those experiment in project level that User can create multiple projects and in that project multiple experiment can be performed.</p>

<p>So, Is their any way to achieve this feature in mlflow </p>",1,2,2020-02-04 09:43:18.747000 UTC,,,1,mlflow,208,2018-09-06 10:36:07.273000 UTC,2022-09-25 05:19:24.110000 UTC,"Surat, Gujarat, India",720,21,6,134,,,,,,['mlflow']
Cannot list pipeline steps using AzureML CLI,"<p>I'm trying to list steps in a pipeline using AzureML CLI extension, but get an error:</p>

<pre><code>&gt;az ml run list -g &lt;group&gt; -w &lt;workspace&gt; --pipeline-run-id 00886abe-3f4e-4412-aec3-584e8c991665
UserErrorException:
        Message: Cannot specify ['--last'] for pipeline runs
        InnerException None
        ErrorResponse
{
    ""error"": {
        ""code"": ""UserError"",
        ""message"": ""Cannot specify ['--last'] for pipeline runs""
    }
}
</code></pre>

<p>From help it looks like <code>--last</code> option takes the default value 10 despite the fact that it is not supported for the <code>--pipeline-run-id</code>. How the latter is supposed to work?</p>",0,3,2020-01-25 16:13:33.963000 UTC,,,0,azure-machine-learning-service,109,2018-02-15 14:47:43.680000 UTC,2022-08-24 16:53:36.967000 UTC,,95,2,0,15,,,,,,['azure-machine-learning-service']
How to change conda environments in custom image?,"<p>I am trying to use a preconfigured conda environment as my kernel in SageMaker Studio. I've gotten this working in the Terminal with a custom image. However, notebooks do not use the correct conda environment by default. Attempting to change environments results in conda errors.</p>
<pre><code>!conda env list
# conda environments:
#
base                  *  /home/ubuntu/miniconda
pipeline                 /home/ubuntu/miniconda/envs/pipeline
</code></pre>
<pre><code>!conda activate pipeline

CommandNotFoundError: Your shell has not been properly configured to use 'conda activate'.
To initialize your shell, run

    $ conda init &lt;SHELL_NAME&gt;

Currently supported shells are:
  - bash
  - fish
  - tcsh
  - xonsh
  - zsh
  - powershell

See 'conda init --help' for more information and options.

IMPORTANT: You may need to close and restart your shell after running 'conda init'.
</code></pre>
<p>The container is set up to use the correct conda environment when using bash (through .bashrc, conda activate) so Terminal works properly.</p>
<p>What's the right approach to using a desired conda environment by default in a notebook?</p>",1,0,2022-03-25 21:18:14.643000 UTC,,,0,amazon-sagemaker,329,2013-04-13 18:37:19.530000 UTC,2022-09-23 19:28:09.003000 UTC,"Boston, MA, USA",383,13,0,19,,,,,,['amazon-sagemaker']
"mlflow not work after installation (Ubuntu 16, Centos 7)","<p><img src=""https://i.stack.imgur.com/5DC76.png"" alt=""enter link description here""></p>

<p>I try to install and run the web-based interface mlflow on VM Azure Ubuntu 16 and Centos 7.
After running the command:
sudo mlflow ui</p>

<p>I can not get access url, either through the dns (mydomain.com:5000), or by IP: <a href=""http://123.456.789.123:5000/"" rel=""nofollow noreferrer"">http://123.456.789.123:5000/</a></p>

<p>Executing on the server:</p>

<p>wget <a href=""http://localhost:5000"" rel=""nofollow noreferrer"">http://localhost:5000</a></p>

<p>I get the html-page mlflow, ie the server is running, but then why can not I connect to it in a browser? - Error:The connection has timed out</p>

<p>p.s. Firewall disabled on this VM.</p>",1,0,2019-10-02 09:38:26.153000 UTC,,2019-10-04 07:25:08.487000 UTC,0,ubuntu|centos|gunicorn|mlflow,218,2019-04-09 14:32:27.590000 UTC,2022-05-11 09:54:16.667000 UTC,,61,0,0,9,,,,,,['mlflow']
Azure ML static Endpoint with dynamic model,"<p>Is it possible to deploy an endpoint with the latest model on Azure ML? So, I have a CICD Pipeline on Azure DevOps that will generate and evaluate a new model with the latest model and it will Register a model with a better one. But, the problem is when I try to deploy the endpoint, it only can attach a specific model, not the best model. So, if I want to deploy a new one, it will generate a new endpoint link.</p>
<p>So my question is, is it possible to deploy an endpoint with the newest model without changing its URL REST endpoint?</p>",1,0,2022-01-18 05:46:52.980000 UTC,,2022-01-18 06:07:06.820000 UTC,1,azure|azure-devops|azure-machine-learning-studio|azure-machine-learning-service|mlops,117,2018-11-16 03:30:26.920000 UTC,2022-09-23 03:35:59.120000 UTC,,683,28,5,166,,,,,,"['azure-machine-learning-service', 'azure-machine-learning-studio']"
Public windows docker image for azure machine learning,"<p>Our machine learning workflow requires use of a custom windows .pyc file. Where can I find a windows docker image file.</p>

<p>I am puzzled by this statement from <a href=""https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-deploy-custom-docker-image#create-a-custom-base-image"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-deploy-custom-docker-image#create-a-custom-base-image</a>. Is it really true that azure cannot use windows images? </p>

<pre><code>Image requirements: Azure Machine Learning only supports Docker images that provide the following software:

Ubuntu 16.04 or greater.
Conda 4.5.# or greater.
Python 3.5.# or 3.6.#.

</code></pre>

<p>Searching on docker hub also did not turn up anything promising</p>",1,0,2019-11-08 18:50:57.720000 UTC,,,0,docker|azure-machine-learning-service,69,2019-08-14 14:48:38.450000 UTC,2022-09-25 02:16:40.673000 UTC,,113,3,0,6,,,,,,['azure-machine-learning-service']
Object Detection in AWS + Sagemaker Neo,"<p>I am trying the inbuilt object detection algorithm available on AWS for a computer vision problem. The training job ran successfully and I have received the model artifacts in the .tar.gz format in an S3 bucket.</p>
<p>To reduce the model footprint, we need to use Sagemaker Neo - a compilation job on the available model artifacts. The compilation job fails with the error - &quot;ClientError: OperatorNotImplemented:('One or more operators are not supported in frontend MXNet:\n_contrib_MultiBoxTarget: 1\nMakeLoss: 3&quot;</p>
<p>How can this be resolved?
Around August 2019, Sagemaker Neo did not support models trained with built-in Sagemaker Object Detection Algorithm. Is there any change in this status today ?</p>
<p>Thanks</p>",0,0,2020-12-08 08:41:40.990000 UTC,,,1,amazon-web-services|amazon-sagemaker,165,2019-12-23 09:20:51.693000 UTC,2021-01-28 06:15:54.547000 UTC,,43,0,0,13,,,,,,['amazon-sagemaker']
Connecting Google's Tensorboard to Vertex AI AutoML,<p>Is there a way to connect Google's Vertex AI <strong>Tensorboard</strong> to <strong>AutoML</strong>? - I remember a brief mention in a coursera specialization but don't know where it was.</p>,1,0,2021-09-28 17:47:56.323000 UTC,,,-2,tensorboard|google-cloud-vertex-ai,128,2013-04-11 06:14:37.440000 UTC,2022-04-26 23:58:37.030000 UTC,,1325,120,2,207,,,,,,['google-cloud-vertex-ai']
"Error while deploying model using Sagemaker endpoint or transformer, which was trained using script mode","<p>I have trained a model using sagemaker SDK using script mode. When I am deploying it I am getting this error.
<br></p>
<p><a href=""https://i.stack.imgur.com/FdD24.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FdD24.png"" alt=""enter image description here"" /></a>
<br></p>
<p><br>Also, I have tried with the transformer, getting the same error.</p>
<p>It's working fine when I am training using the container.</p>
<p>Need help on this...</p>",0,0,2021-01-22 11:03:04.863000 UTC,1.0,,3,python|amazon-web-services|amazon-sagemaker,90,2016-02-22 12:24:53.157000 UTC,2022-09-25 05:52:09.220000 UTC,"Bangalore, Karnataka, India",506,119,0,55,,,,,,['amazon-sagemaker']
Amazon SageMaker: TrainingJobAnalytics returns only one timestamp for inbuilt xgboost,"<p>I am trying to use <code>TrainingJobAnalytics</code> to plot the training and validation loss curves for a training job using XGBoost on SageMaker. The training job completes successfully and I can see the training and validation rmse values in the CloudWatch logs. </p>

<p>However when I try to get them in my notebook using <code>TrainingJobAnalytics</code>, I only get the metrics for a single timestamp and not all of them.</p>

<p>My code is as below:</p>

<pre><code>metrics_dataframe = TrainingJobAnalytics(training_job_name=job_name).dataframe()
</code></pre>

<p>What's going wrong and how can I fix it?</p>",1,3,2019-12-05 20:34:59.463000 UTC,,,0,python|amazon-cloudwatch|xgboost|amazon-sagemaker,346,2013-05-17 17:42:28.267000 UTC,2022-09-21 14:33:23.777000 UTC,"New York, NY, USA",1499,599,22,210,,,,,,['amazon-sagemaker']
Failed to write to mlflow after deleting experiment via mlflow UI,"<p>I'm using mlflow version 1.18.0</p>
<p>When I delete experiment from <code>mlflow</code> <em>UI</em>, and than try to create and write a new experiment (with same name which I just deleted) I'm getting error on this line code:</p>
<pre><code>mlflow.start_run(run_name=run_name)
</code></pre>
<p>Error:</p>
<pre><code>error mlflow.util.rest_util API resest to faild with code 500 != 200
</code></pre>
<p>If I change the experiment name, I have no problem to write new tests.</p>
<ol>
<li>Why is this happening ? (as I wrote, I delete the experiment name)</li>
<li>Is there a way to solve it (without giving new experiment name) ?</li>
</ol>",0,0,2022-05-03 11:22:45.597000 UTC,,,0,python-3.x|mlflow,45,2014-05-23 08:26:35.280000 UTC,2022-09-19 12:37:21.063000 UTC,,3934,1052,6,416,,,,,,['mlflow']
Copy a SageMaker Notebook to SageMaker Studio,"<p>My colleague made a notebook in SageMaker but I want to copy that notebook into SageMaker Studio so that future collaborations and changes are smoother.
From Studio, I can't see anything that relates it to SageMaker classic.</p>
<p>Any advice?</p>",1,0,2021-10-21 16:01:20.270000 UTC,,,0,amazon-web-services|amazon-sagemaker,265,2021-07-13 19:03:16.310000 UTC,2022-09-23 15:03:18.587000 UTC,,1,0,0,1,,,,,,['amazon-sagemaker']
MLFLOW Artifacts stored on ftp server but not showing in ui,"<p>I use MLFLOW to store some parameters and metrics during training on a remote tracking server. Now I tried to also add a .png file as an artifact, but since the MLFLOW server is running remotely I store the file on a ftp server. I gave the ftp server address and path to MLFLOW by:</p>
<pre><code>mlflow server --backend-store-uri sqlite:///mlflow.sqlite --default-artifact-root ftp://user:password@1.2.3.4/artifacts/ --host 0.0.0.0 &amp;
</code></pre>
<p>Now I train a network and store the artifact by running:</p>
<pre><code>mlflow.set_tracking_uri(remote_server_uri)
mlflow.set_experiment(&quot;default&quot;)
mlflow.pytorch.autolog()

with mlflow.start_run():
    mlflow.log_params(flow_params)
    trainer.fit(model)
    trainer.test()
    mlflow.log_artifact(&quot;confusion_matrix.png&quot;)
mlflow.end_run()
</code></pre>
<p>I save the .png file locally and then log it with <code>mlflow.log_artifact(&quot;confusion_matrix.png&quot;)</code> to the ftp server in the right folder corresponding to the experiment. Everything works so far, only that the artifact does not show up in the mlflow ui online. The logged parameters and metrics show up normally. The artifact panel stays empty and only shows</p>
<pre><code>No Artifacts Recorded
Use the log artifact APIs to store file outputs from MLflow runs.
</code></pre>
<p>I found similar threads, but only of users having the same problem on local mlflow storages. Unfortunately, I could not apply these fixes to my problem. Somebody has an idea how to fix this?</p>",0,2,2021-08-10 14:15:42.080000 UTC,,,1,python|machine-learning|ftp|pytorch|mlflow,685,2021-08-09 07:17:06.420000 UTC,2022-09-19 06:56:13.683000 UTC,,11,0,0,2,,,,,,['mlflow']
Sagemaker Studio tabs are not visible,"<p>In Sagemaker Studio my tabs are gone, how can I get them back?</p>
<p><a href=""https://i.stack.imgur.com/T7xHx.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/T7xHx.png"" alt=""enter image description here"" /></a></p>",1,0,2022-02-02 13:24:52.693000 UTC,,,0,amazon-sagemaker,106,2012-10-24 12:12:59.277000 UTC,2022-09-25 05:49:23.950000 UTC,"Leuven, Belgium",3126,1817,2,262,,,,,,['amazon-sagemaker']
Error while deserializing the Apache MXNet object,"<p>I have trained and saved a model using Amazon SageMaker which saves the model in the format of <code>model.tar.gz</code> which when untarred, has a file <code>model_algo-1</code> which is a serialized Apache MXNet object. To load the model in memory I need to deserialize the model. I tried doing so as follows:</p>

<p><code>import mxnet as mx
print(mx.ndarray.load('model_algo-1'))</code></p>

<p>Reference taken from <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/cdf-training.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/cdf-training.html</a></p>

<p>However, doing this yields me the following error:</p>

<pre><code>Traceback (most recent call last):
File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
File ""/usr/local/lib/python3.4/site-packages/mxnet/ndarray/utils.py"", line 
175, in load
ctypes.byref(names)))
File ""/usr/local/lib/python3.4/site-packages/mxnet/base.py"", line 146, in 
check_call
raise MXNetError(py_str(_LIB.MXGetLastError()))
mxnet.base.MXNetError: [19:06:25] src/ndarray/ndarray.cc:1112: Check failed: 
header == kMXAPINDArrayListMagic Invalid NDArray file format

Stack trace returned 10 entries:
[bt] (0) /usr/local/lib/python3.4/site-packages/mxnet/libmxnet.so(+0x192112) 
[0x7fe432bfa112]
[bt] (1) /usr/local/lib/python3.4/site-packages/mxnet/libmxnet.so(+0x192738) 
[0x7fe432bfa738]
[bt] (2) /usr/local/lib/python3.4/site-
packages/mxnet/libmxnet.so(+0x24a5c44) [0x7fe434f0dc44]
[bt] (3) /usr/local/lib/python3.4/site-
packages/mxnet/libmxnet.so(MXNDArrayLoad+0x248) [0x7fe434d19ad8]
[bt] (4) /usr/lib64/libffi.so.6(ffi_call_unix64+0x4c) [0x7fe48c5bbcec]
[bt] (5) /usr/lib64/libffi.so.6(ffi_call+0x1f5) [0x7fe48c5bb615]
[bt] (6) /usr/lib64/python3.4/lib-dynload/_ctypes.cpython-
34m.so(_ctypes_callproc+0x2fb) [0x7fe48c7ce18b]
[bt] (7) /usr/lib64/python3.4/lib-dynload/_ctypes.cpython-34m.so(+0xa4cf) 
[0x7fe48c7c84cf]
[bt] (8) /usr/lib64/libpython3.4m.so.1.0(PyObject_Call+0x8c) 
[0x7fe4942fcb5c]
[bt] (9) /usr/lib64/libpython3.4m.so.1.0(PyEval_EvalFrameEx+0x36c5) 
[0x7fe4943ac915]
</code></pre>

<p>Could someone suggest how this can be resolved?</p>",2,3,2018-04-04 19:21:07.753000 UTC,1.0,,1,mxnet|amazon-sagemaker,971,2016-09-22 15:20:27.237000 UTC,2020-03-12 18:44:22.640000 UTC,,1039,0,0,89,,,,,,['amazon-sagemaker']
"Invoking Sagemaker MultiDataModel Endpoint throws ""ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation""","<p>I'm trying to create a multi-model endpoint on sagemaker, using pre-trained tensorflow models  which were uploaded to s3 (tar.gz files). Creating a 'single-model' endpoint works fine with both of them.</p>
<p>I followed a few blog posts for this task (<a href=""https://dataintegration.info/host-multiple-tensorflow-computer-vision-models-using-amazon-sagemaker-multi-model-endpoints"" rel=""nofollow noreferrer"">1</a>, <a href=""https://towardsdatascience.com/deploy-multiple-tensorflow-models-to-one-endpoint-65bea81c3f2f"" rel=""nofollow noreferrer"">2</a>).</p>
<p>I've successfully deployed a MultiDataModel endpoint on Sagemaker (code attached below the error), but when trying to invoke a model (any of them) I received the following error:</p>
<pre><code>~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/sagemaker/tensorflow/model.py in predict(self, data, initial_args)
    105                 args[&quot;CustomAttributes&quot;] = self._model_attributes
    106 
--&gt; 107         return super(TensorFlowPredictor, self).predict(data, args)

~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/sagemaker/predictor.py in predict(self, data, initial_args, target_model, target_variant, inference_id)
    159             data, initial_args, target_model, target_variant, inference_id
    160         )
--&gt; 161         response = self.sagemaker_session.sagemaker_runtime_client.invoke_endpoint(**request_args)
    162         return self._handle_response(response)

~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/botocore/client.py in _api_call(self, *args, **kwargs)
    413                     &quot;%s() only accepts keyword arguments.&quot; % py_operation_name)
    414             # The &quot;self&quot; in this scope is referring to the BaseClient.
--&gt; 415             return self._make_api_call(operation_name, kwargs)
    416 
    417         _api_call.__name__ = str(py_operation_name)
~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/botocore/client.py in _make_api_call(self, operation_name, api_params)
    743             error_code = parsed_response.get(&quot;Error&quot;, {}).get(&quot;Code&quot;)
    744             error_class = self.exceptions.from_code(error_code)
--&gt; 745             raise error_class(parsed_response, operation_name)
    746         else:
    747             return parsed_response

ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: 
Received server error (504) from model with message &quot;&lt;html&gt;
&lt;head&gt;&lt;title&gt;504 Gateway Time-out&lt;/title&gt;&lt;/head&gt;
&lt;body&gt;
&lt;center&gt;&lt;h1&gt;504 Gateway Time-out&lt;/h1&gt;&lt;/center&gt;
&lt;hr&gt;&lt;center&gt;nginx/1.20.2&lt;/center&gt;
&lt;/body&gt;
&lt;/html&gt;
&quot;. See https://eu-central-1.console.aws.amazon.com/cloudwatch/home?region=eu-central- 1#logEventViewer:group=/aws/sagemaker/Endpoints/mme-tensorflow-2022-05-29-06-38-29 in 
account ******** for more information.
</code></pre>
<p>Here is the code for creating and deploying the models and the endpoint:</p>
<pre><code>import sagemaker
from sagemaker import get_execution_role
from sagemaker.multidatamodel import MultiDataModel
from sagemaker.tensorflow.model import TensorFlowModel


sagemaker_session = sagemaker.Session()
role = get_execution_role()
rating_model_archive = &quot;rating_model.tar.gz&quot;
sim_users_model_archive = &quot;sim_users_model.tar.gz&quot;
current_time = datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d-%H-%M-%S')

sagemaker_model_rating = TensorFlowModel(model_data = f's3://{bucket_name}/model/{rating_model_archive}',
                                         name = f'rating-model-{current_time}',
                                         role = role,
                                         framework_version = &quot;2.8&quot;, #tf.__version__,
                                         entry_point = 'empty_train.py',
                                         sagemaker_session=sagemaker_session)

sagemaker_model_sim = TensorFlowModel(model_data = f's3://{bucket_name}/model/{sim_users_model_archive}',
                                      name = f'similar-users-model-{current_time}',
                                      role = role,
                                      framework_version = &quot;2.8&quot;, #tf.__version__,
                                      entry_point = 'empty_train.py',
                                      sagemaker_session=sagemaker_session)

model_data_prefix = f's3://{bucket_name}/model/'

mme = MultiDataModel(name=f'mme-tensorflow-{current_time}',
                     model_data_prefix=model_data_prefix,
                     model=sagemaker_model_rating,
                     sagemaker_session=sagemaker_session)

tf_predictor = mme.deploy(initial_instance_count=2,
                          instance_type=&quot;ml.m4.xlarge&quot;,#'ml.t2.medium',
                          endpoint_name=f'mme-tensorflow-{current_time}')
</code></pre>
<p>Up until here, as mentioned earlier, it works fine, and I have a running endpoint.
When trying to invoke it with the following code, I get the aforementioned error:</p>
<pre><code>input1 = {
    &quot;instances&quot;: [
        {&quot;user_id&quot;: [854],
         &quot;item_id&quot;: [123]}
                 ]
}

input2 = {
    &quot;instances&quot;: [12]
}

tf_predictor.predict(data=input2, initial_args={'TargetModel': sim_users_model_archive})
# tf_predictor.predict(data=input1, initial_args={'TargetModel': rating_model_archive})
</code></pre>",0,1,2022-05-29 08:23:49.237000 UTC,,,0,python|amazon-web-services|tensorflow|amazon-sagemaker,106,2022-05-25 12:31:07.867000 UTC,2022-08-08 20:26:46.947000 UTC,,1,0,0,2,,,,,,['amazon-sagemaker']
"I am using Azure AML and created training pipeline. tracking metrics like r2, mae available in ""individual steps"" but not in ""job overview""","<p>I am using Azure AML and created training pipeline. I am tracking metrics like r2, mae etc. Metrics are available in ind. steps but not as &quot;Job Metrics&quot;<a href=""https://i.stack.imgur.com/fGbJx.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fGbJx.png"" alt=""Job Overview that does not have metrics"" /></a></p>
<p><a href=""https://i.stack.imgur.com/YpOad.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YpOad.png"" alt=""Evaluate model step has metrics"" /></a></p>
<p>Kindly help me track metrics at job level</p>",1,0,2022-07-15 12:11:10.740000 UTC,,,0,azure|pipeline|metrics|azure-machine-learning-studio|azure-machine-learning-service,35,2019-07-05 09:43:42.373000 UTC,2022-09-23 15:01:08.727000 UTC,,21,0,0,9,,,,,,"['azure-machine-learning-service', 'azure-machine-learning-studio']"
Sagemaker: Training everytime I need make a prediction: how should I structure the solution?,"<p>I have been asked to migrate a custom model to Sagemaker. This model is a forecasting script that trains everytime it is run and then predicts after training. (It is a two-layer forecasting prediction with SARIMAX). The flow is as explained below:</p>

<ol>
<li>train arima model to get exogenous variables (training algorithm 1)</li>
<li>predict with that trained model</li>
<li>use the output variables to train the second layer (training algorithm 2)</li>
<li>predict with this last trained model and output the solution</li>
</ol>

<p>This is not what im used to do in Sagemaker (I train a model once that will be invoked multiple times), so how could I frame this? Train models separately from two separate docker images and create two endpoints? The whole train-predict-train-predict workflow would no longer be automatic, right? How would I trigger this workflow? Please help!</p>",0,0,2020-06-02 07:56:32.243000 UTC,,,2,machine-learning|forecasting|amazon-sagemaker|arima,64,2018-03-21 09:51:17.743000 UTC,2020-11-11 15:52:43.670000 UTC,,33,0,0,2,,,,,,['amazon-sagemaker']
Machine Learning Probem in Training Sagemaker Model,"<p>I am working on Machine learning with MERN(MongoDb,Experss,ReactJS,NodeJS) Stack in which Aws Keys (Access key and Secret Key) place in MongoDB Configurations and for train model Code writtern in Nodejs and Express.</p>
<p><strong>Problem :</strong>
After upload the .csv or .xls file Training is not completed in aws account it will display training failed with following error:</p>
<p><strong>AlgorithmError: CannotStartContainerError. Please make sure the container can be run with 'docker run  train'. Please refer SageMaker documentation for details. It is possible that the Dockerfile's entrypoint is not properly defined, or missing permissions.</strong></p>
<p><strong>For the Machine learning I will done following setttings :</strong>
<strong>S3</strong>
-Create Access key and Secret key and Create Bucket
-Keys are placed to MongoDB Configurations and Bucket and region also Placed there.</p>
<p><strong>Sagemaker -&gt; Notebook Instacne</strong>
-Create Notebook Instance and also service comes in pending status.
-This settings done in aws account.</p>
<p><strong>Sagemaker -&gt; I AM Role</strong>
-Create Role I AM
-This settings done in aws account.</p>
<p><strong>Sagemaker -&gt; Model</strong>
-Create Model and Model ARN and Role ARN.
-This settings done in aws account.</p>
<p><strong>ECR ( Elastic Container Registry)</strong>
-Create Repository with name linear-learner and xgboost
-This settings done in aws account.</p>
<p><strong>Dockerfile</strong>
-Create Dockerfile
-Save in Project Folder.</p>
<p><strong>Docker Hub</strong>
-Create Docker Hub Account</p>
<p><strong>SNS Creadentials</strong>
-SNS Key and Topic ARN
-Set in MongoDB Configurations.</p>
<p><strong>Folllowing Permissions Given by Me :</strong>
<strong>Attached Directory</strong>
AmazonSagemakerFullAccess
AmazonS3FullAccess
AmazonSNSFullAccess
AmazonEC2ContainerRegistryReadOnly</p>
<p><strong>Attached From Group</strong>
AmazonEC2FullAccess
AmazonDymonDBFullAccess
AmazonMachineLearningFullAccess
AdministratorAccess
AWSElasticBeanStalkFullAccess
AmazonSagemakerFullAccess</p>
<p><strong>Dockerfile</strong></p>
<pre><code>FROM Ubuntu
RUN apt-get update
RUN apt-get install curl -y
RUN curl -sL https://deb.nodesource.com/setup_10.x -o nodesource_setup.sh
RUN bash nodesource_setup.sh
RUN apt install nodejs -y
WORKDIR /usr/app
COPY . /usr/app/
RUN npm install
EXPOSE 3000
ENTRYPOINT [ &quot;python3.7&quot;, &quot;/opt/ml/code/train.py&quot; ]
</code></pre>
<pre><code>Code Image 

Linear : &lt;account_id&gt;.dkr.ecr.us-east-2.amazonaws.com/linear-learner:latest

XgBoost : &lt;account_id&gt;.dkr.ecr.us-east-2.amazonaws.com/xgboost:latest
</code></pre>
<pre><code>I run the following commands to build and tag images from dockerfile:
-&gt; $ docker build -t &lt;codeimage&gt;:&lt;tag&gt;
Successfully built &lt;id&gt;
Successfully tagged &lt;codeimage&gt;:&lt;tag&gt;

-&gt; $ docker build -t &lt;account.id&gt;.dkr.ecr.&lt;region&gt;.amazonaws.com/&lt;codeimage&gt;
Successfully build &lt;id&gt;
Successfully tagged &lt;url&gt;

-&gt; $ docker images
-Check all images with tags

-&gt; $ docker tag [image name]:[tag] [repository URI]

&gt; $ sudo aws ecr get-login-password | sudo docker login –username AWS –password -stdin [account.id].dkr.ecr.[region].amazonaws.com
Login Succedded

&gt; $ sudo docker push [account.id].dkr.ecr.[region].amazonaws.com/[repository name]

&lt;id&gt; Pushed

&lt;id&gt; Pushed
</code></pre>
<p><strong>Docker Images</strong>
<a href=""https://i.stack.imgur.com/EOnVN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EOnVN.png"" alt=""enter image description here"" /></a></p>
<p><strong>Docker image is also push in Docker Hub by creating repository with same name which we gave in ECR.</strong>
<a href=""https://i.stack.imgur.com/ERawP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ERawP.png"" alt=""enter image description here"" /></a></p>
<p><strong>After All these settings and things when I upload .csv or .xls file after sometime I got the same error during training model which is training failed as mentions upper and my process is not completed at 100%.</strong>
<a href=""https://i.stack.imgur.com/e81aL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/e81aL.png"" alt=""enter image description here"" /></a></p>
<p>After setting all these things and permissions process is not completed yet and getting this error can anyone help me for process these?</p>",0,4,2021-01-13 10:03:57.003000 UTC,1.0,2021-01-13 12:41:16.113000 UTC,0,node.js|docker|amazon-sagemaker|amazon-ecr|amazon-machine-learning,213,2015-07-06 11:36:30.117000 UTC,2022-01-03 07:39:48.623000 UTC,,13,0,0,18,,,,,,['amazon-sagemaker']
Is there an easy way to train an AWS DeepAR model?,"<p>So I have my data structured right.</p>
<pre><code>{'start': '2017-05-02', 'target':[1,2,3,4,5], 'cat':[1,0,13], 'dynamic_feat':[[1,2,3,4],[4,3,2,1], [6,7,8,9]]}
</code></pre>
<p>But it's difficult to do all of the coding to train the DeepAR model. I've looked all over the internet to see if there's an easier way to do it (like using AutoPilot) but I haven't found anything. Can someone guide me in the right direction? I got close but I've had no luck whatsoever. It fail every time.</p>
<p>Also, I have more than one JSON object (it's actually a list of dictionaries). Is there even just an easier way to train the model using code that doesn't require a file in the S3 bucket?</p>
<p>Thanks for the help guys!</p>",1,0,2021-07-31 14:32:27.713000 UTC,,,0,python|json|amazon-sagemaker|deepar,57,2014-07-17 15:08:26.593000 UTC,2022-08-10 15:49:39.013000 UTC,"Michigan, United States",3113,145,5,317,,,,,,['amazon-sagemaker']
Mlflow and KerasTuner integration,"<p>I am trying to integrate together <code>KerasTuner</code> and <code>Mlflow</code>. I'd like to record the loss at each epoch of each trial of Keras Tuner.</p>
<p>My approach is:</p>
<pre><code>class MlflowCallback(tf.keras.callbacks.Callback):
    
    # This function will be called after each epoch.
    def on_epoch_end(self, epoch, logs=None):
        if not logs:
            return
        # Log the metrics from Keras to MLflow     
        mlflow.log_metric(&quot;loss&quot;, logs[&quot;loss&quot;], step=epoch)
    

from kerastuner.tuners import RandomSearch

with mlflow.start_run(run_name=&quot;myrun&quot;, nested=True) as run:
  
  tuner = RandomSearch(
      train_fn,
      objective='loss',
      max_trials=25, 
  )
  tuner.search(train,
              validation_data=validation, 
              validation_steps=validation_steps,
              steps_per_epoch=steps_per_epoch, 
              epochs=5, 
              callbacks=[MlflowCallback()]
  )
</code></pre>
<p>However, the loss values are reported (sequentially) in one single experiment. Is there a way to record them independently?
<a href=""https://i.stack.imgur.com/awObK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/awObK.png"" alt=""Loss values"" /></a></p>",1,0,2021-01-22 17:17:09.787000 UTC,0.0,,2,tensorflow|keras|callback|mlflow|keras-tuner,390,2011-10-15 10:06:34.370000 UTC,2021-07-07 15:02:50.657000 UTC,Rome,3651,12,1,269,,,,,,['mlflow']
Google.Cloud.AIPlatform.V1 Received http2 header with status: 404,"<p>We are trying to call the Google.Cloud.AIPlatform.V1 predict API using the .Net client and keep getting the following error:   Received http2 header with status: 404</p>
<p>We setup credentials using the API key and environment variable:  GOOGLE_APPLICATION_CREDENTIALS</p>
<p>Here is the code to call the vertex AI predict API:</p>
<pre><code>const string projectId = &quot;xxxxxx&quot;;
const string location = &quot;us-central1&quot;; 
const string endpointId = &quot;xxxxxx&quot;;  

PredictionServiceClient client = PredictionServiceClient.Create();

var structVal = Google.Protobuf.WellKnownTypes.Value.ForStruct(new Struct
{
    Fields =
    {
    [&quot;mimeType&quot;] = Google.Protobuf.WellKnownTypes.Value.ForString(&quot;text/plain&quot;),
    // Sample contents is a string constant defined in a separate file
    [&quot;content&quot;] = Google.Protobuf.WellKnownTypes.Value.ForString(Consts.SampleContents)
    }
});

PredictRequest req = new PredictRequest()
{
    EndpointAsEndpointName = EndpointName.FromProjectLocationEndpoint(projectId, location, endpointId),
    Instances = { structVal }
};

PredictResponse response = client.Predict(req);
</code></pre>
<p>The full error returned:</p>
<p>Status(StatusCode=&quot;Unimplemented&quot;, Detail=&quot;Received http2 header with status: 404&quot;, DebugException=&quot;Grpc.Core.Internal.CoreErrorDetailException: {&quot;created&quot;:&quot;@1644947338.412000000&quot;,&quot;description&quot;:&quot;Received http2 :status header with non-200 OK status&quot;,&quot;file&quot;:&quot;......\src\core\ext\filters\http\client\http_client_filter.cc&quot;,&quot;file_line&quot;:134,&quot;grpc_message&quot;:&quot;Received http2 header with status: 404&quot;,&quot;grpc_status&quot;:12,&quot;value&quot;:&quot;404&quot;}&quot;)</p>
<p>I validate the same call using CURL and was able to successfully make the call.</p>
<pre><code>curl -X POST -H &quot;Authorization: Bearer XXXXX&quot; -H &quot;Content-Type: application/json&quot; https://us-central1-aiplatform.googleapis.com/ui/projects/XXXXXX/locations/us-central1/endpoints/XXXXXX:predict -d @payload.json
</code></pre>
<p>Any help would be greatly appreciated.</p>",1,0,2022-02-15 20:18:14.297000 UTC,,,0,c#|google-cloud-vertex-ai,204,2018-12-26 14:50:25.527000 UTC,2022-09-24 17:43:12.653000 UTC,,1,0,0,6,,,,,,['google-cloud-vertex-ai']
Vertex AI Tensorboard trough user interface,"<p>I have been using the Vertex AI training service with a custom container for my own machine learning pipeline. I would like to get tensorboard logs into the experiments tab to see in real-time the metrics while the model is training.</p>
<p>I was wondering if it is possible to set a custom training job in the user interface setting a <code>TENSORBOARD_INSTANCE_NAME</code>. It seems that this is only possible through a json-post-request.</p>",0,2,2021-10-30 11:18:07.870000 UTC,,,0,tensorboard|google-cloud-ml|google-cloud-vertex-ai,222,2021-08-10 14:58:20.490000 UTC,2022-09-14 19:08:59.140000 UTC,Colombia,3,0,0,4,,,,,,['google-cloud-vertex-ai']
Vertex AI Pipeline is not using the GPU,"<p>I am building a customized pipeline with the following step:</p>
<pre><code>     trainer_task = (trainer(download_task.output).set_cpu_request(&quot;16&quot;).set_memory_request(&quot;60G&quot;).
add_node_selector_constraint('cloud.google.com/gke-accelerator', &quot;NVIDIA_TESLA_K80&quot;).set_gpu_limit(2))
</code></pre>
<p>However, when I check the number of GPU's available it says zero, and the only visible device is a CPU device. I am migrating a project from Kubeflow, and this is the first time using Vertex AI, so I am not pretty sure why this is happening.</p>
<p>The involved step is component that loads a docker image from the Artifact Registry and installs Tensorflow-gpu==2.4.1.</p>
<p>Am I missing something? Why is not enabling the specified GPUs?</p>
<p>Any help will be highly appreciated!</p>",0,2,2022-02-04 09:52:13.550000 UTC,,,1,tensorflow|google-cloud-platform|kubeflow-pipelines|google-cloud-vertex-ai,250,2022-02-04 09:41:08.103000 UTC,2022-06-05 13:09:20.203000 UTC,,11,0,0,0,,,,,,['google-cloud-vertex-ai']
Running an .ipynb file from Lambda function or Sagemaker Lifecycle Config,"<p>ipynb file while starting a Sagemaker instance.
current status is:
Cloudwatch(success) -> Lambda(success) - > Sagemaker instance(success) -> Running Particular Notebook (failed)</p>

<p>1.I tried using ""Sagemaker Lifecycle"" config with the code</p>

<pre><code>jupyter nbconvert --execute prediction-12hr.ipynb --ExecutePreprocessor.kernel_name=conda_tensorflow_p36
</code></pre>

<p>but getting an error</p>

<pre><code>[NbConvertApp] Converting notebook prediction-12hr.ipynb to html [NbConvertApp] Executing notebook with kernel: conda_tensorflow_p36
...
raise NoSuchKernel(kernel_name) jupyter_client.kernelspec.NoSuchKernel: No such kernel named conda_tensorflow_p36
</code></pre>

<p><strong>on running</strong></p>

<pre><code>`!conda env list'
conda environments:

base * /home/ec2-user/anaconda3
JupyterSystemEnv /home/ec2-user/anaconda3/envs/JupyterSystemEnv
chainer_p27 /home/ec2-user/anaconda3/envs/chainer_p27
chainer_p36 /home/ec2-user/anaconda3/envs/chainer_p36
mxnet_p27 /home/ec2-user/anaconda3/envs/mxnet_p27
mxnet_p36 /home/ec2-user/anaconda3/envs/mxnet_p36
python2 /home/ec2-user/anaconda3/envs/python2
python3 /home/ec2-user/anaconda3/envs/python3
pytorch_p27 /home/ec2-user/anaconda3/envs/pytorch_p27
pytorch_p36 /home/ec2-user/anaconda3/envs/pytorch_p36
tensorflow_p27 /home/ec2-user/anaconda3/envs/tensorflow_p27
tensorflow_p36 /home/ec2-user/anaconda3/envs/tensorflow_p36

Also tried injecting a python/bash code to run the instance startup, pausing the start-up code to wait untill conda instance is setup by sagemaker.
Still no luck
</code></pre>

<p><em>Can someone suggest a plan to run .ipynb file in anyways possible.</em></p>",2,0,2018-09-12 04:13:30.150000 UTC,1.0,,3,jupyter-notebook|amazon-sagemaker,1445,2014-02-24 08:17:32.660000 UTC,2022-05-31 05:29:03.163000 UTC,"Bangalore, Karnataka, India",347,35,0,41,,,,,,['amazon-sagemaker']
run id from az ml cli,"<p>How to pass run id of experiment as tag information of model ?</p>

<p>I want to run experiment and register model with tag information with run id of experiment in az ml cli in Azure DevOps Build pipeline.</p>

<ul>
<li><p>run experiment
az ml run submit-script  -e test -d myenv.yml train.py</p></li>
<li><p>model register
az ml model register -n mymodel -p sklearn_regression_model.pkl --tag ""run id""= ????</p></li>
</ul>

<p>I can't figure out how to get run id from experiment run from az ml cli and pass it to --tag argument. Any idea ?</p>",3,0,2019-08-16 12:47:20.240000 UTC,,,1,azure-devops|azure-machine-learning-service,585,2019-08-07 14:59:45.213000 UTC,2021-02-14 14:41:45.463000 UTC,,71,3,0,11,,,,,,['azure-machine-learning-service']
"Is there mlflow REST api to hard delete experiments, runs?","<p>Mlfow exp delete api does soft delete and when you create experiment with that name, it gives error RESOURCE_ALREADY_EXISTS.</p>
<p>Is there any way to delete experiment permanently through api?
<a href=""https://www.mlflow.org/docs/latest/rest-api.html#delete-experiment"" rel=""nofollow noreferrer"">https://www.mlflow.org/docs/latest/rest-api.html#delete-experiment</a></p>
<p>There is similar question here, <a href=""https://stackoverflow.com/questions/60088889/how-do-you-permanently-delete-an-experiment-in-mlflow"">How Do You &quot;Permanently&quot; Delete An Experiment In Mlflow?</a>
where answers are to run delete sql queries directly by connecting to backend DB which i want to avoid.</p>",0,0,2022-04-12 09:46:00.843000 UTC,1.0,,1,rest|mlflow|mlops,93,2017-05-31 04:12:26.490000 UTC,2022-09-24 08:59:44.060000 UTC,,838,89,2,33,,,,,,['mlflow']
mlflow: saving signature gives me warning,"<p>I am using mlflow with sqlite backend. started the server with:</p>
<pre><code>mlflow server --backend-store-uri sqlite:///mlruns_db/mlruns.db --default-artifact-root $PWD/mlruns --host 0.0.0.0 -p 5000
</code></pre>
<p>in the code, I log the model with signature as such</p>
<pre><code>...
signature = infer_signature(X, y)
mlflow.sklearn.log_model(model, model_name, signature=signature)
...
</code></pre>
<p>then I get warnings</p>
<blockquote>
<p>2022/05/26 19:52:17 WARNING mlflow.models.model: Logging model metadata to the tracking server has failed, possibly due older server version. The model artifacts have been logged successfully under ./mlruns/1/d4c8f611d3f24986a32d19c7d8b03f06/artifacts. In addition to exporting model artifacts, MLflow clients 1.7.0 and above attempt to record model metadata to the tracking store. If logging to a mlflow server via REST, consider upgrading the server version to MLflow 1.7.0 or above.</p>
</blockquote>
<p>I am using <code>mlflow, version 1.24.0</code>, though.</p>
<p>I see that the signature is correctly logged inside <code>MLmodel</code> file, but the nice rendering of mlflow ui is lost.</p>
<ol>
<li><p>with logging signature
<a href=""https://i.stack.imgur.com/r2FwI.png"" rel=""nofollow noreferrer"">mlflow ui with logging signature</a></p>
</li>
<li><p>without logging signature
<a href=""https://i.stack.imgur.com/9nQ8w.png"" rel=""nofollow noreferrer"">mlflow ui without logging signature</a></p>
</li>
</ol>
<p>Does this have any consequence later when serving models with signature enforcement?
Also, I see many blog examples with postgres instead of sqlite, and sftp/minio instead of filestore. maybe changing to those setups will solve this?</p>",0,0,2022-05-28 15:14:55.023000 UTC,,,1,postgresql|sqlite|metadata|mlflow,194,2022-05-28 14:30:27.087000 UTC,2022-08-24 16:50:33.540000 UTC,,11,0,0,1,,,,,,['mlflow']
Amazon Sagemaker - Unable to evaluate payload provided,"<p>I built a Sagemaker endpoint that I am attempting to evoke using Lambda+API Gateway.  I'm getting the following error:</p>
<pre><code>&quot;An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from model with message \&quot;unable to evaluate payload provided\&quot;
</code></pre>
<p>I know why what it's complaining about, but I don't quite understand why it's occuring.  I have confirmed that the shape of the input data of my lambda function is the same as how I trained the model.  The following is my input payload in lambda:</p>
<pre><code>X = pd.concat([X, rx_norm_dummies, urban_dummies], axis = 1)
payload = X.to_numpy()

response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME,
                               ContentType='application/json',
                               Body=payload)
</code></pre>
<p>In the jupyter notebook where I created my endpoint/trained my model, I can also access the model using a numpy ndarray so I'm confused why I'm getting this error.</p>
<pre><code>y = X[0:10]
result = linear_predictor.predict(y)
print(result)
</code></pre>
<p>Here is a modificaiton I make to serialization of the endpoint:</p>
<pre><code>from sagemaker.predictor import csv_serializer, json_deserializer

    linear_predictor.content_type = 'text/csv'
    linear_predictor.serializer = csv_serializer
    linear_predictor.deserializer = json_deserializer
</code></pre>
<p>I'm new when it comes to Sagemaker/Lambda, so any help would be appreciated and I can send more code to add context if needed.  Tried various foramts and cannot get this to work.</p>",2,0,2020-11-28 01:02:28.557000 UTC,,2020-12-01 21:13:50.370000 UTC,2,python|aws-lambda|amazon-sagemaker,988,2015-01-16 15:19:31.370000 UTC,2022-09-19 13:25:37.387000 UTC,,1304,87,1,167,,,,,,['amazon-sagemaker']
Version control for machine learning data set with large amount of images?,"<p>We starting to use <a href=""https://dvc.org/"" rel=""noreferrer"">dvc</a>  with git to control versioning of machine learning projects.
For dvc remote storage  we use google cloud storage.</p>

<p>Our data set is OCR data set with more than 100000 small images, total size is about 200 MB.
Using  dvc to track this data set we encountered with next  problems:</p>

<ol>
<li>It took a lot of time to add data set for tracking.</li>
<li>Very slow upload.</li>
<li>Very slow download.</li>
<li>Update/delete/add just one image in data set cause dvc to recompute
a lot of things : hashes etc....</li>
</ol>

<p>From another way if we zipping our data set and track it as single file  dvc work fast enough.But the problem is in this way we can't track changes for particular file.</p>

<p>The goal is to have version control for data set with large amount of files with next functionality.</p>

<ol>
<li>Tracking for each single file.</li>
<li>Committing only changes and not whole data set.</li>
<li>Fast checkout/pull</li>
</ol>

<p>Any suggestion for better solution acceptable.</p>",1,8,2019-05-08 07:33:24.753000 UTC,5.0,,10,git|machine-learning|google-cloud-storage|dvc,856,2014-07-09 13:28:02.227000 UTC,2020-11-15 16:01:42.947000 UTC,,321,3,0,24,,,,,,['dvc']
Best way to work with larger FileDataSet in azureml on docker-based AmlCompute,"<p>What would be the recommended way to work with a FileDataSet on AmlCompute, when submitting an Estimator-based run (with docker enabled)?</p>

<p>My FileDataset is around 1.5Gb and contains a few 1000 images.<br>
I have a tabular dataset with references to images in that FileDataset.  This tabular dataset contain the classes or references to other (mask) images, depending on the model I'm trying to train.</p>

<p>So in order to load the images into memory (np.arrays), I have to read the image from the file location, based on the file name in my TabularDataset.</p>

<p>At this point, I see two options, but none of them are feasible, as they take ages (+1 hour) to complete, and it's just not workable:</p>

<p><strong>Mount the file dataset</strong></p>

<pre class=""lang-py prettyprint-override""><code>image_dataset = ws.datasets['imagedata']
mounted_images = image_dataset.mount()
mounted_images.start()
print('Data set mounted', datetime.datetime.now())

load_image(mounted_images.mount_point + '/myfilename.png')
</code></pre>

<p><strong>Download the dataset</strong></p>

<pre class=""lang-py prettyprint-override""><code>image_dataset = ws.datasets['chart-imagedata']
image_dataset.download(target_path='chartimages', overwrite=False)
</code></pre>

<p>I want to get the fastest possible way to launch an Estimator on AmlCompute and get access to the files as quick and easy as possible.  </p>

<p>I had a look at this <a href=""https://stackoverflow.com/questions/60562966/transfer-from-adls2-to-compute-target-very-slow-azure-machine-learning/60628585#60628585"">post</a> on stackoverflow, where they indicated that it would be good to update the azureml sdk packages in the train.py script, and I have applied that, but no difference.</p>

<p><strong>EDITED (more information)</strong>:</p>

<ul>
<li>Data source is Azure Blob Storage (storage account has ADLS 2.0 enabled)</li>
<li>Size of my compute target (cluster of 0-4, but only using 1 node) of size <code>STANDARD_D2_V2</code></li>
</ul>

<p>The train.py I am using (just for repro purposes):</p>

<pre class=""lang-py prettyprint-override""><code># Force latest prerelease version of certain packages
import subprocess
import sys

def install(package):
    subprocess.check_call([sys.executable, ""-m"", ""pip"", ""install"", ""--upgrade"", ""--pre"", package])

install('azureml-core')
install('azureml-sdk')

# General references
import argparse
import os
import numpy as np
import pandas as pd
import datetime

from azureml.core import Workspace, Dataset, Datastore, Run, Experiment

import sys
import time

ws = Run.get_context().experiment.workspace

# Download file data set
print('Downloading data set', datetime.datetime.now())
image_dataset = ws.datasets['chart-imagedata']
image_dataset.download(target_path='chartimages', overwrite=False)
print('Data set downloaded', datetime.datetime.now())


# mount file data set
print('Mounting data set', datetime.datetime.now())
image_dataset = ws.datasets['chart-imagedata']
mounted_images = image_dataset.mount()
mounted_images.start()
print('Data set mounted', datetime.datetime.now())

print('Training finished')
</code></pre>

<p>And I'm using a TensorFlow Estimator:</p>

<pre class=""lang-py prettyprint-override""><code>from azureml.core.compute import ComputeTarget, AmlCompute
from azureml.core.compute_target import ComputeTargetException


# Choose a name for your CPU cluster
gpu_cluster_name = ""g-train-cluster""

# Verify that cluster does not exist already
try:
    gpu_cluster = ComputeTarget(workspace=ws, name=gpu_cluster_name)
    print('Found existing cluster, use it.')
except ComputeTargetException:
    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2',
                                                           max_nodes=4, min_nodes=0)
    gpu_cluster = ComputeTarget.create(ws, gpu_cluster_name, compute_config)
    print('Creating new cluster')

constructor_parameters = {
    'source_directory':training_name,
    'script_params':script_parameters,
    'compute_target':gpu_cluster,
    'entry_script':'train.py',
    'pip_requirements_file':'requirements.txt', 
    'use_gpu':True,
    'framework_version': '2.0',
    'use_docker':True}

estimator = TensorFlow(**constructor_parameters)
run = self.__experiment.submit(estimator)
</code></pre>",0,6,2020-06-07 13:24:38.500000 UTC,,2020-06-07 17:52:14.433000 UTC,1,python|azure|azure-machine-learning-service,251,2013-02-12 07:50:30.743000 UTC,2022-09-21 18:28:12.907000 UTC,Belgium,2947,297,16,355,,,,,,['azure-machine-learning-service']
UI does display data in MLflow,"<p>This is in reference to rather comment (not answer), I added here: <a href=""https://stackoverflow.com/questions/63255631/mlflow-invalid-parameter-value-unsupported-uri-mlruns-for-model-registry-s/66371465#66371465"">MLflow: INVALID_PARAMETER_VALUE: Unsupported URI './mlruns' for model registry store</a></p>
<p>I extracted files from <a href=""https://github.com/mlflow/mlflow/tree/master/examples/sklearn_elasticnet_wine"" rel=""nofollow noreferrer"">here</a></p>
<pre><code>train.py     MLproject     wine-quality.csv 
</code></pre>
<p>These are in directory:<code>feb24MLFLOW</code></p>
<p>I am in directory <code>feb24MLFLOW</code> with following contents</p>
<pre><code>:memory          mlruns           train.py         wine-quality.csv
</code></pre>
<p>When I run following command</p>
<pre><code>mlflow server --backend-store-uri sqlite:///:memory --default-artifact-root ./mlruns
</code></pre>
<p><strong>The UI loads but does not show any data in it neigther does database as below. see screenshot.</strong>
<a href=""https://i.stack.imgur.com/j3XSe.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/j3XSe.png"" alt=""enter image description here"" /></a>
I am using <code>--default-artifact-root ./mlruns</code> flag because, when I print  print(mlflow.get_tracking_uri()), I get the current directory</p>
<pre><code>file:///&lt;mydirectorylocations&gt;/feb24MLFLOW/mlruns
</code></pre>
<p>For some reason I see my database is not updating (or inserting). I checked that with in terminal.</p>
<pre><code>$  sqlite3
sqlite&gt; .open :memory 
sqlite&gt; .tables
alembic_version        metrics                registered_model_tags
experiment_tags        model_version_tags     registered_models    
experiments            model_versions         runs                 
latest_metrics         params                 tags                 
sqlite&gt; select * from runs;
sqlite&gt; 
</code></pre>
<p>As you can see there is no data after running <code>select * from runs</code> above.</p>
<p>Please note that I have following contents in</p>
<pre><code>./mlruns

d6db5cf1443d49c19971a1b8b606d692 meta.yaml

Can somebody suggest I show results in the UI? or insert in databse? or what am I doing wrong? 
</code></pre>
<p>Please note that when I run <code>mlflow ui</code>, I see data in the UI but I get:</p>
<pre><code>error_code: &quot;INVALID_PARAMETER_VALUE&quot;
message: &quot; Model registry functionality is unavailable; got unsupported URI './mlruns' for model registry data storage. Supported URI schemes are: ['postgresql', 'mysql', 'sqlite', 'mssql']. See https://www.mlflow.org/docs/latest/tracking.html#storage for how to run an MLflow server against one of the supported backend storage locations.&quot; 
</code></pre>",0,5,2021-02-25 15:35:45.170000 UTC,,2021-02-26 18:02:39.173000 UTC,2,databricks|mlflow,1085,2015-07-15 19:03:26.677000 UTC,2022-05-12 14:45:55.907000 UTC,"New York, NY, United States",853,6,2,179,,,,,,['mlflow']
"On tensorflow on sagemaker receiving error as input must be a vector, got shape: [1,2]","<p>I have built a tensorflow model on Sagemaker. The model uses universal sentence encoder </p>

<pre><code>import json
class MixEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        return json.JSONEncoder.default(self, obj)

serialize_data = json.dumps({'input1': np.array((""Hello"",""World"")) } , cls = MixEncoder  )
print(serialize_data)

response = client.invoke_endpoint(EndpointName=endpoint_name,ContentType='application/json' ,Body=serialize_data)-- 
</code></pre>

<p>This line throws an error </p>

<blockquote>
  <p>ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from model with message ""{ ""error"": ""input must be a vector, got shape: [1,2]\n\t </p>
  
  <p>The model expects only one tensor as input . It is defined as 
  input_model_placeholder = tf.placeholder(tf.string, shape=[None], name=""tensor_input_model_1"").</p>
</blockquote>

<p>Please help</p>",1,0,2020-02-01 09:23:11.430000 UTC,1.0,2020-02-01 09:39:37.087000 UTC,3,python|tensorflow2.0|amazon-sagemaker|tensorflow-hub,222,2014-12-27 14:50:49.220000 UTC,2020-02-09 12:33:47.953000 UTC,,31,0,0,4,,,,,,['amazon-sagemaker']
Distributed training example for Temporal Fusion Transformer in SageMaker,"<p>We’re training a big <a href=""https://pytorch-forecasting.readthedocs.io/en/latest/tutorials/stallion.html"" rel=""nofollow noreferrer"">Temporal Fusion Transformer</a> using PyTorch.</p>
<p>We’re looking into using Distributed Training and accelerate training jobs with SageMaker.</p>
<p>Does anyone have any examples of this? Any pattern you can recommend?</p>",1,0,2022-09-09 12:01:28.167000 UTC,,,0,amazon-web-services|pytorch|amazon-sagemaker,19,2022-09-08 07:14:26.503000 UTC,2022-09-23 21:03:19.637000 UTC,,48,2,0,6,,,,,,['amazon-sagemaker']
Vertex AI Workbench notebooks unresponsive,"<p>Having various problems accessing with GCP Vertex AI Workbench managed notebooks.  Could really use some suggestions about recovering, and avoiding further failure.</p>
<p>The original behavior (two days ago) was</p>
<ul>
<li>After working in the JupyterLab instance for a bit over an hour (creating a handful of notebooks within the instance), some kind of connectivity is lost.
<ul>
<li>Inside the JupyterLab interface:  cells won't run, the notebook is unable to save to disk or export, and restarting the kernel doesn't work.</li>
<li>On-screen error pop-up:  502, with message mentioning &quot;bad gateway&quot; or something like that</li>
<li>Back out in the console screen for managing my Workbench instances, I was able to use the Reset command to get the instance back to a working state.</li>
</ul>
</li>
</ul>
<p>**Note: **  This instance was provisioned with a setting to suspend after one hour of idle time.  It's not obviously relevant; the failure was a little more than an hour after creation, but there certainly wasn't an hour of idle time before things went to heck.</p>
<p>Today, I came back and was again able to work in the same instance for a bit over an hour, but then the same symptoms locked in.  Couldn't execute code, couldn't save the notebook.</p>
<p>However, things are worse now, because hitting Reset has led to an endless period of spinning cursor.  The instance won't complete its reset and can't start.  When I hover over the spinning cursor where the OPEN JUPYTERLAB button ought to be, a hover box says &quot;Setting up proxy to JupyterLab&quot;.</p>
<p>The hover text for the Instance status says:  &quot;Provisioning&quot;.</p>
<p>More:  I also tried creating a new notebook instance from the Workbench console screen, and it's stuck in the same condition -- just spinning, never reaching running state.  If I try to Reset it, a minor little pop-up appears at the bottom of the screen like so:
<a href=""https://i.stack.imgur.com/GA5fl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GA5fl.png"" alt=""enter image description here"" /></a></p>
<p>Subsequently, the hover text raised by the Reset button is:
<a href=""https://i.stack.imgur.com/7cbZI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7cbZI.png"" alt=""enter image description here"" /></a></p>
<p>At the least, I'm hoping to regain access to the initial instance at least once to recover some code in the notebooks (and go run it in a less flaky cloud service).  At best, you could help me manage this so that this GCP service is actually viable over time for me.</p>",0,6,2022-06-16 19:27:31.767000 UTC,,2022-07-02 08:08:31.850000 UTC,3,jupyter-notebook|google-cloud-ml|google-cloud-vertex-ai|gcp-ai-platform-notebook,622,2015-11-23 03:15:36.167000 UTC,2022-09-23 21:05:58.280000 UTC,"Austin, TX, United States",919,400,0,64,,,,,,['google-cloud-vertex-ai']
ImportError: cannot import name np_utils in AWS Sagemaker,"<p>I run my .ipynb file in AWS Sagemaker. </p>

<p>Unlike Colab, we need to install the dependencies separately in Sagemaker.</p>

<p>While pip installing tensorflow,I got error like <strong>ImportError: cannot import name np_utils</strong>.</p>

<p>After that I installed that too by <strong>!pip install np_utils</strong>
Then also I face similar error.</p>

<p>If this is in local system, it may be due to system configuration. Since its in Sagemaker,I am not sure about how to proceed futher.</p>",0,2,2020-01-30 14:09:16.327000 UTC,,2020-01-30 16:11:59.820000 UTC,0,python|amazon-web-services|machine-learning|amazon-sagemaker,475,2019-02-13 13:44:21.003000 UTC,2020-03-11 07:57:22.813000 UTC,"Chennai, Tamil Nadu, India",1,0,0,5,,,,,,['amazon-sagemaker']
Azure ML Studio Designer - Is it possible to copy pipeline or pipeline drafts from one workspace to another?,"<p>Is it possible to export or copy Pipelines created in Azure ML Studio <em>Designer</em> from one Workspace to another, using the UI, python sdk, and/or azure CLI?  If so, how?</p>
<p>EDIT:  My Designer does not appear to have the 'Export To Code' option that DeepDave-MT shows below.  How do I enable this ability?</p>
<p><a href=""https://i.stack.imgur.com/xhpCf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xhpCf.png"" alt=""enter image description here"" /></a></p>",1,0,2021-11-05 21:24:48.900000 UTC,,2021-11-09 16:20:55.253000 UTC,3,azure-machine-learning-studio|azureml-python-sdk|azuremlsdk,267,2012-06-27 21:51:16.130000 UTC,2022-09-21 21:19:20.110000 UTC,,751,68,5,73,,,,,,['azure-machine-learning-studio']
Deploying Huggingface model for inference - pytorch-scatter issues,"<p>It's my first time with SageMaker, and I'm having issues when trying to execute this script I took from this <a href=""https://huggingface.co/google/tapas-base-finetuned-wtq"" rel=""nofollow noreferrer"">Huggingface model</a> (deploy tab)</p>
<pre><code>from sagemaker.huggingface import HuggingFaceModel
import sagemaker

role = sagemaker.get_execution_role()
# Hub Model configuration. https://huggingface.co/models
hub = {
    'HF_MODEL_ID':'google/tapas-base-finetuned-wtq',
    'HF_TASK':'table-question-answering'
}

# create Hugging Face Model Class
huggingface_model = HuggingFaceModel(
    transformers_version='4.6.1',
    pytorch_version='1.7.1',
    py_version='py36',
    env=hub,
    role=role, 
)

# deploy model to SageMaker Inference
predictor = huggingface_model.deploy(
    initial_instance_count=1, # number of instances
    instance_type='ml.m5.xlarge' # ec2 instance type
)

predictor.predict({
    'inputs': {
        &quot;query&quot;: &quot;How many stars does the transformers repository have?&quot;,
        &quot;table&quot;: {
            &quot;Repository&quot;: [&quot;Transformers&quot;, &quot;Datasets&quot;, &quot;Tokenizers&quot;],
            &quot;Stars&quot;: [&quot;36542&quot;, &quot;4512&quot;, &quot;3934&quot;],
            &quot;Contributors&quot;: [&quot;651&quot;, &quot;77&quot;, &quot;34&quot;],
            &quot;Programming language&quot;: [
                &quot;Python&quot;,
                &quot;Python&quot;,
                &quot;Rust, Python and NodeJS&quot;,
            ],
        }
    }
})
</code></pre>
<p>The error comes when calling <code>predict()</code>:</p>
<pre><code>ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from model with message &quot;{
  &quot;code&quot;: 400,
  &quot;type&quot;: &quot;InternalServerException&quot;,
  &quot;message&quot;: &quot;\nTapasModel requires the torch-scatter library but it was not found in your environment. You can install it with pip as\nexplained here: https://github.com/rusty1s/pytorch_scatter.\n&quot;
}
</code></pre>
<p>I installed many versions of torch-scatter, but it's always the same, or even worse logs.</p>
<p>In this guide from HuggingFace says conda_pytorch_p36 kernel is needed. It's not working in any kernel, always because attached error log.</p>
<p>Other models are working properly, but this one is failing no matter what combination of versions.</p>",1,0,2021-10-20 12:11:44.200000 UTC,,2021-10-20 14:38:54.177000 UTC,1,pip|jupyter-notebook|pytorch|amazon-sagemaker|huggingface-transformers,178,2021-10-20 10:37:51.507000 UTC,2022-09-22 15:47:56.210000 UTC,,36,10,0,1,,,,,,['amazon-sagemaker']
How to fix trainserver empty server?,"<p>Im trying to install an allegroai trains-server on a k8s cluster.</p>
<p>I tried the following 3 methods</p>
<ul>
<li><a href=""https://allegro.ai/docs/deploying_trains/trains_server_linux_mac/"" rel=""nofollow noreferrer"">bare linux installtion</a></li>
<li><a href=""https://allegro.ai/docs/deploying_trains/trains_server_kubernetes/"" rel=""nofollow noreferrer"">k8s manifest installation</a></li>
<li><a href=""https://allegro.ai/docs/deploying_trains/trains_server_kubernetes_helm/"" rel=""nofollow noreferrer"">helm installation</a></li>
</ul>
<p>I followed the linux installation to the letter,
and in the k8s installations used the following command to access the exposed port of the trains webserver
kubectl port-forward -n trains svc/webserver-service 9999:80</p>
<p>In all three cases i manage to get to the server, but it looks empty and most operations fail.<a href=""https://i.stack.imgur.com/OMhYi.png"" rel=""nofollow noreferrer"">Here is a screen shot of what the webserver looks like</a>.</p>
<p>I tried doing all 3 multiple times from scratch and even rebuilt my k8s cluster but nothing works.</p>
<p>Does anyone know how to solve this?</p>",1,3,2020-11-03 16:27:07.883000 UTC,,2021-01-02 14:59:55.200000 UTC,2,trains|clearml,47,2020-11-03 16:20:18.717000 UTC,2021-09-14 21:30:14.477000 UTC,,21,0,0,1,,,,,,['clearml']
Vertex AI - Endpoint Call with JSON - Invalid JSON payload received,"<p>I successfully trained and deployed a Tensorflow Recommender model on Vertex AI.</p>
<p>Everything is online and to predict the output. In the notebook I do:</p>
<pre><code>loaded = tf.saved_model.load(path)
scores, titles = loaded([&quot;doctor&quot;])
</code></pre>
<p>That returns:</p>
<pre><code>Recommendations: [b'Nelly &amp; Monsieur Arnaud (1995)'
 b'Three Lives and Only One Death (1996)' b'Critical Care (1997)']
</code></pre>
<p>That is, the payload (input for the neural network) must be <code>[&quot;doctor&quot;]</code></p>
<p>Then I generate the JSON for payload (the error is here):</p>
<pre><code>!echo {&quot;\&quot;&quot;instances&quot;\&quot;&quot; : [{&quot;\&quot;&quot;input_1&quot;\&quot;&quot; : {[&quot;\&quot;&quot;doctor&quot;\&quot;&quot;]}}]} &gt; instances0.json
</code></pre>
<p>And submit to the endpoint:</p>
<pre><code>!curl -X POST  \
-H &quot;Authorization: Bearer $(gcloud auth print-access-token)&quot; \
-H &quot;Content-Type: application/json&quot; \
https://us-west1-aiplatform.googleapis.com/v1/projects/my_project/locations/us-west1/endpoints/123456789:predict \
-d @instances0.json &gt; results.json
</code></pre>
<p>... as seen here: <a href=""https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/master/notebooks/community/vertex_endpoints/tf_hub_obj_detection/deploy_tfhub_object_detection_on_vertex_endpoints.ipynb#scrollTo=35348dd21acd"" rel=""nofollow noreferrer"">https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/master/notebooks/community/vertex_endpoints/tf_hub_obj_detection/deploy_tfhub_object_detection_on_vertex_endpoints.ipynb#scrollTo=35348dd21acd</a></p>
<p>However, when I use this payload, I get error 400:</p>
<pre><code>code: 400
message: &quot;Invalid JSON payload received. Expected an object key or }. s&quot; : [{&quot;input_1&quot; : {[&quot;doctor&quot;]}}]} ^&quot;
status: &quot;INVALID_ARGUMENT&quot;
</code></pre>
<p>This below don't work either:</p>
<pre><code>!echo {&quot;inputs&quot;: {&quot;input_1&quot;: [&quot;doctor&quot;]}} &gt; instances0.json
</code></pre>
<p>Even with validated JSON Lint, it does not return the proper prediction.</p>
<p>In another Stackoverflow question is suggested to remove the &quot; \ &quot; in the payload, but this didn't work either.</p>
<p>Running:</p>
<pre><code>!saved_model_cli show --dir /home/jupyter/model --all
</code></pre>
<p>I get:</p>
<pre><code>MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:

signature_def['__saved_model_init_op']:
  The given SavedModel SignatureDef contains the following input(s):
  The given SavedModel SignatureDef contains the following output(s):
    outputs['__saved_model_init_op'] tensor_info:
        dtype: DT_INVALID
        shape: unknown_rank
        name: NoOp
  Method name is: 

signature_def['serving_default']:
  The given SavedModel SignatureDef contains the following input(s):
    inputs['input_1'] tensor_info:
        dtype: DT_STRING
        shape: (-1)
        name: serving_default_input_1:0
  The given SavedModel SignatureDef contains the following output(s):
    outputs['output_1'] tensor_info:
        dtype: DT_FLOAT
        shape: (-1, 10)
        name: StatefulPartitionedCall_1:0
    outputs['output_2'] tensor_info:
        dtype: DT_STRING
        shape: (-1, 10)
        name: StatefulPartitionedCall_1:1
  Method name is: tensorflow/serving/predict


Concrete Functions:
  Function Name: '__call__'
    Option #1
      Callable with:
        Argument #1
          input_1: TensorSpec(shape=(None,), dtype=tf.string, name='input_1')
        Argument #2
          DType: NoneType
          Value: None
        Argument #3
          DType: bool
          Value: True
    Option #2
      Callable with:
        Argument #1
          queries: TensorSpec(shape=(None,), dtype=tf.string, name='queries')
        Argument #2
          DType: NoneType
          Value: None
        Argument #3
          DType: bool
          Value: True
    Option #3
      Callable with:
        Argument #1
          input_1: TensorSpec(shape=(None,), dtype=tf.string, name='input_1')
        Argument #2
          DType: NoneType
          Value: None
        Argument #3
          DType: bool
          Value: False
    Option #4
      Callable with:
        Argument #1
          queries: TensorSpec(shape=(None,), dtype=tf.string, name='queries')
        Argument #2
          DType: NoneType
          Value: None
        Argument #3
          DType: bool
          Value: False

  Function Name: '_default_save_signature'
    Option #1
      Callable with:
        Argument #1
          input_1: TensorSpec(shape=(None,), dtype=tf.string, name='input_1')

  Function Name: 'call_and_return_all_conditional_losses'
    Option #1
      Callable with:
        Argument #1
          input_1: TensorSpec(shape=(None,), dtype=tf.string, name='input_1')
        Argument #2
          DType: NoneType
          Value: None
        Argument #3
          DType: bool
          Value: False
    Option #2
      Callable with:
        Argument #1
          queries: TensorSpec(shape=(None,), dtype=tf.string, name='queries')
        Argument #2
          DType: NoneType
          Value: None
        Argument #3
          DType: bool
          Value: True
    Option #3
      Callable with:
        Argument #1
          queries: TensorSpec(shape=(None,), dtype=tf.string, name='queries')
        Argument #2
          DType: NoneType
          Value: None
        Argument #3
          DType: bool
          Value: False
    Option #4
      Callable with:
        Argument #1
          input_1: TensorSpec(shape=(None,), dtype=tf.string, name='input_1')
        Argument #2
          DType: NoneType
          Value: None
        Argument #3
          DType: bool
          Value: True
</code></pre>
<p>The point is: I'm passing an array and I'm not sure if it must be in b64 format.</p>
<p>This Python code works, but returns a different result than expected:</p>
<pre><code>import tensorflow as tf
import base64
from google.protobuf import json_format
from google.protobuf.struct_pb2 import Value
import numpy as np
from google.cloud import aiplatform
import os
vertex_model = tf.saved_model.load(&quot;gs://bucket/model&quot;)

serving_input = list(
    vertex_model.signatures[&quot;serving_default&quot;].structured_input_signature[1].keys()
)[0]

print(&quot;Serving input :&quot;, serving_input)

aip_endpoint_name = (
    f&quot;projects/my-project/locations/us-west1/endpoints/12345567&quot;
)
endpoint = aiplatform.Endpoint(aip_endpoint_name)

def encode_input(input):
    return base64.b64encode(np.array(input)).decode(&quot;utf-8&quot;)

instances_list = [{serving_input: {&quot;b64&quot;: encode_input(np.array([&quot;doctor&quot;]))}}]
instances = [json_format.ParseDict(s, Value()) for s in instances_list]

results = endpoint.predict(instances=instances)
print(results.predictions[0][&quot;output_2&quot;])


['8 1/2 (1963)', 'Sword in the Stone, The (1963)', 'Much Ado About Nothing (1993)', 'Jumanji (1995)', 'As Good As It Gets (1997)', 'Age of Innocence, The (1993)', 'Double vie de Véronique, La (Double Life of Veronique, The) (1991)', 'Piano, The (1993)', 'Eat Drink Man Woman (1994)', 'Bullets Over Broadway (1994)']
</code></pre>
<p>Any ideas on how to fix / encode the payload ?</p>",1,0,2022-07-27 16:47:24.647000 UTC,,2022-08-02 04:53:07.720000 UTC,0,json|google-cloud-platform|google-cloud-vertex-ai|google-ai-platform,105,2016-09-29 20:35:09.097000 UTC,2022-09-25 02:50:38.173000 UTC,Brazil,4242,735,15,421,,,,,,['google-cloud-vertex-ai']
"SageMaker deploy error ""serve"" executable file not found in $PATH","<p>In Amazon SageMaker, I'm trying to deploy a custom created Docker container with a Scikit-Learn model, but deploying keeps giving errors.</p>
<p>These are my steps:</p>
<ul>
<li><p>On my local machine created a script (script.py) and splitted training and test data. The script contains a main section, accepts parameters 'output-train-dir', 'model-dir', 'train' and 'test', and contains the functions model_fn, input_fn, output_fn and predict_fn</p>
</li>
<li><p>Tested the script locally, which worked</p>
<ul>
<li>python script.py --train . --test . --model-dir .</li>
</ul>
</li>
<li><p>Created a Docker image based on the default Python image (Python 3.9) and push to Amazon ECR, below are the commands I've used</p>
<pre><code> &gt; docker pull python
 create Dockerfile, containing
    FROM python:3.9
    RUN pip3 install --no-cache scikit-learn numpy pandas joblib sagemaker-training
 &gt; docker build -t mymodel .
 &gt; aws ecr create-repository --repository-name mymodel
 &gt; docker tag 123456789012 123456789123.dkr.ecr.eu-central-1.amazonaws.com/mymodel
 &gt; docker push 123456789123.dkr.ecr.eu-central-1.amazonaws.com/mymodel
</code></pre>
</li>
<li><p>Uploaded the training and test data to s3 (mybucket)</p>
</li>
<li><p>Trained the script with local modus</p>
<pre><code> aws_sklearn = SKLearn(entry_point='script.py',
                       framework_version='0.23-1',
                       image_uri='123456789123.dkr.ecr.eu-central-1.amazonaws.com/mymodel',
                       instance_type='local',
                       role=role)
 aws_sklearn.fit({'train': mybucket_train_path, 'test': mybucket_test_path, 'model-dir': mybucket_model_path})
</code></pre>
</li>
</ul>
<p>which was successful</p>
<ul>
<li><p>Next I trained on AWS</p>
<pre><code>  aws_sklearn = SKLearn(entry_point='script.py',
                        framework_version='0.23-1',
                        image_uri='123456789123.dkr.ecr.eu-central-1.amazonaws.com/mymodel',
                        instance_type='ml.m4.xlarge',
                        role=role)
  aws_sklearn.fit({'train': mybucket_train_path, 'test': mybucket_test_path})
</code></pre>
</li>
</ul>
<p>which also was successful (however, providing the model-dir paramater gave errors, so I omitted it)</p>
<ul>
<li><p>deploying however gave an error:</p>
<pre><code>  aws_sklearn_predictor = aws_sklearn.deploy(instance_type='ml.t2.medium',
                                             initial_instance_count=1)
</code></pre>
</li>
</ul>
<p>Error message:</p>
<blockquote>
<p>UnexpectedStatusException: Error hosting endpoint
mymodel-2021-01-24-12-52-02-790: Failed. Reason:  The primary
container for production variant AllTraffic did not pass the ping
health check. Please check CloudWatch logs for this endpoint..</p>
</blockquote>
<p>And Cloudwatch said:</p>
<blockquote>
<p>AWS sagemaker exec: &quot;serve&quot;: executable file not found in $PATH</p>
</blockquote>
<p>I somewhere read that I should add RUN chmod +x /opt/program/serve to the Dockerfile, but in my local image, there is no serve file present, this is something that SageMaker creates, right ?</p>
<p>How or where should I add serve to the $PATH environment variable or grant execute rights to the serve script ?</p>",1,0,2021-01-25 10:53:47.933000 UTC,1.0,,2,docker|deployment|amazon-sagemaker,1814,2016-09-04 06:28:40.890000 UTC,2022-08-13 10:06:28.500000 UTC,"Amersfoort, Nederland",424,15,3,21,,,,,,['amazon-sagemaker']
Error in converting SageMaker XGBoost model to ONNX model,"<p>I'm trying to convert a SageMaker XGBoost model to ONNX, in order to use the ONNX model in .Net application using ML.NET. I've tried to convert the model using <code>winmltools</code> and <code>onnxmltools</code> but both tools are returned similar error.</p>

<p>There is a good resource to use machine learning in business area. I've tried <a href=""http://fasttrackteam.com/using-machine-learning-to-improve-sales-a-simple-example.aspx"" rel=""noreferrer"">Using Machine Learning to Improve Sales</a> in SageMaker to create the model and then convert the model to ONNX model. The example is working well in SageMaker.</p>

<p><a href=""https://i.stack.imgur.com/4ftF1.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/4ftF1.png"" alt=""enter image description here""></a></p>

<p>After running the example, I got a model and the type of the model is <code>sagemaker.estimator.Estimator</code>. I've tried to convert the model by using <code>winmltools</code> and <code>onnxmltools</code>. But both are returned same error. </p>

<p><code>ValueError: No proper operator name found for '&lt;class 'sagemaker.estimator.Estimator'&gt;'</code></p>

<p><a href=""https://i.stack.imgur.com/vQ7kP.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/vQ7kP.png"" alt=""enter image description here""></a></p>

<p>I've tried to follow <a href=""https://docs.microsoft.com/en-us/windows/ai/windows-ml/convert-model-winmltools"" rel=""noreferrer"">Convert ML models to ONNX with WinMLTools</a> and <a href=""https://github.com/onnx/onnxmltools"" rel=""noreferrer"">ONNXMLTools enables conversion of models to ONNX</a> to convert the SageMaker model to ONNX model.</p>

<p>After that, I used <code>xgb.create_model()</code> command to create SageMaker model. Then used the tools to convert the model to ONNX. but no luck. I got same error this time. Just the model is different.</p>

<p><code>ValueError: No proper operator name found for '&lt;class 'sagemaker.model.Model'&gt;'</code></p>

<p><a href=""https://i.stack.imgur.com/QuAiX.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/QuAiX.png"" alt=""enter image description here""></a></p>

<p>Then I load the model using <code>pickle</code> and tried to convert the model. I got same error, just the model is different.</p>

<p><code>ValueError: No proper operator name found for '&lt;class 'xgboost.core.Booster'&gt;'</code></p>

<p><a href=""https://i.stack.imgur.com/WV2Xy.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/WV2Xy.png"" alt=""enter image description here""></a></p>

<p>At this moment, I have no idea about the issues. How should I solve the issues. I've attached the <a href=""https://1drv.ms/u/s!An4yDE4xkdIjhOMgu_vBDiYa8VPrgQ?e=r6muMO"" rel=""noreferrer"">Improve Sales Classification to ONNX notebook file</a> for reference.
Could you please take a look at the issues and let me know a way to solve the issues?
Thanks in advance!</p>",0,4,2019-11-22 19:42:59.947000 UTC,1.0,,5,python|machine-learning|xgboost|amazon-sagemaker|onnx,489,2012-06-21 18:23:52.600000 UTC,2022-09-24 23:28:00.993000 UTC,Bangladesh,3426,1775,134,593,,,,,,['amazon-sagemaker']
Tensorflow Serving in Amazon SageMaker,"<p>I am facing an issue with serving tensorflow models on AWS SageMaker. I have trained the model outside of the SageMaker environment, now I have a savedmodel.pb file and I need to deploy it on a SageMaker endpoint. So I simply zipped the model file and uploaded it to an S3 bucket.
Now, when trying to create an endpoint, I get the following error in my Cloudwatch log:</p>

<blockquote>
  <p>tensorflow_serving/sources/storage_path/file_system_storage_path_source.cc:369]
  FileSystemStoragePathSource encountered a file-system access error:
  Could not find base path /opt/ml/model/export/Servo for servable
  generic_model</p>
</blockquote>

<p>I believe SageMaker is looking for the tar.gz to follow a particular directory structure. However, all I have is a .pb file.</p>",1,5,2019-02-11 12:32:24.507000 UTC,1.0,2019-02-11 12:33:44.130000 UTC,2,python|amazon-web-services|tensorflow|tensorflow-serving|amazon-sagemaker,1039,2016-03-20 15:22:47.247000 UTC,2020-10-11 09:07:12.820000 UTC,Sharjah - United Arab Emirates,21,0,0,7,,,,,,['amazon-sagemaker']
ValueError: Error hosting endpoint. The primary container for production variant AllTraffic did not pass the ping health check,"<p>I'm trying to deploy an SKlearn model on Amazon Sagemaker, and am working through the example provided in their documentation and am getting the above error when I deploy the model.  </p>

<p>I'm following the instructions provided in <a href=""https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/scikit_learn_iris/Scikit-learn%20Estimator%20Example%20With%20Batch%20Transform.ipynb"" rel=""nofollow noreferrer"">this notebook</a>, and so far have just copied and pasted the code that they have.</p>

<p>Right now, this is the exact code I have in my jupyter notebook:</p>

<pre><code># S3 prefix
prefix = 'Scikit-iris'

import sagemaker
from sagemaker import get_execution_role

sagemaker_session = sagemaker.Session()

# Get a SageMaker-compatible role used by this Notebook Instance.
role = get_execution_role()

import numpy as np
import os
from sklearn import datasets

# Load Iris dataset, then join labels and features
iris = datasets.load_iris()
joined_iris = np.insert(iris.data, 0, iris.target, axis=1)

# Create directory and write csv
os.makedirs('./iris', exist_ok=True)
np.savetxt('./iris/iris.csv', joined_iris, delimiter=',', fmt='%1.1f, %1.3f, 
%1.3f, %1.3f, %1.3f')

WORK_DIRECTORY = 'data'

train_input = sagemaker_session.upload_data(WORK_DIRECTORY, key_prefix=""{}/{}"".format(prefix, WORK_DIRECTORY) )

from sagemaker.sklearn.estimator import SKLearn

script_path = 'scikit_learn_iris.py'

sklearn = SKLearn(
  entry_point=script_path,
  train_instance_type=""ml.c4.xlarge"",
  role=role,
  sagemaker_session=sagemaker_session,
  framework_version='0.20.0',
  hyperparameters={'max_leaf_nodes': 30})

sklearn.fit({'train': train_input})

sklearn.deploy(instance_type='ml.m4.xlarge',
                                 initial_instance_count=1)
</code></pre>

<p>And at that point I get the error message.</p>

<p>The contents of <code>'scikit_learn_iris.py'</code> look like this:</p>

<pre><code>import argparse
import pandas as pd
import os
import numpy as np

from sklearn import tree
from sklearn.externals import joblib

if __name__ == '__main__':
    parser = argparse.ArgumentParser()

# Hyperparameters are described here. In this simple example we are just including one hyperparameter.
parser.add_argument('--max_leaf_nodes', type=int, default=-1)

# SageMaker specific arguments. Defaults are set in the environment variables.
parser.add_argument('--output-data-dir', type=str, default=os.environ['SM_OUTPUT_DATA_DIR'])
parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])
parser.add_argument('--train', type=str, default=os.environ['SM_CHANNEL_TRAIN'])

args = parser.parse_args()

# Take the set of files and read them all into a single pandas dataframe
input_files = [ os.path.join(args.train, file) for file in os.listdir(args.train) ]
if len(input_files) == 0:
    raise ValueError(('There are no files in {}.\n' +
                      'This usually indicates that the channel ({}) was incorrectly specified,\n' +
                      'the data specification in S3 was incorrectly specified or the role specified\n' +
                      'does not have permission to access the data.').format(args.train, ""train""))
raw_data = [ pd.read_csv(file, header=None, engine=""python"") for file in input_files ]
train_data = pd.concat(raw_data)

# labels are in the first column
train_y = train_data.ix[:,0].astype(np.int)
train_X = train_data.ix[:,1:]

# We determine the number of leaf nodes using the hyper-parameter above.
max_leaf_nodes = args.max_leaf_nodes

# Now use scikit-learn's decision tree classifier to train the model.
clf = tree.DecisionTreeClassifier(max_leaf_nodes=max_leaf_nodes)
clf = clf.fit(train_X, train_y)

# Save the decision tree model.
joblib.dump(clf, os.path.join(args.model_dir, ""model.joblib""))
</code></pre>

<p>My cloudwatch logs look like this:</p>

<p><a href=""https://i.stack.imgur.com/HGWN2.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HGWN2.jpg"" alt=""enter image description here""></a></p>",1,0,2019-06-21 15:21:49.043000 UTC,,,1,python|scikit-learn|amazon-sagemaker,3261,2014-09-15 23:32:32.337000 UTC,2022-09-23 22:20:44.790000 UTC,"New York, NY, United States",3257,451,0,319,,,,,,['amazon-sagemaker']
Why does SageMaker PyTorch DDP init times out on SageMaker?,"<p>I'm using PyTorch DDP on SageMaker PyTorch Training DLC 1.8.1 The code seems properly DDP-formatted. I'm using instance_count = 2, and launching <code>torch.distributed.launch</code> and I believe the ranks and world size are properly set however the <code>dist.init_process_group</code> waits and times out</p>
<pre><code>RuntimeError: Timed out initializing process group in store based barrier on rank: 0, for key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)
</code></pre>
<p>What could go wrong? machines not networked together?</p>",2,0,2022-09-09 19:51:40.293000 UTC,,,0,pytorch|amazon-sagemaker|distributed-training,27,2022-09-08 07:14:26.503000 UTC,2022-09-23 21:03:19.637000 UTC,,48,2,0,6,,,,,,['amazon-sagemaker']
Azure Machine Learning - Empty score results,"<p>I've trained a model, the test results on test-set are okay.
Now I have saved the model as 'Trained model' and made a new experiment into a new dataset, for making predictions where I don't have the actual value's. </p>

<p>Normally, the trained model gives me a scored label result per instance. 
But now, the scored label results are empty. Also when I convert the score results to CSV the scored labels column is empty.</p>

<p>Even stranger, when I take a look at the Statistics of the score Visualize tab, I DO see the statistics of the scored values. But no actual scored values... </p>

<p><a href=""https://i.stack.imgur.com/R0cGb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/R0cGb.png"" alt=""Screenshot of the empty score results""></a></p>

<p>Is this a bug? Or am I forgetting something important? Whats going on ;) ?</p>",2,4,2016-03-23 10:09:09.117000 UTC,,2016-03-23 12:39:41.830000 UTC,4,azure|machine-learning|azure-machine-learning-studio,1731,2015-04-29 10:35:22.393000 UTC,2016-03-30 11:55:43.320000 UTC,,41,0,0,1,,,,,,['azure-machine-learning-studio']
How to load an external pickle file contents as a data frame in azure ml studio in the 'execute python script' section?,"<p>This is script which I am using:</p>

<pre><code> import warnings
 warnings.filterwarnings('ignore') 
 import pandas as pd
 import sys
 import pickle
 def azureml_main(dataframe1 = None, dataframe2 = None):
 sys.path.append('.\\Script Bundle')
 dataframe1 = pickle.load(open(r'/Script Bundle/descript.pkl', 'rb'))
 return dataframe1,
</code></pre>

<p>but when I execute it , getting below error </p>

<pre><code> FileNotFoundError: [Errno 2] No such file or directory: '/Script 
 Bundle/descript.pkl'
 Process returned with non-zero exit code 1
</code></pre>",1,1,2019-03-08 10:03:59.273000 UTC,,2019-03-08 13:12:33.543000 UTC,1,python|azure|azure-machine-learning-studio,1085,2019-02-11 11:39:40.047000 UTC,2020-01-02 09:14:20.780000 UTC,"Bengaluru, Karnataka, India",51,0,0,9,,,,,,['azure-machine-learning-studio']
Register model from Azure Machine Learning run without downloading to local file,"<p>A model was trained on a remote compute using azureml.core Experiment as follows:</p>
<pre><code>experiment = Experiment(ws, name=experiment_name)
src = ScriptRunConfig(&lt;...&gt;)
run = experiment.submit(src)
run.wait_for_completion(show_output=True)
</code></pre>
<p>How can a model trained in this run be registered with Azure Machine Learning workspace without being downloaded to a local file first?</p>",1,0,2020-12-02 18:41:36.993000 UTC,1.0,,4,azure-machine-learning-studio|azure-machine-learning-service,1141,2020-12-02 18:29:10.397000 UTC,2021-05-18 19:31:00.157000 UTC,"Bellevue, WA, USA",81,4,0,6,,,,,,"['azure-machine-learning-studio', 'azure-machine-learning-service']"
MLFlow: how to get additional methods from a loaded model?,"<p><strong>Use case:</strong></p>
<p>A <a href=""https://www.mlflow.org/docs/latest/python_api/mlflow.pyfunc.html#mlflow.pyfunc.PyFuncModel"" rel=""nofollow noreferrer""><code>mlflow.pyfunc.PyFuncModel</code></a> is defined with some more utilities methods in order to provide a way of parsing its prediction result to different formats.</p>
<p><strong>After the model is loaded from registry, is there a way to access those methods?</strong></p>
<p><strong>A contrived example:</strong></p>
<p>A <code>mlflow.pyfunc.PyFuncModel</code> model defining additional methods:</p>
<pre class=""lang-py prettyprint-override""><code>class MyModel(mlflow.pyfunc.PythonModel):
    def predict(self, context, model_input):
        prediction = # do some prediction
        return prediction

    @staticmethod
    def parse_prediction_to_format_x(prediction):
        prediction_formatted = # do some parsing
        return prediction_formatted

    def parse_prediction_to_format_y(self, prediction):
        prediction_formatted = # do some parsing
        return prediction_formatted
</code></pre>
<p>Note: I added one static and one non static, because both use cases are relevant.</p>
<p>Now, some other system goes to MLFlow Registry and loads the model from there:</p>
<pre class=""lang-py prettyprint-override""><code>        loaded_model = mlflow.pyfunc.load_model(
            model_uri=saved_model_path.absolute().as_uri()
        )
</code></pre>
<p>This system, which naturally does not hold the model source code, but the registry path to load it from there, wants to use the additional methods above.
It can use predict, since it is part of all pyfunc models:</p>
<pre class=""lang-py prettyprint-override""><code>predicted = loaded_model.predict(input_data)
</code></pre>
<p><strong>But how can this system access helper methods in the model class (static or instance methods)?</strong></p>
<pre class=""lang-py prettyprint-override""><code>predicted = loaded_model.predict(input_data)

# pseudo code:
predicted_and_formated = loaded_model.parse_prediction_to_format_y(predicted)
</code></pre>
<p>Thank you.</p>",0,0,2022-09-15 10:04:38.537000 UTC,,,0,python|databricks|mlflow,15,2020-02-13 21:23:52.807000 UTC,2022-09-22 23:27:25.757000 UTC,,477,59,0,56,,,,,,['mlflow']
How to make predictions using a model that requires an input shape with more than two dimensions using MLflow?,"<p>I'm trying to implement a tensorflow (keras) based model into mlflow while learning how it works and if it suite our needs. I'm trying to implement the Fashion MNIST example from tensorflow website <a href=""https://www.tensorflow.org/tutorials/keras/classification?hl=it"" rel=""nofollow noreferrer"">Here the link</a></p>

<p>I was able to train and to log the model successfully into mlflow using this code:</p>

<pre><code>import mlflow
import mlflow.tensorflow
import mlflow.keras

# TensorFlow and tf.keras
import tensorflow as tf
from tensorflow import keras

# Helper libraries
import numpy as np
import matplotlib.pyplot as plt

print(tf.__version__)

fashion_mnist = keras.datasets.fashion_mnist

(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()

class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
           'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']

train_images = train_images / 255.0

test_images = test_images / 255.0

model = keras.Sequential([
    keras.layers.Flatten(input_shape=(28, 28)),
    keras.layers.Dense(128, activation='relu'),
    keras.layers.Dense(10, activation='softmax')
])

model.compile(optimizer='adam',
          loss='sparse_categorical_crossentropy',
          metrics=['accuracy'])

if __name__ == ""__main__"":

    model.fit(train_images, train_labels, epochs=10)
    test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)
    print('\nTest accuracy:', test_acc)

    mlflow.log_metric(""validation accuracy"", float(test_acc))
    mlflow.log_metric(""validation loss"", float(test_loss))
    mlflow.keras.log_model(model, 
                        ""model"", 
                        registered_model_name = ""Fashion MNIST"")
</code></pre>

<p>Then I'm now serving it with the models serve subcommand</p>

<pre><code>$ mlflow models serve -m [model_path_here] -p 1234
</code></pre>

<p>The problem is that I'm not able to make predictions:</p>

<pre><code>fashion_mnist = keras.datasets.fashion_mnist
(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()
train_images = train_images / 255.0
test_images = test_images / 255.0
labels = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
           'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']

url = ""http://127.0.0.1:1234/invocations""

to_predict = test_images[0]

data = {
    ""data"": [to_predict.tolist()]
}
headers = {'Content-type': 'application/json', 'Accept': 'text/plain'}
r = requests.post(url, data=json.dumps(data), headers=headers)
res = r.json()
</code></pre>

<p>I'm getting this error:</p>

<pre><code>{'error_code': 'BAD_REQUEST', 'message': 'Encountered an unexpected error while evaluating the model. Verify that the serialized input Dataframe is compatible with the model for inference.', 'stack_trace': 'Traceback (most recent call last):\n  File ""/home/ferama/.local/lib/python3.6/site-packages/mlflow/pyfunc/scoring_server/__init__.py"", line 196, in transformation\n    raw_predictions = model.predict(data)\n  File ""/home/ferama/.local/lib/python3.6/site-packages/mlflow/keras.py"", line 298, in predict\n    predicted = pd.DataFrame(self.keras_model.predict(dataframe))\n  File ""/home/ferama/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py"", line 909, in predict\n    use_multiprocessing=use_multiprocessing)\n  File ""/home/ferama/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_arrays.py"", line 715, in predict\n    x, check_steps=True, steps_name=\'steps\', steps=steps)\n  File ""/home/ferama/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py"", line 2472, in _standardize_user_data\n    exception_prefix=\'input\')\n  File ""/home/ferama/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_utils.py"", line 564, in standardize_input_data\n    \'with shape \' + str(data_shape))\nValueError: Error when checking input: expected flatten_input to have 3 dimensions, but got array with shape (1, 28)\n'}
</code></pre>

<p>That code above worked fine with a one dimension model</p>

<p>The error seems to me related to the fact that a pandas DataFrame is a two dimensional data structure and the model instead requires a three dimensional input.</p>

<p>The latest words from the error ""...but got array with shape (1, 28)"". The input shape should be (1, 28, 28) instead</p>

<p>There is a way to use this kind of models with mlflow? There is a way to serialize and send numpy arrays directly as input instead of pandas dataframes?</p>",1,0,2019-11-18 15:28:15.563000 UTC,1.0,,3,python|tensorflow|keras|mlflow,1221,2010-12-30 14:55:10.407000 UTC,2022-01-08 07:38:22.447000 UTC,,503,9,0,35,,,,,,['mlflow']
How to Set Java Home for Notebook in SageMaker,"<p>So it seems that I have Java installed after running the below line in the SageMaker Notebook Terminal:</p>
<pre><code>bash-4.2$ sudo yum install java-1.8.0-openjdk 
</code></pre>
<p>In the terminal I write the following to confirm:</p>
<pre><code>bash-4.2$ java -version 
java version &quot;1.7.0_261&quot; OpenJDK Runtime Environment (amzn-2.6.22.1.83.amzn1-x86_64 u261-b02) OpenJDK 64-Bit Server VM (build 24.261-b02, mixed mode)
</code></pre>
<p>In my notebook I have the following Lines of code:</p>
<pre><code>import tabula

tabula.environment_info()
</code></pre>
<p>The notebook results in an error with:</p>
<pre><code>java -version` faild. `java` command is not found from this Pythonprocess. Please ensure Java is installed and PATH is set for `java`
</code></pre>
<p>Yet, in the terminal I see this:</p>
<pre><code>bash-4.2$ java -version 
java version &quot;1.7.0_261&quot; OpenJDK Runtime Environment (amzn-2.6.22.1.83.amzn1-x86_64 u261-b02) OpenJDK 64-Bit Server VM (build 24.261-b02, mixed mode)
</code></pre>
<p>I definitely have a java environment. How can I set my notebook to find this?</p>",1,0,2021-10-15 15:36:22.323000 UTC,0.0,,1,java|amazon-web-services|java-8|jupyter-notebook|amazon-sagemaker,477,2021-03-12 13:24:19.547000 UTC,2022-05-25 02:11:47.037000 UTC,United States,141,4,0,8,,,,,,['amazon-sagemaker']
lambda endpoint prediction failed with error can't convert string to float,"<p>I am facing the issue in getting the prediction from sagemaker xgboost endpoint. When I feed the transformed data in endpoint like: </p>

<pre><code>response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME,ContentType='text/csv',Body = csv_buffer.getvalue())
</code></pre>

<p>throwing the error : </p>

<blockquote>
  <p>can not convert string to float. </p>
</blockquote>

<p>Could you please help me out what I am missing. </p>",0,0,2018-11-21 09:35:50.100000 UTC,,2018-11-21 10:04:13.797000 UTC,1,aws-lambda|amazon-sagemaker,93,2018-11-21 08:21:26.503000 UTC,2019-12-19 11:25:27.570000 UTC,,11,0,0,3,,,,,,['amazon-sagemaker']
Amazon Sagemaker open json from S3 bucket,"<p>I created a S3 bucket and placed both a <code>data.csv</code> and a <code>data.json</code> file inside it. I then created a Sagemaker notebook and specified this S3 bucket in the IAM role.  </p>

<p>This now works from inside the notebook:</p>

<pre><code>import pandas as pd
from sagemaker import get_execution_role

bucket='my-sagemaker-bucket'
data_key = 'data.csv'
data_location = 's3://{}/{}'.format(bucket, data_key)
data = pd.read_csv(data_location)
</code></pre>

<p>But this errors saying file doesn't exist:</p>

<pre><code>import json
from sagemaker import get_execution_role

bucket='my-sagemaker-bucket'
data_key = 'data.json'
data_location = 's3://{}/{}'.format(bucket, data_key)
data = json.load(open(data_location))
</code></pre>

<p>Anyone know why I can read the csv but not the json? I also can't <code>shutil.copy</code> the csv to the notebook's current working directory (also says file doesn't exist). I'm not very well versed with S3 buckets or Sagemaker, so not sure if this is a permissions/policy issue or something else.</p>",2,0,2018-11-04 22:27:10.990000 UTC,,2018-11-04 22:32:48.917000 UTC,0,python|json|amazon-s3|amazon-sagemaker,2805,2013-02-20 05:47:52.693000 UTC,2022-09-23 20:45:28.400000 UTC,NYC,6281,430,17,958,,,,,,['amazon-sagemaker']
How to run a bigger batch with AWS SageMaker Batch Transform,"<p>I created an XGBoost model with AWS SageMaker. Now I'm trying to use it through Batch Transform Job, and it's all going pretty well for small batches.</p>

<p>However, there's a slightly bigger batch of 600.000 rows in a ~16MB file and I can't manage to run it in one go. I tried two things:</p>

<p>1.</p>

<p>Setting 'Max payload size' of the Transform job to its maximum (100 MB):</p>

<pre><code>transformer = sagemaker.transformer.Transformer(
    model_name = config.model_name,
    instance_count = config.inference_instance_count,
    instance_type = config.inference_instance_type,
    output_path = ""s3://{}/{}"".format(config.bucket, config.s3_inference_output_folder),
    sagemaker_session = sagemaker_session,
    base_transform_job_name = config.inference_job_prefix,
    max_payload = 100
    )
</code></pre>

<p>However, I still get an error (through console CloudWatch logs):</p>

<pre><code>413 Request Entity Too Large
The data value transmitted exceeds the capacity limit.
</code></pre>

<p>2.</p>

<p>Setting max_payload to 0, which, by specification, Amazon SageMaker should interpret as no limit on the payload size.</p>

<p>In that case the job finishes successfully, but the output file is empty (0 bytes).</p>

<p>Any ideas either what I'm doing wrong, or how to run a bigger batch?</p>",3,1,2018-10-04 14:20:57.747000 UTC,1.0,,2,xgboost|amazon-sagemaker,6566,2012-12-05 14:29:32.683000 UTC,2022-09-23 00:30:58.527000 UTC,,590,92,1,37,,,,,,['amazon-sagemaker']
Launch multiple times the same instance Amazon Sagemaker Studio,"<p>On Amazon Sagemaker Studio, I would like to be able to launch multiple time the same instance.</p>
<p>So far, I can start only one &quot;ml.t3.medium&quot; instance for example; I would like to launch two separate instance of &quot;ml.t3.medium&quot; in the same jupyterlab. Is it possible?</p>
<p>Thanks</p>
<p><a href=""https://i.stack.imgur.com/UjsNd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UjsNd.png"" alt=""screenshot"" /></a></p>",1,0,2021-12-28 07:54:31.583000 UTC,,2021-12-28 09:34:07.847000 UTC,3,python|jupyter-notebook|amazon|amazon-sagemaker,338,2017-12-13 12:56:50.217000 UTC,2022-09-22 06:20:22.683000 UTC,,409,25,0,54,,,,,,['amazon-sagemaker']
Azure command line 'az ml' on Mac not working,"<p>I am trying to run any <code>az ml</code> function and am running into this error:</p>

<pre><code>$ az ml -h

az: error: argument _command_package: invalid choice: ml
</code></pre>

<p>I do not see any suggestion so far that gets ml to be a supported function for azure-cli. Looking if anyone can help.</p>",1,0,2017-12-10 01:46:54.807000 UTC,,2018-09-07 03:37:12.210000 UTC,3,macos|azure|azure-machine-learning-studio|azure-cli,1311,2015-09-24 14:00:31.900000 UTC,2019-01-10 00:03:00.247000 UTC,,153,7,0,47,,,,,,['azure-machine-learning-studio']
Wandb first run start time is delayed,"<p>I wanted to compare the execution speeds of three data types. The runs were organized in sequence of <code>Original</code>, <code>DictList</code>, <code>DataFrame</code>. So <code>Original</code> was the first run. The x-axis is set as <code>Relative Time (Process)</code></p>
<p>Problem is that each runs starting time is all different! How can I make sure that they all start at time 0?</p>
<p><a href=""https://i.stack.imgur.com/PotUf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PotUf.png"" alt=""Loss Graph"" /></a></p>
<p><a href=""https://i.stack.imgur.com/rwBf4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rwBf4.png"" alt=""Elapsed Steps Graph"" /></a></p>
<p>I can't post the entire code. I'll post every point where <code>wandb</code> API is called.</p>
<pre><code>import wandb
if __name__ == &quot;__main__&quot;:
    wandb.login()
    datatypes = {&quot;Original&quot;: Original, &quot;DictList&quot;: DictList, &quot;DataFrame&quot;: DataFrame}
    for type_name, datatype in datatypes.items():
        wandb_run = wandb.init(project=&quot;Compare&quot;, name=type_name, reinit=True)
        with wandb_run:
            # Initialize RL training session
            storage = datatype()
            # Run RL training session
            # Log (loss, elapsed_steps, etc.)    
</code></pre>",0,0,2022-07-29 11:12:40.443000 UTC,,,0,pytorch|wandb,13,2017-08-16 10:35:04.843000 UTC,2022-09-25 02:02:10.100000 UTC,South Korea,1248,800,23,221,,,,,,['wandb']
Is it possible to use Sagemaker Notebooks with a Docker image as your environment?,"<p>I'm currently developing a system that some private libraries. I'm developing in local mode and then when I need to process something specific I use <em>Sagemaker Processing Jobs</em>. The thing is that in order to speed up the process it would be nice to have the possibility of developing everything in a cloud environment.</p>
<blockquote>
<p>I'm wondering if is possible to use the same <em>Docker image</em> that I use
for batch processing (the one that I use for Sagemaker Processing Job)
in my <em>Sagemaker Jupyter Notebooks</em> of my cloud environment?</p>
</blockquote>
<p>The main problem here is that every time that I work in my cloud Notebooks I have to deal with dependencies conflicts and etc. Using a Docker image would avoid this, and will also allow to each member of the team use the same image to develop in the cloud without having to deal with these kind of conflicts.</p>",1,0,2022-05-04 12:36:17.127000 UTC,,,0,amazon-web-services|docker|cloud|amazon-sagemaker,233,2018-02-24 22:32:25.083000 UTC,2022-09-23 08:27:16.670000 UTC,,43,15,0,2,,,,,,['amazon-sagemaker']
Sagemaker Semantic Segmentation: iou and pixel accuracy per class,"<p>I am doing some semantic segemnetation work on AWS sagemaker. My output is the mean iou and pixel accuracy of all my classes but i need more granularity so require the iou and pixel accuracy for each class. This is to see what classes need to be improved.</p>

<p>I imagine its possible but how? </p>

<p>I have had a look at these similar questions but no solution as yet.</p>

<p><a href=""https://stackoverflow.com/questions/54837529/tensorflow-iou-per-class"">Tensorflow : IOU per class</a></p>

<p><a href=""https://stackoverflow.com/questions/44041096/iou-for-semantic-segmentation-implementation-in-python-caffe-per-class"">IoU for semantic segmentation implementation in python/caffe per class</a></p>",1,0,2019-06-18 10:40:02.057000 UTC,,,1,python|amazon-web-services|amazon-sagemaker|semantic-segmentation,393,2019-05-22 09:45:28.153000 UTC,2022-09-02 09:43:13.340000 UTC,"London, UK",79,6,0,16,,,,,,['amazon-sagemaker']
"Spark Model serving: How to compute the state just once, then keep the state to answer all real-time requests? (recommendation engine)","<p>I try to implement a recommendation engine using Kafka to collect real-time click data and then process it using Spark Structured Streaming. My problem is about how to server predictions in near real-time using this streaming dataframe.</p>
<p>What works fine is: collecting click data, sending to Kafka, subscribing from Spark, using Strcutured Streaming to compute a dataframe which describes the 'state of a visitor'. Now having this streaming dataframe, there are just few lines of code (business logic) telling which is the best recommendation.</p>
<p>Now my problem is: how do I put this in production. I could create a mlflow.pyfunc model. But this would not contain the 'state of a visitor' dataframe. When looking at model serving frameworks, I understand that every inference request would create an independent runtime which would have to do the whole data pipeline again.</p>
<p>My idea would be to have 1 Spark instance which would:</p>
<ol>
<li>create this streaming dataframe</li>
<li>wait for incoming request and answer those by using the dataframe from (1.)</li>
</ol>
<p>Is this a reasonable approach? If yes: How do I set this up? If no: What is the preferred way to do real-time recommendations?</p>",0,0,2022-05-19 11:42:39.700000 UTC,,,0,apache-spark|apache-kafka|spark-streaming|recommendation-engine|mlflow,28,2016-01-08 13:26:58.953000 UTC,2022-08-06 19:01:56.227000 UTC,,39,2,0,7,,,,,,['mlflow']
How to change Sklearn flavors version in mlflow on azure machine learning?,"<p>I need to change the flavors &quot;sklearn_version&quot; in mlflow from &quot;0.22.1&quot; to &quot;1.0.0&quot; on azure machine learning when I log my trained model, since this model will be incompatible with the sklearn version that I am using for deployment during inference. I could change the version of sklearn in conda.yml file by setting &quot;conda_env&quot; in</p>
<p><code>mlflow.sklearn.log_model(conda_env= 'my_env')</code></p>
<p><a href=""https://i.stack.imgur.com/URygm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/URygm.png"" alt=""enter image description here"" /></a></p>
<p>here is the screen shot of requirements.txt
<a href=""https://i.stack.imgur.com/8us2o.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8us2o.png"" alt=""enter image description here"" /></a></p>
<p>however, sklearn version under flavors in MLmodel file remains unchanged and that is the file that causes problem:</p>
<p><a href=""https://i.stack.imgur.com/XfWvJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XfWvJ.png"" alt=""enter image description here"" /></a></p>
<p>and here is script that I use to create this mlflow experiment in azure machine learning notebooks.</p>
<pre><code>import mlflow
from sklearn.tree import DecisionTreeRegressor

from azureml.core import Workspace
from azureml.core.model import Model
from azureml.mlflow import register_model


def run_model(ws, experiment_name, run_name, x_train, y_train):
    
    # set up MLflow to track the metrics
    mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())
    mlflow.set_experiment(experiment_name)  
    
    with mlflow.start_run(run_name=run_name) as run:
        
        # fit model
        regression_model = DecisionTreeRegressor()
        regression_model.fit(x_train, y_train)
    
        # log training score 
        training_score = regression_model.score(x_train, y_train)
        mlflow.log_metric(&quot;Training score&quot;, training_score)

        my_conda_env = {
                    &quot;name&quot;: &quot;mlflow-env&quot;,
                    &quot;channels&quot;: [&quot;conda-forge&quot;],
                    &quot;dependencies&quot;: [
                        &quot;python=3.8.5&quot;,
                        {
                            &quot;pip&quot;: [
                                &quot;pip&quot;,
                                &quot;scikit-learn~=1.0.0&quot;,
                                &quot;uuid==1.30&quot;,
                                &quot;lz4==4.0.0&quot;,
                                &quot;psutil==5.9.0&quot;,
                                &quot;cloudpickle==1.6.0&quot;,
                                &quot;mlflow&quot;,
                            ],
                        },
                    ],
                }

        
        # register the model
        mlflow.sklearn.log_model(regression_model, &quot;model&quot;, conda_env=my_conda_env)

    model_uri = f&quot;runs:/{run.info.run_id}/model&quot;
    model = mlflow.register_model(model_uri, &quot;sklearn_regression_model&quot;)

if __name__ == '__main__':

    # connect to your workspace
    ws = Workspace.from_config()

    # create experiment and start logging to a new run in the experiment
    experiment_name = &quot;exp_name&quot;

    # mlflow run name
    run_name= '1234'

  
    # get train data
    x_train, y_train  = get_train_data()
    
    run_model(ws, experiment_name, run_name, x_train, y_train)
</code></pre>
<p>Any idea how can change the flavor sklearn version in MLmodel file from <strong>&quot;0.22.1&quot;</strong> to <strong>&quot;1.0.0&quot;</strong> in my script?</p>
<p>With many thanks in advance!</p>",2,0,2022-06-20 08:38:43.913000 UTC,,2022-06-20 13:14:39.310000 UTC,0,python|azure|scikit-learn|azure-machine-learning-service|mlflow,102,2017-01-16 23:04:42.440000 UTC,2022-09-23 06:48:12.703000 UTC,,83,3,0,48,,,,,,"['mlflow', 'azure-machine-learning-service']"
Sagemaker Pipelines with Autopilot Step,"<p>I'm looking to combine the Autopilot AutoML training and deployment step into a wider Pipeline using Sagemaker.</p>
<p>I can't find anything in the documentation around using AutoML with Pipelines.</p>
<p>Does anyone have any experience with trying something like this with Sagemaker?</p>",1,0,2022-07-18 12:17:35.917000 UTC,,,0,amazon-web-services|amazon-sagemaker,48,2011-12-22 16:25:37.703000 UTC,2022-09-22 14:36:37.830000 UTC,London,860,2,0,90,,,,,,['amazon-sagemaker']
Batch Prediction Job non-blocking,"<p>I am running a Vertex AI batch prediction using the python API.
The function I am using is from the google cloud docs:</p>
<pre><code>def create_batch_prediction_job_dedicated_resources_sample(
    key_path,
    project: str,
    location: str,
    model_display_name: str,
    job_display_name: str,
    gcs_source: Union[str, Sequence[str]],
    gcs_destination: str,
    machine_type: str = &quot;n1-standard-2&quot;,
    sync: bool = True,
):
    credentials = service_account.Credentials.from_service_account_file(
    key_path)

# Initilaize an aiplatfrom object
 aiplatform.init(project=project, location=location, credentials=credentials)

# Get a list of Models by Model name
 models = aiplatform.Model.list(filter=f'display_name=&quot;{model_display_name}&quot;')
 model_resource_name = models[0].resource_name

# Get the model
 my_model = aiplatform.Model(model_resource_name)

 batch_prediction_job = my_model.batch_predict(
    job_display_name=job_display_name,
    gcs_source=gcs_source,
    gcs_destination_prefix=gcs_destination,
    machine_type=machine_type,
    sync=sync,
)

 #batch_prediction_job.wait_for_resource_creation()
 batch_prediction_job.wait()

 print(batch_prediction_job.display_name)
 print(batch_prediction_job.resource_name)
 print(batch_prediction_job.state)
 return batch_prediction_job

datetime_today = datetime.datetime.now()
model_display_name = 'test_model'
key_path = 'vertex_key.json'
project = 'my_project'
location = 'asia-south1'
job_display_name = 'batch_prediction_' + str(datetime_today)
model_name = '1234'
gcs_source = 'gs://my_bucket/Cleaned_Data/user_item_pairs.jsonl'
gcs_destination = 'gs://my_bucket/prediction'

create_batch_prediction_job_dedicated_resources_sample(key_path,project,location,model_display_name,job_display_name,
                                                      gcs_source,gcs_destination)
</code></pre>
<p>OUTPUT:</p>
<pre><code>92 current state:
JobState.JOB_STATE_RUNNING
INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/my_project/locations/asia-south1/batchPredictionJobs/37737350127597649
</code></pre>
<p>The above output is being printed on the terminal over and over after every few seconds.</p>
<p>The issue that I have is that the python program calling this function keeps on running until it is force stopped. I have tried both <code>batch_prediction_job.wait()</code> &amp; <code>batch_prediction_job.wait_for_resource_creation()</code> with the same results.</p>
<p>How do I start a batch_prediction_job without waiting for it to complete and terminating the program just after the job has be created?</p>",1,8,2022-01-24 09:36:50.770000 UTC,,2022-01-27 05:35:34.943000 UTC,0,google-cloud-platform|google-cloud-vertex-ai,270,2015-04-20 12:22:18.823000 UTC,2022-09-24 14:16:22.667000 UTC,"Gurugram, Haryana, India",363,26,0,32,,,,,,['google-cloud-vertex-ai']
Access underlying jsonl file of AzureML TabularDataset,"<p>I have a <code>TabularDataset</code> containing labels that has been exported from a <code>Data Labeling</code> project.</p>
<p>I'd like to access the dataset through my <code>PythonScriptStep</code> as a <code>FileDataset</code> so I can access the underlying <code>jsonl</code> file directly.</p>
<p>How can I achieve this?</p>",1,0,2022-01-20 21:31:33.467000 UTC,,,1,python|azure|azure-machine-learning-service,60,2012-01-26 14:27:40.553000 UTC,2022-09-24 16:26:41.580000 UTC,,802,288,0,91,,,,,,['azure-machine-learning-service']
Service End point Failure in Azure ML through SDK,"<p>I have been running a simple Linear regression model to predict prices and publishing its endpoint on the azure portal in free subscription trial.</p>
<p>I am facing problem in score.py program where creating the defination of init() and run()</p>
<p>The error that I am getting is 'list' object has no attribute 'predict' when trying to predict new observations on the published endpoint.</p>
<p>Please find the code of Deploy configuration</p>
<pre><code>from azureml.core import Workspace, Experiment
print(&quot;Accessing the workspace from job....&quot;)
ws=Workspace.from_config()


# Get the input dataset
print(&quot;Accessing the Adult Income dataset...&quot;)
input_ds = ws.datasets.get('pricing')


# -------------------------------------------------
# Create custom environment
# -------------------------------------------------
from azureml.core import Environment
from azureml.core.environment import CondaDependencies

# Create the environment
myenv = Environment(name=&quot;MyEnvironment&quot;)

# Create the dependencies object
myenv_dep = CondaDependencies.create(conda_packages=['scikit-learn', 'pip','pandas'],
                                     pip_packages=['azureml-defaults', 'azureml-interpret'])

myenv.python.conda_dependencies = myenv_dep

# Register the environment
print(&quot;Registering the environment...&quot;)
myenv.register(ws)

# Creat an Azure Kubernets Service provisioning Configuration

from azureml.core.compute import AksCompute, ComputeTarget

cluster_name='aks-cluster-12'

if cluster_name not in ws.compute_targets:
    print(cluster_name,&quot;does not exist.Creating a new one&quot;)
    print('Creating provisiong config for Aks cluster')
    

    aks_config=AksCompute.provisioning_configuration(location='centralindia',
                                                 vm_size='Standard_DS11_v2',
                                                 agent_count=1,
                                                 cluster_purpose='DevTest')
    print(&quot;Creating the AKS cluster&quot;)



    production_cluster=ComputeTarget.create(ws,cluster_name,aks_config)
    
    production_cluster.wait_for_completion(show_output=True)
else:
    print(cluster_name,&quot;exists. using it..&quot;)
    production_cluster=ws.compute_targets[cluster_name]

# Creat the inference Configuration

from azureml.core.model import InferenceConfig

inference_config= InferenceConfig(environment=myenv ,entry_script='scoringscriptnew.py',
                                 source_directory=&quot;.&quot;)

from azureml.core.webservice import AksWebservice 

deploy_config=AksWebservice.deploy_configuration(cpu_cores=1,
                                                 memory_gb=0.5)

# Deploy 

from azureml.core.model import Model
model=ws.models['regression01']

service= Model.deploy(workspace=ws,name='regression',models=[model],
                      inference_config=inference_config,
                      deployment_config=deploy_config,
                      deployment_target=production_cluster)

service.wait_for_deployment(show_output=True)
</code></pre>
<p>And code of score.py</p>
<pre><code>import json
import joblib
from azureml.core.model import Model
import pandas as pd

# Called when the service is loaded

def init():
    global predictor
    
    # Get the path to the registered model file and load it
    model_path = Model.get_model_path('regression01')
    predictor = joblib.load(model_path)


# Called when a request is received
def run(raw_data):
    # Get the input data as a dictionary
    data_dict = json.loads(raw_data)['data']
    
    # Convert dictionary to pandas dataframe
    data = pd.DataFrame.from_dict(data_dict)
    
    # Transform the data
    # data = one_hot.transform(data)
    

    # difference of train and deploy

    # Get a prediction from the model
    predictions = predictor.predict(data)

    # Return the predictions
    return predictions
</code></pre>
<p>Can someone help me understand if the <code>init()</code>  function is not performing and if not what could be the reason?</p>",1,0,2022-09-15 15:15:56.787000 UTC,,2022-09-15 17:22:16.440000 UTC,1,python|machine-learning|azure-machine-learning-service,45,2018-02-01 08:16:36.617000 UTC,2022-09-23 06:55:53.343000 UTC,Mumbai,11,0,0,5,,,,,,['azure-machine-learning-service']
How to write a predict function for mlr predict to upload in AzureML as webservice?,"<p>I am trying to upload a R Model in AzureML as webservice, model uses mlr package in R and its predict function, the output of mlr predict is a table of ""PredictionClassif"" ""Prediction"", for the linear model like Regression I use</p>

<pre><code>PredictAction &lt;- function(inputdata){
  predict(RegModel, inputdata, type=""response"")
}
</code></pre>

<p>This is working perfectly fine in Azure.</p>

<p>When I use mlr package for classification with predict type probability, the predict function I have to write as,</p>

<pre><code>PredictAction &lt;- function(inputdata){
  require(mlr)
  predict(randomForest,newdata=inputdata)
}
</code></pre>

<p>When calling the function</p>

<pre><code>publishWebService(ws, fun, name, inputSchema)
</code></pre>

<p>It produces an Error as</p>

<pre><code>converting `inputSchema` to data frame
Error in convertArgsToAMLschema(lapply(x, class)) : 
  Error: data type ""table"" not supported
</code></pre>

<p>as the predict function produces a table which I don't know how to convert or modify, so I give the outputschema</p>

<pre><code>publishWebService(ws, fun, name, inputSchema,outputschema)
</code></pre>

<p>I am not sure how to specify the outputschema <a href=""https://cran.r-project.org/web/packages/AzureML/AzureML.pdf"" rel=""nofollow noreferrer"">https://cran.r-project.org/web/packages/AzureML/AzureML.pdf</a></p>

<p>outputschema is a list, 
the predict function from mlr produces the output of class</p>

<pre><code>class(pred_randomForest)
""PredictionClassif"" ""Prediction""
</code></pre>

<p>and the data output is a dataframe</p>

<pre><code>class(pred_randomForest$data)
""data.frame""
</code></pre>

<p>I am seeking help on the syntax for outputschema in publishWebService function, or whether I have to add any other arguments of the function. Not sure where is the issue, whether AzureML can't read the wrapped Model or whether the predict function of mlr is executed properly in AzureML.</p>

<p>Getting Following Error in AzureML</p>

<pre><code>Execute R Script Piped (RPackage) : The following error occurred during evaluation of R script: R_tryEval: return error: Error in UseMethod(""predict"") : no applicable method for 'predict' applied to an object of class ""c('FilterModel', 'BaseWrapperModel', 'WrappedModel')"" 
</code></pre>",1,0,2019-04-02 22:34:07.427000 UTC,,,0,r|azure-machine-learning-studio|mlr,185,2019-02-13 01:07:12.117000 UTC,2022-09-20 06:02:48.337000 UTC,,1204,219,4,166,,,,,,['azure-machine-learning-studio']
MLflow Rstudio Dockerfile Permission denied,"<h1>Goal</h1>
<p>Track R models in the remote MLflow tracking server (running in kubernetes). The model is developed on the local computer from RStudio run in the Docker container.</p>
<h1>Setup</h1>
<p>Based on my research I need to create RStudio image with conda installed. After that I want to run example from MLflow documentation.</p>
<p>Dockerfile</p>
<pre><code>FROM rocker/rstudio

USER root

ENV PATH=&quot;/root/miniconda3/bin:${PATH}&quot;
ARG PATH=&quot;/root/miniconda3/bin:${PATH}&quot;
ENV MLFLOW_BIN=/root/miniconda3/bin/mlflow
ENV MLFLOW_PYTHON_BIN=/root/miniconda3/bin/python

RUN apt-get update

RUN apt-get install -y wget &amp;&amp; rm -rf /var/lib/apt/lists/*

RUN wget \
    https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh \
    &amp;&amp; mkdir /root/.conda \
    &amp;&amp; bash Miniconda3-latest-Linux-x86_64.sh -b \
    &amp;&amp; rm -f Miniconda3-latest-Linux-x86_64.sh 
RUN conda --version

RUN R -e 'install.packages(&quot;mlflow&quot;)'
RUN R -e 'install.packages(&quot;glmnet&quot;)'
RUN R -e 'install.packages(&quot;carrier&quot;)'

RUN pip install -U mlflow==1.19.0
</code></pre>
<p>train.R (adjusted exmple from <a href=""https://github.com/mlflow/mlflow/tree/master/examples/r_wine"" rel=""nofollow noreferrer"">here</a>)</p>
<pre><code># The data set used in this example is from http://archive.ics.uci.edu/ml/datasets/Wine+Quality
# P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis.
# Modeling wine preferences by data mining from physicochemical properties. In Decision Support Systems, Elsevier, 47(4):547-553, 2009.

library(mlflow)
library(glmnet)
library(carrier)

set.seed(40)

# Read the wine-quality csv file
data &lt;- read.csv(&quot;wine-quality.csv&quot;)

# Split the data into training and test sets. (0.75, 0.25) split.
sampled &lt;- sample(1:nrow(data), 0.75 * nrow(data))
train &lt;- data[sampled, ]
test &lt;- data[-sampled, ]

# The predicted column is &quot;quality&quot; which is a scalar from [3, 9]
train_x &lt;- as.matrix(train[, !(names(train) == &quot;quality&quot;)])
test_x &lt;- as.matrix(test[, !(names(train) == &quot;quality&quot;)])
train_y &lt;- train[, &quot;quality&quot;]
test_y &lt;- test[, &quot;quality&quot;]

alpha &lt;- mlflow_param(&quot;alpha&quot;, 0.5, &quot;numeric&quot;)
lambda &lt;- mlflow_param(&quot;lambda&quot;, 0.5, &quot;numeric&quot;)

Sys.setenv(MLFLOW_S3_ENDPOINT_URL=&quot;&lt;EP&gt;&quot;)
Sys.setenv(AWS_ACCESS_KEY_ID=&quot;&lt;some_key&gt;&quot;)
Sys.setenv(AWS_SECRET_ACCESS_KEY=&quot;&lt;some_secret&gt;&quot;)

mlflow_set_experiment(&quot;Wine R experiment&quot;)
mlflow_set_tracking_uri(&quot;&lt;http...blabla&gt;&quot;)

with(mlflow_start_run(), {
    model &lt;- glmnet(train_x, train_y, alpha = alpha, lambda = lambda, family= &quot;gaussian&quot;, standardize = FALSE)
    predictor &lt;- crate(~ glmnet::predict.glmnet(!!model, as.matrix(.x)), !!model)
    predicted &lt;- predictor(test_x)

    rmse &lt;- sqrt(mean((predicted - test_y) ^ 2))
    mae &lt;- mean(abs(predicted - test_y))
    r2 &lt;- as.numeric(cor(predicted, test_y) ^ 2)

    message(&quot;Elasticnet model (alpha=&quot;, alpha, &quot;, lambda=&quot;, lambda, &quot;):&quot;)
    message(&quot;  RMSE: &quot;, rmse)
    message(&quot;  MAE: &quot;, mae)
    message(&quot;  R2: &quot;, r2)

    mlflow_log_param(&quot;alpha&quot;, alpha)
    mlflow_log_param(&quot;lambda&quot;, lambda)
    mlflow_log_metric(&quot;rmse&quot;, rmse)
    mlflow_log_metric(&quot;r2&quot;, r2)
    mlflow_log_metric(&quot;mae&quot;, mae)

    mlflow_log_model(predictor, &quot;model&quot;)
})
</code></pre>
<p>Its POC don't mind the unsafe env variables.</p>
<p>I run the container like this:</p>
<pre><code>docker run --rm -p 8787:8787 -e PASSWORD=password --mount type=bind,source=$(pwd)/mlflow/examples/r_wine,target=/home/rstudio rstudio-mlflow
</code></pre>
<h1>Problem</h1>
<p>Everytime I run the file from RStudio I get error on the last line (the <code>mlflow_log_model(predictor, &quot;model&quot;)</code>):</p>
<pre><code>cannot start processx process '/root/miniconda3/bin/mlflow' (system error 13, Permission denied) @unix/processx.c:608 (processx_exec)
</code></pre>
<p>I am getting permission denied when listing conda bin folder from RStudio terminal. Can you help me how to correctly install conda with RStudio image.</p>",1,2,2021-08-05 11:00:11.710000 UTC,2.0,,2,r|docker|rstudio|conda|mlflow,351,2014-12-30 11:34:35.500000 UTC,2022-09-23 12:37:47.597000 UTC,"Bratislava, Slovensko",357,34,0,64,,,,,,['mlflow']
"From the cluster, cannot download a Python Wheel from the storage account","<p>1) We upload a python wheel to the storage account associated with the workspace successfully.
2) In the second step we submit an experiment which runs in the cluster and needs to download and run the package from step 1.</p>

<p>The experiment is able to download the package and run when the storage account is not associated with any VNet. However, when we associate the storage account in Vnets the experiment hangs and eventually fails. This storage account is in two Vnet’s, a, and b. The cluster is also in the Vnet, a.</p>

<p>I don’t know why the cluster cannot download the wheel package when the storage account is in a Vnet. It is our policy to have storage accounts in Vnet’s.</p>

<p>It there something else we are missing? I also checked the container registry setting and its set to ‘allow from everywhere’ (we are using std SKU).</p>

<p>Let me know if any further information is required. Thanks.</p>",0,1,2020-03-12 08:56:53.540000 UTC,,,1,azure-machine-learning-studio|azure-machine-learning-service|azure-machine-learning-workbench,81,2011-02-02 10:11:55.120000 UTC,2022-08-19 08:34:49.060000 UTC,,389,10,0,137,,,,,,"['azure-machine-learning-workbench', 'azure-machine-learning-studio', 'azure-machine-learning-service']"
What is the fastest way to pull massive amounts of data from Snowflake Database into AWS SageMaker?,<p>What would be the fastest way to pull in very large datasets from Snowflake into my SageMaker instance in AWS? How does the snowflake python connector (what I currently use) compare to lets say a spark connector to snowflake?</p>,1,4,2022-04-06 20:48:37.853000 UTC,,,0,apache-spark|bigdata|snowflake-cloud-data-platform|amazon-sagemaker,320,2020-04-19 15:43:01.713000 UTC,2022-07-28 14:54:44.953000 UTC,,1,0,0,0,,,,,,['amazon-sagemaker']
my Azure ML api rest return an empty object,"<p><strong>My issue</strong></p>
<p>I try using REST API for machine learning. The following PowerShell doesn't fail, but return an empty objet, whatever the API I am testing.
my SPN has contributor permission, I made grant consent and I checked I get a token.</p>
<p>the doc I was using:</p>
<p><a href=""https://docs.microsoft.com/fr-fr/rest/api/azureml/quotas/list"" rel=""nofollow noreferrer"">https://docs.microsoft.com/fr-fr/rest/api/azureml/quotas/list</a></p>
<p>I tested several other GET API as well.</p>
<p>I don't know what to do more. Any idea?</p>
<p><strong>My Powershell code</strong></p>
<pre><code>$tenant_id = &quot;XXXXXXXXXXXXXXXXXXX&quot;
$ApplicationId = &quot;XXXXXXXXXXXXXXX&quot;
$spn_client_secret = &quot;XXXXXXXXXXXXX&quot;
$subscriptionid=&quot;XXXXXXXXXXXX&quot;
$uri = &quot;https://login.microsoftonline.com/$tenant_id/oauth2/token&quot;

$BodyText = &quot;grant_type=client_credentials&amp;client_id=$ApplicationId&amp;resource=https://management.azure.com&amp;client_secret=$spn_client_secret&quot;

# GET TOKEN
$Response = Invoke-RestMethod -Method POST -Body $BodyText -Uri $URI -ContentType application/x-www-form-urlencoded        
$aad_access_token = $Response.access_token

# tested, I effectively have a token

# READ ml

$urllist = &quot;https://management.azure.com/subscriptions/$subscriptionid/providers/Microsoft.MachineLearningServices/locations/westeurope/quotas?api-version=2021-03-01-preview&quot;

$headers = @{&quot;Authorization&quot; = &quot;Bearer &quot; + $aad_access_token}

Invoke-RestMethod -Method GET -HEADERS $headers -Uri $urllist -ContentType application/x-www-form-urlencoded  
</code></pre>",1,0,2021-11-09 23:29:10.737000 UTC,,,1,azure|machine-learning|oauth-2.0|azure-machine-learning-service,81,2013-11-16 22:05:12.687000 UTC,2022-09-24 17:30:05.067000 UTC,France,511,14,0,158,,,,,,['azure-machine-learning-service']
Run ML pipeline using AWS step function for entire dataset?,"<p>I have a step function setup which calls preprocessing lambda and inference lambda for a data item. Now, I need to do this process on the entire dataset(over 10000 items). One way is to invoke step function parallelly for each input. Is there a better alternative to this approach?</p>",1,0,2021-03-27 10:08:55.833000 UTC,,,0,amazon-web-services|aws-lambda|amazon-sagemaker|aws-step-functions,47,2021-03-26 05:43:58.963000 UTC,2022-03-24 07:23:16.833000 UTC,,1,0,0,2,,,,,,['amazon-sagemaker']
No Artifacts Recorded MLFlow,"<p>I am unable to store, view, and retrieve the artifacts in MLFlow. The artifact folder is empty irrespective of creating a new experiment and assign proper experiment name and location.</p>
<p>Server: mlflow server --backend-store-uri mlruns/ --default-artifact-root mlruns/ --host 0.0.0.0 --port 5000</p>
<p>Create an Experiment: mlflow.create_experiment(exp_name, artifact_location='mlruns/')</p>
<pre><code>with mlflow.start_run():
    mlflow.log_metric(&quot;mse&quot;, float(binary))
    mlflow.log_artifact(data_path, &quot;data&quot;)
    # log model
    mlflow.keras.log_model(model, &quot;models&quot;)
</code></pre>
<p>The code compiles and runs but does not have any artifacts recorded. It has mlflow.log-model.history file but not the model.h5</p>",3,0,2020-07-03 17:19:37.153000 UTC,2.0,,4,python-3.x|machine-learning|keras|mlflow,2676,2018-09-10 09:02:13.123000 UTC,2020-10-02 22:39:09.240000 UTC,"Mumbai, Maharashtra, India",89,0,0,13,,,,,,['mlflow']
"Cloudpickle.dump(pyspark_Alsmodel_object),getting error py4j.Py4JException: Method __getnewargs__([]) does not exist?","<p>After creating ALS model object,using pyspark. </p>

<p>Sample code example:</p>

<pre><code>from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.ml.recommendation import ALS
from pyspark.sql import Row

lines = spark.read.text(""data/mllib/als/sample_movielens_ratings.txt"").rdd
parts = lines.map(lambda row: row.value.split(""::""))
ratingsRDD = parts.map(lambda p: Row(userId=int(p[0]), movieId=int(p[1]),
                                     rating=float(p[2]), timestamp=long(p[3])))
ratings = spark.createDataFrame(ratingsRDD)
(rating_data, test) = ratings.randomSplit([0.8, 0.2])

# Build the recommendation model using ALS on the training data
# Note we set cold start strategy to 'drop' to ensure we don't get NaN evaluation metrics
als = ALS(maxIter=5, regParam=0.01, userCol=""userId"", itemCol=""movieId"", ratingCol=""rating"",
          coldStartStrategy=""drop"")

    als_model = als_spec.fit(rating_data)
</code></pre>

<p>here i am just creating ALS model and making cloudepickel.
if we are using fit then also need to do transform?</p>

<p>I am trying pickel the my als_model object using the below code :</p>

<pre><code>with open(os.path.join(model_path, 'als-als-model.pkl'), 'w') as out:
                cloudpickle.dump(als_model, out)
</code></pre>

<p>I am getting error like below:</p>

<pre><code>  File ""/usr/local/spark/python/lib/py4j-0.10.6-src.zip/py4j/protocol.py"", line 324, in get_return_value
    format(target_id, ""."", name, value))
Py4JError: An error occurred while calling o224.__getnewargs__. Trace:
py4j.Py4JException: Method __getnewargs__([]) does not exist
#011at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)
#011at 

---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-124-8c94f4ee0de9&gt; in &lt;module&gt;()
      1 
----&gt; 2 tree.fit(data_location)

~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sagemaker/estimator.py in fit(self, inputs, wait, logs, job_name)
    152         self.latest_training_job = _TrainingJob.start_new(self, inputs)
    153         if wait:
--&gt; 154             self.latest_training_job.wait(logs=logs)
    155         else:
    156             raise NotImplemented('Asynchronous fit not available')
</code></pre>",0,8,2018-03-15 12:06:49.450000 UTC,,2018-03-16 11:08:40.330000 UTC,0,python|apache-spark|pyspark|pyspark-sql|amazon-sagemaker,255,2015-04-03 14:45:43.217000 UTC,2022-01-14 00:51:53.803000 UTC,,1015,34,0,304,,,,,,['amazon-sagemaker']
Azure ML studio web service output,"<p>I made a web service from a python notebook and the output is:</p>

<pre><code>{""Results"":{""output1"":{""type"":""table"",""value"":{""Values"":[[""1""]]}},""output2"":{""type"":""table"",""value"":{""Values"":[[""data:text/plain,Execution OK\r\n"",null]]}}}}
</code></pre>

<p>But I just wanted the response to be the value in the key ""Values"" so that way I don't have to parse it on the client side. Is that possible?</p>",0,2,2018-06-11 15:36:47.633000 UTC,,,0,jupyter-notebook|azure-machine-learning-studio,88,2011-12-12 04:19:46.257000 UTC,2020-11-26 17:18:02.567000 UTC,"Felgueiras, Portugal",11,0,0,4,,,,,,['azure-machine-learning-studio']
fitting and predicting model with mlflow,"<p>I'm very new to understanding the use of MLFlow but need assistance, I'm trying to understand on how to try and fit and predict my model once again. I'm able to call my model by:</p>
<pre><code>PLS_model = mlflow.pyfunc.load_model(&quot;runs:/FFFFF!@#!@#@!#!/logged_model&quot;, suppress_warnings = True)
</code></pre>
<p>and get:</p>
<pre><code>mlflow.pyfunc.loaded_model:
  artifact_path: logged_model
  flavor: mlflow.sklearn
  run_id: FFFFF!@#!@#@!#!
</code></pre>
<p>But when I try to call any methods as:</p>
<p>1).fit or .predict. I get the following error</p>
<pre><code>AttributeError: 'PyFuncModel' object has no attribute 'fit'

AttributeError: 'PyFuncModel' object has no attribute 'predict'
</code></pre>
<p>Here I encountered on how to actually call these functions but not sure if I'm doing this correctly. In summary, how can I predict, fit to my new data.</p>
<p>Thanks</p>",1,0,2020-07-30 14:47:13.613000 UTC,,,0,python|mlflow,1702,2017-10-24 16:10:51.607000 UTC,2022-08-31 12:33:53.680000 UTC,Netherlands,458,104,1,71,,,,,,['mlflow']
How to delete an experiment from an azure machine learning workspace,"<p>I create experiments in my workspace using the python sdk (azureml-sdk). I now have a lot of 'test' experiments littering our workspace. How can I delete individual experiments either through the api or on the portal. I know I can delete the whole workspace but there are some good experiments we don't want to delete</p>

<p><a href=""https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-export-delete-data#delete-visual-interface-assets"" rel=""noreferrer"">https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-export-delete-data#delete-visual-interface-assets</a> suggests it is possible but my workspace view does not look anything like what is shown there</p>",4,0,2019-08-14 15:02:15.187000 UTC,3.0,,9,azure-machine-learning-service,3890,2019-08-14 14:48:38.450000 UTC,2022-09-25 02:16:40.673000 UTC,,113,3,0,6,,,,,,['azure-machine-learning-service']
Recent change in create Dataset from Datastore UI?,"<p>I'm working on the new feature ""Data Drift in Azure ML"", but I had problems when I was testing this tool, the interface to create Datasets from Datastore is not like the documentation, is it different?.</p>

<p>Also, when I try using format partition like the documentation, the configuration shows me some errors, it only works with this format <code>/{datetime}/filename.cvs</code> instead of <code>/{timestamp:yyyy/MM/dd}/filename.csv</code>, and the schema section doesn't display the schema like in the documentation.</p>",0,2,2020-04-01 21:53:02.300000 UTC,,2020-04-23 00:11:06.860000 UTC,1,azure-machine-learning-service,80,2020-03-02 17:15:09.800000 UTC,2022-05-12 18:30:47.513000 UTC,,11,0,0,3,,,,,,['azure-machine-learning-service']
How to create a smp.DistributedModel which uses pre-trained tf.keras.Models in its call function?,"<p>Let's say we have 2 pre-trained tf.keras.Models which we first need to initialize and then load weights into. I want to create a third model that uses both of these models in its call function and I want to use model parallelism on this third model.</p>
<p>If we were to treat smp.DistributedModel like tf.keras.Model, the code would look something like this.</p>
<pre><code>model_1 = create_build_and_load_weights_model_1()
model_2 = create_build_and_load_weights_model_2()

class smp_model(smp.DistributedModel):

    def __init__(self, model_1, model_2): 
        # initialize Super
        self.model_1 = model_1
        self.model_2 = model_2
        self.output_dense = tf.keras.layers.Dense(num_classes, activation = 'softmax')

    def call(self, inputs):
        x = self.model_1(inputs)
        x = self.model_2(x)
        outputs = self.output_dense(x)
        return outputs

model_3 = smp_model(model_1, model_2)

# code to train the model_3
</code></pre>
<p>How can I implement this with smp.DistributedModel?</p>
<p>Sagemaker_services used : Sagemaker Model Parallel Library</p>
<p>Tensorflow version: 2.X</p>",0,0,2021-11-11 09:36:54.917000 UTC,,2021-11-12 00:35:07.813000 UTC,0,amazon-web-services|tensorflow|keras|deep-learning|amazon-sagemaker,61,2020-05-27 19:19:25.017000 UTC,2022-09-18 00:04:30.017000 UTC,India,41,18,0,18,,,,,,['amazon-sagemaker']
VertexAI Pipeline: How to use an output from a custom kfp component as input for google_cloud_pipeline_components?,"<p>I'm trying to write the Python code for a pipeline in VertexAI using kfp components. I have a step where i create a <code> system.Dataset</code>  object that is the following:</p>
<pre class=""lang-py prettyprint-override""><code>@component(base_image=&quot;python:3.9&quot;, packages_to_install=[&quot;google-cloud-bigquery&quot;,&quot;pandas&quot;,&quot;pyarrow&quot;,&quot;fsspec&quot;,&quot;gcsfs&quot;])
def create_dataframe(
    project: str,
    region: str,
    destination_dataset: str,
    destination_table_name: str,
    dataset: Output[Dataset],
):
    
    from google.cloud import bigquery
    
    client = bigquery.Client(project=project, location=region)
    dataset_ref = bigquery.DatasetReference(project, destination_dataset)
    table_ref = dataset_ref.table(destination_table_name)
    table = client.get_table(table_ref)

    train = client.list_rows(table).to_dataframe()
    train.drop(&quot;&lt;list_of_columns&gt;&quot;, axis=1, inplace=True)
    train['class'] = [0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1]
    
    train.to_csv(dataset.uri)
</code></pre>
<p>Then I use the dataset as input for <code>AutoMLTabularTrainingJobRunOp</code>:</p>
<pre class=""lang-py prettyprint-override""><code>df = create_dataframe(project=project,
                      region=region,
                      destination_dataset=destination_dataset,
                      destination_table_name=destination_table_name,
)
    
# Training with AutoML
training_op = gcc_aip.AutoMLTabularTrainingJobRunOp(
            project=project,
            display_name=&quot;train-automl-task&quot;,
            optimization_prediction_type=&quot;classification&quot;,
            column_transformations=[
                &quot;&lt;nested_dict&gt;&quot;,
            ],
            dataset=df.outputs[&quot;dataset&quot;],
            target_column=&quot;class&quot;,
            budget_milli_node_hours=1000,
)
</code></pre>
<p>Looking at the logs, I found this error:</p>
<pre><code>&quot;Traceback (most recent call last): &quot;

&quot; File &quot;/opt/python3.7/lib/python3.7/runpy.py&quot;, line 193, in _run_module_as_main &quot;

&quot; &quot;__main__&quot;, mod_spec) &quot;

&quot; File &quot;/opt/python3.7/lib/python3.7/runpy.py&quot;, line 85, in _run_code &quot;

&quot; exec(code, run_globals) &quot;

&quot; File &quot;/opt/python3.7/lib/python3.7/site-packages/google_cloud_pipeline_components/remote/aiplatform/remote_runner.py&quot;, line 284, in &lt;module&gt; &quot;

&quot; main() &quot;

&quot; File &quot;/opt/python3.7/lib/python3.7/site-packages/google_cloud_pipeline_components/remote/aiplatform/remote_runner.py&quot;, line 280, in main &quot;

&quot; print(runner(args.cls_name, args.method_name, executor_input, kwargs)) &quot;

&quot; File &quot;/opt/python3.7/lib/python3.7/site-packages/google_cloud_pipeline_components/remote/aiplatform/remote_runner.py&quot;, line 236, in runner &quot;

&quot; prepare_parameters(serialized_args[METHOD_KEY], method, is_init=False) &quot;

&quot; File &quot;/opt/python3.7/lib/python3.7/site-packages/google_cloud_pipeline_components/remote/aiplatform/remote_runner.py&quot;, line 205, in prepare_parameters &quot;

&quot; value = cast(value, param_type) &quot;

&quot; File &quot;/opt/python3.7/lib/python3.7/site-packages/google_cloud_pipeline_components/remote/aiplatform/remote_runner.py&quot;, line 176, in cast &quot;

&quot; return annotation_type(value) &quot;

&quot; File &quot;/opt/python3.7/lib/python3.7/site-packages/google/cloud/aiplatform/datasets/dataset.py&quot;, line 81, in __init__ &quot;

&quot; self._gca_resource = self._get_gca_resource(resource_name=dataset_name) &quot;

&quot; File &quot;/opt/python3.7/lib/python3.7/site-packages/google/cloud/aiplatform/base.py&quot;, line 532, in _get_gca_resource &quot;

&quot; location=self.location, &quot;

&quot; File &quot;/opt/python3.7/lib/python3.7/site-packages/google/cloud/aiplatform/utils/__init__.py&quot;, line 192, in full_resource_name &quot;

&quot; raise ValueError(f&quot;Please provide a valid {resource_noun[:-1]} name or ID&quot;) &quot;

&quot;ValueError: Please provide a valid dataset name or ID &quot;
</code></pre>
<p>So, I looked at source code in <code>google/cloud/aiplatform/utils/__init__.py</code> at line 192 and I found that the resource name should be like: <code>&quot;projects/.../locations/.../datasets/12345&quot;</code> or <code>&quot;projects/.../locations/.../metadataStores/.../contexts/12345&quot; </code>.</p>
<p>Opening the <code>executor_output.json</code> file that is created in my bucket after running <code>create_dataframe</code> I discovered that the file name seems to be in the right format:</p>
<p><code>{&quot;artifacts&quot;: {&quot;dataset&quot;: {&quot;artifacts&quot;: [{&quot;name&quot;: &quot;projects/my_project/locations/my_region/metadataStores/default/artifacts/1299...&quot;, &quot;uri&quot;: &quot;my_bucket/object_folder&quot;, &quot;metadata&quot;: {&quot;name&quot;: &quot;reshaped-training-dataset&quot;}}]}}}</code></p>
<p>I tried also to set a human readable name for dataset in metadata, but I did not work.
Any suggestion would be really helpful.</p>",1,0,2021-11-17 14:51:01.623000 UTC,,,1,python|google-cloud-platform|google-cloud-ml|google-cloud-vertex-ai|kfp,322,2021-03-24 12:34:53.617000 UTC,2022-09-22 15:26:12.103000 UTC,"Alatri, Frosinone, FR",67,3,0,33,,,,,,['google-cloud-vertex-ai']
Uploading additional files for inference from local - Sagemaker deploy,"<p>I'm attempting to deploy a local tensorflow model to a sagemaker endpoint. I want to include custom inference code to transform the input data. Given the model is trained and already located in an S3 bucket, I can run the following, which correctly deploys the model to an endpoint:</p>
<pre><code>tensorflow_model = Model(
        model_data=saved_model,
        entry_point='src/development/document_matcher/inference.py',
        source_dir ='./src/development/document_matcher',      
        role=role, 
        framework_version=tf_framework_version
        )
</code></pre>
<p>The inference file is working correctly; however, I also need the vocabulary for the model to transform the incoming data correctly. This is what I'm trying to upload with source_dir.</p>
<p>My Directory structure is as follows:</p>
<pre><code>- src
 - development
  -document_matcher
   - inference.py
   - total_vocab.pkl
</code></pre>
<p>I need to have total_vocab.pkl accessible to my inference script when it runs. However, inference.py is unable to find it. Am I misunderstanding how sagemaker works?</p>",2,0,2022-02-09 16:24:51.787000 UTC,,2022-02-10 07:58:57.090000 UTC,0,python|amazon-web-services|tensorflow|amazon-sagemaker,170,2021-04-17 19:53:38.020000 UTC,2022-09-14 20:41:08.420000 UTC,,35,4,0,14,,,,,,['amazon-sagemaker']
Sagemaker script processor,"<blockquote>
<pre><code>script_processor = ScriptProcessor(
    base_job_name=job_name,
    image_uri=processing_repository_uri,
    role=role,
    command=[&quot;python3&quot;],
    instance_count=instance_count,
    instance_type=instance_type,
    max_runtime_in_seconds=MAX_RUN_TIM)
</code></pre>
</blockquote>
<pre><code>script_processor.run(
    code=&quot;src/extract_data.py&quot;,
    outputs=[
        ProcessingOutput(source=path, destination=destination),
    ])
</code></pre>
<p>Is there a possibility to specify <code>source_dir</code> when using ScriptProcessor. I tried to figure it out but I could not find anything. Is it designed to run independent scripts? if it is the case what are the alternative for doing it with sagemaker.</p>",1,0,2021-02-01 15:03:57.503000 UTC,,2021-02-01 16:52:41.483000 UTC,0,amazon-web-services|amazon-sagemaker,320,2015-07-21 13:17:26.180000 UTC,2022-09-22 13:12:18.153000 UTC,,71,17,0,11,,,,,,['amazon-sagemaker']
Can you import data from parquet or delta file format data sets into Vertex AI feature store,"<p>Just wondering if it is possible to not just import from CSV-based data sets for the Vertex AI feature store, but from parquet or delta file formats as well. When trying to import a dataset from within GCP, the only options it gives are from BigQuery or from CSV.</p>
<p>I have attached a picture of the options given</p>
<p><a href=""https://i.stack.imgur.com/sHtOv.png"" rel=""nofollow noreferrer"">No Parquet Option - Only CSV and BigQuery</a></p>
<p>Does anyone know if there is an API/plug-in/other methodology from which one can load parquet or delta files directly into the Vertex AI feature store?</p>
<p>Thank you!</p>",1,0,2021-11-23 18:23:41.130000 UTC,,,1,google-cloud-ml|google-cloud-vertex-ai|feature-store,182,2019-07-29 08:01:05.083000 UTC,2022-08-12 16:46:11.920000 UTC,,13,0,0,1,,,,,,['google-cloud-vertex-ai']
Vertex AI forecasting AutoML giving different answers for same input data,"<p>I trained Vertex AI forecasting AutoML model one with target column as String and other numeric input features as String then I trained another AutoML model with target column as float and other input features as Integer.</p>
<p>The predictions are different for both the models. The data is same only the datatypes/schema changed.</p>
<p>Google <a href=""https://cloud.google.com/vertex-ai/docs/datasets/data-types-tabular#forecasting"" rel=""nofollow noreferrer"">documentation</a> says:</p>
<blockquote>
<p>When you train a model with a feature with a numeric transformation,
Vertex AI applies the following data transformations to the feature,
and uses any that provide signal for training:</p>
<ul>
<li>The value converted to float32.</li>
</ul>
</blockquote>
<p>So both the data should be same even after transformation.
Why would results be different? Is it possible?</p>",1,0,2022-02-08 11:40:16.577000 UTC,,2022-02-11 11:51:40.387000 UTC,1,google-cloud-platform|forecasting|google-cloud-automl|automl|google-cloud-vertex-ai,197,2015-12-16 17:49:09.287000 UTC,2022-08-22 11:39:25.350000 UTC,,500,14,0,72,,,,,,['google-cloud-vertex-ai']
Sagemaker model deployment failing due to custom endpoint name,"<p>AWS Sagemaker model deployment is failing when endpoint_name argument is specified. Any thoughts?</p>

<p>Without endpoint_name argument in deploy, model deployment works successfully.
Model training and saving into S3 location is successful either way.</p>

<pre><code>import boto3
import os
import sagemaker
from sagemaker import get_execution_role
from sagemaker.predictor import csv_serializer
from sagemaker.amazon.amazon_estimator import get_image_uri

bucket = 'Y'
prefix = 'Z'

role = get_execution_role()

    train_data, validation_data, test_data = np.split(df.sample(frac=1, random_state=100), [int(0.5 * len(df)), int(0.8 * len(df))])

    train_data.to_csv('train.csv', index=False, header=False)
    validation_data.to_csv('validation.csv', index=False, header=False)
    test_data.to_csv('test.csv', index=False)
    boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train/X/train.csv')).upload_file('train.csv')
    boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'validation/X/validation.csv')).upload_file('validation.csv')

    container = get_image_uri(boto3.Session().region_name, 'xgboost')
    #print(container)

    s3_input_train = sagemaker.s3_input(s3_data='s3://{}/{}/train/{}'.format(bucket, prefix, suffix), content_type='csv')
    s3_input_validation = sagemaker.s3_input(s3_data='s3://{}/{}/validation/{}/'.format(bucket, prefix, suffix), content_type='csv')

    sess = sagemaker.Session()

    output_loc = 's3://{}/{}/output'.format(bucket, prefix)
    xgb = sagemaker.estimator.Estimator(container,
                                        role, 
                                        train_instance_count=1, 
                                        train_instance_type='ml.m4.xlarge',
                                        output_path=output_loc,
                                        sagemaker_session=sess,
                                        base_job_name='X')
    #print('Model output to: {}'.format(output_location))

    xgb.set_hyperparameters(eta=0.5,
                            objective='reg:linear',
                            eval_metric='rmse',
                            max_depth=3,
                            min_child_weight=1,
                            gamma=0,
                            early_stopping_rounds=10,
                            subsample=0.8,
                            colsample_bytree=0.8,
                            num_round=1000)

    #Model fitting
    xgb.fit({'train': s3_input_train, 'validation': s3_input_validation})

    #Deploy model with automatic endpoint created
    xgb_predictor_X = xgb.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge', endpoint_name='X')

    xgb_predictor_X.content_type = 'text/csv'
    xgb_predictor_X.serializer = csv_serializer
    xgb_predictor_X.deserializer = None
</code></pre>

<p>INFO:sagemaker:Creating endpoint with name delaymins
ClientError: An error occurred (ValidationException) when calling the CreateEndpoint operation: Could not find model ""arn:aws:sagemaker:us-west-2::model/X-2019-01-08-18-17-42-158"".</p>",1,0,2019-01-09 00:07:33.447000 UTC,,2019-01-09 00:47:10.410000 UTC,1,amazon-sagemaker,1333,2012-06-04 21:07:02.563000 UTC,2021-01-22 16:23:41.947000 UTC,,355,4,0,44,,,,,,['amazon-sagemaker']
'SageMaker' object has no attribute 'update_feature_group',"<p>I am trying to call udpate_feature_group() function from sagemaker boto3 API.</p>
<pre><code>client = boto3.client('sagemaker')
response = client.update_feature_group(FeatureGroupName=featureGroupName,FeatureAdditions=featureAdditions)
</code></pre>
<p>however I'm getting below error</p>
<pre><code>'SageMaker' object has no attribute 'update_feature_group'
</code></pre>
<p>I verified the boto3 version being used in lambda, it is 1.24.62
which has update_feature_group() function as well
<a href=""https://boto3.amazonaws.com/v1/documentation/api/1.24.62/reference/services/sagemaker.html#SageMaker.Client.update_feature_group"" rel=""nofollow noreferrer"">https://boto3.amazonaws.com/v1/documentation/api/1.24.62/reference/services/sagemaker.html#SageMaker.Client.update_feature_group</a></p>
<p>then it should not complaint? any idea why I am getting that error?</p>",0,1,2022-09-08 20:17:37.527000 UTC,,,0,amazon-web-services|boto3|amazon-sagemaker,10,2013-04-10 05:09:01.220000 UTC,2022-09-14 22:34:07.097000 UTC,,519,13,0,576,,,,,,['amazon-sagemaker']
I want my lambda code to directly upload files into an Amazon S3 bucket of a different account,"<p>So I have a lambda function that triggers an Amazon SageMaker processing job and this job currently writes a few files to my Amazon S3 bucket. I have mentioned my <strong>output_uri ='s3://outputbucket-in-my-acc/'</strong> Now I want the same files to be directly uploaded to a different AWS account and not in my account. How do i achieve this? I want no traces of the file to be stored in my account.</p>
<p>I found a similar solution here but this copies the file into the different account while the original files are still present in the source account:
<a href=""https://stackoverflow.com/questions/59188157/aws-lambda-put-data-to-cross-account-s3-bucket"">AWS Lambda put data to cross account s3 bucket</a></p>",2,0,2021-06-24 11:47:30.927000 UTC,,2021-06-24 12:42:33.987000 UTC,2,amazon-web-services|amazon-s3|aws-lambda|amazon-iam|amazon-sagemaker,321,2020-12-23 18:10:30.727000 UTC,2022-09-10 12:04:24.883000 UTC,,105,6,0,17,,,,,,['amazon-sagemaker']
Secret Manager from Vertex AI Pipeline,"<p>I am working in GCP creating a Vertex AI pipeline with kubeflow and it is time for me to store my API keys more securely. I am very new to GCP and unfamiliar with the environment so I've been trying to follow a few tutorials but have hit a roadblock. I want to store my secrets in Secret Manager and then later access them from the pipeline I've written. I have no problem creating secrets and viewing them in the GUI but when it comes to compiling my pipeline i get the error: <code>google.api_core.exceptions.PermissionDenied: 403 Permission denied on resource project...</code></p>
<p>So it seems that the account running my pipelines does not have access to the secrets I have created. My question is then, how do I check which account is running the pipeline so I can grant it access? Or is there really another underlying problem here?</p>
<p>Code trying to access the secret:</p>
<pre><code> client = secretmanager.SecretManagerServiceClient()
 secret_name = &quot;secret_name&quot;
 request = {'name': f&quot;path/{secret_name}/versions/latest&quot;}
 response = client.access_secret_version(request)
 secret_string = response.payload.data.decode(&quot;UTF-8&quot;)
</code></pre>
<p>EDIT: I can add that I have been playing around a lot with account permissions but my best guess is that the account that is found under Vertex AI&gt;Workbench&gt;the notebook I am using's notebook details&gt;Service account is the one that needs permission. Is this not it?</p>",3,4,2022-03-16 14:21:53.490000 UTC,,2022-03-17 11:00:39.537000 UTC,0,python|google-cloud-platform|google-secret-manager|google-cloud-vertex-ai,311,2014-08-05 13:37:06.703000 UTC,2022-09-22 12:30:19.650000 UTC,,745,210,6,168,,,,,,['google-cloud-vertex-ai']
Mlflow: Log steps in evaluation phase using Tensorflow train_and_evaluate,"<p>I'm trying to log the steps during the evaluation using Mlflow but have only been able to log the last step. Using
mlflow.tensorflow.autolog() I am able to log some metrics (like loss) when a checkpoint is saved, every 100 steps that is defined in RunConfig. However I also need to save the accuracy and top3error every 100 steps the model is evaluated. Here is my code:</p>

<pre><code>def top3error(features, labels, predictions):
    return {'top3error': tf.metrics.mean(tf.nn.in_top_k(predictions=predictions['logits'], 
                                                        targets=labels,
                                                        k=3))}
# Log metrics
mlflow.tensorflow.autolog()

with mlflow.start_run():
    steps = 1000

    mlflow.log_param(""Steps"", steps)    

    '''Training &amp; Validation'''
    train_spec = tf.estimator.TrainSpec(input_fn=generate_input_fn(train), 
                                        max_steps=steps)
    eval_spec = tf.estimator.EvalSpec(name='validation',
                                      input_fn=generate_input_fn(test, num_epochs=1))

    tf.logging.info(""Starting Run..."")
    results = tf.estimator.train_and_evaluate(m, train_spec, eval_spec)    

    '''Log Run'''
    mlflow.log_metric(""accuracy"", results[0]['accuracy'])
    mlflow.log_metric(""top3error"", results[0]['top3error'])
</code></pre>

<p>Here is the RunConfig used in the model:</p>

<pre><code>config=tf.estimator.RunConfig(
  model_dir=model_dir, 
  save_checkpoints_steps=100,
)
</code></pre>

<p>Thanks in advance</p>",1,2,2019-08-01 14:57:08.267000 UTC,,,0,tensorflow|callback|evaluate|mlflow,484,2016-04-24 23:13:14.160000 UTC,2022-09-23 18:12:12.217000 UTC,,737,3,0,116,,,,,,['mlflow']
How does one save a plot in wandb with wandb.log?,"<p>I'm trying to save a plot with wandb.log. Their <a href=""https://docs.wandb.ai/guides/track/log/plots"" rel=""nofollow noreferrer"">docs</a> say to do:</p>
<pre><code>    wandb.log({&quot;chart&quot;: plt})
</code></pre>
<p>but this fails for me.</p>
<p>I get two errors, 1st error (when I do NOT do <code>plt.show()</code> before trying to do wand.log):</p>
<pre><code>Traceback (most recent call last):
  File &quot;/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_exec2.py&quot;, line 3, in Exec
    exec(exp, global_vars, local_vars)
  File &quot;&lt;input&gt;&quot;, line 1, in &lt;module&gt;
  File &quot;/Users/brandomiranda/opt/anaconda3/envs/meta_learning/lib/python3.9/site-packages/wandb/sdk/wandb_run.py&quot;, line 256, in wrapper
    return func(self, *args, **kwargs)
  File &quot;/Users/brandomiranda/opt/anaconda3/envs/meta_learning/lib/python3.9/site-packages/wandb/sdk/wandb_run.py&quot;, line 222, in wrapper
    return func(self, *args, **kwargs)
  File &quot;/Users/brandomiranda/opt/anaconda3/envs/meta_learning/lib/python3.9/site-packages/wandb/sdk/wandb_run.py&quot;, line 1548, in log
    self._log(data=data, step=step, commit=commit)
  File &quot;/Users/brandomiranda/opt/anaconda3/envs/meta_learning/lib/python3.9/site-packages/wandb/sdk/wandb_run.py&quot;, line 1339, in _log
    self._partial_history_callback(data, step, commit)
  File &quot;/Users/brandomiranda/opt/anaconda3/envs/meta_learning/lib/python3.9/site-packages/wandb/sdk/wandb_run.py&quot;, line 1228, in _partial_history_callback
    self._backend.interface.publish_partial_history(
  File &quot;/Users/brandomiranda/opt/anaconda3/envs/meta_learning/lib/python3.9/site-packages/wandb/sdk/interface/interface.py&quot;, line 541, in publish_partial_history
    data = history_dict_to_json(run, data, step=user_step, ignore_copy_err=True)
  File &quot;/Users/brandomiranda/opt/anaconda3/envs/meta_learning/lib/python3.9/site-packages/wandb/sdk/data_types/utils.py&quot;, line 54, in history_dict_to_json
    payload[key] = val_to_json(
  File &quot;/Users/brandomiranda/opt/anaconda3/envs/meta_learning/lib/python3.9/site-packages/wandb/sdk/data_types/utils.py&quot;, line 82, in val_to_json
    val = Plotly.make_plot_media(val)
  File &quot;/Users/brandomiranda/opt/anaconda3/envs/meta_learning/lib/python3.9/site-packages/wandb/sdk/data_types/plotly.py&quot;, line 48, in make_plot_media
    val = util.matplotlib_to_plotly(val)
  File &quot;/Users/brandomiranda/opt/anaconda3/envs/meta_learning/lib/python3.9/site-packages/wandb/util.py&quot;, line 560, in matplotlib_to_plotly
    return tools.mpl_to_plotly(obj)
  File &quot;/Users/brandomiranda/opt/anaconda3/envs/meta_learning/lib/python3.9/site-packages/plotly/tools.py&quot;, line 112, in mpl_to_plotly
    matplotlylib.Exporter(renderer).run(fig)
  File &quot;/Users/brandomiranda/opt/anaconda3/envs/meta_learning/lib/python3.9/site-packages/plotly/matplotlylib/mplexporter/exporter.py&quot;, line 53, in run
    self.crawl_fig(fig)
  File &quot;/Users/brandomiranda/opt/anaconda3/envs/meta_learning/lib/python3.9/site-packages/plotly/matplotlylib/mplexporter/exporter.py&quot;, line 124, in crawl_fig
    self.crawl_ax(ax)
  File &quot;/Users/brandomiranda/opt/anaconda3/envs/meta_learning/lib/python3.9/site-packages/plotly/matplotlylib/mplexporter/exporter.py&quot;, line 146, in crawl_ax
    self.draw_collection(ax, collection)
  File &quot;/Users/brandomiranda/opt/anaconda3/envs/meta_learning/lib/python3.9/site-packages/plotly/matplotlylib/mplexporter/exporter.py&quot;, line 289, in draw_collection
    offset_order = offset_dict[collection.get_offset_position()]
AttributeError: 'LineCollection' object has no attribute 'get_offset_position'
</code></pre>
<p>I get two errors, 2nd error (when I DO <code>plt.show()</code> before trying to do wand.log):</p>
<pre><code>Traceback (most recent call last):
  File &quot;/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_exec2.py&quot;, line 3, in Exec
    exec(exp, global_vars, local_vars)
  File &quot;&lt;input&gt;&quot;, line 1, in &lt;module&gt;
  File &quot;/Users/brandomiranda/opt/anaconda3/envs/meta_learning/lib/python3.9/site-packages/wandb/sdk/wandb_run.py&quot;, line 256, in wrapper
    return func(self, *args, **kwargs)
  File &quot;/Users/brandomiranda/opt/anaconda3/envs/meta_learning/lib/python3.9/site-packages/wandb/sdk/wandb_run.py&quot;, line 222, in wrapper
    return func(self, *args, **kwargs)
  File &quot;/Users/brandomiranda/opt/anaconda3/envs/meta_learning/lib/python3.9/site-packages/wandb/sdk/wandb_run.py&quot;, line 1548, in log
    self._log(data=data, step=step, commit=commit)
  File &quot;/Users/brandomiranda/opt/anaconda3/envs/meta_learning/lib/python3.9/site-packages/wandb/sdk/wandb_run.py&quot;, line 1339, in _log
    self._partial_history_callback(data, step, commit)
  File &quot;/Users/brandomiranda/opt/anaconda3/envs/meta_learning/lib/python3.9/site-packages/wandb/sdk/wandb_run.py&quot;, line 1228, in _partial_history_callback
    self._backend.interface.publish_partial_history(
  File &quot;/Users/brandomiranda/opt/anaconda3/envs/meta_learning/lib/python3.9/site-packages/wandb/sdk/interface/interface.py&quot;, line 541, in publish_partial_history
    data = history_dict_to_json(run, data, step=user_step, ignore_copy_err=True)
  File &quot;/Users/brandomiranda/opt/anaconda3/envs/meta_learning/lib/python3.9/site-packages/wandb/sdk/data_types/utils.py&quot;, line 54, in history_dict_to_json
    payload[key] = val_to_json(
  File &quot;/Users/brandomiranda/opt/anaconda3/envs/meta_learning/lib/python3.9/site-packages/wandb/sdk/data_types/utils.py&quot;, line 82, in val_to_json
    val = Plotly.make_plot_media(val)
  File &quot;/Users/brandomiranda/opt/anaconda3/envs/meta_learning/lib/python3.9/site-packages/wandb/sdk/data_types/plotly.py&quot;, line 48, in make_plot_media
    val = util.matplotlib_to_plotly(val)
  File &quot;/Users/brandomiranda/opt/anaconda3/envs/meta_learning/lib/python3.9/site-packages/wandb/util.py&quot;, line 560, in matplotlib_to_plotly
    return tools.mpl_to_plotly(obj)
  File &quot;/Users/brandomiranda/opt/anaconda3/envs/meta_learning/lib/python3.9/site-packages/plotly/tools.py&quot;, line 112, in mpl_to_plotly
    matplotlylib.Exporter(renderer).run(fig)
  File &quot;/Users/brandomiranda/opt/anaconda3/envs/meta_learning/lib/python3.9/site-packages/plotly/matplotlylib/mplexporter/exporter.py&quot;, line 53, in run
    self.crawl_fig(fig)
  File &quot;/Users/brandomiranda/opt/anaconda3/envs/meta_learning/lib/python3.9/site-packages/plotly/matplotlylib/mplexporter/exporter.py&quot;, line 122, in crawl_fig
    with self.renderer.draw_figure(fig=fig, props=utils.get_figure_properties(fig)):
  File &quot;/Users/brandomiranda/opt/anaconda3/envs/meta_learning/lib/python3.9/contextlib.py&quot;, line 119, in __enter__
    return next(self.gen)
  File &quot;/Users/brandomiranda/opt/anaconda3/envs/meta_learning/lib/python3.9/site-packages/plotly/matplotlylib/mplexporter/renderers/base.py&quot;, line 45, in draw_figure
    self.open_figure(fig=fig, props=props)
  File &quot;/Users/brandomiranda/opt/anaconda3/envs/meta_learning/lib/python3.9/site-packages/plotly/matplotlylib/renderer.py&quot;, line 90, in open_figure
    self.mpl_x_bounds, self.mpl_y_bounds = mpltools.get_axes_bounds(fig)
  File &quot;/Users/brandomiranda/opt/anaconda3/envs/meta_learning/lib/python3.9/site-packages/plotly/matplotlylib/mpltools.py&quot;, line 265, in get_axes_bounds
    x_min, y_min, x_max, y_max = min(x_min), min(y_min), max(x_max), max(y_max)
ValueError: min() arg is an empty sequence
</code></pre>
<p>Note that their trivial example DOES work:</p>
<pre><code>import matplotlib.pyplot as plt

plt.plot([1, 2, 3, 4])
plt.ylabel(&quot;some interesting numbers&quot;)
wandb.log({&quot;chart&quot;: plt})
</code></pre>
<p>for me.</p>
<hr />
<p>cross posted: <a href=""https://community.wandb.ai/t/how-does-one-save-a-plot-in-wandb-with-wandb-log/2373"" rel=""nofollow noreferrer"">https://community.wandb.ai/t/how-does-one-save-a-plot-in-wandb-with-wandb-log/2373</a></p>",1,0,2022-05-05 22:15:04.610000 UTC,,,1,python|wandb,601,2012-06-21 21:22:10.287000 UTC,2022-09-25 02:41:28.613000 UTC,,11435,1807,299,6472,,,,,,['wandb']
Setting max run time for a SageMaker HyperParameter Tuning job,"<p>My training jobs only run for a minute or two so I have increased the resource limit so I can run a large number (500) in parallel. However, I would like to set some upper bound so I don't accidentally have them run for several hours times 500....</p>
<p>From the <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-limits.html"" rel=""nofollow noreferrer"">documentation</a> I can find the following</p>
<blockquote>
<p>Maximum run time for a hyperparameter tuning job: 30 days</p>
</blockquote>
<p>30 days is def too much lol but how can I change it? Would love to just be able to set it to stop if it hits a maximum total training time, but unlike the other limits there's no mention that this can changed.</p>",1,0,2021-11-10 17:55:52.373000 UTC,,,0,amazon-web-services|amazon-sagemaker|hyperparameters,151,2015-01-15 17:43:03.700000 UTC,2022-08-23 22:54:25.603000 UTC,,1387,51,1,153,,,,,,['amazon-sagemaker']
How to add more metrics to a finished MLflow run?,"<p>Once an MLflow run is finished, external scripts can access its parameters and metrics using python <code>mlflow</code> client and <code>mlflow.get_run(run_id)</code> method, but the <code>Run</code> object returned by <code>get_run</code> seems to be read-only.</p>
<p>Specifically, <code>.log_param</code> <code>.log_metric</code>, or <code>.log_artifact</code> cannot be used on the object returned by <code>get_run</code>, raising errors like these:</p>
<pre><code>AttributeError: 'Run' object has no attribute 'log_param'
</code></pre>
<p>If we attempt to run any of the <code>.log_*</code> methods on <code>mlflow</code>, it would log them into to a new run  with auto-generated run ID in the <code>Default</code> experiment.</p>
<p>Example:</p>
<pre><code>final_model_mlflow_run = mlflow.get_run(final_model_mlflow_run_id)

with mlflow.ActiveRun(run=final_model_mlflow_run) as myrun:    
    
    # this read operation uses correct run
    run_id = myrun.info.run_id
    print(run_id)
    
    # this write operation writes to a new run 
    # (with auto-generated random run ID) 
    # in the &quot;Default&quot; experiment (with exp. ID of 0)
    mlflow.log_param(&quot;test3&quot;, &quot;This is a test&quot;)
   
</code></pre>
<p>Note that the above problem exists regardless of the <code>Run</code> status (<code>.info.status</code> can be both &quot;FINISHED&quot; or &quot;RUNNING&quot;, without making any difference).</p>
<p>I wonder if this read-only behavior is by design (given that immutable modeling runs improve experiments reproducibility)? I can appreciate that, but it also goes against code modularity if everything has to be done within a single monolith like the <code>with mlflow.start_run()</code> context...</p>",1,1,2021-03-29 14:12:06.043000 UTC,,2021-03-29 17:50:32.537000 UTC,3,python|databricks|mlflow,1269,2018-06-19 12:32:08.930000 UTC,2022-09-24 20:14:44.457000 UTC,EU,3260,1100,4,466,,,,,,['mlflow']
How to handle the frequent changes in dataset in azure Machine Learning studio?,"<p>How to handle the frequent changes in the dataset in Azure Machine Learning Studio. My dataset may change over time, I need to add more rows to dataset. How will I <strong>refresh the dataset</strong> which I currently use to train the model by using the <em>newly updated dataset</em>. I need this work to be done programmatically(in c# or python) instead of doing it manually in the studio.</p>",2,1,2020-03-12 11:01:07.727000 UTC,,2020-03-14 01:10:19.790000 UTC,0,python|azure|azure-machine-learning-studio|azure-machine-learning-service,763,2018-06-27 07:29:42.977000 UTC,2022-02-24 13:31:33.520000 UTC,"Chennai, Tamil Nadu, India",139,14,0,37,,,,,,"['azure-machine-learning-studio', 'azure-machine-learning-service']"
"What is the advantage of DVC, git-annex, git-lfs for large or binary files over git?","<p>If I have different versions of a file, e.g., in different branches, and I try to reconcile those, git will has great mechanisms for that. However, in order to do the reconciliations, e.g., in a merge, git requires access to the &quot;inside&quot; of the file. Thus files should be text files.</p>
<p>If I change a version controlled file, git does not save the delta between those files, but safes and entire snapshot of the file. If one makes a change, even a small change, to a large file, the entire files will be stored twice by git. Thus files should be small.</p>
<p>Files that are either large or binary (or both), they should not be tracked by Git. If I still need them in my project, I should use something like DVC, git-annex, git-lfs.</p>
<p>As far as I understand, all three of those keep the those other files outside of git, and keep a reference, which is tracked by git. I will use DVC as a stand-in, as I know even less about the other two.</p>
<ol>
<li><p>In DVC, the reference is a text file and thus, git will not get confused. However, since it is only a reference, there is not much merging to be done by git anyways. So, git's reconciliation-capabilities are not really required. What is the advantage of using DVC then regarding this aspect? Can't I just use git and just not use those mechanisms?</p>
</li>
<li><p>In DVC, it seems that if I change a large file, just like in git, a snapshot of that file is created (not a delta saved). So, how does this improve the situation compared to git? I still get lots of (near) copies of this big file.</p>
</li>
</ol>
<p>I understand from <a href=""https://stackoverflow.com/a/35578715/4533188"">here</a> that git-lfs keeps most of the (near) copies of my file in the remote storage. Only if I checkout the respective version of the large file, the files is downloaded. In that case, while I would be correct about my point 2, at least it is only a &quot;problem&quot; of the server (in terms of space), but not on my local disk space and also not for the internet bandwidth usage. This might be the same for DVC.</p>
<p>Are my &quot;objections&quot; or &quot;caveats&quot; of the points 1 and 2 valid?</p>",1,8,2022-03-29 13:52:24.137000 UTC,,,0,git|git-lfs|dvc|git-annex,370,2015-02-05 13:50:19.917000 UTC,2022-09-23 12:45:06.050000 UTC,,11374,415,2,845,,,,,,['dvc']
How do you use pyodbc in Azure Machine Learning Workbench,"<p>I'm trying to use pyodbc to import a dataframe in Azure ML Workbench. This works in local runs, but not for docker. It fails when trying to establish a connection to the SQL Server, because the driver is not present.</p>

<pre><code>cnxn = pyodbc.connect('DRIVER='{ODBC Driver 13 for SQL Server}';PORT=1433;SERVER='+server+';PORT=1443;DATABASE='+database+';UID='+username+';PWD='+ password)
</code></pre>

<p>Error Message:</p>

<blockquote>
  <p>pyodbc.Error: ('01000', ""[01000] [unixODBC][Driver Manager]Can't open
  lib 'ODBC Driver 13 for SQL Server' : file not found (0)
  (SQLDriverConnect)"")</p>
</blockquote>

<p>When searching for a solution i found that i could put these lines in the docker file</p>

<blockquote>
  <p>ADD odbcinst.ini /etc/odbcinst.ini</p>
  
  <p>RUN apt-get update</p>
  
  <p>RUN apt-get install -y tdsodbc unixodbc-dev</p>
  
  <p>RUN apt install unixodbc-bin -y</p>
  
  <p>RUN apt-get clean -y</p>
</blockquote>

<p>However I'm new to docker, and cannot figure out where to put these lines in the ML Workbench. It seems the docker file is generated through <strong>docker.compute</strong> and <strong>conda_dependencies.yml</strong>, but nothing similar to the lines above can be found in either of those or anywere else in the solution.</p>",1,0,2018-02-20 12:52:29.430000 UTC,,2018-02-20 13:00:50.637000 UTC,1,docker|pyodbc|azure-machine-learning-studio|azure-machine-learning-workbench,942,2018-02-20 12:23:51.500000 UTC,2018-04-06 13:00:12.463000 UTC,,11,0,0,3,,,,,,"['azure-machine-learning-workbench', 'azure-machine-learning-studio']"
Access large BDF and EDF files stored on S3 from Sagemaker by URL and read them with mne library,"<p>I have a bucket on <a href=""https://aws.amazon.com/s3/"" rel=""nofollow noreferrer""><strong>S3</strong></a> and a <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/gs-console.html"" rel=""nofollow noreferrer""><strong>SageMaker</strong></a> notebook. I try to get access to rather large <em>BDF</em> and <em>EDF</em> files (1-2 GB), stored on the <strong>S3</strong> bucket, without uploading them to <strong>SageMaker</strong> volume.
I also need to access these files by URL as the <em>EDF</em> processing function <a href=""https://mne.tools/stable/generated/mne.io.read_raw_edf.html"" rel=""nofollow noreferrer""><code>mne.io.read_raw_edf</code></a> receives the absolute path to a file as an input.</p>
<p>The <strong>S3</strong> bucket is in the same region as the <strong>Sagemaker Notebook Instance</strong>. The IAM role associated with the notebook instance is given permission to access the <strong>S3</strong> bucket.</p>
<p>First, I tried to use the approach from <a href=""https://stackoverflow.com/questions/48264656/load-s3-data-into-aws-sagemaker-notebook"">this question</a>, that describes how to read <code>.csv</code> files. Although in my case it worked well with <code>.csv</code> files, it failed with <code>.edf</code> ones.</p>
<pre><code>import pandas as pd
from sagemaker import get_execution_role

role = get_execution_role()

bucket='my-bucket'
key = 'train.edf'
data_location = 's3://{}/{}'.format(bucket, key)

mne.io.read_raw_edf(data_location)
</code></pre>
<p>When I execute this code, I receive the following error:</p>
<pre><code>FileNotFoundError: [Errno 2] No such file or directory: '/home/ec2-user/SageMaker/s3:/my-bucket/train.edf'
</code></pre>
<p>Here I face a path stacking that was not done by me. I don't quite understand why <code>pd.read_csv</code> read paths normally, unlike <code>mne.io.read_raw_edf</code>, which seems to stack the local path and the server one.</p>
<p>Then I found <a href=""https://stackoverflow.com/a/34698521/6936382"">an answer</a> to a very similar question but faced a very similar problem with the stacked paths.</p>
<pre><code>import boto3

bucket_location = boto3.client('s3').get_bucket_location(Bucket=bucket)
object_url = &quot;https://s3-{0}.amazonaws.com/{1}/{2}&quot;.format(
    bucket_location['LocationConstraint'],
    bucket,
    key)
object_url
</code></pre>
<pre><code>'https://s3-us-west-2.amazonaws.com/my-bucket/train.edf'
</code></pre>
<p>Here we can see that the path is stored normally.</p>
<pre><code>mne.io.read_raw_edf(object_url)
</code></pre>
<p>When I execute this code, I receive the following error:</p>
<pre><code>FileNotFoundError: [Errno 2] No such file or directory: '/home/ec2-user/SageMaker/https:/s3-us-west-2.amazonaws.com/my-bucket/train.edf'
</code></pre>
<p><code>mne.io.read_raw_edf</code> performs the weird stacking again.</p>
<p>At last, I tried to follow the approach described in <a href=""https://towardsdatascience.com/working-with-amazon-s3-buckets-with-boto3-785252ea22e0"" rel=""nofollow noreferrer"">this article</a>.</p>
<pre><code>s3 = boto3.client(&quot;s3&quot;, 
                  region_name='us-west-2', 
                  aws_access_key_id='access_key_id', 
                  aws_secret_access_key='secret_access_key')

share_url = s3.generate_presigned_url(ClientMethod=&quot;get_object&quot;, 
                                      ExpiresIn=3600,
                                      Params={&quot;Bucket&quot;: bucket, &quot;Key&quot;: key})
share_url
</code></pre>
<pre><code>'https://my-bucket.s3.amazonaws.com/train.edf?AWSAccessKeyId=access_key_id&amp;Signature=signature&amp;Expires=1616777253'
</code></pre>
<p>The path seems to be normal again.</p>
<pre><code>mne.io.read_raw_edf(share_url)
</code></pre>
<pre><code>NotImplementedError: Only EDF files are supported by read_raw_edf, got edf?awsaccesskeyid=access_key_id&amp;
signature=signature&amp;expires=1616777253
</code></pre>
<p>But here I got another weird <code>mne.io.read_raw_edf</code> behavior. No more stacking but the path was cropped.</p>
<p>I assume that it could be <code>mne.io.read_raw_edf</code> problem itself, but I have never faced anything like this outside <strong>Amazon</strong> products.</p>
<p>Does it make sense to get access to the <em>BDF</em> and <em>EDF</em> files by URLs or is it better to upload the files to the <strong>SageMaker</strong> volume? I apologize if this question seems naive. I've already spent a couple of days solving this problem, but I need to sort it out as soon as possible as <strong>Amazon</strong> takes money for every hour when the notebook is active.</p>",0,1,2021-03-26 16:35:33.403000 UTC,,2021-03-27 12:13:32.310000 UTC,2,python|amazon-web-services|amazon-s3|amazon-sagemaker|mne-python,189,2016-10-07 09:21:35.700000 UTC,2021-12-09 11:06:00.100000 UTC,"Moscow, Russia",57,0,0,11,,,,,,['amazon-sagemaker']
How to create a model on model.tar.gz in SageMaker?,"<p>I want to create a model on my model artefacts (s3:/bucket/output/model.tar.gz) for the purpose of beach transform and deployment? My model is a simple random forest which I trained it using Python SDK and train script. In my train script, I just have the model_fn function and main function.</p>
<p>now I want to create a model for batch transform job using :</p>
<pre><code>from sagemaker.image_uris import retrieve
image = retrieve(region= sagemaker.Session().boto_session.region_name, framework='sklearn', version='0.23-1' )
from sagemaker.model import Model
estimator =  model.deploy(initial_instance_count = 1 , instance_type = 'ml.p2.xlarge')
</code></pre>
<p>and I am getting this error</p>
<pre><code>Error hosting endpoint sagemaker-scikit-learn-2021-05-14-19-43-21-320: Failed. Reason:  The primary container for production variant AllTraffic did not pass the ping health check. Please check CloudWatch logs for this endpoint...
</code></pre>
<p>Also, I tried to get a transform job and my job kept running forever with this error</p>
<pre><code>transformer = model.transformer(instance_count=1, instance_type=&quot;ml.m5.xlarge&quot;)
transformer.transform('address to s3 input')
</code></pre>
<p>Error :</p>
<pre><code>Traceback (most recent call last):
File &quot;/miniconda3/lib/python3.7/site-packages/gunicorn/workers/base_async.py&quot;, line 55, in handle
self.handle_request(listener_name, req, client, addr)
File &quot;/miniconda3/lib/python3.7/site-packages/gunicorn/workers/ggevent.py&quot;, line 143, in handle_request
super().handle_request(listener_name, req, sock, addr)
File &quot;/miniconda3/lib/python3.7/site-packages/gunicorn/workers/base_async.py&quot;, line 106, in handle_request
respiter = self.wsgi(environ, resp.start_response)
File &quot;/miniconda3/lib/python3.7/site-packages/sagemaker_sklearn_container/serving.py&quot;, line 128, in main
serving_env.module_dir)
File &quot;/miniconda3/lib/python3.7/site-packages/sagemaker_sklearn_container/serving.py&quot;, line 105, in import_module
user_module = importlib.import_module(module_name)
File &quot;/miniconda3/lib/python3.7/importlib/__init__.py&quot;, line 118, in import_module
if name.startswith('.'):

AttributeError: 'NoneType' object has no attribute 'startswith'
</code></pre>
<ul>
<li>Should I use Sagemaker.model.SKLearnModel? In that case, what's the difference between them?<br />
If I want to use SKLearnModel then I need to have inference.py, how would it look like? Any sample would be appreciated,</li>
<li>why don't I need it if immediately after training I deploy and create a transform job, is that because the model_fn is in my train script?</li>
<li>Is not having input_fn and output_fn and predict_fn  in my train script a source of the problem?</li>
</ul>",1,0,2021-05-14 23:11:09.813000 UTC,,2021-05-15 17:09:27.880000 UTC,1,python|amazon-web-services|scikit-learn|sdk|amazon-sagemaker,1338,2020-04-30 04:20:29.970000 UTC,2022-09-14 06:58:40.450000 UTC,,71,0,0,8,,,,,,['amazon-sagemaker']
Error: The streaming job failed: Stream Analytics job has validation errors: Multiple input columns to the end point is not currently supported,"<p>For the project I have 2 ML models. In the input stream, in stream analytics I get my data and the type of data. I have deployed an Azure Machine Learning Endpoint with ACI. This contains both my models. I followed this link for the multiple models deployment :</p>
<p><a href=""https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/deployment/deploy-multi-model/multi-model-register-and-deploy.ipynb"" rel=""nofollow noreferrer"">https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/deployment/deploy-multi-model/multi-model-register-and-deploy.ipynb</a></p>
<p>And I used this link to have a second input in my run function, and also my web service for that matter. Here I want to provide the type of data so I can choose the right model for prediction for that data.I used the offical Microsoft documentation for this.</p>
<p><a href=""https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-advanced-entry-script#power-bi-compatible-endpoint"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-advanced-entry-script#power-bi-compatible-endpoint</a></p>
<p>When I try to run the Stream, I get this error:</p>
<p>The streaming job failed: Stream Analytics job has validation errors: Multiple input columns to the end point is not currently supported.</p>
<p>The last link also provides information as to how to generate swagger files automatically, which is something needed for my project as well.</p>
<p>If you know how to get around this issue, or if you know a way to switch between models easily, either in the run function, or in the Stream Analytics Query. Please let me know.</p>
<p>For the choosing a model in the Query, the issue is that only one ML function is allowed per query step.</p>",1,0,2021-06-07 12:13:04.593000 UTC,,,0,azure|azure-stream-analytics|azure-machine-learning-service,368,2020-04-03 11:20:49.467000 UTC,2021-06-22 11:19:23.783000 UTC,,1,0,0,0,,,,,,['azure-machine-learning-service']
How to search for Amazon SageMaker Models using Tags?,"<p>When I open the Model details in the Amazon SageMaker console,(Amazon SageMaker-&gt;Inference-&gt;Models), the details clearly show Tags that have been added to the model during it's creation.</p>
<p><a href=""https://i.stack.imgur.com/iasM4.png"" rel=""nofollow noreferrer"">Tag details in the Amazon SageMaker Models console</a></p>
<p>But the Search API provided by Amazon SageMaker, <a href=""https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_Search.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_Search.html</a></p>
<p>mentions only the following resources can be searched for using Tags:</p>
<blockquote>
<p>Valid Values: TrainingJob | Experiment | ExperimentTrial |
ExperimentTrialComponent | Endpoint | ModelPackage | ModelPackageGroup
| Pipeline | PipelineExecution | FeatureGroup | Project |
FeatureMetadata</p>
</blockquote>
<p>I wish to obtain Model details, not the ModelPackageGroup/ModelPackage details, using Tags so if there is a way to do that, please share. Also if there is no way to obtain it using Tags, like the Search API Documentation suggests, what is the purpose of the Tags still present in the Model details?</p>",1,0,2022-07-04 18:06:40.213000 UTC,,2022-07-04 18:07:31.903000 UTC,0,amazon-web-services|amazon-sagemaker|aws-codebuild|aws-code-deploy|aws-codeartifact,61,2020-12-26 07:35:25.447000 UTC,2022-09-06 06:52:27.550000 UTC,,1,0,0,2,,,,,,['amazon-sagemaker']
How to deploy the best tuned model using sagemaker pipelines?,"<p>I have trained an XGBoost model, finetuned it, evaluated it and registered it using aws sagemaker pipeline. Now I want to deploy the model. However, the location of the model artefact is saved as <code>Join</code> object because of which deployment is not working by <code>best_model.deploy(...)</code>. Any suggestions on how to deploy the best trained model.</p>
<pre><code>tuning_step = TuningStep(name=&quot;HPTuning&quot;,
                        tuner=tuner_log,
                        inputs={
                            &quot;train&quot;:...,
                            &quot;validation&quot;:...
                        },
                        cache_config=cache_config)


best_model = Model(image_uri=image_uri, 
                  model_data=tuning_step.get_top_model_s3_uri(top_k=0,s3_bucket=model_bucket_key),
                  sagemaker_session=sm_session,
                  role=role,
                  predictor_cls=XGBoostPredictor)


register_step = RegisterModel(name=&quot;RegisterBestChurnModel&quot;,
                             estimator=xgb_estimator,
                             model_data=tuning_step.get_top_model_s3_uri(top_k=0, s3_bucket=model_bucket_key),
                             content_types=[&quot;text/csv&quot;],
                             response_types=[&quot;test/csv&quot;],
                             inference_instances=[&quot;ml.t2.medium&quot;, &quot;ml.m5.large&quot;],
                             transform_instances=[&quot;ml.m5.large&quot;],
                             approval_status=&quot;Approved&quot;,
                             model_metrics=model_metrics)


</code></pre>
<p>The problem with <code>best_model.deploy(...)</code> is that <code>tuning_step.get_top_model_s3_uri(top_k=0,s3_bucket=model_bucket_key)</code> is a Join object. And deploy needs the s3 bucket location as a string. So that doesn't work.</p>
<p>I was also trying to deploy the registered model using</p>
<pre><code>model_package_arn = register_step.properties.ModelPackageArn,
model = ModelPackage(role=role, 
                     model_package_arn=create_top_step.properties.ModelArn, 
                     sagemaker_session=session)
model.deploy(initial_instance_count=1, instance_type='ml.m5.xlarge')
</code></pre>
<p>which is giving me the error</p>
<pre><code>...

/opt/conda/lib/python3.7/site-packages/sagemaker/model.py in _create_sagemaker_model(self, *args, **kwargs)
   1532             container_def[&quot;Environment&quot;] = self.env
   1533 
-&gt; 1534         self._ensure_base_name_if_needed(model_package_name.split(&quot;/&quot;)[-1])
   1535         self._set_model_name_if_needed()
   1536 

AttributeError: 'tuple' object has no attribute 'split'
</code></pre>
<p>Which I also suspect arise for the same reason.</p>
<p>I followed this tutorial pretty much</p>
<p><a href=""https://github.com/aws/amazon-sagemaker-examples/blob/main/sagemaker-pipelines/tabular/tuning-step/sagemaker-pipelines-tuning-step.ipynb"" rel=""nofollow noreferrer"">https://github.com/aws/amazon-sagemaker-examples/blob/main/sagemaker-pipelines/tabular/tuning-step/sagemaker-pipelines-tuning-step.ipynb</a></p>",1,0,2022-07-01 12:39:53.943000 UTC,,,1,python|amazon-web-services|deployment|pipeline|amazon-sagemaker,79,2022-06-23 11:22:18.203000 UTC,2022-08-17 05:32:40.397000 UTC,,21,0,0,1,,,,,,['amazon-sagemaker']
how to read/load a pdf file in jupyter notebook and display second page of pdf file in jupyter notebook in python,"<p>How to read/load  a pdf file in jupyter notebook  and display second page of sample.pdf file in jupyter notebook  in python</p>
<p>I tried the below links but not get proper result</p>
<ol>
<li><a href=""https://stackoverflow.com/questions/55251691/how-to-open-pdf-file-in-jupyter-file-browser"">how to open pdf file in jupyter file browser?</a></li>
<li><a href=""https://stackoverflow.com/questions/19470099/view-pdf-image-in-an-ipython-notebook"">View pdf image in an iPython Notebook</a></li>
</ol>
<p>if any help appreciated</p>
<p>Download sample file : <a href=""https://www.africau.edu/images/default/sample.pdf"" rel=""nofollow noreferrer"">sample.pdf</a></p>",0,0,2022-09-19 16:24:32.307000 UTC,,,0,python-3.x|jupyter-notebook|ipython|amazon-sagemaker|wand,18,2016-01-25 11:09:52.560000 UTC,2022-09-24 17:07:59.083000 UTC,"Chennai, Tamil Nadu, India",1126,1263,5,111,,,,,,['amazon-sagemaker']
How to set multiple environment variables in AWS Sagemaker notebook instance?,"<p>I am using the AWS Sagemaker notebook instances for some of my experiments. As I am also using the lifecycle configurations scripts that are executed during notebook startup and also want to set some environment variables.</p>

<p>For some reason, when I set multiple env variables in the lifecycle shell script, they are not set by the instance, i.e. when I execute
<code>echo $FOO</code> 
the relevant variable is not printed. </p>

<p>However, when I set only one env variable it is working and I can use it in my notebook session. </p>

<p>My understanding is that I start the desired Kernel, </p>

<p>I have also tried to set the env variables inside the notebook by running <code>export FOO=BAR</code> but that also did not work. 
Following the example script provided by AWS, I made my changes to set the variables, however when I print $FOO, it doesn't seem to be displayed.</p>

<p>I have tried setting the envs before and after switching to ec2-user (before the commands are executed as root), still nothing helped.</p>

<pre class=""lang-sh prettyprint-override""><code>#!/bin/bash


cd /home/ec2-user/anaconda3/envs/python3
mkdir -p ./etc/conda/activate.d
mkdir -p ./etc/conda/deactivate.d
touch ./etc/conda/activate.d/env_vars.sh
touch ./etc/conda/deactivate.d/env_vars.sh

echo export FOO=BAR &gt;&gt; ./etc/conda/activate.d/env_vars.sh
echo unset FOO &gt;&gt; ./etc/conda/deactivate.d/env_vars.sh

echo export FOO2=BAR2 &gt;&gt; ./etc/conda/activate.d/env_vars.sh
echo unset FOO2 &gt;&gt; ./etc/conda/deactivate.d/env_vars.sh

sudo -u ec2-user -i &lt;&lt;'EOF'

cd /home/ec2-user

# This will affect only the Jupyter kernel called ""conda_python3"".
source activate python3

pip install --upgrade pip

pip install scipy xgboost sklearn

# You can also perform ""conda install"" here as well.


source deactivate

EOF


</code></pre>

<p>I want to set multiple environment variables for this Sagemaker notebook upon start, what is the best way to do this?</p>",2,2,2019-08-05 15:49:07.883000 UTC,2.0,2019-11-20 09:39:32.027000 UTC,4,amazon-web-services|jupyter-notebook|conda|amazon-sagemaker,5171,2015-05-19 08:43:13.980000 UTC,2020-10-20 17:52:26.113000 UTC,"Berlin, Germany",411,2,0,6,,,,,,['amazon-sagemaker']
Can we Implement access management in ML Flow operational tool so that multiple users can access their own experiments only?,"<p>ML flow is a platform to track the ML Model, Can we have access management with the ML flow library so that multiple users can access the tool separately in one server.</p>
<p>Please advise.</p>",0,0,2021-12-13 15:13:54.513000 UTC,,,0,mlflow|aiml,147,2016-03-15 04:49:00.367000 UTC,2021-12-14 05:18:48.897000 UTC,"Bangalore, Karnataka, India",1,0,0,3,,,,,,['mlflow']
Databricks: Migrate a registered model from one workspace to another?,"<p>We have multiple Databricks Workspaces on Azure. On one of them we trained multiple models and registered them in the MLflow registry. Our goal is to move those model from one databricks workspace to another and so far, i could not find a straight forwared way to do this except running the training script again on the new databricks workspace.</p>
<p>Downloading the model an registering them in the new workspace didn't work so far. Should I create a &quot;dummy&quot; training script, that just loads the model, does nothing with it and then logs it away in the new workspace?</p>
<p>Seems to me like databricks never anticipated, that someone might want to migrate ML models?</p>",2,0,2021-08-25 06:59:47.863000 UTC,,2021-08-25 07:42:09.853000 UTC,1,azure|machine-learning|migration|databricks|mlflow,588,2017-01-22 21:52:54.840000 UTC,2022-08-02 14:08:09.970000 UTC,,188,0,1,22,,,,,,['mlflow']
How to Configure a Boosted Tree Model,"<p>I wanna configure a boosted decision tree but don't know how to do it, i know the meaning of every parameter but not the best value i have to assign to it.</p>

<ul>
<li>Here is the parameters : </li>
</ul>

<p><strong>Maximum number of leaves per tree</strong> : i put ( 60,100,200,300 )</p>

<p><strong>Minimum number of samples per leaf node</strong> : i put ( 10,30 ) </p>

<p><strong>Learning rate</strong> : i put (0,1)</p>

<p><strong>Number of trees constructed</strong> : i put (2000,5000,6000)</p>

<p><strong>Random number seed</strong> : i put (4)</p>

<p>The problem is how to know if you put the right values or if you did not overfit the model ?</p>",1,5,2017-07-12 12:40:09.310000 UTC,,,0,azure|machine-learning|decision-tree|azure-machine-learning-studio,230,2017-04-03 15:09:32.533000 UTC,2019-08-19 14:50:06.927000 UTC,"Paris, France",83,4,0,24,,,,,,['azure-machine-learning-studio']
Writing a custom predict method using MLFlow and pyspark,"<p>I am having trouble writing a custom predict method using MLFlow and pyspark (2.4.0). What I have so far is a custom transformer that changes the data into the format I need.</p>
<pre><code>class CustomGroupBy(Transformer):
    def __init__(self):
        pass
    def _transform(self, dataset):
        df = dataset.select(&quot;userid&quot;, explode(split(&quot;widgetid&quot;, ',')).alias(&quot;widgetid&quot;))
        return(df)
</code></pre>
<p>Then I built a custom estimator to run one of the pyspark machine learning algorithms</p>
<pre><code>class PipelineFPGrowth(Estimator, HasInputCol, DefaultParamsReadable, DefaultParamsWritable): 
    def __init__(self, inputCol=None, minSupport=0.005, minConfidence=0.01):
        super(PipelineFPGrowth, self).__init__()
        self.minSupport = minSupport
        self.minConfidence = minConfidence
    def setInputCol(self, value):
        return(self._set(inputCol=value))
    def _fit(self, dataset):
        c = self.getInputCol() 
        fpgrowth = FPGrowth(itemsCol=c, minSupport=self.minSupport, minConfidence=self.minConfidence)
        model = fpgrowth.fit(dataset)
        return(model)
</code></pre>
<p>This runs in the MLFlow pipeline.</p>
<pre><code>pipeline = Pipeline(stages = [CustomGroupBy,PipelineFPGrowth]).fit(df)
</code></pre>
<p>This all works. If I create a new pyspark dataframe with new data to predict on, I get predictions.</p>
<pre><code>newDF = spark.createDataFrame([(123456,['123ABC', '789JSF'])], [&quot;userid&quot;, &quot;widgetid&quot;])
pipeline.stages[1].transform(newDF).show(3, False)

# How to access frequent itemset.
pipeline.stages[1].freqItemsets.show(3, False)
</code></pre>
<p>Where I run into problems is writing a custom predict. I need to append the frequent itemset that FPGrowth generates to the end of the predictions. I have written the logic for that, but I am having a hard time figuring out how to put it into a custom method. I have tried adding it to my custom estimator but this didn't work. Then I wrote a separate class to take in the returned model and give the extended predictions. This was also unsuccessful.</p>
<p>Eventually I need to log and save the model so I can Dockerize it, which means I will need a custom flavor and to use the pyfunc function. Does anyone have a hint on how to extend the predict method and then log and save the model?</p>",0,0,2020-10-02 09:07:10.127000 UTC,0.0,2020-10-02 09:55:20.820000 UTC,3,python|docker|machine-learning|pyspark|mlflow,331,2011-12-06 11:56:12.023000 UTC,2022-07-04 20:51:05.600000 UTC,Switzerland,45,4,0,8,,,,,,['mlflow']
CREATE MODEL with Redshift: Correlation,"<p>how can to see the correlations regarding the target column of the trained models? Is there an option in the Redshift query editor oder Sagemaker?
Cheers</p>",1,0,2022-04-28 14:42:37.347000 UTC,,2022-05-03 05:44:16.803000 UTC,0,amazon-redshift|amazon-sagemaker,56,2020-06-12 12:27:53.257000 UTC,2022-08-02 05:25:00.150000 UTC,,83,2,0,5,,,,,,['amazon-sagemaker']
Unable to connect Azure DevOps and Azure ML,"<p>I have created an automated Service Principal from the service requests on Azure Devops with sufficient permissions. Now, when I am trying to create an artifact which is an ML model (registered) it is not auto populating the registered models and resulting in an error.</p>
<p>I am using a free trial Azure account and attempting to implement CI CD for ML. I turned my firewall off and attempted as well but still the issue persists.</p>
<p><a href=""https://i.stack.imgur.com/imvGo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/imvGo.png"" alt=""enter image description here"" /></a></p>",1,0,2022-09-18 05:51:58.187000 UTC,,,0,azure|azure-devops|azure-machine-learning-service,40,2017-08-03 07:58:30.080000 UTC,2022-09-24 13:41:33.237000 UTC,,51,4,0,47,,,,,,['azure-machine-learning-service']
AUC objective metric for SageMaker Hypertuning,"<p>I have trained the binary classification model using AWS built-in algorithm with SageMaker and want to evaluate the model using the AUC and confusion matrix. However, I see that SageMaker's Training and HyperTuner job just accepts the Accuracy metric.</p>
<ol>
<li>Is there a way in SageMaker to add the custom metric for a built-in image classification algorithm?</li>
<li>As I understand AUC/Confusion Matrix/Precision/Recall/F1 are good metrics for a binary classifier, then Why these are missing in the AWS built-in image classification algorithm?</li>
<li>Is there a way where I can batch transform my test data and get these metrics to evaluate the model as Accuracy alone is not good for evaluation?</li>
</ol>",1,0,2020-09-13 04:23:03.263000 UTC,,,1,python|classification|amazon-sagemaker|multilabel-classification,338,2012-03-15 05:24:23.110000 UTC,2022-03-18 10:06:39.020000 UTC,,247,2,0,32,,,,,,['amazon-sagemaker']
Running MPI python script in MPI azure ml pipeline,"<p>I'm trying to run distributed python job through azure ML pipelines using MPIStep pipeline class, by referring to the below example link - <a href=""https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/machine-learning-pipelines/pipeline-style-transfer/pipeline-style-transfer.ipynb"" rel=""nofollow noreferrer"">https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/machine-learning-pipelines/pipeline-style-transfer/pipeline-style-transfer.ipynb</a></p>

<p>I tried implemented the same but even I change the node count parameter in MpiStep class, while running the script the it shows size (i.e comm.Get_size()) as 1 always. Can you please help me in what I'm missing here. Is there any specific setup required on the cluster?</p>

<p><strong>Code snippets:</strong></p>

<p>Pipeline code snippet:</p>

<pre><code>model_dir = model_ds.path('./'+saved_model_blob+'/',data_reference_name='saved_model_path').as_mount()
label_dir = model_ds.path('./'+model_label_blob+'/',data_reference_name='model_label_blob').as_mount()

input_images = result_ds.path('./'+score_blob_name+'/',data_reference_name='Input_images').as_mount()

output_container = 'abc'
inti_container = 'xyz'



distributed_batch_score_step = MpiStep(
    name=""batch_scoring"",
    source_directory=SCRIPT_FOLDER,
    script_name=""batch_scoring_script_mpi.py"",
    arguments=[""--dataset_path"", input_images, 
               ""--model_name"", model_dir,
               ""--label_dir"", label_dir, 
               ""--intermediate_data_container"", inti_container, 
               ""--output_container"", output_container],
    compute_target=gpu_cluster,
    inputs=[input_images, model_dir,label_dir],
    pip_packages=[""tensorflow"",""tensorflow-gpu==1.13.1"",""pillow"",""azure-keyvault"",""azure-storage-blob""],
    conda_packages=[""mesa-libgl-cos6-x86_64"",""mpi4py==3.0.2"",""opencv=3.4.2"",""scikit-learn=0.21.2""],                                 
    use_gpu=True,
    allow_reuse = False,
    node_count = nodecount_param,
    process_count_per_node = 1

)
</code></pre>

<p>Python Script code snippet:</p>

<pre><code>def run(input_dataset,comm):

rank = comm.Get_rank()
size = comm.Get_size()
print(""Rank:"" , rank)
print(""Size:"", size) # shows always 1, even the input node count is &gt;1
print(MPI.Get_processor_name())


file_names = get_file_names(args.dataset_path)
sorted(file_names)


partition_size = len(file_names) // size
print(""partition_size--&gt;"",partition_size)
partitioned_filenames = file_names[rank * partition_size: (rank + 1) * partition_size]
print(""RANK {}  - is processing {} images out of the total {}"".format(rank, len(partitioned_filenames),
                                                                     len(file_names)))

# call to Function 01

# call to Function 02

img_names = score_df['image_name'].unique()
output_batch = pd.DataFrame()
for i in img_names:
    # call to Function 3
    output_batch = output_batch.append(pp_output, ignore_index=True)
    output_paths_list = comm.gather(output_batch, root=0)



print(""RANK {} - number of pre-aggregated output files {}"".format(rank, len(output_batch)))

print(""saved in"", currentDT + '\\' + 'data.csv')

if rank == 0:
    print(""RANK {} - number of aggregated output files {}"".format(rank, len(output_paths_list)))
    print(""RANK {} - end"".format(rank))

if __name__ == ""__main__"":
    with tf.device('/GPU:0'):
        init()
        comm = MPI.COMM_WORLD
        run(args.dataset_path,comm)
</code></pre>",1,4,2019-10-02 19:19:37.863000 UTC,1.0,,0,python-3.x|azure-pipelines|mpi4py|azure-machine-learning-service,421,2015-11-26 18:08:13.347000 UTC,2021-02-01 06:34:10.567000 UTC,,11,0,0,9,,,,,,['azure-machine-learning-service']
Sagemaker Tensorflow TF Dataset Never Has Available Data for Training,"<p>I have stripped down a TF script I am trying to run on AWS Sagemaker and I cant figure out why I can train a Keras model using a dataset as input locally but as soon as I run it with the Sagemaker framework I get errors that I dont have sufficient data and training doesnt start.</p>
<p><strong>Code that processes the dataset:</strong></p>
<pre><code>import tensorflow as tf
import boto3
import re 

#parse the protobuffer
def _parse_function(proto):
    # define tfrecord again. 
    keys_to_features = {'target': tf.io.FixedLenFeature([], tf.float32),
                        'feature1': tf.io.FixedLenFeature([], tf.float32),
                        'feature2': tf.io.FixedLenFeature([], tf.float32),
                        'feature3': tf.io.FixedLenFeature([], tf.float32),
                        'feature4': tf.io.FixedLenFeature([], tf.float32)
                        }
   
    # Load one example
    parsed_features = tf.io.parse_example(proto, keys_to_features)
    
    return(parsed_features)


# means and standard deviations from the training set
hold_meanstd={
    'Feature1':[12178.24,227789.9],
    'Feature2':[0.002085435,0.01342237],
    'Feature3':[0.002572172,0.01819553],
    'Feature4':[90480.89,352610]
    }

def normalize(example):
        example['Feature1']=(example['Feature1']-hold_meanstd['Feature1'][0])/hold_meanstd['Feature1'][1]
        example['Feature2']=(example['Feature2']-hold_meanstd['Feature2'][0])/hold_meanstd['Feature2'][1]
        example['Feature3']=(example['Feature3']-hold_meanstd['Feature3'][0])/hold_meanstd['Feature3'][1]
        example['Feature4']=(example['Feature4']-hold_meanstd['Feature4'][0])/hold_meanstd['Feature4'][1]
        
        label=example.pop('target')
        
        # keras needs:  Should return a tuple of either (inputs, targets) or (inputs, targets, sample_weights).
        return([example['Feature1'],example['Feature2'],example['Feature3'],example['Feature4']],label)

    
# get tfrecords shards
def get_tfrecords(tfr_dir):
    
    s3 = boto3.resource('s3')
    _bucket = s3.Bucket(‘bucketname’)
    file_lst=[]

    prefix = tfr_dir[re.search(r&quot;[a-z]/&quot;, tfr_dir).start()+2:]
    for i, obj_summary in enumerate(_bucket.objects.filter(Prefix=prefix)):
        if not obj_summary.key.endswith('SUCCESS'):
            file_lst.append('S3-PATH/'+obj_summary.key)
    
    return(file_lst)


# load and return a dataset    
def load_ds(tfr_dir,batchsize):
    file_list = get_tfrecords(tfr_dir)
    dataset = tf.data.TFRecordDataset(file_list)
    dataset = dataset.map(_parse_function)
    dataset = dataset.map(normalize)
    dataset = dataset.shuffle(100)
    dataset= dataset.repeat()
    dataset = dataset.batch(batchsize,drop_remainder=True)
    
    
    return(dataset)
</code></pre>
<p>I create a simple Keras model (not shown, but its basic for testing purposes). Finally, I run the following. There are 5,000 records in tf records. This works fine.</p>
<pre><code>tf_model = create_keras_model(0.001,4, None)
ds = load_ds('PATH_TO_S3_tfrecords', 16)

tf_model.fit(ds, 
             epochs = 5, 
             steps_per_epoch= 5000 // 16 
            )
</code></pre>
<p>When I include the same code as above, in Sagemaker:</p>
<p><em>train_fn.py includes</em></p>
<pre><code>def train_model(args):
    
    train_ds = utilities.load_ds(args.train_dir, args.batchsize)
    tf_model = model.create_keras_model(args.learning_rate,args.inputdim, None)

    tf_model.fit(train_ds, 
                     steps_per_epoch= 5000 // 16, 
                     epochs = 5
                )



if __name__ == '__main__':
    
    args = get_args()
    train_model(args)
</code></pre>
<p>and run:</p>
<pre><code>tf_estimator = sm_tf(
    entry_point=&quot;train_fn.py&quot;,
    source_dir = './code_test',
    role=role,
    instance_count=1, 
    instance_type = &quot;ml.c4.8xlarge&quot;,
    framework_version = &quot;2.8&quot;,
    py_version = &quot;py39&quot;,
    script_mode= True,
    
    
    sagemaker_session = sagemaker_session,
    hyperparameters = hyperparams

)



tf_estimator.fit({'train'  : training_data_uri},                 
                 job_name = job_name
                 
                )
</code></pre>
<p>Error:</p>
<pre><code>WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 1560 batches). You may need to use the repeat() function when building your dataset.
WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 1560 batches). You may need to use the repeat() function when building your dataset.
Traceback (most recent call last):
  File &quot;/opt/ml/code/train_fn.py&quot;, line 68, in &lt;module&gt;
    train_model(args)
  File &quot;/opt/ml/code/train_fn.py&quot;, line 58, in train_model
tf_model.fit(train_ds, 
  File &quot;/usr/local/lib/python3.9/site-packages/keras/utils/traceback_utils.py&quot;, line 67, in error_handler
raise e.with_traceback(filtered_tb) from None
  File &quot;/usr/local/lib/python3.9/site-packages/keras/engine/training.py&quot;, line 1395, in fit
raise ValueError('Unexpected result of `train_function` '
ValueError: Unexpected result of `train_function` (Empty logs). Please use `Model.compile(..., run_eagerly=True)`, or `tf.config.run_functions_eagerly(True)` for more information of where went wrong, or file a issue/bug to `tf.keras`.
2022-05-18 12:52:22,703 sagemaker-training-toolkit ERROR    Reporting training FAILURE
2022-05-18 12:52:22,703 sagemaker-training-toolkit ERROR    ExecuteUserScriptError:
ExitCode 1
</code></pre>",0,0,2022-05-18 13:59:41.087000 UTC,,,0,tensorflow|keras|deep-learning|amazon-sagemaker,41,2011-01-10 02:55:08.597000 UTC,2022-09-22 22:11:16.413000 UTC,,1748,80,5,393,,,,,,['amazon-sagemaker']
Overfitting/Underfitting Machine Learning Models with Azure Machine Learning vs Python,"<p>I'm learning how to perform Machine Learning with Azure ML Studio. At the moment, I've only played around with Machine Learning using Python.</p>

<p>I have run identical Machine Learning projects using both Azure ML and Python to see how close the results of each product with the Root Mean Squared Errors (RMSE). So far the RMSE has been widely different for Azure ML and Python.</p>

<p>I can't figure out why the RMSE is so far apart. The only reason I can think of is because of the way Python 'fits' the model on the training data. Python uses the following code to fit the training data </p>

<pre><code>lr = LinearRegression(labelCol='xxxx')
lrModel = lr.fit(train_data)
</code></pre>

<p>However, I don't know how Azure ML fits the training data.</p>

<p>Can someone let me know how Azure ML accomplishes fitting the training data?</p>",1,3,2019-12-28 11:27:17.513000 UTC,,2019-12-28 11:30:27.940000 UTC,0,python-3.x|machine-learning|azure-machine-learning-studio,138,2016-06-25 22:07:19.907000 UTC,2021-11-21 00:16:09.640000 UTC,,866,49,1,233,,,,,,['azure-machine-learning-studio']
Run experiments on Azure ML with Kedro and Mlflow,"<p>I'm trying to run the whole Kedro pipeline as an Azure ML experiment. I had two options here. The first one was to use the built-in logging feature of Azure ML and the second one was to use the azumeml-mlflow package that integrates Azure ML with Mlflow.</p>
<p>I only tried the second approach as I did not know how to implement the Run() method of Azure ML inside the Kedro hooks.</p>
<p>So, for the second approach, I presumed everything should be the same as when using Mlflow only. However, I couldn't get it to work even though it worked well outside of the Kedro structure ==&gt; I could launch experiments from other scripts.</p>
<p>What I get with Kedro is that the pipeline runs well but nothing happens on Azure ML.</p>
<p>Here's the code (hooks are inside a ModelTrackingHooks class):</p>
<pre><code>@hook_impl
def before_pipeline_run(self, run_params: Dict[str, Any]) -&gt; None:
    &quot;&quot;&quot;Hook implementation to start an MLflow run
    with the same run_id as the Kedro pipeline run.
    &quot;&quot;&quot;


    # Get Azure workspace
    ws = Workspace.get(name=&quot;...&quot;,
                       subscription_id=&quot;...&quot;,
                       resource_group=&quot;...&quot;)
    # Set tracking uri
    mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())

    # Create an Azure ML experiment in the workspace
    experiment = Experiment(workspace=ws, name='kedro-mlflow-experiment')
    mlflow.set_experiment(experiment.name)

    #Start logging
    mlflow.start_run(run_name=run_params[&quot;run_id&quot;])
    mlflow.log_params(run_params)        

@hook_impl
def after_node_run(
    self, node: Node, outputs: Dict[str, Any], inputs: Dict[str, Any]
) -&gt; None:
    &quot;&quot;&quot;Hook implementation to add model tracking after some node runs.
    In this example, we will:
    * Log the parameters after the data splitting node runs.
    * Log the model after the model training node runs.
    * Log the model's metrics after the model evaluating node runs.
    &quot;&quot;&quot;
    
    if node._func_name == &quot;cross_val&quot;:
        mlflow.log_params(
            {&quot;best_estimator&quot;: outputs[&quot;best_estimator&quot;],
             &quot;best_params&quot;: outputs[&quot;best_params&quot;]}
        )
        model = outputs[&quot;validated_model&quot;]
        mlflow.sklearn.log_model(model, &quot;model&quot;)

    elif node._func_name == &quot;fit_and_save_transformer&quot;:
        transformer = outputs[&quot;custom_transformer&quot;]
        mlflow.sklearn.log_model(transformer, &quot;customer_transformer&quot;)

    elif node._func_name == &quot;classification_reporting&quot;:
        mlflow.log_metrics(outputs[&quot;metrics&quot;])
    

@hook_impl
def after_pipeline_run(self) -&gt; None:
    &quot;&quot;&quot;Hook implementation to end the MLflow run
    after the Kedro pipeline finishes.
    &quot;&quot;&quot;

    mlflow.end_run()
</code></pre>
<p>Am I doing it the wrong way ?</p>
<p>Do you have any idea or examples on how to use Kedro and Azure ML by leveraging only the built-in capabilities of Azure ML (i.e. without going through Mlflow) ?</p>
<p>Thank you in advance.</p>",0,0,2021-11-17 19:26:27.910000 UTC,3.0,2021-11-17 21:20:25.437000 UTC,0,python|mlflow|azure-machine-learning-service|kedro|mlops,266,2020-04-10 11:23:52.390000 UTC,2022-08-12 18:19:53.330000 UTC,,127,8,0,20,,,,,,"['mlflow', 'azure-machine-learning-service']"
Azure Machine Learning - Error producing the visualization of the output,"<p>I've recently created new experiment and set ""Azure SQL Database"" as data source. I've typed all necessary information to connect to database and I've run the experiment. Everything seems to be correct, but when I try to ""Visualise"" my data, some prompt at the bottom of the screen says:</p>

<blockquote>
  <p>Error producing the visualization of the output</p>
</blockquote>

<p>And that's all. As well when I click ""Visualise"", a window appears with empty table (column names are correct, so it's sign that it's connected to database, because it had to get names of columns from db).</p>

<p>Where should I look for some additional information about why it doesn't work? What do you think could cause a problem? How to fix it?</p>

<p><strong>EDIT:
More info:</strong></p>

<p>What I've already done is creating blank experiment and drag&amp;drop ""Reader"" from ""Data Input and Output"". In ""Reader"" I've set ""Data source"" to ""Azure SQL Database"" and I've put login credentials. Database query: ""SELECT * FROM Words"" (Words is one of db tables).</p>",1,0,2015-12-02 17:14:58.530000 UTC,,2016-02-01 04:21:09.517000 UTC,0,azure|azure-machine-learning-studio,740,2012-11-06 18:21:26.180000 UTC,2022-09-23 07:12:25.857000 UTC,,10469,626,2,707,,,,,,['azure-machine-learning-studio']
Error when createPresignedDomainUrl is called - AWS SageMaker,"<p>i'm working with nodeJs and i'm trying to get a presigned Url from SageMaker.
Here's my code:</p>
<pre><code>   const sagemaker = new AWS.SageMaker(); 
   module.exports.callSagemaker = async (req, res) =&gt; {

   let params = {
      DomainId: '...', 
      UserProfileName: '...', 
      SessionExpirationDurationInSeconds: 120
   };
   await sagemaker.createPresignedDomainUrl(params, function(err, data) {
      if (err) console.log(err, err.stack); // an error occurred
      else     console.log(data);           // successful response
   });
}
</code></pre>
<p>But i got this error:</p>
<blockquote>
<p>TypeError: Cannot set property 'Timestamp' of undefined
at features.constructor.addAuthorization (/home/loredana/Documents/projects/edison_zeus/backend-zeus/node_modules/aws-sdk/lib/signers/v2.js:14:24)</p>
</blockquote>
<p>My sdk version is:</p>
<blockquote>
<p>&quot;aws-sdk&quot;: &quot;^2.800.0&quot;</p>
</blockquote>
<p>Is anyone facing the same issue?</p>",1,0,2020-12-01 10:14:45.713000 UTC,,2020-12-01 10:31:46.057000 UTC,1,node.js|aws-sdk|amazon-sagemaker|aws-sdk-js|aws-sdk-nodejs,93,2020-11-25 10:51:00.040000 UTC,2021-10-24 15:12:31.470000 UTC,,75,9,0,14,,,,,,['amazon-sagemaker']
How to send a raw input image to an MLflow served endpoint for a CNN model?,"<p>I have started learning MLflow recently and I am trying out ML life cycle on a simple cat-dog classifier. <a href=""https://github.com/abhishekPurandare-obs/mlflow_test"" rel=""nofollow noreferrer"">Here</a> is the code.</p>
<p>I have correctly set up the model for serving using <code>mlflow serve</code>. However, there is one problem. I have configured the model signature to take a specific shape of input and the API endpoint will take an array when sending a request.</p>
<p>In the <a href=""https://github.com/abhishekPurandare-obs/mlflow_test/blob/master/send_request.py"" rel=""nofollow noreferrer"">send_request.py</a> file I am performing preprocessing for CNN input and then sending the Numpy array as a list. What I would like to do is, set up my own custom predict function for the model. So that the endpoint will only take a raw image as an input instead of an array.</p>
<p>I followed <a href=""https://github.com/mlflow/mlflow/tree/master/examples/flower_classifier"" rel=""nofollow noreferrer"">this</a> tutorial. But couldn't make it work for my case.</p>
<p>I understand that while logging/saving the model, I'll need to provide custom objects. But I am not sure how to obtain the object of the model that performs the prediction inside these custom models. There aren't many resources to refer to for this particular case.</p>",0,0,2021-10-05 12:12:53.113000 UTC,,,1,conv-neural-network|mlflow|mlops,130,2021-10-05 11:38:25.427000 UTC,2022-09-16 13:13:37.810000 UTC,,43,0,0,5,,,,,,['mlflow']
Authenticating model upload to VertexAI job from Cloud Scheduler,"<p>I am trying to run a custom training job on VertexAI. The goal is to train a model, save the model to cloud storage and then upload it to VertexAI as a VertexAI Model object. When I run the job from local workstation, it runs, but when I run the job from Cloud Scheduler  it fails. Details below.</p>
<p><strong>Python Code for the job:</strong></p>
<pre><code>from sklearn.datasets import make_classification
from sklearn.ensemble import RandomForestClassifier
import os
import pickle
from google.cloud import storage
from google.cloud import aiplatform

print(&quot;FITTING THE MODEL&quot;)

# define dataset
X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=3)
# define the model
model = RandomForestClassifier(n_estimators=100, n_jobs=-1)
# fit the model
model.fit(X, y)


print(&quot;SAVING THE MODEL TO CLOUD STORAGE&quot;)
if 'AIP_MODEL_DIR' not in os.environ:
    raise KeyError(
        'The `AIP_MODEL_DIR` environment variable has not been' +
        'set. See https://cloud.google.com/ai-platform-unified/docs/tutorials/image-recognition-custom/training'
    )

artifact_filename = 'model' + '.pkl'
# Save model artifact to local filesystem (doesn't persist)
local_path = artifact_filename
with open(local_path, 'wb') as model_file:
    pickle.dump(model, model_file)

# Upload model artifact to Cloud Storage
model_directory = os.environ['AIP_MODEL_DIR']
storage_path = os.path.join(model_directory, artifact_filename)
blob = storage.blob.Blob.from_string(storage_path, client=storage.Client())
blob.upload_from_filename(local_path)


print (&quot;UPLOADING MODEL TO VertexAI&quot;)

# Upload the model to vertex ai
project=&quot;...&quot;
location=&quot;...&quot;
display_name=&quot;custom_mdoel&quot;
artifact_uri=model_directory
serving_container_image_uri=&quot;us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-4:latest&quot;
description=&quot;test model&quot;
sync=True

aiplatform.init(project=project, location=location)
model = aiplatform.Model.upload(
    display_name=display_name,
    artifact_uri=artifact_uri,
    serving_container_image_uri=serving_container_image_uri,
    description=description,
    sync=sync,
)
model.wait()

print(&quot;DONE&quot;)
</code></pre>
<p><strong>Running from Local Workstation:</strong>
I set the GOOGLE_APPLICATION_CREDENTIALS environment variable to point to the location of the Compute Engine default service account keys I have downloaded on my local workstation. I also set the AIP_MODEL_DIR environment variable to point to a cloud storage bucket. After I run the script, I can see the model.pkl file being created in the cloud storage bucket and the Model object  being created in VertexAI.</p>
<p><strong>Triggering the training job from Cloud Scheduler:</strong>
This is what I ultimately want to achieve - to run the custom training job periodically from Cloud Scheduler. I have converted the python script above into a docker image and uploaded to google artifact registry. The job specification for the Cloud Scheduler is below, I can provide additional details if required. The service account email in the <code>oauth_token</code> is the same whose keys I  use to set the GOOGLE_APPLICATION_CREDENTIALS environment variable. When I run this, (either from local workstation or directly in a VertexAI notebook), I can see that the Cloud Scheduler job gets created which keeps triggering the custom job. The custom job is able to train the model and save it to the cloud storage. However, it is not able to upload it to VertexAI and I get the error meessages, <code>status = StatusCode.PERMISSION_DENIED</code> and <code>{...&quot;grpc_message&quot;:&quot;Request had insufficient authentication scopes.&quot;,&quot;grpc_status&quot;:7</code>}. Cannot figure out  what the authentication issue is because in both cases I am using the same service account.</p>
<pre><code>job = {
  &quot;name&quot;: f'projects/{project_id}/locations/{location}/jobs/test_job',
  &quot;description&quot;: &quot;Test scheduler job&quot;,
  &quot;http_target&quot;: {
    &quot;uri&quot;: f'https://{location}-aiplatform.googleapis.com/v1/projects/{project_id}/locations/{location}/customJobs',
    &quot;http_method&quot;: &quot;POST&quot;,
    &quot;headers&quot;: {
      &quot;User-Agent&quot;: &quot;Google-Cloud-Scheduler&quot;,
      &quot;Content-Type&quot;: &quot;application/json; charset=utf-8&quot;
    },
    &quot;body&quot;: &quot;...&quot; // the custom training job body,
    &quot;oauth_token&quot;: {
      &quot;service_account_email&quot;: &quot;...&quot;,
      &quot;scope&quot;: &quot;https://www.googleapis.com/auth/cloud-platform&quot;
    }
  },
  &quot;schedule&quot;: &quot;* * * * *&quot;,
  &quot;time_zone&quot;: &quot;Africa/Abidjan&quot;
}
</code></pre>",0,2,2021-09-13 20:56:27.137000 UTC,,,0,google-cloud-platform|google-cloud-iam|google-cloud-scheduler|google-cloud-vertex-ai,260,2016-08-15 20:29:46.790000 UTC,2022-09-24 22:22:50.413000 UTC,,700,17,1,90,,,,,,['google-cloud-vertex-ai']
Hard to get a GPU / sagemaker studio labs,"<p>Anyone know a solution to make it easier to get a GPU?<a href=""https://i.stack.imgur.com/gKH7a.png"" rel=""nofollow noreferrer"">I always have trouble getting GPU</a></p>",0,0,2022-05-30 04:31:12.280000 UTC,,,0,amazon-sagemaker,84,2022-05-30 04:25:53.583000 UTC,2022-05-30 05:07:14.373000 UTC,,11,0,0,1,,,,,,['amazon-sagemaker']
Connection timed out - using sqlalchemy to access AWS usaspending data,"<p>I created an instance of the usaspending.gov database in my AWS RDS. A description of this database can be found here: <a href=""https://aws.amazon.com/public-datasets/usaspending/"" rel=""nofollow noreferrer"">https://aws.amazon.com/public-datasets/usaspending/</a></p>

<p>The data are available as a PostgreSQL snapshot, and I would like to access the database using Python's sqlalchemy package within a Jupyter notebook within Amazon SageMaker.</p>

<p>I tried to set up my database connection with the code below, but I'm getting a Connection timed out error. I'm pretty new to AWS and Sagemaker, so maybe I messed up my sqlalchemy engine? I think my VPC security settings are OK (it looks like they accept inbound and outbound requests).</p>

<p>Any ideas what I could be missing?</p>

<p>engine = create_engine(‘postgresql://root:password@[my endpoint]/[DB instance]</p>

<p><a href=""https://i.stack.imgur.com/v9nn5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/v9nn5.png"" alt=""connection timed out""></a></p>

<p><a href=""https://i.stack.imgur.com/W2G7R.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/W2G7R.png"" alt=""VPC inbound settings""></a></p>

<p><a href=""https://i.stack.imgur.com/JZ6AI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JZ6AI.png"" alt=""VPC outbound settings""></a></p>",1,0,2017-12-28 21:08:41.563000 UTC,,,1,python-3.x|postgresql|amazon-web-services|sqlalchemy|amazon-sagemaker,1528,2017-05-25 00:08:25.787000 UTC,2018-12-27 20:23:47.160000 UTC,,55,0,0,8,,,,,,['amazon-sagemaker']
Vertex AI Pipelines (Kubeflow) skip step with dependent outputs on later step,"<p>I’m trying to run a Vertex AI Pipelines job where I skip a certain pipeline step if the value of a certain pipeline parameter (in this case <code>do_task1</code>) is <code>False</code>. But because there is another step that runs unconditionally and expects the output of the first potentially skipped step, I get the following error, independently of do_task1 being <code>True</code> or <code>False</code>:</p>
<pre><code>AssertionError: component_input_artifact: pipelineparam--task1-output_path not found. All inputs: parameters {
  key: &quot;do_task1&quot;
  value {
    type: STRING
  }
}
parameters {
  key: &quot;task1_name&quot;
  value {
    type: STRING
  }
}
</code></pre>
<p>It seems like the compiler just cannot find the output <code>output_path</code> from <code>task1</code>. So I wonder if there is any way to have some sort of placeholders for the outputs of those steps that are under a <code>dsl.Condition</code> , and thus they get filled with default values unless the actual steps run and fill them with the non-default values.
The code below represents the problem and is easily reproducible.</p>
<p>I'm using <code>google-cloud-aiplatform==1.14.0</code> and <code>kfp==1.8.11</code></p>
<pre><code>from typing import NamedTuple

from kfp import dsl
from kfp.v2.dsl import Dataset, Input, OutputPath, component
from kfp.v2 import compiler

from google.cloud.aiplatform import pipeline_jobs

@component(
    base_image=&quot;python:3.9&quot;,
    packages_to_install=[&quot;pandas&quot;]
)
def task1(
    # inputs
    task1_name: str,
    # outputs
    output_path: OutputPath(&quot;Dataset&quot;),
) -&gt; NamedTuple(&quot;Outputs&quot;, [(&quot;output_1&quot;, str), (&quot;output_2&quot;, int)]):

    import pandas as pd
    
    output_1 = task1_name + &quot;-processed&quot;
    output_2 = 2

    df_output_1 = pd.DataFrame({&quot;output_1&quot;: [output_1]})
    df_output_1.to_csv(output_path, index=False)

    return (output_1, output_2)

@component(
    base_image=&quot;python:3.9&quot;,
    packages_to_install=[&quot;pandas&quot;]
)
def task2(
    # inputs
    task1_output: Input[Dataset],
) -&gt; str:

    import pandas as pd

    task1_input = pd.read_csv(task1_output.path).values[0][0]

    return task1_input

@dsl.pipeline(
    pipeline_root='pipeline_root',
    name='pipelinename',
)
def pipeline(
    do_task1: bool,
    task1_name: str,
):

    with dsl.Condition(do_task1 == True):

        task1_op = (
            task1(
                task1_name=task1_name,
            )
        )

    task2_op = (
        task2(
            task1_output=task1_op.outputs[&quot;output_path&quot;],
        )
    )


if __name__ == '__main__':
    
    do_task1 = True # &lt;------------ The variable to modify ---------------

    # compile pipeline
    compiler.Compiler().compile(
        pipeline_func=pipeline, package_path='pipeline.json')

    # create pipeline run
    pipeline_run = pipeline_jobs.PipelineJob(
        display_name='pipeline-display-name',
        pipeline_root='pipelineroot',
        job_id='pipeline-job-id',
        template_path='pipelinename.json',
        parameter_values={
            'do_task1': do_task1, # pipeline compilation fails with either True or False values
            'task1_name': 'Task 1',
        },
        enable_caching=False
    )
    
    # execute pipeline run
    pipeline_run.run()
</code></pre>
<p>Any help is much appreciated!</p>",0,0,2022-09-07 15:45:03.980000 UTC,1.0,,0,google-cloud-vertex-ai|kubeflow-pipelines,39,2018-02-10 21:10:15.053000 UTC,2022-09-23 15:42:30.400000 UTC,Spain,21,2,0,2,,,,,,['google-cloud-vertex-ai']
"Is there a way to list all resources/components (Models, endpoints, TrainingJobs, etc) associated with a AWS SageMaker Notebook/ Studio Project?","<p>If I write a script to train and deploy a machine learning model on AWS SageMaker notebook or create a Project using AWS SageMaker Studio, when I try to list all the resources used, using boto3, the information extracted has no pattern linking the resource information to the notebook/project.</p>
<p><a href=""https://i.stack.imgur.com/y5G1B.png"" rel=""nofollow noreferrer"">boto3.client('sagemaker').list_models()</a></p>
<p>The boto3 api returns all resource information associated with my account and region. Is there a way to link these info (Models, endpoints, TrainingJobs, ProcessingJobs etc) to the Sagemaker Notebook/ Studio Project they were created in?</p>",1,0,2022-05-14 07:47:16.643000 UTC,,2022-05-15 12:41:11.833000 UTC,-1,amazon-web-services|amazon-ec2|amazon-sagemaker,287,2020-12-26 07:35:25.447000 UTC,2022-09-06 06:52:27.550000 UTC,,1,0,0,2,,,,,,['amazon-sagemaker']
Update a custom container model in VertexAI in GCP,"<p>I have already deployed a custom container in VertexAI and it is working fine, but now I would like to update my model and endpoint. I have pushed the new version of my Docker image to the Artifact Registry and I was expecting this latest version was gonna be automatically connected to VertexAI, but apparently, VertexAI is still using my old version.</p>
<p>How can I update the model in VertexAI automatically without having to create a new model and endpoint every time I make a new release and change the Docker image?</p>",2,1,2022-01-24 11:33:48.623000 UTC,,,2,google-cloud-platform|google-cloud-vertex-ai,142,2020-12-09 17:29:57.000000 UTC,2022-09-21 16:18:53.273000 UTC,"Madrid, España",306,53,2,19,,,,,,['google-cloud-vertex-ai']
Invoking sagemaker endpoint using AWS-Lamda throwing parameter validation error,"<p>I trained an ML model in AWS Sagemaker and created an endpoint. I want to invoke it using AWS-Lambda. My model has 30 predictor variables. So I passed them into test event of Lambda as dict type as mentioned below</p>

<pre><code>{
  ""Time"": ""10 "",
  ""V1"": ""1.449043781 "",
  ""V2"": ""-1.176338825 "",
  ""V3"": ""0.913859833 "",
  ""V4"": ""-1.375666655 "",
  ""V5"": ""-1.971383165 "",
  ""V6"": ""-0.629152139 "",
  ""V7"": ""-1.423235601 "",
  ""V8"": ""0.048455888 "",
  ""V9"": ""-1.720408393 "",
  ""V10"": ""1.626659058 "",
  ""V11"": ""1.19964395 "",
  ""V12"": ""-0.671439778 "",
  ""V13"": ""-0.513947153 "",
  ""V14"": ""-0.095045045 "",
  ""V15"": ""0.230930409 "",
  ""V16"": ""0.031967467 "",
  ""V17"": ""0.253414716 "",
  ""V18"": ""0.854343814 "",
  ""V19"": ""-0.221365414 "",
  ""V20"": ""-0.387226474 "",
  ""V21"": ""-0.009301897 "",
  ""V22"": ""0.313894411 "",
  ""V23"": ""0.027740158 "",
  ""V24"": ""0.500512287 "",
  ""V25"": ""0.251367359 "",
  ""V26"": ""-0.129477954 "",
  ""V27"": ""0.042849871 "",
  ""V28"": ""0.016253262 "",
  ""Amount"": ""7.8""
}
</code></pre>

<p>Now I ran below mentioned code in AWS Lambda </p>

<pre><code>import json
import os
import csv
import boto3
import io
import codecs

endpoint_name = os.environ['ENDPOINT_NAME']
runtime = boto3.client('runtime.sagemaker')


def lambda_handler(event, context):
    print(""received event: ""+json.dumps(event,indent=2))
    data = json.loads(json.dumps(event))
    payload = data[""Time""]+data[""V1""]+data[""V2""]+data[""V3""]+data[""V4""]+data[""V5""]+data[""V6""]+data[""V7""]+data[""V8""]+data[""V9""]+data[""V10""]+data[""V11""]+data[""V12""]+data[""V13""]+data[""V14""]+data[""V15""]+data[""V16""]+data[""V17""]+data[""V18""]+data[""V19""]+data[""V20""]+data[""V21""]+data[""V22""]+data[""V23""]+data[""V24""]+data[""V25""]+data[""V26""]+data[""V27""]+data[""V28""]+data[""Amount""]
    payload = payload.split("" "")
    payload = [codecs.encode(i,'utf-8') for i in payload]
    payload=[bytearray(i) for i in payload]
    print(payload)
    response = runtime.invoke_endpoint(EndpointName=endpoint_name,ContentType='text/csv',Body=payload)
    print(response)
    result=json.loads(response['Body'].decode())
    pred = int(float(response))
    predicted_label = 'fraud' if pred==1 else 'not fraud'
    return predicted_label
</code></pre>

<p>This code is throwing below this error</p>

<pre><code>[ERROR] ParamValidationError: Parameter validation failed:
Invalid type for parameter Body, value: [bytearray(b'10'), bytearray(b'1.449043781'), bytearray(b'-1.176338825'), bytearray(b'0.913859833'), bytearray(b'-1.375666655'), bytearray(b'-1.971383165'), bytearray(b'-0.629152139'), bytearray(b'-1.423235601'), bytearray(b'0.048455888'), bytearray(b'-1.720408393'), bytearray(b'1.626659058'), bytearray(b'1.19964395'), bytearray(b'-0.671439778'), bytearray(b'-0.513947153'), bytearray(b'-0.095045045'), bytearray(b'0.230930409'), bytearray(b'0.031967467'), bytearray(b'0.253414716'), bytearray(b'0.854343814'), bytearray(b'-0.221365414'), bytearray(b'-0.387226474'), bytearray(b'-0.009301897'), bytearray(b'0.313894411'), bytearray(b'0.027740158'), bytearray(b'0.500512287'), bytearray(b'0.251367359'), bytearray(b'-0.129477954'), bytearray(b'0.042849871'), bytearray(b'0.016253262'), bytearray(b'7.8')], type: &lt;class 'list'&gt;, valid types: &lt;class 'bytes'&gt;, &lt;class 'bytearray'&gt;, file-like object
</code></pre>

<p>I understand that somehow I need to pass my 30 features into Lambda function such that data type of <code>payload</code> is compatible with <code>ContentType</code> for <code>respnse</code> to work. Can someone please explain how to do it? 
edit: I'm trying this problem by looking at <a href=""http://%20https://aws.amazon.com/blogs/machine-learning/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda/"" rel=""nofollow noreferrer"">this</a> aws blog. I don't quite understand how the author of above mentioned blog did it.</p>",1,0,2019-11-18 16:28:11.987000 UTC,,,0,python-3.x|amazon-web-services|machine-learning|aws-lambda|amazon-sagemaker,895,2019-09-12 20:07:41.627000 UTC,2022-09-21 14:35:39.230000 UTC,"Hyderabad, Telangana, India",486,28,2,75,,,,,,['amazon-sagemaker']
How to authenticate within Azure ML pipeline and avoid browser interactive authentication?,"<p>I have created a very simple azure ml pipeline. Basically, it accesses data through an api and prints it.
I have tried using a ClientSecretCredential and a ServicePrincipalAuthentication but the pipeline still asks me for a web browser authentication to continue (even though the script runs when I just run the python file in the terminal)</p>
<pre><code>    from azureml.core.compute import ComputeTarget, AmlCompute
    import azureml.core
    from azureml.core import Workspace, Datastore
    from azure.identity import ClientSecretCredential
    from azureml.core.authentication import ServicePrincipalAuthentication
    
    
    ws = Workspace.from_config()
    
    keyvault = ws.get_default_keyvault()
    
    client_id = keyvault.get_secret(&quot;ClientID&quot;)
    tenant_id = keyvault.get_secret(&quot;TENANTID&quot;)
    secret_id = keyvault.get_secret(&quot;CLIENTSECRET&quot;)
    
    sp = ClientSecretCredential(tenant_id = tenant_id, client_id = client_id, client_secret = secret_id)

#  sp = ServicePrincipalAuthentication(tenant_id, client_id, secret_id)
</code></pre>
<p>This is the output from the pipeline:</p>
<p>2022-07-06 06:08:21,496|azureml._vendor.azure_cli_core._session|INFO|Failed to load or parse file /root/.azureml/auth/azureProfile.json. It will be overridden by default settings.
2022-07-06 06:08:21,496|azureml._vendor.azure_cli_core._session|INFO|Failed to load or parse file /root/.azureml/auth/az.json. It will be overridden by default settings.
2022-07-06 06:08:21,496|azureml._vendor.azure_cli_core._session|INFO|Failed to load or parse file /root/.azureml/auth/az.sess. It will be overridden by default settings.
2022-07-06 06:08:21,498|azureml._vendor.azure_cli_core|DEBUG|Current cloud config:
AzureCloud
2022-07-06 06:08:21,501|azureml._vendor.azure_cli_core|DEBUG|Current cloud config:
AzureCloud
2022-07-06 06:08:21,527|azureml._vendor.azure_cli_core._profile|INFO|No web browser is available. Fall back to device code.
<strong>2022-07-06 06:08:21,628|azureml._vendor.azure_cli_core.auth.identity|WARNING|To sign in, use a web browser to open the page <a href=""https://microsoft.com/devicelogin"" rel=""nofollow noreferrer"">https://microsoft.com/devicelogin</a> and enter the code XXXXXXXXXX to authenticate.</strong>
2022-07-06 06:08:49,950|azureml.core.authentication|DEBUG|Time to expire 1814345.049124 seconds</p>
<p>What can I do for it to pick up the credential in the code and avoid asking for interactive authentication?</p>",1,0,2022-07-06 06:26:29.413000 UTC,,,0,azure|azure-machine-learning-service|azuremlsdk,67,2018-01-15 18:38:26.383000 UTC,2022-09-23 08:46:06.340000 UTC,,43,0,0,21,,,,,,['azure-machine-learning-service']
Can you drop Azure SQL Tables from Azure ML?,"<p>I am currently developing an Azure ML pipeline that is fed data and triggered using Power Automate and outputs to a couple of SQL Tables in Azure SQL. One of the tables that is generated by the pipeline needs to be refreshed each time the pipeline is run, and as such I need to be able to drop the entire table from the SQL database so that the only data present in the table after the run is the newly calculated data.</p>
<p>Now, at the moment I am dropping the table as part of the Power Automate flow that feeds the data into the pipeline initially. However, due to the size of the dataset, this means that there is a 2-6 hour period during which the analytics I am calculating are not available for the end user while the pipeline I created runs.</p>
<p>Hence, my question; is there any way to perform the &quot;DROP TABLE&quot; SQL command from within my Azure ML Pipeline? If this is possible, it would allow me to move the drop to immediately before the export, which would be a great improvement in performance.</p>
<p>EDIT: From discussions with Microsoft Support, it does appear that this is not possible due to how the current ML Platform is designed. Not answering this question in case someone does solve it, but adding this note so that people who come along with the same problem know.</p>",1,0,2021-05-27 01:56:28.390000 UTC,,2021-09-16 02:50:56.837000 UTC,-1,azure|azure-sql-database|power-automate|azure-machine-learning-studio|azure-sql-server,77,2021-01-20 22:28:36.003000 UTC,2021-12-13 22:49:59.460000 UTC,,119,0,0,5,,,,,,['azure-machine-learning-studio']
How to use tensorflow library with sagemaker preprocessing,"<p>I want to use TensorFlow for preprocessing in sagemaker pipelines.</p>
<p>But, I haven't been able to find a way to use it.</p>
<p>Right now, I'm using this library for preprocessing:</p>
<pre><code>from sagemaker.sklearn.processing import SKLearnProcessor

framework_version = &quot;0.23-1&quot;

sklearn_processor = SKLearnProcessor(
    framework_version=framework_version,
    instance_type=processing_instance_type,
    instance_count=processing_instance_count,
    base_job_name=&quot;abcd&quot;,
    role=role,
)
</code></pre>
<p><strong>Now, I need to use TensorFlow in preprocessing but the python module cant import TensorFlow.</strong></p>
<p>Any help would be much appreciated. Thanks.</p>",1,0,2021-08-06 05:51:05.587000 UTC,,,0,tensorflow|amazon-sagemaker|preprocessor,53,2018-09-20 11:21:17.530000 UTC,2022-03-25 12:52:21.713000 UTC,,19,1,0,7,,,,,,['amazon-sagemaker']
SageMaker Batch Transform keeps failing,"<p>I'm able to train and deploy a model to a SageMaker Endpoint and able to get a response from the Endpoint successfully. But when I try to run a BatchTransform job, it keeps failing. Below is my project folder structure, train.py script, and my notebook. I used the AWS Console to launch batch transform job.</p>
<p><strong>Error</strong></p>
<pre><code>JSONDecodeError(&quot;Expecting value&quot;, s, err.value) from None

Traceback (most recent call last):  File &quot;/miniconda3/lib/python3.7/site-packages/sagemaker_containers/_functions.py&quot;, line 93, in wrapper    return fn(*args, **kwargs)  File &quot;/miniconda3/lib/python3.7/site-packages/sagemaker_sklearn_container/serving.py&quot;, line 59, in default_input_fn    np_array = encoders.decode(input_data, content_type)  File &quot;/miniconda3/lib/python3.7/site-packages/sagemaker_containers/_encoders.py&quot;, line 211, in decode    return decoder(obj)  File &quot;/miniconda3/lib/python3.7/site-packages/sagemaker_containers/_encoders.py&quot;, line 97, in json_to_numpy    data = json.loads(string_like)  File &quot;/miniconda3/lib/python3.7/json/__init__.py&quot;, line 348, in loads    return _default_decoder.decode(s)  File &quot;/miniconda3/lib/python3.7/json/decoder.py&quot;, line 337, in decode    obj, end = self.raw_decode(s, idx=_w(s, 0).end())  File &quot;/miniconda3/lib/python3.7/json/decoder.py&quot;, line 355, in raw_decode    raise JSONDecodeError(&quot;Expecting value&quot;, s, err.value) from None
</code></pre>
<p><strong>Project Folder structure:</strong></p>
<pre><code>/
|
|__container
    |__code
        |__train.py
        |__requirements.txt
    |__Dockerfile
|__notebook.ipynb
|__build_and_push.sh
</code></pre>
<p><strong>Dockerfile</strong></p>
<pre><code>ARG REGION=us-east-1

FROM 683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:0.23-1-cpu-py3

ENV PATH=&quot;/opt/ml/code:${PATH}&quot;

COPY /code /opt/ml/code

RUN pip install -r /opt/ml/code/requirements.txt

ENV SAGEMAKER_PROGRAM train.py
</code></pre>
<p><strong>train.py</strong></p>
<pre><code>import os
import warnings
import sys
import json
import ast
import argparse
import pandas as pd
import numpy as np
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.model_selection import train_test_split
from sklearn import linear_model
from sklearn.neighbors import KNeighborsRegressor
from sklearn.preprocessing import PolynomialFeatures
from urllib.parse import urlparse
import logging
import pickle

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def eval_metrics(actual, pred):
    rmse = np.sqrt(mean_squared_error(actual, pred))
    mae = mean_absolute_error(actual, pred)
    r2 = r2_score(actual, pred)
    return rmse, mae, r2

if __name__ =='__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))
    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))
    parser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))
    parser.add_argument('--train-file', type=str, default='kc_house_data_train.csv')
    parser.add_argument('--test-file', type=str, default='kc_house_data_test.csv')
    parser.add_argument('--features', type=str)  
    parser.add_argument('--target', type=str) 

    args, _ = parser.parse_known_args()

    warnings.filterwarnings(&quot;ignore&quot;)
    np.random.seed(40)

    # Reading training and testing datasets
    logging.info('reading training and testing datasets')
    train_df = pd.read_csv(os.path.join(args.train, args.train_file))
    test_df = pd.read_csv(os.path.join(args.test, args.test_file))
    train_x = np.array(train_df[args.features.split(',')]).reshape(-1,1)
    test_x = np.array(test_df[args.features.split(',')]).reshape(-1,1)
    train_y = np.array(train_df[args.target]).reshape(-1,1)
    test_y = np.array(test_df[args.target]).reshape(-1,1)
     
     reg=linear_model.LinearRegression()
     reg.fit(train_x, train_y)

     predicted = reg.predict(test_x)

     (rmse, mae, r2) = eval_metrics(test_y, predicted)
        
      logging.info(f&quot;        Linear model: (features={args.features}, target={args.target})&quot;)
      logging.info(f&quot;            RMSE: {rmse}&quot;)
      logging.info(f&quot;            MAE: {mae}&quot;)
      logging.info(f&quot;            R2: {r2}&quot;)
      logging.info(f&quot;    Tags: {args.tags}&quot;) 

      # Save model
      model_path = os.path.join(args.model_dir, &quot;model.pkl&quot;)
      logging.info(f&quot;saving to {model_path}&quot;)
      with open(model_path, 'wb') as path:
          pickle.dump(reg, path)

def model_fn(model_dir):
    with open(os.path.join(model_dir, &quot;model.pkl&quot;), &quot;rb&quot;) as input_model:
        model = pickle.load(input_model)
    return model
    
def predict_fn(data, model):
    return model.predict(data)
</code></pre>
<p><strong>notebook</strong></p>
<pre><code>import sagemaker as sm
from sagemaker.predictor import  json_serializer, json_deserializer


sm_role = sm.get_execution_role()
___
image_uri = '8345678891234.dkr.ecr.us-east-1.amazonaws.com/sagemaker-test:latest'
___

hyperparameters = {
    'features': 'sqft_living',
    'target': 'price'
    }
local_mode = False
___

Estimator = sm.estimator.Estimator

estimator = Estimator(
    role = sm_role,
    train_instance_count = 1,
    train_instance_type = 'local' if local_mode else 'ml.m5.large',
    image_name = image_uri,
    base_job_name = 'housing-price-prediction',
    hyperparameters = hyperparameters,
    subnets=['subnet-b88aa077', 
             'subnet-u45673d1'],
    security_group_ids=['sg-c08b6532']
    
)   
___

estimator.fit(
    {'train':'s3://sagemaker-us-east-1-1234567/housing-price-prediction/curated/train', 
     'test':'s3://sagemaker-us-east-1-1234567/housing-price-prediction/curated/test'},
     wait=False,
     logs=True)

___
predictor = estimator.deploy(instance_type='ml.t2.medium',
                 initial_instance_count=1)   
predictor.accept = 'application/json'
predictor.content_type = 'application/json'
predictor.serializer = json_serializer
predictor.deserializer = json_deserializer
___
predict_data = pd.read_csv(&quot;s3://sagemaker-us-east-1-1234567/housing-price-prediction/curated/predict/kc_house_data_predict.csv&quot;)
get_prediction=np.array(predict_data['sqft_living']).reshape(-1,1)
predictor.predict(data=get_prediction) #Prediction from real-time endpoint was successful
</code></pre>
<p><strong>SageMaker Batch Transform Job setup using console</strong></p>
<blockquote>
<p>Job name:  Model name: </p>
</blockquote>
<blockquote>
<p>Instance type: ml.m5.large  Instance count: 1 S3 Data type: S3 Prefix</p>
</blockquote>
<blockquote>
<p>Content type: 'application/x-npy' S3 location:</p>
</blockquote>
<blockquote>
<p>'s3://&lt;bucket_name&gt;/predict/input/' S3 output path:</p>
</blockquote>
<blockquote>
<p>'s3://&lt;bucket_name&gt;/predict/output/'</p>
<p>Rest all I left it as default and hit <code>create job</code></p>
</blockquote>",0,0,2020-12-29 21:23:56.740000 UTC,,,1,python|amazon-web-services|amazon-sagemaker,579,2019-02-26 21:26:09.200000 UTC,2021-09-04 18:51:36.037000 UTC,,389,6,0,21,,,,,,['amazon-sagemaker']
MLflow Tracking On EC2,"<p>I'm attempting to follow the instructions given here (<a href=""https://medium.com/@alexanderneshitov/how-to-run-an-mlflow-tracking-server-on-aws-ec2-d7afd0ac8008"" rel=""nofollow noreferrer"">https://medium.com/@alexanderneshitov/how-to-run-an-mlflow-tracking-server-on-aws-ec2-d7afd0ac8008</a>) to test running MLflow tracker on an ec2 instance. I have done the following from the article</p>

<ol>
<li>Install mlflow on ec2</li>
<li>Install and configure NGINX following the steps given</li>
<li>Start mlflow server on ec2 using <code>mlflow server --default-artifact-root s3://test.bucket.for.mlflow/ --host 0.0.0.0</code></li>
<li>Access server using its public DNS</li>
</ol>

<p>According to the article, I should see the mlflow ui when accessing with my ec2 public DNS, but all I see is the following page:</p>

<p><a href=""https://i.stack.imgur.com/U5o1C.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/U5o1C.png"" alt=""NGINX page instead of MLflow UI""></a></p>

<p>Why would I be seeing this page and not the mlflow page like:</p>

<p><a href=""https://i.stack.imgur.com/6RiiT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6RiiT.png"" alt=""enter image description here""></a></p>",2,5,2020-04-10 17:23:33.373000 UTC,,2020-04-13 17:01:16.790000 UTC,0,nginx|amazon-ec2|dns|mlflow,1547,2014-11-26 14:40:35.813000 UTC,2021-08-12 15:37:44.573000 UTC,,945,35,1,148,,,,,,['mlflow']
1 minute Service timeout for AMLS models deployed on ACI or AKS,"<p>We have created an image scoring model on Machine learning Service and deployed using AMLS portal on ACI and AKS both.
Though it runs on smaller images , for larger images it gets timed-out after exactly 1 minute on both ACI and AKS.
It is expected that an image scoring can take few minutes.</p>

<p>Wanted to know , if it’s a limitation on using AMLS deployment,  or on ACI and AKS that they timeout the deployed webservice after 60 seconds??
Any workaround would be welcomed</p>

<p>ACI Error :-
 Post <a href=""http://localhost:5001/score"" rel=""nofollow noreferrer"">http://localhost:5001/score</a>: net/http: request canceled (Client.Timeout exceeded while awaiting headers)</p>

<p>AKS Error :-
 Replica closed connection before replying</p>",2,1,2019-09-23 15:04:16.157000 UTC,1.0,,1,azure|azure-aks|azure-container-instances|azure-machine-learning-service,1976,2017-10-22 09:05:10.973000 UTC,2020-11-06 16:28:37.523000 UTC,,55,1,0,16,,,,,,['azure-machine-learning-service']
Creating and deploying pre-trained tensorflow model with pre-processing and post-processing in AWS SageMaker,"<p>I am trying to deploy the pre-trained MaskRCNN model of <a href=""https://github.com/matterport/Mask_RCNN"" rel=""nofollow noreferrer"">https://github.com/matterport/Mask_RCNN</a> in SageMaker for prediction.</p>
<p>The problem is that the model uses numpy and scikit-learn as preprocessing before feeding inputs to the Keras layers, so just deploying the model as in <a href=""https://github.com/aws-samples/amazon-sagemaker-script-mode/tree/master/tf-2-word-embeddings"" rel=""nofollow noreferrer"">this example</a> would not work.</p>
<p>Things I have tried:</p>
<ul>
<li>Isolating the Keras part of the model and using this, so that the model to be deployed in the endpoint uses the data pre-processed. The problem in this case is that an http error <code>431 Request Header Fields Too Large</code> is prompted, since the pre-processed data (formed by the resized images plus anchors) is way larger than the original data.</li>
<li>Making an <code>entry_point.py</code> script with the <code>input_handler()</code> function. This script looks like the following:</li>
</ul>
<pre><code>def install(package: str):
    &quot;&quot;&quot; pip install a package &quot;&quot;&quot;
    subprocess.check_call([sys.executable, &quot;-q&quot;, &quot;-m&quot;, &quot;pip&quot;, &quot;install&quot;, package])

def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument('--model_dir', type=str, default=os.environ.get('SM_MODEL_DIR'))
    return parser.parse_known_args()

def input_handler(data, context):
    &quot;&quot;&quot; Expects @data to be json. Pre-process the data to be ready to be fed to the Keras layers. 
    https://www.mikulskibartosz.name/custom-preprocessing-in-tensorflow-with-sagemaker-endpoints/ &quot;&quot;&quot;

    data_dec = np.array(json.loads(data['inputs']))

    [molded_images, image_metas, anchors] = preprocess_images(data_dec)

    inputs = { #input_image, input_image_meta and input_anchors are the names of the Input layers of the model
        &quot;inputs&quot;: {
            &quot;input_image&quot;: molded_images.tolist(),
            &quot;input_image_meta&quot;: image_metas.tolist(),
            &quot;input_anchors&quot;: anchors.tolist()
        }
    }

    return json.dumps(inputs)

if __name__ == &quot;__main__&quot;:

    args, _ = parse_args()

    install('scikit-image')
    install('scipy')
</code></pre>
<p>And then creating the model and the endpoint in a SageMaker Notebook Instance:</p>
<pre><code>from sagemaker.tensorflow.model import TensorFlowModel
model = TensorFlowModel(model_data = url_to_saved_model_s3,
                        role = sagemaker.get_execution_role(),
                        framework_version = '2.3.1',
                        entry_point = 'entry_point.py' 
                        )
predictor = model.deploy(initial_instance_count=1, instance_type='ml.t2.medium') # create endpoint
</code></pre>
<p>But does not look like the <code>input_handler()</code> is ever called.</p>
<p>Any help on how to deploy a model for inference that needs to pre-process non-tensors?</p>",1,0,2022-02-03 13:43:45.847000 UTC,1.0,,0,python|tensorflow|amazon-sagemaker,275,2018-11-24 10:02:09.950000 UTC,2022-02-16 10:39:13.960000 UTC,,28,0,0,7,,,,,,['amazon-sagemaker']
Set user_id in Mlflow,"<p>I am running MLflow with several users can instanciate experiments.</p>
<p>Is there any way to override user_id which is set to system username?</p>
<p>Looking for a solution which works with start_run block.</p>
<p>Any ideas?</p>
<pre class=""lang-py prettyprint-override""><code>with mlflow.start_run():

</code></pre>",1,0,2021-06-14 17:20:57.003000 UTC,,,3,python|mlflow,897,2020-05-29 10:16:53.253000 UTC,2022-09-23 10:30:34.197000 UTC,,201,1,0,3,,,,,,['mlflow']
fastapi prediction with machine learning model from mlflow works in local but not online on Heroku,"<p>When I test my API in local it is working fine:</p>
<p><a href=""https://i.stack.imgur.com/TXvj9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TXvj9.png"" alt=""enter image description here"" /></a></p>
<p>When I test it online. It is still working fine.</p>
<p><a href=""https://i.stack.imgur.com/FntrH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FntrH.png"" alt=""enter image description here"" /></a></p>
<p>When I make a prediction in local; it is still working fine. It uses a model saved online on MLflow to make the prediction.</p>
<p><a href=""https://i.stack.imgur.com/TIjlM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TIjlM.png"" alt=""enter image description here"" /></a></p>
<p>But when I push my API online and try the same prediction, is it not working anymore.</p>
<p>When checking the status_code, I have an error 500:</p>
<p><a href=""https://i.stack.imgur.com/LBeYY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LBeYY.png"" alt=""enter image description here"" /></a></p>
<p>And when trying to print the answer it says:</p>
<p><a href=""https://i.stack.imgur.com/dOxbj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dOxbj.png"" alt=""enter image description here"" /></a></p>
<p>Any idea why?</p>
<p>Thank you</p>",1,3,2022-05-28 10:33:18.743000 UTC,,,0,heroku|fastapi|mlflow,99,2015-02-08 14:10:02.997000 UTC,2022-09-22 12:46:19.853000 UTC,,67,9,0,7,,,,,,['mlflow']
"Azure ML - Model not registering, encountering WebServiceException","<p>I've successfully registered a model with the following exact same code snippet before:</p>
<pre><code>#register model
from azureml.core.model import Model

register_model = Model.register(model_path = &quot;./models&quot;,
                       model_name = &quot;cr_tools&quot;,
                       description = &quot;Tools relating to the Customer Relations classifier.&quot;,
                       workspace = ws)

register_model
</code></pre>
<p>But now it's not working for a different model (different <code>./models</code> directory), and I'm encountering the following error:</p>
<pre><code>ServiceException: ServiceException:
    Code: 504
    Message: Operation returned an invalid status code 'Gateway Time-out'
    Details:

    Headers: {
        &quot;Date&quot;: &quot;Tue, 04 Jan 2022 22:12:54 GMT&quot;,
        &quot;Content-Type&quot;: &quot;text/html&quot;,
        &quot;Content-Length&quot;: &quot;160&quot;,
        &quot;Connection&quot;: &quot;keep-alive&quot;,
        &quot;Strict-Transport-Security&quot;: &quot;max-age=15724800; includeSubDomains; preload&quot;,
        &quot;X-Content-Type-Options&quot;: &quot;nosniff&quot;,
        &quot;x-request-time&quot;: &quot;60.019&quot;
    }
    InnerException: 504 Server Error: Gateway Time-out for url: https://eastus.experiments.azureml.net/artifact/v2.0/subscriptions/c450f3d1-583c-495f-b5d3-0b38b99e70c0/resourceGroups/ba-p-zeaus-group020-rg/providers/Microsoft.MachineLearningServices/workspaces/p-group020-aml-ws-001/artifacts/batch/metadata/LocalUpload/220104T215629-7c0d42b6
</code></pre>",1,0,2022-01-04 22:14:41.073000 UTC,,,1,azure|azure-machine-learning-service,151,2017-12-14 06:32:17.037000 UTC,2022-08-09 15:59:17.103000 UTC,,399,2,0,47,,,,,,['azure-machine-learning-service']
DVC dependencies for derived data without imports,"<p>I am new to DVC, and so far I like what I see. But possibly my question is fairly easy to answer.</p>

<p><strong>My question:</strong> how do we correctly track the dependencies to files in an original hugedatarepo  (lets assume that this can also change) in a derivedData project, but WITHOUT the huge files being imported generally when the derived data is checked out? I don't think I can use <code>dvc import</code> to achieve this.</p>

<p><strong>Details:</strong> We have a repository with a large amount of quite big data files (scans) and use this data to design and train various algorithms. Often we want to use only specific files and even only small chunks from within the files for training, annotation and so on. That is, we derive data for specific tasks, that we want to put in new repositories.</p>

<p>Currently my Idea is to <code>dvc get</code> the relevant data, put it in a untracked temporary folder and then again manage the derived data with dvc. But still to put in the dependency to the original data.</p>

<pre><code>hugeFileRepo
 +metaData.csv
 +dataFolder
 +-- hugeFile_1
 ...
 +-- hugeFile_n
</code></pre>

<p>in the derivedData repository I do</p>

<pre><code> dvc import hugeFileRepo.git metaData.csv
 dvc run -f derivedData.dvc \
    -d metaData.csv \
    -d deriveData.py \
    -o derivedDataFolder \
    python deriveData.py 
</code></pre>

<p>My deriveData.py does something along the line (pseudocode)</p>

<pre><code>metaData = read(metaData.csv)

#Hack because I don't know how to it right:
gitRevision = getGitRevision(metaData.csv.dvc)          
...
for metaDataForFile, file in metaData:
   if(iWantFile(metaDataForFile) ):
      #download specific file
      !dvc get --rev {gitRevision} -o tempFolder/{file} hugeFileRepo.git {file}

      #do processing of huge file and store result in derivedDataFolder
      processAndWrite(tempFolder/file)
</code></pre>

<p>So I use the metaData file as a proxy for the actual data. The hugeFileRepo data will not change frequently and the metaData file will be kept up to date. And I am absolutely fine with having a dependency to the data in general and not to the actual files I used. So I believe this solution would work for me, but I am sure there is a better way.</p>",1,0,2020-05-15 09:52:02.927000 UTC,,,2,dvc,179,2016-01-25 12:22:13.500000 UTC,2022-09-13 09:05:05.923000 UTC,,171,2,0,1,,,,,,['dvc']
SageMaker Estimator from EC2,"<p>I have a really big confuse about how AWS integrate Docker ECR with SageMaker, even though it works from sagemaker using just the Dockerfile and train.py script executing it from another script which uses the Estimator class, it doesn't from EC2</p>

<pre><code>from sagemaker.estimator import Estimator
import sagemaker 

estimator = Estimator(
  image_name=""test_docker"",
  role='arn:aws:iam::XXXXXXXXX:role/service-role/AmazonSageMaker-ExecutionRole-XXXXXXXX',
  train_instance_count=1,
  train_instance_type='local'
)
estimator.fit()
</code></pre>

<p>For this, I have the same folder with Dockerfile and train.py script and created another script called exec.py, this script has the estimator code, but when I execute it I get this error</p>

<pre><code>botocore.exceptions.ClientError: An error occurred (ValidationException) when calling the
CreateTrainingJob operation: 1 validation error detected: Value 
'test_docker-2020-05-06-02-50-56-375' at 'trainingJobName' failed to satisfy constraint: Member must 
satisfy regular expression pattern: ^[a-zA-Z0-9](-*[a-zA-Z0-9])*
</code></pre>",1,0,2020-05-06 03:08:35.670000 UTC,,,1,python|amazon-ec2|amazon-sagemaker|amazon-ecr,72,2017-06-28 22:57:01.233000 UTC,2021-06-25 03:28:11.807000 UTC,,11,0,0,44,,,,,,['amazon-sagemaker']
getting aligned val_loss and train_loss plots for each epoch using WandB rather than separate plots,"<p>I have the following code for logging the train and val loss in each epoch using WandB API. I am not sure though why I am not getting val loss and train loss in the same epoch. Any idea how that could be fixed?</p>
<pre><code>wandb.log({&quot;train loss&quot;: train_epoch_loss,
           &quot;val loss&quot;: val_epoch_loss,
           &quot;epoch&quot;: epoch})

wandb.log({&quot;train acc&quot;: train_epoch_acc,
           &quot;val acc&quot;: val_epoch_acc,
           &quot;epoch&quot;: epoch})

wandb.log({&quot;best val acc&quot;: best_acc, &quot;epoch&quot;: epoch})
</code></pre>
<p><a href=""https://i.stack.imgur.com/w6JBF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/w6JBF.png"" alt=""enter image description here"" /></a></p>
<p>As you see, val loss vs epochs and train loss vs epochs are two completely separate entities while I would like to have both of them in one plot in WandB.
<a href=""https://i.stack.imgur.com/cqSkl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cqSkl.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/ZSMye.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZSMye.png"" alt=""enter image description here"" /></a></p>",1,0,2022-03-11 00:46:51.130000 UTC,,,0,python|machine-learning|deep-learning|wandb,463,2013-05-23 18:56:17.410000 UTC,2022-06-28 18:22:36.490000 UTC,"Boston, MA, United States",31183,4284,32,18708,,,,,,['wandb']
SageMaker endpoint can't load huggingface tokenizer,"<p>I used Amazon SageMaker to train a HuggingFace model. At the end of the training script provided to the estimator, I saved the model into the correct path (<code>SM_MODEL_DIR</code>):</p>
<pre><code>if __name__ == &quot;__main__&quot;:
    parser = argparse.ArgumentParser()
    parser.add_argument(&quot;--model-dir&quot;, type=str, default=os.environ[&quot;SM_MODEL_DIR&quot;])
    
    ...
    
    trainer.model.save_pretrained(args.model_dir)
</code></pre>
<p>After the model was trained, I deployed it using the <code>deploy</code> method of the HuggingFace estimator. Once the endpoint was successfully created, I tried inference with the returned predictor:</p>
<pre><code>response = self.predictor.predict(
    {&quot;inputs&quot;: &quot;I want to know where is my order&quot;}
)
</code></pre>
<p>And I received the following client error:</p>
<pre><code>{'code': 400, 'type': 'InternalServerException', 'message': &quot;Can't load tokenizer for '/.sagemaker/mms/models/model'. Make sure that:\n\n- '/.sagemaker/mms/models/model' is a correct model identifier listed on 'https://huggingface.co/models'\n\n- or '/.sagemaker/mms/models/model' is the correct path to a directory containing relevant tokenizer files\n\n&quot;}
</code></pre>
<p>Why cannot the tokenizer be loaded?</p>",1,1,2021-10-01 09:09:07.327000 UTC,1.0,,1,python|amazon-web-services|amazon-sagemaker|huggingface-transformers|huggingface-tokenizers,176,2021-09-30 08:42:37.333000 UTC,2022-09-23 14:16:21.167000 UTC,Barcelona,373,36,4,17,,,,,,['amazon-sagemaker']
What is the difference of xgboost and sagemaker.xgboost,"<p>Question is really clear. Nowadays im learning AWS world, and this question is eating my head up. What is the difference of <code>import xgboost</code> and <code>import sagemaker.xgboost</code>.</p>
<p>On SageMaker i can work with normal XGBoost library, and i know i can select different EC2 types with <code>sagemaker.xgboost</code>. But except this, what is the difference?
Are there any big difference?</p>",1,0,2020-10-09 14:42:05.880000 UTC,,2020-10-10 01:02:34.620000 UTC,0,python|amazon-web-services|amazon-ec2|scikit-learn|amazon-sagemaker,313,2020-04-24 00:32:02.720000 UTC,2022-08-31 20:00:02.297000 UTC,"İzmir, Türkiye",63,0,0,3,,,,,,['amazon-sagemaker']
How to view and Interprete Vertex AI Logs,"<p>We have deployed Models in the Vertex AI endpoint.
Now we want to know and interpret logs regarding events
of Node creation, POD creation, user API call matric etc.</p>
<p>Is there any way or key by which we can filter the logs for Analysis?</p>",2,3,2021-09-13 04:45:36.940000 UTC,,2021-09-22 13:19:31.870000 UTC,1,google-cloud-vertex-ai,309,2017-01-03 19:47:17.570000 UTC,2022-02-22 06:30:56.963000 UTC,"Bangalore, Karnataka, India",72,0,0,6,,,,,,['google-cloud-vertex-ai']
No such file or directory: '/opt/ml/input/data/test/revenue_train.csv' Sagemaker [SM_CHANNEL_TRAIN],"<p>I am trying to deploy my RandomForestClassifier on Amazon Sagemaker using Python SDK. I have been following this example <a href=""https://github.com/aws/amazon-sagemaker-examples/blob/main/sagemaker-script-mode/sagemaker-script-mode.ipynb"" rel=""nofollow noreferrer"">https://github.com/aws/amazon-sagemaker-examples/blob/main/sagemaker-script-mode/sagemaker-script-mode.ipynb</a> but keep getting an error that the train file was not found. I think the file were not uploaded to the correct channel because when I run the training script using</p>
<pre><code>! python script_rf.py --model-dir ./ \
                   --train ./ \
                   --test ./ \

</code></pre>
<p>it works fine.</p>
<p>This is my script code:</p>
<pre><code>
# inference functions ---------------
def model_fn(model_dir):
    clf = joblib.load(os.path.join(model_dir, &quot;model.joblib&quot;))
    return clf

if __name__ =='__main__':

    print('extracting arguments')
    parser = argparse.ArgumentParser()

    # hyperparameters sent by the client are passed as command-line arguments to the script.
    parser.add_argument('--max_depth', type=int, default=2)
    parser.add_argument('--n_estimators', type=int, default=100)
    parser.add_argument('--random_state', type=int, default=0)
    

    # Data, model, and output directories
    parser.add_argument('--model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))
    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TEST'))
    parser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))
    parser.add_argument('--train-file', type=str, default='revenue_train.csv')
    parser.add_argument('--test-file', type=str, default='revenue_test.csv')
    
    args, _ = parser.parse_known_args()
    
    print('reading data')
    train_df = pd.read_csv(os.path.join(args.train, args.train_file))
    test_df = pd.read_csv(os.path.join(args.test, args.test_file))
    
    if len(train_df) == 0:
        raise ValueError(('There are no files in {}.\n').format(args.train, &quot;train&quot;))

    print('building training and testing datasets')
    attributes = ['available_minutes_100','ampido_slots_amount','ampido_slots_amount_100','ampido_slots_amount_200','ampido_slots_amount_300','min_dist_loc','count_event','min_dist_phouses','count_phouses','min_dist_stops','count_stops','min_dist_tickets','count_tickets','min_dist_google','min_dist_psa','count_psa']
    X_train = train_df[attributes]
    X_test = test_df[attributes]
    y_train = train_df['target']
    y_test = test_df['target']
    
    # train
    print('training model')
    model = RandomForestClassifier(
        max_depth =args.max_depth, n_estimators = args.n_estimators)
    
    model.fit(X_train, y_train)
     
    # persist model
    path = os.path.join(args.model_dir, &quot;model_rf.joblib&quot;)
    joblib.dump(model, path)
    print('model persisted at ' + path)
    
    # print accuracy and confusion matrix 
    print('validating model')
    y_pred=model.predict(X_test) 
    print('Confusion Matrix:')
    result = confusion_matrix(y_test, y_pred)
    print(result)
    print('Accuracy:')
    result2 = accuracy_score(y_test, y_pred)
    print(result2)

</code></pre>
<p>The error is raised in the train_df line of the script</p>
<p>I specified the channel_input_dirs separately and the data does exist :</p>
<pre><code># change channel input dirs 

from sagemaker.sklearn.estimator import SKLearn
enable_local_mode_training = False


hyperparameters = {&quot;max_depth&quot;: 2, 'random_state':0, &quot;n_estimators&quot;: 100}

if enable_local_mode_training:
    train_instance_type = &quot;local&quot;
    inputs = {&quot;train&quot;: trainpath, &quot;test&quot;: testpath}

else:
    train_instance_type = &quot;ml.c5.xlarge&quot;
    inputs = {&quot;train&quot;: &quot;s3://ampido-exports/production/revenue_train.csv&quot;, &quot;test&quot;: &quot;s3://ampido-exports/production/revenue_test.csv&quot;}

estimator_parameters = {
    &quot;entry_point&quot;: &quot;script_rf.py&quot;,
    &quot;framework_version&quot;: &quot;1.0-1&quot;,
    &quot;py_version&quot;: &quot;py3&quot;,
    &quot;instance_type&quot;: train_instance_type,
    &quot;instance_count&quot;: 1,
    &quot;hyperparameters&quot;: hyperparameters,
    &quot;role&quot;: role,
    &quot;base_job_name&quot;: &quot;randomforestclassifier-model&quot;,
    'channel_input_dirs' : inputs
}

estimator = SKLearn(**estimator_parameters)
estimator.fit(inputs)

</code></pre>
<p>but i still get the error FileNotFoundError: [Errno 2] No such file or directory: '/opt/ml/input/data/test/revenue_train.csv</p>",1,0,2022-08-28 09:51:28.197000 UTC,,2022-08-28 12:21:00.227000 UTC,0,machine-learning|amazon-sagemaker|channels|oci-python-sdk,60,2021-11-26 08:25:52.293000 UTC,2022-09-22 11:50:10.577000 UTC,,33,0,0,4,,,,,,['amazon-sagemaker']
Missing delimiter error when importing html text,"<p>Playing with Azure Machine Learning using the Designer and am getting a &quot;Delimiter not found&quot; error when importing my data.</p>
<p>I originally started with a few hundred html files stored as azure blobs. Each file would be considered a single row of text, however, I had no luck importing these files for further text analytics.</p>
<p>I created a Data Factory job that imported each file, stripped all the tabs, quotes, cr/lf from the text, added a column for the file name and stored it all as a combined tab-delimited file. In notepad++ I can confirm that the format is <code>FileName tab HtmlText</code>. This is the file I'm trying to import into ML and getting the missing delimiter message as I'm trying to define the import module.</p>
<p>Here is the error when I try and create a dataset:</p>
<pre class=""lang-py prettyprint-override""><code>{
  &quot;message&quot;: &quot;'Delimiter' is not specified or invalid.&quot;
}
</code></pre>
<p>Question 1: Is there a better way to do text analytics on a large collection of html files?</p>
<p>Question 2: Is there a format I need to use in my combined .tsv file that works?</p>
<p>Question 3: Is there maybe a max length to the string column? My html can be 10's of thousands of characters long.</p>",1,2,2020-08-25 23:00:37.553000 UTC,,2020-08-27 03:22:03.073000 UTC,0,azure-machine-learning-studio|azure-machine-learning-service,209,2008-09-16 18:53:55.460000 UTC,2022-09-02 14:15:15.327000 UTC,,936,44,4,105,,,,,,"['azure-machine-learning-studio', 'azure-machine-learning-service']"
Tune Model Hyperparameters Module: tune and cross validate at the same time,"<p>suppose I'm using the following configuration with the Tune Model Hyperparameters module with a Boosted Decision Tree Regression:</p>

<p><a href=""https://i.stack.imgur.com/ysnWu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ysnWu.png"" alt=""enter image description here""></a></p>

<p>Does this configuration let me tune the hyperparameters so that I'll have the best Coefficient of Determination, and at the same time it guarantees the lowest cross-validation average error?
If so, does anyone knows what the Tune Model Hyperparameters module do using this configuration in more details?</p>

<p>Thank you.</p>",1,0,2017-06-03 17:24:40.007000 UTC,1.0,,0,azure-machine-learning-studio|cortana-intelligence,916,2010-08-11 08:10:15.323000 UTC,2022-09-22 15:38:39.777000 UTC,"Bergamo, Italy",799,362,2,80,,,,,,['azure-machine-learning-studio']
Sagemaker saving inference to s3,"<p>I've been trying to find an example on how to upload image file to s3 from inference.py script.  I used my regular boto3 but it's giving me error.</p>
<pre><code>import boto3
s3 = boto3.resource('s3')
bucket='s3:{bucket}'
object_name= file_name
s3_url = s3_client.upload_file(file_name, bucket, object_name)
</code></pre>
<p>It's complaining that it can't find boto3.  Is it not allowed to import boto3 from inference.py?</p>
<pre><code>invokeEndpoint operation: Received server error (500) from model with message \&quot;No module named 'boto3'\n
</code></pre>",0,3,2021-06-09 12:42:28.420000 UTC,,,0,boto3|amazon-sagemaker|inference|machine-learning-model,67,2013-04-18 18:33:07.803000 UTC,2022-09-21 19:19:27.653000 UTC,Pacific NW,71,61,0,5,,,,,,['amazon-sagemaker']
How can I fix a 'Undefined control sequence' error in order to convert a JupyterNotebook to PDF?,"<hr />
<p>Hello everyone,</p>
<p>since this is my first post here (even though not my first time, because I've spent hours reading some of your brilliant answers over time), I hope you guys will forgive me initial mistakes, when it comes to writing a proper post.</p>
<p>I have a problem when trying to convert a JupyterNotebook, that runs on AWS SageMaker to PDF using nbconvert.</p>
<p>I am using the following command via the terminal:</p>
<pre><code>jupyter nbconvert --to pdf MyNotebook.ipynb 
</code></pre>
<p>And instead of the notebook converting into a PDF, I'll end up with the following error:</p>
<pre><code>! Undefined control sequence.
\AfterEndPreamble -&gt;\AddToHook 
                               {begindocument/end}
l.158 \AfterEndPreamble
                       {%
? 
! Emergency stop.
\AfterEndPreamble -&gt;\AddToHook 
                               {begindocument/end}
l.158 \AfterEndPreamble
                       {%
No pages of output.
</code></pre>
<p>Converting it to a .tex file, however, works without any problems.</p>
<p>Now I've spend the last hour, trying to solve this issue by searching the web and looking at similar problems, but for the first time I couldn't find any solution to my specific issue yet.</p>
<p>So I hope, some of you will be able to help me with it.</p>
<p>Appreciate it!</p>",0,4,2021-04-23 14:00:40.767000 UTC,,2021-04-23 14:10:12.917000 UTC,0,pdf|latex|jupyter|amazon-sagemaker|nbconvert,672,2021-04-23 13:49:48.663000 UTC,2021-09-16 13:09:44.253000 UTC,"Malmö, Schweden",41,3,0,8,,,,,,['amazon-sagemaker']
azureml how to deploy docker image to webservice,"<p>Try to containerize models using docker and use this in a web service. Getting the following error &quot;azureml.exceptions._azureml_exception.WebserviceException: WebserviceException:Message: Models must either be of type azureml.core.Model or a str path to a file or folder&quot;.</p>
<pre><code>    env = Environment.from_conda_specification(&quot;env&quot;, &quot;../Environments.yml&quot;)
    inf_conf = InferenceConfig(
    entry_script=&quot;score.py&quot;,
    environment=env)


    docker_image = Model.package(ws, [models_latest], inf_conf)
    docker_image.wait_for_creation(show_output=True) 



    # Deploy the image
    webservice_name = os.environ['WEB_SERVICE_NAME']

    retries = 2
    while retries &gt; 0:
        try:
            service = AciWebservice(workspace = ws,
                                    name = webservice_name)
            service.update(image = docker_image)
            print('Webservice updated')   
        except:
            print('Webservice not found')
            service = Webservice.deploy_from_image(workspace = ws,
                                                name = webservice_name,
                                                image = docker_image,
                                                deployment_config = aciconfig)

        # wait for deployment, get logs if failed
        try:
            service.wait_for_deployment(show_output = True)
            break
        except:
            print(service.get_logs())
            retries -= 1
            if retries == 0:
                raise
</code></pre>",1,0,2022-08-17 10:05:27.093000 UTC,,2022-08-17 10:11:56.870000 UTC,0,python|azure|azure-machine-learning-service|azureml-python-sdk,67,2021-10-27 11:28:41.057000 UTC,2022-09-22 14:14:03.980000 UTC,,128,28,0,24,,,,,,['azure-machine-learning-service']
Using GCP's Vertex AI Image Classification exported (TF SavedModel) model for prediction,"<p>I've trained an Image Classification model via Google Cloud Platform's Vertex AI framework and liked the results. Due to that I then proceeded to export it in Tensorflow SavedModel format (shows up as 'Container' export) for custom prediction because I don't like neither the slowness of Vertex's batch prediction nor the high cost of using a Vertex endpoint.</p>
<p>In my python code I used</p>
<pre><code>model = tensorflow.saved_model.load(model_path)
infer =  model.signatures[&quot;serving_default&quot;]
</code></pre>
<p>When I tried to inspect what <code>infer</code> requires I saw that its input is two parameters: <code>image_bytes</code> and <code>key</code>. Both are string-type tensors.</p>
<p>This question can be broken off into several sub-questions that then make a whole:</p>
<ol>
<li>Isn't inference done on multiple data instances? If so, why is it image_bytes and not images_bytes?</li>
<li>Is image_bytes just the output of <code>open(&quot;img.jpg&quot;, &quot;rb&quot;).read()</code>? If so, don't I have to resize it first? To what size? How do I check that?</li>
<li>What is key? I have absolutely no clue or guess regarding this one's meaning.</li>
</ol>
<p>The documentation for GCP is paid only and so I have decided to ask for help here. I tried to search for an answer on google for multiple days but found no relevant article.
Thank you for reading and your help would be greatly appreciated and maybe even useful to future readers.</p>",0,0,2022-07-10 18:10:34.237000 UTC,,2022-08-06 09:59:53.553000 UTC,0,python|tensorflow|google-cloud-platform|image-classification|google-cloud-vertex-ai,118,2015-09-19 18:35:09.540000 UTC,2022-09-13 18:10:54.683000 UTC,,79,0,0,22,,,,,,['google-cloud-vertex-ai']
How lo create a dataset from files with different formats in azure ML studio,"<p>I trying to load the data from the <a href=""https://www.kaggle.com/c/m5-forecasting-accuracy"" rel=""nofollow noreferrer"">kaggle M5</a> competition to AZ ML Studio</p>
<p>Which contains the following files:</p>
<p><a href=""https://i.stack.imgur.com/6ayBT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6ayBT.png"" alt=""enter image description here"" /></a></p>
<p>I extracted them in a folder in my machine and used the GUI to upload the complete folder as an ML Studio dataset:</p>
<p><a href=""https://i.stack.imgur.com/ZWwmc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZWwmc.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/jHEyX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jHEyX.png"" alt=""enter image description here"" /></a></p>
<p>The problem is that is identifying only one schema of one of the files</p>
<p><a href=""https://i.stack.imgur.com/VSpOD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VSpOD.png"" alt=""enter image description here"" /></a></p>
<p>Which is to corrected way to work with this kind of datasets? should I load each file independently as a dataset or is there a way of handle several files in the same dataset?</p>",1,0,2022-05-24 23:13:43.440000 UTC,,,0,python|azure|azure-blob-storage|azure-machine-learning-studio,118,2015-02-08 23:53:31.840000 UTC,2022-09-24 00:08:32.523000 UTC,,8349,1489,6,949,,,,,,['azure-machine-learning-studio']
Using MLflow and Sagemaker with preprocessing steps,"<p>I'm deploying my models to Sagemaker using MLflow integration. However, my ML pipeline includes some basic preprocessing steps, such as scalers, and I need it to be part of my inference endpoint. Is there a way to do that with MLflow? I looked in the <a href=""https://www.mlflow.org/docs/latest/python_api/mlflow.pyfunc.html"" rel=""nofollow noreferrer"">mlflow_pyfunc</a> is closer to what I want, but I'm not sure if it is compatible with Sagemaker.</p>",0,1,2022-07-20 11:52:22.350000 UTC,,,0,python|amazon-sagemaker|mlflow,34,2016-10-17 11:39:35.777000 UTC,2022-07-28 17:26:21.417000 UTC,,109,1,0,21,,,,,,"['mlflow', 'amazon-sagemaker']"
SageMaker Lifecycle configuration script - LR CF characters added when trying to copy into AWS on mac,"<p>So I have a lifecycle configuration file that I want to use. I have used tools to make sure the illegal CR CF are stripped out and that it is unix friendly. However, there is no way to upload the file &quot;as is&quot; in AWS Sagemaker lifecycle configurations - you have to directly copy in the text, and that adds the CR LF characters back on my Mac. How do I get around this?</p>",1,2,2021-08-02 21:37:19.740000 UTC,,2021-08-02 22:46:37.470000 UTC,0,amazon-web-services|special-characters|amazon-sagemaker,160,2015-01-15 17:43:03.700000 UTC,2022-08-23 22:54:25.603000 UTC,,1387,51,1,153,,,,,,['amazon-sagemaker']
kubeflow component - why so many ways to define a component and what are the differences?,"<p>Please help understand what are the meaningful/significant differences among different ways to create kubeflow pipeline components and the reason for having so many ways?</p>
<pre><code>from kfp.components import func_to_container_op

@func_to_container_op
def add_op(a: float, b: float) -&gt; float:
    &quot;&quot;&quot;Returns sum of two arguments&quot;&quot;&quot;
    return a + b
</code></pre>
<pre><code>from kfp.v2.dsl import component

@component
def add_op(a: float, b: float) -&gt; float:
    &quot;&quot;&quot;Returns sum of two arguments&quot;&quot;&quot;
    return a + b
</code></pre>
<pre><code>from kfp.components import create_component_from_func

@create_component_from_func
def add_op(a: float, b: float) -&gt; float:
    &quot;&quot;&quot;Returns sum of two arguments&quot;&quot;&quot;
    return a + b
</code></pre>",0,0,2022-06-10 23:44:45.283000 UTC,,,0,google-cloud-vertex-ai|kubeflow-pipelines,77,2014-11-22 09:22:35.470000 UTC,2022-09-24 22:13:03.237000 UTC,,14749,641,62,968,,,,,,['google-cloud-vertex-ai']
SageMaker Limits on Sklearn Batch Transformer Payload,"<p>I am following the gist of this tutorial:</p>

<p><a href=""https://aws.amazon.com/blogs/machine-learning/preprocess-input-data-before-making-predictions-using-amazon-sagemaker-inference-pipelines-and-scikit-learn/"" rel=""nofollow noreferrer"">https://aws.amazon.com/blogs/machine-learning/preprocess-input-data-before-making-predictions-using-amazon-sagemaker-inference-pipelines-and-scikit-learn/</a></p>

<p>where I am using a custom sklearn transformer to pre-process data before passing to xgboost. When I get to this point:</p>

<pre><code>transformer = sklearn_preprocessor.transformer(
    instance_count=1, 
    instance_type='ml.m4.xlarge',
    assemble_with = 'Line',
    accept = 'text/csv')

# Preprocess training input
transformer.transform('s3://{}/{}'.format(input_bucket, input_key), content_type='text/csv')
print('Waiting for transform job: ' + transformer.latest_transform_job.job_name)
transformer.wait()
preprocessed_train = transformer.output_path
</code></pre>

<p>The location of the training data is S3 and there are multiple files there. I get an error that the max payload has been exceeded and it appears that you can only set up to 100MB. Does this mean that Sagemaker can not transform larger data as input into another process?</p>",1,0,2019-12-18 14:31:48.463000 UTC,1.0,,2,amazon-sagemaker,1083,2011-01-10 02:55:08.597000 UTC,2022-09-22 22:11:16.413000 UTC,,1748,80,5,393,,,,,,['amazon-sagemaker']
How to mock an S3AFileSystem locally for testing spark.read.csv with pytest?,"<p><strong>What I'm trying to do</strong></p>
<p>I am attempting to unit test an equivalent of the following function, using pytest:</p>
<pre><code>def read_s3_csv_into_spark_df(s3_uri, spark):
    df = spark.read.csv(
        s3_uri.replace(&quot;s3://&quot;, &quot;s3a://&quot;)
    )
    return df
</code></pre>
<p>The test is defined as follows:</p>
<pre><code>def test_load_csv(self, test_spark_session, tmpdir):
    # here I 'upload' a fake csv file using tmpdir fixture and moto's mock_s3 decorator
    
    # now that the fake csv file is uploaded to s3, I try read into spark df using my function
    baseline_df = read_s3_csv_into_spark_df(
        s3_uri=&quot;s3a://bucket/key/baseline.csv&quot;,
        spark=test_spark_session
    )
</code></pre>
<p>In the above test, the test_spark_session fixture used is defined as follows:</p>
<pre><code>@pytest.fixture(scope=&quot;session&quot;)
def test_spark_session():
    test_spark_session = (
        SparkSession.builder.master(&quot;local[*]&quot;).appName(&quot;test&quot;).getOrCreate()
    )
    return test_spark_session
</code></pre>
<p><strong>The problem</strong></p>
<p>I am running <code>pytest</code> on a SageMaker notebook instance, using python 3.7, pytest 6.2.4, and pyspark 3.1.2. I am able to run other tests by creating the DataFrame using <code>test_spark_session.createDataFrame</code>, and then performing aggregations. So the local spark context is indeed working on the notebook instance with pytest.</p>
<p>However, when I attempt to read the csv file in the test I described above, I get the following error:</p>
<pre><code>py4j.protocol.Py4JJavaError: An error occurred while calling o84.csv.
E  : java.lang.RuntimeException: java.lang.ClassNotFoundException: Class 
org.apache.hadoop.fs.s3a.S3AFileSystem not found
</code></pre>
<p>How can I, without actually uploading any csv files to S3, test this function?</p>
<p>I have also tried providing the S3 uri using <code>s3://</code> instead of <code>s3a://</code>, but got a different, related error: <code>org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme &quot;s3&quot;</code>.</p>",0,4,2021-10-27 13:10:35.673000 UTC,,,0,pyspark|pytest|amazon-sagemaker|moto,464,2014-06-29 19:25:29.277000 UTC,2022-09-24 13:31:41.150000 UTC,"Stellenbosch, Südafrika",21,1,0,1,,,,,,['amazon-sagemaker']
Approaches to improve Microsoft ChatBot with each user conversation by learning from it?,"<p>I am building a Microsoft ChatBot using LUIS for natural language processing. I would like LUIS to improve by learning new utterences for the intents identified. </p>

<p>For example, if my 'Greeting' intent has utterences 'Hi', 'Hello', 'Hello, how are you?', the next time it encounters 'How are you?', it may predict the intent as 'Greeting' with a low accuracy. If that utterance is learnt as part of the intent, then in future, this utterence will be predicted with better accuracy and also help us in recognizing utterences closer to this utterence.</p>

<p>The learning could be based on:</p>

<blockquote>
  <ol>
  <li>All input for which intent was identified. (I understand this can cause wrong learnings).</li>
  <li>Inputs identified by LUIS and verified by user or agent real-time or later offline.</li>
  <li>Inputs identified right/wrong but verified and corrected by the agent or  support team later via an easier UI.</li>
  </ol>
</blockquote>

<p>I understand LUIS gives a 'Suggested Utterences' tab which takes care of point 3. I am trying to understand how we can automate this learning by minimal user intervention. </p>

<p>What are the various approaches used in projects?</p>",1,0,2017-10-23 09:33:41.673000 UTC,,2017-10-24 00:10:18.603000 UTC,0,nlp|botframework|azure-language-understanding|azure-machine-learning-studio,77,2015-03-12 16:38:02.987000 UTC,2022-06-15 08:23:14.777000 UTC,"Bangalore, Karnataka, India",232,557,1,55,,,,,,['azure-machine-learning-studio']
Sagemaker Kernel for tensorflow and pymysql,"<p>I am reading from a database using <strong><em>pymysql</em></strong> while using <strong><em>conda_python3</em></strong> kernel but, when I want to use tensorflow/keras for deep learning, notebook instance doesn't recognize it. </p>

<p>I tried installing keras and tensorflow libraries but after showing a success message and restarting kernals, I got the same old error that <strong><em>""No module named 'keras'""</em></strong>.</p>

<p>Reading stackoverflow related posts, I was able to temporarily solve the issue with switching to another kernel <strong><em>""conda_tensorflow_p36""</em></strong>. </p>

<p>Do you know any way to avoid switching between these kernels (maybe a better way of installing packages in sagemaker notebook instances?).<br>
<a href=""https://i.stack.imgur.com/yve6V.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yve6V.png"" alt=""enter image description here""></a></p>",1,0,2019-11-20 14:01:59.020000 UTC,,,1,amazon-web-services|tensorflow|machine-learning|pymysql|amazon-sagemaker,182,2019-10-13 16:20:43.930000 UTC,2022-07-05 12:25:25.607000 UTC,"Toronto, ON, Canada",151,8,0,27,,,,,,['amazon-sagemaker']
Daemon doesn't start after installing docker on Amazon sagemaker,"<p>I'm trying to train an object detection model on Amazon sagemaker and I want to use a framework called mmdetection which is based on pytorch. I learned that in order to use it there I need to create a custom docker environment.</p>
<p>The kernel I'm using on sagemaker is &quot;Python 3 (PyTorch 1.6 Python 3.6 GPU Optimized)&quot; and it is running Ubuntu as it's os. downloading docker on it was already difficult enough for me but I still can't proceed. Whenever I want to actually use docker (I want to pull an already existing one with the command &quot;!docker pull csuhan/s2anet:latest&quot; which I tested on a non sagemaker ubuntu and it worked) it tells me:
&quot;Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?&quot;</p>
<p>The real problem is that whenever I try to run &quot;!dockerd&quot; in the notebook to start the daemon, it says the following long message: &quot;failed to start daemon: Error initializing network controller: error obtaining controller instance: failed to create NAT chain DOCKER: iptables failed: iptables -t nat -N DOCKER: iptables v1.6.1: can't initialize iptables table `nat': Permission denied (you must be root) Perhaps iptables or your kernel needs to be upgraded.&quot;</p>
<p>If I try the line &quot;!dockerd --iptables=false&quot; instead I get the result: &quot;failed to start daemon: Error initializing network controller: Error creating default &quot;bridge&quot; network: operation not permitted&quot;</p>
<p>I suspected that maybe its related to the execution role I'm using but I have no idea what to do and cant seem to find an existing answer on google for this specific problem.</p>",0,1,2021-10-12 07:50:09.467000 UTC,,2021-10-12 08:22:25.293000 UTC,0,python|docker|daemon|amazon-sagemaker,142,2021-10-12 07:38:32.220000 UTC,2022-03-16 08:16:46.347000 UTC,,1,0,0,3,,,,,,['amazon-sagemaker']
What is the right way to get a groundtruth custom labeling job output in the augmented manifest file format for a training job?,"<p>This page: <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/augmented-manifest.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/augmented-manifest.html</a> has the simple example:</p>

<pre><code>{""image-ref"": ""s3://mybucket/sample01/image1.jpg"", ""is-a-cat"": 1}
{""image-ref"": ""s3://mybucket/sample02/image2.jpg"", ""is-a-cat"": 0}
</code></pre>

<p>and says to set <code>AttributeNames</code> to <code>[""image-ref"", ""is-a-cat""]</code>. This page: <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/sms-data-output.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/sms-data-output.html</a> says your output file is in <code>s3://bucket/labeling-job-name/manifests/output/output.manifest</code>
For my custom labeling job that file has a format like:</p>

<pre><code>{
    ""source-ref"": ""s3://bucketname/path/filename.png"",
    ""job-name"": {
        ""annotationsFromAllWorkers"": ""{\""image-similarity\"":{\""label\"":\""Unrecognizable\""}}""
    },
    ""job-name-metadata"": {
        ""type"": ""groundtruth/custom"",
        ""job-name"": ""job-name-20200221002259"",
        ""human-annotated"": ""yes"",
        ""creation-date"": ""2020-02-21T00:26:31+0000""
    }
}
</code></pre>

<p>So the question is, do I need to do something to change the output format in the labeling job, or pass some other kind of data in <code>AttributeNames</code> such as <code>[""source-ref"", ""job-name.annotationsFromAllWorkers.image-similarity.label""]</code>, or write another script to massage the data into the right format?</p>",1,0,2020-03-07 20:35:37.187000 UTC,,,0,amazon-web-services|amazon-sagemaker,226,2008-09-08 00:07:00.710000 UTC,2022-09-22 18:36:12.143000 UTC,United States,978,2253,4,340,,,,,,['amazon-sagemaker']
Unable to integrate Azure ML API with PHP,"<p>I tried integrating Azure ML API with PHP but unfortunately getting an error in response.</p>

<p>Updated: I have used request response API sending through json response</p>

<p>Below is the response obtained on executing PHP script:</p>

<pre><code>array(1) { [""error""]=&gt; array(3) { [""code""]=&gt; string(11) ""BadArgument"" 
    [""message""]=&gt; string(26) ""Invalid argument provided."" [""details""]=&gt; array(1)
    {[0]=&gt; array(2) { [""code""]=&gt; string(18) ""RequestBodyInvalid"" [""message""]=&gt;
    string(68) ""No request body provided or error in deserializing the request
    body."" } } } }
</code></pre>

<p>PHP Script:</p>

<pre><code>$url = 'URL';
$api_key = 'API';
$data = array(
    'Inputs'=&gt; array(
        'My Experiment Name'=&gt; array(
            ""ColumnNames"" =&gt; [['Column1'],
                              ['Column2'],
                              ['Column3'],
                              ['Column4'],
                              ['Column5'],
                              ['Column6'],
                              ['Column7']],
            ""Values"" =&gt; [ ['Value1'],
                          ['Value2'],
                          ['Value3'],
                          ['Value4'],
                          ['Value5'],
                          ['Value6'],
                          ['Value7']]
            ),
        ),
        'GlobalParameters' =&gt; new StdClass(),
    );

$body = json_encode($data);

$ch = curl_init();
curl_setopt($ch, CURLOPT_URL, $url);
curl_setopt($ch, CURLOPT_HTTPHEADER, array('Content-Type: application/json', 'Authorization: Bearer '.$api_key, 'Accept: application/json'));
curl_setopt($ch, CURLOPT_POST, 1);
curl_setopt($ch, CURLOPT_POSTFIELDS, $body);
curl_setopt($ch, CURLOPT_RETURNTRANSFER, true);
curl_setopt($ch, CURLOPT_SSL_VERIFYPEER, false);

$response  = json_decode(curl_exec($ch), true);
//echo 'Curl error: ' . curl_error($ch);
curl_close($ch);

var_dump ($response);
</code></pre>

<p>I have followed few examples, still unable to crack it. Please let me know the solution for this.</p>",1,0,2016-05-30 06:51:11.917000 UTC,1.0,2016-06-02 13:05:08.577000 UTC,2,php|azure|curl|machine-learning|azure-machine-learning-studio,463,2015-10-30 11:54:36.917000 UTC,2016-08-01 12:11:32.813000 UTC,,21,0,0,3,,,,,,['azure-machine-learning-studio']
How to deploy a detectron2 model using file in azureML,"<p>I have a detectron2 detection model trained and saved ( by someone else ) as a file <code>model.pth</code>, I also have a <code>cfg.yaml</code> file that specifies the weights path as the path to <code>model.pth</code> as follows :</p>
<p>inside <code>cfg.yaml</code> we have this line
<code>WEIGHTS: /var/azureml-app/azureml-models/detect_containers/1/outputs/model.pth</code></p>
<p>I noticed that the person before me ( who trained the model .. ) used the <code>cfg.yaml</code> file to create the predictor configuration as follows :</p>
<pre class=""lang-py prettyprint-override""><code>cfg = get_cfg()
cfg.merge_from_file(&quot;cfg.yaml&quot;)
predictor = DefaultPredictor(cfg)
</code></pre>
<p>He created and used his predictor inside the run function of the <code>scoring.py</code> script used to deploy the model as a webservice in azureML.</p>
<p>So the problem here is that I have no access to this person's azureML account nor workspace so when I try deploying the model on my own I get errors mainly path errors indicating that the file <code>/var/azureml-app/azureml-models/detect_containers/1/outputs/model.pth</code> is not found, so I uploaded the file in my azureML workspace in the same folder as my notebooks but apparently, that doesn't work either because the path I provide then is considered a local path and when the model is being deployed I guess it's basically going to a remote Microsoft server ( that I have no idea how its files are organized so i don't have the path ).</p>
<p>I also tried creating a new model with the same name, and uploaded the file to this model in order to use this line during deployment :
<code>model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'model.pth')</code>
This way I managed to avoid path errors but once the model got deployed I couldn't test it because of scoring URI errors that are not explained not even in the logs.</p>
<p>Normally the model is functional ( as it was deployed and used before by the person who trained it lol ).</p>
<p>So do you guys have any idea on how should I approach the problem or how to solve it ?
And does anyone have any kind of resource explaining how to get paths to files after they've been deployed, personally once a model is deployed I feel like it's in a black magical box, I can't see how the files are organized in the remote Microsoft server.</p>",0,2,2021-10-12 00:03:17.583000 UTC,,2021-10-17 20:15:30.743000 UTC,2,azure|azure-machine-learning-service|detectron|azureml-python-sdk,350,2021-10-05 10:01:17.933000 UTC,2022-08-10 00:34:58.650000 UTC,,59,8,0,8,,,,,,['azure-machine-learning-service']
Saving Azure ML pipelines in GIT or backup somewhere,"<p>I have 2 queries.</p>
<p>I am creating a Pipelines with Azure ML Designer. Most of them are in Pipeline Draft state. Accidentally the workspace was deleted by one of the Azure tech team.</p>
<p>Will the pipeline draft will also be saved in the Azure Storage account or only the pipelines which are run/submitted only be saved in the Storage container. If the drafts also saved in the storage, could you share the folder where it is stored so that I could use it for restoration.</p>
<h2>Query 2</h2>
<p>How to save the Azure ML Pipelines created using the Azure ML designer to be saved in the GIT or some other backup device for future restoration purpose incase of any mishap.</p>
<p>Is it possible to backup pipeline drafts in GIT?</p>
<p>Thank you.</p>",1,0,2022-08-08 14:55:18.303000 UTC,,2022-08-25 05:43:56.227000 UTC,0,git|pipeline|azure-machine-learning-service,54,2022-08-08 14:43:38.977000 UTC,2022-09-21 11:21:59.357000 UTC,,1,0,0,0,,,,,,['azure-machine-learning-service']
Which SageMaker server supports server-side batching and how to enable it?,"<p>MMS, TFServing and TorchServe support <strong>server-side batching</strong> (consequent requests can be locally batched in async fashion by the server while maintaining the illusion of synchronous batch-1 size to the client). How to enable those features on SageMaker endpoints?</p>",1,0,2022-09-08 17:54:11.520000 UTC,,,0,amazon-sagemaker,17,2014-10-07 08:13:42.830000 UTC,2022-09-23 14:45:05.230000 UTC,,26,0,0,3,,,,,,['amazon-sagemaker']
Azure machine language tutorial step failed,"<p>I'm following the steps defined in <a href=""https://docs.microsoft.com/en-us/azure/machine-learning/preview/tutorial-classifying-iris-part-3"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/machine-learning/preview/tutorial-classifying-iris-part-3</a></p>

<p>I issued the command line:
<strong>az ml env set -n myenv -g myenvrg</strong></p>

<p>Got the error:
<em>Resource with group myenvrg and name myenv cannot be set, as its provisioning state is Failed. Provisioning state succeeded is required.</em></p>

<p>Any ideas why I get this error and any workaround? </p>",1,5,2017-11-24 03:08:07.003000 UTC,,,-1,azure|azure-machine-learning-studio,300,2013-08-27 06:22:15.203000 UTC,2022-09-19 18:05:53.570000 UTC,,1853,168,0,315,,,,,,['azure-machine-learning-studio']
Forbidden: An error occurred (403) when calling the HeadObject operation:,"<p>my ~/.aws/credentials looks like</p>
<pre><code>[default]
aws_access_key_id = XYZ
aws_secret_access_key = ABC

[testing]
source_profile = default
role_arn = arn:aws:iam::54:role/ad
</code></pre>
<p>I add my remote like</p>
<pre><code>dvc remote add --local -v myremote s3://bib-ds-models-testing/data/dvc-test
</code></pre>
<p>I have made my .dvc/config.local to look like</p>
<pre><code>[‘remote “myremote”’]
url = s3://bib-ds-models-testing/data/dvc-test
access_key_id = XYZ
secret_access_key = ABC/h2hOsRcCIFqwYWV7eZaUq3gNmS
profile=‘testing’
credentialpath = /Users/nyt21/.aws/credentials
</code></pre>
<p>but still after running <code>dvc push -r myremote</code> I get</p>
<blockquote>
<p>ERROR: unexpected error - Forbidden: An error occurred (403) when calling the HeadObject operation: Forbidden</p>
</blockquote>
<p>** Update
here is the output of <code>dvc push -v</code></p>
<pre><code>2021-07-25 22:40:38,887 DEBUG: Check for update is enabled.
2021-07-25 22:40:39,022 DEBUG: Preparing to upload data to 's3://bib-ds-models-testing/data/dvc-test'
2021-07-25 22:40:39,022 DEBUG: Preparing to collect status from s3://bib-ds-models-testing/data/dvc-test
2021-07-25 22:40:39,022 DEBUG: Collecting information from local cache...
2021-07-25 22:40:39,022 DEBUG: Collecting information from remote cache...                                                                                                                     
2021-07-25 22:40:39,022 DEBUG: Matched '0' indexed hashes
2021-07-25 22:40:39,022 DEBUG: Querying 1 hashes via object_exists
2021-07-25 22:40:39,644 ERROR: unexpected error - Forbidden: An error occurred (403) when calling the HeadObject operation: Forbidden                                                          
------------------------------------------------------------
Traceback (most recent call last):
  File &quot;/Users/nyt21/opt/miniconda3/envs/dvc/lib/python3.8/site-packages/s3fs/core.py&quot;, line 246, in _call_s3
    out = await method(**additional_kwargs)
  File &quot;/Users/nyt21/opt/miniconda3/envs/dvc/lib/python3.8/site-packages/aiobotocore/client.py&quot;, line 154, in _make_api_call
    raise error_class(parsed_response, operation_name)
botocore.exceptions.ClientError: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File &quot;/Users/nyt21/opt/miniconda3/envs/dvc/lib/python3.8/site-packages/s3fs/core.py&quot;, line 1057, in _info
    out = await self._simple_info(path)
  File &quot;/Users/nyt21/opt/miniconda3/envs/dvc/lib/python3.8/site-packages/s3fs/core.py&quot;, line 970, in _simple_info
    out = await self._call_s3(
  File &quot;/Users/nyt21/opt/miniconda3/envs/dvc/lib/python3.8/site-packages/s3fs/core.py&quot;, line 265, in _call_s3
    raise translate_boto_error(err)
PermissionError: Access Denied

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;/Users/nyt21/opt/miniconda3/envs/dvc/lib/python3.8/site-packages/s3fs/core.py&quot;, line 246, in _call_s3
    out = await method(**additional_kwargs)
  File &quot;/Users/nyt21/opt/miniconda3/envs/dvc/lib/python3.8/site-packages/aiobotocore/client.py&quot;, line 154, in _make_api_call
    raise error_class(parsed_response, operation_name)
botocore.exceptions.ClientError: An error occurred (403) when calling the HeadObject operation: Forbidden

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File &quot;/Users/nyt21/opt/miniconda3/envs/dvc/lib/python3.8/site-packages/dvc/main.py&quot;, line 55, in main
    ret = cmd.do_run()
  File &quot;/Users/nyt21/opt/miniconda3/envs/dvc/lib/python3.8/site-packages/dvc/command/base.py&quot;, line 50, in do_run
    return self.run()
  File &quot;/Users/nyt21/opt/miniconda3/envs/dvc/lib/python3.8/site-packages/dvc/command/data_sync.py&quot;, line 57, in run
    processed_files_count = self.repo.push(
  File &quot;/Users/nyt21/opt/miniconda3/envs/dvc/lib/python3.8/site-packages/dvc/repo/__init__.py&quot;, line 51, in wrapper
    return f(repo, *args, **kwargs)
  File &quot;/Users/nyt21/opt/miniconda3/envs/dvc/lib/python3.8/site-packages/dvc/repo/push.py&quot;, line 44, in push
    pushed += self.cloud.push(objs, jobs, remote=remote)
  File &quot;/Users/nyt21/opt/miniconda3/envs/dvc/lib/python3.8/site-packages/dvc/data_cloud.py&quot;, line 79, in push
    return remote_obj.push(
  File &quot;/Users/nyt21/opt/miniconda3/envs/dvc/lib/python3.8/site-packages/dvc/remote/base.py&quot;, line 57, in wrapper
    return f(obj, *args, **kwargs)
  File &quot;/Users/nyt21/opt/miniconda3/envs/dvc/lib/python3.8/site-packages/dvc/remote/base.py&quot;, line 494, in push
    ret = self._process(
  File &quot;/Users/nyt21/opt/miniconda3/envs/dvc/lib/python3.8/site-packages/dvc/remote/base.py&quot;, line 351, in _process
    dir_status, file_status, dir_contents = self._status(
  File &quot;/Users/nyt21/opt/miniconda3/envs/dvc/lib/python3.8/site-packages/dvc/remote/base.py&quot;, line 195, in _status
    self.hashes_exist(
  File &quot;/Users/nyt21/opt/miniconda3/envs/dvc/lib/python3.8/site-packages/dvc/remote/base.py&quot;, line 145, in hashes_exist
    return indexed_hashes + self.odb.hashes_exist(list(hashes), **kwargs)
  File &quot;/Users/nyt21/opt/miniconda3/envs/dvc/lib/python3.8/site-packages/dvc/objects/db/base.py&quot;, line 438, in hashes_exist
    remote_hashes = self.list_hashes_exists(hashes, jobs, name)
  File &quot;/Users/nyt21/opt/miniconda3/envs/dvc/lib/python3.8/site-packages/dvc/objects/db/base.py&quot;, line 389, in list_hashes_exists
    ret = list(itertools.compress(hashes, in_remote))
  File &quot;/Users/nyt21/opt/miniconda3/envs/dvc/lib/python3.8/concurrent/futures/_base.py&quot;, line 619, in result_iterator
    yield fs.pop().result()
  File &quot;/Users/nyt21/opt/miniconda3/envs/dvc/lib/python3.8/concurrent/futures/_base.py&quot;, line 444, in result
    return self.__get_result()
  File &quot;/Users/nyt21/opt/miniconda3/envs/dvc/lib/python3.8/concurrent/futures/_base.py&quot;, line 389, in __get_result
    raise self._exception
  File &quot;/Users/nyt21/opt/miniconda3/envs/dvc/lib/python3.8/concurrent/futures/thread.py&quot;, line 57, in run
    result = self.fn(*self.args, **self.kwargs)
  File &quot;/Users/nyt21/opt/miniconda3/envs/dvc/lib/python3.8/site-packages/dvc/objects/db/base.py&quot;, line 380, in exists_with_progress
    ret = self.fs.exists(path_info)
  File &quot;/Users/nyt21/opt/miniconda3/envs/dvc/lib/python3.8/site-packages/dvc/fs/fsspec_wrapper.py&quot;, line 92, in exists
    return self.fs.exists(self._with_bucket(path_info))
  File &quot;/Users/nyt21/opt/miniconda3/envs/dvc/lib/python3.8/site-packages/fsspec/asyn.py&quot;, line 87, in wrapper
    return sync(self.loop, func, *args, **kwargs)
  File &quot;/Users/nyt21/opt/miniconda3/envs/dvc/lib/python3.8/site-packages/fsspec/asyn.py&quot;, line 68, in sync
    raise result[0]
  File &quot;/Users/nyt21/opt/miniconda3/envs/dvc/lib/python3.8/site-packages/fsspec/asyn.py&quot;, line 24, in _runner
    result[0] = await coro
  File &quot;/Users/nyt21/opt/miniconda3/envs/dvc/lib/python3.8/site-packages/s3fs/core.py&quot;, line 802, in _exists
    await self._info(path, bucket, key, version_id=version_id)
  File &quot;/Users/nyt21/opt/miniconda3/envs/dvc/lib/python3.8/site-packages/s3fs/core.py&quot;, line 1061, in _info
    out = await self._version_aware_info(path, version_id)
  File &quot;/Users/nyt21/opt/miniconda3/envs/dvc/lib/python3.8/site-packages/s3fs/core.py&quot;, line 1004, in _version_aware_info
    out = await self._call_s3(
  File &quot;/Users/nyt21/opt/miniconda3/envs/dvc/lib/python3.8/site-packages/s3fs/core.py&quot;, line 265, in _call_s3
    raise translate_boto_error(err)
PermissionError: Forbidden
------------------------------------------------------------
2021-07-25 22:40:39,712 DEBUG: Version info for developers:
DVC version: 2.5.4 (pip)
---------------------------------
Platform: Python 3.8.10 on macOS-10.16-x86_64-i386-64bit
Supports:
        http (requests = 2.26.0),
        https (requests = 2.26.0),
        s3 (s3fs = 2021.6.1, boto3 = 1.18.6)
Cache types: reflink, hardlink, symlink
Cache directory: apfs on /dev/disk3s1s1
Caches: local
Remotes: s3
Workspace directory: apfs on /dev/disk3s1s1
Repo: dvc, git

Having any troubles? Hit us up at https://dvc.org/support, we are always happy to help!
2021-07-25 22:40:39,713 DEBUG: Analytics is enabled.
2021-07-25 22:40:39,765 DEBUG: Trying to spawn '['daemon', '-q', 'analytics', '/var/folders/4x/xhm22wt16gl6m9nvkl9gllkc0000gn/T/tmpo86jdns5']'
2021-07-25 22:40:39,769 DEBUG: Spawned '['daemon', '-q', 'analytics', '/var/folders/4x/xhm22wt16gl6m9nvkl9gllkc0000gn/T/tmpo86jdns5']'
</code></pre>
<p>I can upload through python</p>
<pre><code>import boto3
import os
import pickle

bucket_name = 'bib-ds-models-testing'
os.environ[&quot;AWS_PROFILE&quot;] = &quot;testing&quot;
session = boto3.Session()
s3_client = boto3.client('s3')

s3_client.upload_file('/Users/nyt21/Devel/DVC/test/data/iris.csv',
    'bib-ds-models-testing',
    'data/dvc-test/my_iris.csv')
</code></pre>
<p>I don't use aws CLI but the following also gives an access deny !</p>
<pre><code>aws s3 ls s3://bib-ds-models-testing/data/dvc-test
</code></pre>
<blockquote>
<p>An error occurred (AccessDenied) when calling the ListObjectsV2
operation: Access Denied</p>
</blockquote>
<p>but it works if I add --profile=testing</p>
<pre><code>aws s3 ls s3://bib-ds-models-testing/data/dvc-test --profile=testing
                       
</code></pre>
<blockquote>
<p>PRE dvc-test/</p>
</blockquote>
<p>just you know environment variable <code>AWS_PROFILE</code> is already set to 'testing'</p>
<p><strong>UPDATE</strong></p>
<p>I have tried both <code>AWS_PROFILE='testing'</code> and <code>AWS_PROFILE=testing</code>, neither of them worked.</p>
<p><a href=""https://i.stack.imgur.com/DZQlz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DZQlz.png"" alt=""enter image description here"" /></a></p>",0,7,2021-07-25 10:04:37.173000 UTC,,2021-07-27 10:54:57.973000 UTC,1,dvc,1526,2011-02-10 16:54:52.887000 UTC,2022-09-21 18:56:24.013000 UTC,"Copenhagen, Denmark",4988,350,22,416,,,,,,['dvc']
"Getting error ""Unable to create recipe: Service invocation failed!Request ... "" when training any of my models","<p>I'm getting this error message when training any of my ML models in AML</p>
<pre class=""lang-py prettyprint-override""><code>(run = exp.submit(src))
</code></pre>
<blockquote>
<p>Unable to create recipe: Service invocation failed! Request: POST
<a href=""https://cert-westus2.experiments.azureml.net/rp/workspaces/s"" rel=""nofollow noreferrer"">https://cert-westus2.experiments.azureml.net/rp/workspaces/s</a></p>
</blockquote>
<p>Training for these models was working fine last week.</p>
<p>Thanks for your help!</p>",2,6,2020-07-02 01:18:34.253000 UTC,,2020-07-02 13:07:36.160000 UTC,1,azure-machine-learning-service,316,2019-10-04 21:39:44.120000 UTC,2021-06-02 21:49:42.870000 UTC,,11,0,0,3,,,,,,['azure-machine-learning-service']
Selecting a model by name using Webservice.list on AzureML,"<p>I deployed a huggingface ML model on Azure and now I'm trying to select it using the Python SDK.</p>
<p>I'm trying this:</p>
<pre><code>from azureml.core import Workspace, Webservice
mlw = Workspace.from_config(&quot;mlw.json&quot;)
services = Webservice.list(mlw)
services_filtered = Webservice.list(mlw, model_name=services[0].name)
</code></pre>
<p>Now, <code>services</code> is a list containing two models, but <code>services_filtered</code> is an empty list. What am I doing wrong here?</p>
<p>Of course, I could select the right one afterwards using the names, but that just doesn't seem right.</p>",1,0,2021-03-17 09:53:36.863000 UTC,,2021-03-17 10:21:40.667000 UTC,0,python|azure|api|azure-machine-learning-service,30,2012-05-28 21:32:39.273000 UTC,2022-09-16 10:18:21.743000 UTC,Utrecht,2449,420,7,253,,,,,,['azure-machine-learning-service']
Sending CSV records to Amazon Sagemaker through lambda function,"<p>I am looking to send CSV records coming from kinesis analytics to sagemaker endpoint and getting an inference through a lambda function and then passing it on to a firehose API to dump it into S3. But the data is not getting into sagemaker for some reason.</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>'use strict';
console.log('Loading function');
var AWS = require('aws-sdk');
var sagemakerruntime = new AWS.SageMakerRuntime({apiVersion: '2017-05-13'});
var firehose = new AWS.Firehose({apiVersion: '2015-08-04'});
exports.handler = (event, context, callback) =&gt; {
    let success = 0;
    let failure = 0;
    const output = event.records.map((record) =&gt; {
        /* Data is base64 encoded, so decode here */
        const recordData = Buffer.from(record.data, 'base64');
        try {
            var params = {
                Body: new Buffer('...') || recordData /* Strings will be Base-64 encoded on your behalf */, /* required */
                EndpointName: 'String', /* required */
                Accept: 'text/csv',
                ContentType: 'text/csv'
            };
            sagemakerruntime.invokeEndpoint(params, function(err, data) {
                var result1;
                if (err) console.log(err, err.stack); // an error occurred
                else     console.log(data);           // successful response
                result1=data;
                var params = {
                    DeliveryStreamName: 'String', /* required */
                    Record: { /* required */
                        Data: new Buffer('...') || result1 /* Strings will be Base-64 encoded on your behalf */ /* required */
                    }
                };
                firehose.putRecord(params, function(err, data) {
                    if (err) console.log(err, err.stack); // an error occurred
                    else     console.log(data);           // successful response
                });
            });
            success++;
            return {
                recordId: record.recordId,
                result: 'Ok',
            };
        } catch (err) {
            failure++;
            return {
                recordId: record.recordId,
                result: 'DeliveryFailed',
            };
        }
    });
    console.log(`Successful delivered records ${success}, Failed delivered records ${failure}.`);
    callback(null, {
        records: output,
    });
};</code></pre>
</div>
</div>
</p>",0,1,2018-06-20 11:47:36.510000 UTC,1.0,2018-06-20 11:49:09.167000 UTC,1,javascript|aws-lambda|amazon-sagemaker,351,2018-06-20 11:39:09.780000 UTC,2018-08-09 09:03:43.103000 UTC,,11,0,0,0,,,,,,['amazon-sagemaker']
Type Error while running Keras in Azure Machine Learning Service,"<p>While running a <code>keras</code> inferencing pipeline using Azure Machine Learning Service, I am getting the error:</p>
<blockquote>
<p><code>TypeError</code>: <code>float()</code> argument must be a string or a number, not
<code>azureml.dataprep.native.DataPrepError</code></p>
</blockquote>",0,2,2020-08-31 20:05:56.923000 UTC,,2020-09-01 09:04:26.397000 UTC,1,azure|keras|azure-machine-learning-service,58,2015-07-02 20:04:32.517000 UTC,2021-08-09 17:03:32.633000 UTC,,11,0,0,3,,,,,,['azure-machine-learning-service']
mlflow can't find .py file,"<p>I'm trying to learn to use <code>mlflow</code> by creating a very simple project and log it.</p>

<p>I've tried following <code>mlflow</code>'s example and my code runs properly when running the main.py as a normal bash command.</p>

<p>I couldn't make it run using the <code>mlflow</code> CLI using project and a simple file.
I got the following error.</p>

<pre><code>(rlearning) yair@pc2016:~/reinforced_learning101$ mlflow run src/main.py 
2019/05/11 10:21:41 ERROR mlflow.cli: === Could not find main among entry points [] or interpret main as a runnable script. Supported script file extensions: ['.py', '.sh'] ===
(rlearning) yair@pc2016:~/reinforced_learning101$ mlflow run .
2019/05/11 10:40:25 INFO mlflow.projects: === Created directory /tmp/tmpe26oernf for downloading remote URIs passed to arguments of type 'path' ===
2019/05/11 10:40:25 INFO mlflow.projects: === Running command 'source activate mlflow-21497056aed7961402b515847613ed9f950fa9fc &amp;&amp; python src/main.py 1.0' in run with ID 'ed51446de4c44903ab891d09cfe10e49' === 
bash: activate: No such file or directory
2019/05/11 10:40:25 ERROR mlflow.cli: === Run (ID 'ed51446de4c44903ab891d09cfe10e49') failed ===

</code></pre>

<p>Needless to say my main has a <code>.py</code> suffix.</p>

<p>Is there anything wrong that causes this issue?</p>

<p>My main.py is:</p>

<pre><code>import sys

import gym
import mlflow


if __name__ == '__main__':
    env = gym.make(""CartPole-v0"")
    right_percent = float(sys.argv[1]) if len(sys.argv) &gt; 1 else 1.0
    with mlflow.start_run():
        obs = env.reset()
        print(env.action_space)
        action = 1  # accelerate right
        print(obs)
        mlflow.log_param(""right percent"", right_percent)
        mlflow.log_metric(""mean score"", 1)
        mlflow.log_metric(""std score"", 0)
</code></pre>

<p>conda_env.yaml</p>

<pre><code>name: rlearning
channels:
  - defaults
dependencies:
  - python=3.7
  - numpy
  - pandas
  - tensorflow-gpu
  - pip:
      - mlflow
      - gym
</code></pre>

<p>MLproject</p>

<pre><code>name: reinforced learning

conda_env: files/config/conda_environment.yaml

entry_points:
  main:
    parameters:
      right_percent: {type: float, default: 1.0}
    command: ""python src/main.py {right_percent}""
</code></pre>",1,0,2019-05-11 07:32:43.290000 UTC,,2019-05-11 07:44:35.393000 UTC,0,python|mlflow,1263,2015-06-25 08:50:08.620000 UTC,2022-09-08 07:19:29.283000 UTC,"London, UK",91,12,0,44,,,,,,['mlflow']
DeepAR Building Product Categories,"<p>I have a problem with the understanding of the DeepAR Algorithm. </p>

<p>I tried to forecast the sales of single products with the Algorithm. 
First I tried it for one SKU on a daily frequence but I got the following error message: </p>

<pre><code>ParamValidationError: Parameter validation failed:
Invalid type for parameter Body, value: [datetime
</code></pre>

<p>I thought, that the reason for that error was that I have too many ""NaN""- values in my targets. Could that be the reason?
(I didn't apply any categories or dynamic_feats)</p>

<p>I then tried to make the forecast on a monthly frequence, but the result was that I didn't have enough timestamps for the algorithm. </p>

<p>Would it be possible to group my products within the DeepAR algorithm through the ""cat"" or the ""dynamic_feat"" operators, so that I would have less ""NaN""-values in my targets? </p>

<p>I would like to group the products by different features like color, price or size. Do you know if that is possible, or do I have to do that before I apply DeepAR?</p>

<p>Thanks in advance:)</p>",2,0,2020-01-05 12:20:41.520000 UTC,,,0,pandas|amazon-sagemaker,278,2020-01-04 17:42:08.370000 UTC,2020-12-14 12:51:41.103000 UTC,,1,0,0,2,,,,,,['amazon-sagemaker']
What is a 'XGBoostLabelEncoder' object?,"<p>I'm trying to load a model from an mlflow run. When I do that I get an 'XGBoostLabelEncoder' object, an object with no attributes like predict or predict_proba. I don't really know what you can do with it.</p>
<p>I've googled around but can't find any information about what an 'XGBoostLabelEncoder' object is.</p>
<p>Anybody who knows?</p>",0,0,2022-09-19 07:18:58.730000 UTC,,2022-09-19 07:41:41.817000 UTC,0,python|scikit-learn|xgboost|mlflow,25,2017-08-24 07:25:41.763000 UTC,2022-09-25 05:29:59.790000 UTC,"Malmö, Sverige",796,605,4,136,,,,,,['mlflow']
can I use mlflow python API to register a spark UDF & then use the UDF in Spark scala code?,"<p>I'm trying to use mlflow to do the machine learning work. I register the ML model as UDF using the following python code. The question is how can I use the UDF(test_predict) in my scala code? The reason is that our main code is in Scala. The problem is that UDF created below is a temporary UDF and SparkSession scoped. thanks!</p>

<pre class=""lang-py prettyprint-override""><code>import sys
import mlflow
from mlflow import pyfunc
import numpy as np
from pyspark import SparkContext, SparkConf
from pyspark import SQLContext
from pyspark.sql.session import SparkSession
from pyspark.sql.types import *

sc=SparkContext()
spark = SparkSession.builder.appName(""Python UDF example"").getOrCreate()
pyfunc_udf=mlflow.pyfunc.spark_udf(spark=spark, model_uri=""./sk"",result_type=""float"")
spark.udf.register(""test_predict"",pyfunc_udf)
</code></pre>",0,3,2020-04-25 14:15:05.457000 UTC,,2020-04-28 09:13:51.427000 UTC,2,python|apache-spark|pyspark|user-defined-functions|mlflow,617,2019-03-25 09:33:34.607000 UTC,2021-03-04 14:00:13.497000 UTC,,129,0,0,47,,,,,,['mlflow']
Where do augmentations in ClearML run?,"<p>In ClearML Dataviews, it is possible to add <a href=""https://clear.ml/docs/latest/docs/hyperdatasets/dataviews/#data-augmentation"" rel=""nofollow noreferrer"">augmentations</a>.</p>
<p>Where do these augmentations run?</p>
<p>Options</p>
<ol>
<li>Original data gets downloaded to local, then runs (on which device? How is multiprocessing handled?)</li>
<li>Only augmented data gets downloaded to local cache, augmentations run remotely (who pays for compute? How fast? Should pipelines be changed accordingly?)</li>
</ol>
<p>I couldn't find this in the docs.</p>",1,0,2022-07-25 10:52:01.690000 UTC,,,0,deep-learning|data-augmentation|mlops|clearml,22,2011-08-25 22:58:29.233000 UTC,2022-09-24 23:30:23.147000 UTC,"Technion, Israel",18777,2376,137,2000,,,,,,['clearml']
how to access text file from s3 bucket into sagemaker for training a model?,"<p>I am trying to train chatbot model using tensorflow and seq to seq architecture using sagemaker also I have completed coding in spyder but when 
I am trying to access cornel movie corpus dataset from s3 bucket into sagemaker it says no such file or directory even granting access to s3 bucket</p>",1,0,2020-04-25 06:17:51.797000 UTC,,2020-04-25 09:43:04.553000 UTC,0,amazon-s3|amazon-sagemaker,576,2020-04-25 06:07:24.533000 UTC,2020-09-22 16:42:09.080000 UTC,"Pune, Maharashtra, India",11,0,0,0,,,,,,['amazon-sagemaker']
Sending .tif over post request,"<p>I have some images that were saved in .tif format (satellite images), and I'm not sure how to send them to an endpoint for inference. I tried turning it into a numpy ndarray and then saving it as a JSON, but it grew 10x in size and I can't send it to the endpoint since it is 2x bigger than what the endpoint accepts.</p>
<p>Is there some other format that I can send the data via <code>requests.post()</code> so that it's not too big for the endpoint to accept? I'm not sure it would make sense to convert the .tif file into a png or jpg?</p>
<hr />
<p>As an aside, I'm also using SageMaker for predictions and they have a Serializer and Deserializer for the endpoint, but I'm still working on that. I tried saving the ndarray in a BytesIO(), but apparently there's only a deserializer for bytes, not a serializer...?</p>",0,1,2021-10-01 23:32:08.397000 UTC,,,0,image-processing|python-requests|amazon-sagemaker,47,2016-07-19 00:48:21.237000 UTC,2022-09-13 07:31:55.037000 UTC,,819,42,0,87,,,,,,['amazon-sagemaker']
Load Python Pickle File from S3 Bucket to Sagemaker Notebook,"<p>I have attempted the code on the many posts on how to load a pickle file (1.9GB) from an S3 bucket, but none seem to work for our notebook instance on AWS Sagemaker.  Notebook size is 50GB.</p>
<p>Some of the methods attempted:</p>
<p>Method 1</p>
<pre><code>import io
import boto3

client = boto3.client('s3')
bytes_buffer = io.BytesIO()
client.download_fileobj(Bucket=my_bucket, Key=my_key_path, Fileobj=bytes_buffer)

bytes_io.seek(0) 
byte_value = pickle.load(bytes_io)
</code></pre>
<p>This gives:</p>
<p><a href=""https://i.stack.imgur.com/rMmJx.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rMmJx.png"" alt=""enter image description here"" /></a></p>
<p>Method 2: This actually gets me something back with no error:</p>
<pre><code>client = boto3.client('s3')
bytes_buffer = io.BytesIO()
client.download_fileobj(Bucket=my_bucket, Key=my_key_path, Fileobj=bytes_buffer)
byte_value = bytes_buffer.getvalue()
import sys
sys.getsizeof(byte_value)/(1024**3)
</code></pre>
<p>this returns: 1.93</p>
<p>but how do I convert the byte_value into the pickled object?
I tried this:</p>
<pre><code>pickled_data = pickle.loads(byte_value)
</code></pre>
<p>But the kernel &quot;crashed&quot; - went idle and I lost all variables.</p>",1,0,2020-10-01 15:50:32.000000 UTC,,,2,python|amazon-web-services|amazon-s3|boto3|amazon-sagemaker,2485,2012-05-27 16:46:50.000000 UTC,2022-08-30 09:49:13.917000 UTC,,1937,249,0,221,,,,,,['amazon-sagemaker']
How to display version in Mlflow UI in column,"<p>I have developed multiple modeling in mlflow where in I would like to create model versioning so that the version of that model can be track down for the easy identification based on the timestamp.</p>
<p>Kindly provide the documentation specific to model versioning.</p>",1,2,2022-03-23 05:54:52.247000 UTC,,2022-03-23 16:20:54.723000 UTC,0,mlflow,94,2022-03-23 05:43:50.177000 UTC,2022-09-19 05:47:27.913000 UTC,,1,0,0,2,,,,,,['mlflow']
Vertex AI Endpoints - Failed to create endpoint,"<p>I'm trying to deploy a custom model to an endpoint with Vertex AI. I run the custom training and the model was correctly created in my bucket using Tensorflow 2 <code>export_saved_model</code> for estimators. In this bucket there is the <code>saved_model.pb</code> file with the folder <code>variables</code>.
However, when I try to create an endpoint selecting the path to the saved model, the following error occurs:</p>
<p><code>Failed to create endpoint &quot;endpoint_name&quot; due to error: APPLICATION_ERROR; google.cloud.ml.v1/ModelService.CreateVersion;Field: version.deployment_uri Error: Deployment directory gs://different_bucket/artifacts/ is expected to contain exactly one of: [saved_model.pb, saved_model.pbtxt].</code></p>
<p>It seems it is searching the .pb file in a bucket that is not the one I set.
Suggestions?</p>",1,5,2022-03-21 10:58:50.843000 UTC,,,1,google-cloud-endpoints|google-cloud-vertex-ai,345,2021-03-24 12:34:53.617000 UTC,2022-09-22 15:26:12.103000 UTC,"Alatri, Frosinone, FR",67,3,0,33,,,,,,['google-cloud-vertex-ai']
Azure ML Multiclass Classification output,"<p>I'm using a Multiclass Classification: News categorization example from azure ML and trying to setup a simple predictive web service that should get article as parameter and return me scored label to which category the input article text belongs, however doesn't meter what I tried it always require 2 parameters : ID , Text , and when I'm passing a new ID that was not existed in training , it gives me error that there is no rows in dataset, my question is how it suppose to work , what I'm missing here?</p>

<p><a href=""https://gallery.cortanaintelligence.com/Experiment/Multiclass-Classification-News-categorization-2"" rel=""nofollow"">https://gallery.cortanaintelligence.com/Experiment/Multiclass-Classification-News-categorization-2</a></p>",1,0,2016-06-07 05:41:15.717000 UTC,,,0,azure|azure-machine-learning-studio,388,2011-01-04 18:42:26.803000 UTC,2020-09-01 16:42:52.363000 UTC,,251,31,0,52,,,,,,['azure-machine-learning-studio']
Using cloudwatch api get_metric_data to get sagemaker endpoints invocation metrics,"<p>I am trying to use aws cloudwatch api get_metric_data to get sagemaker endpoint invocation metrics in python, and it returns me empty timestamps and Values, but there are some invocations between the time I specified, so there is something going wrong. Below is the code I write in python.</p>

<pre><code>cloudwatch.get_metric_data(
MetricDataQueries=[
    {
        'Id': 'm1',
        'MetricStat': {
            'Metric': {
                'Namespace': 'AWS/SageMaker',
                'MetricName': 'Invocations',
                'Dimensions': [
                    {
                        'Name': 'EndpointName',
                        'Value': 'users-hcl-2',
                    },
                    {
                        'Name': 'VariantName',
                        'Value': 'AllTraffic',
                    },
                ]
            },
            'Period': 3600,
            'Stat': 'Sum',
            'Unit': 'None'
        },
        'ReturnData': True,
    },
],
StartTime=datetime(2019, 2, 1),
EndTime=datetime(2019,2,13),
)
</code></pre>

<p>And it returns below:</p>

<pre><code>    {'MetricDataResults': [{'Id': 'm1',
   'Label': 'Invocations',
   'Timestamps': [],
   'Values': [],
   'StatusCode': 'Complete'}],
 'ResponseMetadata': {'RequestId': '8dd847eb-3b43-11e9-b50f-5f6fedb3e07d',
  'HTTPStatusCode': 200,
  'HTTPHeaders': {'x-amzn-requestid': '8dd847eb-3b43-11e9-b50f-5f6fedb3e07d',
   'content-type': 'text/xml',
   'content-length': '494',
   'date': 'Thu, 28 Feb 2019 10:28:13 GMT'},
  'RetryAttempts': 0}}
</code></pre>

<p>As I said, the timestamp and values shouldn't be empty, can you help me to sort out where I did wrong, I found some useful links below:</p>

<p>cloudwatch concepts: <a href=""https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_concepts.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_concepts.html</a></p>

<p>invocation metrics info:
<a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/monitoring-cloudwatch.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/monitoring-cloudwatch.html</a></p>

<p>cloudwatch get_metric_data api:
<a href=""https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/"" rel=""nofollow noreferrer"">https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/</a><a href=""https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/cloudwatch.html#CloudWatch.Client.get_metric_data"" rel=""nofollow noreferrer"">cloudwatch.html#CloudWatch.Client.get_metric_data</a></p>

<p>For what I have already tried, changing ""Period"" entity to different values, but it does no help. Thanks in advance.</p>",2,0,2019-02-28 10:39:36.037000 UTC,,,1,python|amazon-web-services|amazon-cloudwatch|amazon-sagemaker,2925,2019-02-28 09:49:41.900000 UTC,2020-02-28 20:24:25.610000 UTC,"Berlin, Germany",11,0,0,5,,,,,,['amazon-sagemaker']
Frozen training of Keras / Tensorflow model,"<p>I am training CNN Keras/TF models in AzureML on GPU compute instance.
Below exemplary architecture that I am running:</p>
<pre><code>def define_model_1D_v17():
   model = Sequential()
   model.add( Conv1D(256, (5,), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(22, 20)))
   model.add(Flatten())
   model.add(Dense(1024, activation='relu', kernel_initializer='he_uniform',))
   model.add(Dense(1, activation='sigmoid'))
   opt = Adam(learning_rate=0.0005)
   model.compile(optimizer=opt, loss='binary_crossentropy ', metrics=['accuracy'])
   return model
</code></pre>
<p>I observe &quot;non deterministic&quot; problem of &quot;freezing&quot; training procedure.
More specifically, some invocations of:</p>
<pre><code>history = model.fit(train_gen, epochs=10, validation_data=test_gen, verbose=1)
</code></pre>
<p>have almost constant loss/accuracy over batches and epochs (only slightly oscillating).
Exemplary printout is following (first run frozen, second ok):</p>
<pre><code>fold 2 a45db86ea43b5a95a96ef7be349d9f2f
Epoch 1/10
1846/1846 [==============================] - 50s 27ms/step - loss: 0.4508 - accuracy: 0.5489 - val_loss: 0.5220 - val_accuracy: 0.47
Epoch 2/10
1846/1846 [==============================] - 49s 27ms/step - loss: 0.4508 - accuracy: 0.5492 - val_loss: 0.5220 - val_accuracy: 0.47
Epoch 3/10
1846/1846 [==============================] - 49s 27ms/step - loss: 0.4508 - accuracy: 0.5492 - val_loss: 0.5220 - val_accuracy: 0.47
Epoch 4/10
1846/1846 [==============================] - 49s 27ms/step - loss: 0.4508 - accuracy: 0.5492 - val_loss: 0.5220 - val_accuracy: 0.47
Epoch 5/10
1846/1846 [==============================] - 49s 27ms/step - loss: 0.4508 - accuracy: 0.5492 - val_loss: 0.5220 - val_accuracy: 0.47
Epoch 6/10
1846/1846 [==============================] - 49s 27ms/step - loss: 0.4508 - accuracy: 0.5492 - val_loss: 0.5220 - val_accuracy: 0.47
Epoch 7/10
1846/1846 [==============================] - 49s 27ms/step - loss: 0.4508 - accuracy: 0.5492 - val_loss: 0.5220 - val_accuracy: 0.47
Epoch 8/10
1846/1846 [==============================] - 49s 27ms/step - loss: 0.4508 - accuracy: 0.5492 - val_loss: 0.5220 - val_accuracy: 0.47
Epoch 9/10
1846/1846 [==============================] - 49s 27ms/step - loss: 0.4508 - accuracy: 0.5492 - val_loss: 0.5220 - val_accuracy: 0.47
Epoch 10/10
1846/1846 [==============================] - 49s 27ms/step - loss: 0.4508 - accuracy: 0.5492 - val_loss: 0.5220 - val_accuracy: 0.47
14716/14716 [==============================] - 26s 2ms/step
fold 3 8fa8e37b3c345f05b5aeff167939cc96
Epoch 1/10
1875/1875 [==============================] - 51s 27ms/step - loss: 0.1372 - accuracy: 0.8224 - val_loss: 0.0954 - val_accuracy: 0.86
Epoch 2/10
1875/1875 [==============================] - 50s 27ms/step - loss: 0.1058 - accuracy: 0.8511 - val_loss: 0.0921 - val_accuracy: 0.87
Epoch 3/10
1875/1875 [==============================] - 51s 27ms/step - loss: 0.1038 - accuracy: 0.8539 - val_loss: 0.0944 - val_accuracy: 0.86
Epoch 4/10
1875/1875 [==============================] - 51s 27ms/step - loss: 0.1021 - accuracy: 0.8563 - val_loss: 0.0931 - val_accuracy: 0.86
Epoch 5/10
1875/1875 [==============================] - 51s 27ms/step - loss: 0.1003 - accuracy: 0.8590 - val_loss: 0.0912 - val_accuracy: 0.87
Epoch 6/10
 1875/1875 [==============================] - 50s 27ms/step - loss: 0.0982 - accuracy: 0.8623 - val_loss: 0.0915 - val_accuracy: 0.87
Epoch 7/10
1875/1875 [==============================] - 51s 27ms/step - loss: 0.0957 - accuracy: 0.8665 - val_loss: 0.0918 - val_accuracy: 0.87
Epoch 8/10
1875/1875 [==============================] - 51s 27ms/step - loss: 0.0924 - accuracy: 0.8723 - val_loss: 0.0928 - val_accuracy: 0.86
Epoch 9/10
1875/1875 [==============================] - 51s 27ms/step - loss: 0.0882 - accuracy: 0.8795 - val_loss: 0.0943 - val_accuracy: 0.86
Epoch 10/10
1875/1875 [==============================] - 51s 27ms/step - loss: 0.0835 - accuracy: 0.8874 - val_loss: 0.0950 - val_accuracy: 0.86
</code></pre>
<p>I tried both &quot;he_uniform&quot; and default kernel initializer, no effect.</p>
<p>I also monitor RAM usage and is not close to full.</p>
<p>I also checked the predictions that those are actually screwed up, not only the printout.</p>
<p>Interestingly, I also experienced this type of problems in the past on CPU machine (with older version of Keras/TF).</p>
<p>How to fix/limit this problem?</p>",0,3,2022-09-01 15:54:50.410000 UTC,,2022-09-05 07:44:56.870000 UTC,0,python|tensorflow|keras|azure-machine-learning-service,64,2019-08-30 07:54:34.137000 UTC,2022-09-21 14:49:35.580000 UTC,"Kraków, Poland",304,221,1,35,,,,,,['azure-machine-learning-service']
Errors on Azure Machine Learning notebooks: The truth value of a DataFrame is ambiguous,"<p>I'm working on deploying a machine learning model on Azure while using their notebooks and I ran into an error. I'm following this <a href=""https://docs.microsoft.com/en-us/azure/machine-learning/tutorial-power-bi-custom-model"" rel=""nofollow noreferrer"">tutorial</a> for a Power BI integration.</p>
<p>When this code got executed:</p>
<pre class=""lang-py prettyprint-override""><code>import sklearn

from azureml.core import Workspace
from azureml.core import Model
from azureml.core.resource_configuration import ResourceConfiguration

ws = Workspace.from_config()
model = Model.register(workspace=ws,
                       model_name='POC',                # Name of the registered model in your workspace.
                       model_path='./final_model.pkl',  # Local file to upload and register as a model.
                       model_framework=Model.Framework.SCIKITLEARN,  # Framework used to create the model.
                       model_framework_version=sklearn.__version__,  # Version of scikit-learn used to create the model.
                       sample_input_dataset=X,
                       sample_output_dataset=y,
                       resource_configuration=ResourceConfiguration(cpu=2, memory_in_gb=4),
                       description='RandomForestClassifier.',
                       tags={'area': 'finance', 'type': 'classification'})

print('Name:', model.name)
print('Version:', model.version)
</code></pre>
<p>I ran into the following error:</p>
<pre class=""lang-py prettyprint-override""><code>ValueError: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().
Menu Uitvoer
Registering model POC
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-65-d8fe09fe9514&gt; in &lt;module&gt;
     15                        resource_configuration=ResourceConfiguration(cpu=2, memory_in_gb=4),
     16                        description='RandomForestClassifier.',
---&gt; 17                        tags={'area': 'finance', 'type': 'classification'})
     18 
     19 print('Name:', model.name)

/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azureml/core/model.py in register(workspace, model_path, model_name, tags, properties, description, datasets, model_framework, model_framework_version, child_paths, sample_input_dataset, sample_output_dataset, resource_configuration)
    593                                            sample_input_dataset=sample_input_dataset,
    594                                            sample_output_dataset=sample_output_dataset,
--&gt; 595                                            resource_configuration=resource_configuration)
    596 
    597         return model
</code></pre>
<p>My sample_input_dataset for X is my Pandas DataFrame and my sample_output_dataset for y is a Pandas Series and I also tried to use Numpy Arrays for this.</p>",1,1,2021-10-19 08:44:33.453000 UTC,,2021-10-19 09:05:37.743000 UTC,0,python|pandas|machine-learning|azure-machine-learning-service,121,2020-02-26 13:32:13.877000 UTC,2022-09-22 20:45:33.947000 UTC,,51,7,0,12,,,,,,['azure-machine-learning-service']
"ERROR: Setup iteration failed: Unidentified error, check logs in portal / compute",<p>Getting error when trying to run the autoML through training cluster. But it is running successfully via the local run. </p>,1,1,2020-02-10 11:59:27.710000 UTC,,,1,python|automl|azure-machine-learning-service,91,2018-04-01 15:41:12.890000 UTC,2022-09-12 07:15:23.173000 UTC,"Kolkata, West Bengal, India",671,6,0,76,,,,,,['azure-machine-learning-service']
"AuthorizationFailed - ""The client 'xxx' with object id 'xxx does not have authorization to perform action","<p>I've tried to get Workspace from config which I do have access to, but it fails with the following error:</p>

<pre><code>import azureml.core

print(""SDK version:"", azureml.core.VERSION)

from azureml.core.workspace import Workspace
ws = Workspace.from_config()
print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep='\n')
</code></pre>

<blockquote>
  <p>SDK version: 0.1.80 Found the config file in:
  C:\Users\gubert\Repos\Gimmonix\HotelMappingAI\aml_config\config.json
  get_workspace error using subscription_id=xxxxxxxxxxxxxxxxxxxxxxx,
  resource_group_name=xxxxxxxxxxxx, workspace_name=gmx-ml-mapping
  Traceback (most recent call last): File
  ""C:\Users\gubert.azureml\envs\myenv\lib\site-packages\azureml_project_commands.py"",
  line 320, in get_workspace workspace_name) File
  ""C:\Users\gubert.azureml\envs\myenv\lib\site-packages\azureml_base_sdk_common\workspace\operations\workspaces_operations.py"",
  line 78, in get raise
  models.ErrorResponseWrapperException(self._deserialize, response)
  azureml._base_sdk_common.workspace.models.error_response_wrapper.ErrorResponseWrapperException:
  Operation returned an invalid status code 'Forbidden'</p>
  
  <p>During handling of the above exception, another exception occurred:</p>
  
  <p>Traceback (most recent call last): File
  ""c:\Users\gubert.vscode\extensions\ms-python.python-2018.10.1\pythonFiles\experimental\ptvsd_launcher.py"",
  line 38, in  main(sys.argv) File
  ""c:\Users\gubert.vscode\extensions\ms-python.python-2018.10.1\pythonFiles\experimental\ptvsd\ptvsd_main_.py"",
  line 265, in main wait=args.wait) File
  ""c:\Users\gubert.vscode\extensions\ms-python.python-2018.10.1\pythonFiles\experimental\ptvsd\ptvsd_main_.py"",
  line 256, in handle_args run_main(addr, name, kind, *extra, **kwargs)
  File
  ""c:\Users\gubert.vscode\extensions\ms-python.python-2018.10.1\pythonFiles\experimental\ptvsd\ptvsd_local.py"",
  line 52, in run_main runner(addr, name, kind == 'module', *extra,
  **kwargs) File ""c:\Users\gubert.vscode\extensions\ms-python.python-2018.10.1\pythonFiles\experimental\ptvsd\ptvsd\runner.py"",
  line 32, in run set_trace=False) File
  ""c:\Users\gubert.vscode\extensions\ms-python.python-2018.10.1\pythonFiles\experimental\ptvsd\ptvsd_vendored\pydevd\pydevd.py"",
  line 1283, in run return self._exec(is_module, entry_point_fn,
  module_name, file, globals, locals) File
  ""c:\Users\gubert.vscode\extensions\ms-python.python-2018.10.1\pythonFiles\experimental\ptvsd\ptvsd_vendored\pydevd\pydevd.py"",
  line 1290, in _exec pydev_imports.execfile(file, globals, locals) #
  execute the script File
  ""c:\Users\gubert.vscode\extensions\ms-python.python-2018.10.1\pythonFiles\experimental\ptvsd\ptvsd_vendored\pydevd_pydev_imps_pydev_execfile.py"",
  line 25, in execfile exec(compile(contents+""\n"", file, 'exec'), glob,
  loc) File ""c:\Users\gubert\Repos\Gimmonix\HotelMappingAI\test.py"",
  line 8, in  ws = Workspace.from_config() File
  ""C:\Users\gubert.azureml\envs\myenv\lib\site-packages\azureml\core\workspace.py"",
  line 153, in from_config auth=auth) File
  ""C:\Users\gubert.azureml\envs\myenv\lib\site-packages\azureml\core\workspace.py"",
  line 86, in init auto_rest_workspace = _commands.get_workspace(auth,
  subscription_id, resource_group, workspace_name) File
  ""C:\Users\gubert.azureml\envs\myenv\lib\site-packages\azureml_project_commands.py"",
  line 326, in get_workspace resource_error_handling(response_exception,
  WORKSPACE) File
  ""C:\Users\gubert.azureml\envs\myenv\lib\site-packages\azureml_base_sdk_common\common.py"",
  line 270, in resource_error_handling raise
  ProjectSystemException(response_message)
  azureml.exceptions._azureml_exception.ProjectSystemException: {
  ""error_details"": { ""error"": { ""code"": ""AuthorizationFailed"",
  ""message"": ""The client 'xxxxxxxxxx@microsoft.com' with object id
  'xxxxxxxxxxxxx' does not have authorization to perform action
  'Microsoft.MachineLearningServices/workspaces/read' over scope
  '/subscriptions/xxxxxxxxxxxxxx/resourceGroups/CarsolizeCloud - Test
  Global/providers/Microsoft.MachineLearningServices/workspaces/gmx-ml-mapping'.""
  } }, ""status_code"": 403, ""url"":
  ""<a href=""https://management.azure.com/subscriptions/xxxxxxxxxxxxx/resourceGroups/CarsolizeCloud%20-%20Test%20Global/providers/Microsoft.MachineLearningServices/workspaces/gmx-ml-mapping?api-version=2018-03-01-preview"" rel=""nofollow noreferrer"">https://management.azure.com/subscriptions/xxxxxxxxxxxxx/resourceGroups/CarsolizeCloud%20-%20Test%20Global/providers/Microsoft.MachineLearningServices/workspaces/gmx-ml-mapping?api-version=2018-03-01-preview</a>""
  }</p>
</blockquote>",1,0,2018-11-25 08:54:30.157000 UTC,,,0,azure-machine-learning-studio,787,2009-04-08 09:33:03.637000 UTC,2022-03-15 15:40:25.727000 UTC,,584,81,1,111,,,,,,['azure-machine-learning-studio']
Kubeflow vs Vertex AI Pipelines,"<p>I was exploring kubeflow pipelines and Vertex AI pipelines. From what I understand, Vertex AI pipelines is a managed version of kubeflow pipelines so one doesn't need to deploy a full fledged kubeflow instance. In that respect, pricing aside, Vertex AI pipelines is a better choice. But then, in kubeflow, one can create <a href=""https://www.kubeflow.org/docs/components/pipelines/concepts/experiment/"" rel=""noreferrer"">experiments</a>, an equivalent for which I have not found in Vertex AI pipelines. The only kubeflow features that Vertex AI does not support that I have been able to spot in the <a href=""https://cloud.google.com/vertex-ai/docs/pipelines/migrate-kfp#features_not_supported_in"" rel=""noreferrer"">documentation</a> are &quot;Cache expiration&quot; and &quot;Recursion&quot; but they do not mention anything about experiments. Makes me wonder if there are other differences that are worth considering when deciding between the two.</p>",2,0,2021-12-15 04:21:00.217000 UTC,,,9,google-cloud-platform|google-ai-platform|kubeflow-pipelines|google-cloud-vertex-ai|google-cloud-ai-platform-pipelines,3371,2016-08-15 20:29:46.790000 UTC,2022-09-24 22:22:50.413000 UTC,,700,17,1,90,,,,,,['google-cloud-vertex-ai']
how to handle wandb Invalid filename characters exception on uploading image,"<p>I am using windows 10 &amp; venv &amp; python 3.9.7
this is my code to upload image to wandb</p>
<pre><code>wandb_log[&quot;Image/train_image&quot;] = wandb.Image('tmp.jpg')
wandb.log(wandb_log, step)
</code></pre>
<p>the full directory of image is “C:\Users\이준혁\Documents\Github\terenz\tmp.jpg”
However it creates this error</p>
<pre><code>Media Image/train_image is invalid. Please remove invalid filename characters
</code></pre>
<p>reinstalling wandb did not help to solve this problem.<br />
Any suggestions? Thanks</p>",0,2,2022-01-07 01:01:11.160000 UTC,,,0,wandb,94,2020-10-22 15:00:31.587000 UTC,2022-09-19 00:28:34.893000 UTC,,161,10,0,29,,,,,,['wandb']
Custom Model for Batch Prediction on Vertex.ai,"<p>I want to run batch predictions inside Google Cloud's vertex.ai using a custom trained model.  I was able to find documentation to get online prediction working with a custom built docker image by setting up an endpoint, but I can't seem to find any documentation on what the Dockerfile should be for batch prediction.  Specifically how does my custom code get fed the input and where does it put the output?</p>
<p>The documentation I've found is <a href=""https://cloud.google.com/vertex-ai/docs/predictions/batch-predictions?_ga=2.262616524.-1738078585.1553812508&amp;_gac=1.83826788.1630620723.Cj0KCQjw7MGJBhD-ARIsAMZ0eesi3e2_fghoLZcWRKqw_ZbncT3LeZvgYPu929bJELdeiX3RSNHPApcaAo8dEALw_wcB#custom-trained_3"" rel=""nofollow noreferrer"">here</a>, it certainly looks possible to use a custom model and when I tried it didn't complain, but eventually it did throw an error.  According to the documentation no endpoint is required for running batch jobs.</p>",0,10,2021-09-20 15:55:16.607000 UTC,,,0,google-cloud-ml|google-cloud-vertex-ai|google-cloud-ai,480,2018-10-30 18:54:03.620000 UTC,2022-09-23 17:00:36.977000 UTC,"Utah, USA",1212,699,4,110,,,,,,['google-cloud-vertex-ai']
Unlinking two modules in the designer - how do I remove the arrow?,"<p>I have a ludicrously simple question but one that I cannot decipher, unfortunately, for myself. I am doing the DP-100 certification and am working through the tutorials and I have insert an &quot;Execute python script&quot; module in between the &quot;Score Model&quot; and &quot;Web service output&quot; modules.</p>
<p>I cannot do this. On the left is what I currently have and on the right is what I'm trying to achieve.</p>
<p>Excruciatingly, it does not let me link the &quot;Execute python script&quot; to the &quot;Web service output&quot; module as it is still linked to the &quot;Score model&quot; module. How can I unlink these two? There is surely an extremely simple way, but nothing intuitive works. Double clicking, right clicking, dragging / dropping, nothing.</p>
<p><a href=""https://i.stack.imgur.com/Xtkey.png"" rel=""nofollow noreferrer"">What I have and what the tutorial is telling me to do</a></p>
<p>I know I can delete the final &quot;Web service output&quot; module and create another but <em>surely</em> that is not the best way to do this. If I had a module with a lot of specific params set up, I'd have to create it again from scratch!</p>
<p>Any help is massively appreciated.</p>
<p><a href=""https://i.stack.imgur.com/fNr4T.png"" rel=""nofollow noreferrer"">Where I get to when I try to add the python script module and then re-draw the connective arrows.</a></p>",1,0,2021-05-27 09:14:56.727000 UTC,,,-1,python|azure-machine-learning-studio,17,2020-02-18 23:15:52.500000 UTC,2021-06-01 00:09:34.777000 UTC,"Sydney NSW, Australia",1,0,0,3,,,,,,['azure-machine-learning-studio']
score/entry file for azure machine learning web services using azuremlsdk,"<p>Looking at <a href=""https://github.com/Azure/azureml-sdk-for-r/blob/master/samples/deployment/deploy-to-aci/deploy-to-aci.R"" rel=""nofollow noreferrer"">this example</a>:</p>
<pre><code>library(azuremlsdk)
library(jsonlite)

ws &lt;- load_workspace_from_config()

# Register the model
model &lt;- register_model(ws, model_path = &quot;model.rds&quot;, model_name = &quot;model.rds&quot;)

# Create environment
r_env &lt;- r_environment(name = &quot;r_env&quot;)

# Create inference config
inference_config &lt;- inference_config(
  entry_script = &quot;score.R&quot;,
  source_directory = &quot;.&quot;,
  environment = r_env)

# Create ACI deployment config
deployment_config &lt;- aci_webservice_deployment_config(cpu_cores = 1,
                                                      memory_gb = 1)

# Deploy the web service
service_name &lt;- paste0('aciwebservice-', sample(1:100, 1, replace=TRUE))
service &lt;- deploy_model(ws, 
                        service_name, 
                        list(model), 
                        inference_config, 
                        deployment_config)
wait_for_deployment(service, show_output = TRUE)
</code></pre>
<p>Could it be that score.R has to be uploaded to the Azure environment and is not local as in the sense that it is on the dev machine? My current thinking is, that source_directory . refers to the local system (i.e. on the dev machine)?</p>",1,0,2021-05-17 13:44:07.413000 UTC,,,0,r|azure-machine-learning-service|azuremlsdk,50,2010-03-01 10:53:04.443000 UTC,2022-09-24 18:56:19.313000 UTC,Somewhere,15705,2171,91,2150,,,,,,['azure-machine-learning-service']
Worker died/Worker disconnected Sagemaker,"<p>I am trying to set up a multi-model endpoint (or more accurately re-set it up as I am pretty sure it was working a while ago, on an earlier version of sagemaker) to do language translation. But am constantly met with the same issue. This is what I am trying to run (from a notebook on sagemaker):</p>
<pre class=""lang-py prettyprint-override""><code>import sagemaker
from sagemaker.pytorch.model import PyTorchModel
from sagemaker.predictor import JSONSerializer, JSONDeserializer

role = 'role_name...'
pytorch_model = PyTorchModel(model_data='s3://foreign-language-models/opus-mt-ROMANCE-en.tar.gz',
                             role=role,
                             framework_version=&quot;1.3.1&quot;,
                             py_version=&quot;py3&quot;,
                             source_dir=&quot;code&quot;,
                             entry_point=&quot;deploy_multi_model.py&quot;)
x = pytorch_model.predictor_cls(endpoint_name='language-translation')
x.serializer = JSONSerializer()
x.deserializer = JSONDeserializer()

x.predict({'model_name': 'opus-mt-ROMANCE-en', 'text': [&quot;Hola que tal?&quot;]})
</code></pre>
<p>To which I am met with the error:</p>
<pre><code>ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from model with message &quot;{
  &quot;code&quot;: 500,
  &quot;type&quot;: &quot;InternalServerException&quot;,
  &quot;message&quot;: &quot;Worker died.&quot;
}
</code></pre>
<p>And when I investigate the the logs the error links to the only notable one says:</p>
<pre><code>epollEventLoopGroup-4-1 com.amazonaws.ml.mms.wlm.WorkerThread - 9000 Worker disconnected. WORKER_MODEL_LOADED
</code></pre>
<p>But I cannot figure out why this is happening.
Any help would be greatly appreciated as this is currently driving me insane! And if you need any more information from me to help, don't hesitate to ask.</p>",0,1,2020-11-27 21:28:57.890000 UTC,,,8,amazon-sagemaker,775,2015-03-07 14:33:57.597000 UTC,2022-08-28 20:06:25.610000 UTC,Ireland,191,6,0,12,,,,,,['amazon-sagemaker']
"AWS sagemaker training data returns UnexpectedStatusException, Reason: AlgorithmError: framework error:","<p>I am trying to learn AWS by following tutorial on <a href=""https://aws.amazon.com/getting-started/hands-on/build-train-deploy-machine-learning-model-sagemaker/?nc1=h_ls"" rel=""nofollow noreferrer"">AWS website</a>.</p>
<p>I followed each step but step 4c where I train model with data returns error.</p>
<p><code>xgb.fit({'train': s3_input_train})</code></p>
<pre><code>UnexpectedStatusException: Error for Training job sagemaker-xgboost-2022-09-13-14-05-21-675: Failed. Reason: AlgorithmError: framework error: 
Traceback (most recent call last):
  File &quot;/miniconda3/lib/python3.7/site-packages/sagemaker_algorithm_toolkit/hyperparameter_validation.py&quot;, line 278, in validate
    hyperparameter_obj = self.hyperparameters[hp]
KeyError: 'silent'
</code></pre>
<p>I searched for solution but couldn't find it.</p>
<p>Does this have something to do with my location? Because I set my region to ap-northeast-2.</p>",0,3,2022-09-13 14:54:31.740000 UTC,,,0,python|amazon-web-services|amazon-s3|amazon-sagemaker,21,2017-01-07 05:02:49.590000 UTC,2022-09-14 14:26:06.260000 UTC,Korea,87,5,0,30,,,,,,['amazon-sagemaker']
MLFlow project run fails during conda env creation,"<p>I am trying to get mlflow mlproject working.</p>

<p>When i run the mlflow run with repo name</p>

<pre><code>mlflow run  git@gitlabe2.xx.yy.zz:name/mlflow-example.git
</code></pre>

<p>The execution fails with the below error</p>

<pre><code>File ""/home/example/miniconda/envs/mlflow/lib/python3.7/site-packages/mlflow/projects/__init__.py"", line 265, in run
use_conda=use_conda, storage_dir=storage_dir, synchronous=synchronous, run_id=run_id)
File ""/home/example/miniconda/envs/mlflow/lib/python3.7/site-packages/mlflow/projects/__init__.py"", line 144, in _run
conda_env_name = _get_or_create_conda_env(project.conda_env_path)
File ""/home/example/miniconda/envs/mlflow/lib/python3.7/site-packages/mlflow/projects/__init__.py"", line 144, in _run
conda_env_name = _get_or_create_conda_env(project.conda_env_path)
File ""/home/example/miniconda/envs/mlflow/lib/python3.7/site-packages/mlflow/projects/__init__.py"", line 498, in _get_or_create_conda_env
conda_env_path], stream_output=True)
File ""/home/example/miniconda/envs/mlflow/lib/python3.7/site-packages/mlflow/utils/process.py"", line 38, in exec_cmd
raise ShellCommandException(""Non-zero exitcode: %s"" % (exit_code))
</code></pre>

<p>Any pointers on where I should look ?</p>

<p>The suspect the conda.yaml file has some issues especially the conda env name.
I have different names for the environment where the project is created and where the project is being run. Does it matter ?</p>

<p>Thanks</p>",3,0,2019-07-28 01:54:17.600000 UTC,,,1,conda|mlflow,917,2016-06-17 02:05:56.870000 UTC,2022-09-25 05:11:52.907000 UTC,"Chengdu, Sichuan, China",2087,31,1,87,,,,,,['mlflow']
How to read json.out file from databricks,"<p>I have been working with databricks for reading output from Object2Vec in Sagemaker. This output is saved as jsonlines with <code>.json.out</code> file format.</p>

<pre><code>df_emb = spark.read.option(""multiLine"", True).option(""mode"", ""PERMISSIVE"").json(bucket+key)
</code></pre>

<p>When i read this file as a json, it is read as a corrupt record. Below is the screenshot. 
<a href=""https://i.stack.imgur.com/jyMBj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jyMBj.png"" alt=""enter image description here""></a></p>

<p>I can provide the actual file if you know the solution.</p>",1,0,2019-05-02 09:40:25.647000 UTC,,,0,amazon-web-services|databricks|amazon-sagemaker,96,2016-03-06 10:55:31.840000 UTC,2022-09-23 23:07:03.117000 UTC,"Vancouver, BC",584,71,6,270,,,,,,['amazon-sagemaker']
Vertex AI 504 Errors in batch job - How to fix/troubleshoot,"<p>We have a Vertex AI model that takes a relatively long time to return a prediction.</p>
<p>When hitting the model endpoint with one instance, things work fine.  But batch jobs of size say 1000 instances end up with around 150 504 errors (upstream request timeout). (We actually need to send batches of 65K but I'm troubleshooting with 1000).</p>
<p>I tried increasing the number of replicas assuming that the # of instances handed to the model would be (1000/# of replicas) but that doesn't seem to be the case.</p>
<p>I then read that the default batch size is 64 and so tried decreasing the batch size to 4 like this from the python code that creates the batch job:</p>
<p>model_parameters = dict(batch_size=4)</p>
<pre><code>def run_batch_prediction_job(vertex_config):

    aiplatform.init(
        project=vertex_config.vertex_project, location=vertex_config.location
    )

    model = aiplatform.Model(vertex_config.model_resource_name)

    model_params = dict(batch_size=4)
    batch_params = dict(
        job_display_name=vertex_config.job_display_name,
        gcs_source=vertex_config.gcs_source,
        gcs_destination_prefix=vertex_config.gcs_destination,
        machine_type=vertex_config.machine_type,
        accelerator_count=vertex_config.accelerator_count,
        accelerator_type=vertex_config.accelerator_type,
        starting_replica_count=replica_count,
        max_replica_count=replica_count,
        sync=vertex_config.sync,
        model_parameters=model_params
    )

    batch_prediction_job = model.batch_predict(**batch_params)

    batch_prediction_job.wait()

    return batch_prediction_job
</code></pre>
<p>I've also tried increasing the machine type to n1-high-cpu-16 and that helped somewhat but I'm not sure I understand how batches are sent to replicas?</p>
<p>Is there another way to decrease the number of instances sent to the model?
Or is there a way to increase the timeout?
Is there log output I can use to help figure this out?
Thanks</p>",1,2,2022-02-22 01:15:43.853000 UTC,,,1,google-cloud-platform|google-cloud-vertex-ai,194,2016-05-07 00:35:30.170000 UTC,2022-09-23 23:45:09.807000 UTC,"Berkeley, CA, United States",329,101,0,65,,,,,,['google-cloud-vertex-ai']
Second `ParallelRunStep` in pipeline times out at start,"<p>Im trying to run a sequence of more than one <code>ParallelRunStep</code> in an AzureML pipeline. To do so, I create a step with the following helper:</p>

<pre><code>def create_step(name, script, inp, inp_ds):
    out = pip_core.PipelineData(name=f""{name}_out"", datastore=dstore, is_directory=True)
    out_ds = out.as_dataset()
    out_ds_named = out_ds.as_named_input(f""{name}_out"")

    config = cont_steps.ParallelRunConfig(
        source_directory=""src"",
        entry_script=script,
        mini_batch_size=""1"",
        error_threshold=0,
        output_action=""summary_only"",
        compute_target=compute_target,
        environment=component_env,
        node_count=2,
        logging_level=""DEBUG""
    )

    step = cont_steps.ParallelRunStep(
        name=name,
        parallel_run_config=config,
        inputs=[inp_ds],
        output=out,
        arguments=[],
        allow_reuse=False,
    )

    return step, out, out_ds_named
</code></pre>

<p>As an example I create two steps like this</p>

<pre><code>step1, out1, out1_ds_named = create_step(""step1"", ""demo_s1.py"", input_ds, named_input_ds)
step2, out2, out2_ds_named = create_step(""step2"", ""demo_s2.py"", out1, out1_ds_named)
</code></pre>

<p>Creating an experiment and submitting it to an existing workspace and Azure ML compute cluster works. Also the first step <code>step1</code> uses the <code>input_ds</code> runs its script <code>demo_s1.py</code> (which produces its output files, and finishes successfully. </p>

<p>However the second step <code>step2</code> never get started. </p>

<p><a href=""https://i.stack.imgur.com/JPnPI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JPnPI.png"" alt=""enter image description here""></a></p>

<p>And there is a final exception</p>

<pre><code>The experiment failed. Finalizing run...
Cleaning up all outstanding Run operations, waiting 300.0 seconds
2 items cleaning up...
Cleanup took 0.16968441009521484 seconds
Starting the daemon thread to refresh tokens in background for process with pid = 394
Traceback (most recent call last):
  File ""driver/amlbi_main.py"", line 52, in &lt;module&gt;
    main()
  File ""driver/amlbi_main.py"", line 44, in main
    JobStarter().start_job()
  File ""/mnt/batch/tasks/shared/LS_root/jobs/pipeline/azureml/08a1e1e1-7c3f-4c5a-84ad-ca99b8a6cb31/mounts/workspaceblobstore/azureml/08a1e1e1-7c3f-4c5a-84ad-ca99b8a6cb31/driver/job_starter.py"", line 48, in start_job
    job.start()
  File ""/mnt/batch/tasks/shared/LS_root/jobs/pipeline/azureml/08a1e1e1-7c3f-4c5a-84ad-ca99b8a6cb31/mounts/workspaceblobstore/azureml/08a1e1e1-7c3f-4c5a-84ad-ca99b8a6cb31/driver/job.py"", line 70, in start
    master.start()
  File ""/mnt/batch/tasks/shared/LS_root/jobs/pipeline/azureml/08a1e1e1-7c3f-4c5a-84ad-ca99b8a6cb31/mounts/workspaceblobstore/azureml/08a1e1e1-7c3f-4c5a-84ad-ca99b8a6cb31/driver/master.py"", line 174, in start
    self._start()
  File ""/mnt/batch/tasks/shared/LS_root/jobs/pipeline/azureml/08a1e1e1-7c3f-4c5a-84ad-ca99b8a6cb31/mounts/workspaceblobstore/azureml/08a1e1e1-7c3f-4c5a-84ad-ca99b8a6cb31/driver/master.py"", line 149, in _start
    self.wait_for_input_init()
  File ""/mnt/batch/tasks/shared/LS_root/jobs/pipeline/azureml/08a1e1e1-7c3f-4c5a-84ad-ca99b8a6cb31/mounts/workspaceblobstore/azureml/08a1e1e1-7c3f-4c5a-84ad-ca99b8a6cb31/driver/master.py"", line 124, in wait_for_input_init
    raise exc
exception.FirstTaskCreationTimeout: Unable to create any task within 600 seconds.
Load the datasource and read the first row locally to see how long it will take.
Set the advanced argument '--first_task_creation_timeout' to a larger value in arguments in ParallelRunStep.

</code></pre>

<p>I have the impression, that the second step is waiting for some data. However the first step creates the supplied output directory and also a file. </p>

<pre><code>import argparse
import os

def init():
    pass

def run(parallel_input):
    print(f""*** Running {os.path.basename(__file__)} with input {parallel_input}"")

    parser = argparse.ArgumentParser(description=""Data Preparation"")
    parser.add_argument('--output', type=str, required=True)
    args, unknown_args = parser.parse_known_args()

    out_path = os.path.join(args.output, ""1.data"")
    os.makedirs(args.output, exist_ok=True)
    open(out_path, ""a"").close()

    return [out_path]
</code></pre>

<p>I have no idea how to debug further. Has anybody an idea?</p>",1,2,2020-04-20 14:13:15.177000 UTC,1.0,,3,python|azure|azure-machine-learning-service,287,2009-07-24 11:35:14.983000 UTC,2022-03-09 06:40:42.810000 UTC,"Ulm, Deutschland",1269,76,10,194,,,,,,['azure-machine-learning-service']
Unable to deploy locally trained Logistic Regression model on AWS Sagemaker,"<p>I have trained a Logistic Regression model on my local machine. Saved the model using Joblib and tried deploying it on Aws Sagemaker using &quot;Linear-Learner&quot; image.</p>
<p>Facing issues while deployment as the deployment process keeps continuing and the Status is always as &quot;Creating&quot; and does not turn to &quot;InService&quot;.</p>
<pre><code>endpoint_name = &quot;DEMO-LogisticEndpoint&quot; + strftime(&quot;%Y-%m-%d-%H-%M-%S&quot;, gmtime())
print(endpoint_name)
create_endpoint_response = sm_client.create_endpoint(
    EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name
)
print(create_endpoint_response[&quot;EndpointArn&quot;])

resp = sm_client.describe_endpoint(EndpointName=endpoint_name)
status = resp[&quot;EndpointStatus&quot;]
print(&quot;Status: &quot; + status)

while status == &quot;Creating&quot;:
    time.sleep(60)
    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)
    status = resp[&quot;EndpointStatus&quot;]
    print(&quot;Status: &quot; + status)
</code></pre>
<p>The while loop keeps executing and the status never change.</p>",1,0,2022-03-22 17:35:59.340000 UTC,,,0,python|amazon-web-services|machine-learning|logistic-regression|amazon-sagemaker,108,2022-03-22 11:09:20.697000 UTC,2022-04-13 14:43:22.597000 UTC,,1,0,0,0,,,,,,['amazon-sagemaker']
Vertex AI Feature Store limits,"<p>Vertex AI has <a href=""https://cloud.google.com/vertex-ai/docs/quotas#featurestore"" rel=""nofollow noreferrer"">quotas and limits</a>. Other than submitting very high quota requests, is there documentation on hard limits for all of the quotas? Things like &quot;online serving requests per minute&quot;, &quot;Concurrent batch jobs&quot;, and &quot;Entity types across all featurestores&quot; are pretty key constraints to know before committing to Vertex.</p>",1,1,2022-08-10 03:27:26.707000 UTC,,,0,google-cloud-vertex-ai,56,2013-12-10 22:23:13.850000 UTC,2022-09-21 23:19:21.323000 UTC,,3767,1128,10,226,,,,,,['google-cloud-vertex-ai']
Getting R package 'bsts' to work on AWS Sagemaker Notebook Instance Python,"<p>Just wondering if anyone has been able to get a Python + R kernel working on AWS Sagemaker Notebook instance?</p>
<p>The reason I'm asking is so I can use a python environment to run R packages within, specifically 'bsts' and 'boom'.</p>
<p>Is there a way to create a kernel that has both Python + R installed?</p>",1,1,2022-06-28 09:17:04.147000 UTC,,,0,python|r|amazon-web-services|amazon-sagemaker,43,2017-05-24 06:39:19.953000 UTC,2022-09-23 17:33:31.663000 UTC,"Wellington, New Zealand",358,10,3,84,,,,,,['amazon-sagemaker']
Error call Google Vertex AI endpoint from a python backend,"<p>I am trying to send an http post request to my google vertex ai endpoint for prediction. Though I do set the Bearer Token in the request header, the request still fails with the below error:</p>
<pre><code>{
&quot;error&quot;: {
    &quot;code&quot;: 401,
    &quot;message&quot;: &quot;Request had invalid authentication credentials. Expected OAuth 2 access token, login cookie or other valid authentication credential. See https://developers.google.com/identity/sign-in/web/devconsole-project.&quot;,
    &quot;status&quot;: &quot;UNAUTHENTICATED&quot;,
    &quot;details&quot;: [
        {
            &quot;@type&quot;: &quot;type.googleapis.com/google.rpc.ErrorInfo&quot;,
            &quot;reason&quot;: &quot;ACCESS_TOKEN_TYPE_UNSUPPORTED&quot;,
            &quot;metadata&quot;: {
                &quot;service&quot;: &quot;aiplatform.googleapis.com&quot;,
                &quot;method&quot;: &quot;google.cloud.aiplatform.v1.PredictionService.Predict&quot;
            }
        }
    ]
}
</code></pre>
<p>}</p>
<p>Since I am making this call from a python backend, I'm not sure if OAuth 2 as suggested in the message would be wise and applicable choice.</p>
<p>The model is already deployed and endpointed test on vertex ai and it worked fine. What I am trying to do is send same prediction task via an http post request using postman and this is what failed.</p>
<p>The request url looks like this:</p>
<pre><code>https://[LOCATION]-aiplatform.googleapis.com/v1/projects/[PROJECT ID]/locations/[LOCATION]/endpoints/[ENDPOINT ID]:predict
</code></pre>
<p>Where token bearer is set in the potman authorization tab and instance set in request body.</p>",3,3,2022-06-20 20:43:46.930000 UTC,,2022-06-22 23:22:27.090000 UTC,0,python|google-cloud-platform|google-cloud-endpoints|google-cloud-ml|google-cloud-vertex-ai,353,2015-03-21 06:55:29.353000 UTC,2022-09-02 13:58:42.187000 UTC,Bangkok,194,8,0,12,,,,,,['google-cloud-vertex-ai']
"No Module named ""Fastai"" when trying to deploy fastai model on sagemaker","<p>I have trained and built a Fastai(v1) model and exported it as a .pkl file.
Now i want to deploy this model for inference in Amazon Sagemaker</p>
<p>Following the Sagemaker documentation for Pytorch model [https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html#write-an-inference-script][1]</p>
<p>Steps taken<br>
Folder structure</p>
<pre>
Sagemaker/
       export.pkl
       code/
           inference.py
           requirement.txt
</pre>
<pre> 
requirement.txt

    spacy==2.3.4
    torch==1.4.0
    torchvision==0.5.0
    fastai==1.0.60
    numpy

</pre>
<p>Command i used to create the zip file</p>
<pre>
    cd Sagemaker/
    tar -czvf /tmp/model.tar.gz ./export.pkl ./code
</pre>
<p>This would generate a model.tar.gz file and i upload it to S3 bucket<br></p>
<p>To deploy this i used the python sagemaker SDK</p>
<pre><code>
    from sagemaker.pytorch import PyTorchModel
        role = &quot;sagemaker-role-arn&quot;
        model_path = &quot;s3 key for the model.tar.gz file that i created above&quot;
        pytorch_model = PyTorchModel(model_data=model_path,role=role,`entry_point='inference.py',framework_version=&quot;1.4.0&quot;, py_version=&quot;py3&quot;)
    
        predictor = pytorch_model.deploy(instance_type='ml.c5.large', initial_instance_count=1)

</code></pre>
<p>After executing the above code i see that the model is created in sagemaker and deployed but i end up getting an error running the inference</p>
<pre><code>
    botocore.errorfactory.ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from primary with message &quot;No module named 'fastai'
    Traceback (most recent call last):
      File &quot;/opt/conda/lib/python3.6/site-packages/sagemaker_inference/transformer.py&quot;, line 110, in transform
        self.validate_and_initialize(model_dir=model_dir)
      File &quot;/opt/conda/lib/python3.6/site-packages/sagemaker_inference/transformer.py&quot;, line 157, in validate_and_initialize
        self._validate_user_module_and_set_functions()
      File &quot;/opt/conda/lib/python3.6/site-packages/sagemaker_inference/transformer.py&quot;, line 170, in _validate_user_module_and_set_functions
        user_module = importlib.import_module(user_module_name)
      File &quot;/opt/conda/lib/python3.6/importlib/__init__.py&quot;, line 126, in import_module
        return _bootstrap._gcd_import(name[level:], package, level)
      File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 994, in _gcd_import
      File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 971, in _find_and_load
      File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 955, in _find_and_load_unlocked
      File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 665, in _load_unlocked
      File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 678, in exec_module
      File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed
      File &quot;/opt/ml/model/code/inference.py&quot;, line 2, in &lt;module&gt;
        from fastai.basic_train import load_learner, DatasetType, Path
    ModuleNotFoundError: No module named 'fastai'

</code></pre>
<p>Clearly the fastai module doesn't get downloaded what is the cause for this and what am i doing wrong in this case</p>",1,0,2022-02-05 13:03:11.863000 UTC,1.0,,0,python|machine-learning|amazon-sagemaker|fast-ai,125,2021-02-19 13:15:39.987000 UTC,2022-09-24 17:23:03.670000 UTC,"Kolkata, West Bengal, India",375,8,0,27,,,,,,['amazon-sagemaker']
How to manually register a sci-kit model with TRAINS python auto-magical experiment manager?,"<p>I'm working mostly with scikit-learn, as far as I understand, the TRAINS auto-magic doesn't catch scikit-learn model store/load automatically.</p>

<p>How do I manually register the model after I have 'pickled' it.</p>

<p>For Example:</p>

<pre><code>import pickle
with open(""model.pkl"", ""wb"") as file:  
    pickle.dump(my_model, file)
</code></pre>",1,1,2019-06-25 22:52:28.140000 UTC,,2021-01-04 16:43:46.447000 UTC,1,python|machine-learning|scikit-learn|trains|clearml,155,2018-06-11 14:35:01.343000 UTC,2022-03-23 09:09:41.957000 UTC,,11,0,0,9,,,,,,['clearml']
Image path from S3 to pytorch script for Sagemaker train,"<p>I am building a AWS Pipeline to train a PyTorch model using Sagemaker. I have specified the s3 bucket (data/training)in Yaml file.</p>

<p>I usually upload a csv file in keras using,</p>

<p><code>dataframe = pd.read_csv(paths.input(channel='training', filename=""ab.csv""))</code></p>

<p>In pytorch I am trying to use S3 bucket to get images in folders (labels as folder name and images inside the folder).</p>

<p>for the local compilation, the path is written as </p>

<p><code>datasource = ""xyz/data/""</code> </p>

<p><code>tr_data = ImageFolder(datasource, transform=tr_tf)</code> in the working directory.</p>

<p>for using in sagemaker tha python bundle is specified as</p>

<p><code>datasource =  paths.input(channel='training')</code> but AWS Sage-maker job fails. </p>

<p>Would be grateful if anyone help me to give a path of training images in S3 bucket.</p>",0,0,2019-10-15 06:54:00.863000 UTC,0.0,2019-10-15 11:23:14.113000 UTC,1,python|amazon-web-services|amazon-s3|pytorch|amazon-sagemaker,376,2018-04-20 10:46:49.263000 UTC,2020-08-31 21:42:48.997000 UTC,"Hamburg, Germany",73,4,0,10,,,,,,['amazon-sagemaker']
Is it possible to use the multiclass classifier of aws to recognize the given place of the text?,"<p>I'm using AWS SageMaker, and i want to create something that, with a given text, it recognize the place of that description. Is it possible?</p>",2,0,2019-08-05 16:06:23.273000 UTC,,,1,amazon-web-services|amazon-s3|classification|amazon-sagemaker|multiclass-classification,103,2018-04-16 15:34:34.820000 UTC,2022-09-22 15:26:41.440000 UTC,,125,8,0,28,,,,,,['amazon-sagemaker']
Weights and Biases: Login and network errors,"<p>I recently installed Weights and Biases (wandb) for recording the metrics of my machine learning projects. Everything worked fine when connected to wandb cloud instance or when I used a local docker image. Now, when I tried to access my local wandb instance from over the network, I started to get API error messages. However, I also noticed that wandb was trying to access my server using port 80, instead of 8080. I installed wandb client on a new cloud server and tried to access my server from there. Still, same error message shown below.</p>
<p>This error happens when I use the command: <code>wandb login host=https://api.wandb.ai</code>
I have tried to delete the .netrc file where the api settings are stored and re-installed wandb. Still same error. Using wandb version 0.10.2 on Ubuntu 18.04; Also, tried downgrading to version 0.8.36, no change.
If I try the command: <code>wandb login --relogin</code>, I get the same error.</p>
<p>Is there some way to reset wandb so it forgets all these settings, or to resolve this issue directly?</p>
<p>Many thanks</p>
<p>Best Regards,</p>
<p>Adeel</p>
<pre><code>Retry attempt failed:
Traceback (most recent call last):
  File &quot;/media/adeel/Space/AI/anaconda3/envs/tf2_gpu/lib/python3.7/site-packages/urllib3/connection.py&quot;, line 160, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw
  File &quot;/media/adeel/Space/AI/anaconda3/envs/tf2_gpu/lib/python3.7/site-packages/urllib3/util/connection.py&quot;, line 84, in create_connection
    raise err
  File &quot;/media/adeel/Space/AI/anaconda3/envs/tf2_gpu/lib/python3.7/site-packages/urllib3/util/connection.py&quot;, line 74, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;/media/adeel/Space/AI/anaconda3/envs/tf2_gpu/lib/python3.7/site-packages/urllib3/connectionpool.py&quot;, line 677, in urlopen
    chunked=chunked,
  File &quot;/media/adeel/Space/AI/anaconda3/envs/tf2_gpu/lib/python3.7/site-packages/urllib3/connectionpool.py&quot;, line 392, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File &quot;/media/adeel/Space/AI/anaconda3/envs/tf2_gpu/lib/python3.7/http/client.py&quot;, line 1277, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File &quot;/media/adeel/Space/AI/anaconda3/envs/tf2_gpu/lib/python3.7/http/client.py&quot;, line 1323, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File &quot;/media/adeel/Space/AI/anaconda3/envs/tf2_gpu/lib/python3.7/http/client.py&quot;, line 1272, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File &quot;/media/adeel/Space/AI/anaconda3/envs/tf2_gpu/lib/python3.7/http/client.py&quot;, line 1032, in _send_output
    self.send(msg)
  File &quot;/media/adeel/Space/AI/anaconda3/envs/tf2_gpu/lib/python3.7/http/client.py&quot;, line 972, in send
    self.connect()
  File &quot;/media/adeel/Space/AI/anaconda3/envs/tf2_gpu/lib/python3.7/site-packages/urllib3/connection.py&quot;, line 187, in connect
    conn = self._new_conn()
  File &quot;/media/adeel/Space/AI/anaconda3/envs/tf2_gpu/lib/python3.7/site-packages/urllib3/connection.py&quot;, line 172, in _new_conn
    self, &quot;Failed to establish a new connection: %s&quot; % e
urllib3.exceptions.NewConnectionError: &lt;urllib3.connection.HTTPConnection object at 0x7f0a26b54c50&gt;: Failed to establish a new connection: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;/media/adeel/Space/AI/anaconda3/envs/tf2_gpu/lib/python3.7/site-packages/requests/adapters.py&quot;, line 449, in send
    timeout=timeout
  File &quot;/media/adeel/Space/AI/anaconda3/envs/tf2_gpu/lib/python3.7/site-packages/urllib3/connectionpool.py&quot;, line 727, in urlopen
    method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]
  File &quot;/media/adeel/Space/AI/anaconda3/envs/tf2_gpu/lib/python3.7/site-packages/urllib3/util/retry.py&quot;, line 439, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='34.71.47.117', port=80): Max retries exceeded with url: /graphql (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object at 0x7f0a26b54c50&gt;: Failed to establish a new connection: [Errno 111] Connection refused'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;/media/adeel/Space/AI/anaconda3/envs/tf2_gpu/lib/python3.7/site-packages/wandb/old/retry.py&quot;, line 96, in __call__
    result = self._call_fn(*args, **kwargs)
  File &quot;/media/adeel/Space/AI/anaconda3/envs/tf2_gpu/lib/python3.7/site-packages/wandb/internal/internal_api.py&quot;, line 128, in execute
    return self.client.execute(*args, **kwargs)
  File &quot;/media/adeel/Space/AI/anaconda3/envs/tf2_gpu/lib/python3.7/site-packages/wandb/vendor/gql-0.2.0/gql/client.py&quot;, line 52, in execute
    result = self._get_result(document, *args, **kwargs)
  File &quot;/media/adeel/Space/AI/anaconda3/envs/tf2_gpu/lib/python3.7/site-packages/wandb/vendor/gql-0.2.0/gql/client.py&quot;, line 60, in _get_result
    return self.transport.execute(document, *args, **kwargs)
  File &quot;/media/adeel/Space/AI/anaconda3/envs/tf2_gpu/lib/python3.7/site-packages/wandb/vendor/gql-0.2.0/gql/transport/requests.py&quot;, line 38, in execute
    request = requests.post(self.url, **post_args)
  File &quot;/media/adeel/Space/AI/anaconda3/envs/tf2_gpu/lib/python3.7/site-packages/requests/api.py&quot;, line 119, in post
    return request('post', url, data=data, json=json, **kwargs)
  File &quot;/media/adeel/Space/AI/anaconda3/envs/tf2_gpu/lib/python3.7/site-packages/requests/api.py&quot;, line 61, in request
    return session.request(method=method, url=url, **kwargs)
  File &quot;/media/adeel/Space/AI/anaconda3/envs/tf2_gpu/lib/python3.7/site-packages/requests/sessions.py&quot;, line 530, in request
    resp = self.send(prep, **send_kwargs)
  File &quot;/media/adeel/Space/AI/anaconda3/envs/tf2_gpu/lib/python3.7/site-packages/requests/sessions.py&quot;, line 643, in send
    r = adapter.send(request, **kwargs)
  File &quot;/media/adeel/Space/AI/anaconda3/envs/tf2_gpu/lib/python3.7/site-packages/requests/adapters.py&quot;, line 516, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='34.71.47.117', port=80): Max retries exceeded with url: /graphql (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object at 0x7f0a26b54c50&gt;: Failed to establish a new connection: [Errno 111] Connection refused'))
wandb: Network error (ConnectionError), entering retry loop. See wandb/debug-internal.log for full traceback.
Traceback (most recent call last):
  File &quot;/media/adeel/Space/AI/anaconda3/envs/tf2_gpu/bin/wandb&quot;, line 8, in &lt;module&gt;
    sys.exit(cli())
  File &quot;/media/adeel/Space/AI/anaconda3/envs/tf2_gpu/lib/python3.7/site-packages/click/core.py&quot;, line 829, in __call__
    return self.main(*args, **kwargs)
  File &quot;/media/adeel/Space/AI/anaconda3/envs/tf2_gpu/lib/python3.7/site-packages/click/core.py&quot;, line 782, in main
    rv = self.invoke(ctx)
  File &quot;/media/adeel/Space/AI/anaconda3/envs/tf2_gpu/lib/python3.7/site-packages/click/core.py&quot;, line 1259, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File &quot;/media/adeel/Space/AI/anaconda3/envs/tf2_gpu/lib/python3.7/site-packages/click/core.py&quot;, line 1066, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File &quot;/media/adeel/Space/AI/anaconda3/envs/tf2_gpu/lib/python3.7/site-packages/click/core.py&quot;, line 610, in invoke
    return callback(*args, **kwargs)
  File &quot;/media/adeel/Space/AI/anaconda3/envs/tf2_gpu/lib/python3.7/site-packages/wandb/cli/cli.py&quot;, line 72, in wrapper
    return func(*args, **kwargs)
  File &quot;/media/adeel/Space/AI/anaconda3/envs/tf2_gpu/lib/python3.7/site-packages/wandb/cli/cli.py&quot;, line 212, in login
    wandb.login(relogin=relogin, key=key, anonymous=anon_mode, host=host, force=True)
  File &quot;/media/adeel/Space/AI/anaconda3/envs/tf2_gpu/lib/python3.7/site-packages/wandb/sdk/wandb_login.py&quot;, line 29, in login
    anonymous=anonymous, key=key, relogin=relogin, host=host, force=force
  File &quot;/media/adeel/Space/AI/anaconda3/envs/tf2_gpu/lib/python3.7/site-packages/wandb/sdk/wandb_login.py&quot;, line 128, in _login
    apikey.write_key(settings, key)
  File &quot;/media/adeel/Space/AI/anaconda3/envs/tf2_gpu/lib/python3.7/site-packages/wandb/lib/apikey.py&quot;, line 223, in write_key
    raise ValueError(&quot;API key must be 40 characters long, yours was %s&quot; % len(key))
ValueError: API key must be 40 characters long, yours was 26
</code></pre>",1,0,2020-09-27 22:41:51.380000 UTC,,2022-08-30 08:58:42.333000 UTC,4,python|wandb,6277,2015-10-28 00:01:57.173000 UTC,2022-09-24 01:20:28.387000 UTC,"Sydney, New South Wales, Australia",689,57,0,87,,,,,,['wandb']
AWS SageMaker RL with ray: ray.tune.error.TuneError: No trainable specified,"<p>I have a training script based on the AWS SageMaker RL example rl_network_compression_ray_custom but changed the env to make a basic gym env Asteroids-v0 (installing dependencies at main entrypoint to the training script). When I run the fit on the RLEstimator it gives the following error <code>ray.tune.error.TuneError: No trainable specified!</code> even if the run is specified in the training config as DQN.</p>

<p>Does anyone know about this issue and how to solve it?</p>

<p>Here is the longer log:</p>

<pre><code>Running experiment with config {
  ""training"": {
    ""env"": ""Asteroids-v0"",
    ""run"": ""DQN"",
    ""stop"": {
      ""training_iteration"": 1
    },
    ""local_dir"": ""/opt/ml/output/intermediate"",
    ""checkpoint_freq"": 10,
    ""config"": {
      ""double_q"": false,
      ""dueling"": false,
      ""num_atoms"": 1,
      ""noisy"": false,
      ""prioritized_replay"": false,
      ""n_step"": 1,
      ""target_network_update_freq"": 8000,
      ""lr"": 6.25e-05,
      ""adam_epsilon"": 0.00015,
      ""hiddens"": [
        512
      ],
      ""learning_starts"": 20000,
      ""buffer_size"": 1000000,
      ""sample_batch_size"": 4,
      ""train_batch_size"": 32,
      ""schedule_max_timesteps"": 2000000,
      ""exploration_final_eps"": 0.01,
      ""exploration_fraction"": 0.1,
      ""prioritized_replay_alpha"": 0.5,
      ""beta_annealing_fraction"": 1.0,
      ""final_prioritized_replay_beta"": 1.0,
      ""num_gpus"": 0.2,
      ""timesteps_per_iteration"": 10000
    },
    ""checkpoint_at_end"": true
  },
  ""trial_resources"": {
    ""cpu"": 1,
    ""extra_cpu"": 3
  }
}
Important! Ray with version &lt;=7.2 may report ""Did not find checkpoint file"" even if the experiment is actually restored successfully. If restoration is expected, please check ""training_iteration"" in the experiment info to confirm.
Traceback (most recent call last):
  File ""train-ray.py"", line 83, in &lt;module&gt;
    MyLauncher().train_main()
  File ""/opt/ml/code/sagemaker_rl/ray_launcher.py"", line 332, in train_main
    launcher.launch()
  File ""/opt/ml/code/sagemaker_rl/ray_launcher.py"", line 313, in launch
    run_experiments(experiment_config)
  File ""/usr/local/lib/python3.6/dist-packages/ray/tune/tune.py"", line 296, in run_experiments
    experiments = convert_to_experiment_list(experiments)
  File ""/usr/local/lib/python3.6/dist-packages/ray/tune/experiment.py"", line 199, in convert_to_experiment_list
    for name, spec in experiments.items()
  File ""/usr/local/lib/python3.6/dist-packages/ray/tune/experiment.py"", line 199, in &lt;listcomp&gt;
    for name, spec in experiments.items()
  File ""/usr/local/lib/python3.6/dist-packages/ray/tune/experiment.py"", line 122, in from_json
    raise TuneError(""No trainable specified!"")
ray.tune.error.TuneError: No trainable specified!
2020-04-22 13:21:15,784 sagemaker-containers ERROR    ExecuteUserScriptError:
Command ""/usr/bin/python train-ray.py --rl.training.checkpoint_freq 1 --rl.training.stop.training_iteration 1 --s3_bucket XXXXX
</code></pre>",1,0,2020-04-22 13:34:54.857000 UTC,,2020-04-22 15:20:00.600000 UTC,0,amazon-web-services|reinforcement-learning|amazon-sagemaker|ray|rllib,384,2015-10-16 12:15:20.637000 UTC,2022-09-14 15:22:35.597000 UTC,,396,3,1,14,,,,,,['amazon-sagemaker']
Best Practices for Azure Machine Learning Pipelines,"<p>I started working with Azure Machine Learning Service. It has a feature called Pipeline, which I'm currently trying to use. There are, however, are bunch of things that are completely unclear from the documentation and the examples and I'm struggling to fully grasp the concept.</p>

<ol>
<li>When I look at 'batch scoring' examples, it is implemented as a Pipeline Step. This raises the question: does this mean that the 'predicting part' is part of the same pipeline as the 'training part', or should there be separate 2 separate pipelines for this? Making 1 pipeline that combines both steps seems odd to me, because you don't want to run your predicting part every time you change something to the training part (and vice versa).</li>
<li>What parts should be implemented as a Pipeline Step and what parts shouldn't? Should the creation of the Datastore and Dataset be implemented as a step? Should registering a model be implemented as a step?</li>
<li>What isn't shown anywhere is how to deal with model registry. I create the model in the training step and then write it to the output folder as a pickle file. Then what? How do I get the model in the next step? Should I pass it on as a PipelineData object? Should train.py itself be responsible for registering the trained model?</li>
</ol>",2,0,2020-04-23 16:16:00.660000 UTC,2.0,2020-04-25 13:07:11.857000 UTC,3,machine-learning|pipeline|azure-machine-learning-service,1359,2011-07-20 21:05:49.443000 UTC,2022-09-16 11:23:45.910000 UTC,"Rotterdam, Netherlands",12086,403,186,1125,,,,,,['azure-machine-learning-service']
AzureML web service error,"<p>I try to implement random forest classifier in AzureML using Python script. It ran well in experiment but I am facing error when test the web services. </p>

<pre><code># The script MUST contain a function named azureml_main
# which is the entry point for this module.

# imports up here can be used to 
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix

# The entry point function can contain up to two input arguments:
#   Param&lt;dataframe1&gt;: a pandas.DataFrame
#   Param&lt;dataframe2&gt;: a pandas.DataFrame
def azureml_main(dataframe1 = None, dataframe2 = None):

    label_x = dataframe1[['NSP']]
    del dataframe1['NSP']

    label_y = dataframe2[['NSP']]
    del dataframe2['NSP']
    trained_model = random_forest_classifier(dataframe1, label_x)
    # print (""Trained model :: "", trained_model)
    predictions = trained_model.predict(dataframe2)


    # print (""Train Accuracy :: "", accuracy_score(label_x, trained_model.predict(dataframe1)))
    print (""Test Accuracy  :: "", accuracy_score(label_y, predictions))
    # print ("" Confusion matrix "", confusion_matrix(dataframe2, predictions))
    cm = confusion_matrix(label_y, predictions)
    TP = cm[0][0]
    FP = cm[0][1]
    FN = cm[1][0]
    TN = cm[1][1]
    print(""True Positive: "", TP, ""False Positive: "", FP, ""True Negative: "", TN, ""False Negative: "",FN)
    # If a zip file is connected to the third input port is connected,
    # it is unzipped under "".\Script Bundle"". This directory is added
    # to sys.path. Therefore, if your zip file contains a Python file
    # mymodule.py you can import it using:
    # import mymodule

    # Return value must be of a sequence of pandas.DataFrame
    return pd.DataFrame(predictions)

def random_forest_classifier(features, target):
    """"""
    To train the random forest classifier with features and target data
    :param features:
    :param target:
    :return: trained random forest classifier
    """"""
    clf = RandomForestClassifier()
    clf.fit(features, target)
    return clf
</code></pre>

<p><a href=""https://i.stack.imgur.com/dI0Xg.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dI0Xg.jpg"" alt=""AzureML""></a></p>

<p>When I test the web services by entering the values, it show this error.</p>

<blockquote>
  <p>85: Error 0085: The following error occurred during script evaluation, please view the output log for more information: ---------- Start of error message from Python interpreter ---------- Caught exception while executing function: Traceback (most recent call last): File ""\server\InvokePy.py"", line 120, in executeScript outframe = mod.azureml_main(*inframes) File ""\temp-6305281036382248739.py"", line 52, in azureml_main File ""C:\pyhome\lib\site-packages\sklearn\ensemble\forest.py"", line 498, in predict proba = self.predict_proba(X) File ""C:\pyhome\lib\site-packages\sklearn\ensemble\forest.py"", line 537, in predict_proba X = self._validate_X_predict(X) File ""C:\pyhome\lib\site-packages\sklearn\ensemble\forest.py"", line 319, in _validate_X_predict return self.estimators_[0]._validate_X_predict(X, check_input=True) File ""C:\pyhome\lib\site-packages\sklearn\tree\tree.py"", line 365, in _validate_X_predict X = check_array(X, dtype=DTYPE, accept_sparse=""csr"") File ""C:\pyhome\lib\site-packages\sklearn\utils\validation.py"", line 407, in check_array context)) ValueError: Found array with 0 sample(s) (shape=(0, 21)) while a minimum of 1 is required. ---------- End of error message from Python interpreter ----------, Error code: ModuleExecutionError, Http status code: 400, Timestamp: Thu, 07 Dec 2017 15:24:18 GMT</p>
</blockquote>

<p>Can anyone tell me where is the error?</p>",0,3,2017-12-07 15:25:45.473000 UTC,,2017-12-07 15:41:22.647000 UTC,0,python|azure|azure-machine-learning-studio,131,2015-06-28 00:17:57.227000 UTC,2022-09-24 01:54:57.743000 UTC,,1232,798,2,187,,,,,,['azure-machine-learning-studio']
How do I create a DatatSet with Data Type: ModelDirectory in Azure Machine Learning Studio?,"<p>I'm attempting to manually create a DataSet with Data Type: ModelDirectory in Azure Machine Learning Studio, in order to use it in an Inference Pipeline.  I have taken an existing ModelDirectory DataSet and attempted to replicate it.  Everything is identical, <em>except</em> that the replica has Data Type:  AnyDirectory, and can not be hooked up to the input of a ScoreModel node in the designer.  How can I (manually in the UI or, better yet, programmatically) create a DataSet with Data Type: ModelDirectory from the output files of a trained model?</p>
<p>Existing DataSet:
<a href=""https://i.stack.imgur.com/56g9x.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/56g9x.png"" alt=""enter image description here"" /></a>
Existing DataSet outputs:
<a href=""https://i.stack.imgur.com/0aBSc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0aBSc.png"" alt=""enter image description here"" /></a></p>
<p>Manually Created Replica DataSet:
<a href=""https://i.stack.imgur.com/DiQdp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DiQdp.png"" alt=""enter image description here"" /></a>
Manually Created Replica DataSet outputs:
<a href=""https://i.stack.imgur.com/DIYFY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DIYFY.png"" alt=""enter image description here"" /></a></p>
<p>As you can see, the outputs of both DataSets are <em>identical</em>.  The only difference between the two DataSets, seems to be the 'Data Type' properties, although in the output view, you can see that both have 'type: ModelDirectory'.</p>",0,0,2021-12-01 15:59:27.860000 UTC,,,1,azure-machine-learning-studio|azure-machine-learning-service|azureml-python-sdk,80,2012-06-27 21:51:16.130000 UTC,2022-09-21 21:19:20.110000 UTC,,751,68,5,73,,,,,,"['azure-machine-learning-studio', 'azure-machine-learning-service']"
I am trying to work on R using Azure ML Studio Notebook. But while installing and importing packages i am facing some challenges,"<p>I'm trying to install the some packages in Azure ML studio R notebook.</p>
<p>Upon typing <code>library(dplyr)</code> I get the error that</p>
<blockquote>
<p>package or namespace load failed for ‘dbplyr’ in loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI[[i]]):
namespace ‘rlang’ 0.4.11 is already loaded, but &gt;= 1.0.0 is required</p>
</blockquote>
<p>Can anyone suggest on how to install and load packages here just like RStudio<br />
<a href=""https://i.stack.imgur.com/nyIV7.png"" rel=""nofollow noreferrer"">Screenshot of error</a></p>",0,5,2022-08-04 08:25:34.797000 UTC,,2022-08-04 15:26:06.307000 UTC,0,r|azure|azure-machine-learning-service,54,2017-06-02 12:27:30.497000 UTC,2022-09-20 06:48:21.120000 UTC,"Bangalore, Karnataka, India",3,0,0,6,,,,,,['azure-machine-learning-service']
Serving MLFlow artifacts through `--serve-artifacts` without passing credentials,"<p>A new version of MLFlow (1.23) provided a <code>--serve-artifacts</code> option (via <a href=""https://github.com/mlflow/mlflow/pull/5045"" rel=""nofollow noreferrer"">this</a> pull request) along with some example code. This <em>should</em> allow me to simplify the rollout of a server for data scientists by only needing to give them one URL for the tracking server, rather than a URI for the tracking server, URI for the artifacts server, and a username/password for the artifacts server. At least, that's how I understand it.</p>
<p>A complication that I have is that I need to use <code>podman</code> instead of <code>docker</code> for my containers (and without relying on <code>podman-compose</code>). I ask that you keep those requirements in mind; I'm aware that this is an odd situation.</p>
<p>What I did before this update (for MLFlow 1.22) was to create a kubernetes play yaml config, and I was successfully able to issue a <code>podman play kube ...</code> command to start a pod and from a different machine successfully run an experiment and save artifacts after setting the appropriate four env variables. I've been struggling with getting things working with the newest version.</p>
<p>I am following the <code>docker-compose</code> example provided <a href=""https://github.com/mlflow/mlflow/tree/master/examples/mlflow_artifacts"" rel=""nofollow noreferrer"">here</a>. I am trying a (hopefully) simpler approach. The following is my kubernetes play file defining a pod.</p>
<pre class=""lang-yaml prettyprint-override""><code>apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: &quot;2022-01-14T19:07:15Z&quot;
  labels:
    app: mlflowpod
  name: mlflowpod
spec:
  containers:
  - name: minio
    image: quay.io/minio/minio:latest
    ports:
    - containerPort: 9001
      hostPort: 9001
    - containerPort: 9000
      hostPort: 9000
    resources: {}
    tty: true
    volumeMounts:
    - mountPath: /data
      name: minio-data
    args:
    - server
    - /data
    - --console-address
    - :9001

  - name: mlflow-tracking
    image: localhost/mlflow:latest
    ports:
    - containerPort: 80
      hostPort: 8090
    resources: {}
    tty: true
    env:
      - name: MLFLOW_S3_ENDPOINT_URL
        value: http://127.0.0.1:9000
      - name: AWS_ACCESS_KEY_ID
        value: minioadmin
      - name: AWS_SECRET_ACCESS_KEY
        value: minioadmin
    command: [&quot;mlflow&quot;]
    args:
      - server
      - -p 
      - 80
      - --host 
      - 0.0.0.0
      - --backend-store-uri 
      - sqlite:///root/store.db
      - --serve-artifacts
      - --artifacts-destination 
      - s3://mlflow
      - --default-artifact-root 
      - mlflow-artifacts:/
#      - http://127.0.0.1:80/api/2.0/mlflow-artifacts/artifacts/experiments
      - --gunicorn-opts 
      - &quot;--log-level debug&quot;
    volumeMounts:
    - mountPath: /root
      name: mlflow-data  

  volumes:
  - hostPath:
      path: ./minio
      type: Directory
    name: minio-data
  - hostPath:
      path: ./mlflow
      type: Directory
    name: mlflow-data
status: {}
</code></pre>
<p>I start this with <code>podman play kube mlflowpod.yaml</code>. On the same machine (or a different one, it doesn't matter), I have cloned and installed <code>mlflow</code> into a virtual environment. From that virtual environment, I set an environmental variable <code>MLFLOW_TRACKING_URI</code> to <code>&lt;name-of-server&gt;:8090</code>. I then run the <code>example.py</code> file in the <a href=""https://github.com/mlflow/mlflow/tree/master/examples/mlflow_artifacts"" rel=""nofollow noreferrer""><code>mlflow_artifacts</code></a> example directory. I get the following response:</p>
<pre><code>....
botocore.exceptions.NoCredentialsError: Unable to locate credentials
</code></pre>
<p>Which seems like the client needs the server credentials to minIO, which I thought the proxy was supposed to take care of.</p>
<p>If I also provide the env variables</p>
<pre><code>$env:MLFLOW_S3_ENDPOINT_URL=&quot;http://&lt;name-of-server&gt;:9000/&quot; 
$env:AWS_ACCESS_KEY_ID=&quot;minioadmin&quot;
$env:AWS_SECRET_ACCESS_KEY=&quot;minioadmin&quot;
</code></pre>
<p>Then things work. But that kind of defeats the purpose of the proxy...</p>
<p>What is it about the proxy setup via kubernates play yaml and podman that is going wrong?</p>",1,0,2022-01-20 20:15:11.853000 UTC,1.0,2022-01-20 20:23:39.863000 UTC,1,kubernetes|mlflow|podman,773,2011-11-14 15:53:50.420000 UTC,2022-01-21 19:03:06.807000 UTC,,428,11,0,33,,,,,,['mlflow']
AlphaFold on VertexAI - Stuck in setting up notebook for 2 hours,"<p>I am trying to run AlphaFold on VertexAI as explained <a href=""https://cloud.google.com/blog/products/ai-machine-learning/running-alphafold-on-vertexai"" rel=""nofollow noreferrer"">here</a>. However, my instance creation is stuck in this state for roughly two hours now. There is no error message either. I am wondering if something has gone wrong or this is just the expected time it will take to setup a new instance?</p>
<p>I actually tried with two different notebooks. One is the <a href=""https://github.com/GoogleCloudPlatform/vertex-ai-samples/raw/main/community-content/alphafold_on_workbench/AlphaFold.ipynb"" rel=""nofollow noreferrer"">default one</a> linked in the above article and the other is <a href=""https://raw.githubusercontent.com/deepmind/alphafold/main/notebooks/AlphaFold.ipynb"" rel=""nofollow noreferrer"">https://raw.githubusercontent.com/deepmind/alphafold/main/notebooks/AlphaFold.ipynb</a></p>
<p>Both are in the same state for roughly the same time.</p>
<p><a href=""https://i.stack.imgur.com/bzK8L.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bzK8L.png"" alt=""enter image description here"" /></a></p>",1,0,2022-01-30 15:43:01.820000 UTC,,,0,google-cloud-platform|google-cloud-ml|google-cloud-vertex-ai,192,2012-11-30 09:46:45.153000 UTC,2022-09-23 18:26:35.997000 UTC,"Islamabad Capital Territory, Pakistan",388,411,0,35,,,,,,['google-cloud-vertex-ai']
Azure passing secrets without depending on Azure Keyvaults,"<p>I am building an Azure ML Pipeline for batch scoring. In one step I need to access a key stored in the workspace's Azure Keyvault.</p>
<p>However, I want to strictly separate the authoring environment (responsible for creating the datasets, building the environment, building and running the pipeline) and the production environment (responsible for transforming data, running the prediction etc.).
Therefore, code in the production environment should be somewhat Azure agnostic. I want to be able to submit my inference script to Google Cloud Compute Instances, if needed.</p>
<p>Thus my question is:
What is the best practise to pass secrets to remote runs without having the remote script retrieve it from the keyvault itself?
Is there a way to have redacted environment variables or command line arguments?</p>
<p>Thanks!</p>
<p>Example of what I would like to happen:</p>
<pre class=""lang-py prettyprint-override""><code># import all azure dependencies
secret = keyvault.get_secret(&quot;my_secret&quot;)

pipeline_step = PythonScriptStep(
   script_name=&quot;step_script.py&quot;,
   arguments=[&quot;--input_data&quot;, input_data, &quot;--output_data&quot;, output_data],
   compute_target=compute,
   params=[&quot;secret&quot;: secret] # This will create an env var on the remote?
)

pipeline = Pipeline(workspace, steps=[pipeline_step])

PipelineEndpoint.publish(...)
</code></pre>
<p>An within <code>step_script.py</code>:</p>
<pre class=""lang-py prettyprint-override""><code># No imports from azureml!
secret = os.getenv(&quot;AML_PARAMETER_secret&quot;)
do_something(secret)
</code></pre>",0,4,2022-05-24 07:49:12.953000 UTC,,2022-05-24 08:29:00.477000 UTC,0,azure|azure-pipelines|azure-keyvault|azure-machine-learning-service|azure-secrets,84,2021-09-03 19:01:41.727000 UTC,2022-09-20 14:54:25.607000 UTC,,101,3,0,5,,,,,,['azure-machine-learning-service']
how to create a serverless endpoint in sagemaker via cloudformation?,"<p>cloudformation template below creates a sagemaker model and an endpoint configuration. my goal is to test serverless inference in sagemaker , but when i added the ServerlessConfig attribute to the endpoint config resource based on aws documentation , i get an error that it is not a valid property. any ideas?</p>
<pre><code>SageMakerModel:
    Type: AWS::SageMaker::Model
    Properties: 
      Containers: 
        -
          Image: !Ref ImageURI
          ModelDataUrl: !Ref ModelData
          Environment: {&quot;SAGEMAKER_PROGRAM&quot;: &quot;inference.py&quot;, &quot;SAGEMAKER_SUBMIT_DIRECTORY&quot;: !Ref ModelData}
      ExecutionRoleArn: !Ref RoleArn


SageMakerEndpointConfig:
    Type: &quot;AWS::SageMaker::EndpointConfig&quot;
    Properties:
      ServerlessConfig: 
        -
          MaxConcurrency: 5
</code></pre>",0,1,2022-09-24 18:38:41.867000 UTC,,,0,amazon-cloudformation|amazon-sagemaker,9,2020-03-15 21:37:55.360000 UTC,2022-09-24 19:51:04.830000 UTC,,365,53,2,94,,,,,,['amazon-sagemaker']
How can I update my custom build model in sagemaker everytime I create new training job?,"<p>I build a custom model on sagemaker and deployed the endpoint by creating a model. Now I want to retrain it every week and also I have to update same endpoint.</p>
<p>I scheduled the training job with lambda function but It is creating new artifact location. This is not I want. I want to update same model.</p>
<p>Does anyone know How can we do that?</p>",0,3,2021-08-26 14:08:29.293000 UTC,,,1,amazon-web-services|amazon-sagemaker,93,2018-07-26 07:55:25.750000 UTC,2022-09-23 13:58:07.923000 UTC,"Pune, Maharashtra, India",69,10,0,12,,,,,,['amazon-sagemaker']
How to force PythonScriptStep to run in Azure ML,"<p>I'm relatively new to Azure ML and trying to run a model via PythonScriptStep</p>
<p>I can publish pipelines and run the model.  However, once it has run once I can't re-submit the step as it states &quot;This run reused the output from a previous run&quot;.</p>
<p>My code declares allow_reuse to be False, but this doesn't seem to make a difference and I can simply not resubmit the step even though the underlying data is changing.</p>
<pre><code>train_step = PythonScriptStep(
        name='model_train',
        script_name=&quot;model_train.py&quot;,
        compute_target=aml_compute,
        runconfig=pipeline_run_config,
        source_directory=train_source_dir,
        allow_reuse=False)
</code></pre>
<p>Many thanks for your help</p>",0,1,2022-03-01 00:53:36.997000 UTC,,,1,azure|azure-pipelines|pipeline|azure-machine-learning-service|azuremlsdk,119,2021-03-12 16:27:51.440000 UTC,2022-03-10 20:16:30.543000 UTC,"London, UK",23,0,0,1,,,,,,['azure-machine-learning-service']
Azure Machine Learning Write output to Azure SQL Database,"<p>I am using Azure Machine Learning to clustering data.</p>

<p>The input data is from an Azure SQL Database, and it works fine.
At the end of everything I want to write the output to a table in the same Azure SQL Database, but I get this error:</p>

<pre><code>Error: Error 1000: AFx Library library exception: 
Sql encountered an error: Login failed for user
</code></pre>

<p>Anyone any idea?
Thank you very much!</p>",2,0,2017-11-16 13:34:00.587000 UTC,,,0,azure-sql-database|azure-machine-learning-studio,1068,2016-03-21 11:00:39.430000 UTC,2022-09-23 09:07:45.753000 UTC,"Trieste, Province of Trieste, Italy",1657,440,4,164,,,,,,['azure-machine-learning-studio']
AWS Sagemaker Labeling - Access Granted but Not working,"<p>AWS newbie. I am following the <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/sms-getting-started.html"" rel=""nofollow noreferrer"">instructions</a> in the video and the steps but I am unable to create labels<a href=""https://console.aws.amazon.com/sagemaker/groundtruth"" rel=""nofollow noreferrer"">enter link description here</a>.</p>
<p>I've granted the users and roles access to the s3 bucket, and yet..</p>
<pre><code>MultipleValidationErrors: There were 2 validation errors: * MissingRequiredParameter: Missing required key 'Bucket' in params * MissingRequiredParameter: Missing required key 'Key' in params - The S3 bucket 'null' you entered in Input dataset location cannot be reached. Either the bucket does not exist, or you do not have permission to access it. If the bucket does not exist, update Input dataset location with a new S3 URI. If the bucket exists, give the IAM entity you are using to create this labeling job permission to read and write to this S3 bucket, and try your request again.
</code></pre>",1,0,2022-02-10 00:33:03.260000 UTC,,,0,amazon-web-services|amazon-sagemaker,187,2018-12-27 16:16:52.730000 UTC,2022-06-10 18:34:53.500000 UTC,,79,12,0,9,,,,,,['amazon-sagemaker']
"""Entry Point Not Found"" Error LightGBM R package in Azure","<p>When using a LightGBM R package in Azure ML I get the following error:</p>

<pre><code>[Error]         +++ NT HARD ERROR (0xc0000139) +++
[Error]             Parameter 0: 0x4ad4bc8 [log2f]
[Error]             Parameter 1: 0x4b5e2e8 [C:\src\lightgbm\libs\x64\lib_lightgbm.dll]
[Error]             Parameter 2: 0xffffffffc0000139
[Error]         [FATAL] Exception: 0xc0000139 (!! HARD ERROR !!) {Params: 0x4ad4bc8, 0x4b5e2e8, 0xffffffffc0000139, 0x0}
[Error]         [ERROR] A fatal error occurred in the running application. The application will be terminated. Code: 0xc0000139.
</code></pre>

<p>on <a href=""https://msdn.microsoft.com/en-us/library/cc704588.aspx"" rel=""nofollow noreferrer"">page</a> <code>0xc0000139</code> is described as ""Entry Point Not Found"". What does this error mean and how can I solve it?</p>

<p>I used XGBoost in the same way in Azure ML and it worked. And it did not ask for external libraries (dlls). LightGBM on the contrary asks lots of libraries (dlls) and I think the problem is connected with the dlls, but this error does not indicate what is actually missing.</p>

<p><strong>What I did:</strong> <br>Installed LightGBM R package  on a Virtual Machine with Windows Server 2016. For this I used:</p>

<ul>
<li>CMake</li>
<li>C++ Development kit (installed almost all packages)</li>
<li>RTools</li>
</ul>

<p>Included in lightgbm\libs\x64 are the following packages because I previously got error <code>0xc0000135</code> with the names of these libraries:</p>

<ul>
<li>msvcp140.dll</li>
<li>vcomp140.dll</li>
<li>vcruntime140.dll</li>
<li>all api-ms-win-core-*.dll and all api-ms-win-crt-*.dll</li>
</ul>

<p>I tried to change the R version from Microsoft R Open 3.2.2 to CRAN R 3.1.0. It executes witout errors but does not execute code after library import.</p>

<p>The full output of Azure ML R script:</p>

<pre><code>Record Starts at UTC 08/03/2017 12:28:27:

Run the job:""/dll ""LanguageWorker, Version=6.0.0.0, Culture=neutral, PublicKeyToken=69c3241e6f0468ca;Microsoft.MetaAnalytics.LanguageWorker.LanguageWorkerClientRS;RunRSNR"" /Output0 ""..\..\Result Dataset\Result Dataset.dataset"" /Output1 ""..\..\R Device\R Device.dataset""  /dataset1 ""..\..\Dataset1\Dataset1.csv""    /bundlePath ""..\..\Script Bundle\Script Bundle.zip""  /rStreamReader ""script.R""  /rLibVersion ""Microsoft R Open 3.2.2""  /ContextFile ""..\..\_context\ContextFile.txt""""
[Start] Program::Main
[Start]     DataLabModuleDescriptionParser::ParseModuleDescriptionString
[Stop]     DataLabModuleDescriptionParser::ParseModuleDescriptionString. Duration = 00:00:00.0045866
[Start]     DllModuleMethod::DllModuleMethod
[Stop]     DllModuleMethod::DllModuleMethod. Duration = 00:00:00.0000221
[Start]     DllModuleMethod::Execute
[Start]         DataLabModuleBinder::BindModuleMethod
[Verbose]             moduleMethodDescription LanguageWorker, Version=6.0.0.0, Culture=neutral, PublicKeyToken=69c3241e6f0468ca;Microsoft.MetaAnalytics.LanguageWorker.LanguageWorkerClientRS;RunRSNR
[Verbose]             assemblyFullName LanguageWorker, Version=6.0.0.0, Culture=neutral, PublicKeyToken=69c3241e6f0468ca
[Start]             DataLabModuleBinder::LoadModuleAssembly
[Verbose]                 Loaded moduleAssembly LanguageWorker, Version=6.0.0.0, Culture=neutral, PublicKeyToken=69c3241e6f0468ca
[Stop]             DataLabModuleBinder::LoadModuleAssembly. Duration = 00:00:00.0093763
[Verbose]             moduleTypeName Microsoft.MetaAnalytics.LanguageWorker.LanguageWorkerClientRS
[Verbose]             moduleMethodName RunRSNR
[Information]             Module FriendlyName : Execute R Script
[Information]             Module Release Status : Release
[Stop]         DataLabModuleBinder::BindModuleMethod. Duration = 00:00:00.0125213
[Start]         ParameterArgumentBinder::InitializeParameterValues
[Verbose]             parameterInfos count = 6
[Verbose]             parameterInfos[0] name = dataset1 , type = Microsoft.Numerics.Data.Local.DataTable
[Start]             DataTableCsvHandler::HandleArgumentString
[Stop]             DataTableCsvHandler::HandleArgumentString. Duration = 00:00:00.2364734
[Verbose]             parameterInfos[1] name = dataset2 , type = Microsoft.Numerics.Data.Local.DataTable
[Verbose]             Set optional parameter dataset2 value to NULL
[Verbose]             parameterInfos[2] name = bundlePath , type = System.String
[Verbose]             parameterInfos[3] name = rStreamReader , type = System.IO.StreamReader
[Verbose]             parameterInfos[4] name = seed , type = System.Nullable`1[System.Int32]
[Verbose]             Set optional parameter seed value to NULL
[Verbose]             parameterInfos[5] name = rLibVersion , type = Microsoft.MetaAnalytics.LanguageWorker.LanguageWorkerClientRS+ExecuteRScriptRVersion
[Verbose]             Converted string 'Microsoft R Open 3.2.2' to enum of type Microsoft.MetaAnalytics.LanguageWorker.LanguageWorkerClientRS+ExecuteRScriptRVersion
[Stop]         ParameterArgumentBinder::InitializeParameterValues. Duration = 00:00:00.3175225
[Verbose]         Begin invoking method RunRSNR ... 
[ModuleOutput] Executing Against R 3.2.2.0
[ModuleOutput] Executing Against R 3.2.2.0
[Information]         Microsoft Drawbridge Console Host [Version 1.0.2108.0]
[Error]         +++ NT HARD ERROR (0xc0000139) +++
[Error]             Parameter 0: 0x4ad4bc8 [log2f]
[Error]             Parameter 1: 0x4b5e2e8 [C:\src\lightgbm\libs\x64\lib_lightgbm.dll]
[Error]             Parameter 2: 0xffffffffc0000139
[Error]         [FATAL] Exception: 0xc0000139 (!! HARD ERROR !!) {Params: 0x4ad4bc8, 0x4b5e2e8, 0xffffffffc0000139, 0x0}
[Error]         [ERROR] A fatal error occurred in the running application. The application will be terminated. Code: 0xc0000139.
[Information]         [1] 56000
[Information]         The following files have been unzipped for sourcing in path=[""src""]:
[Information]                            Name  Length                Date
[Information]         1 data.table_1.10.4.zip 1487417 2017-07-07 16:48:00
[Information]         2            lgb1.model   45142 2017-08-02 17:38:00
[Information]         3            lgb2.model   83455 2017-08-02 17:38:00
[Information]         4    lightgbm_2.0.4.zip 1350111 2017-08-03 14:26:00
[Information]         5      magrittr_1.5.zip  152732 2017-07-07 15:34:00
[Information]         6                R6.zip  317766 2017-08-03 10:33:00
[Information]         Loading objects:
[Information]           port1
[Information]         [1] ""Loading variable port1...""
[Information]         package 'magrittr' successfully unpacked and MD5 sums checked
[Information]         package 'R6' successfully unpacked and MD5 sums checked
[Information]         package 'data.table' successfully unpacked and MD5 sums checked
[Information]         package 'lightgbm' successfully unpacked and MD5 sums checked
[Information]         data.table 1.10.4
[Information]           The fastest way to learn (by data.table authors): https://www.datacamp.com/courses/data-analysis-the-data-table-way
[Information]           Documentation: ?data.table, example(data.table) and browseVignettes(""data.table"")
[Information]           Release notes, videos and slides: http://r-datatable.com
[Error]         Process returned with non-zero exit code -1073741511
[Stop]     DllModuleMethod::Execute. Duration = 00:00:14.8676292
[Critical]     Error: Error 1000: RPackage library exception: Attempting to obtain R output before invoking execution process
[Critical]     {""InputParameters"":{""DataTable"":[{""Rows"":50,""Columns"":131,""estimatedSize"":16928768,...........""Errors"":""Microsoft.Analytics.Exceptions.ErrorMapping+ModuleException: Error 1000: RPackage library exception: Attempting to obtain R output before invoking execution process ---&gt; Microsoft.Analytics.Modules.R.ErrorHandling.RInvalidOperationException: Attempting to obtain R output before invoking execution process\r\n   at Microsoft.MetaAnalytics.LanguageWorker.NewRWorker.GetProcessOutputs(Boolean scrub) in d:\\_Bld\\8831\\7669\\Sources\\Product\\Source\\Modules\\LanguageWorker\\LanguageWorker.Dll\\TempWorkers\\NewRWorker.cs:line 459\r\n   at Microsoft.MetaAnalytics.LanguageWorker.LanguageWorkerClientRS.ExecuteR(NewRWorker worker, DataTable dataset1, DataTable dataset2, IEnumerable`1 bundlePath, StreamReader rStreamReader, Nullable`1 seed) in d:\\_Bld\\8831\\7669\\Sources\\Product\\Source\\Modules\\LanguageWorker\\LanguageWorker.Dll\\EntryPoints\\RModule.cs:line 278\r\n   at Microsoft.MetaAnalytics.LanguageWorker.LanguageWorkerClientRS._RunImpl(NewRWorker worker, DataTable dataset1, DataTable dataset2, String bundlePath, StreamReader rStreamReader, Nullable`1 seed, ExecuteRScriptExternalResource source, String url, ExecuteRScriptGitHubRepositoryType githubRepoType, SecureString accountToken) in d:\\_Bld\\8831\\7669\\Sources\\Product\\Source\\Modules\\LanguageWorker\\LanguageWorker.Dll\\EntryPoints\\RModule.cs:line 207\r\n   at Microsoft.MetaAnalytics.LanguageWorker.LanguageWorkerClientRS.RunRSNR(DataTable dataset1, DataTable dataset2, String bundlePath, StreamReader rStreamReader, Nullable`1 seed, ExecuteRScriptRVersion rLibVersion) in d:\\_Bld\\8831\\7669\\Sources\\Product\\Source\\Modules\\LanguageWorker\\LanguageWorker.Dll\\EntryPoints\\REntryPoint.cs:line 105\r\n   --- End of inner exception stack trace ---"",""Warnings"":[],""Duration"":""00:00:14.8605990""}
Module finished after a runtime of 00:00:15.3186157 with exit code -2
Module failed due to negative exit code of -2
</code></pre>",1,0,2017-08-03 13:09:04.790000 UTC,,2017-08-03 14:12:27.343000 UTC,0,c++|r|azure|dll|azure-machine-learning-studio,245,2014-05-02 13:15:11.573000 UTC,2022-09-23 13:52:40.840000 UTC,"Moscow, Russia",2853,1981,6,228,,,,,,['azure-machine-learning-studio']
Azure ML Studio- Container has crashed. Did your init method fail,"<p>I am trying to deploy an ML model through the Azure ML Studio using the notebook itself. The commands we are using can be found here <a href=""https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-and-where?tabs=python#define-an-inference-configuration"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-and-where?tabs=python#define-an-inference-configuration</a></p>
<p>We have registered the model as below-</p>
<pre><code>from azureml.core.model import Model
model = Model.register(ws, model_name=&quot;pdmrfull&quot;, model_path=&quot;pdmrfull.model&quot;)
</code></pre>
<p>But while running this command-</p>
<pre><code>service = Model.deploy(
    ws,
    &quot;myservice&quot;,
    [&quot;pdmrfull.model&quot;],
    dummy_inference_config,
    deployment_config,
    overwrite=True,
)
service.wait_for_deployment(show_output=True)
</code></pre>
<p>We are getting the error that <code>container has crashed. Did your init method fail?</code></p>
<pre><code>  File &quot;/var/azureml-app/pdmscore.py&quot;, line 3, in &lt;module&gt;
    from pyspark.ml import Pipeline
ModuleNotFoundError: No module named 'pyspark'
</code></pre>
<p>The <code>init</code> method is-</p>
<pre><code>def init():
    pipeline = PipelineModel.load('pdmrfull.model')
</code></pre>",2,0,2021-12-29 08:56:35.997000 UTC,,2022-08-18 06:13:47.493000 UTC,1,python|azure|machine-learning|azure-machine-learning-studio,499,2018-07-18 04:30:47.987000 UTC,2022-09-21 03:37:23.473000 UTC,India,981,34,11,268,,,,,,['azure-machine-learning-studio']
Deploying from AzureML into AKS - Set Taints & Tolerations,"<p>We are attempting to deploy a model from AzureML into an AKS Kluster which has been configured to use taints and tolerations.</p>
<p>When we try to deploy, we receive the below error message...</p>
<p><strong>&quot;details&quot;: [ { &quot;code&quot;: &quot;Unschedulable&quot;, &quot;message&quot;: &quot;0/15 nodes are available: 12 node(s) had taint {Workload: MachineLearning}, that the pod didn't tolerate, 3 node(s) didn't match Pod's node affinity/selector.&quot; }, { &quot;code&quot;: &quot;DeploymentFailed&quot;, &quot;message&quot;: &quot;Couldn't schedule because the kubernetes cluster didn't have available resources after trying for 00:05:00. You can address this error by either adding more nodes, changing the SKU of your nodes or changing the resource requirements of your service. Please refer to https://aka.ms/debugimage#container-cannot-be-scheduled for more information.&quot; }, { &quot;code&quot;: &quot;DeploymentFailed&quot;, &quot;message&quot;: &quot;Your container endpoint is not available. Please follow the steps to debug: 1. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs. Please refer to https://aka.ms/debugimage#dockerlog for more information. 2. You can also interactively debug your scoring file locally. Please refer to https://docs.microsoft.com/azure/machine-learning/how-to-debug-visual-studio-code#debug-and-troubleshoot-deployments for more information. 3. For AKS deployment with custom certificate, you need to update your DNS record to point to the IP address of scoring endpoint. Please refer to https://docs.microsoft.com/azure/machine-learning/how-to-secure-web-service#update-your-dns for more information. 4. View the diagnostic events to check status of container, it may help you to debug the issue. {&quot;InvolvedObject&quot;:&quot;am-prod-app-c88d8d49c-vbxsv&quot;,&quot;InvolvedKind&quot;:&quot;Pod&quot;,&quot;Type&quot;:&quot;Warning&quot;,&quot;Reason&quot;:&quot;FailedScheduling&quot;,&quot;Message&quot;:&quot;0/15 nodes are available: 15 pod has unbound immediate PersistentVolumeClaims.&quot;,&quot;LastTimestamp&quot;:null} {&quot;InvolvedObject&quot;:&quot;am-prod-app-c88d8d49c-vbxsv&quot;,&quot;InvolvedKind&quot;:&quot;Pod&quot;,&quot;Type&quot;:&quot;Warning&quot;,&quot;Reason&quot;:&quot;FailedScheduling&quot;,&quot;Message&quot;:&quot;0/15 nodes are available: 15 pod has unbound immediate PersistentVolumeClaims.&quot;,&quot;LastTimestamp&quot;:null} {&quot;InvolvedObject&quot;:&quot;am-prod-app-c88d8d49c-vbxsv&quot;,&quot;InvolvedKind&quot;:&quot;Pod&quot;,&quot;Type&quot;:&quot;Warning&quot;,&quot;Reason&quot;:&quot;FailedScheduling&quot;,&quot;Message&quot;:&quot;0/15 nodes are available: 12 node(s) had taint {Workload: MachineLearning}, that the pod didn't tolerate, 3 node(s) didn't match Pod's node affinity/selector.&quot;,&quot;LastTimestamp&quot;:null} {&quot;InvolvedObject&quot;:&quot;am-prod-app-c88d8d49c-vbxsv&quot;,&quot;InvolvedKind&quot;:&quot;Pod&quot;,&quot;Type&quot;:&quot;Normal&quot;,&quot;Reason&quot;:&quot;NotTriggerScaleUp&quot;,&quot;Message&quot;:&quot;pod didn't trigger scale-up: 5 pod has unbound immediate PersistentVolumeClaims&quot;,&quot;LastTimestamp&quot;:&quot;2022-04-05T14:33:02Z&quot;} &quot; } ] }</strong></p>
<p>Is there any way to specify the taints and tolerations from the deployment?</p>
<p>Thanks in advance!</p>",1,0,2022-04-06 13:28:20.400000 UTC,,,0,python|kubernetes|azure-machine-learning-service,87,2022-04-06 13:22:52.747000 UTC,2022-07-13 10:36:26.563000 UTC,,1,0,0,1,,,,,,['azure-machine-learning-service']
Wrong input data to a TensforFlow model,"<p>I have built a TensorFlow model on Sagemaker and it works fine with real time inference, however I want to use Batch transform functionality and I started to look to input data. I started to debug my model locally with saved_model_cli:</p>

<pre><code>saved_model_cli show \
--dir . \
--tag_set serve \
--signature_def serving_default

The given SavedModel SignatureDef contains the following input(s):
inputs['inputs'] tensor_info:
  dtype: DT_FLOAT
  shape: (-1, 50, 11)
  name: lstm_input:0
The given SavedModel SignatureDef contains the following output(s):
outputs['dense/BiasAdd:0'] tensor_info:
  dtype: DT_FLOAT
  shape: (-1, 1)
  name: dense/BiasAdd:0
Method name is: tensorflow/serving/predict
</code></pre>

<p>I assume, that my input data is called <code>inputs</code> as per output above, however, when I run the following code, I get an error </p>

<pre><code>saved_model_cli run \
--dir . \
--tag_set serve \
--signature_def predict \
--input_examples 'inputs=[{"""":[1.2]}]'
</code></pre>

<p><strong>ValueError: ""inputs"" is not a valid input key. Please choose from """", or use --show option.</strong></p>

<p>I tried to supply a npy file (<code>--inputs inputs=batch_transform.npy</code>), different representations of data, but always the same error. </p>

<p>My model is saved with the following code:</p>

<pre><code>tf.saved_model.simple_save(  
   tf.keras.backend.get_session(),  
   os.path.join(model_dir, 'model/1'),  
   inputs={'inputs': model.input},  
   outputs={t.name: t for t in model.outputs})
</code></pre>

<p>I tried TF 1.12 and 1.14 versions, but the outcome is the same.</p>

<p>Any advice?</p>",1,1,2019-11-01 06:42:57.833000 UTC,1.0,,2,tensorflow|keras|amazon-sagemaker,376,2011-03-01 15:11:31.030000 UTC,2022-09-24 08:39:28.467000 UTC,"Luxembourg City, Luxembourg",175,150,1,38,,,,,,['amazon-sagemaker']
azure ML studio terrible performance,"<p>the last day or two Azure ML studio performance is terrible.</p>

<p>It can take up to 10 mins to save a simple experiment and another 10 mins to run it.</p>

<p>These are simple tutorial experiments, nothing massive, using at maximum 18mb of data.</p>

<p>When i finally get the experiment to run and try to view the evaluation results, ML Studio spins for 5 mins before giving the error ""Error producing the visualization of the output ""</p>

<p>Note this error also occasionally occurs when i am just trying to view the list of saved experiments.</p>

<p>Im in the process of completing the Microsoft data science professional course and this is completely blocking me from making any progress.</p>

<p>Any info on what might be wrong would be appreciated. </p>",0,2,2017-07-04 16:10:24.330000 UTC,,,1,azure|machine-learning|azure-machine-learning-studio,478,2015-11-16 13:58:07.793000 UTC,2018-11-15 18:44:42.427000 UTC,"Nairobi, Kenya",194,0,0,44,,,,,,['azure-machine-learning-studio']
Is it possible to change the legend of the plot chart in mlflow metrics?,"<p>Thanks for the development of mlflow. I love it very much.</p>
<p>I want to compare several runs with different hyper parameters, but I found that it is very difficult to differenciate these runs from the legend (some random numbers as the run ID) as shown in the screenshot.</p>
<p>I hope the legend could be set to the hyper parameters in which these runs have different values. For instance, the legend could be set to different <code>patch size</code>, or different <code>learning rate</code>, etc.</p>
<p>So is it possible for the current mlflow? If not, do you have the plan to develop this feature?</p>
<p>This question is similar with this <a href=""https://github.com/mlflow/mlflow/issues/5523"" rel=""nofollow noreferrer"">issue</a>. But the issue proposed to use the customed <code>name</code> as the legend, while I think it is better to set it as the different hyperparemeters. Or it is best to let users to choose how to set the legend.</p>
<p><a href=""https://i.stack.imgur.com/0Wagl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0Wagl.png"" alt=""enter image description here"" /></a></p>",0,0,2022-03-24 23:49:55.930000 UTC,,2022-03-25 00:11:55.347000 UTC,0,mlflow,68,2018-01-17 13:59:57.947000 UTC,2022-09-23 21:54:08.630000 UTC,"Leiden, 荷兰",908,568,12,151,,,,,,['mlflow']
How to use HMM in two-class classification?,"<p>I am using Azure Machine Learning to build a model which will predict if a project will be approved (1) or not (0).</p>

<p>My dataset is composed of a list of projects. Each line represents a project and its details - <strong>starting day, theme, author, place, people involved, stage, date of last stage and approved</strong>.</p>

<p><em>There are 15 possible crescent stages a project can pass through before being approved. On the other hand, in some special cases, a project can be approved mid-way, that is, before getting to the last stage which is the most commom</em></p>

<p>I will be receiving daily updates on some projects, as well as, new projects that are coming in. I am trying to build a model which will predict the probability of it being approved or not based on my inputs (which will inclue stage).</p>

<p>I want to use <strong>stage</strong> as an input, but if I use it with a two-class boosted decision tree it will indirectly give the answer to my model.</p>

<p>I've read a little bit about HMM and tried to learn how to apply to my ML model but did not understand how to. Could anyone guide me to the right path, please? Should I really use HMM?</p>",1,0,2017-04-27 01:33:20.267000 UTC,1.0,,-1,machine-learning|classification|hidden-markov-models|azure-machine-learning-studio,213,2015-10-26 12:25:19.373000 UTC,2020-06-04 14:49:16.817000 UTC,,155,1,0,37,,,,,,['azure-machine-learning-studio']
Error notification not working in background job Neptune Software,"<p>For a particular server script, we are adding a background job.
In that ,it has an option to add error notification emails ,which is not working.  There is error in my script, which I can see in job log but not getting any notifications on email.</p>",0,1,2021-12-12 09:16:01.957000 UTC,,2022-04-12 12:31:11.957000 UTC,0,background|amazon-neptune|aws-neptune|neptune,36,2020-03-06 11:19:32.323000 UTC,2022-08-18 03:17:39.480000 UTC,,1,0,0,9,,,,,,['neptune']
pip install fails when installing mlflow,"<p>I'm working on a Window 10 machine and trying to pip install mlflow but I'm getting the following error message.</p>

<pre><code>Traceback (most recent call last):
File ""C:\Users\username\AppData\Local\Continuum\anaconda3\lib\site-packages\pip\_vendor\urllib3\response.py"", line 360, in _error_catcher
yield
File ""C:\Users\username\AppData\Local\Continuum\anaconda3\lib\site-packages\pip\_vendor\urllib3\response.py"", line 442, in read
data = self._fp.read(amt)
File ""C:\Users\username\AppData\Local\Continuum\anaconda3\lib\http\client.py"", line 447, in read
n = self.readinto(b)
 File ""C:\Users\username\AppData\Local\Continuum\anaconda3\lib\http\client.py"", line 491, in readinto
n = self.fp.readinto(b)
File ""C:\Users\username\AppData\Local\Continuum\anaconda3\lib\socket.py"", line 589, in readinto
return self._sock.recv_into(b)
File ""C:\Users\username\AppData\Local\Continuum\anaconda3\lib\ssl.py"", line 1052, in recv_into
return self.read(nbytes, buffer)
File ""C:\Users\username\AppData\Local\Continuum\anaconda3\lib\ssl.py"", line 911, in read
return self._sslobj.read(len, buffer)
socket.timeout: The read operation timed out
....

During handling of the above exception, another exception occurred:
</code></pre>

<p>What's this issue?</p>",1,0,2019-06-17 19:29:32.213000 UTC,,,1,pip|mlflow,175,2012-12-12 20:12:11.933000 UTC,2022-04-05 02:26:00.750000 UTC,,5655,73,3,629,,,,,,['mlflow']
"Undo changes in pandas Dataframe(column drops, row drops, edits performed on a single cell)","<p>I am currently working on developing a 'undo' operation for my interface that deals with changes performed on csv files. I want to provide an option for the user to revert the changes that he had done to the csv file, these changes include edit a cell, deleting column, deleting row, adding row, adding column etc. For this I want to know, does version control works in this scenario? If yes, which data version control should I prefer? If not, please suggest me an another alternative.</p>",0,2,2022-06-16 06:15:34.273000 UTC,,2022-06-20 09:15:41.577000 UTC,1,python|pandas|git|version-control|dvc,39,2022-06-16 06:09:28.507000 UTC,2022-07-21 05:33:22.853000 UTC,,11,0,0,1,,,,,,['dvc']
Regular expressions error when trying to deploy Sagemaker OD model,"<p>I am following this tutorial to try and deploy my own previously trained model, which I have artifacts for in a tar.gc file, stored in an S3 bucket (<a href=""https://aws.amazon.com/blogs/machine-learning/object-detection-and-model-retraining-with-amazon-sagemaker-and-amazon-augmented-ai/"" rel=""nofollow noreferrer"">https://aws.amazon.com/blogs/machine-learning/object-detection-and-model-retraining-with-amazon-sagemaker-and-amazon-augmented-ai/</a>)</p>
<p>Everything seems to run fine until I get to this point in the code:</p>
<pre><code># deploying the model into one ml.m4.xlarge instance
object_detector = model.deploy(initial_instance_count = 1,
                               instance_type = 'ml.m4.xlarge',
                               endpoint_name = endpoint_name)
</code></pre>
<p>Which spits out this error for me:</p>
<p><code>ClientError: An error occurred (ValidationException) when calling the CreateModel operation: 2 validation errors detected: Value 'model.tar.gz-2021-08-26-08-43-29-384' at 'modelName' failed to satisfy constraint: Member must satisfy regular expression pattern: ^[a-zA-Z0-9](-*[a-zA-Z0-9])*; Value '685385470294.dkr.ecr.eu-west-1.amazonaws.com/object-detection:1' at 'primaryContainer.modelDataUrl' failed to satisfy constraint: Member must satisfy regular expression pattern: ^(https|s3)://([^/]+)/?(.*)$</code></p>
<p>I checked the attributes which it seems to refer to and they are:</p>
<pre><code>model.model_data
</code></pre>
<p><code>'685385470294.dkr.ecr.eu-west-1.amazonaws.com/object-detection:1'</code></p>
<p>and</p>
<pre><code>model.name
</code></pre>
<p><code>'model.tar.gz-2021-08-26-08-43-29-384'</code></p>
<p>But I'm not sure what the issue is supposed to be. Earlier in the demo it seems to want to copy the tar.gc file from the output location to another, and I don't understand the significance of that. Can't it just stay where it is in /output? I just copied it to one sub-directory below the folder it was deposited into after training, if that helps. e.g. S3/folder/output -&gt; S3/folder/output/model/</p>",1,0,2021-08-26 08:58:35.473000 UTC,,,0,amazon-web-services|deployment|amazon-sagemaker|endpoint,730,2017-10-09 18:20:54.673000 UTC,2022-09-23 16:33:16.797000 UTC,"Gatineau, QC, Canada",505,24,0,86,,,,,,['amazon-sagemaker']
DVC pull returns ERROR: configuration error - Failed to authenticate GDrive remote: name: drive version: v2,"<p>I ran this github actions workflow with several variations, but I cannot pull the data from DVC.</p>
<pre><code>name: auto-testing
on: [push]
jobs:
  run:
    runs-on: [ubuntu-latest]
    steps:
      - uses: actions/checkout@v2
      - uses: iterative/setup-dvc@v1
      - name: Get data
        run: |
          echo '---'
          echo GDRIVE_CREDENTIALS_DATA: $GDRIVE_CREDENTIALS_DATA
          echo '---'
          #pip list
          dvc remote default storage
          #dvc remote modify storage --local gdrive_use_service_account true
          #dvc remote modify storage --local gdrive_service_account_json_file_path .dvc/gdrive-access.json
          dvc pull
        env:
          REPO_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          #GDRIVE_CREDENTIALS_DATA : ${{ secrets.GDRIVE_CREDENTIALS_DATA }}   
      - name: Install requirements
        run: |
          pip install -r requirements.txt
      - name: Run tests
        run: python src/test.py  
</code></pre>
<p>GDRIVE_CREDENTIALS_DATA is populated with the contents of the json file that I donwloaded from google and it was tested on two other computers. I tried using the environment variable as well as adding the json file to the repo to see if it would work. But no. I am getting this error message:</p>
<blockquote>
<p>ERROR: configuration error - Failed to authenticate GDrive remote: name: drive  version: v2
ERROR: Failed to authenticate GDrive remote
Learn more about configuration settings at <a href=""https://man.dvc.org/remote/modify"" rel=""nofollow noreferrer"">https://man.dvc.org/remote/modify</a>.
Error: Process completed with exit code 251.</p>
</blockquote>",0,1,2022-07-18 15:41:49.770000 UTC,1.0,,1,dvc,129,2015-10-16 01:35:39.707000 UTC,2022-09-22 23:31:19.043000 UTC,,6513,1296,60,1057,,,,,,['dvc']
Azure Machine Learning recommendation,"<p>I'm trying to create a recommendation service using Azure ML, more specifically using the Train Matchbox Recommender and Score Matchbox Recommender. Basically, what I want to do is to <strong>recommend the same items</strong> a user has bought in the past, instead of recommending items bought by other people.</p>

<p><a href=""https://i.stack.imgur.com/kC2xH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kC2xH.png"" alt=""enter image description here""></a></p>

<p>In the picture above, the experiment is configured to recommend items from rated items, then a Evaluate Recommender evaluates the result. In every step I'm saving the data into sqlserver, so that I can check the results myself and see whether it works or not.</p>

<p><a href=""https://i.stack.imgur.com/gTaTE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gTaTE.png"" alt=""enter image description here""></a></p>

<p>Above we have in the first dataset my purchase history (the userId, the itemId and how many times the user bought that item) and the second is the 5 recommendations. It pretty much nailed the recomendations!</p>

<p><strong>The problem:</strong> when creating the Predictive Experiment, I must change the Score Matchbox Recommender ""Recommended item selection"" from ""Rated Items (for model evaluation)"" to ""All items"" and deploy the web service. </p>

<p><a href=""https://i.stack.imgur.com/ngk2X.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ngk2X.png"" alt=""enter image description here""></a></p>

<p>After deploying the web service, I ask the recommendations for the user 'FELIPE' and it recommends items that the user has never bought.</p>

<p>Why during the training phase I got the results I expected, and after deploying the service I don't?</p>

<p><a href=""https://i.stack.imgur.com/ifTPu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ifTPu.png"" alt=""enter image description here""></a></p>",0,4,2018-10-03 07:02:56.380000 UTC,,2018-10-03 11:04:11.593000 UTC,0,azure|machine-learning|recommendation-engine|azure-machine-learning-studio,300,2014-03-28 12:28:13.540000 UTC,2022-09-24 06:35:42.150000 UTC,,64,118,0,45,,,,,,['azure-machine-learning-studio']
Read csv files in a MLFlow pipeline,"<p>I'm following <a href=""https://www.mlflow.org/docs/latest/pipelines.html"" rel=""nofollow noreferrer"">this documentation</a> to use ML Flow pipelines, which requires to clone <a href=""https://github.com/mlflow/mlp-regression-template"" rel=""nofollow noreferrer"">this repository</a>. If I run the complete pipeline as it is It works perfectly:</p>
<pre><code>import os
from mlflow.pipelines import Pipeline

os.chdir(&quot;~/mlp-regression-template&quot;)
regression_pipeline = Pipeline(profile=&quot;local&quot;)
# Display a visual overview of the pipeline graph
regression_pipeline.inspect()
# Run the full pipeline
regression_pipeline.run()
</code></pre>
<p>But when I try to change the first part to read diferent dataset, I get the following error:</p>
<pre><code>mlflow.exceptions.MlflowException: Resolved data file with path '/tmp/tmpv201mpms/precio_leche.csv' does not have the expected format 'parquet'.
</code></pre>
<p>Which is correct, the input file is not in csv format now, I added a new file to the data folder and changed the profile local.yaml file:</p>
<pre><code>INGEST_DATA_LOCATION: &quot;./data/precio_leche.csv&quot; 
</code></pre>
<p>What I dont understand is that in the in the pipeline the insgest step, executes the ingest.py code:</p>
<p><a href=""https://i.stack.imgur.com/fUx48.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fUx48.png"" alt=""enter image description here"" /></a></p>
<p>Which funtion is to convert the read csv files:</p>
<pre><code>def load_file_as_dataframe(file_path: str, file_format: str) -&gt; DataFrame:
    &quot;&quot;&quot;
    Load content from the specified dataset file as a Pandas DataFrame.

    This method is used to load dataset types that are not natively  managed by MLflow Pipelines
    (datasets that are not in Parquet, Delta Table, or Spark SQL Table format). This method is
    called once for each file in the dataset, and MLflow Pipelines automatically combines the
    resulting DataFrames together.

    :param file_path: The path to the dataset file.
    :param file_format: The file format string, such as &quot;csv&quot;.
    :return: A Pandas DataFrame representing the content of the specified file.
    &quot;&quot;&quot;

    if file_format == &quot;csv&quot;:
        import pandas

        _logger.warning(
            &quot;Loading dataset CSV using `pandas.read_csv()` with default arguments and assumed index&quot;
            &quot; column 0 which may not produce the desired schema. If the schema is not correct, you&quot;
            &quot; can adjust it by modifying the `load_file_as_dataframe()` function in&quot;
            &quot; `steps/ingest.py`&quot;
        )
        return pandas.read_csv(file_path, index_col=0)
    else:
        raise NotImplementedError
</code></pre>
<p>After getting this error I also tried changing the pipeline.yaml file to have .csv as the default format:</p>
<pre><code>  format: {{INGEST_DATA_FORMAT|default('csv')}}
</code></pre>
<p>But it didn't work either, also I notice that when I run the ingest step for the default dataset it return this summary from the pandas profilling library:</p>
<p><a href=""https://i.stack.imgur.com/anW8q.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/anW8q.png"" alt=""enter image description here"" /></a></p>
<p>But I do not see where this is in the code, am I changing the wrong files? or what should I do in order to read csv files in the ingest step? also. I'm looking to read several files, not only one.</p>",1,3,2022-09-18 00:41:01.670000 UTC,,,1,python|csv|machine-learning|mlflow|mlops,71,2015-02-08 23:53:31.840000 UTC,2022-09-24 00:08:32.523000 UTC,,8349,1489,6,949,,,,,,['mlflow']
AzureML pyarrow dependency with transfromers,"<p>I was trying to import transformers in AzureML designer pipeline, it says for importing transformers and datasets the version of pyarrow needs to &gt;=3.0.0, but then after upgrading pyarrow's version to 3.0.0 and importing transformers pyarrow version is reset to original version of 0.16.0. attaching few error samples. please have a look.</p>
<blockquote>
<p>Got exception when invoking script: 'RuntimeError: Failed to import transformers.trainer because of the following error (look up to see its traceback):To use <code>datasets</code>, the module <code>pyarrow&gt;=3.0.0</code> is required, and the current version of <code>pyarrow</code> doesn't match this condition.If you are running this in a Google Colab, you should probably just restart the runtime to use the right version of <code>pyarrow</code>.'
azureml-designer-core 0.0.68 requires pyarrow==0.16.0, but you'll have pyarrow 3.0.0 which is incompatible.</p>
</blockquote>",0,0,2022-02-28 08:18:19.070000 UTC,,,0,python|pyarrow|azure-machine-learning-service,85,2022-02-28 07:49:34.683000 UTC,2022-08-01 11:04:39.153000 UTC,,1,0,0,0,,,,,,['azure-machine-learning-service']
Highlight.js not respecting parent regex of a sub mode,"<p>I need to write a lexer which highlights my command-line tool commands properly.</p>

<pre><code>$ dvc add file.csv
$ dvc pipeline list
</code></pre>

<p>So the command starts with <code>dvc</code> and it may have one or two subcommands - <code>add</code> or <code>pipeline list</code> respectively.</p>

<p>Therefore, it should highlight <code>dvc add</code> and <code>dvc pipeline list</code> in first and second case respectively.</p>

<pre><code>contains: [
          {
            begin: /^\s*\$\s(dvc|git) [a-z-]+/,
            returnBegin: true,
            contains: [
              {
                begin: /dvc [a-z-]+ ?/,
                lexemes: '[a-z-]+',
                keywords: {
                  built_in:
                    'dvc'
                },
                contains: [
                  {
                    begin: /\w+(?![\S])/,
                    keywords: {
                      built_in: 'list'
                    }
                  }
                ],
                className: 'strong'
              }
            ]
          }
        ]
</code></pre>

<p>It matches <code>dvc pipeline list</code> even though the parent regex i.e. <code>/^\s*\$\s(dvc|git) [a-z-]+/</code> should only match till <code>dvc pipeline</code>. How is it exactly functioning?</p>

<p>How does <code>/dvc [a-z-]+ ?/</code> override it and continues matching the expression?</p>

<p>Please refer to this library docs here: <a href=""https://highlightjs.readthedocs.io/en/latest/reference.html"" rel=""nofollow noreferrer"">https://highlightjs.readthedocs.io/en/latest/reference.html</a></p>",1,7,2019-10-22 20:02:30.387000 UTC,2.0,2019-10-26 00:03:06.933000 UTC,3,javascript|syntax-highlighting|highlight|highlight.js|dvc,177,2019-07-07 20:04:39.143000 UTC,2021-07-07 17:33:03.400000 UTC,,209,32,0,27,,,,,,['dvc']
ML Model pod keeps restarting in Seldon deployment,"<p>I have a Seldon deployment like this:</p>
<pre><code>apiVersion: machinelearning.seldon.io/v1alpha2
kind: SeldonDeployment
metadata:
  name: mlflow
spec:
  name: wines
  predictors:
    - graph:
        children: []
        implementation: MLFLOW_SERVER
        modelUri: gs://seldon-models/mlflow/elasticnet_wine
        name: classifier
      name: default
      replicas: 1     
</code></pre>
<p>Model is downloaded successfully from the server, but, after a while, pods go to state <code>crashloop</code> and restart again and again.</p>
<p>When I see the logs, there is no errors since logs have re-started and I can only see how python packages are being downloaded.</p>
<pre><code>PS C:\Users\xxx\mlflow&gt; kubectl logs -p -c wines-classifier model-a-wines-classifier-0-wines-classifier-5b8bc7889d-5t7wp
</code></pre>
<pre><code>Executing before-run script
---&gt; Creating environment with Conda...
INFO:root:Copying contents of /mnt/models to local
INFO:root:Reading MLmodel file
INFO:root:Creating Conda environment 'mlflow' from conda.yaml
Warning: you have pip-installed dependencies in your environment file, but you do not list pip itself as one of your conda dependencies.  Conda may not use the correct pip to install your packages, and they may end up in the wrong place.  Please add an explicit pip dependency.  I'm adding one for you, but still nagging you.
Collecting package metadata (repodata.json): ...working... done
Solving environment: ...working... done

Downloading and Extracting Packages
_libgcc_mutex-0.1    | 3 KB      | ########## | 100%
readline-7.0         | 324 KB    | ########## | 100%
ncurses-6.2          | 817 KB    | ########## | 100%
tbb4py-2020.0        | 209 KB    | ########## | 100%
scipy-1.1.0          | 13.2 MB   | ########## | 100%
zlib-1.2.11          | 103 KB    | ########## | 100%
xz-5.2.5             | 341 KB    | ########## | 100%
openssl-1.1.1g       | 2.5 MB    | ########## | 100%
mkl_fft-1.0.6        | 135 KB    | ########## | 100%
blas-1.0             | 6 KB      | ########## | 100%
pip-20.1.1           | 1.8 MB    | ########## | 100%
wheel-0.34.2         | 51 KB     | ########## | 100%
libffi-3.2.1         | 40 KB     | ########## | 100%
scikit-learn-0.19.1  | 3.9 MB    | ########## | 100%
libgfortran-ng-7.3.0 | 1006 KB   | ########## | 100%
sqlite-3.32.3        | 1.1 MB    | ########## | 100%
numpy-1.15.4         | 34 KB     | ########## | 100%
tk-8.6.10            | 3.0 MB    | ########## | 100%
libgcc-ng-9.1.0      | 5.1 MB    | ########## | 100%
setuptools-47.3.1    | 514 KB    | ########## | 100%
mkl_random-1.0.1     | 324 KB    | ########## | 100%
python-3.6.9         | 30.2 MB   | ########## | 100%
certifi-2020.6.20    | 156 KB    | ########## | 100%
numpy-base-1.15.4    | 3.4 MB    | ########## | 100%
intel-openmp-2019.4  | 729 KB    | ########## | 100%
libedit-3.1.20191231 | 167 KB    | ########## | 100%
libstdcxx-ng-9.1.0   | 3.1 MB    | ########## | 100%
tbb-2020.0           | 1.1 MB    | ########## | 100%
mkl-2018.0.3         | 126.9 MB  | #########  |  91%
</code></pre>
<p>Now, trying with <code>-p</code> parameter as proposed by @arghya-sadhu:</p>
<pre><code>PS C:\Users\xxx\mlflow&gt; kubectl logs -p model-a-wines-classifier-0-wines-classifier-5b8bc7889d-5t7wp wines-classifier
</code></pre>
<pre><code>---&gt; Creating environment with Conda...
INFO:root:Copying contents of /mnt/models to local
INFO:root:Reading MLmodel file
INFO:root:Creating Conda environment 'mlflow' from conda.yaml
Warning: you have pip-installed dependencies in your environment file, but you do not list pip itself as one of your conda dependencies.  Conda may not use the correct pip to install your packages, and they may end up in the wrong place.  Please add an explicit pip dependency.  I'm adding one for you, but still nagging you.
Collecting package metadata (repodata.json): ...working... done
Solving environment: ...working... done

Downloading and Extracting Packages
scikit-learn-0.19.1  | 3.9 MB    | ########## | 100%
ncurses-6.2          | 817 KB    | ########## | 100%
_libgcc_mutex-0.1    | 3 KB      | ########## | 100%
zlib-1.2.11          | 103 KB    | ########## | 100%
tbb4py-2020.0        | 209 KB    | ########## | 100%
setuptools-47.3.1    | 514 KB    | ########## | 100%
libedit-3.1.20191231 | 167 KB    | ########## | 100%
tbb-2020.0           | 1.1 MB    | ########## | 100%
xz-5.2.5             | 341 KB    | ########## | 100%
mkl_random-1.0.1     | 324 KB    | ########## | 100%
libgcc-ng-9.1.0      | 5.1 MB    | ########## | 100%
python-3.6.9         | 30.2 MB   | ########## | 100%
libgfortran-ng-7.3.0 | 1006 KB   | ########## | 100%
libffi-3.2.1         | 40 KB     | ########## | 100%
mkl-2018.0.3         | 126.9 MB  | ########## | 100%
libstdcxx-ng-9.1.0   | 3.1 MB    | ########## | 100%
readline-7.0         | 324 KB    | ########## | 100%
intel-openmp-2019.4  | 729 KB    | ########## | 100%
tk-8.6.10            | 3.0 MB    | ########## | 100%
pip-20.1.1           | 1.8 MB    | ########## | 100%
numpy-base-1.15.4    | 3.4 MB    | ########## | 100%
wheel-0.34.2         | 51 KB     | ########## | 100%
scipy-1.1.0          | 13.2 MB   | #########3 |  93%
</code></pre>
<p>And the description of the pod:</p>
<pre><code>PS C:\Users\ivarea\repo\smartgraph\mlflow-v2&gt; kubectl describe pod model-a-wines-classifier-0-wines-classifier-5b8bc7889d-5t7wp
</code></pre>
<pre><code>Name:         model-a-wines-classifier-0-wines-classifier-5b8bc7889d-5t7wp
Namespace:    default
Priority:     0
Node:         mlops-control-plane/172.19.0.2
Start Time:   Thu, 25 Jun 2020 10:08:20 +0200
Labels:       app=model-a-wines-classifier-0-wines-classifier
              fluentd=true
              pod-template-hash=5b8bc7889d
              seldon-app=model-a-wines-classifier
              seldon-app-svc=model-a-wines-classifier-wines-classifier
              seldon-deployment-id=model-a
              version=wines-classifier
Annotations:  prometheus.io/path: /prometheus
              prometheus.io/scrape: true
Status:       Running
IP:           10.244.0.17
IPs:
  IP:           10.244.0.17
Controlled By:  ReplicaSet/model-a-wines-classifier-0-wines-classifier-5b8bc7889d
Init Containers:
  wines-classifier-model-initializer:
    Container ID:  containerd://6a3b158cf4218f8c177f6d18eb5d0387946bf9cc36f1173754b68a029483da8b
    Image:         gcr.io/kfserving/storage-initializer:0.2.2
    Image ID:      gcr.io/kfserving/storage-initializer@sha256:7a7d3cf4c5121a3e6bad0acc9e88bbdfa9c7f774d80bd64d8e35a84dcfef8890
    Port:          &lt;none&gt;
    Host Port:     &lt;none&gt;
    Args:
      gs://seldon-models/mlflow/model-a
      /mnt/models
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Thu, 25 Jun 2020 10:08:24 +0200
      Finished:     Thu, 25 Jun 2020 10:08:47 +0200
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     1
      memory:  1Gi
    Requests:
      cpu:        100m
      memory:     100Mi
    Environment:  &lt;none&gt;
    Mounts:
      /mnt/models from wines-classifier-provision-location (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-6vqwk (ro)
Containers:
  wines-classifier:
    Container ID:   containerd://536753d25877994a17d1f1a63bbaf8717dc9180b80f061152688e4c8504c8468
    Image:          seldonio/mlflowserver_rest:0.5
    Image ID:       docker.io/seldonio/mlflowserver_rest@sha256:0fd54a0a314fafc82c490c91df0c4776be454702a307b4b76e12ed6958b4ee00
    Ports:          6000/TCP, 9000/TCP
    Host Ports:     0/TCP, 0/TCP
    State:          Running
      Started:      Thu, 25 Jun 2020 10:23:28 +0200
    Last State:     Terminated
      Reason:       Error
      Exit Code:    137
      Started:      Thu, 25 Jun 2020 10:19:09 +0200
      Finished:     Thu, 25 Jun 2020 10:20:41 +0200
    Ready:          False
    Restart Count:  7
    Liveness:       tcp-socket :http delay=60s timeout=1s period=5s #success=1 #failure=3
    Readiness:      tcp-socket :http delay=20s timeout=1s period=5s #success=1 #failure=3
    Environment:
      PREDICTIVE_UNIT_SERVICE_PORT:          9000
      PREDICTIVE_UNIT_ID:                    wines-classifier
      PREDICTIVE_UNIT_IMAGE:                 seldonio/mlflowserver_rest:0.5
      PREDICTOR_ID:                          wines-classifier
      PREDICTOR_LABELS:                      {&quot;version&quot;:&quot;wines-classifier&quot;}
      SELDON_DEPLOYMENT_ID:                  model-a
      PREDICTIVE_UNIT_METRICS_SERVICE_PORT:  6000
      PREDICTIVE_UNIT_METRICS_ENDPOINT:      /prometheus
      PREDICTIVE_UNIT_PARAMETERS:            [{&quot;name&quot;:&quot;model_uri&quot;,&quot;value&quot;:&quot;/mnt/models&quot;,&quot;type&quot;:&quot;STRING&quot;}]
    Mounts:
      /etc/podinfo from podinfo (rw)
      /mnt/models from wines-classifier-provision-location (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-6vqwk (ro)
  seldon-container-engine:
    Container ID:  containerd://938e8f7e3ac23355c8a7a475b71ab54b858aff5ca485f26b99feaba09bb60069
    Image:         docker.io/seldonio/seldon-core-executor:1.1.0
    Image ID:      docker.io/seldonio/seldon-core-executor@sha256:661173fcbc6cb4e9b56db353b19e97d04d9c086e9dc445217f84dc1721bdf894
    Ports:         8000/TCP, 8000/TCP, 5001/TCP
    Host Ports:    0/TCP, 0/TCP, 0/TCP
    Args:
      --sdep
      model-a
      --namespace
      default
      --predictor
      wines-classifier
      --http_port
      8000
      --grpc_port
      5001
      --transport
      rest
      --protocol
      seldon
      --prometheus_path
      /prometheus
    State:          Running
      Started:      Thu, 25 Jun 2020 10:08:51 +0200
    Ready:          False
    Restart Count:  0
    Requests:
      cpu:      100m
    Liveness:   http-get http://:8000/live delay=20s timeout=60s period=5s #success=1 #failure=3
    Readiness:  http-get http://:8000/ready delay=20s timeout=60s period=5s #success=1 #failure=3
    Environment:
      ENGINE_PREDICTOR:  &lt;binary ommited&gt;
      REQUEST_LOGGER_DEFAULT_ENDPOINT_PREFIX:  http://default-broker.
      SELDON_LOG_MESSAGES_EXTERNALLY:          false
    Mounts:
      /etc/podinfo from podinfo (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-6vqwk (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             False
  ContainersReady   False
  PodScheduled      True
Volumes:
  podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.annotations -&gt; annotations
  wines-classifier-provision-location:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:
    SizeLimit:  &lt;unset&gt;
  default-token-6vqwk:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-6vqwk
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  &lt;none&gt;
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type     Reason     Age                  From                          Message
  ----     ------     ----                 ----                          -------
  Normal   Scheduled  &lt;unknown&gt;            default-scheduler             Successfully assigned default/model-a-wines-classifier-0-wines-classifier-5b8bc7889d-5t7wp to mlops-control-plane
  Normal   Pulled     15m                  kubelet, mlops-control-plane  Container image &quot;gcr.io/kfserving/storage-initializer:0.2.2&quot; already present on machine
  Normal   Created    15m                  kubelet, mlops-control-plane  Created container wines-classifier-model-initializer
  Normal   Started    15m                  kubelet, mlops-control-plane  Started container wines-classifier-model-initializer
  Normal   Pulled     15m                  kubelet, mlops-control-plane  Container image &quot;seldonio/mlflowserver_rest:0.5&quot; already present on machine
  Normal   Created    15m                  kubelet, mlops-control-plane  Created container wines-classifier
  Normal   Started    15m                  kubelet, mlops-control-plane  Started container wines-classifier
  Normal   Pulled     15m                  kubelet, mlops-control-plane  Container image &quot;docker.io/seldonio/seldon-core-executor:1.1.0&quot; already present on machine
  Normal   Created    14m                  kubelet, mlops-control-plane  Created container seldon-container-engine
  Normal   Started    14m                  kubelet, mlops-control-plane  Started container seldon-container-engine
  Warning  Unhealthy  14m (x8 over 14m)    kubelet, mlops-control-plane  Readiness probe failed: dial tcp 10.244.0.17:9000: connect: connection refused
  Warning  Unhealthy  28s (x171 over 14m)  kubelet, mlops-control-plane  Readiness probe failed: HTTP probe failed with statuscode: 503
</code></pre>
<p>How can I disable restarting so I can inspect logs to see the actual error?</p>",2,0,2020-06-25 07:09:40.727000 UTC,,2020-06-25 08:40:53.297000 UTC,2,kubernetes|mlflow|seldon,701,2013-02-28 09:37:55.997000 UTC,2022-01-03 12:35:46.230000 UTC,Spain,2570,183,2,106,,,,,,['mlflow']
Azure dataset .to_pandas_dataframe() error,"<p>I am following an azure ml course on udemy and cannot get around the following error:</p>
<p>Execution failed in operation 'to_pandas_dataframe' for Dataset(id='id', name='Loan Applications Using SDK', version=1, error_code=None, exception_type=PandasImportError)</p>
<p>Here is the code for Submitting the Script:</p>
<pre><code>from azureml.core import Workspace, Experiment, ScriptRunConfig, 
Environment


ws = Workspace.from_config(path=&quot;./config&quot;)


new_experiment = Experiment(workspace=ws,
                            name=&quot;Loan_Script&quot;)


script_config = ScriptRunConfig(source_directory=&quot;.&quot;,
                                script=&quot;180 - Script to Run.py&quot;)

script_config.framework = &quot;python&quot;
script_config.environment = Environment(&quot;conda_env&quot;)

new_run = new_experiment.submit(config=script_config)
</code></pre>
<p>Here is the Script being run:</p>
<pre><code>from azureml.core import Workspace, Datastore, Dataset, 
Experiment

from azureml.core import Run

ws = Workspace.from_config(path=&quot;./config&quot;)
az_store = Datastore.get(ws, &quot;bencouser_sdk_blob01&quot;)
az_dataset = Dataset.get_by_name(ws, name='Loan Applications Using SDK')
az_default_store = ws.get_default_datastore()


#%%----------------------------------------------------
# Get context of the run
#------------------------------------------------------


new_run = Run.get_context()


#%%----------------------------------------------------
# Stuff that will be logged
#------------------------------------------------------

df = az_dataset.to_pandas_dataframe()

total_observations = len(df)

nulldf = df.isnull().sum()

#%%----------------------------------------------------
# Complete the Experiment 
#------------------------------------------------------

new_run.log(&quot;Total Observations:&quot;, total_observations)

for columns in df.columns:
    new_run.log(columns, nulldf[columns])

new_run.complete()
</code></pre>
<p>I have run the .to_pandas_dataframe() part outside of an experiment and it worked without error. I have also tried the following (that was recommended in the driver log):</p>
<p>InnerException Could not import pandas. Ensure a compatible version is installed by running: pip install azureml-dataprep[pandas]</p>
<p>I have seen people come across this before but I cannot find a solution, any help is appreciated.</p>",2,2,2022-04-20 12:24:54.460000 UTC,,2022-04-21 08:23:10.953000 UTC,1,python|pandas|azure|machine-learning|azure-machine-learning-service,437,2020-10-24 10:49:09.850000 UTC,2022-09-21 12:00:40.910000 UTC,,51,2,0,7,,,,,,['azure-machine-learning-service']
Bad Identifier error with the initial-value attribute of the crowd-semantic-segmentation widget in Amazon Mechanical Turk,"<p>I am trying to create a Human Intelligence Task (HIT) on the Amazon Mechanical Turk (MTurk) platform, where I would like workers to update semantic segmentation mask that have been created by an algorithm.</p>
<p>Here you can see an example of the autogenerated mask:
<a href=""https://semantic-segmentation-bucket.s3.eu-central-1.amazonaws.com/females_masks/988.png"" rel=""nofollow noreferrer"">Sample Image</a></p>
<p>As you can see, there is a little bit of 'noise' in the mask, that could easily be fixed by workers within a few minutes. I have set-up a semantic segmentation HIT using the crowd-semantic-segmentation widget before on MTurk, which works perfectly fine. In the documentation (<a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-dg.pdf#sms-ui-template-crowd-semantic-segmentation"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-dg.pdf#sms-ui-template-crowd-semantic-segmentation</a>) , I see that there is an optional argument <em>initial-value</em>, specified as:</p>
<blockquote>
<p>A JSON object containing the color mappings of a prior semantic segmentation job and a link to the
overlay image output by the prior job. Include this when you want a human worker to verify the results
of a prior labeling job and adjust it if necessary.</p>
</blockquote>
<p>This is exactly what I am looking for. When I put the example code in my html file to test the syntax of this option,</p>
<pre><code>  initial-value='{
&quot;labelMappings&quot;: {
    &quot;Bird&quot;: {
      &quot;color&quot;: &quot;#ff7f0e&quot;
    },
    &quot;Cat&quot;: {
      &quot;color&quot;: &quot;#2ca02c&quot;
    },
    &quot;Cow&quot;: {
      &quot;color&quot;: &quot;#d62728&quot;
    },
    &quot;Dog&quot;: {
      &quot;color&quot;: &quot;#1f77b4&quot;
    }
  },
&quot;src&quot;: {{ &quot;S3 file URL for image&quot; | grant_read_access }}
</code></pre>
<p>}'</p>
<p>I get the following bad identifier error:</p>
<p><img src=""https://imgur.com/8wrQarw.png"" alt=""Error"" /></p>
<p>I would like to point the <em>src</em> attribute to a variable that links to all auto-generated mask images in my S3 bucket. With the <code>&quot;${image_url}&quot;</code> variable, I am able to use a CSV input file containing multiple image file URLs. Now, I try to substitute the <code>&quot;S3 file URL for image&quot;</code> with a <code>&quot;${mask_url}&quot;</code> variable that points to all 'noisy auto-generated masks' in my S3 bucket. I keep getting the same error message for this code:</p>
<p>My HTML code for the HIT can be found below. Can someone help me correctly set-up the <em>initial-value</em> option of the <em>crowd-semantic-segmentation</em> widget, with a variable pointing to the 'prior labeling jobs' that are auto-generated by my algorithm?</p>
<p>Thanks in advance!</p>
<p><strong>EDIT 23/07</strong>
Based on the answer of Amazon Mechanical Turk, I have altered my HTML code to the following:</p>
<pre><code>  &lt;crowd-semantic-segmentation
    src=&quot;${image_url}&quot;
    labels=&quot;['Background', 'Face', 'Hair', 'Beard', 'Brows', 'Eyes', 'Nose', 'Upper Lip', 'Lower Lip', 'Mouth', 'Body Skin', 'Ears', 'Ear Rings', 'Hat', 'Clothes' ]&quot;
    name=&quot;annotatedResult&quot;
    header=&quot;Color all facial features, clothes and background in the image&quot;
    initial-value='{
    &quot;labelMappings&quot;: {
        &quot;Background&quot;: {
          &quot;color&quot;: &quot;#000000&quot;
        },
        &quot;Clothes&quot;: {
          &quot;color&quot;: &quot;#00ff00&quot;
        },
      },
    &quot;src&quot; = &quot;${mask_url}&quot; 
    }'
  &gt;
</code></pre>
<p>Now, I get a different error:</p>
<p><img src=""https://i.imgur.com/xMxk9um.png"" alt=""Error"" /></p>
<p>I am probably mixing up the usage of <code>:</code> and <code>=</code>,  <code>&quot; .. &quot;</code> and <code>'{ .. }'</code>. At this point, I am completely lost at the correct usage of the initial-value attribute. Could you please have another look at the correct code and be very precise in the usage of quotes, brackets, colons and equal sign?</p>",1,0,2020-07-09 15:43:53.567000 UTC,,2020-07-23 13:23:31.640000 UTC,0,label|amazon-sagemaker|mechanicalturk|semantic-segmentation,263,2015-01-29 22:16:48.513000 UTC,2022-09-22 11:31:39.723000 UTC,,677,16,0,81,,,,,,['amazon-sagemaker']
How connect Azure Function with my own model? It is possible to use Azure Storage?,"<h3>Intro</h3>
<p>I created my own model locally and then register it and deploy it to azure and it works.</p>
<h3>deployed model output:</h3>
<p><a href=""https://i.stack.imgur.com/gaLCH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gaLCH.png"" alt=""enter image description here"" /></a></p>
<h3>my approach</h3>
<p>I used <a href=""https://medium.com/microsoftazure/deploying-azure-machine-learning-containers-41bcb02a4e1b"" rel=""nofollow noreferrer"">this tutorial</a>, and I want use my model in Azure Function and I can do it:</p>
<pre class=""lang-py prettyprint-override""><code>def main(req: func.HttpRequest, msg: func.Out[func.QueueMessage]) -&gt; str:
    name = req.params.get('name')
    scoring_uri = 'http://1f72b1bf-5ca9-42d9-bedd-f41773591a4f.francecentral.azurecontainer.io/score'
    headers = {'Content-Type':'application/json'}
    test_data = json.dumps({'text': 'Today is a great day!'})
    response = requests.post(scoring_uri, data=test_data, headers=headers)

if not name:
    try:
        req_body = req.get_json()
    except ValueError:
        pass
    else:
        name = req_body.get('name')

if name:
    msg.set(name)
    return func.HttpResponse(f&quot;Hello {name}! Najlepszy wynik: {response.json()}&quot;)
else:
    return func.HttpResponse(
        &quot;Please pass a name on the query string or in the request body&quot;,
        status_code=400
    )
</code></pre>
<h3>questions</h3>
<ol>
<li>Is my usage correct?</li>
<li>Is it possible to use azure storage for model storage and how to do it?</li>
<li>Is there any other way to use the model in Azure Function?</li>
</ol>
<p>I am wondering because I had specified in the requirements that I should use azure functions and azure storage. I don't understand why.</p>",1,0,2021-01-05 21:22:46.290000 UTC,,2021-01-06 15:01:34.497000 UTC,0,azure|azure-functions|azure-storage|azure-machine-learning-service,210,2020-11-20 01:00:01.337000 UTC,2021-04-10 20:06:50.227000 UTC,,73,1,0,24,,,,,,['azure-machine-learning-service']
When can one find logs for Vertex AI Batch Prediction jobs?,<p>I couldn't find relevant information in the Documentation. I have tried all options and links in the batch transform pages.</p>,2,0,2021-10-14 06:08:05.233000 UTC,2.0,,4,google-cloud-platform|google-cloud-vertex-ai,492,2021-10-14 05:56:24.710000 UTC,2022-09-23 07:04:34.437000 UTC,,41,0,0,2,,,,,,['google-cloud-vertex-ai']
Azureml ignore environment variables in condas env.yml,"<p>I am configuring an Environment in azureml based on a conda enviroment file.
The azureml environment seems to be ignoring the enviromnet variables however.</p>
<pre><code>from azureml.core import Environment
from azureml.core.conda_dependencies import CondaDependencies

CondaDependencies._VALID_YML_KEYS.append(&quot;variables&quot;)
pipeline_env = Environment.from_conda_specification(&quot;pipeline_env&quot;, &quot;env.yml&quot;)
print(pipeline_env.environment_variables)
</code></pre>
<p>This results in the following being printed.</p>
<pre><code>{'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE'}
</code></pre>
<p>My env.yml contain the follow section at the bottom</p>
<pre><code>variables:
- KEY_ONE: 1.1.0.1
- KEY_TWO: 1.1.0.1
</code></pre>
<p>And if i save my environment to directory like this</p>
<pre><code>pipeline_env.save_to_directory(&quot;env&quot;)
</code></pre>
<p>it produces a folder named &quot;env&quot; which contain two files.</p>
<ul>
<li>conda_dependencies.yml</li>
<li>azureml_environment.json</li>
</ul>
<p>In the azureml_environment i can see that my two keys do <strong>not</strong> exist.
They do however exist in the conda_dependancies.yml which indicate to me that they are correctly defined in the env.yml file.</p>
<p>I also had to add the &quot;varialbes&quot; key as a valid yml key as shown, if not azureml threw an error.</p>
<p>I am starting to suspect that azureml does not allow this method of setting the environment variables, and that the only way to set them correctly is to use the following method:</p>
<pre><code> pipeline_env.environment_variables = {&quot;KEY_ONE&quot;, &quot;1.1.0.1&quot;,
                                       &quot;KEY_TWO&quot;, &quot;1.1.0.1&quot;)
</code></pre>
<p>As this does work, i would prefer to use the .yml file however.
So i guess my question is: <strong>Should i be able to set environment variables using the .yml file, or is my assumption correct that i have to use the enviroment_variables function?</strong></p>",1,0,2022-01-31 14:14:59.807000 UTC,,,1,python|azure-machine-learning-service|azureml-python-sdk,214,2015-07-22 12:57:19.603000 UTC,2022-03-21 08:35:07.183000 UTC,Norway,1186,128,31,133,,,,,,['azure-machine-learning-service']
How to get the Endpoint for a model built in my machine,"<p>I have built a model in my machine and want to deploy the model (catboost classifier) in the amazon sagemaker. Not able to get the endpoint of the model. Looking for a code/ process to get the end point. I tried with deploy function, but it is not giving the endpoint, the message is deploy function don't exist for catboostclassifier()</p>",1,0,2022-09-15 02:04:57.110000 UTC,,,0,python|deployment|amazon-sagemaker|endpoint|catboost,34,2022-09-15 01:56:16.433000 UTC,2022-09-20 19:15:06.920000 UTC,,1,0,0,2,,,,,,['amazon-sagemaker']
AWS SageMaker: ValueError: The least populated class in y has only 1 member error,"<p>i'm working with AWS SageMaker and i'm trying to recreating my own model taking as example the movie genre predictions.</p>

<p>That's the code:</p>

<pre><code>def split(df, test_size):
    data = df.values
    data_y = df.drop(['luogo', 'testo', 'lingua'], axis=1).values
    #StratifiedShuffleSplit does not work with one hot encoded / multiple 
    labels. Doing the split on basis of arg max labels.
    data_y = np.argmax(data_y, axis=1)
    data_y.shape
    stratified_split = StratifiedShuffleSplit(n_splits=2, 
test_size=test_size, random_state=42)
    for train_index, test_index in stratified_split.split(data, data_y):
        train, test = df.iloc[train_index], df.iloc[test_index]
     return train, test

train, test = split(df, 0.33)
#Split the train further into train and validation
train, validation = split(train, 0.2)
</code></pre>

<p>This is the dataframe: 
<a href=""https://i.stack.imgur.com/zTCSO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zTCSO.png"" alt=""enter image description here""></a></p>

<p>And this is the example dataframe:</p>

<p><a href=""https://i.stack.imgur.com/PGRGi.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PGRGi.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/QlMUA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QlMUA.png"" alt=""enter image description here""></a></p>

<p>This is the error:</p>

<pre><code>ValueError: The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.
</code></pre>

<p>How can i modify my df?</p>

<p>Notice that the example df has more than one '1' in the same row.</p>",1,0,2019-08-17 14:16:02.620000 UTC,,,0,python-3.x|python-2.7|amazon-web-services|jupyter-notebook|amazon-sagemaker,102,2018-04-16 15:34:34.820000 UTC,2022-09-22 15:26:41.440000 UTC,,125,8,0,28,,,,,,['amazon-sagemaker']
Can i Pass Dataset as a parameter to AzureML experiment in Streaming Analytics Job?,"<p>Can i Pass Dataset as a parameter to AzureML experiment in Streaming Analytics Job? Right now im passing parameters like this ,  </p>

<pre><code>   SELECT test (var1,var2,var3,var4,var5) as Result
   FROM [Input-eventhub]
</code></pre>

<p>So instead of that can i pass dataset instead of this like,
      SELECT test (datset) as Result
       FROM [Input-eventhub]Azurestre</p>",1,0,2017-07-12 07:32:11.840000 UTC,,2017-07-13 06:47:50.637000 UTC,0,tsql|azure-stream-analytics|azure-machine-learning-studio,47,2017-06-22 07:04:57.193000 UTC,2019-08-30 09:51:10.357000 UTC,,61,0,0,17,,,,,,['azure-machine-learning-studio']
`pipeline_job` registered model input,"<p>I'm using the pipeline private preview from CLI (v2).</p>
<p>I'd like to know how to use a registered model as an input of my pipeline.</p>
<p>Similarly to how I access the blob storage:</p>
<pre class=""lang-yaml prettyprint-override""><code>inputs:
  input_data:
    data:
      datastore: azureml:dualcam
      path: /image-20210701*
</code></pre>
<p>I'm expecting to be able to use my registered model but I can find the schema to add it.</p>",1,0,2021-07-12 21:40:44.697000 UTC,,,0,azure-machine-learning-service,29,2012-01-26 14:27:40.553000 UTC,2022-09-24 16:26:41.580000 UTC,,802,288,0,91,,,,,,['azure-machine-learning-service']
Wandb throws Permission denied error although I am logged in,"<p>I am using the cleanrl library, in particular the script <a href=""https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/dqn_atari.py"" rel=""nofollow noreferrer"">dqn_atari.py</a>  dqn_atari.py where  I followed the  <a href=""https://docs.cleanrl.dev/advanced/resume-training/"" rel=""nofollow noreferrer"">instructions</a> in order to save and load the target and Q-network.</p>
<p>I am running it locally within a conda environment.</p>
<p>I haven't loaded something before, so the error may be due to my wandb configuration. The error is &quot;wandb: ERROR Permission denied to access wandb_entity/wandb_project_name/project_id&quot; and appears on line:</p>
<pre><code>model = run.file(&quot;agent.pt&quot;)
</code></pre>
<p>The full  output is:</p>
<pre><code>wandb: Currently logged in as: elena (use `wandb login --relogin` to force relogin)
    wandb: Tracking run with wandb version 0.12.15
wandb: Run data is saved locally in /home/elena/workspace/playground/cleanrl/wandb/run-20220424_180429-2moec0qp
wandb: Run `wandb offline` to turn off syncing.
wandb: Resuming run BreakoutNoFrameskip-v4__dqn-save__1__1650816268
wandb: ⭐ View project at https://wandb.ai/elena/test
wandb:  View run at https://wandb.ai/elena/test/runs/2moec0qp
A.L.E: Arcade Learning Environment (version 0.7.4+069f8bd)
[Powered by Stella]
/home/elena/anaconda3/envs/cleanrl/lib/python3.8/site-packages/stable_baselines3/common/buffers.py:219: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 28.24GB &gt; 8.35GB
  warnings.warn(
wandb: ERROR Permission denied to access elena/test/2moec0qp
Traceback (most recent call last):
  File &quot;/home/elena/anaconda3/envs/cleanrl/lib/python3.8/site-packages/wandb/sdk/lib/retry.py&quot;, line 102, in __call__
    result = self._call_fn(*args, **kwargs)
  File &quot;/home/elena/anaconda3/envs/cleanrl/lib/python3.8/site-packages/wandb/apis/public.py&quot;, line 2428, in download
    util.download_file_from_url(path, self.url, Api().api_key)
  File &quot;/home/elena/anaconda3/envs/cleanrl/lib/python3.8/site-packages/wandb/util.py&quot;, line 1197, in download_file_from_url
    response.raise_for_status()
  File &quot;/home/elena/anaconda3/envs/cleanrl/lib/python3.8/site-packages/requests/models.py&quot;, line 960, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://api.wandb.ai/files/elena/test/2moec0qp/td_network.pt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;/home/elena/anaconda3/envs/cleanrl/lib/python3.8/site-packages/wandb/apis/normalize.py&quot;, line 22, in wrapper
    return func(*args, **kwargs)
  File &quot;/home/elena/anaconda3/envs/cleanrl/lib/python3.8/site-packages/wandb/sdk/lib/retry.py&quot;, line 159, in wrapped_fn
    return retrier(*args, **kargs)
  File &quot;/home/elena/anaconda3/envs/cleanrl/lib/python3.8/site-packages/wandb/sdk/lib/retry.py&quot;, line 118, in __call__
    if not check_retry_fn(e):
  File &quot;/home/elena/anaconda3/envs/cleanrl/lib/python3.8/site-packages/wandb/util.py&quot;, line 877, in no_retry_auth
    raise CommError(f&quot;Permission denied to access {wandb.run.path}&quot;)
wandb.errors.CommError: Permission denied to access elena/test/2moec0qp

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;cleanrl/dqn_atari_save.py&quot;, line 184, in &lt;module&gt;
    model.download(f&quot;models/{args.exp_name}/&quot;)
  File &quot;/home/elena/anaconda3/envs/cleanrl/lib/python3.8/site-packages/wandb/apis/normalize.py&quot;, line 58, in wrapper
    raise CommError(message, err).with_traceback(sys.exc_info()[2])
  File &quot;/home/elena/anaconda3/envs/cleanrl/lib/python3.8/site-packages/wandb/apis/normalize.py&quot;, line 22, in wrapper
    return func(*args, **kwargs)
  File &quot;/home/elena/anaconda3/envs/cleanrl/lib/python3.8/site-packages/wandb/sdk/lib/retry.py&quot;, line 159, in wrapped_fn
    return retrier(*args, **kargs)
  File &quot;/home/elena/anaconda3/envs/cleanrl/lib/python3.8/site-packages/wandb/sdk/lib/retry.py&quot;, line 118, in __call__
    if not check_retry_fn(e):
  File &quot;/home/elena/anaconda3/envs/cleanrl/lib/python3.8/site-packages/wandb/util.py&quot;, line 877, in no_retry_auth
    raise CommError(f&quot;Permission denied to access {wandb.run.path}&quot;)
wandb.errors.CommError: Permission denied to access elena/test/2moec0qp
wandb: Waiting for W&amp;B process to finish... (failed 1). Press Control-C to abort syncing.
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: global_step ▁
wandb: 
wandb: Run summary:
wandb: charts/episodic_return 0
wandb:         charts/epsilon 0.01
wandb:          charts/update 1969
wandb:            global_step 0
wandb: 
wandb: Synced BreakoutNoFrameskip-v4__dqn-save__1__1650816268: https://wandb.ai/elena/test/runs/2moec0qp
wandb: Synced 3 W&amp;B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20220424_180429-2moec0qp/logs
</code></pre>
<p>So as you can see I am logged in and I can see the files under &quot;https://wandb.ai/elena/test/runs/2moec0qp?workspace=user-elena&quot;. Something that drew my attention was &quot;requests.exceptions.HTTPError: 404 Client Error: Not Found for url: <a href=""https://api.wandb.ai/files/elena/test/2moec0qp/agent.pt%22"" rel=""nofollow noreferrer"">https://api.wandb.ai/files/elena/test/2moec0qp/agent.pt&quot;</a>. This path indeed looks different from the https path, but maybe this is not the issue?
Any ideas?</p>",1,0,2022-04-24 16:44:28.313000 UTC,,,2,python|wandb,596,2014-12-13 21:14:38.743000 UTC,2022-09-23 11:32:16.703000 UTC,,319,15,0,34,,,,,,['wandb']
Why is AzureML SDK corrupting the default datastore?,"<p>I have tried following the documentation instructions  <a href=""https://docs.microsoft.com/en-us/azure/machine-learning/how-to-create-machine-learning-pipelines"" rel=""nofollow noreferrer"">here</a> (see my code below), and the pipeline seems to run okay. However, when I view it on ML Studio, it says the pipeline has failed because the container does not exist.</p>
<p>Worse, if I log into Microsoft Azure Storage Explorer, the default datastore appears to be corrupted somehow and displays the following message: <code>The specified container does not exist.</code>.
Before running this I was able to add files and folders to the container.</p>
<p>I have now tried this on two separate ML instances.</p>
<p>Does anyone know why?</p>
<p>I need to persist some data, so am using an <code>OutputFileDatasetConfig</code> object, and I am running code below on an Azure ML compute instance.</p>
<pre><code>from azureml.core import Workspace, Dataset, Datastore
from azureml.core.compute.compute import ComputeTarget
import azureml.core
from azureml.core import Workspace, Experiment
from azureml.core.environment import Environment
from azureml.pipeline.core import Pipeline
from azureml.pipeline.steps import PythonScriptStep
from azureml.core.runconfig import RunConfiguration
from azureml.data import OutputFileDatasetConfig


interactive_auth = InteractiveLoginAuthentication(tenant_id=tenant_id)
workspace = Workspace(
    subscription_id, resource_group, workspace_name, auth=interactive_auth)
output_datastore = Datastore(
    workspace=workspace,
     name=resource_manager.get_output_datastore())
output_config = OutputFileDatasetConfig(
    destination=(output_datastore, 'DomainConfiguration'))

step1 = PythonScriptStep(
    name=&quot;Script&quot;,
    script_name=&quot;script.py&quot;, 
    compute_target=compute_target, 
    source_directory=source_directory,
    arguments=[
        &quot;--output_configuration&quot;,
        output_config.as_mount(),
        ],
    allow_reuse=True,
    runconfig=runconfig,
    )
steps.append(step1)
pipeline = Pipeline(workspace=workspace, steps=steps)
pipeline.validate()
experiment = Experiment(workspace,'ExperimentName')
run = experiment.submit(pipeline, regenerate_outputs=False)
</code></pre>",1,0,2022-02-18 17:16:58.153000 UTC,,,0,azure-storage|azure-machine-learning-service|azureml-python-sdk,161,2022-02-18 17:01:42.717000 UTC,2022-07-25 08:42:30.473000 UTC,,1,0,0,0,,,,,,['azure-machine-learning-service']
What is the use of git commits in mlflow?,"<p>Why mlflow tracks git commits, we already have run_id for tracking experiment. Can we use those commits to go back to previous commit like we do in git.</p>",0,0,2022-04-21 07:00:26.890000 UTC,,,1,mlflow,161,2020-12-17 08:34:25.657000 UTC,2022-09-01 12:02:46.403000 UTC,,51,3,0,6,,,,,,['mlflow']
AzureML: Unable to unpickle LightGBM model,"<p>I am trying to run an Azure ML pipeline. This pipeline trains a model, saves it a pickle file and then tries to unpickle it in the next step. When trying to unpickle it, I am facing the below issue in any random run:</p>

<blockquote>
  <p>Traceback (most recent call last):
    File ""batch_scoring.py"", line 199, in 
      clf = joblib.load(open(model_path, 'rb'))
    File ""/azureml-envs/azureml_347514cea2002d6bd71b42aceb1e4eeb/lib/python3.6/site-packages/joblib/numpy_pickle.py"", line 595, in load
      obj = _unpickle(fobj)
    File ""/azureml-envs/azureml_347514cea2002d6bd71b42aceb1e4eeb/lib/python3.6/site-packages/joblib/numpy_pickle.py"", line 529, in _unpickle
      obj = unpickler.load()
    File ""/azureml-envs/azureml_347514cea2002d6bd71b42aceb1e4eeb/lib/python3.6/pickle.py"", line 1048, in load
      raise EOFError
  EOFError</p>
</blockquote>

<p>Has anyone faced this issue before?</p>",1,5,2020-02-04 12:01:02.377000 UTC,,,1,azure-machine-learning-service,319,2019-12-22 14:44:05.723000 UTC,2020-02-27 09:00:24.303000 UTC,,21,0,0,10,,,,,,['azure-machine-learning-service']
How to set NotebookApp.iopub_data_rate_limit in sagemaker,"<p>I got the error when i running the jupyter notebook in sagemaker:</p>

<pre><code>OPub data rate exceeded.
The notebook server will temporarily stop sending output
to the client in order to avoid crashing it.
To change this limit, set the config variable
`--NotebookApp.iopub_data_rate_limit`.
Current values:
NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)
NotebookApp.rate_limit_window=3.0 (secs)
</code></pre>

<p>I tried several solutions (e.g., <a href=""https://stackoverflow.com/questions/43288550/iopub-data-rate-exceeded-in-jupyter-notebook-when-viewing-image"">solution1</a>, <a href=""https://github.com/aws/sagemaker-python-sdk/issues/239"" rel=""nofollow noreferrer"">solution2</a>), but it did not work. </p>

<p>Could someone know how to solve the issue?</p>",1,1,2020-02-06 10:13:50.903000 UTC,,,3,python-3.x|amazon-sagemaker,2017,2019-08-22 12:56:15.907000 UTC,2022-09-19 03:24:31.930000 UTC,,171,7,0,14,,,,,,['amazon-sagemaker']
How to input data from S3 Bucket to Amazon Sagemaker,"<p>I would like to input data(Caltech 256 dataset) from Amazon s3 to sagemaker. I am doing this because I would like to modify the dataset if I can get this to work. Any ideas?</p>

<p>Tried using the pandas code from 'Load S3 Data into AWS SageMaker Notebook'</p>

<p>I hope the data from the S3 bucket will work just like downloading straight from the url. Obviously it isn't working.</p>",1,2,2019-03-29 20:39:45.250000 UTC,,2021-02-14 07:12:17.433000 UTC,0,amazon-s3|deep-learning|artificial-intelligence|amazon-sagemaker|aws-deeplens,194,2019-03-29 15:27:32.897000 UTC,2020-06-29 22:55:47.223000 UTC,,11,0,0,1,,,,,,['amazon-sagemaker']
Invoking SageMaker endpoint with data payload in S3 from a python boto client,"<p>I have an image that I upload to s3 in <code>mybucket</code>. Suppose my s3 endpoint for this data is <code>s3://mybucket/imgname</code></p>

<p>Now I also have a model deployed in SageMaker at <code>sagemaker-model-endpoint</code></p>

<p>I looked into examples of how to invoke this SageMaker endpoint from a boto client <a href=""https://aws.amazon.com/blogs/machine-learning/deploy-trained-keras-or-tensorflow-models-using-amazon-sagemaker/"" rel=""nofollow noreferrer"">here</a> but I am not sure how to specify the s3 path <code>s3://mybucket/imgname</code> in the <code>invoke_endpoint</code> call.</p>

<pre><code>client = boto3.client(""runtime.sagemaker"", region_name = 's3.us-east-2.amazonaws.com')
client.invoke_endpoint(
      EndpointName=sagemaker-model-endpoint
      Body=payload,
      ContentType='image/jpg',
      Accept='Accept')
</code></pre>

<p>What should be the <code>payload</code> in this case? Where do I specify the <code>s3</code> url?</p>",2,2,2019-02-12 00:26:02.893000 UTC,,2019-02-12 00:38:26.567000 UTC,3,amazon-web-services|amazon-s3|amazon-sagemaker,1358,2012-06-12 04:14:13.300000 UTC,2022-08-13 03:10:50.523000 UTC,,2448,445,5,337,,,,,,['amazon-sagemaker']
Does MLflow require to have docker installed?,"<p>I am reading a book about how to use MLflow.
The method is to install MLflow <em>inside</em> a container (not natively)
The dockerfile is</p>
<pre><code>FROM continuumio/miniconda3

RUN pip install mlflow&gt;=1.18.0 \
    &amp;&amp; pip install numpy \
    &amp;&amp; pip install scipy \
    &amp;&amp; pip install pandas \
    &amp;&amp; pip install scikit-learn \
    &amp;&amp; pip install cloudpickle \
    &amp;&amp; pip install pandas_datareader==0.10.0 \
    &amp;&amp; pip install yfinance
</code></pre>
<p>So I build this with <code>docker build -t stockpred -f Dockerfile .</code></p>
<p>Then I run it with <code>docker run -v $(pwd):/workfolder -it --rm stockpred</code></p>
<p>So I am inside the container, mlflow is installed there and I do:</p>
<pre><code>mlflow run .
2022/06/05 08:55:12 ERROR mlflow.cli: === Could not find Docker executable. Ensure Docker is installed as per the instructions at https://docs.docker.com/install/overview/. ===
</code></pre>
<p>What does this mean? MLflow requires to have docker installed <em>inside</em> the docker container? Does that mean that MLflow <em>uses docker</em>?</p>
<p>EDIT:
Reading MLflow tutorial (which uses conda) it seems that in fact Docker has to be installed <em>inside</em> Docker because when I use a <code>MLproject</code> file that uses <code>conda_env</code> and not <code>docker_env</code> <code>mlflow run .</code> seems to work well.</p>",0,1,2022-06-05 09:18:34.013000 UTC,,2022-06-06 12:32:42.367000 UTC,0,docker|mlflow,81,2015-01-14 01:17:49.333000 UTC,2022-09-24 09:09:14.427000 UTC,,5585,792,53,1350,,,,,,['mlflow']
SageMaker: TypeError: Object of type Join is not JSON serializable,"<p>I'm trying to build a SM pipeline for a computer vision model.
The data is images stored in S3 bucket. I did the preprocessing using ScriptProcessor and now am trying to build the estimator.
Preprocessing works alright. But the estimator part is giving me TypeError: Object of type Join is not JSON serializable: error.</p>
<pre><code>from sagemaker.tensorflow import TensorFlow


output_config = preprocessing_job_description[&quot;ProcessingOutputConfig&quot;]
for output in output_config[&quot;Outputs&quot;]:
    if output[&quot;OutputName&quot;] == &quot;train_data&quot;:
        preprocessed_training_data = output[&quot;S3Output&quot;][&quot;S3Uri&quot;]
    if output[&quot;OutputName&quot;] == &quot;valid_data&quot;:
        preprocessed_test_data = output[&quot;S3Output&quot;][&quot;S3Uri&quot;]

s3_train = &quot;s3://bucketname/image_data/train/&quot;
s3_val = &quot;s3://bucketname/image_data/val/&quot;


tf_estimator = TensorFlow(entry_point=&quot;train.py&quot;,
                          sagemaker_session=sess,
                          role=role,
                          instance_count=1, 
                          instance_type=&quot;ml.m5.xlarge&quot;,
                          # output_path = &quot;/opt/ml/processing/output&quot;,
                          model_dir=&quot;s3://bucketname/image_data/output&quot;,
                          py_version='py37',
                          framework_version='2.4', 
                          hyperparameters={'epochs': epochs,
                                           'learning_rate': learning_rate, 
                                           'train_batch_size': 64,
                                          },
                          metric_definitions=metrics_definitions,
                          script_mode=True,
                          max_run=7200 # max 2 hours * 60 minutes seconds per hour * 60 sec per minutes
                         )

tf_estimator.fit({&quot;train&quot;: preprocessed_training_data})
</code></pre>
<p>This gives me the following error:</p>
<hr />
<blockquote>
<p>TypeError                                 Traceback (most recent call
last)  in 
36                          )
37
---&gt; 38 tf_estimator.fit({&quot;train&quot;: preprocessed_training_data})
39 # tf_estimator.fit({&quot;train&quot;: s3_train})</p>
<p>/opt/conda/lib/python3.7/site-packages/sagemaker/workflow/pipeline_context.py
in wrapper(*args, **kwargs)
207             return self_instance.sagemaker_session.context
208
--&gt; 209         return run_func(*args, **kwargs)
210
211     return wrapper</p>
<p>/opt/conda/lib/python3.7/site-packages/sagemaker/estimator.py in
fit(self, inputs, wait, logs, job_name, experiment_config)
976         self._prepare_for_training(job_name=job_name)
977
--&gt; 978         self.latest_training_job = _TrainingJob.start_new(self, inputs, experiment_config)
979         self.jobs.append(self.latest_training_job)
980         if wait:</p>
<p>/opt/conda/lib/python3.7/site-packages/sagemaker/estimator.py in
start_new(cls, estimator, inputs, experiment_config)    1806<br />
train_args = cls._get_train_args(estimator, inputs, experiment_config)
1807
-&gt; 1808         estimator.sagemaker_session.train(**train_args)    1809     1810         return cls(estimator.sagemaker_session,
estimator._current_job_name)</p>
<p>/opt/conda/lib/python3.7/site-packages/sagemaker/session.py in
train(self, input_mode, input_config, role, job_name, output_config,
resource_config, vpc_config, hyperparameters, stop_condition, tags,
metric_definitions, enable_network_isolation, image_uri,
algorithm_arn, encrypt_inter_container_traffic, use_spot_instances,
checkpoint_s3_uri, checkpoint_local_path, experiment_config,
debugger_rule_configs, debugger_hook_config,
tensorboard_output_config, enable_sagemaker_metrics,
profiler_rule_configs, profiler_config, environment, retry_strategy)
592             encrypt_inter_container_traffic=encrypt_inter_container_traffic,
593             use_spot_instances=use_spot_instances,
--&gt; 594             checkpoint_s3_uri=checkpoint_s3_uri,
595             checkpoint_local_path=checkpoint_local_path,
596             experiment_config=experiment_config,</p>
<p>/opt/conda/lib/python3.7/site-packages/sagemaker/session.py in
_intercept_create_request(self, request, create, func_name)    4201         &quot;&quot;&quot;    4202         region = self.boto_session.region_name
-&gt; 4203         sts_client = self.boto_session.client(    4204             &quot;sts&quot;, region_name=region, endpoint_url=sts_regional_endpoint(region)
4205         )</p>
<p>/opt/conda/lib/python3.7/site-packages/sagemaker/session.py in
submit(request)
589             enable_network_isolation=enable_network_isolation,
590             image_uri=image_uri,
--&gt; 591             algorithm_arn=algorithm_arn,
592             encrypt_inter_container_traffic=encrypt_inter_container_traffic,
593             use_spot_instances=use_spot_instances,</p>
<p>/opt/conda/lib/python3.7/json/<strong>init</strong>.py in dumps(obj, skipkeys,
ensure_ascii, check_circular, allow_nan, cls, indent, separators,
default, sort_keys, **kw)
236         check_circular=check_circular, allow_nan=allow_nan, indent=indent,
237         separators=separators, default=default, sort_keys=sort_keys,
--&gt; 238         **kw).encode(obj)
239
240</p>
<p>/opt/conda/lib/python3.7/json/encoder.py in encode(self, o)
199         chunks = self.iterencode(o, _one_shot=True)
200         if not isinstance(chunks, (list, tuple)):
--&gt; 201             chunks = list(chunks)
202         return ''.join(chunks)
203</p>
<p>/opt/conda/lib/python3.7/json/encoder.py in _iterencode(o,
_current_indent_level)
429             yield from _iterencode_list(o, _current_indent_level)
430         elif isinstance(o, dict):
--&gt; 431             yield from _iterencode_dict(o, _current_indent_level)
432         else:
433             if markers is not None:</p>
<p>/opt/conda/lib/python3.7/json/encoder.py in _iterencode_dict(dct,
_current_indent_level)
403                 else:
404                     chunks = _iterencode(value, _current_indent_level)
--&gt; 405                 yield from chunks
406         if newline_indent is not None:
407             _current_indent_level -= 1</p>
<p>/opt/conda/lib/python3.7/json/encoder.py in _iterencode_dict(dct,
_current_indent_level)
403                 else:
404                     chunks = _iterencode(value, _current_indent_level)
--&gt; 405                 yield from chunks
406         if newline_indent is not None:
407             _current_indent_level -= 1</p>
<p>/opt/conda/lib/python3.7/json/encoder.py in _iterencode(o,
_current_indent_level)
436                     raise ValueError(&quot;Circular reference detected&quot;)
437                 markers[markerid] = o
--&gt; 438             o = _default(o)
439             yield from _iterencode(o, _current_indent_level)
440             if markers is not None:</p>
<p>/opt/conda/lib/python3.7/json/encoder.py in default(self, o)
177
178         &quot;&quot;&quot;
--&gt; 179         raise TypeError(f'Object of type {o.<strong>class</strong>.<strong>name</strong>} '
180                         f'is not JSON serializable')
181</p>
<p>TypeError: Object of type Join is not JSON serializable</p>
</blockquote>
<p>I have tried changing all the arguments I have given for the estimator. Sometimes enabling them and sometimes disabling them.
--&gt; 594 checkpoint_s3_uri=checkpoint_s3_uri,
If this is the origin, I have tried giving it also.</p>
<p>No idea where I'm messing up.
I'm using</p>
<pre><code>sagemaker 2.94.0
Python3 Data Science kernel
boto3 '1.24.8'
</code></pre>",0,1,2022-06-23 12:30:33.777000 UTC,,,1,python|amazon-web-services|tensorflow|amazon-sagemaker,155,2022-06-23 11:22:18.203000 UTC,2022-08-17 05:32:40.397000 UTC,,21,0,0,1,,,,,,['amazon-sagemaker']
How to solve vertica connection/vpn issue on sagemaker,"<p>I am uploading my jupyter notebook code to sagemaker. In the code, vertica python connection failed. I got the error message:
Failed to establish a connection to the primary server or any backup address.</p>
<p>The code looks like this:</p>
<pre><code># load data
conn_info = {'host': xxxx,
             'port': xxxx,
             'user': xxxx,
             'password': xxxx,
             'database': xxxx,
             'schema': xxxx}

connection = vertica_python.connect(**conn_info)

cur = connection.cursor()
</code></pre>",0,3,2021-04-26 19:01:50.407000 UTC,,2021-04-27 04:52:53.667000 UTC,0,amazon-web-services|vpn|amazon-sagemaker|vertica,535,2021-04-26 18:56:32.777000 UTC,2022-09-21 16:21:35.003000 UTC,,1,0,0,1,,,,,,['amazon-sagemaker']
Running scripts from a datastore on Azure Machine Learning Service,"<p>I am migrating from Batch AI to the new Azure Machine Learning Service. Previously I had my python scripts on an Azure Files share and those scripts ran directly from there.</p>

<p>In the new service when you create an Estimator you have to provide a source directory and an entry script. The documentation states the source directory is a local directory that is copied to the remote computer.</p>

<p>However, the Estimator constructor also allows you to specify a datastore name that is supposed to specify the datastore for the project share.</p>

<p>To me, this sounds like you can specify a datastore and then the source directory is relative to that however this does not work, it still wants to find the source directory on the local machine.</p>

<pre><code>tf_est = TensorFlow(source_directory='./script',
                source_directory_data_store=ds,
                script_params=script_params,
                compute_target=compute_target,
                entry_script='helloworld.py',
                use_gpu=False)
</code></pre>

<p>Does anybody know if its possible to run a training job using a datastore for execution?</p>",0,1,2019-01-30 00:03:26.603000 UTC,,2019-04-09 12:35:31.197000 UTC,2,azure|machine-learning|azure-machine-learning-service,141,2012-05-03 00:32:17.537000 UTC,2022-09-22 19:58:47.533000 UTC,,752,0,4,117,,,,,,['azure-machine-learning-service']
AWS SageMaker Notebook instance cannot connect to internet,"<p>Trying to understand why my SageMaker notebook instance cannot connect to the internet.</p>

<pre><code># jupyter notebook running conda_python3 kernel

from sagemaker import get_execution_role
role = get_execution_role()
print(role)

'Could not connect to the endpoint URL: ""https://api.sagemaker.us-east-1.amazonaws.com/""'
</code></pre>

<p><br></p>

<pre><code># terminal

wget tools.geekflare.com

'unable to resolve host address ""tools.geekflare.com""'
</code></pre>

<p>Any tips on how to debug the issue?</p>",1,0,2019-12-11 15:38:27.953000 UTC,,,0,amazon-sagemaker,2613,2015-03-13 19:05:58.767000 UTC,2022-09-23 15:20:14.637000 UTC,,82,3,0,17,,,,,,['amazon-sagemaker']
Vertex ai custom model training for pyspark ml model,<p>Is it possible to train a spark/pyspark ML lib model using VertexAI custom container model building? I couldn't find any reference in the vertex ai documents regarding spark model training. For distributed processing model building only options available are PyTorch or TensorFlow.</p>,1,0,2021-09-03 06:00:46.963000 UTC,,,0,apache-spark|pyspark|apache-spark-mllib|machine-learning-model|google-cloud-vertex-ai,432,2021-09-03 05:52:21.403000 UTC,2022-09-23 09:40:44.083000 UTC,,1,0,0,6,,,,,,['google-cloud-vertex-ai']
Readding missing files to DVC,"<p>A ran into problem with DVC when some files are missing in remote. For example when I execute <code>dvc pull</code> I get the output</p>

<pre><code>[##############################] 100% Analysing status.
WARNING: Cache 'c31bcdd6910977a0e3a86446f2f3bdaa' not found. File 'data/2.mp4' won't be created.
WARNING: Cache '77186c4596da7dbc85fefec6d0779049' not found. File 'data/3.mp4' won't be created.
</code></pre>

<p>The <code>dvc status</code> command gives me:</p>

<pre><code>data/2.mp4.dvc:
    changed outs:
        not in cache:       data/2.mp4
data/3.mp4.dvc:
    changed outs:
        not in cache:       data/3.mp4
</code></pre>

<p>It seems that <code>2.mp4</code> and <code>3.mp4</code> where added under dvc control but <code>dvc push</code> command has not been executed.</p>

<p>I have access to the original mp4 files and I have tried to readd them. I copied mp4 files to data folder and executed the command:</p>

<pre><code>dvc remove data/2.mp4.dvc
dvc remove data/3.mp4.dvc

dvc add data/2.mp4 
dvc add data/3.mp4 
</code></pre>

<p>But there is no effect. How can I remove files from under dvc control and add them again?</p>",1,0,2019-05-23 06:41:59.503000 UTC,,2019-05-23 18:46:43.487000 UTC,4,dvc,1065,2018-03-28 16:31:38.710000 UTC,2022-09-23 10:08:33.687000 UTC,Russia,784,32,0,77,,,,,,['dvc']
Azure ML workspace: how to publish pipeline to existing endpoint instead of creating new,"<p>I'm working on deploying an inference pipeline in Azure machine learning workspace.</p>
<p>I have created a pipeline using a couple of <code>PythonScriptStep</code>s and want to automate the pipeline publishing using CI/CD.</p>
<p>Reference: <a href=""https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-pipelines#publish-a-pipeline"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-pipelines#publish-a-pipeline</a></p>
<pre class=""lang-py prettyprint-override""><code>pipeline = Pipeline(workspace=workspace, steps=[step1, step2])
pipeline_endpoint = pipeline.publish(name='deployment-test', version=1)
</code></pre>
<p>Every time I publish, it is creating new endpoints but I want to deploy to existing so that nothing has to be changed in the consumer end.</p>",2,1,2021-01-29 14:01:15.903000 UTC,0.0,2021-05-18 12:33:40.827000 UTC,4,azure|azure-pipelines|azure-machine-learning-studio|azure-machine-learning-service|azureml-python-sdk,1433,2021-01-29 13:48:57.687000 UTC,2021-06-07 13:15:18.357000 UTC,,41,0,0,4,,,,,,"['azure-machine-learning-service', 'azure-machine-learning-studio']"
Multiple outputs from custom module,"<p><a href=""https://docs.microsoft.com/de-de/azure/machine-learning/studio/custom-r-modules"" rel=""nofollow noreferrer"">This Microsoft Azure documentation</a> shows how to author custom modules for the Azure Machine Learning Studio. There is a paragraph about returning multiple outputs from your module. Yet following the instructions I can only see data in the visualization of the first output port while the others remain empty.</p>

<p>This is a follow-up question to <a href=""https://stackoverflow.com/questions/46340959/multiple-inputs-outputs-from-execute-r-script"">this one</a>. I accepted the answer there because I misinterpreted the result of the custom module I wrote - it is possible for some output ports to be empty and I hastily assumed the output to be correct. However, running the same code in RStudio does indeed generate data that should have been returned in ML Studio as well. Also, printing the data works.  </p>

<p><strong>Minimal example:</strong></p>

<p>The source files contained in the module's ZIP file:</p>

<p>test.R</p>

<pre><code>foo &lt;- function() {
    require(data.table)
    out1 &lt;- data.table(mtcars)
    out2 &lt;- data.table(cars)

    print(""out1:"")
    print(head(out1))
    print(""out2:"")
    print(head(out2))

    return(list(out1, out2))
}
</code></pre>

<p>test.xml</p>

<pre><code>&lt;Module name=""Multiple outputs""&gt;
  &lt;Owner&gt;...&lt;/Owner&gt;
  &lt;Language name=""R"" sourceFile=""test.R"" entryPoint=""foo""/&gt; 
    &lt;Ports&gt;
      &lt;Output id=""out_1"" name=""out1"" type=""DataTable""&gt;
        &lt;Description&gt;...&lt;/Description&gt;
      &lt;/Output&gt;
      &lt;Output id=""out_2"" name=""out2"" type=""DataTable""&gt;
        &lt;Description&gt;...&lt;/Description&gt;
      &lt;/Output&gt;   
    &lt;/Ports&gt;
&lt;/Module&gt;
</code></pre>

<p>Which yields this module that runs successfully:</p>

<p><a href=""https://i.stack.imgur.com/tyKFa.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tyKFa.png"" alt=""Module in Azure""></a></p>

<p>The visualizations of the output however look like this:
<a href=""https://i.stack.imgur.com/QSJ98.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QSJ98.png"" alt=""Correct visualization of output 1""></a>
<a href=""https://i.stack.imgur.com/wie4o.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wie4o.png"" alt=""Empty visualization of output 2""></a></p>

<p>Whereas the output log looks good:</p>

<pre><code>[ModuleOutput] [1] ""out1:""
[ModuleOutput] 
[ModuleOutput]     mpg cyl disp  hp drat    wt  qsec vs am gear carb
[ModuleOutput] 
[ModuleOutput] 1: 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4
[ModuleOutput] 
[ModuleOutput] 2: 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4
[ModuleOutput] 
[ModuleOutput] 3: 22.8   4  108  93 3.85 2.320 18.61  1  1    4    1
[ModuleOutput] 
[ModuleOutput] 4: 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1
[ModuleOutput] 
[ModuleOutput] 5: 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2
[ModuleOutput] 
[ModuleOutput] 6: 18.1   6  225 105 2.76 3.460 20.22  1  0    3    1
[ModuleOutput] 
[ModuleOutput] [1] ""out2:""
[ModuleOutput] 
[ModuleOutput]    speed dist
[ModuleOutput] 
[ModuleOutput] 1:     4    2
[ModuleOutput] 
[ModuleOutput] 2:     4   10
[ModuleOutput] 
[ModuleOutput] 3:     7    4
[ModuleOutput] 
[ModuleOutput] 4:     7   22
[ModuleOutput] 
[ModuleOutput] 5:     8   16
[ModuleOutput] 
[ModuleOutput] 6:     9   10
</code></pre>

<p>I think I followed the instructions from the documentation correctly.
Has someone encountered this problem before? Are there any known solutions?</p>

<p>Any help would be much appreciated!</p>",1,0,2017-09-26 09:08:08.300000 UTC,,,1,r|azure|azure-machine-learning-studio,256,2014-01-15 14:12:16.837000 UTC,2022-09-21 12:01:29.240000 UTC,,625,88,3,36,,,,,,['azure-machine-learning-studio']
How to run docker image prepared locally in Sagemaker?,"<p>I have a train.py and using a docker pushed an image from local to AWS ECR.
But i am getting this error</p>
<pre><code>       The primary container for production variant variant-1 did not pass the ping 
       health check. Please check CloudWatch logs for this endpoint.
</code></pre>
<p>Here is the complete <strong>Docker file.</strong> What am I missing.</p>
<pre><code>FROM python:3.7
RUN python -m pip install sagemaker-training snowflake-connector-python[pandas] \
pandas scikit-learn boto3 numpy joblib sagemaker flask gevent gunicorn

ENV PYTHONUNBUFFERED=TRUE
ENV PYTHONDONTWRITEBYTECODE=TRUE
ENV PATH=&quot;/opt/ml:${PATH}&quot;


# Set up the program in the image
COPY pred_demo_sm/train.py /opt/ml/code/train.py
COPY pred_demo_sm/serve /opt/ml/code/serve
COPY pred_demo_sm/output /opt/ml/output
COPY pred_demo_sm/model /opt/ml/model

WORKDIR /opt/ml

ENTRYPOINT [ &quot;python3.7&quot;, &quot;/opt/ml/code/train.py&quot;]
</code></pre>
<p>Here is the complete <strong>.sh file</strong> which builds and pushes the image to AWS ECR</p>
<pre><code>#!/usr/bin/env bash

# This script shows how to build the Docker image and push it to ECR to be ready for use
# by SageMaker.

# The argument to this script is the image name. This will be used as the image on the local
# machine and combined with the account and region to form the repository name for ECR.
image=$1

if [ &quot;$image&quot; == &quot;&quot; ]
then
    echo &quot;Usage: $0 &lt;image-name&gt;&quot;
    exit 1
fi

chmod +x pred_demo_sm/train.py
chmod +x pred_demo_sm/serve
chmod +x pred_demo_sm/model/*

# Get the account number associated with the current IAM credentials
account=$(aws sts get-caller-identity --query Account --output text)

if [ $? -ne 0 ]
then
    exit 255
fi


# Get the region defined in the current configuration (default to us-west-2 if none defined)
region=$(aws configure get region)
region=${region:-us-west-2}


fullname=&quot;${account}.dkr.ecr.${region}.amazonaws.com/${image}:latest&quot;

# If the repository doesn't exist in ECR, create it.

aws ecr describe-repositories --repository-names &quot;${image}&quot; &gt; /dev/null 2&gt;&amp;1

if [ $? -ne 0 ]
then
    aws ecr create-repository --repository-name &quot;${image}&quot; &gt; /dev/null
fi

# Get the login command from ECR and execute it directly
aws ecr get-login-password --region &quot;${region}&quot; | docker login --username AWS --password-stdin &quot;${account}&quot;.dkr.ecr.&quot;${region}&quot;.amazonaws.com

# Build the docker image locally with the image name and then push it to ECR
# with the full name.

docker build  -t ${image}
docker tag ${image} ${fullname}

docker push ${fullname}
</code></pre>
<p>The training job gets successfully completed in Sagemaker.
But fails while deploying the model in sagemaker.</p>",1,1,2022-01-14 08:30:46.983000 UTC,,2022-02-11 14:52:34.930000 UTC,0,docker|amazon-sagemaker,335,2022-01-14 08:07:35.003000 UTC,2022-05-10 03:57:24.183000 UTC,,1,0,0,1,,,,,,['amazon-sagemaker']
Log (and then apply) Spark MLlib model from R to MLflow,"<p>I'm using Spark MLlib functions (through the <em>sparklyr</em> package) to train a model but now seem unable to save the model in <em>MLflow</em> for future use.</p>
<pre><code>iris_tbl &lt;- sparklyr::copy_to(sc, iris, &quot;iris_spark&quot;)
mdl_mllib &lt;- iris_tbl %&gt;% sparklyr::ml_linear_regression(formula = Sepal_Width ~ Sepal_Length)
mlflow::mlflow_log_model(mdl_mllib, &quot;artifact_path_where_saved&quot;)
Error in UseMethod(&quot;mlflow_save_model&quot;) : 
  no applicable method for 'mlflow_save_model' applied to an object of class &quot;c('ml_model_linear_regression', 'ml_model_regression', 'ml_model_prediction', 'ml_model')&quot;

packageVersion(&quot;mlflow&quot;)
[1] ‘1.17.0’
</code></pre>
<p>What is a simple way to save this model in <em>mlflow</em> for later use <strong>on a Spark DataFrame</strong> such as:</p>
<p><code>mlflow::mlflow_load_model(model_uri = &quot;models:/mdl_mllib_project01/Staging&quot;)</code></p>
<p>For context, I'm using Azure Databricks as the ecosystem.</p>
<h3>Other places I've looked for answers</h3>
<ul>
<li>These links don't seem to directly solve my problem (<a href=""https://www.mlflow.org/docs/latest/models.html#id28;"" rel=""nofollow noreferrer"">https://www.mlflow.org/docs/latest/models.html#id28;</a> <a href=""https://www.mlflow.org/docs/latest/python_api/mlflow.spark.html#mlflow.spark.log_model"" rel=""nofollow noreferrer"">https://www.mlflow.org/docs/latest/python_api/mlflow.spark.html#mlflow.spark.log_model</a>), although maybe it's that I need to make a &quot;pipeline&quot; or that this works for Python but not yet for R?</li>
<li>This question seems relevant but offers abstract answers rather than details (<a href=""https://stackoverflow.com/questions/40533582/how-to-serve-a-spark-mllib-model"">How to serve a Spark MLlib model?</a>).</li>
</ul>",0,3,2021-08-02 09:20:06.663000 UTC,,,2,apache-spark-mllib|sparklyr|mlflow,107,2021-04-21 16:43:43.857000 UTC,2022-09-20 15:49:55.297000 UTC,,33,0,0,2,,,,,,['mlflow']
Why can't I access Output from Vertex pipeline kfp component?,"<p>In a Vertex AI pipeline (google_cloud_pipeline_components version: 1.0.19
kfp version: 1.8.13), I try:</p>
<pre><code>does_endpoint_exist_op = does_endpoint_exist(project=project, 
    location=location, endpoint_name_in=endpoint_name) 
endpoint_name=does_endpoint_exist_op.outputs['endpoint_name']
</code></pre>
<p>but this gives:</p>
<pre><code>AssertionError: component_input_parameter: pipelineparam--does-endpoint-exist-endpoint_name not found.
</code></pre>
<p>The component is defined:</p>
<pre><code>@component(
    packages_to_install=[&quot;google-cloud-aiplatform&quot;, 
                         &quot;google-cloud-pipeline-components==1.0.19&quot;],
    output_component_file=&quot;does_endpoint_exist_component.yaml&quot;,
)

def does_endpoint_exist(project: str, 
                    location: str,
                    endpoint_name_in: str, endpoint: Output[Artifact], 
                    endpoint_name: Output[Artifact]) -&gt; str:
</code></pre>
<p>I can do:</p>
<pre><code>endpoint_name=does_endpoint_exist_op.outputs['Output'] 
</code></pre>
<p>OK, so why can't I access <code>endpoint_name</code>?</p>",0,0,2022-09-14 11:05:16.447000 UTC,,,0,google-cloud-platform|google-cloud-vertex-ai|kfp,17,2012-10-25 08:48:34.717000 UTC,2022-09-23 10:10:32.783000 UTC,,2564,304,8,451,,,,,,['google-cloud-vertex-ai']
ClientError: An error occurred (ValidationException) when calling the CreateHyperParameterTuningJob operation:The objective metric :[mse],"<p>Getting error: <strong>ClientError: An error occurred (ValidationException) when calling the CreateHyperParameterTuningJob operation: The objective metric for the hyperparameter tuning job, [mse], isn’t valid for the [720646828776.dkr.ecr.ap-south-1.amazonaws.com/sagemaker-xgboost:0.90-2-cpu-py3] algorithm. Choose a valid objective metric.</strong></p>
<pre><code>import datetime
import time
import tarfile    
import boto3
import pandas as pd
import numpy as np
from sagemaker import get_execution_role
import sagemaker
from sklearn.model_selection import train_test_split
from sklearn.datasets import fetch_california_housing
from sagemaker.tuner import (
    IntegerParameter,
    CategoricalParameter,
    ContinuousParameter,
    HyperparameterTuner,
)

s3 = boto3.client(&quot;s3&quot;)
sm_boto3 = boto3.client(&quot;sagemaker&quot;)

sagemaker_session = sagemaker.Session()

region = sess.boto_session.region_name

role = get_execution_role()
#Set the required configurations
model_name = &quot;abc_model&quot;
env = &quot;dev&quot;
#S3 Bucket
bucket = &quot;abcpoc&quot;
print(&quot;Using bucket &quot; + bucket)


from sagemaker.debugger import Rule, rule_configs
from sagemaker.session import TrainingInput

s3_input_train = TrainingInput(
    s3_data=f&quot;s3://{default_bucket}/train/&quot;,content_type=&quot;csv&quot;)
s3_input_validation = TrainingInput(
    s3_data=f&quot;s3://{default_bucket}/validation/&quot;,content_type=&quot;csv&quot;)
prefix = 'output'

container=sagemaker.image_uris.retrieve(&quot;xgboost&quot;, region, &quot;1.2-1&quot;)
print(container)
xgb = sagemaker.estimator.Estimator(
    image_uri=container,
    role=role,
    base_job_name=&quot;xgboost-random-search&quot;,
    instance_count=1,
    instance_type=&quot;ml.m4.xlarge&quot;,
    output_path=&quot;s3://{}/{}/output&quot;.format(bucket, prefix),
    sagemaker_session= sagemaker.Session(),
    rules=[Rule.sagemaker(rule_configs.create_xgboost_report())]
)


xgb.set_hyperparameters(
    max_depth = 5,
    eta = 0.2,
    gamma = 4,
    min_child_weight = 6,
    subsample = 0.7,
    objective = &quot;reg:squarederror&quot;,
    num_round = 1000
)
hyperparameter_ranges = {
    &quot;eta&quot;: ContinuousParameter(0, 1),
    &quot;min_child_weight&quot;: ContinuousParameter(1, 10),
    &quot;alpha&quot;: ContinuousParameter(0, 2),
    &quot;max_depth&quot;: IntegerParameter(1, 10),
}

objective_metric_name = &quot;mse&quot;
metric_definitions = [{&quot;Name&quot;: &quot;mse&quot;, &quot;Regex&quot;: &quot;mse: ([0-9\\.]+)&quot;}]

tuner = HyperparameterTuner(estimator, 
                    objective_metric_name, 
                    hyperparameter_ranges, 
                    metric_definitions=None, 
                    strategy='Bayesian', 
                    objective_type='Maximize', 
                    max_jobs=1, 
                    max_parallel_jobs=1, 
                    tags=None, 
                    base_tuning_job_name=None)

#Tune
tuner.fit({
    &quot;train&quot;:s3_input_train,
    &quot;validation&quot;:s3_input_validation
    },include_cls_metadata=False)

#Explore the best model generated
tuning_job_result = boto3.client(&quot;sagemaker&quot;).describe_hyper_parameter_tuning_job(
    HyperParameterTuningJobName=tuner.latest_tuning_job.job_name
)

job_count = tuning_job_result[&quot;TrainingJobStatusCounters&quot;][&quot;Completed&quot;]
print(&quot;%d training jobs have completed&quot; %job_count)
#10 training jobs have completed

#Get the best training job

from pprint import pprint
if tuning_job_result.get(&quot;BestTrainingJob&quot;,None):
    print(&quot;Best Model found so far:&quot;)
    pprint(tuning_job_result[&quot;BestTrainingJob&quot;])
else:
    print(&quot;No training jobs have reported results yet.&quot;)
</code></pre>",1,0,2022-09-16 04:34:17.860000 UTC,,,0,amazon-web-services|amazon-s3|amazon-sagemaker,20,2014-09-26 13:42:23.827000 UTC,2022-09-23 08:45:56.160000 UTC,,21,0,0,14,,,,,,['amazon-sagemaker']
AzureML data sharing between pipelines,"<p>Moving my first steps in AML...</p>
<p>I am trying to create several pipelines, the idea s that some of the data generated by one pipeline will eventually be used by other pipelines. The way that I am going this is as follows:</p>
<ul>
<li>In the first pipeline, I am registering the data that I want to use later on as datasets by</li>
</ul>
<pre><code>dir = OutputFileDatasetConfig(&lt;&lt;name&gt;&gt;).read_delimited_files().register_on_complete(&lt;&lt;ds_name&gt;&gt;)
</code></pre>
<ul>
<li>I am saving data normally (data is numpy arrays)</li>
</ul>
<pre><code>np.savetxt(os.path.join(&lt;&lt;dir&gt;&gt;, &lt;&lt;file&gt;&gt;), X_test, delimiter=&quot;,&quot;)
</code></pre>
<ul>
<li>In the second pipeline I am reading the location of the data</li>
</ul>
<pre><code>dir = Run.get_context().input_datasets[&lt;&lt;ds_name&gt;&gt;].download()
</code></pre>
<p>and then loading it in numpy</p>
<pre><code>a = np.loadtxt(dir[0])
</code></pre>
<p>Not sure if there are better ways to achieve this, and ideas, please?</p>",0,0,2022-04-26 18:14:01.550000 UTC,,,0,azure-machine-learning-service,17,2016-05-25 17:54:32.787000 UTC,2022-08-01 18:26:20.177000 UTC,,11,4,0,10,,,,,,['azure-machine-learning-service']
How do I specify encryption type when using s3remote for DVC,"<p>I have just started to explore DVC. I am trying with s3 as my DVC remote. I am getting </p>

<p>But when I run the <code>dvc push</code> command, I get the generic error saying </p>

<pre><code>An error occurred (AccessDenied) when calling the PutObject operation: Access Denied
</code></pre>

<p>which I know for a fact that I get that error when I don't specify the encryption.</p>

<p>It is similar to running <code>aws s3 cp</code> with <code>--sse</code> flag or specifying <code>ServerSideEncryption</code> when using boto3 library. How can I specify the encryption type when using DVC. Coz underneath DVC uses boto3 so there must be an easy way to do this.</p>",1,0,2020-03-26 05:45:08.167000 UTC,,,1,dvc,293,2017-04-11 13:31:59.307000 UTC,2022-03-29 20:52:48.753000 UTC,,1756,82,5,199,,,,,,['dvc']
Is there a way to export or view a classifier created in sagemaker so that we can see what weights/constants are used in model evaluation,"<p>I created a simple linear learner model using sagemaker, and although I can deploy it on a test data set, I would like to be able to get the actual equation that the model uses to classify values (ie for linear regression the equation of the line).</p>",1,0,2019-07-03 19:43:35.333000 UTC,,,4,python|amazon-web-services|amazon-sagemaker,160,2015-10-31 23:05:25.740000 UTC,2020-12-27 15:50:38.097000 UTC,Pristina,129,5,0,1,,,,,,['amazon-sagemaker']
Azure ML MSI deployment over ARM Templates enables purge protection on Key Vault,"<p>I have discovered lately that when you deploy an <code>Azure ML</code> instance from the <code>ARM Template</code>, the <code>MSI</code> will override the purge protection settings of the <code>Key Vault</code>. It will enable purge protection on the <code>Key Vault</code>. This is not the behavior that I am looking for, because when trying to deploy it again, the template will fail saying that the <code>Key Vault</code> with the name already exists and you can't deleted before the retention period.</p>
<p>If you deploy the <code>Azure ML</code> instance manually and select the Key Vault, it will keep the disable purge settings. Any ideas how can we keep purge disabled hier?</p>
<p>The Azure ML properties that we used are mentioned bellow:</p>
<pre><code>  {
    &quot;type&quot;: &quot;Microsoft.MachineLearningServices/workspaces&quot;,
    &quot;apiVersion&quot;: &quot;2020-09-01-preview&quot;,
    &quot;name&quot;: &quot;[variables('machineLearningWorkspaceName')]&quot;,
    &quot;location&quot;: &quot;[parameters('location')]&quot;,
    &quot;identity&quot;: {
      &quot;type&quot;: &quot;[parameters('amlManagedIdentityOption')]&quot;
    },
    &quot;dependsOn&quot;: [
      &quot;[resourceId('Microsoft.Storage/storageAccounts', variables('storageAccountName'))]&quot;,
      &quot;[resourceId('Microsoft.Insights/components', variables('applicationInsightsName'))]&quot;,
      &quot;[resourceId('Microsoft.ContainerRegistry/registries', variables('containerRegistryName'))]&quot;
    ],
    &quot;tags&quot;: &quot;[parameters('resourceTags')]&quot;,
    &quot;properties&quot;: {
      &quot;friendlyName&quot;: &quot;[variables('machineLearningWorkspaceName')]&quot;,
      &quot;storageAccount&quot;: &quot;[variables('storageAccount')]&quot;,
      &quot;keyVault&quot;: &quot;[variables('keyVault')]&quot;,
      &quot;applicationInsights&quot;: &quot;[variables('applicationInsights')]&quot;,
      &quot;containerRegistry&quot;: &quot;[ variables('containerRegistry')]&quot;,
      &quot;adbWorkspace&quot;: &quot;[variables('adbWorkSpace')]&quot;,
      &quot;hbiWorkspace&quot;: &quot;[parameters('confidential_data')]&quot;,
      &quot;allowPublicAccessWhenBehindVnet&quot;: &quot;[parameters('allowPublicAccessWhenBehindVnet')]&quot;
    }
  }
</code></pre>
<p>On the Key Vault ARM we have the following properties:</p>
<pre><code>         &quot;properties&quot;: {
                 &quot;enabledForDeployment&quot;: &quot;[parameters('enabledForDeployment')]&quot;,
                 &quot;enabledForTemplateDeployment&quot;: &quot;[parameters('enabledForTemplateDeployment')]&quot;,
                 &quot;enabledForVolumeEncryption&quot;: &quot;[parameters('enableVaultForVolumeEncryption')]&quot;,
                 &quot;softDeleteRetentionInDays&quot;: 7,
                 &quot;tenantId&quot;: &quot;[subscription().tenantId]&quot;,
                 &quot;copy&quot;: [
                     {
                         &quot;name&quot;: &quot;accessPolicies&quot;,
                         &quot;count&quot;: &quot;[length(parameters('userObjectId'))]&quot;,
                         &quot;input&quot;: {
                             &quot;tenantId&quot;: &quot;[subscription().tenantId]&quot;,
                             &quot;objectId&quot;: &quot;[parameters('userObjectId')[copyIndex('accessPolicies')].Id]&quot;,
                             &quot;permissions&quot;: &quot;[parameters('userObjectId')[copyIndex('accessPolicies')].Permissions]&quot;
                         }
 }
</code></pre>
<p><a href=""https://i.stack.imgur.com/Qywye.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Qywye.png"" alt=""enter image description here"" /></a></p>",2,3,2021-07-28 10:13:53.883000 UTC,1.0,2021-07-28 14:37:37.640000 UTC,0,azure|azure-devops|azure-resource-manager|azure-machine-learning-studio|infrastructure-as-code,444,2014-07-27 10:46:49.257000 UTC,2022-09-23 08:53:07.093000 UTC,"Stuttgart, Deutschland",599,204,5,224,,,,,,['azure-machine-learning-studio']
AWS Studio ModuleNotFoundError: No module named 'sagemaker',"<p>I am trying to replicate the below example for churn prediction.
<a href=""https://towardsdatascience.com/a-practical-guide-to-mlops-in-aws-sagemaker-part-i-1d28003f565"" rel=""nofollow noreferrer"">https://towardsdatascience.com/a-practical-guide-to-mlops-in-aws-sagemaker-part-i-1d28003f565</a></p>
<p>Preprocessing.py has to import sagemaker but it's throwing ModuleNotFoundError as I run the pipeline. Same sagemaker package is also imported in pipeline.py but it works fine there. Please let me know how we can install packages in studio environment with the syntax. I tried with pip and conda install in a cell in another ipynb file.. Requirement already satisfied message is only displayed when it gets installed.</p>",1,0,2022-06-11 11:24:02.777000 UTC,,,0,python|amazon-web-services|amazon-sagemaker,333,2022-06-11 10:51:04.630000 UTC,2022-09-23 06:37:51.990000 UTC,,1,0,0,0,,,,,,['amazon-sagemaker']
Sagemaker Model is trying to find an old docker image,"<p>I'm stuck in a situation where Sagemaker is looking for a docker image in ECS registry which I had to remove and I can't figure out how to make it forget about that.</p>
<p>I had to rebuild a docker with torchserve for Sagemaker. I removed the old one (let's call it <code>torchserve-old-name</code>, and uploaded a new <code>torchserve:v2</code>. For some weird reason SM is still looking for the old one.</p>
<pre><code>from sagemaker.model import Model
from sagemaker.predictor import Predictor

image = &quot;134244564256.dkr.ecr.us-west-2.amazonaws.com/torchserve:v2&quot;
model_data = &quot;s3://my-models/torchserve/my-model.tar.gz&quot;
sm_model_name = 'my-model'

print(f&quot;docker image: {image}&quot;)
# docker image: 134244564256.dkr.ecr.us-west-2.amazonaws.com/torchserve:v2

torchserve_model = Model(model_data = model_data, 
                         image_uri = image,
                         role  = role,
                         predictor_cls=Predictor,
                         name  = sm_model_name)
endpoint_name = 'torchserve-endpoint-' + time.strftime(&quot;%Y-%m-%d-%H-%M-%S&quot;, time.gmtime())

predictor = torchserve_model.deploy(instance_type='ml.m4.xlarge',
                                    initial_instance_count=1,
                                    endpoint_name = endpoint_name)
</code></pre>
<p>Results in:</p>
<pre><code>---------------------------------------------------------------------------
UnexpectedStatusException                 Traceback (most recent call last)
&lt;ipython-input-12-67fff0e32d2d&gt; in &lt;module&gt;
      3 predictor = torchserve_model.deploy(instance_type='ml.m4.xlarge',
      4                                     initial_instance_count=1,
----&gt; 5                                     endpoint_name = endpoint_name)

~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/model.py in deploy(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, tags, kms_key, wait, data_capture_config, **kwargs)
    762             kms_key=kms_key,
    763             wait=wait,
--&gt; 764             data_capture_config_dict=data_capture_config_dict,
    765         )
    766 

~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/session.py in endpoint_from_production_variants(self, name, production_variants, tags, kms_key, wait, data_capture_config_dict)
   3454 
   3455             self.sagemaker_client.create_endpoint_config(**config_options)
-&gt; 3456         return self.create_endpoint(endpoint_name=name, config_name=name, tags=tags, wait=wait)
   3457 
   3458     def expand_role(self, role):

~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/session.py in create_endpoint(self, endpoint_name, config_name, tags, wait)
   2957         )
   2958         if wait:
-&gt; 2959             self.wait_for_endpoint(endpoint_name)
   2960         return endpoint_name
   2961 

~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/session.py in wait_for_endpoint(self, endpoint, poll)
   3243                 ),
   3244                 allowed_statuses=[&quot;InService&quot;],
-&gt; 3245                 actual_status=status,
   3246             )
   3247         return desc
 UnexpectedStatusException: Error hosting endpoint torchserve-endpoint-2021-02-06-21-39-31: Failed. Reason: 
 The repository 'torchserve-old-name' does not exist in the registry with id '134244564256'..
</code></pre>
<p>I am 100% sure that the old image 'torchserve-old-name' has not been mentioned in this code. I've restarted and re-ran notebook.</p>
<p>Where is it cached, and how can I clear that cache? Sagemaker function documentation doesn't seem to mention it.</p>",0,1,2021-02-07 04:47:16.417000 UTC,,2021-02-14 12:04:31.143000 UTC,3,docker|amazon-sagemaker,88,2012-10-03 10:06:31.847000 UTC,2022-09-25 03:37:17.877000 UTC,"Zurich, Switzerland",12050,2598,4,488,,,,,,['amazon-sagemaker']
OSError: [Errno 30] Read-only file system: '/app' when executing mlflow.pyfunc.log_model,"<p>I am getting below error when tried to execute this code:</p>
<pre><code>import mlflow
import os

#removed below params due to confidentiality

os.environ['MLFLOW_S3_ENDPOINT_URL'] = &quot;&quot;
os.environ['AWS_ACCESS_KEY_ID'] = &quot;&quot;
os.environ['AWS_SECRET_ACCESS_KEY'] = &quot;&quot;
mlflow.set_tracking_uri(&quot;&quot;)
mlflow.set_registry_uri(&quot;&quot;)

class AwesomeModel(mlflow.pyfunc.PythonModel):
    def load_context(self, context):
        pass
    def predict(self,context,inp_df):
        return 5

with mlflow.start_run() as run:
    mlflow.pyfunc.log_model(
                        python_model=AwesomeModel(),
                        artifact_path=&quot;ml-storage&quot;,
                        artifacts=None,
                        registered_model_name=&quot;ml_serving_demo_model&quot;)
</code></pre>
<p>ERROR:
OSError: [Errno 30] Read-only file system: '/app'</p>
<p>Python version: 3.8
Mlflow version: 1.12.1</p>",1,0,2022-04-04 11:07:06.123000 UTC,,,1,mlflow,241,2015-09-12 05:35:46.580000 UTC,2022-09-23 11:10:59.307000 UTC,India,109,25,0,13,,,,,,['mlflow']
Is it possible to load the Dataset to Microsoft Azure Machine Learning Studio programmatically?,<p>I'm working in a <code>.NET</code> project where I will generate a dataset. I need to load that dataset into Azure Machine Learning Studio. Is there a way to load that dataset into ML studio programmatically (perhaps with an <code>apikey</code> and <code>RequestURI</code>) instead of manually loading dataset in the Azure ML Studio?</p>,2,2,2020-03-11 09:07:17.900000 UTC,,2020-03-14 04:34:36.733000 UTC,1,.net|azure|azure-machine-learning-studio|azure-machine-learning-service,1772,2018-06-27 07:29:42.977000 UTC,2022-02-24 13:31:33.520000 UTC,"Chennai, Tamil Nadu, India",139,14,0,37,,,,,,"['azure-machine-learning-studio', 'azure-machine-learning-service']"
python ray tune unable to stop trial or experiment,"<p>I am trying to make ray tune with wandb stop the experiment under certain conditions.</p>
<ul>
<li>stop all experiment if any trial raises an Exception (so i can fix the code and resume)</li>
<li>stop if my score gets -999</li>
<li>stop if the variable <code>varcannotbezero</code> gets 0</li>
</ul>
<p><strong>The following things i tried all failed in achieving desired behavior:</strong></p>
<ul>
<li>stop={&quot;score&quot;:-999,&quot;varcannotbezero&quot;:0}</li>
<li>max_failures=0</li>
<li>defining a Stoper class did also not work</li>
</ul>
<pre><code>class RayStopper(Stopper):
    def __init__(self):
        self._start = time.time()
        #self._deadline = 300
    def __call__(self, trial_id, result):
        self.score=result[&quot;score&quot;]
        self.varcannotbezero=result[&quot;varcannotbezero&quot;]
        return False
    def stop_all(self):
        if self.score==-999 or self.varcannotbezero==0:
            return True
        else:
            return False
</code></pre>
<p>Ray tune just continues to run</p>
<pre><code>    wandb_project=&quot;ABC&quot;
    wandb_api_key=&quot;KEY&quot;
    ray.init(configure_logging=False)

    if current_best_params is None:
        algo = HyperOptSearch()
    else:
        algo = HyperOptSearch(points_to_evaluate=current_best_params,n_initial_points=n_initial_points)
    algo = ConcurrencyLimiter(algo, max_concurrent=1)

    scheduler = AsyncHyperBandScheduler()
    analysis = tune.run(
        tune_obj,
        name=&quot;Name&quot;,
        resources_per_trial={&quot;cpu&quot;: 1},
        search_alg=algo,
        scheduler=scheduler,
        metric=&quot;score&quot;,
        mode=&quot;max&quot;,
        num_samples=10,
        stop={&quot;score&quot;:-999,&quot;varcannotbezero&quot;:0},
        max_failures=0,
        config=config,
        callbacks=[WandbLoggerCallback(project=wandb_project,entity=&quot;mycompany&quot;,api_key=wandb_api_key,log_config=True)],
        local_dir=local_dir,
        resume=&quot;AUTO&quot;,
        verbose=0
    )

</code></pre>",1,2,2022-02-15 23:28:05.327000 UTC,,,0,optimization|hyperparameters|ray|ray-tune|wandb,284,2011-03-21 21:12:39.900000 UTC,2022-09-24 13:50:59.750000 UTC,,2390,56,1,308,,,,,,['wandb']
Deploy model from local jupyter notebook to amazon sagemaker,"<p>Is it necessary to create the notebook in amazon sagemaker and then bring the model file and deploy, <strong>if i have amazon sagamaker access, can i deploy my model from my local jupyter notebook into amazon sagemaker directly?</strong></p>
<p>Or is it compulsory to use amazon sagemaker jupyter notebooks for the deplopyment.</p>
<p><strong>I tried deploying the model from local jupyter notebook, predictor works</strong></p>
<ol>
<li>predictor = model.deploy()</li>
<li>predictor.predict(test_data) works and give predictions</li>
</ol>
<p>but When i am listing the objects in the bucket from local, no directory for that deployment is created in aws,</p>
<p>but when am doing that in aws sagemaker notebook instance it shows the model being deployed and directory got created.</p>
<p>Kindly help please..</p>",1,0,2022-03-04 10:59:27.810000 UTC,,,0,python|amazon-s3|jupyter-notebook|amazon-sagemaker,309,2020-07-24 06:17:43.600000 UTC,2022-03-22 16:32:59.770000 UTC,,21,0,0,1,,,,,,['amazon-sagemaker']
Amazon SageMaker: Customer Error: Training did not complete successfully for binary text classification,"<p>This is my first try of Amazon SageMaker. Essentially I am trying to create a spam detection filter using a binary classifier with Blazing Text in SageMaker. Attempted to train the model w/these commands:</p>
<pre><code>bt_model = sagemaker.estimator.Estimator(container,
                                         role, 
                                         train_instance_count=1, 
                                         train_instance_type='ml.c4.4xlarge',
                                         train_volume_size = 100,
                                         train_max_run = 360000,
                                         input_mode= 'File',
                                         output_path=s3_output_location,
                                         sagemaker_session=sess)

</code></pre>
<p>and</p>
<pre><code>bt_model.set_hyperparameters(mode=&quot;supervised&quot;,
                            epochs=500,
                            min_count=2,
                            learning_rate=0.05,
                            vector_dim=15,
                            early_stopping=True,
                            patience=10,
                            min_epochs=200,
                            word_ngrams=2)
</code></pre>
<p>But then when I try to run this, I get the following log:</p>
<pre><code>----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
|   timestamp   |                                                                                                                                                                                                             message                                                                                                                                                                                                              |
|---------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 1592870667409 | Arguments: train                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
| 1592870667409 | [06/23/2020 00:04:24 INFO 139842440988480] nvidia-smi took: 0.0252470970154 secs to identify 0 gpus                                                                                                                                                                                                                                                                                                                              |
| 1592870667409 | [06/23/2020 00:04:24 INFO 139842440988480] Running single machine CPU BlazingText training using supervised mode.                                                                                                                                                                                                                                                                                                                |
| 1592870667409 | [06/23/2020 00:04:24 INFO 139842440988480] Processing /opt/ml/input/data/train/clause.train . File size: 2 MB                                                                                                                                                                                                                                                                                                                    |
| 1592870667409 | [06/23/2020 00:04:24 INFO 139842440988480] Processing /opt/ml/input/data/validation/clause.validation . File size: 2 MB                                                                                                                                                                                                                                                                                                          |
| 1592870667409 | Read 0M words                                                                                                                                                                                                                                                                                                                                                                                                                    |
| 1592870667409 | Number of words:  12005                                                                                                                                                                                                                                                                                                                                                                                                          |
| 1592870667409 | Loading validation data from /opt/ml/input/data/validation/clause.validation                                                                                                                                                                                                                                                                                                                                                     |
| 1592870667409 | Loaded validation data.                                                                                                                                                                                                                                                                                                                                                                                                          |
| 1592870667409 | -------------- End of epoch: 43                                                                                                                                                                                                                                                                                                                                                                                                  |
| 1592870667409 | ##### Alpha: 0.0433  Progress: 13.37%  Million Words/sec: 86.39 #####                                                                                                                                                                                                                                                                                                                                                            |
| 1592870667409 | -------------- End of epoch: 66                                                                                                                                                                                                                                                                                                                                                                                                  |
| 1592870667409 | -------------- End of epoch: 90                                                                                                                                                                                                                                                                                                                                                                                                  |
| 1592870667409 | ##### Alpha: 0.0387  Progress: 22.54%  Million Words/sec: 86.66 #####                                                                                                                                                                                                                                                                                                                                                            |
| 1592870667409 | -------------- End of epoch: 112                                                                                                                                                                                                                                                                                                                                                                                                 |
| 1592870667409 | -------------- End of epoch: 135                                                                                                                                                                                                                                                                                                                                                                                                 |
| 1592870667409 | ##### Alpha: 0.0341  Progress: 31.81%  Million Words/sec: 87.08 #####                                                                                                                                                                                                                                                                                                                                                            |
| 1592870667409 | -------------- End of epoch: 159                                                                                                                                                                                                                                                                                                                                                                                                 |
| 1592870667409 | -------------- End of epoch: 182                                                                                                                                                                                                                                                                                                                                                                                                 |
| 1592870667409 | ##### Alpha: 0.0295  Progress: 41.07%  Million Words/sec: 87.30 #####                                                                                                                                                                                                                                                                                                                                                            |
| 1592870667409 | -------------- End of epoch: 205                                                                                                                                                                                                                                                                                                                                                                                                 |
| 1592870667409 | Using 16 threads for prediction!                                                                                                                                                                                                                                                                                                                                                                                                 |
| 1592870667409 | Validation accuracy: -nan                                                                                                                                                                                                                                                                                                                                                                                                        |
| 1592870667409 | Validation accuracy has not improved for last 1 epochs.                                                                                                                                                                                                                                                                                                                                                                          |
| 1592870667409 | -------------- End of epoch: 207                                                                                                                                                                                                                                                                                                                                                                                                 |
| 1592870667409 | Using 16 threads for prediction!                                                                                                                                                                                                                                                                                                                                                                                                 |
| 1592870667409 | Validation accuracy: -nan                                                                                                                                                                                                                                                                                                                                                                                                        |
| 1592870667409 | Validation accuracy has not improved for last 2 epochs.                                                                                                                                                                                                                                                                                                                                                                          |
| 1592870667410 | -------------- End of epoch: 208                                                                                                                                                                                                                                                                                                                                                                                                 |
| 1592870667410 | Using 16 threads for prediction!                                                                                                                                                                                                                                                                                                                                                                                                 |
| 1592870667410 | Validation accuracy: -nan                                                                                                                                                                                                                                                                                                                                                                                                        |
| 1592870667410 | Validation accuracy has not improved for last 3 epochs.                                                                                                                                                                                                                                                                                                                                                                          |
| 1592870667410 | -------------- End of epoch: 209                                                                                                                                                                                                                                                                                                                                                                                                 |
| 1592870667410 | Using 16 threads for prediction!                                                                                                                                                                                                                                                                                                                                                                                                 |
| 1592870667410 | Validation accuracy: -nan                                                                                                                                                                                                                                                                                                                                                                                                        |
| 1592870667410 | Validation accuracy has not improved for last 4 epochs.                                                                                                                                                                                                                                                                                                                                                                          |
| 1592870667410 | -------------- End of epoch: 211                                                                                                                                                                                                                                                                                                                                                                                                 |
| 1592870667410 | Using 16 threads for prediction!                                                                                                                                                                                                                                                                                                                                                                                                 |
| 1592870667410 | Validation accuracy: -nan                                                                                                                                                                                                                                                                                                                                                                                                        |
| 1592870667410 | Validation accuracy has not improved for last 5 epochs.                                                                                                                                                                                                                                                                                                                                                                          |
| 1592870667410 | -------------- End of epoch: 213                                                                                                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                                                                                                                                                                                       | ...                                                                                                                                                                                                                                                                                                                                                                                               |
| 1592870667410 | Using 16 threads for prediction!                                                                                                                                                                                                                                                                                                                                                                                                 |
| 1592870667410 | Validation accuracy: -nan                                                                                                                                                                                                                                                                                                                                                                                                        |
| 1592870667410 | Validation accuracy has not improved for last 9 epochs.                                                                                                                                                                                                                                                                                                                                                                          |
| 1592870667410 | -------------- End of epoch: 219                                                                                                                                                                                                                                                                                                                                                                                                 |
| 1592870667410 | Using 16 threads for prediction!                                                                                                                                                                                                                                                                                                                                                                                                 |
| 1592870667410 | Validation accuracy: -nan                                                                                                                                                                                                                                                                                                                                                                                                        |
| 1592870667410 | Validation accuracy has not improved for last 10 epochs.                                                                                                                                                                                                                                                                                                                                                                         |
| 1592870667410 | Reached patience. Terminating training.                                                                                                                                                                                                                                                                                                                                                                                          |
| 1592870667410 | Best epoch: 0                                                                                                                                                                                                                                                                                                                                                                                                                    |
| 1592870667410 | Best validation accuracy: 0                                                                                                                                                                                                                                                                                                                                                                                                      |
| 1592870667410 | ##### Alpha: 0.0000  Progress: 100.00%  Million Words/sec: 99.57 #####                                                                                                                                                                                                                                                                                                                                                           |
| 1592870669411 | [06/23/2020 00:04:29 ERROR 139842440988480] Customer Error: Training did not complete successfully! Please check the logs for errors.                                                                                                                                                                                                                                                                                            |
| 1592870669411 | Traceback (most recent call last):   File &quot;/opt/amazon/lib/python2.7/site-packages/blazingtext/train.py&quot;, line 75, in main     train_blazing_single(resource_config, train_config, data_config)   File &quot;/opt/amazon/lib/python2.7/site-packages/blazingtext/train_methods.py&quot;, line 245, in train_blazing_single     raise exceptions.CustomerError(&quot;Training did not complete successfully! Please check the logs for errors.&quot;) |
| 1592870669411 | CustomerError: Training did not complete successfully! Please check the logs for errors.                                                                                                                                                                                                                                                                                                                                         |
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
</code></pre>
<p>I believe my training and test datasets are preprocessed correctly, so any and all help would be much appreciated. Thank you so much!</p>",0,4,2020-06-23 00:26:39.280000 UTC,,,1,amazon-web-services|machine-learning|amazon-s3|text-classification|amazon-sagemaker,216,2017-06-29 15:29:43.817000 UTC,2022-08-31 04:15:42.560000 UTC,,219,9,0,40,,,,,,['amazon-sagemaker']
Sagemaker Studio Lab GPU not working with pytorch,"<p>I can't get the GPU to work even though i chose the GPU option with 4 hours at the start. Is there something to do in order to activate it?</p>
<p><a href=""https://i.stack.imgur.com/GCzc0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GCzc0.png"" alt=""enter image description here"" /></a></p>
<p>edit:</p>
<p>I installed pytorch with cuda:
%conda install pytorch torchvision torchaudio cudatoolkit=11.4 -c pytorch</p>",1,0,2022-01-28 00:25:16.280000 UTC,,,0,pytorch|amazon-sagemaker,244,2020-05-18 09:25:18.473000 UTC,2022-09-14 18:51:17.880000 UTC,,19,3,0,6,,,,,,['amazon-sagemaker']
R Script in AzureML,"<p>I am trying to develop a survey App and using R Script with Azure ML for same.</p>

<p>I have developed below code for the same and it works perfectly fine on Local Machine:</p>

<pre><code>dataset1 &lt;-maml.mapInputPort(2)
dataset3 &lt;-maml.mapInputPort(1)
Z &lt;- as.numeric((dataset3),stringsAsFactors=TRUE)
Y &lt;- mdBinaryDesign(Z,4,dataset1)
Y.aggregate=mdBinaryToAggregateDesign(Y)
survey.design=mdDesignNames(Y.aggregate, dataset1)
data.set &lt;- as.data.frame(survey.design)
maml.mapOutputPort(""data.set"")
</code></pre>

<p>Now we plan to deploy this Application on server , for which we are using Azure MIL .</p>

<p>Now my Dataset1 and Dataset3 are coming using Input port in R Model , by using the above code , I get error ""missing value where TRUE/FALSE needed"". My Dataset3 contains a simple number eg: ""5"" .</p>

<p>Since my model will only run having three dynamic inputs (e.g. a, b, c), is there a way I can call a web service which will give me three output parameters via JSON and I can assign same to my model?</p>

<p>The part where I want to dynamically apply parameters are:</p>

<pre><code>Y &lt;- mdBinaryDesign(parameter_1,parameter_2,parameters3)
</code></pre>

<p>Since I am new to R , Please suggest we what library to use as well how to assigns value to parameter_1 and so on.</p>",1,0,2016-11-02 11:54:24.743000 UTC,,2017-01-04 12:05:17.737000 UTC,0,r|azure-machine-learning-studio,203,2015-02-02 08:45:49.097000 UTC,2021-10-13 14:11:20.567000 UTC,,31,0,0,31,,,,,,['azure-machine-learning-studio']
How to get current run_id inside of mlflow.start_run()?,"<p><code>mlflow.active_run()</code> returns nothing so I can't just use
<code>current_rui_id = mlflow.active_run().info.run_id</code></p>

<p>I have to get <strong>run_id</strong> inside of this construction for being able to continue logging parameters,  metrics and artifacts inside of another block but for the same model:</p>

<pre><code>with mlflow.start_run(run_name=""test_ololo""):

    """""" 
       fitting a model here ...
    """"""

    for name, val in metrics:
        mlflow.log_metric(name, np.float(val))

    # Log our parameters into mlflow
    for k, v in params.items():
        mlflow.log_param(key=k, value=v)

    pytorch.log_model(learn.model, f'model')
    mlflow.log_artifact('./outputs/fig.jpg')
</code></pre>

<p>I have to get current <strong>run_id</strong> to continue training inside the same run</p>

<pre><code>with mlflow.start_run(run_id=""215d3a71925a4709a9b694c45012988a""):

    """"""
       fit again
       log_metrics
    """"""

    pytorch.log_model(learn.model, f'model')
    mlflow.log_artifact('./outputs/fig2.jpg')
</code></pre>",3,0,2020-01-20 16:33:13.127000 UTC,1.0,,4,python|mlflow,6042,2015-09-30 15:37:49.260000 UTC,2022-09-22 12:24:05.573000 UTC,,863,100,2,22,,,,,,['mlflow']
validation exception while featuregroup creation through sagemaker in ec2,"<p>I am referring to document <code>https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-featurestore/sagemaker_featurestore_fraud_detection_python_sdk.html#Ingest-Data-into-FeatureStore</code> to create featuregroup in ec2 instance.</p>
<p>getting below error while executing the script.</p>
<pre><code>An error occurred (ValidationException) when calling the CreateFeatureGroup operation: The execution role ARN is invalid. Please ensure that the role exists and that its trust relationship policy allows the action 'sts:AssumeRole' for the service principal 'sagemaker.amazonaws.com'.
</code></pre>
<p>I have confusion regarding roles here.
In above script, get_execution_role() is returning ec2-role-123  which is attached to ec2 instance and has trust policy as &quot;ec2.amazonaws.com&quot;.Also this ec2 role I am passing to other role sync-role-123 which has both &quot;ec2.amazonaws.com&quot;, &quot;sagemaker.amazonaws.com&quot; as trusted entities. I have attached below permission to ec2 role</p>
<pre><code>{
            &quot;Sid&quot;: &quot;AllowIAMPassRoleAccess&quot;,
            &quot;Effect&quot;: &quot;Allow&quot;,
            &quot;Action&quot;: [
                &quot;iam:PassRole&quot;
            ],
            &quot;Resource&quot;: &quot;arn:aws:iam::1212121:role/sync-role-123&quot;
        }
</code></pre>
<p>now my confusion is since we are running script inside EC2 does it uses ec2-role-123 as execution role for both ec2 and sagemaker as well
OR<br />
ec2-role-123 execution role will be passed to sync-role-123 for sagemaker API calls?
I am not getting clear understanding on how sagemaker API works specially inside EC2?
can someone please guide.
so that I can apply the fix accordingly.</p>",1,0,2022-07-25 21:24:47.027000 UTC,,2022-07-25 21:30:09.067000 UTC,0,amazon-web-services|amazon-ec2|amazon-sagemaker,37,2013-04-10 05:09:01.220000 UTC,2022-09-14 22:34:07.097000 UTC,,519,13,0,576,,,,,,['amazon-sagemaker']
azure ML on AKS memory and CPU parameters,"<p>When specifying the memory and core for the AksWebService (deploying Azure ML as a service in AKS), do the values for cpu_cores and memory_gb apply to each replica OR all replicas combined?</p>
<pre><code>AksWebservice.deploy_configuration(cpu_cores=1, memory_gb=1, auth_enabled=True, autoscale_enabled=True, autoscale_min_replicas=4, autoscale_max_replicas=10)
</code></pre>
<p>I am assuming its per replica but just wanted to confirm.</p>",1,0,2022-07-05 16:31:15.127000 UTC,,,0,azure-aks|azure-machine-learning-service,32,2013-08-15 14:39:30.773000 UTC,2022-09-23 17:28:32.673000 UTC,,301,27,0,52,,,,,,['azure-machine-learning-service']
jsonschema 4.4.0 does not provide the extra 'isoduration',"<p>So I'm trying to run some piece of code and keep getting the following error:</p>
<pre><code>File &quot;/opt/conda/lib/python3.8/site-packages/pkg_resources/__init__.py&quot;, line 770, in resolve
    raise DistributionNotFound(req, requirers)
pkg_resources.DistributionNotFound: The 'isoduration; extra == &quot;format&quot;' distribution was not found and is required by jsonschema
</code></pre>
<p>However, after running</p>
<pre><code>pip uninstall -y jsonschema &amp;&amp; pip install -U jsonschema &amp;&amp; pip install jsonschema[isoduration]
</code></pre>
<p>I get</p>
<pre><code>Requirement already satisfied: jsonschema[isoduration] in /opt/conda/lib/python3.8/site-packages (4.4.0)
  WARNING: jsonschema 4.4.0 does not provide the extra 'isoduration'
</code></pre>
<p>and surely, running my code again I get the same error message as before.</p>
<p><strong>I tried:</strong></p>
<ul>
<li><code>pip install isoduration</code>, but different format showed up as
missing</li>
<li>hard removing <code>jsonschema</code> with <code>rm -rf ...</code></li>
<li>installing <code>jsonschema==3.2.0</code> as it supposedly worked for a friend of mine</li>
</ul>
<p>I'm very confused with what's going on here, any help would be appreciated.</p>",1,0,2022-04-13 11:55:05.623000 UTC,,2022-04-13 12:41:29.193000 UTC,1,python|jsonschema|kedro|neptune,74,2018-07-22 10:19:06.377000 UTC,2022-07-04 12:47:02.310000 UTC,,33,0,0,1,,,,,,['neptune']
Writing csv to s3 bucket inconsistent error,"<blockquote>
<p>The unspecified location constraint is incompatible for the region
specific endpoint this request was sent to</p>
</blockquote>
<p>I need to write a lot of CSVs to an s3 bucket from a sagemaker notebook</p>
<p>I have the following code that I use.</p>
<pre><code>def create_user_block(algo
                  ,user
                  ,items
                  , pathTo
                  , popularity=None):

log('Creation of user block for {} started'.format(user))

recomendations = []
for item in items:
    recomendations.append(algo.predict(uid=user, iid=item))

    
recomendations = pd.DataFrame(recomendations)['iid est'.split()]
recomendations['est'] = recomendations.est.round()
recomendations = recomendations[ recomendations.est &gt;=4]


if popularity is not None:
    popularity.columns = 'iid Popularity'.split()
    recomendations = recomendations.merge(right=popularity, how='left', on='iid')
    recomendations = recomendations.sort_values(by='est Popularity'.split(), ascending = False)

else:
    recomendations = recomendations.sort_values(by='est', ascending = False)

log('User block for {} returned'.format(user))

recomendations.to_csv( pathTo+'user_'+user+'.csv', index=False)
</code></pre>
<p>The function creates a csv file from a model and then writes it to a specific location.</p>
<p>So far so good</p>
<p>That function is wrapped around a second function that simply increments and calculates a few global variables in order to track my progress</p>
<pre><code>def create_user_block_progress(algo
                           , user
                           , items
                           , popularity
                           , pathTo =os.environ['UPCARS_RECOMMENDATION_LOCATION']):
global count, noUI, progress

try:
    create_user_block(algo=algo, user=user, items = items, pathTo=pathTo, popularity=popularity)

    count = count+1


    if progress &lt; int(((count/noUI)*100)):
        progress = int(((count/noUI)*100))

        update_item(field = 'progress'
                , value = progress
                , tableName = os.environ['UPCARS_ACTIVE_TASK_TABLE']
                , key = os.environ['UPCARS_ACTIVE_TASK_KEY']
                , createdAt = os.environ['UPCARS_ACTIVE_TASK_CREATED_AT'])

except Exception as e:
    meta_log('FAILED to create user block for {} due to the following error: {}'.format(user, e), True)
</code></pre>
<p>and then I run the wrapper function on multiple threads as written below:</p>
<pre><code> with concurrent.futures.ThreadPoolExecutor() as executor:
        threads = [executor.submit(create_user_block_progress
                                    , algo
                                    , u
                                    , items
                                    , itemPopularity) for u in users]
</code></pre>
<p>The pathTo is an S3 bucket and up until yesterday there were no issues. As of the last two days I keep getting an occasional error. As in this error could or could not occur for the exact same user regardless. Sometimes it happens sometimes it does not. Sometimes it does not happen at all. Sometimes it happens all the time.</p>
<p>And when it does happen I get the following error:</p>
<blockquote>
<p>The unspecified location constraint is incompatible for the region
specific endpoint this request was sent to</p>
</blockquote>
<p>So far I am baffled at what the issue is. If anyone could help me please I would appreciated.</p>
<p>In effect the issue appears sometimes and I do not know why when I try to write a csv to an s3 bucket.</p>
<p>The command that gives the error is something like:</p>
<pre><code>recomendations.to_csv( 's3://local-recommendations/m1/knn/v8/user_123567.csv', index=False)
</code></pre>
<p>And it is this command that gives an error sometimes but sometimes not. The inconsistency is maddening</p>",0,1,2021-06-30 08:03:06.580000 UTC,,2021-07-01 09:42:32.490000 UTC,1,python|amazon-web-services|amazon-s3|amazon-sagemaker,76,2021-06-30 07:50:35.737000 UTC,2021-07-08 12:02:47.873000 UTC,,11,0,0,0,,,,,,['amazon-sagemaker']
What AWS endpoint does sklearn.transformer.transform call? Sagemaker/Processing Job,"<p>I'm currently deploying a Jupyter Notebook inside a docker container via this github:
<a href=""https://github.com/aws-samples/sagemaker-run-notebook"" rel=""nofollow noreferrer"">https://github.com/aws-samples/sagemaker-run-notebook</a></p>
<p>Sagemaker creates a 'processing job' which launches my docker container and runs the jupyter notebook within it via papermill.</p>
<p>However sagemaker Processing Jobs don't have access it seems to any internal AWS services despite operating within a VPC. This is normal behaviour and you need to create VPC Endpoints within your VPC that provides access for services within the VPC to specific AWS internal API's. Here's an example: &quot;com.amazonaws.eu-west-2.s3&quot; which provides access for a processing job to S3.</p>
<p>So, my problem is when I run my script it hangs/timesout on this cell:</p>
<pre><code>transformer = sklearn_preprocessor.transformer(
    instance_count=1, 
    instance_type='ml.m4.xlarge',
    assemble_with='Line',
    accept='text/csv',
    output_path = f&quot;s3://{BUCKET}/{PROJECT_NAME}/preprocessor/&quot;)
transformer.transform(train_inputs[0], content_type='text/csv')
</code></pre>
<p>I'm assuming that this .transform function is calling some internal service that I haven't built and endpoint for.</p>
<p>And ideas?</p>",0,0,2022-05-26 21:25:34.660000 UTC,,,0,amazon-web-services|amazon-sagemaker,37,2017-05-24 06:39:19.953000 UTC,2022-09-23 17:33:31.663000 UTC,"Wellington, New Zealand",358,10,3,84,,,,,,['amazon-sagemaker']
Azure ML custom image tag and registry,"<p>Is there a way we can customize the image tags and names produced by Azure ML?
Also, is it possible to push the images to private registry rather than container registry (the default one in azure ML)?</p>
<p>Or if we can download image from container registry to Jfrog in Azure Devops?</p>
<p>I need to do this to write deploy pipeline in AzDO. I am not AZML expert, want to help team as DevOps who has started using AZML newly and not aware of how to set this up.</p>",1,3,2021-06-27 00:04:34.887000 UTC,,2021-06-27 10:06:00.533000 UTC,0,machine-learning|azure-devops|azure-machine-learning-studio|azure-machine-learning-service|azure-devops-pipelines,181,2021-01-29 16:16:26.203000 UTC,2021-10-20 13:38:24.500000 UTC,United Kingdom,55,0,0,5,,,,,,"['azure-machine-learning-service', 'azure-machine-learning-studio']"
is there any symbols forbidden in run_id?,"<p>I just started working with wandb. I am curious if there is any requirement in the format of the run_id (e.g. cannot contain brackets '[]' or '()')</p>
<p>the reason why I am asking is that I cannot log my runs (image data) properly after adding a '[..]' prefix to my run_id. There is no modification in the code which could lead to this problem and the wandb version remain the same.</p>
<p>Would be great if anyone could gives some clue in this.</p>
<p>Many thanks</p>",1,0,2022-08-16 11:08:11.100000 UTC,,,0,wandb,25,2022-08-16 11:02:23.310000 UTC,2022-09-22 10:35:05.063000 UTC,,1,0,0,0,,,,,,['wandb']
how to include environment when submitting an automl experiment in azure machine learning,"<p>I use code like below to create an AutoML object to submit an experiment for classification training</p>
<pre><code>automl_settings = {
       &quot;n_cross_validations&quot;: 2,
       &quot;primary_metric&quot;: 'accuracy',
       &quot;enable_early_stopping&quot;: True,
       &quot;experiment_timeout_hours&quot;: 1.0,
       &quot;max_concurrent_iterations&quot;: 4,
       &quot;verbosity&quot;: logging.INFO,
   }

   automl_config = AutoMLConfig(task = 'classification',
                               compute_target = compute_target,
                               training_data = train_data,
                               label_column_name = label,
                               **automl_settings
                               )

   ws = Workspace.from_config()
   experiment = Experiment(ws, &quot;your-experiment-name&quot;)
   run = experiment.submit(automl_config, show_output=True)
</code></pre>
<p>I want to include my conda yml file (like below) in my experiment submission.</p>
<pre><code>env = Environment.from_conda_specification(name='myenv', file_path='conda_dependencies.yml')
</code></pre>
<p>However, I don't see any environment parameter in <a href=""https://docs.microsoft.com/en-us/python/api/azureml-train-automl-client/azureml.train.automl.automlconfig.automlconfig?view=azure-ml-py"" rel=""nofollow noreferrer"">AutoMLConfig class documentation</a> (similar to what environment parameter does in <code>ScriptRunConfig</code>) or find any example how to do so.</p>
<p>I notice after the experiment is submitted, I get message like this</p>
<pre><code>Running on remote.
No run_configuration provided, running on aml-compute with default configuration
</code></pre>
<p>Is <code>run_configuration</code> used for specifying environment? If so, how do I provide run_configuration in my <strong>AutoML experiment run</strong>?</p>
<p>Thank you.</p>",1,3,2020-12-11 03:23:25.680000 UTC,,,1,azure-machine-learning-studio|azure-machine-learning-service|azure-sdk-python,229,2017-05-24 01:09:10.030000 UTC,2020-12-16 08:24:08.143000 UTC,,127,0,0,15,,,,,,"['azure-machine-learning-studio', 'azure-machine-learning-service']"
Python- Unable to Train Tensorflow Model Container in Sagemaker,"<p>I'm fairly new to Sagemaker and Docker.I am trying to train my own custom object detection algorithm in Sagemaker using an ECS container. I'm using this repo's files:</p>

<p><a href=""https://github.com/svpino/tensorflow-object-detection-sagemaker"" rel=""nofollow noreferrer"">https://github.com/svpino/tensorflow-object-detection-sagemaker</a></p>

<p>I've followed the instructions exactly, and I'm able to run the image in a container perfectly fine on my local machine. But when I push the image to ECS to run in Sagemaker, I get the following message in Cloudwatch: <a href=""https://i.stack.imgur.com/0MWTG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0MWTG.png"" alt=""I get the following error message in Cloudwatch:""></a></p>

<p>I understand that for some reason, when deployed to ECS suddenly the image can't find python. At the top of my training script is the text <em>#!/usr/bin/env python</em>. I've tried to run the *which python * command and changed up text to point to <em>#!/usr/local/bin python</em>, but I just get additional errors. I don't understand why this image would work on my local (tested with both docker on windows and docker CE for WSL). Here's a snippet of the docker file:</p>

<pre><code>ARG ARCHITECTURE=1.15.0-gpu
FROM tensorflow/tensorflow:${ARCHITECTURE}-py3

RUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \
        wget zip unzip git ca-certificates curl nginx python

# We need to install Protocol Buffers (Protobuf). Protobuf is Google's language and platform-neutral,  
# extensible mechanism for serializing structured data. To make sure you are using the most updated code,
# replace the linked release below with the latest version available on the Git repository.
RUN curl -OL https://github.com/protocolbuffers/protobuf/releases/download/v3.10.1/protoc-3.10.1-linux-x86_64.zip
RUN unzip protoc-3.10.1-linux-x86_64.zip -d protoc3
RUN mv protoc3/bin/* /usr/local/bin/
RUN mv protoc3/include/* /usr/local/include/

# Let's add the folder that we are going to be using to install all of our machine learning-related code
# to the PATH. This is the folder used by SageMaker to find and run our code.
ENV PATH=""/opt/ml/code:${PATH}""
RUN mkdir -p /opt/ml/code
WORKDIR /opt/ml/code

RUN pip install --upgrade pip
RUN pip install cython
RUN pip install contextlib2
RUN pip install pillow
RUN pip install lxml
RUN pip install matplotlib
RUN pip install flask
RUN pip install gevent
RUN pip install gunicorn
RUN pip install pycocotools

# Let's now download Tensorflow from the official Git repository and install Tensorflow Slim from
# its folder.
RUN git clone https://github.com/tensorflow/models/ tensorflow-models
RUN pip install -e tensorflow-models/research/slim

# We can now install the Object Detection API, also part of the Tensorflow repository. We are going to change
# the working directory for a minute so we can do this easily.
WORKDIR /opt/ml/code/tensorflow-models/research
RUN protoc object_detection/protos/*.proto --python_out=.
RUN python setup.py build
RUN python setup.py install

# If you are interested in using COCO evaluation metrics, you can tun the following commands to add the
# necessary resources to your Tensorflow installation.
RUN git clone https://github.com/cocodataset/cocoapi.git
WORKDIR /opt/ml/code/tensorflow-models/research/cocoapi/PythonAPI
RUN make 
RUN cp -r pycocotools /opt/ml/code/tensorflow-models/research/

# Let's put the working directory back to where it needs to be, copy all of our code, and update the PYTHONPATH
# to include the newly installed Tensorflow libraries.
WORKDIR /opt/ml/code
COPY /code /opt/ml/code

ENV PYTHONPATH=${PYTHONPATH}:tensorflow-models/research:tensorflow-models/research/slim:tensorflow-models/research/object_detection

RUN chmod +x /opt/ml/code/train
CMD [""/bin/bash"",""-c"",""chmod +x /opt/ml/code/train &amp;&amp; /opt/ml/code/train""]
</code></pre>",0,5,2020-03-16 14:40:48.277000 UTC,,2020-03-16 17:35:42.043000 UTC,0,python|docker|tensorflow|amazon-ecs|amazon-sagemaker,237,2015-08-22 16:22:34.927000 UTC,2022-02-09 06:23:16.160000 UTC,,91,1,0,30,,,,,,['amazon-sagemaker']
Configuring a local Azure ML target in WSL?,"<p>Currently, I'm having no luck getting a local compute target to run in WSL.  I can happily execute my training script, and I can execute my driver, but the job never leaves the starting state.  I've enabled 'node' in the Firewall - not sure if that is related - it popped up at the same time.</p>
<p>Driver Code:</p>
<pre><code>    ws = Workspace.from_config()
    experiment = Experiment(workspace=ws, name='mnist-training-local')
    
    env = Environment(&quot;system-managed-env&quot;)
    env.python.user_managed_dependencies = True

    config = ScriptRunConfig(source_directory='src', script='hello.py', environment=env)

    run = experiment.submit(config)
    run.wait_for_completion(show_output=True)
</code></pre>
<p>There is a diagnostic link output -</p>
<pre><code>&quot;data_container_id&quot;: &quot;dcid.mnist-training-local_1605046048_2a328e04&quot;,
    &quot;diagnostics_uri&quot;: &quot;https://eastus2.experiments.azureml.net/execution/v1.0/subscriptions/c14a37bd-a658-463c-9d44-9a9326fe5fbe/resourceGroups/TutorialResourceGroup/providers/Microsoft.MachineLearningServices/workspaces/TutorialWorkspace/experiments/mnist-training-local/runId/mnist-training-local_1605046048_2a328e04/diagnostics&quot;
</code></pre>
<p>But the link is never active.</p>
<p>Viewing the job through the portal results in the job just staying in the 'Starting' state.  It's like there's a firewall issue between WSL and Azure, but I'm not sure where.</p>
<p>I have tried stopping the Windows Defender entirely though - no luck.  I suspect that there's an incoming connection that is not getting routed to WSL correctly?</p>
<p>'top' shows no real activity on the system, the system just is ... waiting.  For eternity.</p>
<p>This is the first step of the following Azure ML notebook: <a href=""https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/training/train-on-local/train-on-local.ipynb"" rel=""nofollow noreferrer"">https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/training/train-on-local/train-on-local.ipynb</a></p>
<p>User managed pre-reqs.</p>
<p>I can in fact, submit from a DOS (Powershell) terminal:</p>
<pre><code>Web View: https://ml.azure.com/experiments/mnist-training-local/runs/mnist-training-local_1605107830_4f84d648?wsid=/subscriptions/c14a37bd-a658-463c-9d44-9a9326fe5fbe/resourcegroups/TutorialResourceGroup/workspaces/TutorialWorkspace

Streaming azureml-logs/60_control_log.txt
=========================================

[2020-11-11T15:17:14.044950] Using urllib.request Python 3.0 or later
Streaming log file azureml-logs/60_control_log.txt
Running: ['cmd.exe', '/c', 'C:\\Users\\allan\\AppData\\Local\\Temp\\azureml_runs\\mnist-training-local_1605107830_4f84d648\\azureml-environment-setup/conda_env_checker.bat']

&lt;snip&gt;
After variable expansion, calling script [ hello.py ] with arguments: []

Script type = None
Hello world!
Starting the daemon thread to refresh tokens in background for process with pid = 21880
</code></pre>
<p>Thanks in advance.</p>",0,2,2020-11-10 22:25:00.573000 UTC,,2020-11-11 15:21:35.907000 UTC,2,azure|azure-machine-learning-service,100,2015-08-12 12:04:42.937000 UTC,2021-03-18 21:22:14.747000 UTC,,21,0,0,1,,,,,,['azure-machine-learning-service']
Python: Ssl Certificate verify failed,"<p>I have installed <code>dvc</code> on my <code>ubuntu-18.04-LTS</code> system and while trying to download the <code>data</code> files from github using dvc, it fails with below error.</p>
<pre><code>$ dvc get https://github.com/iterative/dataset-registry get-started/data.xml -o data/data.xml -v

2022-07-22 12:55:22,260 DEBUG: Creating external repo https://github.com/iterative/dataset-registry@None
2022-07-22 12:55:22,260 DEBUG: erepo: git clone 'https://github.com/iterative/dataset-registry' to a temporary dir
2022-07-22 12:55:23,683 DEBUG: Removing '/dvc/dvc_test/data/.UEeAzwmJCY3q85YQuCeahx'
2022-07-22 12:55:23,684 ERROR: failed to get 'get-started/data.xml' from 'https://github.com/iterative/dataset-registry' - Failed to clone repo 'https://github.com/iterative/dataset-registry' to '/tmp/tmpvmrmu9qsdvc-clone'
------------------------------------------------------------
Traceback (most recent call last):
  File &quot;urllib3/connectionpool.py&quot;, line 703, in urlopen
  File &quot;urllib3/connectionpool.py&quot;, line 386, in _make_request
  File &quot;urllib3/connectionpool.py&quot;, line 1042, in _validate_conn
  File &quot;urllib3/connection.py&quot;, line 414, in connect
  File &quot;urllib3/util/ssl_.py&quot;, line 449, in ssl_wrap_socket
  File &quot;urllib3/util/ssl_.py&quot;, line 493, in _ssl_wrap_socket_impl
  File &quot;ssl.py&quot;, line 500, in wrap_socket
  File &quot;ssl.py&quot;, line 1040, in _create
  File &quot;ssl.py&quot;, line 1309, in do_handshake
ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1108)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;dvc/scm.py&quot;, line 145, in clone
  File &quot;scmrepo/git/__init__.py&quot;, line 143, in clone
  File &quot;scmrepo/git/backend/dulwich/__init__.py&quot;, line 199, in clone
scmrepo.exceptions.CloneError: Failed to clone repo 'https://github.com/iterative/dataset-registry' to '/tmp/tmpvmrmu9qsdvc-clone'
</code></pre>
<p>Already our corporate proxy certificate has been installed and traffic to <code>github.com</code> allowed I'm able to clone above repository separately on CLI. But with <code>dvc</code>the above errors are occurring, Even the below couldn't solve the issue.</p>
<pre><code>$ python -c &quot;import ssl; print(ssl.get_default_verify_paths())&quot;

DefaultVerifyPaths(cafile=None, capath='/usr/lib/ssl/certs', openssl_cafile_env='SSL_CERT_FILE', openssl_cafile='/usr/lib/ssl/cert.pem', openssl_capath_env='SSL_CERT_DIR', openssl_capath='/usr/lib/ssl/certs')
</code></pre>
<pre><code>export SSL_CERT_DIR=/etc/ssl/certs/
export REQUESTS_CA_BUNDLE=/usr/local/lib/python2.7/dist-packages/certifi/cacert.pem
pip install --upgrade certifi
export PYTHONHTTPSVERIFY=0

sudo apt install ca-certificates
sudo update-ca-certificates --fresh
</code></pre>
<pre><code>$ python --version
Python 2.7.17

$ dvc doctor
DVC version: 2.13.0 (deb)
---------------------------------
Platform: Python 3.8.3 on Linux-5.4.0-92-generic-x86_64-with-glibc2.14
Supports:
        azure (adlfs = 2022.7.0, knack = 0.9.0, azure-identity = 1.10.0),
        gdrive (pydrive2 = 1.10.1),
        gs (gcsfs = 2022.5.0),
        hdfs (fsspec = 2022.5.0, pyarrow = 8.0.0),
        webhdfs (fsspec = 2022.5.0),
        http (aiohttp = 3.8.1, aiohttp-retry = 2.5.1),
        https (aiohttp = 3.8.1, aiohttp-retry = 2.5.1),
        s3 (s3fs = 2022.5.0, boto3 = 1.21.21),
        ssh (sshfs = 2022.6.0),
        oss (ossfs = 2021.8.0),
        webdav (webdav4 = 0.9.7),
        webdavs (webdav4 = 0.9.7)
</code></pre>
<p>Tp bypass the ssl validation in git we have <code>git config http.sslVerify &quot;false&quot;</code> Similarly do we have option in dvc?</p>
<p>Further what should i update to resolve this issue?</p>",1,2,2022-07-26 15:36:20.953000 UTC,,2022-07-27 05:01:17.490000 UTC,1,python|ssl|pip|ssl-certificate|dvc,157,2015-05-28 11:02:07.473000 UTC,2022-09-25 03:23:10.223000 UTC,,1609,68,0,447,,,,,,['dvc']
How to schedule task in AWS Sagemaker to run training job,"<p>I have an optimization code running in sagemaker. I want to run the code in every 1 hr. How can I schedule the run in sagemaker?. I do not want to call model endpoint, but I want to run the whole code in every 1 hr.</p>",1,0,2019-04-09 06:30:21.367000 UTC,,,1,amazon-sagemaker,1511,2019-04-09 06:25:51.317000 UTC,2021-11-02 05:06:34.580000 UTC,,11,0,0,2,,,,,,['amazon-sagemaker']
How to load a trained BlazingText model,"<p>I have  trained a text classification model using blazingText on AWS sagemaker, I can load the trained model and deploy an inference endpoint</p>
<pre><code>model = bt_model.deploy(initial_instance_count=1, endpoint_name=endpoint_name, instance_type='ml.m5.xlarge', serializer=JSONSerializer())
payload = {&quot;instances&quot;: terms}

response = model.predict(payload)

predictions = json.loads(response)
</code></pre>
<p>and it's working fine,  now I need to load the model's <code>bin</code> file using an <code>entry_point</code> in order to do some logic before and after predictions in the <code>input_fn</code> and <code>output_fn</code>.</p>
<p>I extracted the <code>bin</code> file from the <code>model.tar.gz</code> and I can load it, but I get <code>Segmentation Fault</code> when I try to run a prediction</p>
<pre><code>from gensim.models import FastText
from  gensim.models.fasttext import load_facebook_model, load_facebook_vectors

model=FastText.load('model.bin')
model.predict('hello world')
</code></pre>
<p>As per blazingText <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext.html"" rel=""nofollow noreferrer"">documentations</a></p>
<blockquote>
<p>For both supervised (text classification) and unsupervised (Word2Vec)
modes, the binaries (*.bin) produced by BlazingText can be
cross-consumed by fastText and vice versa. You can use binaries
produced by BlazingText by fastText. Likewise, you can host the model
binaries created with fastText using BlazingText.</p>
<p>Here is an example of how to use a model generated with BlazingText
with fastText:</p>
<p>#Download the model artifact from S3 aws s3 cp s3://&lt;YOUR_S3_BUCKET&gt;//model.tar.gz model.tar.gz</p>
<p>#Unzip the model archive tar -xzf model.tar.gz</p>
<p>#Use the model archive with fastText fasttext predict ./model.bin test.txt</p>
</blockquote>
<p>but for some reason it's not working as expected</p>",0,2,2022-08-12 11:24:28.927000 UTC,,,0,machine-learning|amazon-sagemaker|fasttext,52,2012-04-26 13:33:06.710000 UTC,2022-09-25 00:14:52.287000 UTC,Egypt,1972,701,11,547,,,,,,['amazon-sagemaker']
How to use ExitHandler with Kubeflow Pipelines SDK v2,"<p>I'm trying to move all my Kubeflow Pipelines from using the previous SDK v1 (<code>kfp</code>), to the newer <a href=""https://www.kubeflow.org/docs/components/pipelines/sdk-v2/"" rel=""nofollow noreferrer"">Pipelines SDK v2</a> (<code>kfp.v2</code>). I'm using version <code>1.8.12</code>.This refactoring have proved successful for almost all code, except for the <code>ExitHandler</code>, which still exists; <code>from kfp.v2.dsl import ExitHandler</code>. It seems like the previous way of compiling the pipeline object into a <code>tar.gz</code>-file using <code>kfp.compiler.Compiler().compile(pipeline, 'basic_pipeline.tar.gz')</code> file preserved some type of Argo placeholders, while the new <code>.json</code> pipelines using <code>compiler.Compiler().compile(pipeline_func=pipeline, package_path=&quot;basic-pipeline.json&quot;)</code> doesn't work the same way. Below, I will go into detail what works in Pipelines SDK v1 and how I've tried to implement it in v2.</p>
<p><strong>Previously</strong>, using Kubeflow Pipelines v1, I could use an ExitHandler as shown <a href=""https://stackoverflow.com/questions/57508382/kubeflow-pipeline-termination-notificaiton"">in this StackOverflow question</a> to eg. send a message to Slack when one of the pipeline components failed. I would define the pipeline as</p>
<pre class=""lang-py prettyprint-override""><code>import kfp.dsl as dsl

@dsl.pipeline(
    name='Basic-pipeline'
)
def pipeline(...):
    exit_task = dsl.ContainerOp(
        name='Exit handler that catches errors and post them in Slack',
        image='eu.gcr.io/.../send-error-msg-to-slack',
        arguments=[
                    'python3', 'main.py',
                    '--message', 'Basic-pipeline failed'
                    '--status', &quot;{{workflow.status}}&quot;
                  ]
    )
    with dsl.ExitHandler(exit_task):
        step_1 = dsl.ContainerOp(...)
        step_2 = dsl.ContainerOp(...) \
            .after(step_1)

if __name__ == '__main__':
    import kfp.compiler as compiler
    compiler.Compiler().compile(pipeline, 'basic_pipeline.tar.gz')
</code></pre>
<p>where the <code>exit_task</code> would send the <code>message</code> to our Slack if any of the steps of the pipeline failed. The code for the <code>exit_task</code> image looks like</p>
<pre class=""lang-py prettyprint-override""><code>import argparse

def get_args():
    parser = argparse.ArgumentParser()
    parser.add_argument('--message', type=str)
    parser.add_argument('--status', type=str)
    return parser.parse_known_args()

def main(FLAGS):
    def post_to_slack(msg):
        ...

    if FLAGS.status == &quot;Failed&quot;:
        post_to_slack(FLAGS.message)
    else:
        pass

if __name__ == '__main__':
    FLAGS, unparsed = get_args()
    main(FLAGS)
</code></pre>
<p>This worked, because the underlying Argo workflow could somehow understand the <code>&quot;{{workflow.status}}&quot;</code> notion.</p>
<p><strong>However</strong>, I'm now trying to use Vertex AI to run the pipeline, leveraging the Kubeflow Pipelines SDK v2, <code>kfp.v2</code>. Using the same exit-handler image as before, <code>'eu.gcr.io/.../send-error-msg-to-slack'</code>, I now define a yaml component file (<code>exit_handler.yaml</code>) instead,</p>
<pre class=""lang-yaml prettyprint-override""><code>name: Exit handler
description: Prints to Slack if any step of the pipeline fails

inputs:
  - {name: message, type: String}
  - {name: status, type: String}

implementation:
  container:
    image: eu.gcr.io/.../send-error-msg-to-slack
    command: [
      python3,
      main.py,
      --message, {inputValue: message},
      --status, {inputValue: status}
    ]
</code></pre>
<p>The pipeline code now looks like this instead,</p>
<pre class=""lang-py prettyprint-override""><code>from google.cloud import aiplatform
from google.cloud.aiplatform import pipeline_jobs
from kfp.v2 import compiler
from kfp.v2.dsl import pipeline, ExitHandler
from kfp.components import load_component_from_file

@pipeline(name=&quot;Basic-pipeline&quot;,
          pipeline_root='gs://.../basic-pipeline')
def pipeline():
    exit_handler_spec = load_component_from_file('./exit_handler.yaml')
    exit_handler = exit_handler_spec(
        message=&quot;Basic pipeline failed.&quot;,
        status=&quot;{{workflow.status}}&quot;
    )
    with ExitHandler(exit_handler):
        step_0_spec = load_component_from_file('./comp_0.yaml')
        step0 = step_0_spec(...)

        step_1_spec = load_component_from_file('./comp_1.yaml')
        step1 = step_1_spec(...) \
            .after(step0)

if __name__ == '__main__':
    compiler.Compiler().compile(
        pipeline_func=pipeline,
        package_path=&quot;basic-pipeline.json&quot;
    )
    from google.oauth2 import service_account
    credentials = service_account.Credentials.from_service_account_file(&quot;./my-key.json&quot;)
    aiplatform.init(project='bsg-personalization',
                    location='europe-west4',
                    credentials=credentials)

    job = pipeline_jobs.PipelineJob(
        display_name=&quot;basic-pipeline&quot;,
        template_path=&quot;basic-pipeline.json&quot;,
        parameter_values={...}
    )
    job.run()

</code></pre>
<p>This &quot;works&quot; (no exceptions) to compile and run, but the ExitHandler code interprets the <code>status</code> as a string with value {{workflow.status}}, which is also indicated by the compiled pipeline json generated from the code above (<code>basic-pipeline.json</code>), which you can see below (<code>&quot;stringValue&quot;: &quot;{{workflow.status}}&quot;</code>):</p>
<pre class=""lang-json prettyprint-override""><code>...
         &quot;exit-handler&quot;: {
            &quot;componentRef&quot;: {
              &quot;name&quot;: &quot;comp-exit-handler&quot;
            },
            &quot;dependentTasks&quot;: [
              &quot;exit-handler-1&quot;
            ],
            &quot;inputs&quot;: {
              &quot;parameters&quot;: {
                &quot;message&quot;: {
                  &quot;runtimeValue&quot;: {
                    &quot;constantValue&quot;: {
                      &quot;stringValue&quot;: &quot;Basic pipeline failed.&quot;
                    }
                  }
                },
                &quot;status&quot;: {
                  &quot;runtimeValue&quot;: {
                    &quot;constantValue&quot;: {
                      &quot;stringValue&quot;: &quot;{{workflow.status}}&quot;
                    }
                  }
                }
              }
            },
            &quot;taskInfo&quot;: {
              &quot;name&quot;: &quot;exit-handler&quot;
            },
            &quot;triggerPolicy&quot;: {
              &quot;strategy&quot;: &quot;ALL_UPSTREAM_TASKS_COMPLETED&quot;
            }
          }
...
</code></pre>
<p><strong>Any idea</strong> of how I can refactor my old <code>ExitHandler</code> code using v1 to the new SDK v2, to make the exit handler understand <strong>if the status of my pipeline is failed or not</strong>?</p>",0,2,2022-04-23 14:47:30.873000 UTC,,2022-04-24 08:37:27.133000 UTC,1,kubeflow|argo-workflows|google-cloud-vertex-ai|kubeflow-pipelines|kfp,554,2019-07-10 09:28:47.333000 UTC,2022-09-23 09:47:17.127000 UTC,"Stockholm, Sverige",803,123,7,73,,,,,,['google-cloud-vertex-ai']
how to predict more multiple values in azure ml?,<p>I am creating Azure ML experienment to predict multiple values. but in azure ml we can not train a model to predict multiple values. my question is how to bring multiple trained models in single experienment and create webout put that gives me multiple prediction.</p>,1,0,2018-02-01 10:13:16.490000 UTC,,2018-02-07 09:27:38.060000 UTC,0,azure|azure-machine-learning-studio,666,2018-02-01 09:59:53.607000 UTC,2018-05-07 08:02:09.147000 UTC,,3,0,0,7,,,,,,['azure-machine-learning-studio']
"Error 403, Problem with importing image data in s3 to SageMaker Notebook","<p>I am working with SageMaker Notebook and image data in S3 bucket with name s3:///train/ and validate data in other dir.
I create an IAM Role and put previous specific bucket, in the notebook I load this bucket with:</p>
<pre><code>s3_train = 's3://&lt;BucketName&gt;/train'
train_data = sagemaker.session.s3_input(s3_train, distribution='FullyReplicated', 
                        content_type='application/x-image', s3_data_type='S3Prefix')
</code></pre>
<p>The same for train lst file, validation data and validation lst data, after create data channels with this:</p>
<pre><code>data_channels = {'train': train_data, 'validation': validation_data, 
                 'train_lst': train_data_lst, 'validation_lst': validation_data_lst}
</code></pre>
<p>After create a TensorFlow estimator, and finally in fit pass the data with this:</p>
<pre><code>tf_estimator.fit(inputs=data_channels, logs=True)
</code></pre>
<p>And return this menssage error:
<em>An error occurred (403) when calling the HeadObject operation: Forbidden</em></p>",2,1,2020-07-04 20:06:40.180000 UTC,,,0,python|amazon-web-services|tensorflow|amazon-s3|amazon-sagemaker,362,2020-06-30 03:52:09.130000 UTC,2021-12-24 19:50:54.610000 UTC,"Concepción, Chile",13,0,0,6,,,,,,['amazon-sagemaker']
"Model testing on AWS sagemaker ""could not convert string to float""","<p>The XGboost model was trained on AWS sagemaker and deployed successfully but I keep getting the following error: <strong>ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (415) from model with message ""could not convert string to float: "".</strong> 
Any thoughts?</p>

<pre><code>Test data is as following:
      size       mean
269   5600.0  17.499633
103   1754.0   9.270272
160   4968.0  14.080601
40       4.0  17.500000
266  36308.0  11.421855

test_data_array = test_data.drop(['mean'], axis=1).as_matrix()
test_data_array = np.array([np.float32(x) for x in test_data_array])
xgb_predictor.content_type = 'text/csv'
xgb_predictor.serializer = csv_serializer

def predict(data, rows=32):
    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))
    #print(split_array)
    predictions = ''

    for array in split_array:
        print(array[0], type(array[0]))
        predictions = ','.join([predictions, xgb_predictor.predict(array[0]).decode('utf-8')])

    return np.fromstring(predictions[1:], sep=',')

predictions = predict(test_data_array)
</code></pre>",1,1,2018-10-05 19:28:38.613000 UTC,,,3,data-science|amazon-sagemaker,1605,2012-06-04 21:07:02.563000 UTC,2021-01-22 16:23:41.947000 UTC,,355,4,0,44,,,,,,['amazon-sagemaker']
"Failing to run a training job in SageMaker - Unknown parameter in input: ""ProfilerRuleConfigurations""","<p>I tried to run a training job with the latest SageMaker SDK (2.24.5) in a clean virtual environment and am getting the error:</p>
<blockquote>
<p>botocore.exceptions.ParamValidationError: Parameter validation failed:
Unknown parameter in input: &quot;ProfilerRuleConfigurations&quot;, must be one
of: TrainingJobName, HyperParameters, AlgorithmSpecification, RoleArn,
InputDataConfig, OutputDataConfig, ResourceConfig, VpcConfig,
StoppingCondition, Tags, EnableNetworkIsolation,
EnableInterContainerTrafficEncryption, EnableManagedSpotTraining,
CheckpointConfig, DebugHookConfig, DebugRuleConfigurations,
TensorBoardOutputConfig, ExperimentConfig, ProfilerConfig</p>
</blockquote>",1,0,2021-02-21 07:13:35.487000 UTC,,,2,boto3|amazon-sagemaker,204,2009-06-12 12:07:58.037000 UTC,2022-09-24 01:04:37.123000 UTC,Israel,4932,158,4,405,,,,,,['amazon-sagemaker']
AWS Sagemaker Image Classification Hyper-parameters Configuration,"<p><strong>Image Classification Hyper-parameters Configuration</strong></p>

<p>I have use SageMaker built-in Image Classification to train model own datasets which contain raw images of three classes objects. Each class contains different phone model images like iphone6plus, iphone7plus and samsung s7edge.</p>

<p>Those images are captured by phone camera and then resize to 224*224 dimension for training. The total num of training samples are 1920 and num of classes is 3. After many times of training, the average model validation accuracy I get is 0.4 or lower which is not accurate. </p>

<p><a href=""https://i.stack.imgur.com/H0BGg.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/H0BGg.jpg"" alt=""enter image description here""></a></p>

<p>What is the recommendation values that I need to put in parameters in order to increase the model accuracy? This was my first time to train the model and I need some guidance on it. Thank you.</p>",2,0,2019-08-16 12:43:33.253000 UTC,,,0,image-processing|amazon-sagemaker|hyperparameters|resnet,273,2019-07-26 04:23:33.777000 UTC,2019-08-22 08:53:13.567000 UTC,,33,0,0,4,,,,,,['amazon-sagemaker']
MLFlow doesnt copy properly my artifacts to the mlruns folder when used in a container,"<p>I am logging a few models using MLflow. The corresponding scripts are executed in a docker container.</p>
<p>The command <code>mlflow.log_artifacts</code> doesn't seem to work properly since I can t see the corresponding files under the mlruns folder. The command doesn't return any error though.</p>
<p>EDIT1: after some further investigations, <strong>it seems that the problem arises whenever I mount the folder containing <code>mlruns/</code></strong> as a docker volume.
I did some tests with the example provided by the <a href=""https://github.com/mlflow/mlflow/blob/4bf54ce9ae22248d2692d2f981e776719a767acf/mlflow/tracking/fluent.py#L565"" rel=""nofollow noreferrer"">docstring</a> of <code>log_artifacts</code></p>
<pre><code>import os
import json
import mlflow
# Create some files to preserve as artifacts
features = &quot;rooms, zipcode, median_price, school_rating, transport&quot;
data = {&quot;state&quot;: &quot;TX&quot;, &quot;Available&quot;: 25, &quot;Type&quot;: &quot;Detached&quot;}
# Create couple of artifact files under the directory &quot;data&quot;
os.makedirs(&quot;data&quot;, exist_ok=True)
with open(&quot;data/data.json&quot;, 'w', encoding='utf-8') as f:
    json.dump(data, f, indent=2)
with open(&quot;data/features.txt&quot;, 'w') as f:
    f.write(features)
# Write all files in &quot;data&quot; to root artifact_uri/states
with mlflow.start_run():
    mlflow.log_artifacts(&quot;data&quot;, artifact_path=&quot;states&quot;)
</code></pre>
<p>If I run this in a <strong>container without volume mounting, it runs just fine</strong>, ie the artifacts appear under <code>mlruns/&lt;exp-id&gt;/&lt;run-id&gt;/artifacts/state</code></p>
<p>However If I run this in a container <strong>with the folder containing <code>mlruns/</code> mounted, it doesn't work,</strong> ie the folder <code>mlruns/&lt;exp-id&gt;/&lt;run-id&gt;/artifacts/</code> is empty.</p>",1,0,2021-09-13 09:00:07.203000 UTC,1.0,2021-09-15 16:07:11.070000 UTC,0,python|docker|mlflow,519,2016-02-02 12:29:08.173000 UTC,2022-09-23 16:13:35.343000 UTC,,515,72,2,68,,,,,,['mlflow']
Use PyTorch DistributedDataParallel with Hugging Face on Amazon SageMaker,"<p>Even for single-instance training, PyTorch DistributedDataParallel (DDP) is generally recommended over PyTorch DataParallel (DP) because DP's strategy is less performant and it uses more memory on the default device. (Per <a href=""https://discuss.pytorch.org/t/cuda-out-of-memory-error-when-using-multi-gpu/72333"" rel=""nofollow noreferrer"">this PyTorch forums thread</a>)</p>
<p>Hugging Face <a href=""https://github.com/huggingface/transformers/tree/master/examples/pytorch#distributed-training-and-mixed-precision"" rel=""nofollow noreferrer"">recommend</a> to run distributed training via the <code>python -m torch.distributed.launch</code> launcher, because their Trainer API supports DDP but will fall back to DP if you don't. (Per <a href=""https://discuss.huggingface.co/t/multi-gpu-training/4021"" rel=""nofollow noreferrer"">this HF forums thread</a>)</p>
<p>I recently ran in to this problem: scaling a HF training job from <code>p3.8xlarge</code> to <code>p3.16xlarge</code> increased memory consumption on (I think) one of the GPUs to the point where I had to significantly reduce batch size to avoid CUDA Out of Memory errors - basically losing all scaling advantage.</p>
<p>So the good news is for p3.16xl+ I can just <a href=""https://huggingface.co/docs/sagemaker/train#distributed-training"" rel=""nofollow noreferrer"">enable SageMaker Distributed Data Parallel</a> and the PyToch DLC will automatically <a href=""https://github.com/aws/sagemaker-pytorch-training-toolkit/blob/88ca48a831bf4f099d4c57f3c18e0ff92fa2b48c/src/sagemaker_pytorch_container/training.py#L23"" rel=""nofollow noreferrer"">launch via torch.distributed for me</a>.</p>
<p>The bad news for use cases with smaller workloads or wanting to test before they scale up, is that SMDistributed <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/data-parallel-intro.html#data-parallel-allreduce"" rel=""nofollow noreferrer"">doesn't support all multi-GPU instance types</a>. No p3.8xl or g series, for example. I did try manually setting the <code>sagemaker_distributed_dataparallel_enabled</code> environment variable, but no joy.</p>
<p>So how else can we launch HF Trainer scripts with PyTorch DDP on SageMaker?</p>",1,0,2022-09-08 09:03:22.657000 UTC,,,0,pytorch|amazon-sagemaker|huggingface-transformers,16,2022-09-08 07:14:26.503000 UTC,2022-09-23 21:03:19.637000 UTC,,48,2,0,6,,,,,,['amazon-sagemaker']
Difference in performance between Mlflow run and manual training,"<p>I'm working in Databricks trying to train a <code>XGBClassifier</code> model in an sklearn <code>Pipeline</code> using both manual code and within an Mlflow run. I have a training set <code>X_train</code>, a validation set <code>X_val</code>, and a test set <code>X_test</code>. When I run the model on my test set, the performance metrics for each method are different, with the Mlflow metrics being much better. Below is my code for the manual fitting and testing (preprocessor is a standard scaler and one-hot encoder):</p>
<pre><code>model = Pipeline([
    (&quot;column_selector&quot;, col_selector),
    (&quot;preprocessor&quot;, preprocessor),
    (&quot;classifier&quot;, xgb_model),
])

model.fit(X_train, y_train, classifier__early_stopping_rounds=5,
classifier__eval_set=[(X_val_processed,y_val)], classifier__verbose=False)

y_pred = model.predict(X_test)
y_probs = model.predict_proba(X_test)

from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, log_loss

test_acc = accuracy_score(y_test, y_pred)
test_f1 = f1_score(y_test, y_pred)
test_prec = precision_score(y_test, y_pred)
test_rec = recall_score(y_test, y_pred)
test_logloss = log_loss(y_test, y_pred)
test_rocauc = roc_auc_score(y_test, y_probs[:, 1])
metric_cols = ['accuracy', 'f1_score', 'precision', 'recall', 'log_loss', 'roc_auc']
pd.DataFrame([[test_acc, test_f1, test_prec, test_rec, test_logloss, test_rocauc]], columns=metric_cols, index=['test'])
</code></pre>
<p>Which gets me the following:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th></th>
<th>accuracy</th>
<th>f1_score</th>
<th>precision</th>
<th>recall</th>
<th>log_loss</th>
<th>roc_auc</th>
</tr>
</thead>
<tbody>
<tr>
<td>test</td>
<td>0.865784</td>
<td>0.326917</td>
<td>0.753635</td>
<td>0.208731</td>
<td>4.635675</td>
<td>0.738647</td>
</tr>
</tbody>
</table>
</div>
<p>Here is my code for the mlflow run:</p>
<pre><code>import mlflow
import databricks.automl_runtime

mlflow.sklearn.autolog(disable=True)

mlflow.sklearn.autolog(log_input_examples=True, silent=True)

with mlflow.start_run(experiment_id=&quot;363669966797473&quot;, run_name=&quot;xgb-mod-3-run&quot;) as mlflow_run:
    model.fit(X_train, y_train, classifier__early_stopping_rounds=5, classifier__eval_set=[(X_val_processed,y_val)], classifier__verbose=False)

    xgbc_test_metrics = mlflow.sklearn.eval_and_log_metrics(model, X_test, y_test, prefix=&quot;test_&quot;)

    xgbc_test_metrics = {k.replace(&quot;test_&quot;, &quot;&quot;): v for k, v in xgbc_test_metrics.items()}

    display(pd.DataFrame([xgbc_test_metrics], index=[&quot;test&quot;]))
</code></pre>
<p>After which I get much better results for precision, recall, f1_score, and log_loss:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th></th>
<th>accuracy</th>
<th>f1_score</th>
<th>precision</th>
<th>recall</th>
<th>log_loss</th>
<th>roc_auc</th>
</tr>
</thead>
<tbody>
<tr>
<td>test</td>
<td>0.865784</td>
<td>0.83134</td>
<td>0.85255</td>
<td>0.86578</td>
<td>0.36501</td>
<td>0.738647</td>
</tr>
</tbody>
</table>
</div>
<p>While accuracy and ROC AUC remain unchanged.</p>
<p>What is causing this difference in performance? Is Mlflow performing any sort of tuning/optimization under the hood? Is the experiment run actually fitting multiple models other than the <code>xgb_model</code> I instantiated? Let me know if I need to provide more code.</p>",0,1,2022-06-23 15:18:29.287000 UTC,,,0,python|machine-learning|databricks|mlflow|xgbclassifier,65,2021-06-15 19:55:08.787000 UTC,2022-07-06 21:04:46.540000 UTC,,23,3,0,2,,,,,,['mlflow']
How to transfer data from Azure ML (Notebooks) to a storage container,"<p>I need to transfer a file from my Azure ML workspace(notebooks folder) to a storage container. Tried this in jupyter notebook;</p>
<pre><code>import azureml.core
from azureml.core import Workspace, Datastore
import json

subscription_id = 'key1'
resource_group = 'rg_grp'
workspace_name = 'dev'

workspace = Workspace(subscription_id, resource_group, workspace_name)

# get the datastore to upload prepared data
    datastore = Datastore.get(workspace, datastore_name='input')  # using an existing mapped datastore in azure ML

# upload the local file from src_dir to the target_path in datastore

datastore.upload_files(['./folder1/output.csv'], relative_root='folder1', target_path='folder1', overwrite=True, show_progress=True)
</code></pre>
<p>As soon as I run the block of code to upload, I get this error msg,</p>
<pre><code>UserErrorException: UserErrorException:
    Message: './folder1/output.csv' does not point to a file. Please upload the file to cloud-first if running in a cloud notebook.
    InnerException None
    ErrorResponse 
{
    &quot;error&quot;: {
        &quot;code&quot;: &quot;UserError&quot;,
        &quot;message&quot;: &quot;'./folder1/output.csv' does not point to a file. Please upload the file to cloud first if running in a cloud notebook.&quot;
    }
}
</code></pre>
<p>CSV file is already in my notebook.</p>
<p>Any help would be appreciated</p>
<p>Regards</p>",1,0,2021-06-09 05:09:39.720000 UTC,,,0,python-3.x|azure|azure-container-service|azure-machine-learning-service|azureml-python-sdk,610,2021-02-08 22:15:16.840000 UTC,2022-08-24 01:24:13.723000 UTC,Australia,429,36,2,42,,,,,,['azure-machine-learning-service']
MLflow - How can I run python code using a REST API,"<p>I'am a newbie on MachineLearning. Just a simple question, how can I run python code using REST API?
Here is the documentation
<a href=""https://mlflow.org/docs/latest/rest-api.html"" rel=""nofollow noreferrer"">https://mlflow.org/docs/latest/rest-api.html</a>
But there are no examples for REST API.
I just created an experiment, but I cant create python run?
Any examples like this? &quot;This is create just a experiment&quot;
curl -X POST http://localhost:5000/api/2.0/preview/mlflow/experiments/create -d '{&quot;name&quot;:&quot;TEST&quot;}'</p>",0,0,2020-07-01 22:22:02.807000 UTC,,2020-07-01 22:28:41.280000 UTC,2,machine-learning|artificial-intelligence|mlflow,494,2020-07-01 22:08:39.837000 UTC,2022-09-14 19:48:23.993000 UTC,Turkey,21,0,0,0,,,,,,['mlflow']
AWS Sagemaker - Training on Video Stills v Images,"<p>I am using sagemaker, GroundTruth, to build training data. The data will then be used to create model for object detection across videos. I want to understand the best way of doing this.</p>
<p>At this stage I am going to using humans (our team) to label stills to mark out the objects.</p>
<p>Questions:</p>
<ol>
<li>Data Format: What is the difference between uploading frames and then selecting the data format as &quot;video frame&quot; v &quot;images&quot;?  Aren't they both images - so does it make a difference to the training set up?</li>
</ol>
<p><a href=""https://i.stack.imgur.com/fUxwL.jpg"" rel=""nofollow noreferrer"">Sage Maker Data Format </a></p>
<ol start=""2"">
<li>Training Now v Later: At the moment we have capacity to be training the data set, though we are a long way off building the model. We know what labels we are going to be using what we are detecting. Is there any reason why we can't do the training now? Would it be a waste of resources -without knowing more about the future model?</li>
</ol>
<p>Thanks</p>",1,0,2021-06-03 15:44:52.533000 UTC,,2021-06-04 19:36:53.627000 UTC,0,amazon-web-services|amazon-sagemaker,33,2020-06-02 06:57:28.890000 UTC,2021-06-20 10:44:36.057000 UTC,London,3,0,0,2,,,,,,['amazon-sagemaker']
How to Edit Sagemaker Labeling Shortcut Tab?,"<p>I'm able to change the instructions in the SageMaker GroundTruth labeling user interface as in the picture, but I can't edit the shortcuts tab. I can simply edit inside the 'full-instructions' or 'short-instructions' tags in the html template, but there is no such tag for 'shortcuts' as far I see from my online search and going through sample templates. How can I update the shortcuts to reflect the new shortcuts I added?</p>
<p>You can see the Instructions tab updated in the top partial screenshot, but I still don't know how to update the Shortcuts tab.</p>
<p><a href=""https://i.stack.imgur.com/6UFel.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6UFel.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/FzfEA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FzfEA.png"" alt=""enter image description here"" /></a></p>",0,0,2021-10-08 19:20:55.583000 UTC,,2021-10-09 00:35:10.677000 UTC,1,amazon-sagemaker|amazon-ground-truth,53,2013-03-18 04:35:21.770000 UTC,2022-09-22 09:38:42.343000 UTC,California,345,118,0,59,,,,,,['amazon-sagemaker']
How to configure comet (comet.ml) to create pull requests on GitHub?,"<p>Ive followed this this to link my comet.ml project to GitHub - <a href=""https://github.com/comet-ml/comet-quickstart-guide/blob/master/github-pullrequest/README.md"" rel=""nofollow noreferrer"">link</a></p>

<p>and had some models already trained (using keras)in my project</p>

<p><a href=""https://i.stack.imgur.com/L44Bi.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/L44Bi.png"" alt=""enter image description here""></a></p>

<p>Ive linked my GitHub account and when creating a pull request I get error </p>

<p><strong>Cant create pull request</strong></p>

<p><a href=""https://i.stack.imgur.com/zlkYy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zlkYy.png"" alt=""enter image description here""></a></p>

<p>please advise</p>",1,0,2017-12-17 12:47:14.923000 UTC,1.0,,1,github|keras|comet-ml,73,2014-01-07 10:14:00.833000 UTC,2017-12-17 12:38:52.643000 UTC,,303,35,0,33,,,,,,['comet-ml']
Semantic Segmentation failing in SageMaker using AugmentedManifest,"<p>I'm using a augmented manifest, with all labelling done using mTurk, and I'm trying to train a model using this files.</p>
<p>I have a Jupyter Notebook, Python 3.7 and TensorFlow 2.</p>
<p>First, I do some basic initializations and configure the manifest file location.</p>
<pre><code>import boto3
import re
import sagemaker
from sagemaker import get_execution_role
import time
from time import gmtime, strftime
import json

role = get_execution_role()
sess = sagemaker.Session()
s3 = boto3.resource(&quot;s3&quot;)

training_image = sagemaker.amazon.amazon_estimator.image_uris.retrieve(
    &quot;semantic-segmentation&quot;, boto3.Session().region_name
)

augmented_manifest_filename_train = (
    &quot;output.manifest&quot;
)
bucket_name = &quot;&lt;private&gt;&quot;  

s3_output_path = &quot;s3://{}/output&quot;.format(bucket_name)
s3_train_data_path = &quot;s3://{}/output/trees-and-houses/manifests/output/{}&quot;.format(
    bucket_name, augmented_manifest_filename_train)

augmented_manifest_s3_key = s3_train_data_path.split(bucket_name)[1][1:]
s3_obj = s3.Object(bucket_name, augmented_manifest_s3_key)
augmented_manifest = s3_obj.get()[&quot;Body&quot;].read().decode(&quot;utf-8&quot;)
augmented_manifest_lines = augmented_manifest.split(&quot;\n&quot;)
num_training_samples = len(augmented_manifest_lines)
</code></pre>
<p>All this works well.
I can print my manifest files and see its attributes.
Then, I configure the job:</p>
<pre><code># Create unique job name
job_name_prefix = &quot;groundtruth-augmented-manifest-demo&quot;
timestamp = time.strftime(&quot;-%Y-%m-%d-%H-%M-%S&quot;, time.gmtime())
job_name = job_name_prefix + timestamp
s3_output_location = &quot;s3://{}/training_outputs/&quot;.format(bucket_name)
</code></pre>
<p>And create the estimator plus hyperparameters</p>
<pre><code># Create a model object set to using &quot;Pipe&quot; mode.
model = sagemaker.estimator.Estimator(training_image,
                                      role,
                                      instance_count=1,
                                      instance_type='ml.p3.8xlarge',
                                      volume_size = 50,
                                      max_run = 360000,
                                      input_mode = 'Pipe',
                                      output_path=s3_output_location,
                                      job_name=job_name,
                                      sagemaker_session=sess)

model.set_hyperparameters(
    backbone=&quot;resnet-101&quot;,
    algorithm=&quot;psp&quot;,
    use_pretrained_model=&quot;False&quot;, 
    crop_size=240,
    num_classes=3,
    epochs=10,
    base_size=540,
    learning_rate=0.0001,
    optimizer=&quot;rmsprop&quot;,
    lr_scheduler=&quot;poly&quot;,
    mini_batch_size=4,
    early_stopping=True,
    early_stopping_patience=2,
    early_stopping_min_epochs=10,
    num_training_samples=num_training_samples
)
</code></pre>
<p>Finally, since my files are quite large, I use 'Pipe' training input.</p>
<pre><code># Create a train data channel with S3_data_type as 'AugmentedManifestFile' and attribute names.
train_data = sagemaker.inputs.TrainingInput(s3_data= s3_train_data_path,
                                        distribution='FullyReplicated',
                                        content_type='application/x-recordio',
                                        s3_data_type='AugmentedManifestFile',
                                        compression='Gzip',
                                        attribute_names=attribute_names,
                                        input_mode='Pipe',
                                        record_wrapping='RecordIO') 
data_channels = {'train': train_data }
</code></pre>
<p>Lastly, I try to train my model, just like AWS' example. Since I'm using an augmented manifest, I'm not supposed to need a validation channel.</p>
<pre><code># Train a model.
model.fit(inputs=data_channels, logs=True, wait=True)
</code></pre>
<p>However, I get the following error when starting to train:</p>
<pre><code>UnexpectedStatusException: Error for Training job semantic-segmentation-2021-05-28-23-53-46-966: Failed. Reason: ClientError: Unable to initialize the algorithm. Failed to validate input data configuration. (caused by ValidationError)

Caused by: 'validation' is a required property

Failed validating 'required' in schema['allOf'][2]:
    {'required': ['validation']}

On instance:
    {'train': {'ContentType': 'application/x-recordio',
               'RecordWrapperType': 'RecordIO',
               'S3DistributionType': 'FullyReplicated',
               'TrainingInputMode': 'Pipe'}}
</code></pre>",1,0,2021-05-29 00:10:07.653000 UTC,,2021-05-29 00:19:42.160000 UTC,0,tensorflow|amazon-sagemaker|semantic-segmentation,312,2011-10-03 17:16:32.897000 UTC,2021-10-11 10:46:27.180000 UTC,,9950,177,16,701,,,,,,['amazon-sagemaker']
How can install additional python packages in Azure Machine Learning Studio?,"<p>Hey I am trying to install <code>dash</code> and <code>jupyter-dash</code> in Azure Machine Learning Studio (New- not classic). I opened terminal and installed them using <code>pip install</code> and they successully were installed upon checking with <code>pip freeze</code>. However when I open notebook I still get error:
<code>ModuleNotFoundError</code>.</p>
<p>Is there a way to add those missing libraries that I need in order to run the script in my notebook?</p>",1,4,2021-12-09 15:10:14.880000 UTC,,,0,python|azure|jupyter-notebook|azure-machine-learning-studio,926,2016-07-29 09:35:18.373000 UTC,2022-09-24 18:24:05.530000 UTC,,3261,236,7,412,,,,,,['azure-machine-learning-studio']
How to edit Azure ML entry or scoring script file once it is deployed in aks or aci,"<p>**I have deployed model endpoint in aci way back 1 month , now i want to change few thing in entry script for same model , so how i can do that without changing the restendpoint .?</p>
<p>My Entry script looks like below :**</p>
<pre><code>SENTIMENT_THRESHOLDS = (0.4, 0.7)
SEQUENCE_LENGTH = 300
def run(data):
try:
    # Pick out the text property of the JSON request.
    # This expects a request in the form of {&quot;text&quot;: &quot;some text to score for sentiment&quot;}
    data = json.loads(data)
    prediction = predict(data['text'])
    #Return prediction
    return prediction
except Exception as e:
    error = str(e)
    return error
</code></pre>
<p>**Now i want to change the variable SEQUENCE_LENGTH and update the restendpoint with this entry script file **</p>",1,0,2021-04-04 18:24:14.903000 UTC,,,1,azure|machine-learning|azure-aks|azure-machine-learning-service|azure-container-instances,471,2015-03-14 04:45:18.630000 UTC,2022-02-14 04:44:37.810000 UTC,,179,5,0,44,,,,,,['azure-machine-learning-service']
Concurrent AzureML REST requests fail with Too many requests for service (overloaded),"<p>I have deployed my model to a production Azure Kubernetes Service with 6 nodes.</p>
<p>Sequential inference requests get the expected response from score.py.</p>
<p>When I more than one concurrent async inference requests all the requests except for the first return 503 <code>Too many requests for service {my service name} (overloaded)</code>.</p>
<p>I built my service and deployed my model based on the example @ <a href=""https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/deployment/production-deploy-to-aks/production-deploy-to-aks.ipynb"" rel=""nofollow noreferrer"">https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/deployment/production-deploy-to-aks/production-deploy-to-aks.ipynb</a>.</p>
<p>I am sending requests as large as 4mb.  It seems to work when I send trivially small requests.</p>",1,0,2020-11-02 21:25:48.667000 UTC,,2020-11-04 14:15:10.543000 UTC,2,azure-aks|azure-machine-learning-service,457,2010-12-29 01:52:20.030000 UTC,2022-09-23 21:20:09.273000 UTC,"Boston, MA",722,84,2,41,,,,,,['azure-machine-learning-service']
Connecting Julia to Weights & Biases over Python,"<p>I am trying to use weights&amp;biases for my models written in Julia. I am using <code>WeightsAndBiasLogger.jl</code> and try to test their demo code:</p>
<pre><code>using Logging, WeightsAndBiasLogger

args = (n_epochs=1_000, lr=1e-3)
logger = WBLogger(project=&quot;sample-project&quot;)
config!(logger, args)

with(logger) do
    loss = 0
    for i in 1:args.n_epochs
        loss += randn() * args.lr
        @info &quot;train&quot; i=i loss=loss
    end
end
</code></pre>
<p>I receive an error: <strong>&quot;ArgumentError: ref of NULL PyObject&quot;</strong> (considering the line: logger = WBLogger(project=&quot;sample-project&quot;)
)</p>
<p>Then I tried to fix this with the following command:</p>
<pre><code>using Logging, WeightsAndBiasLogger, PyCall

args = (n_epochs=1_000, lr=1e-3)

const logger = PyNULL()
function __init__()
    copy!(logger, WBLogger(project=&quot;sample-project&quot;))
end

config!(logger, args)

with(logger) do
    loss = 0
    for i in 1:args.n_epochs
        loss += randn() * args.lr
        @info &quot;train&quot; i=i loss=loss
    end
end
</code></pre>
<p>It creates the <code>logger</code> object, but now the error is:</p>
<p><strong>MethodError: no method matching config!(::PyObject, ::NamedTuple{(:n_epochs, :lr), Tuple{Int64, Float64}})
Closest candidates are: config!(!Matched::WBLogger, ::Any; kwargs...)</strong> (this consider the line: config!()...</p>
<p>So, does anyone know how to solve the issue? Obviously, I am new to Julia, thus I apologize if asking something very stupid. In addition, if you know a better solution to integrate Julia into W&amp;B or any good alternatives, I would be glad to hear it.</p>
<p>PS: Julia ver 1.7.2</p>",0,2,2022-02-22 19:53:40.073000 UTC,,,2,python|julia|pycall|wandb,102,2018-09-03 01:38:16.457000 UTC,2022-09-24 20:59:24.283000 UTC,"Roskilde, Denmark",1443,339,26,122,,,,,,['wandb']
Can you add widgets to SageMaker Notebooks similar to Azure DataBricks?,"<p>I have used Azure DataBricks in my earlier job and it comes with extended support for Notebook widgets to execute notebook manually/ commission a notebook job by selecting some values (Ideally your run state params or variables.)</p>
<p>For information here: <a href=""https://docs.microsoft.com/en-us/azure/databricks/notebooks/widgets"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/databricks/notebooks/widgets</a></p>
<p>Is there a similar service or option that I can build while working on SageMaker notebooks?</p>",1,0,2022-07-14 02:59:32.897000 UTC,,,0,jupyter-notebook|azure-databricks|amazon-sagemaker,45,2020-02-23 07:50:57.040000 UTC,2022-09-16 18:42:35.310000 UTC,,1,0,0,4,,,,,,['amazon-sagemaker']
Does not have BatchGetImage permission - While deploying model in sagemaker,"<p><strong>I am Able to train my modelusing Sagemaker TensorFlow container.</strong></p>
<p><strong>Below is the code:</strong></p>
<pre><code>model_dir = '/opt/ml/model'
train_instance_type = 'ml.c4.xlarge'
hyperparameters = {'epochs': 10, 'batch_size': 256, 'learning_rate': 0.001}

script_mode_estimator = TensorFlow(
    entry_point='model.py',
    train_instance_type=train_instance_type,
    train_instance_count=1,
    model_dir=model_dir,
    hyperparameters=hyperparameters,
    role=sagemaker.get_execution_role(),
    base_job_name='tf-fashion-mnist',
    framework_version='1.12.0',
    py_version='py3',
    output_path='s3://my_bucket/testing',
    script_mode=True
)
</code></pre>
<p><strong>Model Fitting:</strong></p>
<pre><code>script_mode_estimator.fit(inputs)
</code></pre>
<p><strong>But when i ama trying to deploy model i ama getting this below error:</strong></p>
<p><strong>Deploy code is:</strong></p>
<pre><code>script_mode_d=script_mode_estimator.deploy(initial_instance_count=1,
                 instance_type=&quot;ml.m4.xlarge&quot;)
</code></pre>
<p><strong>Error is:</strong></p>
<p>UnexpectedStatusException: Error hosting endpoint tf-fashion-mnist-2020-09-23-09-05-25-791: Failed. Reason:  The role 'xyz' does not have BatchGetImage permission for the image: '520713654638.dkr.ecr.us-east-1.amazonaws.com/sagemaker-tensorflow-serving:1.12-cpu'.</p>
<p><strong>Please help me to resolve this issue.</strong></p>",2,4,2020-09-24 05:17:50.507000 UTC,1.0,,2,amazon-web-services|tensorflow|amazon-s3|deployment|amazon-sagemaker,1045,2019-01-19 12:40:10.847000 UTC,2022-07-21 10:22:14.223000 UTC,"Bangalore, Karnataka, India",41,0,0,14,,,,,,['amazon-sagemaker']
How to upload images in a class-specific folders to Azure Blob Storage on Azure ML Studio,"<p>I am trying to run an image classifier using Keras on Azure ML Studio. However, I have huge amount of images sorted into class-specific subfolders on my local machine that I'd like to upload to Azure Blob Storage. When I use &quot;Create Dataset&quot; there is an option to upload the entire folder but it removes all subfolders, uploads only images in each subfolder and dumps them into the main folder. The reason I want to keep the subfolders is that each subfolder is a class and I want to use Keras flow_from_directory().
So, the folder structure I want to upload images should be as follows:</p>
<pre><code>- Main_folder
  --- subfolder1
  --- subfolder2
  --- subfolder3
  --- subfolder4
  --- subfolder5
        .
        .
        .
  --- subfolderN
</code></pre>
<p>Does anyone know how to do this?
Any other alternatives would be appreciated as well</p>",1,0,2021-08-25 21:57:39.103000 UTC,,,2,azure|keras|azure-machine-learning-studio|image-classification,72,2020-12-15 19:54:05.733000 UTC,2022-05-18 20:54:24.423000 UTC,,51,0,0,6,,,,,,['azure-machine-learning-studio']
Problem with init() function for model deployment in Azure,"<p>I want to deploy model in Azure but I'm struggling with the following problem.</p>
<p>I have my model registered in Azure. The file with extension .sav is located locally. The registration looks the following:</p>
<pre><code>import urllib.request
from azureml.core.model import Model

# Register model
model = Model.register(ws, model_name=&quot;my_model_name.sav&quot;, model_path=&quot;model/&quot;) 
</code></pre>
<p>I have my <code>score.py</code> file. The <code>init()</code> function in the file looks like this:</p>
<pre><code>import json
import numpy as np
import pandas as pd
import os
import pickle
from azureml.core.model import Model

 def init():
    
    global model
    model_path = Model.get_model_path(model_name = 'my_model_name.sav', _workspace='workspace_name')
    model = pickle(open(model_path, 'rb'))
</code></pre>
<p>But when I try to deploy I se the following error:</p>
<pre><code>&quot;code&quot;: &quot;AciDeploymentFailed&quot;,
  &quot;statusCode&quot;: 400,
  &quot;message&quot;: &quot;Aci Deployment failed with exception: Your container application crashed. This may be caused by errors in your scoring file's init() function.
    1. Please check the logs for your container instance: leak-tester-pm. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs.
</code></pre>
<p>And when I run <code>print(service.logs())</code> I have the following output (I have only one model registered in Azure):</p>
<pre><code>None
</code></pre>
<p>Am I doing something wrong with loading  model in score.py file?</p>
<p>P.S. The .yml file for the deployment:</p>
<pre><code>name: project_environment
dependencies:
  # The python interpreter version.
  # Currently Azure ML only supports 3.5.2 and later.
- python=3.6.2

- pip:
  - scikit-learn==0.24.2
  - azureml-defaults
  - numpy
  - pickle-mixin
  - pandas
  - xgboost
  - azure-ml-api-sdk
channels:
- anaconda
- conda-forge
</code></pre>",2,0,2022-04-15 09:04:33.623000 UTC,,,0,python|azure|azure-machine-learning-studio,271,2019-07-30 13:42:54.517000 UTC,2022-09-21 08:53:39.343000 UTC,"Wrocław, Польша",73,22,0,33,,,,,,['azure-machine-learning-studio']
Data bricks:- Cannot display the predicted output by using ml flow registered model,"<p>I have created a model using diabetes dataset for prediction. I have trained, evaluated, logged and registered it as a new model in ML flow. Now I am trying to load the registered model and trying to predict on new data. All though I was able to predict the results. I am not able to display it. When I try to display using command <code>.show()</code> or <code>display()</code> it is throwing an error. What is the cause of the error? and How do I display the results?</p>
<p>Note: I have programmed using pure pyspark and all the ML flow operation was done on Data bricks</p>
<p>Code:-</p>
<pre><code>model_details = mlflow.tracking.MlflowClient().get_latest_versions('model1',stages=['staging'])[0]
model = mlflow.pyfunc.spark_udf(spark,model_details.source)
input_df = sdf.drop('progression')
columns = list(map(lambda c: f&quot;{c}&quot;, input_df.columns))
df = input_df.withColumn(&quot;progression&quot;, model(*columns))
df.show(truncate=False)
</code></pre>
<p>Error :-</p>
<pre><code>PythonException: An exception was thrown from a UDF: 'Exception: Java gateway process exited before sending its port number'. Full traceback below:
PythonException                           Traceback (most recent call last)
&lt;command-1343735193245452&gt; in &lt;module&gt;
     34 df = input_df.withColumn(&quot;progression&quot;, model(*columns))
     35 
---&gt; 36 df.show(truncate=False)

/databricks/spark/python/pyspark/sql/dataframe.py in show(self, n, truncate, vertical)
    441             print(self._jdf.showString(n, 20, vertical))
    442         else:
--&gt; 443             print(self._jdf.showString(n, int(truncate), vertical))
    444 
    445     def __repr__(self):

/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py in __call__(self, *args)
   1303         answer = self.gateway_client.send_command(command)
   1304         return_value = get_return_value(
-&gt; 1305             answer, self.gateway_client, self.target_id, self.name)
   1306 
   1307         for temp_arg in temp_args:

/databricks/spark/python/pyspark/sql/utils.py in deco(*a, **kw)
    131                 # Hide where the exception came from that shows a non-Pythonic
    132                 # JVM exception message.
--&gt; 133                 raise_from(converted)
    134             else:
    135                 raise

/databricks/spark/python/pyspark/sql/utils.py in raise_from(e)

PythonException: An exception was thrown from a UDF: 'Exception: Java gateway process exited before sending its port number'. Full traceback below:
Traceback (most recent call last):
  File &quot;/databricks/spark/python/pyspark/worker.py&quot;, line 654, in main
    process()
  File &quot;/databricks/spark/python/pyspark/worker.py&quot;, line 646, in process
    serializer.dump_stream(out_iter, outfile)
  File &quot;/databricks/spark/python/pyspark/sql/pandas/serializers.py&quot;, line 281, in dump_stream
    timely_flush_timeout_ms=self.timely_flush_timeout_ms)
  File &quot;/databricks/spark/python/pyspark/sql/pandas/serializers.py&quot;, line 97, in dump_stream
    for batch in iterator:
  File &quot;/databricks/spark/python/pyspark/sql/pandas/serializers.py&quot;, line 271, in init_stream_yield_batches
    for series in iterator:
  File &quot;/databricks/spark/python/pyspark/worker.py&quot;, line 467, in mapper
    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)
  File &quot;/databricks/spark/python/pyspark/worker.py&quot;, line 467, in &lt;genexpr&gt;
    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)
  File &quot;/databricks/spark/python/pyspark/worker.py&quot;, line 111, in &lt;lambda&gt;
    verify_result_type(f(*a)), len(a[0])), arrow_return_type)
  File &quot;/databricks/spark/python/pyspark/util.py&quot;, line 109, in wrapper
    return f(*args, **kwargs)
  File &quot;/databricks/python/lib/python3.7/site-packages/mlflow/pyfunc/__init__.py&quot;, line 827, in predict
    model = SparkModelCache.get_or_load(archive_path)
  File &quot;/databricks/python/lib/python3.7/site-packages/mlflow/pyfunc/spark_model_cache.py&quot;, line 64, in get_or_load
    SparkModelCache._models[archive_path] = load_pyfunc(temp_dir)
  File &quot;/databricks/python/lib/python3.7/site-packages/mlflow/utils/annotations.py&quot;, line 43, in deprecated_func
    return func(*args, **kwargs)
  File &quot;/databricks/python/lib/python3.7/site-packages/mlflow/pyfunc/__init__.py&quot;, line 693, in load_pyfunc
    return load_model(model_uri, suppress_warnings)
  File &quot;/databricks/python/lib/python3.7/site-packages/mlflow/pyfunc/__init__.py&quot;, line 667, in load_model
    model_impl = importlib.import_module(conf[MAIN])._load_pyfunc(data_path)
  File &quot;/databricks/python/lib/python3.7/site-packages/mlflow/spark.py&quot;, line 707, in _load_pyfunc
    .master(&quot;local[1]&quot;)
  File &quot;/databricks/spark/python/pyspark/sql/session.py&quot;, line 189, in getOrCreate
    sc = SparkContext.getOrCreate(sparkConf)
  File &quot;/databricks/spark/python/pyspark/context.py&quot;, line 384, in getOrCreate
    SparkContext(conf=conf or SparkConf())
  File &quot;/databricks/spark/python/pyspark/context.py&quot;, line 134, in __init__
    SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)
  File &quot;/databricks/spark/python/pyspark/context.py&quot;, line 333, in _ensure_initialized
    SparkContext._gateway = gateway or launch_gateway(conf)
  File &quot;/databricks/spark/python/pyspark/java_gateway.py&quot;, line 105, in launch_gateway
    raise Exception(&quot;Java gateway process exited before sending its port number&quot;)
Exception: Java gateway process exited before sending its port number
</code></pre>",0,0,2021-09-14 10:18:11.113000 UTC,,,1,pyspark|apache-spark-sql|user-defined-functions|databricks|mlflow,168,2021-08-10 12:49:57.537000 UTC,2021-11-22 17:00:30.000000 UTC,,111,12,0,18,,,,,,['mlflow']
AWS Sagemaker Ground Truth WorkerID for private team,"<p>I've set up an AWS Sagemaker Ground Truth labeling project and am using a private team for the work. I want to track which member of my team gives each answer.</p>

<p>The only user specific information is a <code>workerId</code> as seen, for example, <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/sms-custom-templates-step2-demo2.html"" rel=""noreferrer"">here</a>.</p>

<p>The sagemaker documentation does not have any information about this ID, nor is it anywhere in the cognito documentation, which I need to use to manage my worker team.</p>

<p>As far as I can tell, the <code>workerId</code> is a mturk related id. A <code>workerId</code> shows up in the data structures <a href=""https://docs.aws.amazon.com/AWSMechTurk/latest/AWSMturkAPI/ApiReference_AssignmentDataStructureArticle.html"" rel=""noreferrer"">here</a>.</p>

<p>My question is how can I map the <code>workerId</code> to the specific user in my cognito group? Without the ability to do so, the project will not work.</p>",2,0,2019-05-03 09:57:36.437000 UTC,2.0,2019-05-03 10:07:00.297000 UTC,6,amazon-web-services|amazon-cognito|amazon-sagemaker,1272,2014-12-05 17:54:52.627000 UTC,2022-08-01 17:04:27.443000 UTC,,268,0,0,8,,,,,,['amazon-sagemaker']
Kill all processes at port on AWS Linux,"<p>I am trying to kill processes on port 6006. However, I seem to be unable to use many of the recommended methods to do so. For example:</p>

<p><code>sh-4.2$  kill -9 $(sudo lsof -t -i:6006)</code></p>

<p>Does not work, throwing the error:</p>

<p><code>kill: usage: kill [-s sigspec | -n signum | -sigspec] pid | jobspec ... or kill -l [sigspec]</code></p>

<p>If it helps, this is the system I am using:</p>

<pre><code>sh-4.2$  cat /etc/os-release
NAME=""Amazon Linux AMI""
VERSION=""2018.03""
ID=""amzn""
ID_LIKE=""rhel fedora""
VERSION_ID=""2018.03""
PRETTY_NAME=""Amazon Linux AMI 2018.03""
ANSI_COLOR=""0;33""
CPE_NAME=""cpe:/o:amazon:linux:2018.03:ga""
HOME_URL=""http://aws.amazon.com/amazon-linux-ami/""
</code></pre>",2,1,2020-02-24 19:30:22.893000 UTC,,2020-02-24 19:39:55.813000 UTC,2,linux|port|amazon-sagemaker,3215,2015-08-06 05:39:02.880000 UTC,2021-08-27 18:52:47.253000 UTC,,566,24,0,53,,,,,,['amazon-sagemaker']
How to host a composite models on AWS SageMaker,"<p>I created separate predictive models (using SageMaker's in-built algorithms) on different segments of the data. In production these models needs to be called based on the segment of input data.</p>

<p>Is it possible to host a composite model in SageMaker? How to define the config for deploying a composite model?</p>",1,0,2018-03-22 10:27:55.410000 UTC,,,0,machine-learning|amazon-sagemaker|awsdeploy,313,2017-06-22 17:49:22.837000 UTC,2019-11-21 13:51:01.787000 UTC,,31,0,0,2,,,,,,['amazon-sagemaker']
How to REMOVE shortcuts from AWS Ground Truth labeler UI,<p>I'm using Sagemaker Ground Truth for a custom labeling task. I want to remove the shift enter shortcut for submission. I want to remove it because I want to only show items in a sequence and have the labeler annotate them one at a time. Only when they are all done should they be able to submit. That has been built in some custom javascript but I don't want the user to just submit with the shortcut. Thanks</p>,0,0,2021-11-12 06:01:16.047000 UTC,,,0,amazon-sagemaker|amazon-ground-truth,57,2015-05-26 03:48:47.580000 UTC,2022-09-25 04:15:12.283000 UTC,,273,28,0,14,,,,,,['amazon-sagemaker']
Time difference reading files from Blob-Storage-Container,"<p>We are using <a href=""https://docs.microsoft.com/en-us/azure/storage/blobs/storage-how-to-mount-container-linux"" rel=""nofollow noreferrer"">Blobfuse</a> for &quot;mounting&quot; our blob-storage-container to an Azure virtual machine as well as to Azure ML Studio.<br>In our blob-storage-container there are around 400 files each about 1.5MB
<br>
<br>
With the Azure VM, the algorithm needs 45 seconds to read all files.<br>
With Azure ML Studio, the <strong>same</strong> algorithm needs 5 minutes to read all files.
<br>
<br>
The Azure VM resource as well as the Azure ML Studio resource are in the same tenant.<br>
These resources use two different computes but have the <strong>same</strong> specifications.
<br>
<br>
Why does it take so much longer to read all the files when using Azure ML Studio compared to Azure VM?<br>
Is it possible to reduce the time needed for reading all files when using Azure ML Studio without changing the storage file hierarchy in any way?</p>",1,0,2022-02-04 10:46:04.240000 UTC,,,0,azure|azure-blob-storage|azure-virtual-machine|mount|azure-machine-learning-studio,190,2021-09-29 07:06:53.153000 UTC,2022-09-23 08:06:23.973000 UTC,,139,6,0,12,,,,,,['azure-machine-learning-studio']
"""Kernel appears to have died"" error when I try to train my model. Is it too big? What could be the issue?","<p>None of the other solutions to this question on here have worked for me. </p>

<p>I am trying to train a model on a Jupiter notebook on amazon Sagemaker instance ml.c4.8xlarge with 15,000GB memory. However, this error keeps coming up when I run train. The model is as follows:</p>

<pre><code>model = Sequential()
model.add(Embedding(vocab_size, 200, input_length=max_length))
model.add(GRU(units=400,  dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(800, activation='sigmoid'))
model.add(Dense(400, activation='sigmoid'))
model.add(Dense(200, activation='sigmoid'))
model.add(Dense(100, activation='sigmoid'))
model.add(Dense(50, activation='sigmoid'))
model.add(Dense(20, activation='sigmoid'))
model.add(Dense(10, activation='sigmoid'))
model.add(Dense(5, activation='sigmoid'))
model.add(Dense(1, activation='sigmoid'))
</code></pre>

<p>With the following summary:
<a href=""https://i.stack.imgur.com/Swx1W.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Swx1W.png"" alt=""enter image description here""></a></p>

<p>Is the model too big? Do I not have enough space to run it or could it be some other issue?</p>",2,0,2019-02-19 15:18:36.220000 UTC,1.0,,4,python-3.x|keras|jupyter-notebook|amazon-sagemaker,2255,2017-06-05 18:45:17.603000 UTC,2021-05-26 16:22:03.750000 UTC,,173,3,0,25,,,,,,['amazon-sagemaker']
Registering models from Databricks to Azure ML and save Azure ML image into provided ACR(Non Default ACR of AML Workspace),"<p>I'm trying to register a data bricks model tp <code>Azure ML</code> workspace with <code>mlflow.azure.base_image</code> model. But with this method, we can save the <code>Azure ML</code> image to default <code>ACR</code> connected to the <code>Azure ML</code> workspace.</p>
<p>But I want to save the <code>Azure ML</code> image to another existing <code>ACR</code>. Need help in figuring out the design.</p>
<p>The method I'm using is as follows</p>
<pre><code>    workspace = Workspace.create(name = workspace_name,
                                 location = workspace_location,
                                 resource_group = resource_group,
                                 subscription_id = subscription_id,
                                 auth=svc_pr,
                                 exist_ok=True)

    import mlflow.azureml

    model_image, azure_model = mlflow.azureml.build_image(model_uri=model_uri, 
                                                          workspace=workspace,
                                                          model_name=&quot;winequality&quot;,
                                                          image_name=&quot;winequality&quot;,
                                                          description=&quot;Sklearn ElasticNet image for predicting wine quality&quot;,
                                                          synchronous=True)

    #model_image.wait_for_creation(show_output=True)
    print(&quot;Access the following URI for build logs: {}&quot;.format(model_image.image_build_log_uri))                                    
</code></pre>",1,0,2020-10-22 11:02:29.390000 UTC,,2020-10-22 11:10:53.553000 UTC,0,docker|azure-databricks|mlflow|azure-machine-learning-service,158,2018-10-30 10:19:53.043000 UTC,2022-09-22 15:34:05.270000 UTC,,21,11,0,75,,,,,,"['mlflow', 'azure-machine-learning-service']"
How to solve VertexAI prediction endpoint error?,"<p>I am trying to get predictions from an endpoint that is already created in VertexAI through UI.</p>
<p>I am getting an error when I run the following code</p>
<pre><code>from typing import Dict
import pandas as pd
from google.cloud import aiplatform
from google.protobuf import json_format
from google.protobuf.struct_pb2 import Value
from google.oauth2 import service_account

key_path = '../golden-tempest-xxxxx.json'
credentials = service_account.Credentials.from_service_account_file(key_path, scopes=[
    &quot;https://www.googleapis.com/auth/cloud-platform&quot;], )


aiplatform.init(
    project='golden-tempest-xxxxx',
    location='us-central1',
    credentials=credentials,
)

def predict_tabular_classification_sample(
    project: str,
    endpoint_id: str,
    instance_dict: Dict,
    location: str = &quot;us-central1&quot;,
    api_endpoint: str = &quot;us-central1-aiplatform.googleapis.com&quot;,
):
    client_options = {&quot;api_endpoint&quot;: api_endpoint}
    client = aiplatform.gapic.PredictionServiceClient(client_options=client_options)
    instance = json_format.ParseDict(instance_dict, Value())
    instances = [instance]
    parameters_dict = {}
    parameters = json_format.ParseDict(parameters_dict, Value())
    endpoint = client.endpoint_path(
        project=project, location=location, endpoint=endpoint_id
    )
    response = client.predict(
        endpoint=endpoint, instances=instances, parameters=parameters
    )
    print(&quot;response&quot;)
    print(&quot; deployed_model_id:&quot;, response.deployed_model_id)
    # See gs://google-cloud-aiplatform/schema/predict/prediction/tabular_classification_1.0.0.yaml for the format of the predictions.
    predictions = response.predictions
    for prediction in predictions:
        print(&quot; prediction:&quot;, dict(prediction))


df = pd.read_csv('../btc_test_classification_2.csv')

df_dict = df.to_dict('index')

predict_tabular_classification_sample(
    project=&quot;xxxxx&quot;,
    endpoint_id=&quot;886215168080478208&quot;,
    location=&quot;us-central1&quot;,
    instance_dict={'instances': [df_dict[4]]}
)
</code></pre>
<p><strong>Error</strong>:</p>
<pre><code>InvalidArgument: 400 {&quot;error&quot;: &quot;Column prefix: . Error: Missing struct property: tick_count.&quot;}
</code></pre>
<p>My training data contains the same columns I have in test. <a href=""https://i.stack.imgur.com/zEQ1c.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zEQ1c.png"" alt=""columns"" /></a></p>
<p>I am not sure why I am getting this error.</p>",0,3,2022-08-30 12:46:34.927000 UTC,,,0,google-api|google-cloud-vertex-ai,54,2015-03-14 11:20:42.323000 UTC,2022-09-23 08:37:39.243000 UTC,,1026,236,1,256,,,,,,['google-cloud-vertex-ai']
Create two life-cycle configuration scripts for an Amazon sagemaker notebook,"<p>I have two life-cycle scripts doing two logic blocks, technically, all can be a single script by definition (just a shell script to be run at start of the instance).</p>
<p>In reality, my two lifecycles are very different for multiple notebooks, one is generic: changes only per technical environment: dev, prod, sandbox; But the other is more specific: per region and per user.</p>
<p>It would be so difficult to refactor existing code, so that the already present life-cycle (generic one) be this time session specific to user/region.</p>
<p>Plus, at runtime, based on user choice, a new sagemaker can have either policy or both or none.</p>
<p>I passed some time reading AWS documentation but it seems only one life-cycle can be attached to one sagemaker notebook.</p>
<p>Is there a workaround ?</p>",0,0,2020-06-30 09:55:40.003000 UTC,,,3,amazon-web-services|amazon-sagemaker,335,2013-01-05 15:39:37.250000 UTC,2022-09-24 20:29:25.133000 UTC,"Paris, France",771,105,3,437,,,,,,['amazon-sagemaker']
Error can't get attribute Net when saving PyTorch model with MLFlow,"<p>After installing MLFlow using <a href=""https://github.com/artefactory/one-click-mlflow"" rel=""nofollow noreferrer"">one-click-mlflow</a> I save a pytorch model using the default command that I found in the user guide. You can find the command bellow:</p>
<pre class=""lang-py prettyprint-override""><code>mlflow.pytorch.log_model(net, artifact_path=&quot;model&quot;, pickle_module=pickle)
</code></pre>
<p>The neural network saved is very simple, this is basically a two layer neural network with Xavier initialization and hyperbolic tangent as activation function.</p>
<pre class=""lang-py prettyprint-override""><code>class Net(T.nn.Module):
    
    def __init__(self):
        super(Net, self).__init__()
        self.hid1 = T.nn.Linear(n_features, 10)
        self.hid2 = T.nn.Linear(10, 10)
        self.oupt = T.nn.Linear(10, 1)
        T.nn.init.xavier_uniform_(self.hid1.weight) 
        T.nn.init.zeros_(self.hid1.bias)
        T.nn.init.xavier_uniform_(self.hid2.weight)
        T.nn.init.zeros_(self.hid2.bias)
        T.nn.init.xavier_uniform_(self.oupt.weight)
        T.nn.init.zeros_(self.oupt.bias)
        
    def forward(self, x):
        z = T.tanh(self.hid1(x))
        z = T.tanh(self.hid2(z))
        z = self.oupt(z)
        return z
</code></pre>
<p>Every things is runing fine in the Jupyter Notebook. I can log metrics and other artifact but when I save the model I got the following error message:</p>
<pre class=""lang-sh prettyprint-override""><code>2021/10/13 09:21:00 WARNING mlflow.utils.requirements_utils: Found torch version (1.9.0+cu111) contains a local version label (+cu111). MLflow logged a pip requirement for this package as 'torch==1.9.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.
2021/10/13 09:21:00 WARNING mlflow.utils.requirements_utils: Found torchvision version (0.10.0+cu111) contains a local version label (+cu111). MLflow logged a pip requirement for this package as 'torchvision==0.10.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.
2021/10/13 09:21:01 ERROR mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmpnl9dsoye/model/data, flavor: pytorch)
Traceback (most recent call last):
  File &quot;/home/ucsky/.virtualenv/mymodel/lib/python3.9/site-packages/mlflow/utils/environment.py&quot;, line 212, in infer_pip_requirements
    return _infer_requirements(model_uri, flavor)
  File &quot;/home/ucsky/.virtualenv/mymodel/lib/python3.9/site-packages/mlflow/utils/requirements_utils.py&quot;, line 263, in _infer_requirements
    modules = _capture_imported_modules(model_uri, flavor)
  File &quot;/home/ucsky/.virtualenv/mymodel/lib/python3.9/site-packages/mlflow/utils/requirements_utils.py&quot;, line 221, in _capture_imported_modules
    _run_command(
  File &quot;/home/ucsky/.virtualenv/mymodel/lib/python3.9/site-packages/mlflow/utils/requirements_utils.py&quot;, line 173, in _run_command
    raise MlflowException(msg)
mlflow.exceptions.MlflowException: Encountered an unexpected error while running ['/home/ucsky/.virtualenv/mymodel/bin/python', '/home/ucsky/.virtualenv/mymodel/lib/python3.9/site-packages/mlflow/utils/_capture_modules.py', '--model-path', '/tmp/tmpnl9dsoye/model/data', '--flavor', 'pytorch', '--output-file', '/tmp/tmplyj0w2fr/imported_modules.txt', '--sys-path', '[&quot;/home/ucsky/project/ofi-ds-research/incubator/ofi-pe-fr/notebook/guillaume-simon/06-modelisation-pytorch&quot;, &quot;/home/ucsky/.virtualenv/mymodel/lib/python3.9/site-packages/git/ext/gitdb&quot;, &quot;/usr/lib/python39.zip&quot;, &quot;/usr/lib/python3.9&quot;, &quot;/usr/lib/python3.9/lib-dynload&quot;, &quot;&quot;, &quot;/home/ucsky/.virtualenv/mymodel/lib/python3.9/site-packages&quot;, &quot;/home/ucsky/.virtualenv/mymodel/lib/python3.9/site-packages/IPython/extensions&quot;, &quot;/home/ucsky/.ipython&quot;, &quot;/home/ucsky/.virtualenv/mymodel/lib/python3.9/site-packages/gitdb/ext/smmap&quot;]']
exit status: 1
stdout: 
stderr: Traceback (most recent call last):
  File &quot;/home/ucsky/.virtualenv/mymodel/lib/python3.9/site-packages/mlflow/utils/_capture_modules.py&quot;, line 125, in &lt;module&gt;
    main()
  File &quot;/home/ucsky/.virtualenv/mymodel/lib/python3.9/site-packages/mlflow/utils/_capture_modules.py&quot;, line 118, in main
    importlib.import_module(f&quot;mlflow.{flavor}&quot;)._load_pyfunc(model_path)
  File &quot;/home/ucsky/.virtualenv/mymodel/lib/python3.9/site-packages/mlflow/pytorch/__init__.py&quot;, line 723, in _load_pyfunc
    return _PyTorchWrapper(_load_model(path, **kwargs))
  File &quot;/home/ucsky/.virtualenv/mymodel/lib/python3.9/site-packages/mlflow/pytorch/__init__.py&quot;, line 626, in _load_model
    return torch.load(model_path, **kwargs)
  File &quot;/home/ucsky/.virtualenv/mymodel/lib/python3.9/site-packages/torch/serialization.py&quot;, line 607, in load
    return _load(opened_zipfile, map_location, pickle_module, **pickle_load_args)
  File &quot;/home/ucsky/.virtualenv/mymodel/lib/python3.9/site-packages/torch/serialization.py&quot;, line 882, in _load
    result = unpickler.load()
  File &quot;/home/ucsky/.virtualenv/mymodel/lib/python3.9/site-packages/torch/serialization.py&quot;, line 875, in find_class
    return super().find_class(mod_name, name)
AttributeError: Can't get attribute 'Net' on &lt;module '__main__' from '/home/ucsky/.virtualenv/mymodel/lib/python3.9/site-packages/mlflow/utils/_capture_modules.py'&gt;
</code></pre>
<p>Can somebody explain me what is wrong?</p>",0,0,2021-10-13 07:44:49.723000 UTC,,,2,python|pytorch|virtualenv|mlflow|mlops,343,2010-01-10 16:57:11.610000 UTC,2022-08-25 13:06:27.853000 UTC,,382,248,3,61,,,,,,['mlflow']
Passing multiple lines of input for SageMaker prediction,"<p>I have built a SageMaker pipeline which uses a combination of Custom Transformer (using SKLearn Transformer and an XGBoost model). A sample pipeline is shown below:</p>
<pre><code>from sagemaker.model import Model
from sagemaker.pipeline import PipelineModel
import boto3
from time import gmtime, strftime

timestamp_prefix = strftime(&quot;%Y-%m-%d-%H-%M-%S&quot;, gmtime())

scikit_learn_inferencee_model = sklearn_preprocessor.create_model()
xg_model = xg_estimator.create_model()

model_name = 'inference-pipeline-' + timestamp_prefix
endpoint_name = 'inference-pipeline-ep-' + timestamp_prefix
sm_model = PipelineModel(
    name=model_name, 
    role=role, 
    models=[
        scikit_learn_inference_model, 
        xg_model])

sm_model.deploy(initial_instance_count=1, instance_type='ml.c4.xlarge', endpoint_name=endpoint_name)
</code></pre>
<p>Tbe sklearn preprocessor takes 100 rows of input and generates a single line that is passed as input to the XGBoost model for prediction. So for every 100 rows of input, I get only one prediction.</p>
<p>However, all the examples for SageMaker prediction point to only one row of input as follows:</p>
<pre><code>from sagemaker.predictor import json_serializer, csv_serializer, json_deserializer, RealTimePredictor
from sagemaker.content_types import CONTENT_TYPE_CSV, CONTENT_TYPE_JSON

payload = 'M, 0.44, 0.365, 0.125, 0.516, 0.2155, 0.114, 0.155'
actual_rings = 10

predictor = RealTimePredictor(
    endpoint=endpoint_name,
    sagemaker_session=sagemaker_session,
    serializer=csv_serializer,
    content_type=CONTENT_TYPE_CSV,
    accept=CONTENT_TYPE_JSON)

print(predictor.predict(payload))
</code></pre>
<p>How can I pass multiple of rows of input to a SageMaker endpoint and get 1 prediction out?</p>
<p>Thank you.</p>",1,1,2020-11-01 17:29:28.273000 UTC,,,0,python|amazon-web-services|scikit-learn|amazon-sagemaker,383,2011-10-19 10:12:30.600000 UTC,2022-09-09 09:12:03.143000 UTC,,3073,238,5,341,,,,,,['amazon-sagemaker']
How to extract S3uri as string in sagemaker,"<p>am trying to execute the below code in sagemaker pipeline</p>
<pre><code>train_data = TrainingInput(
    s3_data = step_process.properties.ProcessingOutputConfig.Outputs[&quot;train_data&quot;].S3Output.S3Uri+'/training.txt',
    distribution=&quot;FullyReplicated&quot;,
    content_type=&quot;text/plain&quot;,
    s3_data_type=&quot;S3Prefix&quot;,
)
</code></pre>
<p>I keep getting this error <code>AttributeError: 'Properties' object has no attribute 'path'</code> but am not sure how to extract the URI as string</p>
<p>I can put that in a general question, how to access/print <code>properties</code> data like <code>step_train.properties.ModelArtifacts.S3ModelArtifacts</code> will print  <code>&lt;sagemaker.workflow.properties.Properties at 0x7fa7d68acbd0&gt;</code> instead of the models path</p>",1,0,2022-08-24 04:11:09.700000 UTC,,2022-08-24 15:32:58.537000 UTC,0,amazon-web-services|amazon-sagemaker,31,2012-04-26 13:33:06.710000 UTC,2022-09-25 00:14:52.287000 UTC,Egypt,1972,701,11,547,,,,,,['amazon-sagemaker']
Forecast data with multiple Features seperately in Azure Designer,"<p>I am currently predicting values of &quot;type A&quot; in Azure Machine Learning Studio Designer. I am importing a file from azure blob storage and use that file as my past data. The current structure of the file is the following:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>timestamp</th>
<th>value</th>
</tr>
</thead>
<tbody>
<tr>
<td>2022-01-01</td>
<td>12345</td>
</tr>
<tr>
<td>2022-02-01</td>
<td>12345</td>
</tr>
<tr>
<td>2022-03-01</td>
<td>12345</td>
</tr>
</tbody>
</table>
</div>
<p>I now want to predict multiple different types of that value, while still using one pipeline and one input file. The file structure would look something like this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>timestamp</th>
<th>type</th>
<th>value</th>
</tr>
</thead>
<tbody>
<tr>
<td>2022-01-01</td>
<td>type A</td>
<td>12345</td>
</tr>
<tr>
<td>2022-01-01</td>
<td>type B</td>
<td>12345</td>
</tr>
<tr>
<td>2022-01-01</td>
<td>type C</td>
<td>12345</td>
</tr>
<tr>
<td>2022-02-01</td>
<td>type A</td>
<td>12345</td>
</tr>
<tr>
<td>2022-02-01</td>
<td>type B</td>
<td>12345</td>
</tr>
<tr>
<td>2022-02-01</td>
<td>type C</td>
<td>12345</td>
</tr>
</tbody>
</table>
</div>
<p>I can currently predict those values and extract them properly, but the quality of those results is way worse then when predicting them one by one. This is probably because the linear regression is trying to find connections between type A and type C for example. I've edited the metadata and changed the &quot;type&quot; into a categorical feature but it is still not treating each type one by one.</p>
<p>Is there any possible option that the types will be predicted one by one, so first type A for all dates and after that type B, etc.?
Is there any way to increase the forecasting quality so that it will reach the same quality as predicting them one by one?
Using multiple pipelines or multiple files is not an option due to the high amount of different types (300+). Already using hyperparamter tuning so that is not an option.</p>
<p>Thanks in advance!</p>",1,0,2022-04-20 10:00:50.947000 UTC,,,1,linear-regression|prediction|forecasting|azure-machine-learning-studio,62,2020-06-15 13:06:57.127000 UTC,2022-09-22 19:00:57.357000 UTC,Germany,13,0,0,4,,,,,,['azure-machine-learning-studio']
"Getting the error ""str' object has no attribute 'decode"" when trying to use custom weights for image classification","<p>I am trying to develop a simple image classification model in Azure ML notebooks. ResNet50 model was trained and the custom weights from the model is being used in the following code for image classification. The custom weights are saved in a folder called Model.</p>
<pre><code>import os
working_directory = os.getcwd()
model_directory = working_directory + &quot;/Model/model.h5&quot; 
</code></pre>
<p>The above code is being used for accessing the saved model.</p>
<pre><code>def classifyingImages(image_list):
    value = 0

    for image in image_list:
        image_resized = cv2.resize(image, (img_height, img_width))
        image = np.expand_dims(image_resized, axis=0)

        
        model = load_model(model_directory)
        #classifying the image
        prediction = model.predict(image)
        output_class = class_names[np.argmax(prediction)]

        #getting the image name
        image_name = img_name_list[value]
        print(image_name)

        if(np.argmax(prediction)==0):
            print(&quot;check negative&quot;)
            # cv2.imwrite((negative_path+&quot;/&quot;+image_name), image)
        else:
            print(&quot;check positive&quot;)
            # cv2.imwrite((path_positive+&quot;/&quot;+image_name), image)

        value = value +1


    return value

classifyingImages(image_list)
</code></pre>
<p>The code added above is the image classification code</p>
<p>The <code>image_list</code> contains the test images which saved in the blob storage.</p>
<p>After running the classification function i get the error <code>str' object has no attribute 'decode</code> and as a solution i tried to change the h5py lib version using below code. But still it gives me the same error. It would be great if i could a solution for this issue. Thank you in advance.
<code>!pip install h5py==2.10.0</code></p>
<p>The stack trace</p>
<pre><code>AttributeError                            Traceback (most recent call last)
&lt;ipython-input-96-6e38a2f74291&gt; in &lt;module&gt;
     42     return value
     43 
---&gt; 44 classifyingImages(image_list)

&lt;ipython-input-96-6e38a2f74291&gt; in classifyingImages(image_list)
     20         print(image.dtype)
     21 
---&gt; 22         model = load_model(model_directory)
     23 
     24         #classifying the image

/anaconda/envs/azureml_py36/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/save.py in load_model(filepath, custom_objects, compile)
    144   if (h5py is not None and (
    145       isinstance(filepath, h5py.File) or h5py.is_hdf5(filepath))):
--&gt; 146     return hdf5_format.load_model_from_hdf5(filepath, custom_objects, compile)
    147 
    148   if isinstance(filepath, six.string_types):

/anaconda/envs/azureml_py36/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/hdf5_format.py in load_model_from_hdf5(filepath, custom_objects, compile)
    164     if model_config is None:
    165       raise ValueError('No model found in config file.')
--&gt; 166     model_config = json.loads(model_config.decode('utf-8'))
    167     model = model_config_lib.model_from_config(model_config,
    168                                                custom_objects=custom_objects)

AttributeError: 'str' object has no attribute 'decode'
</code></pre>",0,9,2022-03-13 08:19:14.620000 UTC,,2022-03-13 09:10:39.363000 UTC,0,python|tensorflow|azure-machine-learning-studio,68,2019-05-01 13:40:46.720000 UTC,2022-09-11 08:42:22.377000 UTC,"1040/3 Athurugiriya Road, Malabe, Sri Lanka",25,14,0,6,,,,,,['azure-machine-learning-studio']
How Can I use gensim package in Azure ML?,"<p>I am using text analysis with Azure ML. So in my python script I want to create a bag of word model and then calculate TFIDF of each words. For that I am using gensim model, It's not working on Azure ML. So is there any options for me? </p>",2,0,2016-10-26 03:51:57.853000 UTC,1.0,,1,machine-learning|text-analysis|azure-machine-learning-studio,1024,2016-02-11 19:00:50.737000 UTC,2022-09-23 07:23:55.703000 UTC,"Auckland, New Zealand",3811,118,13,703,,,,,,['azure-machine-learning-studio']
Deployment deep learning system with some models with MLaaS,"<p>I read some articles with deployment examples and they were about deploying one model but not a whole deep learning system.</p>
<p>If I want to deploy my project including launch of multiple deep models built with different frameworks (Pytorch, tensorflow) then what's good option for that:</p>
<ol>
<li>build Docker image with whole project and deploy it with ml
service (azure, aws lambda etc);</li>
<li>or deploy every single model with
chosen MLaaS and and elsewhere deploy the logic that makes requests
to the above models;</li>
</ol>
<p>I would appreciate any reference/link on the subject.
Thanx.</p>",1,0,2021-01-19 04:34:50.767000 UTC,,2021-01-19 05:00:58.747000 UTC,2,azure-machine-learning-service,51,2017-11-09 18:19:40.537000 UTC,2022-09-08 09:03:41.323000 UTC,,326,80,1,28,,,,,,['azure-machine-learning-service']
Does SageMaker Feature Store use Athena queries (data virtualization) to make its queries performant?,"<p>SageMaker boasts rapid performance for reading data from batch and online. I am wondering what is happening behind the scenes. Are these Athena queries behind the scenes that the feature stores uses? Is it accessing materialized views, or are these queries running against partitioned data in S3 directly (and thus faster due to partitioning rather than materialized views)?</p>",1,1,2021-07-22 15:34:39.547000 UTC,,,0,amazon-web-services|amazon-athena|presto|amazon-sagemaker|feature-store,172,2012-08-31 20:08:40.090000 UTC,2022-09-25 04:17:41.297000 UTC,,11650,6318,21,977,,,,,,['amazon-sagemaker']
AWS Sagemaker Error for Training job - Algorithm Error,"<p>I am receiving the following error when I try and train an XGBoost model and have no idea how to fix it. Any help please?</p>
<pre><code>UnexpectedStatusException: Error for Training job sagemaker-xgboost-2022-08-22-21-37-39-774: Failed. Reason: AlgorithmError: framework error: 
Traceback (most recent call last):
  File &quot;/miniconda3/lib/python3.6/site-packages/sagemaker_containers/_trainer.py&quot;, line 84, in train
    entrypoint()
  File &quot;/miniconda3/lib/python3.6/site-packages/sagemaker_xgboost_container/training.py&quot;, line 94, in main
    train(framework.training_env())
  File &quot;/miniconda3/lib/python3.6/site-packages/sagemaker_xgboost_container/training.py&quot;, line 90, in train
    run_algorithm_mode()
  File &quot;/miniconda3/lib/python3.6/site-packages/sagemaker_xgboost_container/training.py&quot;, line 68, in run_algorithm_mode
    checkpoint_config=checkpoint_config
  File &quot;/miniconda3/lib/python3.6/site-packages/sagemaker_xgboost_container/algorithm_mode/train.py&quot;, line 110, in sagemaker_train
    validated_train_config = hyperparameters.validate(train_config)
  File &quot;/miniconda3/lib/python3.6/site-packages/sagemaker_algorithm_toolkit/hyperparameter_validation.py&quot;, line 270, in validate
    raise exc.UserError(&quot;Missing required hyperparameter: {}&quot;.format(hp)
</code></pre>
<p>My full notebook is too large to post here, but below I have also added an image of the code right before the training</p>
<p><a href=""https://i.stack.imgur.com/eXDtH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eXDtH.png"" alt=""enter image description here"" /></a></p>",0,0,2022-08-22 23:04:04.407000 UTC,,,0,amazon-web-services|amazon-sagemaker,51,2017-12-19 09:02:53.560000 UTC,2022-09-21 08:41:22.723000 UTC,,101,4,0,12,,,,,,['amazon-sagemaker']
Dask on AWS Sagemaker Dying with only 3GB of data on 240GB instances - Exception: ‘FSTimeoutError()’,"<p>I’m trying out the <a href=""https://github.com/aws/amazon-sagemaker-examples/blob/main/sagemaker_processing/feature_transformation_with_sagemaker_processing_dask/feature_transformation_with_sagemaker_processing_dask.ipynb"" rel=""nofollow noreferrer"">AWS Sagemaker Dask</a> example on my own data that sits in S3. Currently I’m focusing on retrieving the data and processing it. I extract my files in a dask dataframe, however, when I run <code>len(df)</code> to see the number of rows in my dataframe the code fails.</p>
<pre><code> File &quot;/opt/conda/lib/python3.8/site-packages/pyarrow/parquet.py&quot;, line 401, in read
return self.reader.read_all(column_indices=column_indices,
File &quot;pyarrow/_parquet.pyx&quot;, line 1139, in pyarrow._parquet.ParquetReader.read_all
File &quot;/opt/conda/lib/python3.8/site-packages/fsspec/spec.py&quot;, line 1483, in read
out = self.cache._fetch(self.loc, self.loc + length)
File &quot;/opt/conda/lib/python3.8/site-packages/fsspec/caching.py&quot;, line 40, in _fetch
return self.fetcher(start, stop)
File &quot;/opt/conda/lib/python3.8/site-packages/s3fs/core.py&quot;, line 1920, in _fetch_range
return _fetch_range(
File &quot;/opt/conda/lib/python3.8/site-packages/s3fs/core.py&quot;, line 2070, in _fetch_range
return sync(fs.loop, resp[&quot;Body&quot;].read)
File &quot;/opt/conda/lib/python3.8/site-packages/fsspec/asyn.py&quot;, line 67, in sync
raise FSTimeoutError
fsspec.exceptions.FSTimeoutError
</code></pre>
<p>I’ve tried changing limit to 3h on the dask ymal file but to no avail. I'm using the instance with 128 GiB Memory and 16 CPUs.
The DF contains only 3 GB worth of data so I don’t understand at all why it would fail. Pandas would have executed it in minutes if not seconds. Maybe it is not distributing correctly?</p>
<p>Code:</p>
<pre><code>if __name__ == &quot;__main__&quot;:
  parser = argparse.ArgumentParser()
  parser.add_argument(&quot;--train-test-split-ratio&quot;, type=float, default=0.3)
  args, _ = parser.parse_known_args()

  # Get processor scrip arguments
  args_iter = iter(sys.argv[1:])
  script_args = dict(zip(args_iter, args_iter))
  scheduler_ip = sys.argv[-1]

  # S3 client
  s3_region = script_args[&quot;s3_region&quot;]
  s3_client = boto3.resource(&quot;s3&quot;, s3_region)
  s3_client2 = boto3.client(&quot;s3&quot;, s3_region)
  print(f&quot;Using the {s3_region} region&quot;)

  # Start the Dask cluster client
  try:
     client = Client(&quot;tcp://{ip}:8786&quot;.format(ip=scheduler_ip))
     logging.info(&quot;Printing cluster information: {}&quot;.format(client))
  except Exception as err:
     logging.exception(err)
  df_1 = dd.read_parquet('s3://bucket/prefix/data_0_*.parquet')
  df_2 = client.persist(df_1)
  print(len(df_2))
</code></pre>",0,1,2022-04-18 15:05:00.163000 UTC,,2022-04-19 10:52:34.597000 UTC,0,python|amazon-web-services|dask|amazon-sagemaker|dask-distributed,155,2022-04-08 18:54:46.043000 UTC,2022-08-19 14:28:47.103000 UTC,,26,0,0,4,,,,,,['amazon-sagemaker']
How to convert bert model output to json?,"<p>I have fine-tuned a Bert model and testing my output from different layers. I tested this in sagemaker , with my own custom script (see below) and the output i get is of BaseModelOutputWithPoolingAndCrossAttentions class. How can i convert the output of this , specially the tensor values from the last_hidden_state to json?</p>
<p>inference.py</p>
<pre><code>
from transformers import BertModel, BertConfig

def model_fn():

   config = BertConfig.from_pretrained(&quot;xxx&quot;, output_hidden_states=True)
   model = BertModel.from_pretrained(&quot;xxx&quot;, config=config)

....
def predict_fn():
    ....

    return model(inputs)

</code></pre>
<p>model output</p>
<pre><code>BaseModelOutputWithPoolingAndCrossAttentions( 
last_hidden_state=
tensor([[[-1.6968,  1.9364, -2.1796, -0.0819,  1.8027,  0.3540,  1.3269,  0.1532],
        [-0.4969,  0.4169,  0.5677,  1.0968,  0.0742,  1.5354,  0.9387,  0.0343]]])
device='cuda:0', grad_fn=&lt;NativeLayerNormBackward&gt;), 
hidden_states=None,
attentions=None, 
...
</code></pre>",1,0,2022-09-23 16:46:42.360000 UTC,,,0,python|json|tensorflow|amazon-sagemaker|bert-language-model,21,2020-05-30 00:10:41.983000 UTC,2022-09-24 19:51:20.543000 UTC,,525,69,0,98,,,,,,['amazon-sagemaker']
AWS Sagemker debugger examples TypeError missing argument,"<p><strong>TypeError: __init__() missing 1 required positional argument: 'action_str'</strong></p>
<p>The error pops up in the official example jupyter notebook (<strong>mnist_tensor_analysis.ipynb</strong>) of the amazon Sagemaker debugger. Occurs when calling the constructor of the Rule package from smdebug.rules.rule.
<a href=""https://i.stack.imgur.com/O00Um.jpg"" rel=""nofollow noreferrer"">screenshot of the error</a></p>
<p>Are there any solutions to this problem? The error is visible in the official repo of the AWS Sagemaker debugger. Below is the link to the notebook.</p>
<p>link : <a href=""https://github.com/aws/amazon-sagemaker-examples/blob/main/sagemaker-debugger/mnist_tensor_analysis/mnist_tensor_analysis.ipynb"" rel=""nofollow noreferrer"">https://github.com/aws/amazon-sagemaker-examples/blob/main/sagemaker-debugger/mnist_tensor_analysis/mnist_tensor_analysis.ipynb</a></p>",1,0,2022-02-17 07:13:10.160000 UTC,,,0,amazon-web-services|amazon-ec2|typeerror|amazon-sagemaker,147,2022-02-17 06:50:55.170000 UTC,2022-04-20 17:12:31.137000 UTC,,1,0,0,0,,,,,,['amazon-sagemaker']
How to get the best trained model from AWS StepFunctions TuningStep with SageMaker,"<p>I've been pondering on it for a while, and I can't seem to be able to get the best trained model from a <code>stepfunctions.steps.TuningStep</code>. I need it for a workflow:</p>
<pre><code>workflow_definition = steps.Chain(
    [etl_step, tuning_step, model_step, lambda_step, check_accuracy_step]
)
</code></pre>
<p>In the documentation:</p>
<p><a href=""https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/sagemaker.html#stepfunctions.steps.sagemaker.TuningStep"" rel=""nofollow noreferrer"">https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/sagemaker.html#stepfunctions.steps.sagemaker.TuningStep</a></p>
<p>there is no method similar to
<code>get_expected_model(model_name=None)</code>
from TrainingStep.
On the other hand, if I use a sagemaker.workflow.steps TuningStep there are ways to obtain the best model:</p>
<pre><code>get_top_model_s3_uri()
get_expected_model()
</code></pre>
<p>Am I missing something or is that really impossible?</p>",1,2,2021-11-19 23:43:28.190000 UTC,,,0,amazon-sagemaker|aws-step-functions,137,2014-04-10 08:36:15.073000 UTC,2022-09-22 18:48:08.427000 UTC,Greece,343,254,0,43,,,,,,['amazon-sagemaker']
Authenticate With Workspace,"<p>I have a Pipeline registered in my AML workspace. Now I would like to trigger a pipeline run from an Azure Notebook in the same Workspace.
In order to get a reference object to the workspace in the notebook I need to authenticate, e.g. </p>

<p><code>ws = Workspace.from_config()</code></p>

<p>However, InteractiveLoginAthentication is blocked by my company's domain and MsiAuthentication throws an error as well. ServicePrincipalAuthentication works, but how do I keep the secret safe? What is the prefered way of dealing with secrets in the Azure Machine Learning Service Notebooks?</p>",0,0,2020-02-20 08:50:30.347000 UTC,,,2,azure-machine-learning-service|azure-notebooks,116,2020-02-20 08:37:04.717000 UTC,2021-04-11 10:00:37.967000 UTC,,23,0,0,3,,,,,,['azure-machine-learning-service']
Streamlit Browser App does not open from Sagemaker Terminal,"<p>I am working on building a movie recommendation engine on aws sagemaker environment and i plan to show working demonstration for the same. I am trying to use the streamlit library for running the app.</p>
<p>After running the command: streamlit run app.py, it provides me 2 urls i.e. Network and External urls.</p>
<p>The issues is none of the url works. It results in connection timed out, the server does not respond.</p>
<p>Please help</p>",1,1,2020-08-15 15:48:54.870000 UTC,1.0,2020-08-17 05:35:20.803000 UTC,2,python|user-interface|jupyter|amazon-sagemaker|streamlit,724,2020-05-31 11:45:58.247000 UTC,2021-01-25 14:00:10.543000 UTC,,23,0,0,13,,,,,,['amazon-sagemaker']
Pytorch lightning progress bar not working in AWS Sagemaker Jupyter Lab,"<p>We started using Sagemaker Jupyter Lab to run a few Depp Learning experiments we previously ran on GoogleColabPro+. The training starts fine and everything seems to work, however, the progress bar appears as follows:</p>
<p><strong>Validation sanity check: 0it [00:00, ?it/s]
Training: 0it [00:00, ?it/s]</strong></p>
<p>The progress bar was working fine on GoogleColab. I tried uninstalling ipywidgets as <a href=""https://github.com/PyTorchLightning/pytorch-lightning/issues/11208"" rel=""nofollow noreferrer"">suggested here</a>, but still no luck. Anyone has an idea of how to fix the problem?</p>
<p>Below you will find a copy of the TrainerFunction I am using.</p>
<pre><code>class T5FineTuner(pl.LightningModule):
    def __init__(self, hparams):
        super(T5FineTuner, self).__init__()
        self.hparams = hparams

        self.model = T5ForConditionalGeneration.from_pretrained(hparams['model_name_or_path'])
        self.tokenizer = T5Tokenizer.from_pretrained(hparams['tokenizer_name_or_path'])

    def hparams(self):
        return self.hparams

    def is_logger(self):
        return True #self.trainer.proc_rank &lt;= 0

    def forward(
            self, input_ids, attention_mask=None, decoder_input_ids=None, decoder_attention_mask=None, labels=None
    ):
        return self.model(
            input_ids,
            attention_mask=attention_mask,
            decoder_input_ids=decoder_input_ids,
            decoder_attention_mask=decoder_attention_mask,
            labels=labels,
        )

    def _step(self, batch):
        labels = batch[&quot;target_ids&quot;]
        labels[labels[:, :] == self.tokenizer.pad_token_id] = -100

        outputs = self(
            input_ids=batch[&quot;source_ids&quot;],
            attention_mask=batch[&quot;source_mask&quot;],
            labels=labels,
            decoder_attention_mask=batch['target_mask']
        )

        loss = outputs[0]

        return loss

    def training_step(self, batch, batch_idx):
        loss = self._step(batch)

        tensorboard_logs = {&quot;train_loss&quot;: loss}
        return {&quot;loss&quot;: loss, &quot;log&quot;: tensorboard_logs}

    def training_epoch_end(self, outputs):
        avg_train_loss = torch.stack([x[&quot;loss&quot;] for x in outputs]).mean()
        tensorboard_logs = {&quot;avg_train_loss&quot;: avg_train_loss}
        # return {&quot;avg_train_loss&quot;: avg_train_loss, &quot;log&quot;: tensorboard_logs, 'progress_bar': tensorboard_logs}

    def validation_step(self, batch, batch_idx):
        loss = self._step(batch)
        return {&quot;val_loss&quot;: loss}

    def validation_epoch_end(self, outputs):
        avg_loss = torch.stack([x[&quot;val_loss&quot;] for x in outputs]).mean()
        tensorboard_logs = {&quot;val_loss&quot;: avg_loss}
        return {&quot;avg_val_loss&quot;: avg_loss, &quot;log&quot;: tensorboard_logs, 'progress_bar': tensorboard_logs}

    def configure_optimizers(self):
        &quot;Prepare optimizer and schedule (linear warmup and decay)&quot;

        model = self.model
        no_decay = [&quot;bias&quot;, &quot;LayerNorm.weight&quot;]
        optimizer_grouped_parameters = [
            {
                &quot;params&quot;: [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],
                &quot;weight_decay&quot;: self.hparams['weight_decay'],
            },
            {
                &quot;params&quot;: [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],
                &quot;weight_decay&quot;: 0.0,
            },
        ]
        optimizer = AdamW(optimizer_grouped_parameters, lr=self.hparams['learning_rate'], eps=self.hparams['adam_epsilon'])
        self.opt = optimizer
        return [optimizer]

    def optimizer_step(self, epoch=None, batch_idx=None, optimizer=None, optimizer_idx=None, optimizer_closure=None,  second_order_closure=None, on_tpu=False, using_native_amp=False, using_lbfgs=False):
        # if self.trainer.use_tpu:
        #     xm.optimizer_step(optimizer)
        # else:
        optimizer.step(closure=optimizer_closure)
        optimizer.zero_grad()
        self.lr_scheduler.step()

    def get_tqdm_dict(self):
        tqdm_dict = {&quot;loss&quot;: &quot;{:.3f}&quot;.format(self.trainer.avg_loss), &quot;lr&quot;: self.lr_scheduler.get_last_lr()[-1]}

        return tqdm_dict

    def train_dataloader(self):
        train_dataset = get_dataset(tokenizer=self.tokenizer, type_path=&quot;translated_train&quot;, args=self.hparams)
        dataloader = DataLoader(train_dataset, batch_size=self.hparams['train_batch_size'], drop_last=True, shuffle=True,
                                num_workers=4)
        t_total = (
                (len(dataloader.dataset) // (self.hparams['train_batch_size'] * max(1, self.hparams['n_gpu'])))
                // self.hparams['gradient_accumulation_steps']
                * float(self.hparams['num_train_epochs'])
        )
        scheduler = get_linear_schedule_with_warmup(
            self.opt, num_warmup_steps=self.hparams['warmup_steps'], num_training_steps=t_total
        )
        self.lr_scheduler = scheduler
        return dataloader

    def val_dataloader(self):
        val_dataset = get_dataset(tokenizer=self.tokenizer, type_path=&quot;test_2k&quot;, args=self.hparams)
        return DataLoader(val_dataset, batch_size=self.hparams['eval_batch_size'], num_workers=4)

logger = logging.getLogger(__name__)

class LoggingCallback(pl.Callback):
    def on_validation_end(self, trainer, pl_module):
        logger.info(&quot;***** Validation results *****&quot;)
        if pl_module.is_logger():
            metrics = trainer.callback_metrics
            # Log results
            for key in sorted(metrics):
                if key not in [&quot;log&quot;, &quot;progress_bar&quot;]:
                    logger.info(&quot;{} = {}\n&quot;.format(key, str(metrics[key])))

    def on_test_end(self, trainer, pl_module):
        logger.info(&quot;***** Test results *****&quot;)

        if pl_module.is_logger():
            metrics = trainer.callback_metrics

            # Log and save results to file
            output_test_results_file = os.path.join(pl_module.hparams[&quot;output_dir&quot;], &quot;test_results.txt&quot;)
            with open(output_test_results_file, &quot;w&quot;) as writer:
                for key in sorted(metrics):
                    if key not in [&quot;log&quot;, &quot;progress_bar&quot;]:
                        logger.info(&quot;{} = {}\n&quot;.format(key, str(metrics[key])))
                        writer.write(&quot;{} = {}\n&quot;.format(key, str(metrics[key])))
</code></pre>",0,0,2022-05-16 11:23:44.600000 UTC,,,0,progress-bar|jupyter-lab|amazon-sagemaker|pytorch-lightning,354,2021-11-28 17:43:31.363000 UTC,2022-09-21 16:44:23.063000 UTC,Paris,21,0,0,2,,,,,,['amazon-sagemaker']
What is Threshold in the Evaluate Model Module?,"<p>Notice in the image below, if I increase the value of &quot;Threshold,&quot; the accuracy of the model seems to increase (with diminishing returns after about .62).</p>
<p>What does this mean and can I somehow update this value such that my model will retain this setting?</p>
<p>For example, I am using a boosted decision tree, but I don't see any such value for &quot;threshold.&quot;</p>
<p>Ref. <a href=""https://docs.microsoft.com/en-us/previous-versions/azure/machine-learning/studio-module-reference/evaluate-model?redirectedfrom=MSDN"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/previous-versions/azure/machine-learning/studio-module-reference/evaluate-model?redirectedfrom=MSDN</a></p>
<p><a href=""https://i.stack.imgur.com/E0o5I.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/E0o5I.png"" alt=""enter image description here"" /></a></p>",1,1,2022-06-12 04:01:44.270000 UTC,,,0,azure-machine-learning-studio,70,2012-06-23 21:35:13.997000 UTC,2022-09-10 03:19:49.497000 UTC,,20199,7887,10,2396,,,,,,['azure-machine-learning-studio']
AWS SageMaker Estimator cannot access the internet,"<p>I'm trying to run a training job on a SageMaker Tensorflow estimator. Before starting the training job I need to install some dependencies. As suggested in the Python SDK SageMaker documentation, I put a requirements.txt file in the code root directory.</p>
<p>The training job fails upon trying to install these dependencies with the following error:</p>
<pre><code>sagemaker.exceptions.UnexpectedStatusException: Error for Training job tensorflow-training-2021-09-15-10-34-05-979: Failed. Reason: AlgorithmError: InstallRequirementsError:
Command &quot;/usr/local/bin/python3.7 -m pip install -r requirements.txt&quot;
WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(&lt;pip._vendor.urllib3.connection.HTTPSConnection object at 0x7f41d0448550&gt;, 'Connection to pypi.org timed out. (connect timeout=15)')': /simple/efficientnet/
</code></pre>
<p>I've specified the subnet and security group in the estimator construct</p>
<pre class=""lang-py prettyprint-override""><code>estimator = TensorFlow(
    entry_point=&quot;train.py&quot;,
    source_dir=job_dir,
    role=role,
    instance_count=1,
    instance_type=instance_type,
    py_version=&quot;py37&quot;,
    framework_version=&quot;2.4&quot;,
    subnets=[environ[&quot;SUBNET_ID&quot;]],
    security_group_ids=[environ[&quot;SECURITY_GROUP_ID&quot;]],
)
</code></pre>
<p>The security group allows all outbound ipv4 traffic, the subnet is public and has an internet gateway.</p>
<p>Moreover I've tested this networking configuration by spawning an ec2 instance in the same subnet-security group, connecting via ssh and successfully installing a pip package.</p>
<p>I can't understand why the sagemaker instance can't connect to pypi.org, nor find a way to debug this issue.</p>",1,0,2021-09-15 13:07:25.797000 UTC,2.0,2021-09-15 18:56:06.623000 UTC,3,tensorflow|amazon-ec2|amazon-vpc|amazon-sagemaker,180,2017-06-13 14:55:04.753000 UTC,2022-08-26 14:12:06.797000 UTC,,339,9,0,7,,,,,,['amazon-sagemaker']
How to download large files from SageMaker Studio Lab?,<p>In Colab one can move large files to Google Drive and then download them easy to local computer but how does one do it in SageMaker Studio Lab? The Download option doesn't work for large files. Do one need to use a script or special application?</p>,2,7,2022-01-09 19:57:35.700000 UTC,1.0,,4,download|amazon-sagemaker|large-files,1056,2022-01-09 19:52:07.493000 UTC,2022-03-29 10:46:27.720000 UTC,,41,0,0,6,,,,,,['amazon-sagemaker']
SageMaker pipeline execution after commit is not triggered,"<p>I am trying to create a simple pipeline in SageMaker Studio with the tutorial presented here: <a href=""https://sagemaker-immersionday.workshop.aws/lab6.html"" rel=""noreferrer"">https://sagemaker-immersionday.workshop.aws/lab6.html</a></p>
<p>I have followed every step and everything seemed fine, but when I pushed the code to the repository, a new pipeline execution was not created.</p>
<p>There are no errors in the SageMaker Studio and the Amazon EventBridge rules (that are used to trigger the pipeline execution) are defined.</p>
<p>Does anyone know how to fix this problem?</p>",0,1,2021-08-11 08:15:54.843000 UTC,,,6,amazon-sagemaker|aws-event-bridge,300,2021-08-11 08:06:07.623000 UTC,2022-04-27 19:11:58.330000 UTC,,61,0,0,1,,,,,,['amazon-sagemaker']
Lambda read DynamoDB and send to ML Endpoint,"<p><strong>Background:</strong></p>

<blockquote>
  <p>I have a DynamoDB table with column's ""TimeStamp | Data1 | Data2"". I
  also have a ML endpoint in SageMaker which needs Data1 and Data2 to
  generate one output value(score).</p>
</blockquote>

<p><strong>Question:</strong></p>

<blockquote>
  <p>My ambition is to script a Lambda function (Java or Python) to read
  the latest row in the DynamoDB table, and send this through the
  Endpoint and receive the score.</p>
</blockquote>

<p><strong>What I have tried:</strong></p>

<blockquote>
  <p>I have only found guides where you do this by exporting the whole
  DynamoDB table to s3 and in Data Pipeline send it to the Endpoint.
  This is not how I want it to work!</p>
</blockquote>",1,0,2018-10-31 12:27:26.833000 UTC,,,1,amazon-web-services|aws-lambda|amazon-sagemaker,782,2015-03-16 08:02:10.157000 UTC,2022-09-14 09:01:01.523000 UTC,,398,37,1,84,,,,,,['amazon-sagemaker']
Vertax AI pipeline quota,"<p>I got a custom_model_training_cpus error when runing a submitted pipeline on Vertex AI. I could not find any documents. And I am using the n1-standard-4 for the deployment machine, I do not see any issue. Any commnents would be much appriciated.</p>
<blockquote>
<p>com.google.cloud.ai.platform.common.errors.AiPlatformException: code=RESOURCE_EXHAUSTED, message=The following quota metrics exceed quota limits: aiplatform.googleapis.com/custom_model_training_cpus, cause=null; Failed to create custom job for the task.</p>
</blockquote>
<p><a href=""https://i.stack.imgur.com/TgCJD.png"" rel=""nofollow noreferrer"">DAG flow and error message</a></p>",1,2,2022-08-16 02:58:55.810000 UTC,,,3,google-cloud-platform|google-cloud-vertex-ai,110,2022-08-16 02:53:42.463000 UTC,2022-08-19 02:01:06.550000 UTC,,31,0,0,0,,,,,,['google-cloud-vertex-ai']
Deployment timeout while deploying the ML model in the Azure Container Instance with the azure pipeline,"<p>Our code was working fine till yesterday while deploying the ML model on Azure ACI, but facing the deployment timeout or sometimes 400 (Bad response) today.</p>
<p>Is there any issue with microsoft datcenters or there is something else ?</p>
<p>The errors are given below</p>
<blockquote>
<p>&quot;message&quot;: &quot;Received bad response from Model Management Service:\nResponse Code: 400\nHeaders: {'Date': 'Thu, 06 Aug 2020 21:05:31 GMT', 'Content-Type': 'application/json', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'Request-Context': 'appId=cid-v1:2d2e8e63-272e-4b3c-8598-4ee570a0e70d', 'x-ms-client-request-id': '8aa4dcafb0ec40ef822b55fc4c7a196f', 'x-ms-client-session-id': 'bf2decff-8879-4a9c-bb5f-2fae7034946c', 'api-supported-versions': '1.0, 2018-03-01-preview, 2018-11-19', 'x-request-time': '0.044', 'Strict-Transport-Security': 'max-age=15724800; includeSubDomains; preload'}\nContent: b'{&quot;code&quot;:&quot;BadRequest&quot;,&quot;statusCode&quot;:400,&quot;message&quot;:&quot;The request is invalid.&quot;,&quot;details&quot;:[{&quot;code&quot;:&quot;InvalidOverwriteRequest&quot;,&quot;message&quot;:&quot;Invalid overwrite request - cannot update container resource requirements, dns name label, or deployment type. Please delete and redeploy this service.&quot;}],&quot;correlation&quot;:{&quot;RequestId&quot;:&quot;8aa4dcafb0ec40ef822b55fc4c7a196f&quot;}}'&quot;</p>
</blockquote>
<blockquote>
<p>&quot;message&quot;: &quot;Service deployment polling reached non-successful terminal state, current service state: Unhealthy\nOperation ID: 1b6746b3-cdb6-4e0c-a43d-b31b34ac99b2\nMore information can be found using '.get_logs()'\nError:\n{\n &quot;code&quot;: &quot;DeploymentTimedOut&quot;,\n &quot;statusCode&quot;: 504,\n &quot;message&quot;: &quot;The deployment operation polling has TimedOut. The service creation is taking longer than our normal time. We are still trying to achieve the desired state for the web service. Please check the webservice state for the current webservice health. You can run print(service.state) from the python SDK to retrieve the current state of the webservice.&quot;\n}&quot;</p>
</blockquote>",1,1,2020-08-07 07:37:36.050000 UTC,,2020-08-07 08:51:44.580000 UTC,0,azure-devops|azure-pipelines|azure-machine-learning-service,606,2020-06-26 15:24:04.877000 UTC,2020-08-27 13:20:22.423000 UTC,"Lahore, Pakistan",11,0,0,9,,,,,,['azure-machine-learning-service']
How to make predictions on a sagemaker endpoint? (JSON error),"<p>I have deployed a sagemaker endpoint and want to run predictions on the endpoint now. The endpoint represents a sagemaker pipeline and model. I followed the tutorial <a href=""https://github.com/aws/amazon-sagemaker-examples/blob/master/sagemaker-pipelines/tabular/train-register-deploy-pipeline-model/train%20register%20and%20deploy%20a%20pipeline%20model.ipynb"" rel=""nofollow noreferrer"">here</a>. My code to set up the predictor and make the predictions is as follows:</p>
<pre><code>from sagemaker.predictor import Predictor
predictor = Predictor(endpoint_name=endpoint_name)
data_df = data_df.drop(&quot;LABEL_NAME&quot;, axis=1)
pred_count = 1
payload = data_df.iloc[:pred_count].to_string(header=False, index=False).replace(&quot;  &quot;, &quot;,&quot;)
p = predictor.predict(payload, initial_args={&quot;ContentType&quot;: &quot;text/csv&quot;})
</code></pre>
<p>This code is pretty much what they have displayed in the example I linked and it makes sense to me. My preprocess.py code for the pipeline includes the following functions which I am including (although not sure they are relevant):</p>
<pre><code>def input_fn(input_data, content_type):
    print(&quot;BAHHHHHH&quot;)
    if content_type == &quot;text/csv&quot;:
        # Read the raw input data as CSV.
        df = pd.read_csv(StringIO(input_data), header=None)
        return df
    else:
        raise ValueError(&quot;{} not supported by script!&quot;.format(content_type))

def output_fn(prediction, accept):
    print(&quot;BAHHHHHH&quot;)
    if accept == &quot;application/json&quot;:
        instances = []
        for row in prediction.tolist():
            instances.append(row)
        json_output = {&quot;instances&quot;: instances}

        return worker.Response(json.dumps(json_output), mimetype=accept)
    elif accept == &quot;text/csv&quot;:
        return worker.Response(encoders.encode(prediction, accept), mimetype=accept)
    else:
        raise RuntimeException(&quot;{} accept type is not supported by this script.&quot;.format(accept))

def predict_fn(input_data, model):
    print(&quot;BAHHHHHH&quot;)
    features = model.transform(input_data)
    return features

def model_fn(model_dir):
    print(&quot;BAHHHHHH&quot;)
    &quot;&quot;&quot;Deserialize fitted model&quot;&quot;&quot;
    preprocessor = joblib.load(os.path.join(model_dir, &quot;model.joblib&quot;))
    return preprocessor
</code></pre>
<p>When running the predictor.predict() method I get the following error:</p>
<pre><code>botocore.errorfactory.ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from primary with message &quot;{
    &quot;error&quot;: &quot;JSON Parse error: Missing a comma or ']' after an array element. at offset: 16&quot;
</code></pre>
<p>I printed out the payload variable right before it was passed to the predict method and it looks like this (I truncated it as it's quite long but this should be enough to see what is is like:</p>
<pre><code>0 999.105105 888.607813 6.0 1 los angeles 2431.666667 1.0 NaN 1177.813623 1.076833e+06 los angeles$1$6 0 60376511012 0.0 0.0 0.0 0.0 0.0 0.0 ............
</code></pre>
<p>The error message also provides a url to look at for more information. It is the cloud watch logs for the endpoint. looking through these logs I see no extra information, just a 400 error with NO additional information apart from the 400 error.</p>
<p>So there is obviously some issue with the format of the data I am passing in. The input_fn, output_fn, predict_fn and model_fn methods all have a print statements in them at the start of the method but none of these show up in the logs so I don't think any of these are being reached.</p>
<p>What am i doing wrong?</p>",1,1,2022-01-02 02:29:44.810000 UTC,,,0,python|amazon-web-services|prediction|amazon-sagemaker,635,2016-07-01 18:37:49.073000 UTC,2022-09-23 18:25:38.923000 UTC,,2733,40,2,273,,,,,,['amazon-sagemaker']
Can I update NGINX version in Azure AKS Cluster?,"<p>Hopefully, I'm posting this in the right place. We have a ML team using an Azure AKS cluster which was built by me. Because this is all built around ML Studio I figured this might be the best place to ask for a dev viewpoint.</p>
<p>A recent security scan identified several open ports on the nodes and workloads which identify themselves as runnning NGINX v1.10.3:</p>
<pre><code>[root ~]# curl 10.210.100.62:32570 -ik
HTTP/1.1 200 OK
Server: nginx/1.10.3 (Ubuntu)
Date: Wed, 09 Mar 2022 14:19:55 GMT
Content-Type: text/html; charset=utf-8
Content-Length: 7
Connection: keep-alive
</code></pre>
<p>The cluster is strictly used to host ML Studio inference endpoints.</p>
<p>The open ports running NGINX are:</p>
<pre><code>5001
31366
31419
32570
</code></pre>
<p>I'm pretty sure 5001 is the listening port on all the inference endpoints, so I imagine it might have something to do with the ML Studio and how it deploys the inference endpoints. The other ports are probably some control ports on the Kubernetes nodes?</p>
<p>I tried updating the kubernetes version on the control plain and nodes. But this didn't make any difference to the running NGINX version, even on the nodes. I connected to one of the hosts in a root shell, but the environment is really stripped and I didn't get very far in trying to identify where NGINX is running from or if it is even possible to update. I suspect attempting to do so in a shell can only break things.</p>
<p>Does anybody know if it is even possible to update this in anyway?</p>",1,0,2022-03-09 15:15:19.210000 UTC,,,0,nginx|azure-aks|azure-machine-learning-service,278,2017-08-31 13:17:58.297000 UTC,2022-09-23 15:50:26.207000 UTC,,473,37,2,66,,,,,,['azure-machine-learning-service']
Invoke endpoint error - detectron2 on AWS Sagemaker: ValueError: Type [application/x-npy] not support this type yet,"<p>I have been following <a href=""https://github.com/aws-samples/amazon-sagemaker-pytorch-detectron2"" rel=""nofollow noreferrer"">this guide</a> for implementing a Detectron2 model on Sagemaker.
It all looks good, both on the training and the batch transform side.</p>
<p>However, I tried to tweak a bit the code to create an Endpoint that can be invoked by sending a payload, and I am having some troubles with it.</p>
<p>At the end of <a href=""https://github.com/aws-samples/amazon-sagemaker-pytorch-detectron2/blob/main/d2_custom_sku110k.ipynb"" rel=""nofollow noreferrer"">this notebook</a>, after creating the SageMaker model object:</p>
<pre><code>model = PyTorchModel(
    name=&quot;d2-sku110k-model&quot;,
    model_data=training_job_artifact,
    role=role,
    sagemaker_session=sm_session,
    entry_point=&quot;predict_sku110k.py&quot;,
    source_dir=&quot;container_serving&quot;,
    image_uri=serve_image_uri,
    framework_version=&quot;1.6.0&quot;,
    code_location=f&quot;s3://{bucket}/{prefix_code}&quot;,
)
</code></pre>
<p>I added the following code:</p>
<pre><code>predictor = model.deploy(initial_instance_count=1, instance_type='ml.m5.xlarge')
</code></pre>
<p>And I can see that the model has been successfully deployed.</p>
<p>However, when I try to predict an image with :</p>
<pre><code>predictor.predict(input)
</code></pre>
<p>I get the following error:</p>
<blockquote>
<p>ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from primary with message &quot;Type [application/x-npy] not support this type yet
Traceback (most recent call last):
File &quot;/opt/conda/lib/python3.6/site-packages/sagemaker_inference/transformer.py&quot;, line 126, in transform
result = self._transform_fn(self._model, input_data, content_type, accept)
File &quot;/opt/conda/lib/python3.6/site-packages/sagemaker_inference/transformer.py&quot;, line 215, in _default_transform_fn
data = self._input_fn(input_data, content_type)
File &quot;/opt/ml/model/code/predict_sku110k.py&quot;, line 98, in input_fn
raise ValueError(err_msg)
ValueError: Type [application/x-npy] not support this type yet</p>
</blockquote>
<p>I tried a bunch of different input types: a image byte-encoded (created with cv2.imencode('.jpg', cv_img)[1].tobytes()), a numpy array, a BytesIO object (created with io module), a dictionary of the form {'input': image} where image is any of the previous (this is because this format was used by a tensorflow endpoint I created some time ago).</p>
<p>As I think it might be relevant, I also copy paste here the <a href=""https://github.com/aws-samples/amazon-sagemaker-pytorch-detectron2/blob/main/container_serving/predict_sku110k.py"" rel=""nofollow noreferrer"">Inference script</a> used as entry point:</p>
<pre><code>&quot;&quot;&quot;Code used for sagemaker batch transform jobs&quot;&quot;&quot;
from typing import BinaryIO, Mapping
import json
import logging
import sys
from pathlib import Path

import numpy as np
import cv2
import torch

from detectron2.engine import DefaultPredictor
from detectron2.config import CfgNode

##############
# Macros
##############

LOGGER = logging.Logger(&quot;InferenceScript&quot;, level=logging.INFO)
HANDLER = logging.StreamHandler(sys.stdout)
HANDLER.setFormatter(logging.Formatter(&quot;%(levelname)s | %(name)s | %(message)s&quot;))
LOGGER.addHandler(HANDLER)

##########
# Deploy
##########
def _load_from_bytearray(request_body: BinaryIO) -&gt; np.ndarray:
    npimg = np.frombuffer(request_body, np.uint8)
    return cv2.imdecode(npimg, cv2.IMREAD_COLOR)


def model_fn(model_dir: str) -&gt; DefaultPredictor:
    r&quot;&quot;&quot;Load trained model

    Parameters
    ----------
    model_dir : str
        S3 location of the model directory

    Returns
    -------
    DefaultPredictor
        PyTorch model created by using Detectron2 API
    &quot;&quot;&quot;
    path_cfg, path_model = None, None
    for p_file in Path(model_dir).iterdir():
        if p_file.suffix == &quot;.json&quot;:
            path_cfg = p_file
        if p_file.suffix == &quot;.pth&quot;:
            path_model = p_file

    LOGGER.info(f&quot;Using configuration specified in {path_cfg}&quot;)
    LOGGER.info(f&quot;Using model saved at {path_model}&quot;)

    if path_model is None:
        err_msg = &quot;Missing model PTH file&quot;
        LOGGER.error(err_msg)
        raise RuntimeError(err_msg)
    if path_cfg is None:
        err_msg = &quot;Missing configuration JSON file&quot;
        LOGGER.error(err_msg)
        raise RuntimeError(err_msg)

    with open(str(path_cfg)) as fid:
        cfg = CfgNode(json.load(fid))

    cfg.MODEL.WEIGHTS = str(path_model)
    cfg.MODEL.DEVICE = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;

    return DefaultPredictor(cfg)


def input_fn(request_body: BinaryIO, request_content_type: str) -&gt; np.ndarray:
    r&quot;&quot;&quot;Parse input data

    Parameters
    ----------
    request_body : BinaryIO
        encoded input image
    request_content_type : str
        type of content

    Returns
    -------
    np.ndarray
        input image

    Raises
    ------
    ValueError
        ValueError if the content type is not `application/x-image`
    &quot;&quot;&quot;
    if request_content_type == &quot;application/x-image&quot;:
        np_image = _load_from_bytearray(request_body)
    else:
        err_msg = f&quot;Type [{request_content_type}] not support this type yet&quot;
        LOGGER.error(err_msg)
        raise ValueError(err_msg)
    return np_image


def predict_fn(input_object: np.ndarray, predictor: DefaultPredictor) -&gt; Mapping:
    r&quot;&quot;&quot;Run Detectron2 prediction

    Parameters
    ----------
    input_object : np.ndarray
        input image
    predictor : DefaultPredictor
        Detectron2 default predictor (see Detectron2 documentation for details)

    Returns
    -------
    Mapping
        a dictionary that contains: the image shape (`image_height`, `image_width`), the predicted
        bounding boxes in format x1y1x2y2 (`pred_boxes`), the confidence scores (`scores`) and the
        labels associated with the bounding boxes (`pred_boxes`)
    &quot;&quot;&quot;
    LOGGER.info(f&quot;Prediction on image of shape {input_object.shape}&quot;)
    outputs = predictor(input_object)
    fmt_out = {
        &quot;image_height&quot;: input_object.shape[0],
        &quot;image_width&quot;: input_object.shape[1],
        &quot;pred_boxes&quot;: outputs[&quot;instances&quot;].pred_boxes.tensor.tolist(),
        &quot;scores&quot;: outputs[&quot;instances&quot;].scores.tolist(),
        &quot;pred_classes&quot;: outputs[&quot;instances&quot;].pred_classes.tolist(),
    }
    LOGGER.info(f&quot;Number of detected boxes: {len(fmt_out['pred_boxes'])}&quot;)
    return fmt_out


# pylint: disable=unused-argument
def output_fn(predictions, response_content_type):
    r&quot;&quot;&quot;Serialize the prediction result into the desired response content type&quot;&quot;&quot;
    return json.dumps(predictions)
</code></pre>
<p>Can anyone point out what is the correct format for invoking the model (or how to tweak the code to use the endpoint)? I am thinking to change the request_content_type to 'application/json', but I am not sure that it will help much.</p>
<p>Edit: I tried a solution inspired by <a href=""https://stackoverflow.com/questions/67622080/aws-sagemaker-custom-pytorch-model-inference-on-raw-image-input"">this SO thread</a> but it did not work for my case.</p>",1,0,2021-11-19 10:26:15.700000 UTC,,2021-11-19 14:29:49.750000 UTC,0,amazon-web-services|pytorch|amazon-sagemaker|detectron,380,2017-10-24 21:38:37.760000 UTC,2022-09-23 10:53:16.547000 UTC,,157,20,0,25,,,,,,['amazon-sagemaker']
Access GlobalParameters in Azure ML Python script,"<p>How can one access the global parameters (""GlobalParameters"") sent from a web service in a Python script on Azure ML?</p>

<p>I tried:</p>

<pre><code>if 'GlobalParameters' in globals():
    myparam = GlobalParameters['myparam']
</code></pre>

<p>but with no success. </p>

<h2>EDIT: Example</h2>

<p>In my case, I'm sending a sound file over the web service (as a list of samples). I would also like to send a sample rate and the number of bits per sample. I've successfully configured the web service (I think) to take these parameters, so the GlobalParameters now look like:</p>

<pre><code>""GlobalParameters"": {
     ""sampleRate"": ""44100"",
     ""bitsPerSample"": ""16"",
}
</code></pre>

<p>However, I cannot access these variables from the Python script, neither as <code>GlobalParameters[""sampleRate""]</code> nor as <code>sampleRate</code>. Is it possible? Where are they stored?</p>",3,2,2015-08-05 16:27:15.673000 UTC,,2015-08-07 17:05:09.633000 UTC,0,python|azure|azure-machine-learning-studio,673,2010-02-19 16:12:07.520000 UTC,2021-10-11 05:53:43.067000 UTC,,7681,284,8,361,,,,,,['azure-machine-learning-studio']
azure ml web service return internal error,"<p>I deployed a web service of multiclass decision forest, but when i tested it, just kept getting this returned:</p>

<p>Error code: InternalError, Http status code: 500</p>

<p>How can I solve it?</p>",1,0,2016-04-21 07:59:33.297000 UTC,,,1,azure-machine-learning-studio,322,2016-04-21 07:44:58.243000 UTC,2016-08-16 08:32:30.650000 UTC,,11,0,0,0,,,,,,['azure-machine-learning-studio']
Sagemaker Instance error: [Errno 28] No space left on device,"<p>I finally managed to train a regressor model with big data using dask and now I am getting this error and I am not sure what to do since I don't see any straight solution for Sagemaker instances.</p>
<p>I even tried increasing the instance size volumen.</p>
<p>Does anyone might know what to do on this specific case scenario?</p>",1,0,2022-07-01 13:20:46.100000 UTC,,,0,scikit-learn|jupyter-notebook|bigdata|amazon-sagemaker,54,2020-04-17 01:31:21.620000 UTC,2022-09-24 12:53:17.577000 UTC,"Caracas, Venezuela",113,25,0,25,,,,,,['amazon-sagemaker']
"In MLflow project using docker environment, how to setup aws credentials","<p>I am working on using 'MLflow' project and one use case is like this.</p>

<pre><code>The MLflow running target/environment is docker.
Data lives on aws s3
When developing on a laptop. The laptop has an aws profile to access data. 
(When developing on EC2, the EC2 have role attached to access s3) 
</code></pre>

<p>Currently, I have credentials stored on the host as '~/.aws/credential', and can access s3 in the host. Question is: In MLflow project, how do I make program running on docker access s3 files? </p>

<p>Note that the question is not ""in general"" how to setup docker. The question is the recommended way to do the aws setup/configuration in a MLflow project. Thanks!</p>",1,0,2019-05-17 23:41:22.550000 UTC,,2019-06-11 07:41:03.490000 UTC,1,docker|amazon-s3|amazon-ec2|mlflow,495,2013-12-11 04:18:22.123000 UTC,2022-09-23 23:07:48.663000 UTC,,3405,641,8,1094,,,,,,['mlflow']
AWS Sagemaker Auto-Scaling | assumed-role role does not have access,"<p>I have a sagemaker instance which I want to auto scale, currently it is working on 4 instances but I want to auto-scale it from 1 to 4, as per the load.</p>

<p>This is the code I am using to auto-scale </p>

<pre><code>resource_id = 'endpoint/[end-point-name]/variant/config1'
sc_client = boto3.client('application-autoscaling')
role = 'arn:aws:iam::[1234]:role/service-role/AmazonSageMaker-ExecutionRole-[1234]'

response = sc_client.register_scalable_target(
    ServiceNamespace='sagemaker',
    ResourceId=resource_id,
    ScalableDimension='sagemaker:variant:DesiredInstanceCount',
    MinCapacity=1,
    MaxCapacity=4,
    RoleARN= role,
    SuspendedState={
        'DynamicScalingInSuspended': True,
        'DynamicScalingOutSuspended': True,
        'ScheduledScalingSuspended': True
    }
)
</code></pre>

<p>I have given all the access (sagemaker and cloudwatch) on all resources to this role : AmazonSageMaker-ExecutionRole-[1234]</p>

<p>Now I am getting this error whenever i ran this code </p>

<pre><code>ClientError: An error occurred (AccessDeniedException) when calling the RegisterScalableTarget 
operation: User: arn:aws:sts::[1234]:assumed-role/AmazonSageMaker-ExecutionRole-[1234]/SageMaker 
is not authorized to perform: iam:PassRole on resource: arn:aws:iam::[1234]:role/service-role/AmazonSageMaker-ExecutionRole-[1234]
</code></pre>

<p>Now I am not sure how it is pickin 'assumed-role' instead of 'service-role' and how to fix the issue, I am using admin account which have all the access and the above 'service-role' also have all the access </p>",1,1,2019-09-18 07:31:53.883000 UTC,,,0,amazon-web-services|aws-sdk|amazon-sagemaker,414,2017-07-30 08:26:08.107000 UTC,2022-09-20 14:45:30.870000 UTC,"Delhi, India",1370,94,1,125,,,,,,['amazon-sagemaker']
How to Access Managed Dataset in Vertex AI using Custom Container,"<p>In the google cloud documentation below:</p>
<p><a href=""https://cloud.google.com/vertex-ai/docs/training/using-managed-datasets#access_a_dataset_from_your_training_application"" rel=""nofollow noreferrer"">https://cloud.google.com/vertex-ai/docs/training/using-managed-datasets#access_a_dataset_from_your_training_application</a></p>
<p>It says that the following environment variables are sent to the training container:</p>
<pre><code>AIP_DATA_FORMAT: The format that your dataset is exported in. Possible values include: jsonl, csv, or bigquery.
AIP_TRAINING_DATA_URI: The location that your training data is stored at.
AIP_VALIDATION_DATA_URI: The location that your validation data is stored at.
AIP_TEST_DATA_URI: The location that your test data is stored at.
</code></pre>
<p>Where each of the URI values are wildcards that annotate training, validation, and test data files in <code>.jsonl</code> format as such:</p>
<pre><code>gs://bucket_name/path/training-*
gs://bucket_name/path/validation-*
gs://bucket_name/path/test-*
</code></pre>
<p><strong>Now, in your custom container that contains the python code, how do you actually access the contents of each of the files?</strong></p>
<p>I've tried splitting the URI string using the following regex to obtain the <code>bucket_name</code> and the <code>prefix</code> info, and attempted the grab it using <code>bucket.list_blobs(delimiter='/', prefix=prefix[:-1])</code> but it returns nothing when the files are definitely there. Here is a minimal example of the attempted code:</p>
<pre><code>import os
import re
from google.cloud import storage

aip_training_data_uri = os.environ.get('AIP_TRAINING_DATA_URI')
match = re.match('gs://(.*?)/(.*)', aip_training_data_uri)
bucket_name, prefix = match.groups()

client = storage.Client()
bucket = client.bucket(bucket_name)
blobs = bucket.list_blobs(delimiter='/', prefix=prefix[:-1]) # &quot;[:-1]&quot; to remove wildcard asterisks

for blob in blobs:
   print(blob.download_as_string()) # This returns an empty string
</code></pre>",0,3,2022-09-02 18:44:38.260000 UTC,,,1,google-cloud-platform|google-cloud-vertex-ai,41,2018-01-26 07:18:10.470000 UTC,2022-09-21 19:53:34.450000 UTC,,705,9,0,42,,,,,,['google-cloud-vertex-ai']
aws auto-stop-idle does not detect papermill,"<p>I am using papermill to parametrize jupyter notebook deployed on AWS Sagemaker. I also used this <a href=""https://github.com/aws-samples/amazon-sagemaker-notebook-instance-lifecycle-config-samples/tree/master/scripts/auto-stop-idle"" rel=""nofollow noreferrer"">lifestyle configuration</a> that will auto shutdown if there are no running/idle notebooks. Unfortunately, it does not detect the Papermill process and continues to shutdown after reaching the specified idle time. What do I need to do to keep Sagemaker alive until the completion of Papermill</p>",1,0,2022-06-08 07:21:50.050000 UTC,,2022-06-08 07:36:16.513000 UTC,1,linux|amazon-web-services|amazon-sagemaker|papermill,29,2014-12-19 07:54:52.670000 UTC,2022-09-22 08:42:47.217000 UTC,Philippines,594,464,8,181,,,,,,['amazon-sagemaker']
Weights and Biases - cumulative max (highwater mark) for distributed training and sweep,"<p>I have an algorithm that I run 10 times, and return the best run by a cumulative maximum - So for each run, I return the highest validation score of the entire run. For example, this graph:
<a href=""https://i.stack.imgur.com/1Iy28.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1Iy28.png"" alt=""actual validation"" /></a></p>
<p>turns to this graph:</p>
<p><a href=""https://i.stack.imgur.com/csUOd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/csUOd.png"" alt=""enter image description here"" /></a></p>
<p>I ran 7 of these, and grouped them together aggregating with maximum. However, since each experiment validates at different timestep, the resulting graph is not a cumulative maximum of the entire 7 runs. That happens because at each validation point, not all runs are present:
<a href=""https://i.stack.imgur.com/VhVqe.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VhVqe.png"" alt=""enter image description here"" /></a></p>
<p>What I would like to have is something like this:
<a href=""https://i.stack.imgur.com/wrlxx.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wrlxx.png"" alt=""enter image description here"" /></a></p>
<ol>
<li>Is this achievable?</li>
<li>How can I set a sweep that uses the cumulative validation of the entire experiment (not not a single trial)?</li>
</ol>",1,0,2022-03-14 14:17:02.323000 UTC,,,0,wandb,101,2019-03-25 10:05:49.353000 UTC,2022-04-01 14:54:28.747000 UTC,Israel,86,3,0,14,,,,,,['wandb']
what are the events (ex MODEL_VERSION_CREATED) associated with ML FLow Databricks CI/CD,<p>ML Flow has multiple events to subscribe like MODEL_VERSION_CREATED when a model version is created. what are the other events available to subscribe.</p>,1,1,2021-09-28 18:27:45.547000 UTC,,,1,databricks|azure-databricks|mlflow,26,2013-06-12 10:44:50.553000 UTC,2021-11-16 16:58:46.133000 UTC,,11,0,0,1,,,,,,['mlflow']
Amazon Sagemaker ResourceLimitExceeded Error for XGBoost (Free Tier),"<p>I am trying to create an XGBoost model in free tier AWS Sagemaker. I am getting an error of:</p>

<p><em>""ResourceLimitExceeded: An error occurred (ResourceLimitExceeded) when calling the CreateEndpoint operation: The account-level service limit 'ml.m5.xlarge for endpoint usage' is 0 Instances, with current utilization of 0 Instances and a request delta of 1 Instances.""</em>.</p>

<p>What is the right train_instance_type I should use?</p>

<p>Here is my code:</p>

<pre><code># import libraries
import boto3, re, sys, math, json, os, sagemaker, urllib.request
from sagemaker import get_execution_role
import numpy as np                                
import pandas as pd                               
import matplotlib.pyplot as plt                   
from IPython.display import Image                 
from IPython.display import display               
from time import gmtime, strftime                 
from sagemaker.predictor import csv_serializer   

# Define IAM role
role = get_execution_role()
prefix = 'sagemaker/DEMO-xgboost-dm'
containers = {'us-west-2': '433757028032.dkr.ecr.us-west-2.amazonaws.com/xgboost:latest',
              'us-east-1': '811284229777.dkr.ecr.us-east-1.amazonaws.com/xgboost:latest',
              'us-east-2': '825641698319.dkr.ecr.us-east-2.amazonaws.com/xgboost:latest',
              'eu-west-1': '685385470294.dkr.ecr.eu-west-1.amazonaws.com/xgboost:latest'} # each region has its XGBoost container
my_region = boto3.session.Session().region_name # set the region of the instance

# Create an instance of the XGBoost model (an estimator), and define the model’s hyperparameters.
# Note: train_instance_type='ml.m5.large' has 0 free credits! Use one of https://aws.amazon.com/sagemaker/pricing/ 
sess = sagemaker.Session()
xgb = sagemaker.estimator.Estimator(containers[my_region],role, train_instance_count=1, train_instance_type='ml.m5.xlarge',output_path='s3://{}/{}/output'.format('my_s3_bucket', prefix),sagemaker_session=sess)
xgb.set_hyperparameters(max_depth=1,eta=0.2,gamma=4,min_child_weight=6,subsample=0.8,silent=0,objective='binary:logistic',num_round=100)
# Train the model using gradient optimization on a ml.m4.xlarge instance
# After a few minutes, you should start to see the training logs being generated.
xgb.fit({'train': s3_input_train})
</code></pre>

<p>At this step this is what I see:</p>

<pre><code>2019-10-22 06:32:51 Starting - Starting the training job...
2019-10-22 06:33:00 Starting - Launching requested ML instances......
2019-10-22 06:33:54 Starting - Preparing the instances for training...
2019-10-22 06:34:41 Downloading - Downloading input data...
2019-10-22 06:35:22 Training - Training image download completed. Training in progress..Arguments: train
[2019-10-22:06:35:22:INFO] Running standalone xgboost training.
[2019-10-22:06:35:22:INFO] Path /opt/ml/input/data/validation does not exist!
[2019-10-22:06:35:22:INFO] File size need to be processed in the node: 3.38mb. Available memory size in the node: 8089.9mb
[2019-10-22:06:35:22:INFO] Determined delimiter of CSV input is ','
[06:35:22] S3DistributionType set as FullyReplicated
[06:35:22] 28831x59 matrix with 1701029 entries loaded from /opt/ml/input/data/train?format=csv&amp;label_column=0&amp;delimiter=,
[06:35:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1
[0]#011train-error:0.102182
[06:35:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1
[1]#011train-error:0.102182
[06:35:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1
[2]#011train-error:0.102182
[06:35:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1
[3]#011train-error:0.102182
[06:35:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1
[4]#011train-error:0.102182
[06:35:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1
[5]#011train-error:0.102182
[06:35:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1
[6]#011train-error:0.102182
[06:35:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1
[7]#011train-error:0.10839
[06:35:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1
[8]#011train-error:0.102737
[06:35:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1
[9]#011train-error:0.107697
</code></pre>

<p>And then when I deploy this:</p>

<pre><code># Deploy the model on a server and create an endpoint that you can access
xgb_predictor = xgb.deploy(initial_instance_count=1,instance_type='ml.m5.xlarge')
---------------------------------------------------------------------------
ResourceLimitExceeded                     Traceback (most recent call last)
&lt;ipython-input-38-6d149f3edc98&gt; in &lt;module&gt;()
      1 # Deploy the model on a server and create an endpoint that you can access
----&gt; 2 xgb_predictor = xgb.deploy(initial_instance_count=1,instance_type='ml.m5.xlarge')

~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/estimator.py in deploy(self, initial_instance_count, instance_type, accelerator_type, endpoint_name, use_compiled_model, update_endpoint, wait, model_name, kms_key, **kwargs)
    559             tags=self.tags,
    560             wait=wait,
--&gt; 561             kms_key=kms_key,
    562         )
    563 

~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/model.py in deploy(self, initial_instance_count, instance_type, accelerator_type, endpoint_name, update_endpoint, tags, kms_key, wait)
    464         else:
    465             self.sagemaker_session.endpoint_from_production_variants(
--&gt; 466                 self.endpoint_name, [production_variant], tags, kms_key, wait
    467             )
    468 

~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/session.py in endpoint_from_production_variants(self, name, production_variants, tags, kms_key, wait)
   1361 
   1362             self.sagemaker_client.create_endpoint_config(**config_options)
-&gt; 1363         return self.create_endpoint(endpoint_name=name, config_name=name, tags=tags, wait=wait)
   1364 
   1365     def expand_role(self, role):

~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/session.py in create_endpoint(self, endpoint_name, config_name, tags, wait)
    975 
    976         self.sagemaker_client.create_endpoint(
--&gt; 977             EndpointName=endpoint_name, EndpointConfigName=config_name, Tags=tags
    978         )
    979         if wait:

~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/client.py in _api_call(self, *args, **kwargs)
    355                     ""%s() only accepts keyword arguments."" % py_operation_name)
    356             # The ""self"" in this scope is referring to the BaseClient.
--&gt; 357             return self._make_api_call(operation_name, kwargs)
    358 
    359         _api_call.__name__ = str(py_operation_name)

~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/client.py in _make_api_call(self, operation_name, api_params)
    659             error_code = parsed_response.get(""Error"", {}).get(""Code"")
    660             error_class = self.exceptions.from_code(error_code)
--&gt; 661             raise error_class(parsed_response, operation_name)
    662         else:
    663             return parsed_response

ResourceLimitExceeded: An error occurred (ResourceLimitExceeded) when calling the CreateEndpoint operation: The account-level service limit 'ml.m5.xlarge for endpoint usage' is 0 Instances, with current utilization of 0 Instances and a request delta of 1 Instances. Please contact AWS support to request an increase for this limit.
</code></pre>

<p><strong>Edit:</strong> Trying <strong>ml.m4.xlarge</strong> instance:</p>

<p>When I use ml.m4.xlarge, I get the same message of ""ResourceLimitExceeded: An error occurred (ResourceLimitExceeded) when calling the CreateEndpoint operation: The account-level service limit 'ml.m4.xlarge for endpoint usage' is 0 Instances, with current utilization of 0 Instances and a request delta of 1 Instances. Please contact AWS support to request an increase for this limit.""</p>",3,0,2019-10-24 06:40:58.980000 UTC,,2019-10-25 11:45:47.573000 UTC,3,python|amazon-web-services|boto3|amazon-sagemaker,5276,2016-06-27 19:09:37.583000 UTC,2022-06-15 16:52:21.103000 UTC,"Dallas, TX, United States",4281,374,160,681,,,,,,['amazon-sagemaker']
Tensorflow model saved_model.load() is getting error to predict multiple batch input,"<pre><code>tokenizer = Tokenizer(num_words=5000)
tokenizer.fit_on_texts(X1_train)

X1_train = tokenizer.texts_to_sequences(X1_train)
X1_val = tokenizer.texts_to_sequences(X1_val)
X1_test = tokenizer.texts_to_sequences(X1_test)

vocab_size = len(tokenizer.word_index) + 1

maxlen = 5000

X1_train = pad_sequences(X1_train, padding='post', maxlen=maxlen)
X1_val = pad_sequences(X1_val, padding='post', maxlen=maxlen)
X1_test = pad_sequences(X1_test, padding='post', maxlen=maxlen)


embeddings_dictionary = dict()
df_g = pd.read_csv('gs://----------/glove.6B.100d.txt', sep=&quot; &quot;, quoting=3, header=None, index_col=0)
embeddings_dictionary = {key: val.values for key, val in df_g.T.items()}

embedding_matrix = zeros((vocab_size, 100))
for word, index in tokenizer.word_index.items():
    embedding_vector = embeddings_dictionary.get(word)
    if embedding_vector is not None:
        embedding_matrix[index] = embedding_vector
input_2_col_list= [x1,x2,...................., x30]
X2_train = X_train[input_2_col_list].values

X2_val = X_val[input_2_col_list].values

X2_test = X_test[[input_2_col_list].values


input_1 = Input(shape=(maxlen,))

input_2 = Input(shape=(30,))

embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], trainable=False)(input_1)
Bi_layer= Bidirectional(LSTM(128, return_sequences=True, dropout=0.15, recurrent_dropout=0.15))(embedding_layer) # Dimn shd be (None,200,128)
con_layer = Conv1D(64, kernel_size=3, padding='valid', kernel_initializer='glorot_uniform')(Bi_layer)
avg_pool = GlobalAveragePooling1D()(con_layer)
max_pool = GlobalMaxPooling1D()(con_layer)

dense_layer_1 = Dense(64, activation='relu')(input_2)
dense_layer_2 = Dense(64, activation='relu')(dense_layer_1)

concat_layer = Concatenate()([avg_pool,max_pool, dense_layer_2])
dense_layer_3 = Dense(50, activation='relu')(concat_layer)
output = Dense(2, activation='softmax')(dense_layer_3)
model = Model(inputs=[input_1, input_2], outputs=output)


model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc',f1_m,precision_m, recall_m])

print(model.summary())

history = model.fit(x=[X1_train, X2_train], y=y_train, batch_size=30, epochs=10, verbose=1, validation_data=([X1_val,X2_val],y_val))

loss, accuracy, f1_score, precision, recall = model.evaluate(x=[X1_test, X2_test], y=y_test, verbose=0)


model.save('gs://----------/Tuned_hybrid_GCP_5000_CASETYPE_8_9.tf')

##################################################

loaded_model=tf.keras.models.load_model( 'gs://----------/Tuned_hybrid_GCP_5000_CASETYPE_8_9.tf', custom_objects={&quot;f1_m&quot;: f1_m , &quot;recall_m&quot;: recall_m, &quot;precision_m&quot;: precision_m } ) 
loss, accuracy, f1_score, precision, recall = loaded_model.evaluate(x=[X1_test, X2_test], y=y_test, verbose=0) ###This is getting no error BUT the predictions are wrong


y_pred = loaded_model.predict(x=[X1_test, X2_test], batch_size=64, verbose=1)
y_pred_bool = np.argmax(y_pred, axis=1)  ###This is getting no error BUT the predictions are wrong
##################################################################


import tensorflow_hub as hub
x=[X1_test, X2_test]
loaded_model_2 = tf.keras.Sequential([hub.KerasLayer('gs:---------------/Tuned_hybrid_GCP_100_CASETYPE_8_11_save.tf')])
loaded_model_2.build(x.shape) #### Getting an error

y_pred_2 = loaded_model_2.predict(x=[X1_test, X2_test], batch_size=64, verbose=1)
y_pred_bool_2 = np.argmax(y_pred_2, axis=1)


###################################################

#### Inside of the model folder the files and dirs are: assets/, variables/, saved_model.pb, keras_metadata.pb
#### Using 'us-docker.pkg.dev/vertex-ai/training/tf-gpu.2-8:latest' to train the model on Vertex AI

</code></pre>
<ol>
<li>I have tried multiple saving a loading function with custom objects, but not of them are working properly</li>
<li>The working loaded model is predicting, but the outputs are not accurate. I have tested the similar TEST data to predict on the loaded model with another test script. The predictions are not matching after I loaded the model.</li>
<li>similar issues on StackOverflow: 'https://stackoverflow.com/questions/68937973/how-can-i-fix-the-problem-of-loading-the-model-to-get-new-predictions'</li>
</ol>",0,0,2022-08-12 15:51:21.417000 UTC,,2022-08-14 03:40:55.943000 UTC,0,tensorflow2.0|google-cloud-vertex-ai,32,2022-08-12 14:39:49.930000 UTC,2022-09-10 02:51:04.710000 UTC,,1,0,0,0,,,,,,['google-cloud-vertex-ai']
Azureml Web Service - How to create a Rest Service from an Experiment to be consumed by a mobile app?,"<p>I've looked all over the google and stackoverflow for the answer but I cant seem to find it. I'm trying to get the output from an azure experiment to an app. I've made the app using <code>ibuildapp</code> and google forms. How can I use the inputs from the google form, pass it to azure and get an output to display on the app?</p>",1,0,2015-04-15 10:26:48.083000 UTC,,2015-04-16 19:48:33.290000 UTC,1,json|web-services|azure|webforms|azure-machine-learning-studio,603,2014-07-18 09:25:23.040000 UTC,2017-05-08 10:19:20.807000 UTC,,23,0,0,9,,,,,,['azure-machine-learning-studio']
How to set learner type for Python Model in Azure Machine Learning Designer?,"<p>I am testing Azure Machine Learning Designer by having a custom Python Model (a simple kNN classification). I would like to tune the value of 'k' and get the best performing model but ""Tune Model Hyperparameters"" module gives following error when giving output from my ""Create Python Model"" as input.</p>

<pre><code>ModuleExceptionMessage:LearnerTypesNotCompatible: Got incompatible learner type: ""None"". Expected learner types are: ""(&lt;TaskType.BinaryClassification: 1&gt;, &lt;TaskType.MultiClassification: 2&gt;, &lt;TaskType.Regression: 3&gt;)"".
</code></pre>

<p>How I can set the learner type of my own Python model? Is it even possible? Should I just code the parameter tuning myself with ""Execute Python Script""-module?</p>

<p>My ""Create Python model""-module script:</p>

<pre><code>import pandas as pd
from sklearn.neighbors import KNeighborsClassifier

class AzureMLModel:
    def __init__(self, k = 3):
        self.model = KNeighborsClassifier(n_neighbors = k)
        self.feature_column_names = list()

    def train(self, df_train, df_label):
        self.feature_column_names = df_train.columns.tolist()
        self.model.fit(df_train, df_label)

    def predict(self, df):
        return pd.DataFrame({'Scored Labels': self.model.predict(df[self.feature_column_names])})
</code></pre>",2,0,2020-06-16 11:17:27.567000 UTC,,2020-06-16 11:22:11.683000 UTC,1,python|azure|machine-learning|azure-machine-learning-studio|azure-machine-learning-service,108,2020-06-16 10:55:49.617000 UTC,2020-06-25 14:16:32.020000 UTC,,11,0,0,1,,,,,,"['azure-machine-learning-service', 'azure-machine-learning-studio']"
Torchvision 0.3.0 for training a model on AML service,"<p>I'm building an image to train on AML service, trying to get torchvision==0.3.0 onboard that image. The notebook VM that I'm using has torchvision 0.3.0 and pytorch 1.1.0 that and it allowed me to do what I'm trying to do... but only on the notebook VM. When I submit the job to AML, I get an error :</p>

<p>Error occurred: module 'torchvision.models' has no attribute 'googlenet'</p>

<p>I've managed to capture the logs at image creation. This a part of the extract that shows partially what's going on:</p>

<pre><code>  Created wheel for dill: filename=dill-0.3.0-cp36-none-any.whl size=77512 sha256=b39463bd613a2337f86181d449e55c84446bb76c2fad462b0ff7ed721872f817

  Stored in directory: /root/.cache/pip/wheels/c9/de/a4/a91eec4eea652104d8c81b633f32ead5eb57d1b294eab24167

Successfully built horovod future json-logging-py psutil absl-py pathspec liac-arff dill

Installing collected packages: tqdm, ptvsd, gunicorn, applicationinsights, urllib3, idna, chardet, requests, asn1crypto, cryptography, pyopenssl, isodate, oauthlib, requests-oauthlib, msrest, jsonpickle, azure-common, PyJWT, python-dateutil, adal, msrestazure, azure-mgmt-authorization, azure-mgmt-containerregistry, pyasn1, ndg-httpsclient, pathspec, azure-mgmt-keyvault, websocket-client, docker, contextlib2, azure-mgmt-resource, backports.weakref, backports.tempfile, jeepney, SecretStorage, pytz, azure-mgmt-storage, ruamel.yaml, azure-graphrbac, jmespath, azureml-core, configparser, json-logging-py, werkzeug, click, MarkupSafe, Jinja2, itsdangerous, flask,liac-arff, pandas, dill, azureml-model-management-sdk, azureml-defaults, torchvision, cloudpickle, psutil, horovod, markdown, protobuf, grpcio, absl-py, tensorboard, future

  Found existing installation: torchvision 0.3.0

    Uninstalling torchvision-0.3.0:

      Successfully uninstalled torchvision-0.3.0

Successfully installed Jinja2-2.10.1 MarkupSafe-1.1.1 PyJWT-1.7.1 SecretStorage-3.1.1 absl-py-0.7.1 adal-1.2.2 applicationinsights-0.11.9 asn1crypto-0.24.0 azure-common-1.1.23 azure-graphrbac-0.61.1 azure-mgmt-authorization-0.60.0 azure-mgmt-containerregistry-2.8.0 azure-mgmt-keyvault-2.0.0 azure-mgmt-resource-3.1.0 azure-mgmt-storage-4.0.0 azureml-core-1.0.55 azureml-defaults-1.0.55 azureml-model-management-sdk-1.0.1b6.post1 backports.tempfile-1.0 backports.weakref-1.0.post1 chardet-3.0.4 click-7.0 cloudpickle-1.2.1 configparser-3.7.4 contextlib2-0.5.5 cryptography-2.7 dill-0.3.0 docker-4.0.2 flask-1.0.3 future-0.17.1 grpcio-1.22.0 gunicorn-19.9.0 horovod-0.16.1 idna-2.8 isodate-0.6.0 itsdangerous-1.1.0 jeepney-0.4.1 jmespath-0.9.4 json-logging-py-0.2 jsonpickle-1.2 liac-arff-2.4.0 markdown-3.1.1 msrest-0.6.9 msrestazure-0.6.1 ndg-httpsclient-0.5.1 oauthlib-3.1.0 pandas-0.25.0 pathspec-0.5.9 protobuf-3.9.1 psutil-5.6.3 ptvsd-4.3.2 pyasn1-0.4.6 pyopenssl-19.0.0 python-dateutil-2.8.0 pytz-2019.2 requests-2.22.0 requests-oauthlib-1.2.0 ruamel.yaml-0.15.89 tensorboard-1.14.0 torchvision-0.2.1 tqdm-4.33.0 urllib3-1.25.3 websocket-client-0.56.0 werkzeug-0.15.5
</code></pre>

<p>Without going into too much details, here's the code that I use to create the estimator, and then, submit the job. Nothing particularly fancy.</p>

<p>I tried debugging the image creation process (looking into the logs) and this is where I've captured what's shown above. I've also tried connecting using a python debugger to the running processes, and/or log to bash inside the running docker container to try python interactive to see what my problem is. Originally the problem is I can't use the <code>torchvision.models.googlenet</code> as it's not figuring in the version in use.</p>

<pre class=""lang-py prettyprint-override""><code>conda_packages=['pytorch', 'scikit-learn', 'torchvision==0.3.0']
pip_packages=['tqdm', 'ptvsd']
</code></pre>

<p>and I create my estimator with this :</p>

<pre class=""lang-py prettyprint-override""><code>pyTorchEstimator = PyTorch(source_directory='./aml-image-models',
                           compute_target=ct,
                           entry_script='train_network.py',
                           script_params=script_params,
                           node_count=1,
                           process_count_per_node=1,
                           conda_packages=conda_packages,
                           pip_packages=pip_packages,
                           use_gpu=True,
                           framework_version = '1.1')
</code></pre>

<p>and submit with typical code.</p>

<p>I'd expect given that I'm specifying 0.3.0 in the dependencies, that it would just work.</p>

<p>Thoughts?</p>",1,0,2019-08-15 02:14:25.717000 UTC,,,0,azure-machine-learning-service,84,2018-09-30 02:52:40.603000 UTC,2022-07-22 02:57:21.830000 UTC,"Montreal, QC, Canada",381,75,2,50,,,,,,['azure-machine-learning-service']
AWS SageMaker + Tensorflow + GPU,"<p>I'm trying to extract ELMo embeddings and running the code on AWS SageMaker and TensorFlow. The code runs fine on cpu instance in SageMaker <strong>but I want to run it on GPU</strong>. Below are the steps I performed and error listed:</p>

<ol>
<li>Started AWS SageMaker instance: ml.p3.8xlarge - it has 8 GPUs </li>
<li>Started JuperterLab and selected conda_tensorflow_p36 for the notebook</li>
<li>Ran the following code:</li>
</ol>

<p>!pip3 install <strong>tensorflow-gpu==1.15</strong>.<br>
!pip3 install ""tensorflow-hub&lt;0.5.0""</p>

<pre><code>url = ""https://tfhub.dev/google/elmo/2""
embed = hub.Module(url)

def defineEmbeddings(start, end, extractions):    
    embeddings = embed(extractions[start:end],signature=""default"",as_dict=True)[""default""]
    return embeddings
def scoreExtractions (embeddings):

    config = tf.compat.v1.ConfigProto(log_device_placement=True)
    config.gpu_options.allow_growth = True
    config.gpu_options.per_process_gpu_memory_fraction = 0.9
    with tf.compat.v1.Session(config=config) as sess:
        sess.run(tf.global_variables_initializer())
        sess.run(tf.tables_initializer())
        x = sess.run(embeddings)
    return x
</code></pre>

<p>I get the following error with sess.run(embeddings)</p>

<p>UnknownError: 2 root error(s) found.
  (0) Unknown: <strong>Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.</strong>
     [[{{node module_apply_default_1/bilm/CNN_1/Conv2D}}]]
     [[module_apply_default_1/truediv/_155]]
  (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
     [[{{node module_apply_default_1/bilm/CNN_1/Conv2D}}]]
0 successful operations.
0 derived errors ignored.</p>

<p>But following works with the above setup:</p>

<pre><code>with tf.device('/device:GPU:3'):
    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')
    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')
    c = tf.matmul(a, b)
with tf.Session() as sess:
    print(sess.run(c))
</code></pre>

<p>Based on my reading up on this issue on StackOverflow, I also tried to use <strong>tensorflow-gpu = 1.8.0</strong> also and with that, I'm getting the error in import tensorflow as tf :</p>

<p><strong>ImportError</strong>: libcublas.so.9.0: cannot open shared object file: No such file or directory</p>",0,1,2020-04-28 17:19:50.093000 UTC,,,1,amazon-web-services|tensorflow|gpu|amazon-sagemaker,454,2019-09-09 16:13:32.273000 UTC,2020-05-20 02:26:47.300000 UTC,,97,6,0,17,,,,,,['amazon-sagemaker']
Annotation value 21 found in labels. This is greater than number of classes,"<p>from the item it seems to be a pretty obvious issue right?</p>

<p>But for the life of me I swear I have 21 labels and 21 classes.</p>

<p>So just as a sanity check I thought i'd ask!</p>

<ol>
<li>I have a load of training images (640,640)</li>
<li>I've gone through them and used DataTurks to annotate the data.</li>
<li>From that I've created a set of PNG masks where I've used 255 for blank space then tan <code>Int</code> for the corresponding number to make an NP array to then convert to a png.</li>
<li>I've then followed this <a href=""https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/semantic_segmentation_pascalvoc/semantic_segmentation_pascalvoc.ipynb"" rel=""nofollow noreferrer"">sagemaker example for segmentation</a> which seems to work until I run <code>ss_model.fit</code>.</li>
</ol>

<p>This is where I start to get some errors. The full log can be seen in <a href=""https://gist.github.com/jackdh/a8fc2816b0845821585b4f022207d9ba"" rel=""nofollow noreferrer"">this Gist</a></p>

<p><strong>The first error</strong> to jump out at me is:</p>

<p><code>label maps not provided, using defaults.</code></p>

<p>Which is strange as I believe I've loaded them correctly in S3 <code>&lt;bucket&gt;/label_map/train_label_map.json</code></p>

<p>That label map looks like so : <a href=""https://gist.github.com/jackdh/44685e5825c921b0387571aae7092b52"" rel=""nofollow noreferrer"">Gist</a> (Perhaps it fails as it's not valid JSON however I was copying how another sagemaker example uses it?)</p>

<p><strong>The second error</strong> to jump out is the one in the title.</p>

<p>Now it could be that my masks are competely wrong (I'm still very new to ML) but them look like this but 640x640:</p>

<pre><code>[ 
   255, 255, 255
   255, 2, 2,
   255, 2, 2
]
</code></pre>

<p>Where 255 is null and 2 is the annotation. </p>

<p>Could this error be because I'm not including the <code>255: ""null""</code> in the label_map ?</p>

<p>Any insight would be really helpful! Thanks.</p>",1,1,2019-01-26 22:04:01.100000 UTC,,,0,amazon-web-services|machine-learning|image-segmentation|amazon-sagemaker|semantic-segmentation,249,2012-08-11 23:03:27.077000 UTC,2022-09-23 15:21:35.443000 UTC,London,2701,382,23,493,,,,,,['amazon-sagemaker']
"How do I set a custom gunicorn worker timeout when serving an MLflow model with the ""mlflow models serve"" CLI?","<p>When serving an MLflow Python model with the ""pyfunc"" backend (<a href=""https://github.com/mlflow/mlflow/blob/master/mlflow/pyfunc/backend.py"" rel=""nofollow noreferrer"">https://github.com/mlflow/mlflow/blob/master/mlflow/pyfunc/backend.py</a>), how can I set a custom gunicorn worker timeout? The default timeout of 60 seconds may be insufficient when serving large models that take a long time to load.</p>",1,0,2019-08-20 16:55:18.683000 UTC,,,2,mlflow,1763,2019-08-20 16:51:28.340000 UTC,2022-09-23 19:12:51.593000 UTC,,61,0,0,2,,,,,,['mlflow']
"How to save/register a model using the ""Execute Python Script"" component in ML Designer?","<p>The out-of-the-box Azure ML Designer components do not allow enough customization for building/scoring my scikit-learn model. I am trying to save the ML model into the <code>outputs</code> folder so I can register it. The <code>outputs</code> folder is not being displayed after running the pipeline nor is the pickle formatted model file.</p>
<p>Error Output:</p>
<pre><code>Got exception when invoking script at line 112 in function azureml_main: 
'FileNotFoundError: [Errno 2] No such file or directory: './outputs/knn_model.pkl''
</code></pre>
<p>Snippet of relevant code:</p>
<pre><code>run=Run.get_context()
clf_model.fit(train_x,train_y)

output_dir = Path('./outputs/')
    if not output_dir.exists():
        output_dir.mkdir()
        
joblib.dump(value=clf_model , filename='./outputs/knn_model.pkl')

run.register_model(model_name='knn_model_security_roles',
                                path='./outputs/knn_model.pkl')
</code></pre>",0,0,2022-08-03 18:20:44.567000 UTC,,2022-08-03 21:02:31.490000 UTC,0,python|scikit-learn|azure-machine-learning-service|azureml-python-sdk|azure-ml-pipelines,44,2019-07-03 18:52:49.727000 UTC,2022-09-01 15:34:07.773000 UTC,,1,0,0,1,,,,,,['azure-machine-learning-service']
Can you create new directories in azure ml designer,"<p>So I'm creating a web service using the azure machine learning designer, I can safely import libraries from the Script Bundle and load files from it. But I can't seem to create new directories in it and save files inside this new directory. Tried using os.mkdirs and pathlib but didn't manage to do it. Is there any way to do so? Or in the current version this is not supported yet</p>
<p>Edit: So to reproduce the problem i'm facing wrote two python scripts connected to a test Script Bundle in the Azure designer, now this zip file only contains a test txt file to make sure the script bundle isn't empty.
As follows in the images the first script creates a new directory in the bundle and creates a DataFrame containing the directories in the root of the zip file, the next script is responsible for creating and saving a txt file inside this newly created directory. What follows are screenshots of the problem and the scripts used:</p>
<p><a href=""https://i.stack.imgur.com/DsyPL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DsyPL.png"" alt=""How the pipline structure looks like"" /></a></p>
<p><a href=""https://i.stack.imgur.com/vXfxN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vXfxN.png"" alt=""The first py script creating the new dir"" /></a></p>
<p><a href=""https://i.stack.imgur.com/9xfjQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9xfjQ.png"" alt=""Trying to save a txt file to this new dir"" /></a></p>",1,3,2021-07-02 20:02:36.070000 UTC,,2021-07-05 13:30:19.783000 UTC,2,python|azure|azure-web-app-service|azure-machine-learning-studio|azure-machine-learning-service,428,2021-07-02 19:59:07.257000 UTC,2022-09-24 00:47:37.170000 UTC,,31,0,0,2,,,,,,"['azure-machine-learning-service', 'azure-machine-learning-studio']"
How to call a model's artifacts (pickeled vectorizer) when the model is on Production in databricks?,"<p>I am using databrick, machine learning view. I have successfully created and saved my model and also logged my pickled vectorizer as artifacts to it. I would like to load it in a different notebook; the model and the artifacts belong to the model which is currently in production.</p>
<pre><code>   import mlflow.pyfunc

model_name = &quot;Sentiment&quot;
stage = 'Production'

model = mlflow.pyfunc.load_model(
    model_uri=f&quot;models:/{model_name}/{stage}&quot;
)
</code></pre>
<p>So this code seems to be working but it does not load the artifacts or if it does, i do not know how to display them.</p>
<p>I have found this code but not sure what to do with it to only get the artifacts from the model which is in Production.</p>
<pre><code>  from mlflow.store.artifact.models_artifact_repo import ModelsArtifactRepository
version =1
model_uri = MlflowClient.get_model_version_download_uri(name=model_name, version=version)
ModelsArtifactRepository(model_uri).download_artifacts(artifact_path=&quot;&quot;)
</code></pre>
<p>even when i run this i get an error :</p>
<p>TypeError: get_model_version_download_uri() missing 1 required positional argument: 'self'</p>",0,0,2022-09-16 12:56:25.133000 UTC,,2022-09-16 13:15:02.563000 UTC,0,model|databricks|mlflow|artifacts,10,2022-06-06 08:24:57.990000 UTC,2022-09-19 13:50:16.620000 UTC,,47,0,0,6,,,,,,['mlflow']
How to set SageMaker xgboost's eval_metric to f1?,"<p>I tried SageMaker's AutoPilot to solve a binary classification problem and I found it is using f1 as the evaluation metric. But when I tried to write some code without tuning like this:</p>
<pre class=""lang-py prettyprint-override""><code>xgb.set_hyperparameters(max_depth=5,
                        eta=0.2,
                        gamma=4,
                        min_child_weight=6,
                        subsample=0.8,
                        objective='binary:logistic',
                        eval_metric='f1',
                        num_round=100)
</code></pre>
<p>This generates the following error:</p>
<blockquote>
<p>[2021-10-17:00:02:19:ERROR] Customer Error: Metric 'f1' is not
supported. Parameter 'eval_metric' should be one of these
options:'rmse', 'mae', 'logloss', 'error', 'merror', 'mlogloss',
'auc', 'ndcg', 'map', 'poisson-nloglik', 'gamma-nloglik',
'gamma-deviance', 'tweedie-nloglik'.</p>
</blockquote>
<p>Since the autopilot was able to compute F1, I feel like it is supported in the hyperparameter setting in some fashion? Am I misunderstanding?</p>
<p>Any help is going to be appreciated.</p>",1,3,2021-10-17 01:22:38.807000 UTC,,2021-10-17 09:08:57.223000 UTC,0,xgboost|amazon-sagemaker|xgbclassifier,388,2011-05-13 06:51:53.437000 UTC,2022-09-25 04:56:44.010000 UTC,,10317,268,1,595,,,,,,['amazon-sagemaker']
VertexAI AutoML SavedModel's directory doesn't have the correct structure,"<p>I'm trying to export an AutoML model trained on VexterAI to Storage, but the structure of the directory of destiny is wrong. The model was exported with the format id <code>tf-saved-model</code>, a TensorFlow model. The command I used to export was the following:</p>
<pre><code>response = model.export_model(export_format_id='tf-saved-model', artifact_destination='gs://storage_path/')
</code></pre>
<p>And I believe the destination directory should have the structure of a TensorFlow SavedModel like:</p>
<pre><code> model/
    ├── assets/
    │   ├── ...
    ├── saved_model.pb
    ├── variables/
    │   ├── ...
</code></pre>
<p>But my directory has the following structure:</p>
<pre><code>model /
   ├── environment.json
   ├── feature_attributions.yaml
   ├── lower_bound/
   |   ├── 001/
   |   |   ├── assets.extra/
   |   |   |   ├── ...
   |   |   ├── assets/
   |   |   |   ├── ...
   |   |   ├── saved_model.pb
   |   |   ├── variables/
   |   |   |   ├── ...
   ├── predict/
   |   ├── 001/
   |   |   ├── assets.extra/
   |   |   |   ├── ...
   |   |   ├── assets/
   |   |   |   ├── ...
   |   |   ├── saved_model.pb
   |   |   ├── variables/
   |   |   |   ├── ...
   ├── upper_bound/
   |   ├── 001/
   |   |   ├── assets.extra/
   |   |   |   ├── ...
   |   |   ├── assets/
   |   |   |   ├── ...
   |   |   ├── saved_model.pb
   |   |   ├── variables/
   |   |   |   ├── ...
   | ...


</code></pre>
<p>I had already searched through so many documentation pages, and I didn't find what I'm doing wrong... Can someone help me?</p>",0,3,2022-06-02 20:13:07.963000 UTC,,,0,tensorflow|google-cloud-platform|google-cloud-storage|google-cloud-automl|google-cloud-vertex-ai,54,2019-07-13 14:14:21.237000 UTC,2022-09-24 22:32:14.170000 UTC,,315,44,0,81,,,,,,['google-cloud-vertex-ai']
How to create a pipeline in sagemaker with pytorch,"<p>I am dealing with a classification problem with text data in sagemaker. Where, i first fit and transform it into structured format(say by using TFIDF in sklearn) then i kept the result in S3 bucket and i used it for training my pytorch model for which i have written the code in my entry point.</p>

<p>if we notice, by the end of the above process, i have two models</p>

<ol>
<li>sklearn TFIDF model </li>
<li>actual PyTorch model</li>
</ol>

<p>So, when every time i need to predict on a new text data, i need to separately process(transform) the text data with TFIDF model which i created during my training.</p>

<p>How can i create a pipeline in sagemaker with sklearn's TFIDF and pytorch models.  </p>

<p>if i fit and transform text data using TFIDF in my main method in entrypoint then if i train my pytorch model in my main method, i can return only one model which will be used in model_fn()</p>",2,0,2019-09-03 08:32:41.723000 UTC,,,0,python|scikit-learn|pytorch|amazon-sagemaker,1034,2018-04-02 06:38:43.863000 UTC,2022-09-18 12:02:20.327000 UTC,,31,0,0,25,,,,,,['amazon-sagemaker']
Importing images Azure Machine Learning Studio,"<p>Is it possible to import images from your Azure storage account from within a Python script module as opposed to using the Import Images module that Azure ML Studio provides. Ideally I would like to use <code>cv2.imread()</code>. I only want to read in grayscale data but the Import Images module reads in RGB. 
Can I use the <code>BlockBlobService</code> library as if I were calling it from an external Python script?</p>",2,0,2017-02-07 03:27:22.043000 UTC,,2017-02-07 06:01:13.667000 UTC,0,python|azure|azure-blob-storage|azure-machine-learning-studio,812,2015-10-12 03:10:51.790000 UTC,2022-09-22 00:25:05.207000 UTC,"Sydney NSW, Australia",823,98,8,48,,,,,,['azure-machine-learning-studio']
"AZURE ML, python code gets: 'tls_process_server_certificate', 'certificate verify failed'","<p>The following code works fine on a jupyter notebook in a compute instance.
However I need to test this locally on Visual Studio Code</p>
<pre><code>import pandas as pd
import datetime
import csv
import numpy as np
from azureml.core import Workspace
from azureml.core import Dataset, Datastore
from azureml.data.datapath import DataPath
from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient
from azureml.core.authentication import AzureCliAuthentication

# #azure-cosmosdb-table
# from azure.cosmosdb.table.tableservice import TableService
# from azure.cosmosdb.table.models import Entity

#Azure blob storage

cli_auth = AzureCliAuthentication()

ws = Workspace(subscription_id=&quot;xx&quot;,
               resource_group=&quot;xx&quot;,
               workspace_name=&quot;xx&quot;,
               auth=cli_auth)

print(&quot;Found workspace {} at location {}&quot;.format(ws.name, ws.location))


import os
from azureml.core.authentication import ServicePrincipalAuthentication

svc_pr_password = &quot;xx&quot; #os.environ.get(&quot;AZUREML_PASSWORD&quot;)

svc_pr = ServicePrincipalAuthentication(
    tenant_id=&quot;xx&quot;,
    service_principal_id=&quot;xx&quot;,
    service_principal_password=svc_pr_password)


ws = Workspace(
    subscription_id=&quot;xx&quot;,
    resource_group=&quot;xx&quot;,
    workspace_name=&quot;xx&quot;,
    auth=svc_pr
    )

print(&quot;Found workspace {} at location {}&quot;.format(ws.name, ws.location))
</code></pre>
<p>However this line:</p>
<pre><code>  datastore = Datastore.get(ws, 'yy')
</code></pre>
<p>gets me this error:</p>
<pre><code>---------------------------------------------------------------------------
Error                                     Traceback (most recent call last)
~\AppData\Local\Programs\Python\Python38\lib\site-packages\urllib3\contrib\pyopenssl.py in wrap_socket(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname)
    487             try:
--&gt; 488                 cnx.do_handshake()
    489             except OpenSSL.SSL.WantReadError:

~\AppData\Local\Programs\Python\Python38\lib\site-packages\OpenSSL\SSL.py in do_handshake(self)
   1933         result = _lib.SSL_do_handshake(self._ssl)
-&gt; 1934         self._raise_ssl_error(self._ssl, result)
   1935 

~\AppData\Local\Programs\Python\Python38\lib\site-packages\OpenSSL\SSL.py in _raise_ssl_error(self, ssl, result)
   1670         else:
-&gt; 1671             _raise_current_error()
   1672 

~\AppData\Local\Programs\Python\Python38\lib\site-packages\OpenSSL\_util.py in exception_from_error_queue(exception_type)
     53 
---&gt; 54     raise exception_type(errors)
     55 

Error: [('SSL routines', 'tls_process_server_certificate', 'certificate verify failed')]

During handling of the above exception, another exception occurred:

SSLError                                  Traceback (most recent call last)
~\AppData\Local\Programs\Python\Python38\lib\site-packages\urllib3\connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)
    669             # Make the request on the httplib connection object.
--&gt; 670             httplib_response = self._make_request(
    671                 conn,

~\AppData\Local\Programs\Python\Python38\lib\site-packages\urllib3\connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw)
    380         try:
--&gt; 381             self._validate_conn(conn)
    382         except (SocketTimeout, BaseSSLError) as e:

~\AppData\Local\Programs\Python\Python38\lib\site-packages\urllib3\connectionpool.py in _validate_conn(self, conn)
    975         if not getattr(conn, &quot;sock&quot;, None):  # AppEngine might not have  `.sock`
--&gt; 976             conn.connect()
    977 

~\AppData\Local\Programs\Python\Python38\lib\site-packages\urllib3\connection.py in connect(self)
    360 
--&gt; 361         self.sock = ssl_wrap_socket(
    362             sock=conn,

~\AppData\Local\Programs\Python\Python38\lib\site-packages\urllib3\util\ssl_.py in ssl_wrap_socket(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data)
    376         if HAS_SNI and server_hostname is not None:
--&gt; 377             return context.wrap_socket(sock, server_hostname=server_hostname)
    378 

~\AppData\Local\Programs\Python\Python38\lib\site-packages\urllib3\contrib\pyopenssl.py in wrap_socket(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname)
    493             except OpenSSL.SSL.Error as e:
--&gt; 494                 raise ssl.SSLError(&quot;bad handshake: %r&quot; % e)
    495             break

SSLError: (&quot;bad handshake: Error([('SSL routines', 'tls_process_server_certificate', 'certificate verify failed')])&quot;,)

During handling of the above exception, another exception occurred:

MaxRetryError                             Traceback (most recent call last)
~\AppData\Local\Programs\Python\Python38\lib\site-packages\requests\adapters.py in send(self, request, stream, timeout, verify, cert, proxies)
    438             if not chunked:
--&gt; 439                 resp = conn.urlopen(
    440                     method=request.method,

~\AppData\Local\Programs\Python\Python38\lib\site-packages\urllib3\connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)
    723 
--&gt; 724             retries = retries.increment(
    725                 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]

~\AppData\Local\Programs\Python\Python38\lib\site-packages\urllib3\util\retry.py in increment(self, method, url, response, error, _pool, _stacktrace)
    438         if new_retry.is_exhausted():
--&gt; 439             raise MaxRetryError(_pool, url, error or ResponseError(cause))
    440 

MaxRetryError: HTTPSConnectionPool(host='westeurope.experiments.azureml.net', port=443): Max retries exceeded with url: /discovery (Caused by SSLError(SSLError(&quot;bad handshake: Error([('SSL routines', 'tls_process_server_certificate', 'certificate verify failed')])&quot;)))

During handling of the above exception, another exception occurred:

SSLError                                  Traceback (most recent call last)
 in 
----&gt; 1 datastore = Datastore.get(ws, 'utilisationdb')

~\AppData\Local\Programs\Python\Python38\lib\site-packages\azureml\core\datastore.py in get(workspace, datastore_name)
    139                 or azureml.data.dbfs_datastore.DBFSDatastore
    140         &quot;&quot;&quot;
--&gt; 141         return Datastore._client().get(workspace, datastore_name)
    142 
    143     @staticmethod

~\AppData\Local\Programs\Python\Python38\lib\site-packages\azureml\data\datastore_client.py in get(workspace, datastore_name)
     63         :rtype: AzureFileDatastore or AzureBlobDatastore
     64         &quot;&quot;&quot;
---&gt; 65         return _DatastoreClient._get(workspace, datastore_name)
     66 
     67     @staticmethod

~\AppData\Local\Programs\Python\Python38\lib\site-packages\azureml\data\datastore_client.py in _get(ws, name, auth, host)
    541         module_logger.debug(&quot;Getting datastore: {}&quot;.format(name))
    542 
--&gt; 543         client = _DatastoreClient._get_client(ws, auth, host)
    544         datastore = client.data_stores.get(subscription_id=ws._subscription_id, resource_group_name=ws._resource_group,
    545                                            workspace_name=ws._workspace_name, name=name,

~\AppData\Local\Programs\Python\Python38\lib\site-packages\azureml\data\datastore_client.py in _get_client(ws, auth, host)
    726         host_env = os.environ.get('AZUREML_SERVICE_ENDPOINT')
    727         auth = auth or ws._auth
--&gt; 728         host = host or host_env or get_service_url(
    729             auth, _DatastoreClient._get_workspace_uri_path(ws._subscription_id, ws._resource_group,
    730                                                            ws._workspace_name), ws._workspace_id, ws.discovery_url)

~\AppData\Local\Programs\Python\Python38\lib\site-packages\azureml\_base_sdk_common\service_discovery.py in get_service_url(auth, workspace_scope, workspace_id, workspace_discovery_url, service_name)
    118 
    119     cached_service_object = CachedServiceDiscovery(auth)
--&gt; 120     return cached_service_object.get_cached_service_url(workspace_scope, service_name,
    121                                                         unique_id=workspace_id, discovery_url=workspace_discovery_url)
    122 

~\AppData\Local\Programs\Python\Python38\lib\site-packages\azureml\_base_sdk_common\service_discovery.py in get_cached_service_url(self, arm_scope, service_name, unique_id, discovery_url)
    280         :rtype: str
    281         &quot;&quot;&quot;
--&gt; 282         return self.get_cached_services_uris(arm_scope, service_name, unique_id=unique_id,
    283                                              discovery_url=discovery_url)[service_name]

~\AppData\Local\Programs\Python\Python38\lib\site-packages\azureml\_base_sdk_common\service_discovery.py in wrapper(self, *args, **kwargs)
    180             try:
    181                 lock_to_use.acquire()
--&gt; 182                 return test_function(self, *args, **kwargs)
    183             finally:
    184                 lock_to_use.release()

~\AppData\Local\Programs\Python\Python38\lib\site-packages\azureml\_base_sdk_common\service_discovery.py in get_cached_services_uris(self, arm_scope, service_name, unique_id, discovery_url)
    255 
    256         # Actual service discovery only understands arm_scope
--&gt; 257         cache[cache_key][DEFAULT_FLIGHT] = super(CachedServiceDiscovery, self).discover_services_uris_from_arm_scope(arm_scope, discovery_url)
    258         try:
    259             with open(self.file_path, &quot;w+&quot;) as file:

~\AppData\Local\Programs\Python\Python38\lib\site-packages\azureml\_base_sdk_common\service_discovery.py in discover_services_uris_from_arm_scope(self, arm_scope, discovery_url)
    136     def discover_services_uris_from_arm_scope(self, arm_scope, discovery_url=None):
    137         discovery_url = self.get_discovery_url(arm_scope, discovery_url)
--&gt; 138         return self.discover_services_uris(discovery_url)
    139 
    140     def discover_services_uris(self, discovery_url=None):

~\AppData\Local\Programs\Python\Python38\lib\site-packages\azureml\_base_sdk_common\service_discovery.py in discover_services_uris(self, discovery_url)
    139 
    140     def discover_services_uris(self, discovery_url=None):
--&gt; 141         status = ClientBase._execute_func(requests.get, discovery_url)
    142         status.raise_for_status()
    143         return status.json()

~\AppData\Local\Programs\Python\Python38\lib\site-packages\azureml\_restclient\clientbase.py in _execute_func(cls, func, *args, **kwargs)
    340     @classmethod
    341     def _execute_func(cls, func, *args, **kwargs):
--&gt; 342         return cls._execute_func_internal(
    343             DEFAULT_BACKOFF, DEFAULT_RETRIES, module_logger, func, _noop_reset, *args, **kwargs)
    344 

~\AppData\Local\Programs\Python\Python38\lib\site-packages\azureml\_restclient\clientbase.py in _execute_func_internal(cls, back_off, total_retry, logger, func, reset_func, *args, **kwargs)
    334                 return func(*args, **kwargs)
    335             except Exception as error:
--&gt; 336                 left_retry = cls._handle_retry(back_off, left_retry, total_retry, error, logger, func)
    337 
    338             reset_func(*args, **kwargs)  # reset_func is expected to undo any side effects from a failed func call.

~\AppData\Local\Programs\Python\Python38\lib\site-packages\azureml\_restclient\clientbase.py in _handle_retry(cls, back_off, left_retry, total_retry, error, logger, func)
    364         &quot;&quot;&quot;
    365         if left_retry == 0:
--&gt; 366             raise error
    367         elif isinstance(error, HttpOperationError):
    368             if error.response.status_code == 403:

~\AppData\Local\Programs\Python\Python38\lib\site-packages\azureml\_restclient\clientbase.py in _execute_func_internal(cls, back_off, total_retry, logger, func, reset_func, *args, **kwargs)
    332         while left_retry &gt;= 0:
    333             try:
--&gt; 334                 return func(*args, **kwargs)
    335             except Exception as error:
    336                 left_retry = cls._handle_retry(back_off, left_retry, total_retry, error, logger, func)

~\AppData\Local\Programs\Python\Python38\lib\site-packages\requests\api.py in get(url, params, **kwargs)
     74 
     75     kwargs.setdefault('allow_redirects', True)
---&gt; 76     return request('get', url, params=params, **kwargs)
     77 
     78 

~\AppData\Local\Programs\Python\Python38\lib\site-packages\requests\api.py in request(method, url, **kwargs)
     59     # cases, and look like a memory leak in others.
     60     with sessions.Session() as session:
---&gt; 61         return session.request(method=method, url=url, **kwargs)
     62 
     63 

~\AppData\Local\Programs\Python\Python38\lib\site-packages\requests\sessions.py in request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)
    528         }
    529         send_kwargs.update(settings)
--&gt; 530         resp = self.send(prep, **send_kwargs)
    531 
    532         return resp

~\AppData\Local\Programs\Python\Python38\lib\site-packages\requests\sessions.py in send(self, request, **kwargs)
    641 
    642         # Send the request
--&gt; 643         r = adapter.send(request, **kwargs)
    644 
    645         # Total elapsed time of the request (approximately)

~\AppData\Local\Programs\Python\Python38\lib\site-packages\requests\adapters.py in send(self, request, stream, timeout, verify, cert, proxies)
    512             if isinstance(e.reason, _SSLError):
    513                 # This branch is for urllib3 v1.22 and later.
--&gt; 514                 raise SSLError(e, request=request)
    515 
    516             raise ConnectionError(e, request=request)

SSLError: HTTPSConnectionPool(host='westeurope.experiments.azureml.net', port=443): Max retries exceeded with url: /discovery (Caused by SSLError(SSLError(&quot;bad handshake: Error([('SSL routines', 'tls_process_server_certificate', 'certificate verify failed')])&quot;)))
</code></pre>",0,0,2020-07-10 14:17:18.313000 UTC,,,1,python|python-3.x|azure|jupyter-notebook|azure-machine-learning-service,219,2011-04-05 19:05:03.093000 UTC,2022-09-16 12:42:27.473000 UTC,"Brussels, Bélgica",30340,1667,79,2937,,,,,,['azure-machine-learning-service']
best practice for datasets conversions for usage in AML,"<p>What are the best practices for large datasets conversion? In many of the cases I deal with there is always a first step where the input dataset is converted to a format that is consumable by the training (I deal with thousands of images). The conversion script was naively created to work locally (input directory - > output directory), and we run inside an estimator (blob storage - > blob storage). Based on the guidelines here <a href=""https://docs.microsoft.com/en-us/azure/machine-learning/how-to-train-with-datasets#mount-vs-download"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/machine-learning/how-to-train-with-datasets#mount-vs-download</a> it looks like is better to do download and then upload rather than mount, am I correct? A part from that what about parallel processing or distributed processing guidelines?</p>

<p>looking at this post: <a href=""https://docs.microsoft.com/en-us/azure/machine-learning/how-to-data-ingest-adf"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/machine-learning/how-to-data-ingest-adf</a>, it looks like they are suggesting to use batch for custom parallel processing. If so what is the advantage of using ADF? Why not use an AML pipeline with a first stage that runs batch?</p>",1,1,2020-03-10 07:09:03.103000 UTC,,,1,azure-data-factory|azure-batch|azure-machine-learning-service,266,2018-03-01 09:03:25.313000 UTC,2022-03-13 07:26:26.350000 UTC,,33,0,0,2,,,,,,['azure-machine-learning-service']
Azure AutoML with historical data context forecast,"<p>I have a question about the peculiar behaviour of Azure AutoML when using forecasting with historical data context.</p>
<p>Basically, I want to apply this usecase from the documentation (<a href=""https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/automated-machine-learning/forecasting-forecast-function/auto-ml-forecasting-function.ipynb"" rel=""nofollow noreferrer"">documentation</a>)</p>
<p><a href=""https://i.stack.imgur.com/Qz6pW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Qz6pW.png"" alt=""Automl usecase"" /></a></p>
<p>The idea is to train a model with historical data (imagine, 3 months of historical data) and then feed the model the current prediction context (for example, the last two weeks) in order to predict a certain prediction horizon.</p>
<p>According to the documentation, to train the model with historical data,  need to do something like this for configuration:</p>
<pre><code>    forecasting_parameters = ForecastingParameters(time_column_name='Timestamp', 
                                               target_aggregation_function = &quot;mean&quot;,
                                               freq='H',
                                               forecast_horizon = prediction_horizon_hours,
                                               target_lags = 'auto',
                                               )

    label = signalTags

automl_config = AutoMLConfig(task='forecasting',
                             primary_metric='normalized_root_mean_squared_error',
                             experiment_timeout_minutes=30,
                             blocked_models=[&quot;AutoArima&quot;],
                             enable_early_stopping=True,
                             training_data=Data,
                             label_column_name=label,
                             n_cross_validations=3,
                             enable_ensembling=False,
                             verbosity=logging.INFO,
                             forecasting_parameters = forecasting_parameters)
</code></pre>
<p>After training, in order to perform a predictiton I need to feed the &quot;context&quot; according to what I want to predict in the form of a dataframe (where the values for the target column are filled in in case of the context and empty in case of values I want to predict) and then just call forecast. Something like this:</p>
<pre><code>     Timestamp                               Signal
0    2022-08-07T23:00:00Z                     63.16
1    2022-08-08T00:00:00Z                     62.92
2    2022-08-08T01:00:00Z                     62.89
3    2022-08-08T02:00:00Z                     62.79
4    2022-08-08T03:00:00Z                     62.75
..                    ...                       ...
233  2022-08-23T17:00:00Z                       nan
234  2022-08-23T18:00:00Z                       nan
235  2022-08-23T19:00:00Z                       nan
236  2022-08-23T20:00:00Z                       nan
237  2022-08-23T21:00:00Z                       nan
</code></pre>
<p>After all this context (pun intended) here is the question/problem.</p>
<p>When I use the above dataframe to forecast ahead I get an error that mentions the following:</p>
<pre><code>ForecastingConfigException:
    Message: Expected column(s) target value column not found in y_pred.
    InnerException: None
    ErrorResponse 
{
    &quot;error&quot;: {
        &quot;code&quot;: &quot;UserError&quot;,
        &quot;message&quot;: &quot;Expected column(s) target value column not found in y_pred.&quot;,
        &quot;target&quot;: &quot;y_pred&quot;,
        &quot;inner_error&quot;: {
            &quot;code&quot;: &quot;BadArgument&quot;,
            &quot;inner_error&quot;: {
                &quot;code&quot;: &quot;MissingColumnsInData&quot;
            }
        },
        &quot;reference_code&quot;: &quot;ac316505-87e4-4877-a855-65a24c3a796b&quot;
    }
}
</code></pre>
<p>However, if I feed a slightly different dataframe (where the data to be forecasted has any other time except exactly on the hour, i.e. 10h30,11h01, 10h23 etc.) it works normally. If I give it something like this:</p>
<pre><code> Timestamp                               Signal
0    2022-08-07T23:00:00Z                     63.16
1    2022-08-08T00:00:00Z                     62.92
2    2022-08-08T01:00:00Z                     62.89
3    2022-08-08T02:00:00Z                     62.79
4    2022-08-08T03:00:00Z                     62.75
..                    ...                       ...
233  2022-08-23T17:00:01Z                       nan
234  2022-08-23T18:00:01Z                       nan
235  2022-08-23T19:00:01Z                       nan
236  2022-08-23T20:00:01Z                       nan
237  2022-08-23T21:00:01Z                       nan
</code></pre>
<p>It outputs good results. What gives?</p>
<p>I have tried resetting the index of the dataframe, replace None with nan but nothing seems to work. Azure Automl can predict any date except ones that are on the hour.</p>
<p>What can I do to fix this?</p>
<p>Thanks!</p>",1,0,2022-08-22 05:55:51.903000 UTC,,,0,azure|azure-machine-learning-service|automl|azuremlsdk,30,2018-01-15 18:38:26.383000 UTC,2022-09-23 08:46:06.340000 UTC,,43,0,0,21,,,,,,['azure-machine-learning-service']
How to continually migrate data from on-premises SQL Db to Azure SQL Db,"<p>As a part of <a href=""https://docs.microsoft.com/en-us/azure/machine-learning/"" rel=""nofollow noreferrer"">Azure Machine Learning</a> process, I need to <code>continually</code> migrate data from on-premises SQL Db to Azure SQL Db using <code>Data Management Gateway</code>.</p>

<p>This Azure official article describes how to: <a href=""https://docs.microsoft.com/en-us/azure/machine-learning/team-data-science-process/move-sql-azure-adf"" rel=""nofollow noreferrer"">Move data from an on-premises SQL server to SQL Azure with Azure Data Factory</a>. But the details are a bit confusing to me. If someone to briefly describe the process, how would you do that. What are 2-3 <code>main</code> steps one needs to perform on <code>on-premises</code> and 2-3 steps on <code>Azure Cloud</code>? No details are needed. <strong>Note</strong>: The solution has to involve using <code>Data Management Gateway</code></p>",1,0,2018-05-15 16:50:46.690000 UTC,1.0,2018-05-15 18:37:13.763000 UTC,1,azure|azure-sql-database|azure-storage|azure-data-factory|azure-machine-learning-studio,103,2012-02-25 04:28:19.340000 UTC,2022-09-24 17:06:32.277000 UTC,,19815,2703,22,2272,,,,,,['azure-machine-learning-studio']
Converting a .pickle model to a tar.gz model for use in Sagemaker,"<p>How would I convert a model saved as a .pickle file to tar.gz format?</p>
<p>This is an XGBoost model that has been trained using Scikit-learn and Cross Validation. The tar.gz format is required for deployment in Amazon Sagemaker.</p>",1,0,2022-03-02 08:14:57.997000 UTC,,,-1,scikit-learn|xgboost|amazon-sagemaker|tar.gz,513,2017-02-09 22:33:42.723000 UTC,2022-09-05 07:12:06.623000 UTC,,9,0,0,7,,,,,,['amazon-sagemaker']
Deploy R application in AWS SageMaker RStudio,"<p>I have an analytics application written in R, using the Shiny package.I build the code using RStudio IDE in AWS SageMaker. Could you please guide the possible ways that I can deploy the web app in AWS and make it available in the public internet. We don't like to use ec2.</p>
<p>Any help would be highly appreciated.</p>",0,2,2022-09-18 06:30:37.947000 UTC,,2022-09-18 13:58:26.507000 UTC,1,r|shiny|deployment|rstudio|amazon-sagemaker,64,2022-03-28 16:33:26.790000 UTC,2022-09-23 12:44:15.100000 UTC,,54,4,0,22,,,,,,['amazon-sagemaker']
"AWS Forecast - What does the parameter ""backtest windows"" really mean?","<p>I am new to &quot;AWS Forecast&quot; but I am training a model on retail data using the AWS UI. Two of the fields to setup the model, have optional input for: &quot;Number of backtest windows&quot; and &quot;Backtest window offset&quot;.</p>
<p>Although, these are optional I want to understand what they mean and the AWS description does not help me understand it. Would anyone be able to provide an example with explicit numbers and dates, so that it makes more sense in how it is applied?</p>
<p>Thanks so much.</p>",1,0,2021-07-19 03:42:18.367000 UTC,,2021-07-19 04:25:10.630000 UTC,1,amazon-web-services|amazon-sagemaker,117,2021-06-18 16:32:04.133000 UTC,2022-08-04 07:57:36.273000 UTC,,105,9,0,17,,,,,,['amazon-sagemaker']
Azure ML Python SDK Unable to get MSI token using identity secret,"<p>I've been using the Azure ML Python SDK to create pipelines for weeks now, but all of the sudden I started getting this error when trying to get the default datastore</p>
<pre><code>ws = Workspace.from_config()
def_blob_store = ws.get_default_datastore()
</code></pre>
<blockquote>
<p>Traceback (most recent call last):   File &quot;lstm_evaluate_pipeline.py&quot;,
line 14, in 
def_blob_store = ws.get_default_datastore()   File &quot;/opt/anaconda3/envs/azure_ml/lib/python3.8/site-packages/azureml/core/workspace.py&quot;,
line 1154, in get_default_datastore
return _DatastoreClient.get_default(self)   File &quot;/opt/anaconda3/envs/azure_ml/lib/python3.8/site-packages/azureml/data/datastore_client.py&quot;,
line 699, in get_default
return _DatastoreClient._get_default(workspace)   File &quot;/opt/anaconda3/envs/azure_ml/lib/python3.8/site-packages/azureml/data/_exception_handler.py&quot;,
line 19, in decorated
raise UserErrorException(str(e)) azureml.exceptions._azureml_exception.UserErrorException:
UserErrorException:   Message: (UserError) Unable to get MSI token
using identity secret. The application associated with this managed
identity  InnerException None     ErrorResponse  {
&quot;error&quot;: {
&quot;code&quot;: &quot;UserError&quot;,
&quot;message&quot;: &quot;(UserError) Unable to get MSI token using identity secret. The application associated with this managed identity&quot;
} }</p>
</blockquote>
<p>How can I fix this? I'm running this on MacOS Monterey in a conda environment using Python 3.8. The sdk version is 1.42.0</p>",1,0,2022-06-17 19:00:53.573000 UTC,,2022-06-17 19:46:24.433000 UTC,0,python|azure|azure-machine-learning-service|azureml-python-sdk,52,2014-05-08 02:24:42.223000 UTC,2022-09-24 23:00:17.213000 UTC,Chicago,3015,821,0,203,,,,,,['azure-machine-learning-service']
Documentation on Vertex AI Feature Store online serving architecture?,"<p>There's documentation on <a href=""https://cloud.google.com/vertex-ai/docs/featurestore/serving-online"" rel=""nofollow noreferrer"">Vertex AI online serving</a>, but no mention of the underlying system being used other than <a href=""https://cloud.google.com/vertex-ai/docs/featurestore/managing-featurestores"" rel=""nofollow noreferrer"">&quot;online serving nodes&quot;</a>. Is it <a href=""https://cloud.google.com/datastore"" rel=""nofollow noreferrer"">Datastore</a>? Something else?</p>",0,0,2022-08-10 03:44:33.257000 UTC,,,0,google-cloud-vertex-ai,21,2013-12-10 22:23:13.850000 UTC,2022-09-21 23:19:21.323000 UTC,,3767,1128,10,226,,,,,,['google-cloud-vertex-ai']
How to load data from your S3 bucket to Sagemaker jupyter notebook to train the model?,"<p>I have csv files in S3 bucket, I want to use those to train model in sagemaker.</p>

<p>using this code but it gives an error (file not found)</p>

<pre><code>import boto3
import pandas as pd
region = boto3.Session().region_name
train_data_location = 's3://taggingu-{}/train.csv'.format(region)
df=pd.read_csv(train_data_location, header = None)
print df.head
</code></pre>

<p>What can be the solution to this ?</p>",1,1,2018-11-27 09:00:27.697000 UTC,,,2,pandas|amazon-web-services|amazon-s3|amazon-sagemaker,6609,2017-08-13 09:52:13.543000 UTC,2019-12-22 07:02:13.140000 UTC,"New Delhi, Delhi, India",47,2,0,22,,,,,,['amazon-sagemaker']
Job submittal fails with : CondaHTTPError: HTTP 000 CONNECTION FAILED,"<p>I'm trying to use the rep <a href=""https://github.com/microsoft/MLAKSDeployAML/"" rel=""nofollow noreferrer"">https://github.com/microsoft/MLAKSDeployAML/</a> to deploy an AML service with AKS. </p>

<p>Created this on an NC6_v2 DSVM machine and after struggling through getting conda to work at all, I finally got my environment setup and started running notebooks. </p>

<p>I submit the experiment then wait on run.wait_for_completion(show_output=True) and it bombs with the HTTP Error. Full control log is attached below.</p>

<p>Is this something to do with being a GPU machine, perhaps, or is there something else going on with the service?</p>

<pre><code>Streaming log file azureml-logs/60_control_log.txt
Starting the daemon thread to refresh tokens in background for process with pid = 13317
nvidia-docker is installed on the target. Using nvidia-docker for docker operations.
Running: ['/bin/bash', '/tmp/azureml_runs/mlaks-train-on-local_1569245453_408a217b/azureml-environment-setup/docker_env_checker.sh']

Materialized image not found on target: azureml/azureml_473a6fe028e178fff5c9a8d49bc938f3


Logging experiment preparation status in history service.
Running: ['/bin/bash', '/tmp/azureml_runs/mlaks-train-on-local_1569245453_408a217b/azureml-environment-setup/docker_env_builder.sh']
Running: ['nvidia-docker', 'build', '-f', 'azureml-environment-setup/Dockerfile', '-t', 'azureml/azureml_473a6fe028e178fff5c9a8d49bc938f3', '.']
Sending build context to Docker daemon  410.1kB
Step 1/15 : FROM continuumio/miniconda3@sha256:54eb3dd4003f11f6a651b55fc2074a0ed6d9eeaa642f1c4c9a7cf8b148a30ceb
 ---&gt; 4a51de2367be
Step 2/15 : USER root
 ---&gt; Using cache
 ---&gt; 42491a367cef
Step 3/15 : RUN mkdir -p $HOME/.cache
 ---&gt; Using cache
 ---&gt; 0771da9ffb76
Step 4/15 : WORKDIR /
 ---&gt; Using cache
 ---&gt; a8db57273ffb
Step 5/15 : COPY azureml-environment-setup/99brokenproxy /etc/apt/apt.conf.d/
 ---&gt; Using cache
 ---&gt; b2a669b740ca
Step 6/15 : RUN if dpkg --compare-versions `conda --version | grep -oE '[^ ]+$'` lt 4.4.11; then conda install conda==4.4.11; fi
 ---&gt; Using cache
 ---&gt; 1e430aeb68b0
Step 7/15 : COPY azureml-environment-setup/mutated_conda_dependencies.yml azureml-environment-setup/mutated_conda_dependencies.yml
 ---&gt; Using cache
 ---&gt; 0c6a9fafa84b
Step 8/15 : RUN ldconfig /usr/local/cuda/lib64/stubs &amp;&amp; conda env create -p /azureml-envs/azureml_6303d702d8163bbfc0017533e979d4a3 -f azureml-environment-setup/mutated_conda_dependencies.yml &amp;&amp; rm -rf ""$HOME/.cache/pip"" &amp;&amp; conda clean -aqy &amp;&amp; CONDA_ROOT_DIR=$(conda info --root) &amp;&amp; rm -rf ""$CONDA_ROOT_DIR/pkgs"" &amp;&amp; find ""$CONDA_ROOT_DIR"" -type d -name __pycache__ -exec rm -rf {} + &amp;&amp; ldconfig
 ---&gt; Running in a579672607b3
Warning: you have pip-installed dependencies in your environment file, but you do not list pip itself as one of your conda dependencies.  Conda may not use the correct pip to install your packages, and they may end up in the wrong place.  Please add an explicit pip dependency.  I'm adding one for you, but still nagging you.
Collecting package metadata (repodata.json): ...working... failed

CondaHTTPError: HTTP 000 CONNECTION FAILED for url &lt;https://conda.anaconda.org/conda-forge/linux-64/repodata.json&gt;
Elapsed: -

An HTTP error occurred when trying to retrieve this URL.
HTTP errors are often intermittent, and a simple retry will get you on your way.
ConnectionError(MaxRetryError(""HTTPSConnectionPool(host='conda.anaconda.org', port=443): Max retries exceeded with url: /conda-forge/linux-64/repodata.json (Caused by NewConnectionError('&lt;urllib3.connection.VerifiedHTTPSConnection object at 0x7fbb8c38cda0&gt;: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))""))


The command '/bin/sh -c ldconfig /usr/local/cuda/lib64/stubs &amp;&amp; conda env create -p /azureml-envs/azureml_6303d702d8163bbfc0017533e979d4a3 -f azureml-environment-setup/mutated_conda_dependencies.yml &amp;&amp; rm -rf ""$HOME/.cache/pip"" &amp;&amp; conda clean -aqy &amp;&amp; CONDA_ROOT_DIR=$(conda info --root) &amp;&amp; rm -rf ""$CONDA_ROOT_DIR/pkgs"" &amp;&amp; find ""$CONDA_ROOT_DIR"" -type d -name __pycache__ -exec rm -rf {} + &amp;&amp; ldconfig' returned a non-zero code: 1


CalledProcessError(1, ['nvidia-docker', 'build', '-f', 'azureml-environment-setup/Dockerfile', '-t', 'azureml/azureml_473a6fe028e178fff5c9a8d49bc938f3', '.'])

Building docker image failed with exit code: 1



Logging error in history service: Failed to run ['/bin/bash', '/tmp/azureml_runs/mlaks-train-on-local_1569245453_408a217b/azureml-environment-setup/docker_env_builder.sh'] 
 Exit code 1 
Details can be found in azureml-logs/60_control_log.txt log file.

Uploading control log...
Sending final run history status...
Logging experiment failed status in history service.
Control script execution completed
</code></pre>",1,0,2019-09-23 13:57:42.700000 UTC,,,1,azure-machine-learning-service,401,2015-05-14 12:33:57.487000 UTC,2022-09-24 12:10:12.350000 UTC,,11,0,0,3,,,,,,['azure-machine-learning-service']
how to export/move my Azure Machine Learning Workspace to different subscription?,"<p>I am using azure machine learning services. I have created an experiment and deployed as a webservice on Azure Machine Learning Workspace. </p>

<p>My problem Is my subscription has expired and now I want to export/move my Azure Machine Learning Workspace to different subscription so I can reuse its all content(model, experiment etc.) without losing.</p>

<p>How can I save my all important work and  export or move Azure Machine Learning Workspace with all working functionalities in different subscription?</p>

<p>Thank you</p>

<p>Regards,</p>

<p>Ahmad</p>",2,0,2020-04-02 13:28:50.850000 UTC,,,0,azure|azure-machine-learning-service,1936,2020-01-24 07:14:02.550000 UTC,2020-04-30 14:52:33.557000 UTC,,11,0,0,13,,,,,,['azure-machine-learning-service']
sagemaker endpoint invocation randomly throws error,"<p>Env:</p>

<ul>
<li>XGBoost model trained on static data (excel).</li>
<li>Model Saved and deployed to Sagemaker with MLFlow.</li>
<li>Sagemaker Model and Sagemaker Endpoint are running.</li>
</ul>

<p>Invocation:
 - I invoke the model with new data via REST Request (POSTMAN) and via boto3.sagemaker</p>

<pre><code>client.invoke_endpoint(
    EndpointName=""xxxxxxx"",
    Body=data,
    ContentType=""application/json"",
    Accept=""string"",
)
</code></pre>

<p>Problem:
It seems that Sagemaker randomly (~50% of the time) fails with following Exception:</p>

<pre><code>{
""ErrorCode"": ""CLIENT_ERROR_FROM_MODEL"",
""LogStreamArn"": ""arn:aws:logs:eu-central-1:xxxxxxxxxx:log-group:/aws/sagemaker/Endpoints/xxxxxxxx"",
""Message"": ""Received client error (400) from mfs-xxxxxxxxx-model-dztwabjotscyoc-zx7lzyfg with message \""{\""error_code\"": \""BAD_REQUEST\"", \""message\"": \""Encountered an unexpected error while evaluating the model. Verify that the serialized input Dataframe is compatible with the model for inference.\"", \""stack_trace\"": \""Traceback (most recent call last):\\n  File \\\""/miniconda/envs/custom_env/lib/python3.6/site-packages/mlflow/pyfunc/scoring_server/__init__.py\\\"", line 196, in transformation\\n    raw_predictions = model.predict(data)\\n  File \\\""/miniconda/envs/custom_env/lib/python3.6/site-packages/mlflow/xgboost.py\\\"", line 198, in predict\\n    return self.xgb_model.predict(xgb.DMatrix(dataframe))\\n  File \\\""/miniconda/envs/custom_env/lib/python3.6/site-packages/xgboost/core.py\\\"", line 1443, in predict\\n    self._validate_features(data)\\n  File \\\""/miniconda/envs/custom_env/lib/python3.6/site-packages/xgboost/core.py\\\"", line 1862, in _validate_features\\n    data.feature_names))\\nValueError: feature_names mismatch: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81'] ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85']\\ntraining data did not have the following fields: 83, 85, 84, 82\\n\""}\"". See https://eu-central-1.console.aws.amazon.com/cloudwatch/home?region=eu-central-1#logEventViewer:group=/aws/sagemaker/Endpoints/xxxxxxxxx in account xxxxxxxfor more information."",
""OriginalMessage"": ""{\""error_code\"": \""BAD_REQUEST\"", \""message\"": \""Encountered an unexpected error while evaluating the model. Verify that the serialized input Dataframe is compatible with the model for inference.\"", \""stack_trace\"": \""Traceback (most recent call last):\\n  File \\\""/miniconda/envs/custom_env/lib/python3.6/site-packages/mlflow/pyfunc/scoring_server/__init__.py\\\"", line 196, in transformation\\n    raw_predictions = model.predict(data)\\n  File \\\""/miniconda/envs/custom_env/lib/python3.6/site-packages/mlflow/xgboost.py\\\"", line 198, in predict\\n    return self.xgb_model.predict(xgb.DMatrix(dataframe))\\n  File \\\""/miniconda/envs/custom_env/lib/python3.6/site-packages/xgboost/core.py\\\"", line 1443, in predict\\n    self._validate_features(data)\\n  File \\\""/miniconda/envs/custom_env/lib/python3.6/site-packages/xgboost/core.py\\\"", line 1862, in _validate_features\\n    data.feature_names))\\nValueError: feature_names mismatch: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81'] ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85']\\ntraining data did not have the following fields: 83, 85, 84, 82\\n\""}"",
""OriginalStatusCode"": 400
</code></pre>

<p>}</p>

<p>After this Exception, i hit again ""Send Request"" in Postman (with the exact same Body Data) and get a successful response:</p>

<pre><code>[
0.9989840388298035
]
</code></pre>

<p>The problem is exactly the same (randomly) when using boto3.sagemaker either in AWS Lambda or locally when testing.</p>

<p><strong>UPDATE 1:</strong>
When i load the exported function with mlflow.xgboost.load_model() and run the prediction, it works everytime. Here is the code:</p>

<pre class=""lang-py prettyprint-override""><code>    import mlflow
    from mlflow import xgboost
    from xgboost import DMatrix
    import numpy as np
    import pandas as pd
    base_path = ""src/models/""


    model_path_name_a = f""{base_path}/model_a""
    model_path_name_b = f""{base_path}/model_b""
    # xgboost.log_model(xgb_model_a, artifact_path='https://mlflow-server-tracking.s3.eu-central-1.amazonaws.com')
    xgb_model_a = xgboost.load_model(model_path_name_a)
    xgb_model_b = xgboost.load_model(model_path_name_b)

    X_test = DMatrix(
        np.array(
            [
                [7.60000e+01, 2.57000e+02, 9.25200e+03, 2.00400e+03, 0.00000e+00, 1.00000e+00, 0.00000e+00, 0.00000e+00,
                 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,
                 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,
                 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,
                 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,
                 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,
                 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,
                 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,
                 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,
                 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 1.00000e+00, 0.00000e+00, 0.00000e+00,
                 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00]
            ]
        )
    )


    ############# Prediction Model A ################
    y_predA = xgb_model_a.predict(X_test)
    print(f""y_predA: {y_predA}"")

    ############# Prediction Model B ################
    y_predB = xgb_model_b.predict(X_test)
    print(f""y_predA: {y_predB}"")
</code></pre>

<p>Does that mean that the problem is with Sagemaker Inference/Endpoint Service?</p>",0,5,2020-04-15 09:13:53.927000 UTC,,2020-04-15 11:20:45.103000 UTC,0,python|amazon-web-services|xgboost|amazon-sagemaker|mlflow,558,2017-02-19 13:12:04.900000 UTC,2022-08-26 11:25:49.307000 UTC,"Nuremberg, Germany",177,227,5,82,,,,,,"['mlflow', 'amazon-sagemaker']"
Does sagemaker use nvidia-docker or docker runtime==nvidia by default or user need to manually set up?,"<p>As stated in the question, ""Does sagemaker use nvidia-docker or docker runtime==nvidia by default or user need to manually set up?""</p>

<p>Some common error message showed as ""CannotStartContainerError. Please ensure the model container for variant variant-name-1 starts correctly when invoked with 'docker run  serve’."" and it didn't show as running with nividia driver.</p>

<p>So, do we need manually set up?</p>",1,0,2018-09-21 06:05:40.017000 UTC,,,0,amazon-web-services|docker|nvidia-docker|amazon-sagemaker,940,2018-09-18 22:59:28.807000 UTC,2021-02-09 19:28:53.127000 UTC,,11,0,0,3,,,,,,['amazon-sagemaker']
How to save model.tar.gz file in sagemaker using Estimator,"<p>I am not able to save model artifacts in S3 bucket using below code. I am successfully able to save the result in output data path and training job is getting completed successfully. I am using the following piece of code.</p>
<p>Can anyone please confirm how do we save the model_artifacts in model-dir using below code.</p>
<pre><code># train.py code
#!/usr/bin/env python

from __future__ import print_function
import os
import sys
import pandas as pd

prefix = '/opt/ml/'

input_dir = prefix + 'input/data'
output_data_dir = os.path.join(prefix, 'output/data')
model_dir = os.path.join(prefix, 'model')

channel_name='training'
training_path = os.path.join(input_dir, channel_name)

# The function to execute the training.
def train():
    print('Starting the training.')
    # Take the set of files and read them all into a single pandas dataframe
    input_files = [ os.path.join(training_path, file) for file in os.listdir(training_path) ]
    raw_data = [ pd.read_csv(file, header=None) for file in input_files ]
    input_data = pd.concat(raw_data)
    
    print(pd.DataFrame(input_data))
    
    output_data = input_data.to_csv(os.path.join(output_data_dir, 'output.csv'), header=False, index=False)
    

if __name__ == '__main__':
    train()

# Below are the S3 input and output paths :
output_path = &quot;s3://{}/{}&quot;.format(bucket, prefix_output)
S3_input = &quot;s3://{}/{}&quot;.format(bucket, prefix)

#Estimator Code

test_estimator = sagemaker.estimator.Estimator(ecr_image,                                # ECR image arn,
                                          role=role,                                 # execution role
                                          instance_count=1,                     # no. of sagemaker instances
                                          instance_type='ml.m4.xlarge',         # instance type
                                          output_path=output_path,              # output path to store model outputs
                                          base_job_name='sagemaker-job1',       # job name prefix
                                          sagemaker_session=session             # session
                                         )

# Launch instance and start training
test_estimator.fit({'training':S3_input})
</code></pre>
<p>What is missing in this code?</p>",1,0,2021-02-15 09:26:37.257000 UTC,,2021-02-15 09:32:50.440000 UTC,2,amazon-sagemaker,3436,2021-02-15 08:56:36.053000 UTC,2022-09-23 13:46:44.050000 UTC,,21,1,0,0,,,,,,['amazon-sagemaker']
How to download a package from GitHub,"<p>I know how to download a project from GitHub as explained <a href=""https://stackoverflow.com/a/6466993/1232087"">here</a>. Working on <a href=""https://docs.microsoft.com/en-us/azure/bot-service/?view=azure-bot-service-3.0"" rel=""nofollow noreferrer"">Microsoft Azure Bot Service</a>. And need to download a <code>package</code> that is for <a href=""https://docs.microsoft.com/en-us/azure/bot-service/bot-service-debug-emulator?view=azure-bot-service-3.0"" rel=""nofollow noreferrer"">Microsoft Bot Framework Emulator</a>. The link for the download is given in the in the section <code>Download the Bot Framework Emulator</code> of <a href=""https://docs.microsoft.com/en-us/azure/bot-service/bot-service-debug-emulator?view=azure-bot-service-3.0"" rel=""nofollow noreferrer"">this</a> Microsoft article. That link takes me to <a href=""https://github.com/Microsoft/BotFramework-Emulator/releases"" rel=""nofollow noreferrer"">this GitHub site</a>. But I'm not clear on how to download the package from there.</p>",1,0,2018-05-16 18:06:16.403000 UTC,,,1,git|azure|github|azure-machine-learning-studio|azure-bot-service,525,2012-02-25 04:28:19.340000 UTC,2022-09-24 17:06:32.277000 UTC,,19815,2703,22,2272,,,,,,['azure-machine-learning-studio']
mlflow on Windows10 and desktop.ini,"<p>it's the first time for me to leave a question here.<br />
I'm currently using PyTorch on my research and trying to organize results with MLFlow.<br />
I know that many problems when using MLflow on Windows10 but since there are no options for this... I'm trying to get used to it.
Errors that I encounter here are &quot;Metrics 'desktop.ini' is malformed ...'<br />
This error is nagging me when -</p>
<ol>
<li>Using <code>mlflow ui</code> to see experiment results from the past <a href=""https://i.stack.imgur.com/k070v.png"" rel=""nofollow noreferrer"">mlflow ui error</a></li>
<li>When trying to use <code>mlflow.pytorch.log_model(model, ...)</code> <a href=""https://i.stack.imgur.com/X3mUX.png"" rel=""nofollow noreferrer"">pytorch.log_model error</a></li>
</ol>
<p>These two are the main concerns for me. My question here is</p>
<ol>
<li>Is there other result organizing tools that I can use except tensorboard?</li>
<li>Is there any method that can save <code>model.pth</code> from pytorch to mlflow? + if it's impossible, are there  any other formats that we use to save the configuration (such as YAML, other hierarchical languages like XML..)</li>
</ol>
<p>Thank you</p>",0,0,2021-03-04 09:10:21.700000 UTC,,,2,pytorch|mlflow,58,2020-07-14 08:34:06.357000 UTC,2022-01-05 07:08:20.457000 UTC,Korea,21,1,0,0,,,,,,['mlflow']
Botocore Error when calling Amazon SageMaker Training Job : Please Ensure that the role exists,"<p>I have an error message that goes like this.</p>
<pre><code>botocore.exceptions.ClientError: An error occurred (ValidationException)
when calling the CreateTrainingJob operation: Could not assume role arn:aws:iam::&lt;'role'&gt;.
 Please ensure that the role exists and allows principal 'sagemaker.amazonaws.com' to assume the role.
</code></pre>
<p>Then, I have a script that goes as</p>
<pre><code>try:
    sagemaker_role = sagemaker.get_execution_role()
except ValueError:
    iam = boto3.client('iam')
    sagemaker_role = iam.get_role(RoleName='&lt;sagemaker-IAM-role-name&gt;')['Role']['Arn']
    
print('role_is', sagemaker_role)
    


training_image = image_uris.retrieve(
    framework=&quot;sklearn&quot;,
    region='us-east-2',
    version='0.20.0',
    py_version=&quot;py3&quot;,
    instance_type=&quot;ml.c5.xlarge&quot;,
)
print(training_image)

sm_boto3 = boto3.client(&quot;sagemaker&quot;, region_name='us-east-2')
print(sm_boto3)

response = sm_boto3.create_training_job( ....
     )
</code></pre>
<p>I think there is a problem with how I setup my botocore.<br />
But I am unsure how to specify my boro3, any link or help would be greately appreciated.</p>",1,0,2022-06-08 01:27:14.083000 UTC,,2022-06-08 02:19:37.517000 UTC,1,amazon-web-services|boto3|amazon-sagemaker,194,2020-09-21 20:00:48.277000 UTC,2022-09-16 06:59:29.497000 UTC,"Seoul, South Korea",69,8,0,4,,,,,,['amazon-sagemaker']
Unable to view Vertex AI pipeline node logs,"<p>I created a Vertex AI pipeline to perform a simple ML flow of creating a dataset, training a model on it and then predicting on the test set. There is a python function based component (train-logistic-model) where I train the model. However, in the component I specify an invalid package and hence the step in the pipeline fails. I know this because when I corrected the package name the step worked fine. However, for the failed pipeline I am unable to see any logs. When I click on the &quot;VIEW JOB&quot; under &quot;Execution Info&quot; on the pipeline Runtime Graph (pic attached) it takes me to the &quot;CUSTOM JOB&quot; page which the pipeline ran. There is a message:</p>
<blockquote>
<p>Custom job failed with error message: The replica workerpool0-0 exited
with a non-zero status of 1 ...</p>
</blockquote>
<p>When I click the VIEW LOGS button, it takes me to the Logs Explorer where there are NO logs. Why are there no logs? Do I need to enable logging somewhere in the pipeline for this? Or could it be a permission issue (it does not mention anything about it though, just this message on the Logs Explorer and 0 logs below it.</p>
<blockquote>
<p>Showing logs for time specified in query. To view more results update
your query</p>
</blockquote>
<p><a href=""https://i.stack.imgur.com/btVwP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/btVwP.png"" alt=""enter image description here"" /></a></p>",0,3,2021-12-14 23:18:47.930000 UTC,,,1,google-cloud-platform|google-ai-platform|google-cloud-vertex-ai|google-cloud-ai-platform-pipelines,216,2016-08-15 20:29:46.790000 UTC,2022-09-24 22:22:50.413000 UTC,,700,17,1,90,,,,,,['google-cloud-vertex-ai']
Schedule the deployment of a sagemaker model,"<p>I'm trying out SageMaker and I've created a model using autopilot. The point is that SageMaker only allows you to deploy directly to an endpoint. But since I'll only be using the model a couple of times a day, what is the most direct way to  schedule deployments by events (for example when loading new csv's into an s3 directory or when I see a queue in sqs) or at least periodically?</p>",2,1,2020-03-04 22:39:47.500000 UTC,,2020-03-04 22:54:22.607000 UTC,0,python|amazon-web-services|amazon-sagemaker,225,2018-12-07 18:01:35.740000 UTC,2022-05-08 18:44:56.820000 UTC,"Santiago, Chile",410,20,1,46,,,,,,['amazon-sagemaker']
Re-using created dataset for different task (object detection - image classification),"<p>I have created a large dataset in Amazon sagemaker and labeled it using bounding boxes. I used this dataset for object detection and everything worked fine.
Later, I wanted to use this dataset for simple image classification. But every time, I try to run it, I get an error: <em><strong>Customer Error: Label was not a float.</strong></em></p>
<p>I think that the problem are probably bounding boxes as the image classification algorithm does not expect them, but is there any way, how to change it?? My goal is to use the parts of image that are in bounding boxes for image classification training. Is there any way, how to set parameters, so that the algorithm could accept as input the information in bounding boxes?</p>
<p>Bellow is a snippet from a log file that was generated, when I tried to run image classification on dataset with bounding boxes.</p>
<pre><code>[14:42:27] /opt/brazil-pkg-cache/packages/AIApplicationsPipeIterators/AIApplicationsPipeIterators-1.0.1145.0/AL2012/generic-flavor/src/data_iter/src/ease_image_iter.cpp:452: JSON Logic Error while parsing 
{
    &quot;annotations&quot;: [
        {
            &quot;class_id&quot;: 0,
            &quot;height&quot;: 194,
            &quot;left&quot;: 34,
            &quot;top&quot;: 16,
            &quot;width&quot;: 150
        }
    ],
    &quot;image_size&quot;: [
        {
            &quot;depth&quot;: 3,
            &quot;height&quot;: 256,
            &quot;width&quot;: 185
        }
    ]
}
: Value is not convertible to float.
</code></pre>
<p>PS: The dataset is an augmented manifest file.</p>
<p>I would be very grateful for any help.</p>",1,0,2020-07-22 08:27:38.483000 UTC,1.0,2020-07-22 08:28:53.607000 UTC,4,object-detection|amazon-sagemaker|logfile|labeling|one-class-classification,108,2020-07-07 13:56:42.820000 UTC,2020-09-16 13:12:19.143000 UTC,,41,25,0,6,,,,,,['amazon-sagemaker']
Connecting Amazon Sagemaker Notebook Instance to Specific S3 Bucket and specific subdirectory in Git Remote Repository,"<p>this is a very tricky one.</p>
<p>I'm very new to Amazon Sagemaker and I can't seem to find any answer to this problem. I don't know if what I want to do is even possible.</p>
<p>Basically, suppose I have a notebook instance on Amazon Sagemaker, I want to connect this notebook instance automatically to:</p>
<ul>
<li>A specific S3 bucket (or even a specific sub-directory inside an S3 bucket)</li>
<li>A <strong>specific sub-directory in a remote Git Repository</strong> <em>(hosted in GitHub/BitBucket/other platforms)</em></li>
</ul>
<p>And this has to be done automatically every time the notebook instance is started. Would something like this be possible?</p>
<p>I've tried looking at lifecycle configurations but as I'm not fully aware of its capabilities, I don't know if it's even possible to do this with the lifecycle config bash script.</p>
<p>I'm very open to other ideas if anyone know how to do something similar to this even if it means that I have to tinker with AWS CLI/Sagemaker SDK/API/GitHub and BitBucket API, other AWS services like lambda, etc.</p>
<p>Thanks heaps in advance!</p>",1,0,2021-07-28 07:37:04.270000 UTC,,,0,git|amazon-web-services|amazon-s3|lifecycle|amazon-sagemaker,386,2017-03-01 10:34:52.173000 UTC,2022-09-25 01:44:27.570000 UTC,,701,9,0,50,,,,,,['amazon-sagemaker']
how to load image data from the bucket to AWS sagemaker notebook?,"<p>The images are present as folders - train and test in my s3 bucket. I want to use them as it is in my sagemaker notebook. For example, like on my local server I use test_dir = ""C:\Users\catvdog\dataset\test"".</p>",2,0,2019-12-22 14:37:43.090000 UTC,,,0,python|machine-learning|amazon-s3|deep-learning|amazon-sagemaker,931,2019-12-19 09:20:45.357000 UTC,2021-09-24 06:27:18.653000 UTC,,43,0,0,19,,,,,,['amazon-sagemaker']
SageMager Studio Notebook Kernel keeps starting,<p>Trying to execute cells in an Amazon SageMager Studio Notebook I continuously receive the message &quot;Note: The kernel is still starting. Please execute this cell again after the kernel is started.&quot; The bottom status bar claims &quot;Kernel: Starting...&quot; The &quot;Running Terminals and Kernels&quot; overview shows a running instance ml.t3.medium with running app datascience-1.0 and kernel session corresponding to the notebook title. I tried restarting SageMaker Studio and opened it in another region but neither helped.</p>,1,2,2022-03-11 15:57:01.243000 UTC,,,1,amazon-web-services|amazon-sagemaker,661,2012-03-17 14:47:44.973000 UTC,2022-08-24 18:17:30.443000 UTC,,377,29,0,57,,,,,,['amazon-sagemaker']
How to use cx_Oracle within AWS SageMaker (Jupyter Notebook),"<p>I've followed the official <a href=""https://oracle.github.io/odpi/doc/installation.html#linux"" rel=""nofollow noreferrer"">installation guide</a> but haven't had any luck so far. I wonder if <code>cx_Oracle</code> <em>can</em> work on AWS SageMaker's virtual environment. The steps I've used so far are:</p>

<ol>
<li>Create a <code>/opt/oracle</code> directory and unzip the basic instantclient in it.</li>
<li><code>sudo yum install libaio</code></li>
<li><code>sudo sh -c ""echo /opt/oracle/instantclient_18_3 &gt; /etc/ld.so.conf.d/oracle-instantclient.conf""</code> and
<code>sudo ldconfig</code></li>
<li>And finally exported the <code>LD_LIBRARY_PATH</code> with: <code>export LD_LIBRARY_PATH=/opt/oracle/instantclient_18_3:$LD_LIBRARY_PATH</code></li>
</ol>

<p>When trying to run a connection inside the notebook with <code>connection = cx_Oracle.connect(usr + '/' + pwd + '@' + url)</code>, I receive the <code>DPI-1047</code> error code that says that <code>libclntsh.so</code> cannot be open, however that library <em>is</em> in the <code>/opt/oracle</code> folder. As another option, when running the same connection through the terminal Python console, I get the <code>ORA-01804</code> error code, which says that the timezone files were not properly read, which is something I'm trying to fix also but suspect is related to <code>cx_Oracle</code> not finding its library folder. (Now, explain to me: why does it have to be so difficult for a billionaire company to create a decent library import and installation?)</p>

<p>Is there a step I'm missing? Is there a detail about AWS SageMaker that I should account for? Also, is there another option for extracting data from an oracle server through Python and AWS?</p>",1,5,2019-03-29 17:43:16.203000 UTC,1.0,,1,python-3.x|amazon-web-services|cx-oracle|amazon-sagemaker,1573,2015-04-06 20:51:24.247000 UTC,2022-09-24 23:05:58.387000 UTC,São Paulo - Brazil,4934,1446,42,605,,,,,,['amazon-sagemaker']
"Best way to train a deep learning model in AWS Sagemaker, when data (image data > 10000 images) lies on a S3 bucket","<p>What is the best was to train a deep learning model on AWS Sagemaker when I have a huge image dataset stored on a AWS S3 bucket.
The Dataset shouldn't be downloaded to the EBS volume of the notebook instance.</p>",1,0,2022-07-02 11:28:51.573000 UTC,,,0,amazon-web-services|amazon-s3|deep-learning|amazon-sagemaker,43,2016-05-01 12:35:36.470000 UTC,2022-09-24 10:05:59.780000 UTC,"Chennai, India",21,48,0,7,,,,,,['amazon-sagemaker']
Reinstall opencv will bug the entire project for opencv import,"<p>I was trying to install opencv-contrib-python but had trouble getting a hold of img_hash.
When I try to reinstall opencv-python, it seems to mess up the entire cnoda enviornment. Is there any way to reset the environment from scratch?</p>
<pre><code>import cv2
!pip uninstall -y opencv-python
!pip uninstall -y opencv-contrib-python
!pip install opencv-python
!pip install opencv-contrib-python
hsh = cv2.img_hash.BlockMeanHash_create()
cv2.imread('image.png')
hsh.compute(a_1)
</code></pre>
<p>Error:</p>
<pre><code>~/.conda/envs/default/lib/python3.9/site-packages/cv2/gapi/__init__.py in &lt;module&gt;
    288 
    289 
--&gt; 290 cv.gapi.wip.GStreamerPipeline = cv.gapi_wip_gst_GStreamerPipeline

AttributeError: partially initialized module 'cv2' has no attribute 'gapi_wip_gst_GStreamerPipeline' (most likely due to a circular import)
</code></pre>",0,0,2022-06-24 02:53:34.693000 UTC,,,0,amazon-sagemaker,55,2010-10-04 00:23:33.433000 UTC,2022-06-26 16:50:47.600000 UTC,,1941,11,1,157,,,,,,['amazon-sagemaker']
How to install wandb on a docker image for arm?,"<p>My docker building failed at the <code>RUN </code></p>
<p>with:</p>
<pre><code>(meta_learning) brandomiranda~ ❯ docker build -f ~/iit-term-synthesis/Dockerfile_arm -t brandojazz/iit-term-synthesis:test_arm ~/iit-term-synthesis/

[+] Building 184.7s (20/28)
 =&gt; [internal] load build definition from Dockerfile_arm                                                                                           0.0s
 =&gt; =&gt; transferring dockerfile: 41B                                                                                                                0.0s
 =&gt; [internal] load .dockerignore                                                                                                                  0.0s
 =&gt; =&gt; transferring context: 2B                                                                                                                    0.0s
 =&gt; [internal] load metadata for docker.io/continuumio/miniconda3:latest                                                                           0.0s
 =&gt; [ 1/24] FROM docker.io/continuumio/miniconda3                                                                                                  0.0s
 =&gt; https://api.github.com/repos/IBM/pycoq/git/refs/heads/main                                                                                     0.3s
 =&gt; CACHED [ 2/24] RUN apt-get update   &amp;&amp; apt-get install -y --no-install-recommends     ssh     git     m4     libgmp-dev     opam     wget      0.0s
 =&gt; CACHED [ 3/24] RUN useradd -m bot                                                                                                              0.0s
 =&gt; CACHED [ 4/24] WORKDIR /home/bot                                                                                                               0.0s
 =&gt; CACHED [ 5/24] ADD https://api.github.com/repos/IBM/pycoq/git/refs/heads/main version.json                                                     0.0s
 =&gt; CACHED [ 6/24] RUN opam init --disable-sandboxing                                                                                              0.0s
 =&gt; CACHED [ 7/24] RUN opam switch create ocaml-variants.4.07.1+flambda_coq-serapi.8.11.0+0.11.1 ocaml-variants.4.07.1+flambda                     0.0s
 =&gt; CACHED [ 8/24] RUN opam switch ocaml-variants.4.07.1+flambda_coq-serapi.8.11.0+0.11.1                                                          0.0s
 =&gt; CACHED [ 9/24] RUN eval $(opam env)                                                                                                            0.0s
 =&gt; CACHED [10/24] RUN opam repo add coq-released https://coq.inria.fr/opam/released                                                               0.0s
 =&gt; CACHED [11/24] RUN opam repo --all-switches add --set-default coq-released https://coq.inria.fr/opam/released                                  0.0s
 =&gt; CACHED [12/24] RUN opam update --all                                                                                                           0.0s
 =&gt; CACHED [13/24] RUN opam pin add -y coq 8.11.0                                                                                                  0.0s
 =&gt; [14/24] RUN opam install -y coq-serapi                                                                                                       176.3s
 =&gt; [15/24] RUN eval $(opam env)                                                                                                                   0.2s
 =&gt; ERROR [16/24] RUN pip install wandb --upgrade                                                                                                  8.0s
------
 &gt; [16/24] RUN pip install wandb --upgrade:
#20 0.351 Defaulting to user installation because normal site-packages is not writeable
#20 0.637 Collecting wandb
#20 0.986   Downloading wandb-0.13.2-py2.py3-none-any.whl (1.8 MB)
#20 1.365 Requirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from wandb) (61.2.0)
#20 1.366 Requirement already satisfied: six&gt;=1.13.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (1.16.0)
#20 1.409 Collecting promise&lt;3,&gt;=2.0
#20 1.472   Downloading promise-2.3.tar.gz (19 kB)
#20 2.087 Collecting PyYAML
#20 2.154   Downloading PyYAML-6.0-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (731 kB)
#20 2.431 Collecting protobuf&lt;4.0dev,&gt;=3.12.0
#20 2.492   Downloading protobuf-3.20.1-cp39-cp39-manylinux2014_aarch64.whl (917 kB)
#20 2.648 Collecting setproctitle
#20 2.706   Downloading setproctitle-1.3.2-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (30 kB)
#20 2.763 Collecting Click!=8.0.0,&gt;=7.0
#20 2.818   Downloading click-8.1.3-py3-none-any.whl (96 kB)
#20 2.902 Collecting sentry-sdk&gt;=1.0.0
#20 2.962   Downloading sentry_sdk-1.9.8-py2.py3-none-any.whl (158 kB)
#20 3.112 Collecting psutil&gt;=5.0.0
#20 3.172   Downloading psutil-5.9.2.tar.gz (479 kB)
#20 3.871 Collecting pathtools
#20 3.937   Downloading pathtools-0.1.2.tar.gz (11 kB)
#20 4.431 Collecting shortuuid&gt;=0.5.0
#20 4.509   Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)
#20 4.512 Requirement already satisfied: requests&lt;3,&gt;=2.0.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (2.27.1)
#20 4.568 Collecting docker-pycreds&gt;=0.4.0
#20 4.636   Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)
#20 4.695 Collecting GitPython&gt;=1.0.0
#20 4.781   Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)
#20 4.834 Collecting gitdb&lt;5,&gt;=4.0.1
#20 4.892   Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)
#20 4.934 Collecting smmap&lt;6,&gt;=3.0.1
#20 4.992   Downloading smmap-5.0.0-py3-none-any.whl (24 kB)
#20 5.005 Requirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests&lt;3,&gt;=2.0.0-&gt;wandb) (1.26.8)
#20 5.005 Requirement already satisfied: certifi&gt;=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests&lt;3,&gt;=2.0.0-&gt;wandb) (2021.10.8)
#20 5.006 Requirement already satisfied: idna&lt;4,&gt;=2.5 in /opt/conda/lib/python3.9/site-packages (from requests&lt;3,&gt;=2.0.0-&gt;wandb) (3.3)
#20 5.006 Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests&lt;3,&gt;=2.0.0-&gt;wandb) (2.0.4)
#20 5.075 Collecting urllib3&lt;1.27,&gt;=1.21.1
#20 5.135   Downloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)
#20 5.172 Building wheels for collected packages: promise, psutil, pathtools
#20 5.172   Building wheel for promise (setup.py): started
#20 5.851   Building wheel for promise (setup.py): finished with status 'done'
#20 5.852   Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21503 sha256=6de0373376d2a8e995959e6173507e13cba502c79b648b5884b1eac45d1ec9ae
#20 5.852   Stored in directory: /home/bot/.cache/pip/wheels/e1/e8/83/ddea66100678d139b14bc87692ece57c6a2a937956d2532608
#20 5.854   Building wheel for psutil (setup.py): started
#20 6.226   Building wheel for psutil (setup.py): finished with status 'error'
#20 6.226   ERROR: Command errored out with exit status 1:
#20 6.226    command: /opt/conda/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'/tmp/pip-install-vgietl2j/psutil_c905945489d349018aaad0a17600df0b/setup.py'&quot;'&quot;'; __file__='&quot;'&quot;'/tmp/pip-install-vgietl2j/psutil_c905945489d349018aaad0a17600df0b/setup.py'&quot;'&quot;';f = getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(__file__) if os.path.exists(__file__) else io.StringIO('&quot;'&quot;'from setuptools import setup; setup()'&quot;'&quot;');code = f.read().replace('&quot;'&quot;'\r\n'&quot;'&quot;', '&quot;'&quot;'\n'&quot;'&quot;');f.close();exec(compile(code, __file__, '&quot;'&quot;'exec'&quot;'&quot;'))' bdist_wheel -d /tmp/pip-wheel-4y62c4eb
#20 6.226        cwd: /tmp/pip-install-vgietl2j/psutil_c905945489d349018aaad0a17600df0b/
#20 6.226   Complete output (45 lines):
#20 6.226   running bdist_wheel
#20 6.226   running build
#20 6.226   running build_py
#20 6.226   creating build
#20 6.226   creating build/lib.linux-aarch64-3.9
#20 6.226   creating build/lib.linux-aarch64-3.9/psutil
#20 6.226   copying psutil/_psosx.py -&gt; build/lib.linux-aarch64-3.9/psutil
#20 6.226   copying psutil/_psbsd.py -&gt; build/lib.linux-aarch64-3.9/psutil
#20 6.226   copying psutil/_common.py -&gt; build/lib.linux-aarch64-3.9/psutil
#20 6.226   copying psutil/_pswindows.py -&gt; build/lib.linux-aarch64-3.9/psutil
#20 6.226   copying psutil/_psposix.py -&gt; build/lib.linux-aarch64-3.9/psutil
#20 6.226   copying psutil/__init__.py -&gt; build/lib.linux-aarch64-3.9/psutil
#20 6.226   copying psutil/_compat.py -&gt; build/lib.linux-aarch64-3.9/psutil
#20 6.226   copying psutil/_pslinux.py -&gt; build/lib.linux-aarch64-3.9/psutil
#20 6.226   copying psutil/_pssunos.py -&gt; build/lib.linux-aarch64-3.9/psutil
#20 6.226   copying psutil/_psaix.py -&gt; build/lib.linux-aarch64-3.9/psutil
#20 6.226   creating build/lib.linux-aarch64-3.9/psutil/tests
#20 6.226   copying psutil/tests/__main__.py -&gt; build/lib.linux-aarch64-3.9/psutil/tests
#20 6.226   copying psutil/tests/test_process.py -&gt; build/lib.linux-aarch64-3.9/psutil/tests
#20 6.226   copying psutil/tests/test_aix.py -&gt; build/lib.linux-aarch64-3.9/psutil/tests
#20 6.226   copying psutil/tests/test_misc.py -&gt; build/lib.linux-aarch64-3.9/psutil/tests
#20 6.226   copying psutil/tests/test_bsd.py -&gt; build/lib.linux-aarch64-3.9/psutil/tests
#20 6.226   copying psutil/tests/test_linux.py -&gt; build/lib.linux-aarch64-3.9/psutil/tests
#20 6.226   copying psutil/tests/runner.py -&gt; build/lib.linux-aarch64-3.9/psutil/tests
#20 6.226   copying psutil/tests/__init__.py -&gt; build/lib.linux-aarch64-3.9/psutil/tests
#20 6.226   copying psutil/tests/test_connections.py -&gt; build/lib.linux-aarch64-3.9/psutil/tests
#20 6.226   copying psutil/tests/test_unicode.py -&gt; build/lib.linux-aarch64-3.9/psutil/tests
#20 6.226   copying psutil/tests/test_windows.py -&gt; build/lib.linux-aarch64-3.9/psutil/tests
#20 6.226   copying psutil/tests/test_contracts.py -&gt; build/lib.linux-aarch64-3.9/psutil/tests
#20 6.226   copying psutil/tests/test_sunos.py -&gt; build/lib.linux-aarch64-3.9/psutil/tests
#20 6.226   copying psutil/tests/test_testutils.py -&gt; build/lib.linux-aarch64-3.9/psutil/tests
#20 6.226   copying psutil/tests/test_osx.py -&gt; build/lib.linux-aarch64-3.9/psutil/tests
#20 6.226   copying psutil/tests/test_memleaks.py -&gt; build/lib.linux-aarch64-3.9/psutil/tests
#20 6.226   copying psutil/tests/test_posix.py -&gt; build/lib.linux-aarch64-3.9/psutil/tests
#20 6.226   copying psutil/tests/test_system.py -&gt; build/lib.linux-aarch64-3.9/psutil/tests
#20 6.226   running build_ext
#20 6.226   building 'psutil._psutil_linux' extension
#20 6.226   creating build/temp.linux-aarch64-3.9
#20 6.226   creating build/temp.linux-aarch64-3.9/psutil
#20 6.226   gcc -pthread -B /opt/conda/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -n1 .2-a+fp16+rcpc+dotprod+crypto -isystem /opt/conda/include -I/opt/conda/include -fPIC -O2 -n1 .2-a+fp16+rcpc+dotprod+crypto -isystem /opt/conda/include -fPIC -DPSUTIL_POSIX=1 -DPSUTIL_SIZEOF_PID_T=4 -DPSUTIL_VERSION=592 -DPSUTIL_LINUX=1 -I/opt/conda/include/python3.9 -c psutil/_psutil_common.c -o build/temp.linux-aarch64-3.9/psutil/_psutil_common.o
#20 6.226   gcc: error: .2-a+fp16+rcpc+dotprod+crypto: No such file or directory
#20 6.226   gcc: error: .2-a+fp16+rcpc+dotprod+crypto: No such file or directory
#20 6.226   gcc: error: unrecognized command-line option ‘-n1’; did you mean ‘-n’?
#20 6.226   gcc: error: unrecognized command-line option ‘-n1’; did you mean ‘-n’?
#20 6.226   error: command '/usr/bin/gcc' failed with exit code 1
#20 6.226   ----------------------------------------
#20 6.226   ERROR: Failed building wheel for psutil
#20 6.226   Running setup.py clean for psutil
#20 6.550   Building wheel for pathtools (setup.py): started
#20 7.135   Building wheel for pathtools (setup.py): finished with status 'done'
#20 7.135   Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=8e205a0f68c9c7a3c0107d1cc40d94f1d2843c78270217378dcbe98212958b82
#20 7.135   Stored in directory: /home/bot/.cache/pip/wheels/b7/0a/67/ada2a22079218c75a88361c0782855cc72aebc4d18d0289d05
#20 7.136 Successfully built promise pathtools
#20 7.136 Failed to build psutil
#20 7.195 Installing collected packages: smmap, urllib3, gitdb, shortuuid, setproctitle, sentry-sdk, PyYAML, psutil, protobuf, promise, pathtools, GitPython, docker-pycreds, Click, wandb
#20 7.262   WARNING: The script shortuuid is installed in '/home/bot/.local/bin' which is not on PATH.
#20 7.262   Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.
#20 7.345     Running setup.py install for psutil: started
#20 7.727     Running setup.py install for psutil: finished with status 'error'
#20 7.727     ERROR: Command errored out with exit status 1:
#20 7.727      command: /opt/conda/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'/tmp/pip-install-vgietl2j/psutil_c905945489d349018aaad0a17600df0b/setup.py'&quot;'&quot;'; __file__='&quot;'&quot;'/tmp/pip-install-vgietl2j/psutil_c905945489d349018aaad0a17600df0b/setup.py'&quot;'&quot;';f = getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(__file__) if os.path.exists(__file__) else io.StringIO('&quot;'&quot;'from setuptools import setup; setup()'&quot;'&quot;');code = f.read().replace('&quot;'&quot;'\r\n'&quot;'&quot;', '&quot;'&quot;'\n'&quot;'&quot;');f.close();exec(compile(code, __file__, '&quot;'&quot;'exec'&quot;'&quot;'))' install --record /tmp/pip-record-gb2y421d/install-record.txt --single-version-externally-managed --user --prefix= --compile --install-headers /home/bot/.local/include/python3.9/psutil
#20 7.727          cwd: /tmp/pip-install-vgietl2j/psutil_c905945489d349018aaad0a17600df0b/
#20 7.727     Complete output (47 lines):
#20 7.727     running install
#20 7.727     /opt/conda/lib/python3.9/site-packages/setuptools/command/install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.
#20 7.727       warnings.warn(
#20 7.727     running build
#20 7.727     running build_py
#20 7.727     creating build
#20 7.727     creating build/lib.linux-aarch64-3.9
#20 7.727     creating build/lib.linux-aarch64-3.9/psutil
#20 7.727     copying psutil/_psosx.py -&gt; build/lib.linux-aarch64-3.9/psutil
#20 7.727     copying psutil/_psbsd.py -&gt; build/lib.linux-aarch64-3.9/psutil
#20 7.727     copying psutil/_common.py -&gt; build/lib.linux-aarch64-3.9/psutil
#20 7.727     copying psutil/_pswindows.py -&gt; build/lib.linux-aarch64-3.9/psutil
#20 7.727     copying psutil/_psposix.py -&gt; build/lib.linux-aarch64-3.9/psutil
#20 7.727     copying psutil/__init__.py -&gt; build/lib.linux-aarch64-3.9/psutil
#20 7.727     copying psutil/_compat.py -&gt; build/lib.linux-aarch64-3.9/psutil
#20 7.727     copying psutil/_pslinux.py -&gt; build/lib.linux-aarch64-3.9/psutil
#20 7.727     copying psutil/_pssunos.py -&gt; build/lib.linux-aarch64-3.9/psutil
#20 7.727     copying psutil/_psaix.py -&gt; build/lib.linux-aarch64-3.9/psutil
#20 7.727     creating build/lib.linux-aarch64-3.9/psutil/tests
#20 7.727     copying psutil/tests/__main__.py -&gt; build/lib.linux-aarch64-3.9/psutil/tests
#20 7.727     copying psutil/tests/test_process.py -&gt; build/lib.linux-aarch64-3.9/psutil/tests
#20 7.727     copying psutil/tests/test_aix.py -&gt; build/lib.linux-aarch64-3.9/psutil/tests
#20 7.727     copying psutil/tests/test_misc.py -&gt; build/lib.linux-aarch64-3.9/psutil/tests
#20 7.727     copying psutil/tests/test_bsd.py -&gt; build/lib.linux-aarch64-3.9/psutil/tests
#20 7.727     copying psutil/tests/test_linux.py -&gt; build/lib.linux-aarch64-3.9/psutil/tests
#20 7.727     copying psutil/tests/runner.py -&gt; build/lib.linux-aarch64-3.9/psutil/tests
#20 7.727     copying psutil/tests/__init__.py -&gt; build/lib.linux-aarch64-3.9/psutil/tests
#20 7.727     copying psutil/tests/test_connections.py -&gt; build/lib.linux-aarch64-3.9/psutil/tests
#20 7.727     copying psutil/tests/test_unicode.py -&gt; build/lib.linux-aarch64-3.9/psutil/tests
#20 7.727     copying psutil/tests/test_windows.py -&gt; build/lib.linux-aarch64-3.9/psutil/tests
#20 7.727     copying psutil/tests/test_contracts.py -&gt; build/lib.linux-aarch64-3.9/psutil/tests
#20 7.727     copying psutil/tests/test_sunos.py -&gt; build/lib.linux-aarch64-3.9/psutil/tests
#20 7.727     copying psutil/tests/test_testutils.py -&gt; build/lib.linux-aarch64-3.9/psutil/tests
#20 7.727     copying psutil/tests/test_osx.py -&gt; build/lib.linux-aarch64-3.9/psutil/tests
#20 7.727     copying psutil/tests/test_memleaks.py -&gt; build/lib.linux-aarch64-3.9/psutil/tests
#20 7.727     copying psutil/tests/test_posix.py -&gt; build/lib.linux-aarch64-3.9/psutil/tests
#20 7.727     copying psutil/tests/test_system.py -&gt; build/lib.linux-aarch64-3.9/psutil/tests
#20 7.727     running build_ext
#20 7.727     building 'psutil._psutil_linux' extension
#20 7.727     creating build/temp.linux-aarch64-3.9
#20 7.727     creating build/temp.linux-aarch64-3.9/psutil
#20 7.727     gcc -pthread -B /opt/conda/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -n1 .2-a+fp16+rcpc+dotprod+crypto -isystem /opt/conda/include -I/opt/conda/include -fPIC -O2 -n1 .2-a+fp16+rcpc+dotprod+crypto -isystem /opt/conda/include -fPIC -DPSUTIL_POSIX=1 -DPSUTIL_SIZEOF_PID_T=4 -DPSUTIL_VERSION=592 -DPSUTIL_LINUX=1 -I/opt/conda/include/python3.9 -c psutil/_psutil_common.c -o build/temp.linux-aarch64-3.9/psutil/_psutil_common.o
#20 7.727     gcc: error: .2-a+fp16+rcpc+dotprod+crypto: No such file or directory
#20 7.727     gcc: error: .2-a+fp16+rcpc+dotprod+crypto: No such file or directory
#20 7.727     gcc: error: unrecognized command-line option ‘-n1’; did you mean ‘-n’?
#20 7.727     gcc: error: unrecognized command-line option ‘-n1’; did you mean ‘-n’?
#20 7.727     error: command '/usr/bin/gcc' failed with exit code 1
#20 7.727     ----------------------------------------
#20 7.728 ERROR: Command errored out with exit status 1: /opt/conda/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'/tmp/pip-install-vgietl2j/psutil_c905945489d349018aaad0a17600df0b/setup.py'&quot;'&quot;'; __file__='&quot;'&quot;'/tmp/pip-install-vgietl2j/psutil_c905945489d349018aaad0a17600df0b/setup.py'&quot;'&quot;';f = getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(__file__) if os.path.exists(__file__) else io.StringIO('&quot;'&quot;'from setuptools import setup; setup()'&quot;'&quot;');code = f.read().replace('&quot;'&quot;'\r\n'&quot;'&quot;', '&quot;'&quot;'\n'&quot;'&quot;');f.close();exec(compile(code, __file__, '&quot;'&quot;'exec'&quot;'&quot;'))' install --record /tmp/pip-record-gb2y421d/install-record.txt --single-version-externally-managed --user --prefix= --compile --install-headers /home/bot/.local/include/python3.9/psutil Check the logs for full command output.
------
executor failed running [/bin/sh -c pip install wandb --upgrade]: exit code: 1
</code></pre>
<p>why?</p>
<p>Docker file so far:</p>
<pre><code>FROM continuumio/miniconda3

RUN apt-get update \
  &amp;&amp; apt-get install -y --no-install-recommends \
    ssh \
    git \
    m4 \
    libgmp-dev \
    opam \
    wget \
    ca-certificates \
    rsync \
    strace

RUN useradd -m bot
WORKDIR /home/bot
USER bot

## https://stackoverflow.com/questions/73642349/how-to-have-miniconda-work-properly-with-docker-especially-naming-my-conda-en
#RUN wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh  \
#    &amp;&amp; bash Miniconda3-latest-Linux-x86_64.sh -b -f
#ENV PATH=&quot;/home/bot/miniconda3/bin:${PATH}&quot;
#RUN conda create -n pycoq python=3.9 -y
## somehow this &quot;works&quot; but conda isn't fully aware of this. Fix later?
#ENV PATH=&quot;/home/bot/miniconda3/envs/pycoq/bin:${PATH}&quot;

ADD https://api.github.com/repos/IBM/pycoq/git/refs/heads/main version.json

# -- setup opam like VP's PyCoq
RUN opam init --disable-sandboxing
# compiler + '_' + coq_serapi + '.' + coq_serapi_pin
RUN opam switch create ocaml-variants.4.07.1+flambda_coq-serapi.8.11.0+0.11.1 ocaml-variants.4.07.1+flambda
RUN opam switch ocaml-variants.4.07.1+flambda_coq-serapi.8.11.0+0.11.1
RUN eval $(opam env)

RUN opam repo add coq-released https://coq.inria.fr/opam/released
# RUN opam pin add -y coq 8.11.0
# ['opam', 'repo', '--all-switches', 'add', '--set-default', 'coq-released', 'https://coq.inria.fr/opam/released']
RUN opam repo --all-switches add --set-default coq-released https://coq.inria.fr/opam/released
RUN opam update --all
RUN opam pin add -y coq 8.11.0

#RUN opam install -y --switch ocaml-variants.4.07.1+flambda_coq-serapi_coq-serapi_8.11.0+0.11.1 coq-serapi 8.11.0+0.11.1
RUN opam install -y coq-serapi

RUN eval $(opam env)

# makes sure depedencies for pycoq are installed once already in the docker image
ENV WANDB_API_KEY=&quot;SECRET&quot;
RUN pip install wandb --upgrade
</code></pre>
<p><a href=""https://community.wandb.ai/t/how-to-install-wandb-on-a-docker-image-for-arm/3080"" rel=""nofollow noreferrer"">https://community.wandb.ai/t/how-to-install-wandb-on-a-docker-image-for-arm/3080</a></p>",2,3,2022-09-08 00:07:15.817000 UTC,,,0,python|linux|docker|anaconda|wandb,49,2012-06-21 21:22:10.287000 UTC,2022-09-25 02:41:28.613000 UTC,,11435,1807,299,6472,,,,,,['wandb']
how can I preprocess input data before making predictions in sagemaker?,"<p>I am calling a Sagemaker endpoint using java Sagemaker SDK. The data that I am sending needs little cleaning before the model can use it for prediction. How can I do that in Sagemaker.</p>

<p>I have a pre-processing function in the Jupyter notebook instance which is cleaning the training data before passing that data to train the model. Now I want to know if I can use that function while calling the endpoint or is that function already being used?
I can show my code if anyone wants?</p>

<p><strong>EDIT 1</strong>
Basically, in the pre-processing, I am doing label encoding. Here is my function for preprocessing</p>

<pre><code>def preprocess_data(data):
 print(""entering preprocess fn"")
 # convert document id &amp; type to labels
 le1 = preprocessing.LabelEncoder()
 le1.fit(data[""documentId""])
 data[""documentId""]=le1.transform(data[""documentId""])
 le2 = preprocessing.LabelEncoder()
 le2.fit(data[""documentType""])
 data[""documentType""]=le2.transform(data[""documentType""])
 print(""exiting preprocess fn"")
 return data,le1,le2
</code></pre>

<p>Here the 'data' is a pandas dataframe.</p>

<p>Now I want to use these le1,le2 at the time of calling endpoint. I want to do this preprocessing in sagemaker itself not in my java code.  </p>",4,0,2018-03-30 20:40:06.957000 UTC,,2018-04-05 07:46:08.473000 UTC,8,aws-java-sdk|amazon-sagemaker,4215,2015-05-01 05:16:40.073000 UTC,2022-09-21 20:19:42.027000 UTC,,863,50,0,79,,,,,,['amazon-sagemaker']
Improving Sagemaker latency,"<p>I have created a model using notebook and using java aws sdk when i invoke the endpoint its taking around 7sec. How can i reduce this further and is there any way to have parallel calls.</p>

<pre><code>InvokeEndpointRequest request = new InvokeEndpointRequest();
InvokeEndpointResult p = amazonSageMakerRuntime.invokeEndpoint(request.withEndpointName(""&lt;endpoint&gt;"").withAccept(""application/json"").withContentType(""application/json"").withBody(ByteBuffer.wrap(data.getBytes())));
</code></pre>",0,1,2019-07-02 15:21:41.200000 UTC,,,1,amazon-sagemaker,825,2018-10-15 09:33:27.860000 UTC,2022-09-22 13:55:04.627000 UTC,,33,1,0,39,,,,,,['amazon-sagemaker']
How to log cross validation results as a table in azureml?,"<p>I recently started using the <code>azureml</code> for automated experiments and logging in the Azure Machine Learning Studio.</p>
<p>In an experiment, I'd like to store the results from a <code>GridSearchCV</code> in a table.</p>
<pre><code>run.log_table(name='Gridsearch results', value=search.cv_results_)
</code></pre>
<p>According to the <a href=""https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.run(class)?view=azure-ml-py"" rel=""nofollow noreferrer"">documentation</a>, <code>value</code> should be a dictionary. In my case, it looks as the dictionary at the bottom of this question. However, I get the following error:</p>
<pre><code>Value of type &lt;class 'list'&gt; is not supported, supported types include [[&lt;class 'float'&gt;, &lt;class 'str'&gt;, &lt;class 'bool'&gt;, &lt;class 'NoneType'&gt;, &lt;class 'int'&gt;]]
</code></pre>
<p>Even transforming it to a format similar to the one given in the documentation using</p>
<pre><code>run.log_table(name='Gridsearch results', value=pd.DataFrame(search.cv_results_).to_dict(orient=&quot;list&quot;))
</code></pre>
<p>yields the same error. Any ideas?</p>
<pre><code>{'mean_fit_time': array([ 4.44100904,  0.01762947, 12.24124289,  0.01914111]),
 'std_fit_time': array([1.66466241e+00, 8.54067066e-04, 2.84891905e+00, 1.26775086e-03]), 
 'mean_score_time': array([0.00462735, 0.00775236, 0.00512046, 0.00737476]),
 'std_score_time': array([0.00048182, 0.0004347 , 0.00092224, 0.00069597]),
 'param_C': masked_array(data=[1, 1, 10, 10], mask=[False, False, False, False], fill_value='?', dtype=object),
 'param_kernel': masked_array(data=['linear', 'rbf', 'linear', 'rbf'],
             mask=[False, False, False, False],
       fill_value='?',
            dtype=object), 'params': [{'C': 1, 'kernel': 'linear'}, {'C': 1, 'kernel': 'rbf'}, {'C': 10, 'kernel': 'linear'}, {'C': 10, 'kernel': 'rbf'}], 'split0_test_score': array([0.81111111, 0.54444444, 0.81111111, 0.62222222]), 'split1_test_score': array([0.75555556, 0.61111111, 0.75555556, 0.64444444]), 'split2_test_score': array([0.80898876, 0.75280899, 0.80898876, 0.7752809 ]), 'split3_test_score': array([0.7752809 , 0.65168539, 0.7752809 , 0.69662921]), 'split4_test_score': array([0.78651685, 0.69662921, 0.78651685, 0.76404494]), 'split5_test_score': array([0.71910112, 0.68539326, 0.71910112, 0.70786517]), 'split6_test_score': array([0.79775281, 0.74157303, 0.79775281, 0.7752809 ]), 'split7_test_score': array([0.78651685, 0.6741573 , 0.78651685, 0.75280899]), 'mean_test_score': array([0.780103  , 0.66972534, 0.780103  , 0.7173221 ]), 'std_test_score': array([0.02858483, 0.06374784, 0.02858483, 0.0559392 ]), 'rank_test_score': array([1, 4, 1, 3])}
</code></pre>",1,0,2021-06-30 10:23:19.657000 UTC,,2021-06-30 11:07:01.440000 UTC,0,python|scikit-learn|azure-machine-learning-studio|azure-machine-learning-service|azureml-python-sdk,131,2020-04-06 10:20:27.240000 UTC,2022-09-23 14:24:39.773000 UTC,,299,44,2,21,,,,,,"['azure-machine-learning-service', 'azure-machine-learning-studio']"
How to overcome client error in aws sagemaker when I try to create a S3 bucket?,"<p>I am getting ClientError: An error occurred (AccessDenied) when calling the CreateBucket operation: Access Denied when I try to create aws s3 bucket from the notebook instance of aws sagemaker.<a href=""https://i.stack.imgur.com/TQVUw.png"" rel=""nofollow noreferrer"">enter image description here</a> Please anyone reply to this issue to solve.</p>",0,2,2021-12-12 07:12:50.730000 UTC,,,0,python|amazon-sagemaker|amazon-s3,119,2021-09-08 12:05:32.300000 UTC,2021-12-27 05:55:15.403000 UTC,"Chennai, Tamil Nadu, India",1,0,0,1,,,,,,['amazon-sagemaker']
AWS Sagemaker Notebook Stuck on Pending,"<p>I have an AWS Sagemaker notebook that is I attempted to launch again. The status of the notebook has been <code>Pending</code> for over 3 hours now. I've had a look at the Cloudwatch logs and the last few entry in there are: </p>

<pre><code>[I 19:14:57.107 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[W 19:14:57.138 NotebookApp] No web browser found: could not locate runnable browser.
[I 19:14:57.140 NotebookApp] Starting initial scan of virtual environments...
[I 19:15:28.507 NotebookApp] Found new kernels in environments: conda_pytorch_p36, conda_amazonei_mxnet_p27, conda_chainer_p27, conda_mxnet_p27, conda_tensorflow_p27, conda_amazonei_tensorflow_p27, conda_amazonei_tensorflow_p36, conda_mxnet_p36, conda_python3, conda_tensorflow_p36, conda_python2, conda_pytorch_p27, conda_chainer_p36, conda_amazonei_mxnet_p36
</code></pre>

<p>There isn't anything in the logs the would indicate why it failed. Looking at that the last time I launched everything looks identical to that point. Is there anything I can do start the notebook or stop and relaunch the notebook?</p>",1,0,2020-03-18 22:43:40.993000 UTC,1.0,,1,amazon-web-services|amazon-sagemaker,3553,2013-07-22 01:18:53.410000 UTC,2022-09-25 03:42:45.950000 UTC,"Montreal, QC, Canada",2306,1283,3,182,,,,,,['amazon-sagemaker']
Alter JSON file in CLOUD SHELL Terminal,"<p>this is my file :</p>
<pre><code>$ cat INPUT-JSON
{&quot;endpointId&quot;: &quot;1411183591831896064&quot;, &quot;instance&quot;: &quot;[{age: 40.77430558, ClientID: '997', income: 44964.0106, loan: 3944.219318}]&quot;}
</code></pre>
<p>I want to alter it to :</p>
<pre><code>$ cat INPUT-JSON
{&quot;endpointId&quot;: &quot;1411183591831896064&quot;, &quot;instance&quot;: &quot;[{age: 30.00, ClientID: '998', income: 50000.00, loan: 20000.00}]&quot;}
</code></pre>
<p>How do I do that using CLOUD SHELL Terminal ? (on google cloud platform)</p>
<p>(this is part of a Qwiklab : Vertex AI: Predicting Loan Risk with AutoML from <a href=""https://www.cloudskillsboost.google/course_templates/3?hl=es_419&amp;locale=fr_CA&amp;skip_cache=true&amp;utm_campaign=cgc&amp;utm_medium=website&amp;utm_source=gcp_training"" rel=""nofollow noreferrer"">https://www.cloudskillsboost.google/course_templates/3?hl=es_419&amp;locale=fr_CA&amp;skip_cache=true&amp;utm_campaign=cgc&amp;utm_medium=website&amp;utm_source=gcp_training</a> )</p>
<p>Thanks a lot for your help</p>",1,2,2022-04-26 12:20:21.613000 UTC,,,2,json|command-line-interface|google-cloud-vertex-ai,276,2022-01-14 12:42:50.640000 UTC,2022-09-15 21:28:40.643000 UTC,,33,0,0,1,,,,,,['google-cloud-vertex-ai']
"I'm working on model bias monitoring in sagemaker, Is it possible to run the monitoring job manually?","<p>I've done the baseline creation part where I'm getting <em>analysis_config.json</em> file.
So next step is doing the monitoring job part, Is it possible to run  monitoring job manually for <strong>Monitor Bias Drift</strong> and the next step <strong>Feature Attribution Drift</strong> ?</p>",1,0,2022-09-14 07:39:56.077000 UTC,,,0,python|python-3.x|amazon-web-services|amazon-sagemaker,31,2022-08-10 08:15:11.500000 UTC,2022-09-24 14:19:19.210000 UTC,,15,4,0,1,,,,,,['amazon-sagemaker']
Could not find member 'intellectualPropertyPublisher' on object of type 'JobProperties' in Azure Machine Learning Designer,"<p>From today (Aug 24th, 2021) I'm receiving the following error message when submit any operation in Azure Machine Learning Designer with a dataset:</p>
<p><strong>Could not find member 'intellectualPropertyPublisher' on object of type 'JobProperties'</strong></p>
<p>Complete error message:</p>
<p><em>UserError: Job submission to AzureML Compute encountered an Exception with status code , Could not find member 'intellectualPropertyPublisher' on object of type 'JobProperties'. Path 'properties.intellectualPropertyPublisher', line 309, position 36.</em></p>
<p>I'm seeing there's new items in user interface, maybe could be an updating error?
Someone is receiving something that?</p>",0,2,2021-08-24 18:10:02.630000 UTC,,2021-08-30 23:00:58.667000 UTC,1,azure-machine-learning-studio|azure-machine-learning-service,30,2021-08-24 18:04:02.253000 UTC,2022-05-27 17:17:39.740000 UTC,,11,0,0,0,,,,,,"['azure-machine-learning-studio', 'azure-machine-learning-service']"
AzureML - Trained clustering Model cannot be connected with Web Service Output,"<p>In AzureML it is not possible to connect trained Clustering Model with web service output. </p>

<p>Why does AzureML only allow ILearnerDotNet to be connected with Web Service Output and not IClusterDotNet? </p>

<p>This is a serious bug which halts clustering models from deploying them as a web service.</p>",0,1,2017-02-03 08:37:01.187000 UTC,,2017-02-03 08:50:07.267000 UTC,1,azure|machine-learning|cluster-analysis|azure-machine-learning-studio,192,2016-01-12 14:22:43.363000 UTC,2019-07-02 21:45:24.250000 UTC,,105,10,0,12,,,,,,['azure-machine-learning-studio']
AWS Sagemaker Custom Widget Installation Jupyter Notebook or Lab,"<p>I am trying to install our custom Jupyter widget on AWS Sagemaker. I assume that I need to use Lifecycle configuration. However, I first tried to install on the Classic Jupyter notebook terminal</p>
<pre><code>$ pip install clustergrammer2
$  jupyter nbextension enable --py --sys-prefix clustergrammer2
&gt;&gt; clustergrammer2 backend version 0.17.0
Enabling notebook extension clustergrammer2/extension...
      - Validating: OK
</code></pre>
<p>However, when I re-start the notebook kernel and do a hard refresh on the browser I get the following error in the browser console</p>
<pre><code>require.js?v=951f856e81496aaeec2e71a1c2c0d51f:168 Uncaught (in promise) Error: Script error for &quot;clustergrammer2&quot;
http://requirejs.org/docs/errors.html#scripterror
    at makeError (require.js?v=951f856e81496aaeec2e71a1c2c0d51f:168)
    at HTMLScriptElement.onScriptError (require.js?v=951f856e81496aaeec2e71a1c2c0d51f:1735)
</code></pre>
<p>Next, I tried the following Lifecycle configuration</p>
<pre><code>#!/bin/bash

set -e

# OVERVIEW
# This script installs a single pip package in a single SageMaker conda environments.

sudo -u ec2-user -i &lt;&lt;'EOF'
# PARAMETERS
ENVIRONMENT=python3
source /home/ec2-user/anaconda3/bin/activate &quot;$ENVIRONMENT&quot;
pip install scipy scikit-learn ipywidgets matplotlib statsmodels clustergrammer2
jupyter nbextension enable --py --sys-prefix clustergrammer2
source /home/ec2-user/anaconda3/bin/deactivate
EOF
</code></pre>
<p>but the widget is not working and I'm seeing an error</p>
<pre><code>Could not instantiate widget
</code></pre>
<p>in the JavaScript console.</p>
<p>The issue has also been raised on GitHub <a href=""https://github.com/jupyter-widgets/ipywidgets/issues/2958"" rel=""nofollow noreferrer"">https://github.com/jupyter-widgets/ipywidgets/issues/2958</a></p>",1,0,2020-09-15 04:52:57.353000 UTC,,2020-09-15 05:18:52.940000 UTC,1,amazon-web-services|widget|jupyter|amazon-sagemaker,641,2014-08-19 19:25:27.313000 UTC,2022-09-22 16:33:31.173000 UTC,,970,409,2,41,,,,,,['amazon-sagemaker']
AzureMl pipeline: How to access data of step1 into step2,"<p>I am following this <a href=""https://docs.microsoft.com/en-us/azure/machine-learning/how-to-move-data-in-out-of-pipelines"" rel=""nofollow noreferrer"">article</a> from microsoft to create azure ml pipeline with two steps and want to use data written by step1 into step2. According to the article below code should provide path of data written by step1 into script used for step2 as an argument</p>
<pre><code>datastore = workspace.datastores['my_adlsgen2']
step1_output_data = OutputFileDatasetConfig(name=&quot;processed_data&quot;, destination=(datastore, &quot;mypath/{run-id}/{output-name}&quot;)).as_upload()

step1 = PythonScriptStep(
    name=&quot;generate_data&quot;,
    script_name=&quot;step1.py&quot;,
    runconfig = aml_run_config,
    arguments = [&quot;--output_path&quot;, step1_output_data]
)

step2 = PythonScriptStep(
    name=&quot;read_pipeline_data&quot;,
    script_name=&quot;step2.py&quot;,
    compute_target=compute,
    runconfig = aml_run_config,
    arguments = [&quot;--pd&quot;, step1_output_data.as_input]

)

pipeline = Pipeline(workspace=ws, steps=[step1, step2])
</code></pre>
<p>But when I acccess the pd argument in  step2.py it provides the</p>
<blockquote>
<p>&quot;&lt;bound method OutputFileDatasetConfig.as_mount of
&lt;azureml.data.output_dataset_config.OutputFileDatasetConfig object at
0x7f8ae7f478d0&gt;&gt;&quot;</p>
</blockquote>
<p><strong>Any idea how to pass blob storage location used by step1 to write data in step2?</strong></p>",1,1,2021-02-15 11:54:13.657000 UTC,,,0,azure-machine-learning-service|azureml-python-sdk,1062,2009-11-10 14:07:58.920000 UTC,2022-09-09 06:39:04.550000 UTC,"Pune, India",6219,227,7,587,,,,,,['azure-machine-learning-service']
Py4JJavaError while loading Jar file in SageMaker jupyter notebook,"<p>I am having issue when I try to load jar file in SageMaker Jupyter notebook.</p>
<pre><code>import sagemaker_pyspark
from pyspark.sql import SparkSession

classpath = &quot;/home/ec2-user/SageMaker/someJar.jar&quot;
spark = SparkSession.builder.config(&quot;spark.driver.extraClassPath&quot;, classpath).getOrCreate()
</code></pre>
<p><a href=""https://i.stack.imgur.com/DRZBS.png"" rel=""nofollow noreferrer"">This is image of the error I am getting</a></p>
<pre><code>Py4JJavaError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.
: java.lang.NoClassDefFoundError: Could not initialize class scala.xml.Null$
    at org.apache.spark.ui.jobs.AllJobsPage.&lt;init&gt;(AllJobsPage.scala:43)
    at org.apache.spark.ui.jobs.JobsTab.&lt;init&gt;(JobsTab.scala:45)
    at org.apache.spark.ui.SparkUI.initialize(SparkUI.scala:61)
    at org.apache.spark.ui.SparkUI.&lt;init&gt;(SparkUI.scala:80)
    at org.apache.spark.ui.SparkUI$.create(SparkUI.scala:175)
    at org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:444)
    at org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:58)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
    at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
    at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
    at py4j.Gateway.invoke(Gateway.java:238)
    at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
    at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
    at py4j.GatewayConnection.run(GatewayConnection.java:238)
    at java.lang.Thread.run(Thread.java:748)
</code></pre>
<p>I was able to load different jar file without any issue.</p>
<p><img src=""https://i.stack.imgur.com/3jjUm.png"" alt=""enter image description here"" /></p>
<p>My assumption is that it could be because of Scala version mismatch between jar and SageMaker spark. I don't know how to resolve this issue. If anyone have any insight into this issue. Please let me know.</p>
<p>Thank you</p>",0,1,2022-04-01 18:21:43.433000 UTC,,2022-04-01 18:32:39.450000 UTC,0,scala|jupyter-notebook|jvm|amazon-sagemaker|py4j,184,2022-03-30 14:13:10.150000 UTC,2022-09-19 23:04:55.960000 UTC,,13,0,0,2,,,,,,['amazon-sagemaker']
Automatic flavor detection with ml_flow load_model?,"<p>I am using mlflow and want to handle different flavors (e.g. <code>sklearn</code> , <code>tensorflow</code> and <code>keras</code>) while loading.</p>
<p>Actually I only find the information about the stored <code>flavor</code> as string in</p>
<pre class=""lang-py prettyprint-override""><code>Run.to_dictionary()['data']['tags']['mlflow.log-model.history']
</code></pre>
<p>output:</p>
<pre class=""lang-py prettyprint-override""><code>[{&quot;run_id&quot;: &quot;8ea47843f7b446828dbd9cd3a1ed2339&quot;, &quot;artifact_path&quot;: &quot;model&quot;, &quot;utc_time_created&quot;: &quot;2022-04-26 10:39:55.639791&quot;, &quot;flavors&quot;: {&quot;keras&quot;: {&quot;keras_module&quot;: &quot;tensorflow.keras&quot;, &quot;keras_version&quot;: &quot;2.7.0&quot;, &quot;save_format&quot;: &quot;tf&quot;, &quot;data&quot;: &quot;data&quot;, &quot;code&quot;: null}, &quot;python_function&quot;: {&quot;loader_module&quot;: &quot;mlflow.keras&quot;, &quot;python_version&quot;: &quot;3.8.10&quot;, &quot;data&quot;: &quot;data&quot;, &quot;env&quot;: &quot;conda.yaml&quot;}}, &quot;model_uuid&quot;: &quot;3bd37bdb0aa1409aabc65f8314018642&quot;, &quot;mlflow_version&quot;: &quot;1.25.1&quot;}]
</code></pre>
<p>Run is the <code>mlflow.entities.Run</code> object.</p>
<p>Using <code>ast.literal_eval</code> to transform the string into the dictionary fails.</p>
<pre class=""lang-py prettyprint-override""><code>ast.literal_eval(str(self.run.to_dictionary()['data']['tags']['mlflow.log-model.history'][1:-1]))
</code></pre>",1,0,2022-04-26 11:46:38.640000 UTC,,,0,python|tensorflow|keras|mlflow,75,2014-12-23 23:50:02.193000 UTC,2022-09-21 11:10:16.000000 UTC,,1499,98,14,297,,,,,,['mlflow']
Monitoring the performance of ML model on EC2 Instance,"<p>If we go back and use dockerized ML models on EC2 Instances - is there any native way to check the model metrics (for classification, for example, accuracy, precision, recall and f1-score)? For sure, Cloudwatch can be used but it will just gives the overall info regarding endpoint, disk utilisation, etc., but not ML model metrics.</p>",1,0,2021-03-16 13:14:39.000000 UTC,,2021-03-16 19:07:31.600000 UTC,0,amazon-web-services|machine-learning|amazon-ec2|amazon-sagemaker|production,67,2013-12-08 08:33:34.717000 UTC,2022-09-21 18:08:28.293000 UTC,,2778,138,1,352,,,,,,['amazon-sagemaker']
ModuleNotFoundError: No module named 'sagemaker',"<p>I am attempting to perform pre-processing using sklearn in sagemaker. As a pre-requisite I am trying to import sagemaker module:</p>
<pre><code>%pip install -qU 'sagemaker&gt;=2.15.0'
import boto3
import sagemaker
</code></pre>
<p>but I get error as:</p>
<pre><code>An error was encountered:
No module named 'sagemaker'
Traceback (most recent call last):
ModuleNotFoundError: No module named 'sagemaker'

</code></pre>
<p>Any pointers will be helpful.</p>",1,0,2022-03-01 15:40:26.003000 UTC,,2022-03-03 11:54:00.017000 UTC,0,python|scikit-learn|amazon-sagemaker,1237,2021-04-09 05:21:43.777000 UTC,2022-09-22 02:31:03.023000 UTC,"Texas, USA",1,0,0,3,,,,,,['amazon-sagemaker']
Mlflow download_artifacts giving Not Found error,"<p>I have mlflow and minio running under docker compose. Mlflow successfully logs artifacts to minio and retrieves them. Minio and mlflow have their relevant ports 900 and 5000 exposed by docker.
If I run MlflowClient().download_artifacts for predictions from within the docker environment, everything works smoothly.
If I run MlflowClient().download_artifacts from outside docker (local machine or remotely), I get the below error. There is no issue in fetching the logged metrics.</p>
<p>botocore.exceptions.ClientError: An error occurred (404) when calling the ListObjectsV2 operation: Not Found</p>
<p>My code:</p>
<pre><code>os.environ['AWS_ACCESS_KEY_ID'] = &quot;x&quot;
mlflow.set_tracking_uri('http://10.0.0.1:5000')
os.environ['AWS_SECRET_ACCESS_KEY'] = &quot;x&quot;
os.environ['MINIO_ACCESS_KEY_ID'] = &quot;x&quot;
os.environ['MINIO_SECRET_ACCESS_KEY'] = &quot;x/me &quot;

from mlflow.tracking import MlflowClient
MlflowClient().download_artifacts(&quot;323e1527d49d4e77bd14c387bbdf6372&quot;, &quot;model&quot;, local_dir)
</code></pre>
<p>Any help would be most appreciated.</p>
<p>Thanks</p>
<p>Best Regards,</p>
<p>Adeel</p>",0,0,2021-01-21 09:40:48.143000 UTC,,,2,docker|docker-compose|mlflow,254,2015-10-28 00:01:57.173000 UTC,2022-09-24 01:20:28.387000 UTC,"Sydney, New South Wales, Australia",689,57,0,87,,,,,,['mlflow']
Can SageMaker distributed training be used for training non-deep learning models?,"<p>I am following this <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/distributed-training.html"" rel=""nofollow noreferrer"">documentation</a> page to understand SageMaker's distributed training feature.</p>
<p>It says here that:- </p>
<blockquote>
<p>The SageMaker distributed training libraries are available only through the AWS deep learning containers for the TensorFlow, PyTorch, and HuggingFace frameworks within the SageMaker training platform. </p>
</blockquote>
<p>Does this mean that we cannot use SageMaker distributed training to train machine learning models with traditional machine learning algorithms such as <em>linear regression</em>, <em>random forest</em> or <em>XGBoost</em>?</p>
<p>I have a use cases where the <strong>data set is very large</strong> and distributed training can help with model parallelism and data parallelism. What other options can be recommended to avoid bringing in large amounts of data in memory on a training instance?</p>",2,0,2022-09-17 04:05:38.307000 UTC,,,-1,amazon-web-services|machine-learning|amazon-sagemaker|distributed-training|amazon-machine-learning,23,2014-01-16 15:43:59.673000 UTC,2022-09-25 03:22:08.463000 UTC,Singapore,5854,155,70,794,,,,,,['amazon-sagemaker']
How to install ODBC driver in azure DevOps built pipeline to run a script on Azure ML compute?,<p>I am running a Python script to connect to my Azure SQL db in a DevOps built pipeline. I am attaching the azure ML compute to the script while running it from DevOps using azure CLI. But how can I install the ODBC driver on the Azure agent? or is the docker image I need to install which is being called while submitting the script in cli?</p>,0,2,2021-12-21 14:23:26.213000 UTC,,2021-12-21 14:28:40.667000 UTC,1,azure|azure-devops|azure-machine-learning-service|mlops,336,2016-12-26 17:11:02.960000 UTC,2022-09-20 05:53:30.493000 UTC,India,349,1,0,60,,,,,,['azure-machine-learning-service']
locked out of wandb local server - change user password,"<p>I am using a local weight and biases (wandb) instance running on a server with no internet connection.
I have a user there and having no problems logging results from the server.</p>
<p>However, when trying to see them in the UI it asked me to login again but unfortunately I forgot my password and reset password doesn't work with the message of <code>Error while trying to reset password</code>.</p>
<p>I tried searching all over the documentation and found nothing to help with that.</p>
<p>Any help for locally recovering my account will be appreciated!</p>",1,0,2022-04-07 09:31:43.133000 UTC,1.0,2022-07-10 20:14:49.700000 UTC,0,wandb,304,2020-12-23 20:02:41.683000 UTC,2022-09-22 12:34:43.317000 UTC,,21,1,0,1,,,,,,['wandb']
Boto3 and Sagemaker Access Issues whehn creating Training job,"<pre><code> botocore.exceptions.ClientError: An error occurred (AccessDeniedException) when calling the CreateTrainingJob operation: User: arn:aws .... is not authorized to perform: sagemaker:CreateTrainingJob on resource: arn:aws:sagemaker:... because no identity-based policy allows the sagemaker:CreateTrainingJob action
</code></pre>
<p>I have a message as above.
Is there anyway to specify my</p>
<pre><code>sm_boto3 = boto3.client(&quot;sagemaker&quot;)
</code></pre>
<p>so that this sm_boto3 instance will allow me to create a training job ?</p>",1,0,2022-06-02 07:33:47.923000 UTC,,,0,amazon-web-services|boto3|amazon-sagemaker,197,2020-09-21 20:00:48.277000 UTC,2022-09-16 06:59:29.497000 UTC,"Seoul, South Korea",69,8,0,4,,,,,,['amazon-sagemaker']
aws sagemaker python sdk : Predict function giving weird errors,"<p>I have trained a tf model (not using any sagemaker service). I used tf 2.1.0 version. 
I saved it with the following signature def 
MetaGraphDef with tag-set: 'serve' contains the following SignatureDef:</p>

<pre><code>  The given SavedModel SignatureDef contains the following input(s):
    inputs['inputs'] tensor_info:
        dtype: DT_FLOAT
        shape: (-1, 80)
        name: text_input_1:0
  The given SavedModel SignatureDef contains the following output(s):
    outputs['predictions'] tensor_info:
        dtype: DT_FLOAT
        shape: (-1, 738)
        name: output_1/Identity:0
  Method name is: tensorflow/serving/predict
</code></pre>

<p>I zipped the model and created a Model object using <a href=""https://sagemaker.readthedocs.io/en/stable/model.html#sagemaker.model.Model.deploy"" rel=""nofollow noreferrer"">AWS sagemaker's Python SDK </a></p>

<pre><code>model = Model(model_data=s3_location,
              role=role,
              framework_version='2.1.0')
predictor = model.deploy(initial_instance_count=1, instance_type='ml.m5.xlarge') {code}
</code></pre>

<p><strong>After the Model is created, I have a lot of trouble predicting.</strong> I get weird errors such as 
<code>""error"": ""Session was not created with a graph before Run()!""</code> or
<code>""error"": ""Failed to process element: 0 of 'instances' list. Error: Invalid argument: JSON Value:</code></p>

<p>I tried both predictor.predict() method as well as client.invoke_endpoint() methods using various formats such as predictor.predict(list), various variations of tf serving REST API formats using <code>""instances""</code> format. I tried json.dumps() on all variations as well. But nothing works. 
Can someone suggest a way to predict. As shown in the signature_def my input is of shape (1,80).
Thank you </p>

<p>I'm looking for an answer of the form <code>predictor.predict($what here?)</code>. Or suggest what I am doing wrong here.</p>

<p>Thank you</p>",1,0,2020-03-27 00:49:57.723000 UTC,,,2,tensorflow|tensorflow-serving|amazon-sagemaker,523,2017-11-23 22:07:22.953000 UTC,2022-02-16 21:51:28.880000 UTC,"Atlanta, GA, United States",51,0,0,8,,,,,,['amazon-sagemaker']
If we have installed nltk zip file and then we want to install the setup of the same using the python script,"<p>If we have installed nltk zip file and then we want to install the setup of the same using the python script then how can we do it?</p>

<p>I have created  the zip file and kept all the packages in it and then importing it from python script </p>

<pre><code>from mynltk.corpus import *
from mynltk.corpus.reader import *
brown.words()[0:10]
</code></pre>",0,0,2015-04-24 06:47:30.847000 UTC,,2015-04-24 07:20:49.860000 UTC,1,python|subprocess|azure-machine-learning-studio,44,2015-01-22 10:00:09.500000 UTC,2022-03-16 20:44:48.503000 UTC,,547,4,0,42,,,,,,['azure-machine-learning-studio']
I keep getting UnexpectedStatusException: Error for HyperParameterTuning job in AWS sagemaker,"<p>As mentioned in question, I keep getting UnexpectedStatusException: Error for HyperParameterTuning job xgboost-211***-1631: Failed. Reason: No training job succeeded after 5 attempts. For additional details, please take a look at the training job failures by listing training jobs for the hyperparameter tuning job.</p>
<p>I looked into parameter ranges based on <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost-tuning.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost-tuning.html</a> to make sure that ranges are good and they seem to be ok.  Data is definitely good because I can train the model but can't tune it.</p>
<p>Here is the code I am using :</p>
<pre><code>import sagemaker
import boto3
import numpy as np                                # For matrix operations and numerical processing
import pandas as pd                               # For munging tabular data
import os 
from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner
from sagemaker.session import TrainingInput
from sagemaker.debugger import Rule, rule_configs
 
region = boto3.Session().region_name    
smclient = boto3.Session().client('sagemaker')

role = sagemaker.get_execution_role()



s3_output_location='s3://{}/{}/{}'.format(bucket, prefix, 'output')
container=sagemaker.image_uris.retrieve(&quot;xgboost&quot;, region, &quot;latest&quot;)


xgb_model=sagemaker.estimator.Estimator(
    image_uri=container,
    role=role,
    instance_count=1,
    instance_type='ml.m4.xlarge',
    volume_size=5,
    output_path=s3_output_location,
    sagemaker_session=sagemaker.Session(),
    rules=[Rule.sagemaker(rule_configs.create_xgboost_report())]
)

xgb_model.set_hyperparameters(
    max_depth = 5,
    eta = 0.2,
    gamma = 4,
    min_child_weight = 6,
    subsample = 0.7,
    objective = &quot;binary:logistic&quot;,
    num_round = 10
)

hyperparameter_ranges = {'eta': ContinuousParameter(0.1, 0.5),
                        'min_child_weight': ContinuousParameter(1, 10),
                        'alpha': ContinuousParameter(0, 3),
                        'max_depth': IntegerParameter(0, 4)}
objective_metric_name = 'validation:auc'


tuner = HyperparameterTuner(xgb_model,
                            objective_metric_name,
                            hyperparameter_ranges,
                            max_jobs=60,
                            max_parallel_jobs=6)



train_input = TrainingInput(
    &quot;s3://{}/{}/{}&quot;.format(bucket, prefix, &quot;train/train.csv&quot;), content_type=&quot;csv&quot;
)
validation_input = TrainingInput(
    &quot;s3://{}/{}/{}&quot;.format(bucket, prefix, &quot;validate/validation.csv&quot;), content_type=&quot;csv&quot;
)

tuner.fit({&quot;train&quot;: train_input, &quot;validation&quot;: validation_input}, include_cls_metadata=False)
</code></pre>
<p>This is the error I get</p>
<pre><code>&gt; --------------------------------------------------------------------------- UnexpectedStatusException                 Traceback (most recent call
&gt; last) &lt;ipython-input-2-7824ad80a8bb&gt; in &lt;module&gt;
&gt;      62 )
&gt;      63 
&gt; ---&gt; 64 tuner.fit({&quot;train&quot;: train_input, &quot;validation&quot;: validation_input}, include_cls_metadata=False)
&gt; 
&gt; ~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/tuner.py
&gt; in fit(self, inputs, job_name, include_cls_metadata, estimator_kwargs,
&gt; wait, **kwargs)
&gt;     449 
&gt;     450         if wait:
&gt; --&gt; 451             self.latest_tuning_job.wait()
&gt;     452 
&gt;     453     def _fit_with_estimator(self, inputs, job_name, include_cls_metadata, **kwargs):
&gt; 
&gt; ~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/tuner.py
&gt; in wait(self)    1595     def wait(self):    1596        
&gt; &quot;&quot;&quot;Placeholder docstring.&quot;&quot;&quot;
&gt; -&gt; 1597         self.sagemaker_session.wait_for_tuning_job(self.name)    1598     1599 
&gt; 
&gt; ~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/session.py
&gt; in wait_for_tuning_job(self, job, poll)    3253         &quot;&quot;&quot;    3254   
&gt; desc = _wait_until(lambda: _tuning_job_status(self.sagemaker_client,
&gt; job), poll)
&gt; -&gt; 3255         self._check_job_status(job, desc, &quot;HyperParameterTuningJobStatus&quot;)    3256         return desc    3257 
&gt; 
&gt; ~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/session.py
&gt; in _check_job_status(self, job, desc, status_key_name)    3336        
&gt; ),    3337                 allowed_statuses=[&quot;Completed&quot;, &quot;Stopped&quot;],
&gt; -&gt; 3338                 actual_status=status,    3339             )    3340 
&gt; 
&gt; UnexpectedStatusException: Error for HyperParameterTuning job
&gt; xgboost-211XXX-1641: Failed. Reason: No training job succeeded after 5
&gt; attempts. For additional details, please take a look at the training
&gt; job failures by listing training jobs for the hyperparameter tuning
&gt; job.
</code></pre>
<p>Thank you in advance,</p>
<p>Sam</p>",0,4,2021-12-23 16:52:42.207000 UTC,,,4,python-3.x|xgboost|amazon-sagemaker,508,2014-12-08 15:57:43.180000 UTC,2022-09-22 16:24:12.740000 UTC,,1138,43,7,101,,,,,,['amazon-sagemaker']
Connect locally to Jupyter in Sagemaker,"<p>When I run <code>%connect_info</code> in Jupyterlab on Sagemaker I get session info. and</p>
<pre><code>{&quot;shell_port&quot;: ,
  &quot;iopub_port&quot;: ,
  &quot;stdin_port&quot;: ,
  &quot;control_port&quot;: ,
  &quot;hb_port&quot;: ,
  &quot;ip&quot;: &quot;&quot;,
  &quot;key&quot;: &quot;&quot;,
  &quot;transport&quot;: &quot;&quot;,
  &quot;signature_scheme&quot;: &quot;&quot;,
  &quot;kernel_name&quot;: &quot;&quot;}

Paste the above JSON into a file, and connect with:
    $&gt; jupyter &lt;app&gt; --existing &lt;file&gt;
or, if you are local, you can connect with just:
    $&gt; jupyter &lt;app&gt; --existing kernel-052ed888-e682-4786-aa4c-cdb19c6145bf.json
or even just:
    $&gt; jupyter &lt;app&gt; --existing
if this is the most recent Jupyter kernel you have started.
</code></pre>
<p>But when I run the Jupiter statement against the saved text file, it doesn't connect.</p>
<p>Sometimes, it would be convenient to connect my local VS code app to my notebooks instead of developing in the browser.</p>
<p>Any suggestions are appreciated.</p>",1,0,2022-02-11 22:08:37.783000 UTC,,,0,jupyter|jupyter-lab|amazon-sagemaker,385,2012-05-23 18:47:11.903000 UTC,2022-09-15 21:30:03.487000 UTC,,971,132,8,104,,,,,,['amazon-sagemaker']
Does Microsoft Azure Machine Learning use Hadoop as its underlying layer?,"<p>I'm going to use Microsoft Azure ML for some text analysis purposes such as keyword extraction and as the size of my input is big I want to know whether ML package actually uses the Hadoop (HDP) as its underlying layer or not? If not, how can I use the ML in combination with Hadoop?</p>

<p>Does Mahout have some text analysis tools?</p>",1,0,2015-01-29 00:39:11.960000 UTC,1.0,2022-01-21 08:54:07.410000 UTC,1,java|azure|mahout|azure-machine-learning-studio,660,2012-12-12 20:12:11.933000 UTC,2022-04-05 02:26:00.750000 UTC,,5655,73,3,629,,,,,,['azure-machine-learning-studio']
Not able to create condo environment in SageMaker studio,"<p>Sagemaker studio terminal shows default python version as &quot;Python 3.7.10&quot;
I am trying to create conda environment with python3.9 version as below.</p>
<pre><code>conda create --name custom_python_39 python=3.9 
</code></pre>
<p>But I get the following error:</p>
<p>RemoveError: 'setuptools' is a dependency of conda and cannot be removed from
conda's operating environment.</p>
<p>But pip list doesn't show setuptools.</p>
<p>I already tried the following</p>
<pre><code>conda update conda -n base
conda update --force conda
</code></pre>
<p>But I get the following erros:
bash-4.2$ conda update conda -n base
Collecting package metadata: done
Solving environment: /
The environment is inconsistent, please check the package plan carefully
The following packages are causing the inconsistency:</p>
<pre><code>  - conda-forge/noarch::pyopenssl==22.0.0=pyhd8ed1ab_0
  - conda-forge/noarch::requests==2.27.1=pyhd8ed1ab_0
  - conda-forge/noarch::urllib3==1.26.9=pyhd8ed1ab_0
  - conda-forge/noarch::argon2-cffi==21.3.0=pyhd8ed1ab_0
  - defaults/linux-64::argon2-cffi-bindings==21.2.0=py37h7f8727e_0
  - conda-forge/noarch::bleach==3.1.4=pyh9f0ad1d_0
  - conda-forge/linux-64::brotlipy==0.7.0=py37h5e8e339_1001
  - conda-forge/linux-64::conda==4.6.14=py37_0
  - conda-forge/linux-64::ipykernel==5.2.0=py37h43977f1_1
  - conda-forge/linux-64::ipython==7.13.0=py37hc8dfbb8_2
  - conda-forge/linux-64::jedi==0.16.0=py37hc8dfbb8_1
  - conda-forge/noarch::jinja2==2.11.3=pyhd8ed1ab_1
  - conda-forge/noarch::jsonschema==3.2.0=pyhd8ed1ab_3
  - conda-forge/noarch::nbconvert==5.6.1=pyhd8ed1ab_2
  - conda-forge/noarch::notebook==6.4.1=pyha770c72_0
  - conda-forge/noarch::prompt-toolkit==3.0.5=py_1
  - conda-forge/noarch::pygments==2.6.1=py_0
/ Killed
</code></pre>",0,2,2022-07-21 06:49:09.150000 UTC,,,0,python-3.x|amazon-sagemaker,29,2019-08-15 06:38:39.327000 UTC,2022-09-23 09:38:31.217000 UTC,,844,83,1,148,,,,,,['amazon-sagemaker']
How to get preprocess/postprocess steps from model created using Google Vertex AI?,"<p>A client of mine wants to run their Google Vertex AI model on NVIDIA Jetson boards using TensorRT as accelerator. The problem with this is that their model uses certain operators (DecodeJpeg) that are not supported by ONNX. I've been able to isolate the feature extrator subgraph from the model, so everything supported by ONNX is being used, while the preprocess and postprocess will be written separate from the model.</p>
<p>I'm asking because I need to be provided the pre/postprocess of the model so I could implement them separately, so is there a way to get pre/postprocess from Google Vertex AI console?</p>
<p>I've tried running a loop that rescales the image to a squared tile from 0 to 512, but none of those gave the adequate result.</p>",0,4,2022-04-04 13:36:49.933000 UTC,,,0,tensorflow|onnx|tensorrt|google-cloud-vertex-ai,107,2021-03-22 12:13:48.997000 UTC,2022-05-25 13:38:20.393000 UTC,,16,0,0,8,,,,,,['google-cloud-vertex-ai']
ERROR: Cannot uninstall 'ruamel-yaml' while creating docker image for azure ML ACI deployment,"<p><strong>I am trying to deploy machine learning model in azure ACI but i am getting following error while creating a docker image</strong></p>
<pre><code>Pip subprocess error:
ERROR: Cannot uninstall 'ruamel-yaml'. It is a distutils installed project and thus we cannot 
accurately determine which files belong to it which would lead to only a partial uninstall.
</code></pre>
<p>Below is my yml file for pip dependencies</p>
<pre><code>name: project_environment
dependencies:
# The python interpreter version.

# Currently Azure ML only supports 3.5.2 and later.


- pip:
  # Required packages for AzureML execution, history, and data preparation.
  - pandas
  - azureml-defaults
  - azureml-sdk
  - azureml-widgets
  - numpy
  - tensorflow-gpu
  - keras
  - azureml-defaults
  - torch==1.4.0
  - scikit-learn==0.22.2.post1
</code></pre>
<p>and if i use conda instead of pip then i am getting following error</p>
<pre><code>Step 11/13 : RUN CONDA_ROOT_DIR=$(conda info --root) &amp;&amp; if [ -n 
&quot;$AZUREML_CONDA_ENVIRONMENT_PATH&quot; ]; then conda env update -p 
&quot;$AZUREML_CONDA_ENVIRONMENT_PATH&quot; -f '/var/azureml-app/binary_2.yml'; else 
conda env update -n base -f '/var/azureml-app/binary_2.yml'; fi &amp;&amp; conda 
clean -aqy &amp;&amp; rm -rf /root/.cache/pip &amp;&amp; rm -rf &quot;$CONDA_ROOT_DIR/pkgs&quot; &amp;&amp; 
find &quot;$CONDA_ROOT_DIR&quot; -type d -name __pycache__ -exec rm -rf {} +
---&gt; Running in 9e6eb7278bfc  
[91mUnable to install package for Conda.

Please double check and ensure you dependencies file has
the correct spelling.  You might also try installing the
conda-env-Conda package to see if provides the required
installer. 
[0mThe command '/bin/sh -c CONDA_ROOT_DIR=$(conda info --root) &amp;&amp; if [ -n 
&quot;$AZUREML_CONDA_ENVIRONMENT_PATH&quot; ]; then conda env update -p 
&quot;$AZUREML_CONDA_ENVIRONMENT_PATH&quot; -f '/var/azureml-app/binary_2.yml'; else 
 conda env update -n base -f '/var/azureml-app/binary_2.yml'; fi &amp;&amp; conda 
clean 
-aqy &amp;&amp; rm -rf /root/.cache/pip &amp;&amp; rm -rf &quot;$CONDA_ROOT_DIR/pkgs&quot; &amp;&amp; find 
&quot;$CONDA_ROOT_DIR&quot; -type d -name __pycache__ -exec rm -rf {} +' returned a 
non- 
 zero code: 255
 2020/08/12 19:36:09 Container failed during run: acb_step_0. No retries 
 remaining.
 failed to run step ID: acb_step_0: exit status 255
</code></pre>
<p>**Can anyone please help me **</p>",3,6,2020-08-12 19:19:19.573000 UTC,,2020-08-12 19:48:49.537000 UTC,9,python|azure|docker|machine-learning|azure-machine-learning-service,9743,2015-03-14 04:45:18.630000 UTC,2022-02-14 04:44:37.810000 UTC,,179,5,0,44,,,,,,['azure-machine-learning-service']
How to use AWS Sagemaker XGBoost framework?,"<p>I'm building XGBoost model on sagemaker for IRIS dataset. I have two files model.py and train.py as follows:</p>

<p><code>Model.py</code>:</p>

<pre><code>import boto3, sagemaker
import pandas as pd
import numpy as np
from sagemaker import get_execution_role
from sagemaker.xgboost.estimator import XGBoost

role = get_execution_role()
bucket_name = 'my-bucket-name'
train_prefix = 'iris_data/train'
test_prefix = 'iris_data/test'

session = boto3.Session()
sg_session = sagemaker.Session(session)

# Read training data from S3
train_channel = 's3://{0}/{1}'.format(bucket_name, train_prefix)

data_channels = {'train': train_channel}

hyperparameters = {
    'max_leaf_nodes': 30
}

model = XGBoost(entry_point=""train.py"",
                train_instance_type=""ml.m4.xlarge"",
                train_instance_count=1,
                role=role,
                framework_version = '0.90-2',
                sagemaker_session=sg_session,
                hyperparameters=hyperparameters)

model.fit(inputs=data_channels,  logs=True)

transformer = model.transformer(instance_count=1, instance_type='ml.m4.xlarge')
test_channel = 's3://{0}/{1}'.format(bucket_name, test_prefix)

transformer.transform(test_channel, content_type='text/csv')
print('Waiting for transform job: ' + transformer.latest_transform_job.job_name)
transformer.wait()

batch_output = transformer.output_path
print(batch_output)

</code></pre>

<p><code>train.py</code>:</p>

<pre><code>from __future__ import print_function
import argparse
import os
import pandas as pd
import pickle
from xgboost import XGBClassifier

if __name__ == '__main__':
    parser = argparse.ArgumentParser()

    # Hyperparameters are described here. In this simple example we are just including one hyperparameter.
    parser.add_argument('--max_leaf_nodes', type=int, default=-1)

    # Sagemaker specific arguments. Defaults are set in the environment variables.
    parser.add_argument('--output-data-dir', type=str, default=os.environ['SM_OUTPUT_DATA_DIR'])
    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])
    parser.add_argument('--train', type=str, default=os.environ['SM_CHANNEL_TRAIN'])

    args = parser.parse_args()

    # Take the set of files and read them all into a single pandas dataframe
    input_files = [ os.path.join(args.train, file) for file in os.listdir(args.train) ]
    if len(input_files) == 0:
        raise ValueError(('There are no files in {}.\n' +
                          'This usually indicates that the channel ({}) was incorrectly specified,\n' +
                          'the data specification in S3 was incorrectly specified or the role specified\n' +
                          'does not have permission to access the data.').format(args.train, ""train""))
    raw_data = [ pd.read_csv(file, header=None, engine=""python"") for file in input_files ]
    train_data = pd.concat(raw_data)

    # labels are in the first column
    train_y = train_data.iloc[:,0]
    train_X = train_data.iloc[:,1:]

    # Here we support a single hyperparameter, 'max_leaf_nodes'. Note that you can add as many
    # as your training my require in the ArgumentParser above.
    max_leaf_nodes = args.max_leaf_nodes

    # Now use scikit-learn's decision tree classifier to train the model.
    clf = XGBClassifier(max_depth=10, n_estimators=100, random_state=78432)
    clf = clf.fit(train_X, train_y)

    # Print the coefficients of the trained classifier, and save the coefficients
    pickle.dump(clf, open(os.path.join(args.model_dir, ""model.bin""), ""wb""))    


def model_fn(model_dir):
    """"""Deserialized and return fitted model

    Note that this should have the same name as the serialized model in the main method
    """"""
    model = pickle.load(open(os.path.join(model_dir, ""model.bin""), ""rb""))
    return model

</code></pre>

<p>I cannot use XGBoost built-in container because I want to use XGBoost framework at the end for my work, where train.py does much more than just reading data from S3. So as of now, for testing I am checking this with IRIS data. </p>

<p>But when I execute model.py, model training is completed successfully but it is failing in btach transform with the following error:</p>

<pre><code>Waiting for transform job: sagemaker-xgboost-2020-03-31-17-21-48-649
......................Processing /opt/ml/code
Building wheels for collected packages: train
  Building wheel for train (setup.py): started
  Building wheel for train (setup.py): finished with status 'done'
  Created wheel for train: filename=train-1.0.0-py2.py3-none-any.whl size=6872 sha256=dd15ce5260f45f557b284e58a126d91440fb72155eca544c64e4faa9ce48ff38
  Stored in directory: /tmp/pip-ephem-wheel-cache-zlsbuj5_/wheels/95/c1/85/65aaf48b35aba88c6e896d2fd04a4b69f1cee0d81ea32993ca
Successfully built train
Installing collected packages: train
Successfully installed train-1.0.0
[2020-03-31 17:25:18 +0000] [38] [INFO] Starting gunicorn 19.10.0
[2020-03-31 17:25:18 +0000] [38] [INFO] Listening at: unix:/tmp/gunicorn.sock (38)
[2020-03-31 17:25:18 +0000] [38] [INFO] Using worker: gevent
[2020-03-31 17:25:18 +0000] [41] [INFO] Booting worker with pid: 41
[2020-03-31 17:25:18 +0000] [45] [INFO] Booting worker with pid: 45
[2020-03-31 17:25:19 +0000] [46] [INFO] Booting worker with pid: 46
[2020-03-31 17:25:19 +0000] [47] [INFO] Booting worker with pid: 47
[2020-03-31:17:25:53:INFO] No GPUs detected (normal if no gpus installed)
[2020-03-31:17:25:53:INFO] Installing module with the following command:
/miniconda3/bin/python -m pip install . 
Processing /opt/ml/code
Building wheels for collected packages: train
  Building wheel for train (setup.py): started
  Building wheel for train (setup.py): finished with status 'done'
  Created wheel for train: filename=train-1.0.0-py2.py3-none-any.whl size=6871 sha256=e8f227b103bf75716d7967683595b4e6d5caacd312a79b5231b8f653225be8d0
  Stored in directory: /tmp/pip-ephem-wheel-cache-hlc0kry6/wheels/95/c1/85/65aaf48b35aba88c6e896d2fd04a4b69f1cee0d81ea32993ca
Successfully built train
Installing collected packages: train
  Attempting uninstall: train
    Found existing installation: train 1.0.0
    Uninstalling train-1.0.0:
      Successfully uninstalled train-1.0.0
Successfully installed train-1.0.0
169.254.255.130 - - [31/Mar/2020:17:25:55 +0000] ""GET /ping HTTP/1.1"" 200 0 ""-"" ""Go-http-client/1.1""
[2020-03-31:17:25:55:INFO] No GPUs detected (normal if no gpus installed)
[2020-03-31:17:25:55:INFO] Installing module with the following command:
/miniconda3/bin/python -m pip install . 
Processing /opt/ml/code
Building wheels for collected packages: train
  Building wheel for train (setup.py): started
  Building wheel for train (setup.py): finished with status 'done'
  Created wheel for train: filename=train-1.0.0-py2.py3-none-any.whl size=6870 sha256=dab9513d234f721f798249797424c388f0659588903c01880dc21811e1bf4ea5
  Stored in directory: /tmp/pip-ephem-wheel-cache-j30gnab9/wheels/95/c1/85/65aaf48b35aba88c6e896d2fd04a4b69f1cee0d81ea32993ca
Successfully built train
Installing collected packages: train
  Attempting uninstall: train
    Found existing installation: train 1.0.0
    Uninstalling train-1.0.0:
      Successfully uninstalled train-1.0.0
Successfully installed train-1.0.0
169.254.255.130 - - [31/Mar/2020:17:25:56 +0000] ""GET /execution-parameters HTTP/1.1"" 404 232 ""-"" ""Go-http-client/1.1""
[2020-03-31:17:25:56:INFO] Determined delimiter of CSV input is ','
[2020-03-31:17:25:56:ERROR] Exception on /invocations [POST]
TypeError: float() argument must be a string or a number, not 'list'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/miniconda3/lib/python3.6/site-packages/sagemaker_containers/_functions.py"", line 93, in wrapper
    return fn(*args, **kwargs)
  File ""/miniconda3/lib/python3.6/site-packages/sagemaker_xgboost_container/serving.py"", line 55, in default_input_fn
    return xgb_encoders.decode(input_data, content_type)
  File ""/miniconda3/lib/python3.6/site-packages/sagemaker_xgboost_container/encoder.py"", line 121, in decode
    return decoder(obj)
  File ""/miniconda3/lib/python3.6/site-packages/sagemaker_xgboost_container/encoder.py"", line 50, in csv_to_dmatrix
    np_payload = np.array(list(map(lambda x: _clean_csv_string(x, delimiter), string_like.split('\n')))).astype(dtype)
ValueError: setting an array element with a sequence.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/miniconda3/lib/python3.6/site-packages/flask/app.py"", line 2446, in wsgi_app
    response = self.full_dispatch_request()
  File ""/miniconda3/lib/python3.6/site-packages/flask/app.py"", line 1951, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File ""/miniconda3/lib/python3.6/site-packages/flask/app.py"", line 1820, in handle_user_exception
    reraise(exc_type, exc_value, tb)
  File ""/miniconda3/lib/python3.6/site-packages/flask/_compat.py"", line 39, in reraise
    raise value
  File ""/miniconda3/lib/python3.6/site-packages/flask/app.py"", line 1949, in full_dispatch_request
    rv = self.dispatch_request()
  File ""/miniconda3/lib/python3.6/site-packages/flask/app.py"", line 1935, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File ""/miniconda3/lib/python3.6/site-packages/sagemaker_containers/_transformer.py"", line 200, in transform
    self._model, request.content, request.content_type, request.accept
  File ""/miniconda3/lib/python3.6/site-packages/sagemaker_containers/_transformer.py"", line 227, in _default_transform_fn
    data = self._input_fn(content, content_type)
  File ""/miniconda3/lib/python3.6/site-packages/sagemaker_containers/_functions.py"", line 95, in wrapper
    six.reraise(error_class, error_class(e), sys.exc_info()[2])
  File ""/miniconda3/lib/python3.6/site-packages/six.py"", line 702, in reraise
    raise value.with_traceback(tb)
  File ""/miniconda3/lib/python3.6/site-packages/sagemaker_containers/_functions.py"", line 93, in wrapper
    return fn(*args, **kwargs)
  File ""/miniconda3/lib/python3.6/site-packages/sagemaker_xgboost_container/serving.py"", line 55, in default_input_fn
    return xgb_encoders.decode(input_data, content_type)
  File ""/miniconda3/lib/python3.6/site-packages/sagemaker_xgboost_container/encoder.py"", line 121, in decode
    return decoder(obj)
  File ""/miniconda3/lib/python3.6/site-packages/sagemaker_xgboost_container/encoder.py"", line 50, in csv_to_dmatrix
    np_payload = np.array(list(map(lambda x: _clean_csv_string(x, delimiter), string_like.split('\n')))).astype(dtype)
sagemaker_containers._errors.ClientError: setting an array element with a sequence.
169.254.255.130 - - [31/Mar/2020:17:25:56 +0000] ""POST /invocations HTTP/1.1"" 500 290 ""-"" ""Go-http-client/1.1""
</code></pre>

<p>If I change my framework from XGBoost and SKLearn to run DecisionTree model, everything is working perfect and I am able to see the prediction results. Please let me know what I'm missing here and how to rectify this.</p>",1,3,2020-03-31 18:05:44.030000 UTC,,2020-03-31 19:22:59.553000 UTC,0,python|amazon-web-services|scikit-learn|xgboost|amazon-sagemaker,1733,2017-11-27 11:55:19.283000 UTC,2021-07-19 09:19:05.473000 UTC,"Bangalore, Karnataka, India",347,7,0,70,,,,,,['amazon-sagemaker']
Received client error (400) deploying huggingface bigscience/bloom to SageMaker,"<p>I want to deploy Bloom on SageMaker so that I have a Bloom inference API I can use. I started by running the following in a SageMaker jupyter notebook:</p>
<pre class=""lang-py prettyprint-override""><code>from sagemaker.huggingface import HuggingFaceModel
import sagemaker

role = sagemaker.get_execution_role()
# Hub Model configuration. https://huggingface.co/models
hub = {
    'HF_MODEL_ID':'bigscience/bloom',
    'HF_TASK':'text-generation'
}

# create Hugging Face Model Class
huggingface_model = HuggingFaceModel(
    transformers_version='4.17.0',
    pytorch_version='1.10.2',
    py_version='py38',
    env=hub,
    role=role, 
)

# deploy model to SageMaker Inference
predictor = huggingface_model.deploy(
    initial_instance_count=1, # number of instances
    instance_type='ml.m5.xlarge' # ec2 instance type
)

predictor.predict({
    'inputs': &quot;Can you please let us know more details about your &quot;
})
</code></pre>
<p>which produced:</p>
<pre><code>---------------------------------------------------------------------------
ModelError                                Traceback (most recent call last)
/tmp/ipykernel_15151/842216467.py in &lt;cell line: 1&gt;()
----&gt; 1 predictor.predict({
      2         'inputs': &quot;Can you please let us know more details about your &quot;
      3 })

~/anaconda3/envs/python3/lib/python3.8/site-packages/sagemaker/predictor.py in predict(self, data, initial_args, target_model, target_variant, inference_id)
    159             data, initial_args, target_model, target_variant, inference_id
    160         )
--&gt; 161         response = self.sagemaker_session.sagemaker_runtime_client.invoke_endpoint(**request_args)
    162         return self._handle_response(response)
    163 

~/anaconda3/envs/python3/lib/python3.8/site-packages/botocore/client.py in _api_call(self, *args, **kwargs)
    393                     &quot;%s() only accepts keyword arguments.&quot; % py_operation_name)
    394             # The &quot;self&quot; in this scope is referring to the BaseClient.
--&gt; 395             return self._make_api_call(operation_name, kwargs)
    396 
    397         _api_call.__name__ = str(py_operation_name)

~/anaconda3/envs/python3/lib/python3.8/site-packages/botocore/client.py in _make_api_call(self, operation_name, api_params)
    723             error_code = parsed_response.get(&quot;Error&quot;, {}).get(&quot;Code&quot;)
    724             error_class = self.exceptions.from_code(error_code)
--&gt; 725             raise error_class(parsed_response, operation_name)
    726         else:
    727             return parsed_response

ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from primary with message &quot;{
  &quot;code&quot;: 400,
  &quot;type&quot;: &quot;InternalServerException&quot;,
  &quot;message&quot;: &quot;\u0027bloom\u0027&quot;
}
&quot;. See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/huggingface-pytorch-inference-2022-07-29-23-06-38-076 in account 162923941922 for more information.
</code></pre>
<p>The cloudwatch log just shows:</p>
<pre><code>2022-07-29T23:09:09,135 [INFO ] W-bigscience__bloom-4-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -     raise PredictionException(str(e), 400)
</code></pre>
<p>How can I deploy it on sagemaker without encountering this problem?</p>",1,0,2022-08-06 20:48:40.120000 UTC,,,0,python-3.x|amazon-sagemaker|huggingface-transformers,141,2022-02-20 22:52:20.693000 UTC,2022-09-25 02:43:06.963000 UTC,,7,0,0,31,,,,,,['amazon-sagemaker']
How to copy local runs from mlflow to remote tracking server?,"<p>Dear mlflow community,</p>
<p>I am struggeling with a performant environment for both testing and production.</p>
<p>What I want to do is, train models locally and compare them in mlflow. In case I have found a good model I would like to push it to a remote mlflow instance.</p>
<p>For this purpose I have set up a tracking server with postgres and S3.</p>
<p>How can I push the model and all artifacts in the run to the remote mlflfow instance?</p>",0,0,2022-05-06 14:19:46.927000 UTC,,,0,python-3.x|tensorflow|mlflow,72,2014-12-23 23:50:02.193000 UTC,2022-09-21 11:10:16.000000 UTC,,1499,98,14,297,,,,,,['mlflow']
seems like GPU is not working in AWS Sagemaker Studio Lab,"<p>I've selected compute type as GPU, and opened my project.</p>
<p>but checking the local devices, it doesn't look like any GPU is deployed.</p>
<pre><code>import tensorflow as tf
from tensorflow.python.client import device_lib
device_lib.list_local_devices()
</code></pre>
<p>outputs:</p>
<pre><code>[name: &quot;/device:CPU:0&quot;
 device_type: &quot;CPU&quot;
 memory_limit: 268435456
 locality {
 }
 incarnation: 13079107644747151451]
</code></pre>
<p>How can I enable GPU usage in AWS Sagemaker?</p>",1,0,2021-12-21 01:53:04.293000 UTC,,,2,python|amazon-web-services|tensorflow|gpu|amazon-sagemaker,459,2021-12-21 01:46:20.483000 UTC,2022-09-16 03:57:51.900000 UTC,,21,0,0,0,,,,,,['amazon-sagemaker']
NoSuchEntityException: An error occurred (NoSuchEntity) when calling the GetRole operation: The user with name <name> cannot be found,"<p>Call to <code>get_execution_role()</code> from notebook instance fails with the error message <code>NoSuchEntityException: An error occurred (NoSuchEntity) when calling the GetRole operation: The user with name &lt;name&gt; cannot be found.</code></p>

<p>Stack trace:</p>

<pre><code>NoSuchEntityExceptionTraceback (most recent call last)
&lt;ipython-input-1-1e2d3f162cfe&gt; in &lt;module&gt;()
      5 sagemaker_session = sagemaker.Session()
      6 
----&gt; 7 role = get_execution_role()

/home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/sagemaker/session.pyc in get_execution_role(sagemaker_session)
    871     if not sagemaker_session:
    872         sagemaker_session = Session()
--&gt; 873     arn = sagemaker_session.get_caller_identity_arn()
    874 
    875     if 'role' in arn:

/home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/sagemaker/session.pyc in get_caller_identity_arn(self)
    701         # Call IAM to get the role's path
    702         role_name = role[role.rfind('/') + 1:]
--&gt; 703         role = self.boto_session.client('iam').get_role(RoleName=role_name)['Role']['Arn']
    704 
    705         return role

/home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/botocore/client.pyc in _api_call(self, *args, **kwargs)
    312                     ""%s() only accepts keyword arguments."" % py_operation_name)
    313             # The ""self"" in this scope is referring to the BaseClient.
--&gt; 314             return self._make_api_call(operation_name, kwargs)
    315 
    316         _api_call.__name__ = str(py_operation_name)

/home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/botocore/client.pyc in _make_api_call(self, operation_name, api_params)
    610             error_code = parsed_response.get(""Error"", {}).get(""Code"")
    611             error_class = self.exceptions.from_code(error_code)
--&gt; 612             raise error_class(parsed_response, operation_name)
    613         else:
    614             return parsed_response

NoSuchEntityException: An error occurred (NoSuchEntity) when calling the GetRole operation: The user with name &lt;name&gt; cannot be found.
</code></pre>

<p>However using boto client directly to get info about the role succeeds. This works fine:</p>

<pre><code>response = client.get_role(
    RoleName='role-name',
)['Role']['Arn']
</code></pre>",2,0,2018-07-14 16:02:32.177000 UTC,,,1,amazon-sagemaker,2538,2009-06-17 05:10:17.663000 UTC,2022-04-22 22:18:06.087000 UTC,"San Francisco Bay Area, CA, United States",1417,73,1,192,,,,,,['amazon-sagemaker']
Creating Sagemaker training job with terraform?,"<p>I am new to terraform and was looking through the documentation. From what I can tell, there's nothing in terraform with regards to create a training job that has the model artifacts. Does this mean I can't use terraform to set up the full sagemaker pipeline? It seems to me you would have to first create the training job in some way, and then you can use terraform to create a model enpoint that uses what is there (but you can't do the training job itself with terraform).</p>",1,3,2019-07-02 13:20:42.210000 UTC,,,4,amazon-web-services|terraform|amazon-sagemaker,938,2015-01-15 17:43:03.700000 UTC,2022-08-23 22:54:25.603000 UTC,,1387,51,1,153,,,,,,['amazon-sagemaker']
Error with NaN values in input data file for Sagemaker Linear Learner model,"<p>I am trying to run a Linear Regression with sagemaker.  My matrix has some null values and the linear learner algorithm is failing because of this.  Is there something I can do to make the algorithm handle null values?</p>
<p>Matrix data below:</p>
<pre><code>array([[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ..., 0.0000000e+00,
        1.7883900e+05, 9.6533337e+00],
       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ..., 0.0000000e+00,
        4.9014000e+04, 1.3181389e+01],
       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ..., 0.0000000e+00,
        1.2483900e+05, 1.1561944e+01],
       ...,
       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ..., 0.0000000e+00,
        4.7306000e+04, 1.8681944e+01],
       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ..., 0.0000000e+00,
        1.3530000e+04, 1.1964444e+01],
       [0.0000000e+00,           nan,           nan, ..., 0.0000000e+00,
        8.4100000e+03, 1.8925833e+01]], dtype=float32)
</code></pre>
<h1>Run the model</h1>
<pre><code>from sagemaker import get_execution_role
role = get_execution_role()
linear = sagemaker.estimator.Estimator(containers[boto3.Session().region_name],
                                       role, 
                                       train_instance_count=1, 
                                       train_instance_type='ml.c4.xlarge',
                                       output_path=output_location,
                                       sagemaker_session=sess)

#Model Parameters
linear.set_hyperparameters(feature_dim=25,
                           predictor_type='regressor',
                           normalize_data=False)

linear.fit({'train': s3_train_data})
linear_predictor = linear.deploy(initial_instance_count=1,
                                 instance_type='ml.m4.xlarge')
</code></pre>
<blockquote>
<p>Blockquote</p>
</blockquote>
<p>Output:</p>
<pre><code>2019-08-16 12:40:21 Starting - Starting the training job...
2019-08-16 12:40:24 Starting - Launching requested ML instances......
2019-08-16 12:41:23 Starting - Preparing the instances for training......
2019-08-16 12:42:34 Downloading - Downloading input data...
2019-08-16 12:43:15 Training - Training image download completed. Training in progress.
2019-08-16 12:43:15 Uploading - Uploading generated training model
2019-08-16 12:43:15 Failed - Training job failed

UnexpectedStatusException: Error for Training job linear-learner-2019-08-16-12-40-21-312: Failed. Reason: ClientError: Unable to read data channel 'train'. Found missing (NaN) values. Please remove any missing (NaN) values in the input data. (caused by MXNetError)

Caused by: [12:43:11] /opt/brazil-pkg-cache/packages/AIAlgorithmsCppLibs/AIAlgorithmsCppLibs-2.0.1649.0/AL2012/generic-flavor/src/src/aialgs/io/iterator_base.cpp:103: (Input Error) (NaN) NaN value encountered in the dataset.
</code></pre>",1,0,2019-08-16 13:27:20.143000 UTC,,2020-06-20 09:12:55.060000 UTC,1,amazon-sagemaker,373,2019-08-16 12:03:38.013000 UTC,2019-09-02 19:55:34.877000 UTC,,11,0,0,1,,,,,,['amazon-sagemaker']
How do I setup the _SERVER_MODEL_PATH variable?,"<p>I'm trying to replicate the quickstart save and serve example.
I go to the example folder, run the python script and can see the model runs and artifacts when I type <code>mlflow ui</code>.
However, when I try the mlflow serve command with different model run Ids and ports I get a 404 in my browser, even though the command seems successful:</p>
<pre><code>mlflow models serve -m runs:/e1dabe8fc6e84286af5bee28ca89cdde/model --port 1234
2022/07/11 07:40:01 INFO mlflow.models.cli: Selected backend for flavor 'python_function'
2022/07/11 07:40:02 INFO mlflow.utils.conda: Conda environment mlflow-ddf4db606beaa0e9bb42ff0ed98e8f4c4c7cb1f4 already exists.
2022/07/11 07:40:02 INFO mlflow.pyfunc.backend: === Running command 'conda activate mlflow-ddf4db606beaa0e9bb42ff0ed98e8f4c4c7cb1f4 &amp; waitress-serve --host=127.0.0.1 --port=1234 --ident=mlflow mlflow.pyfunc.scoring_server.wsgi:app'
INFO:waitress:Serving on http://127.0.0.1:1234
</code></pre>
<p>I tried running directly from anaconda prompt, and I get the following error:</p>
<pre><code>conda activate mlflow-ddf4db606beaa0e9bb42ff0ed98e8f4c4c7cb1f4 &amp; waitress-serve --host=127.0.0.1 --port=1234 --ident=mlflow mlflow.pyfunc.scoring_server.wsgi:app

Traceback (most recent call last):
File &quot;C:\Users\sergio ferro.conda\envs\mlflow-ddf4db606beaa0e9bb42ff0ed98e8f4c4c7cb1f4\lib\runpy.py&quot;, line 197, in _run_module_as_main
return _run_code(code, main_globals, None,

File &quot;C:\Users\sergio ferro.conda\envs\mlflow-ddf4db606beaa0e9bb42ff0ed98e8f4c4c7cb1f4\lib\runpy.py&quot;, line 87, in run_code
exec(code, run_globals)

File &quot;C:\Users\sergio ferro.conda\envs\mlflow-ddf4db606beaa0e9bb42ff0ed98e8f4c4c7cb1f4\Scripts\waitress-serve.exe_main.py&quot;, line 7, in

File &quot;C:\Users\sergio ferro.conda\envs\mlflow-ddf4db606beaa0e9bb42ff0ed98e8f4c4c7cb1f4\lib\site-packages\waitress\runner.py&quot;, line 283, in run
app = resolve(module, obj_name)

File &quot;C:\Users\sergio ferro.conda\envs\mlflow-ddf4db606beaa0e9bb42ff0ed98e8f4c4c7cb1f4\lib\site-packages\waitress\runner.py&quot;, line 218, in resolve
obj = import(module_name, fromlist=segments[:1])

File &quot;C:\Users\sergio ferro.conda\envs\mlflow-ddf4db606beaa0e9bb42ff0ed98e8f4c4c7cb1f4\lib\site-packages\mlflow\pyfunc\scoring_server\wsgi.py&quot;, line 6, in
app = scoring_server.init(load_model(os.environ[scoring_server._SERVER_MODEL_PATH]))

File &quot;C:\Users\sergio ferro.conda\envs\mlflow-ddf4db606beaa0e9bb42ff0ed98e8f4c4c7cb1f4\lib\os.py&quot;, line 679, in getitem
raise KeyError(key) from None
KeyError: 'pyfunc_model_path'
</code></pre>
<p>I have tried deleting and creating a new anaconda environment, ran from git bash, anaconda prompt, added anaconda3 environment variables. I know it has something to do with the <code>_SERVER_MODEL_PATH</code> variable but I wouldn't know how to set it up or which path add to my environment variables so it can read this variable from there.</p>",0,1,2022-07-11 12:06:46.680000 UTC,,2022-07-24 11:12:09.070000 UTC,0,anaconda|mlflow,14,2017-03-21 14:16:28.373000 UTC,2022-09-20 12:01:20.917000 UTC,,1,0,0,0,,,,,,['mlflow']
How to solve original size problem of MLFlow Artifacts?,"<p>I want to save my feature importance plot into mlflow project. But my figure size is looking like this; x axis names are cropped. How can i fix this problem?</p>
<p>Code;</p>
<pre><code>fig, ax = plt.subplots()
ax.plot(forest_importances[-25:])
for tick in ax.get_xticklabels():
    tick.set_rotation(90)
    
mlflow.log_figure(fig, &quot;figure.png&quot;)
</code></pre>
<p>Normal;
<a href=""https://i.stack.imgur.com/AHpCV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AHpCV.png"" alt=""enter image description here"" /></a></p>
<p>In mlflow ;
<a href=""https://i.stack.imgur.com/iHY4g.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/iHY4g.png"" alt=""enter image description here"" /></a></p>",1,0,2022-08-17 14:45:10.943000 UTC,,,0,python|mlflow,13,2018-07-05 11:57:46.730000 UTC,2022-09-23 13:39:15.477000 UTC,,979,251,110,571,,,,,,['mlflow']
mlflow model serving not working properly,"<p>I am trying to serve my model after model registry. The model is successfully being registered in mlflow but when I am going to serve it it is showing the following error.</p>
<p>Error: Got unexpected extra arguments (Class Classification New log)
Thu May 19 06:52:51 UTC 2022
Waiting for another process to start...
Usage: mlflow artifacts download [OPTIONS]
Try 'mlflow artifacts download --help' for help.</p>",0,0,2022-05-19 06:55:41.443000 UTC,,,0,databricks|mlflow,45,2022-02-27 20:42:11.227000 UTC,2022-09-21 06:56:42.217000 UTC,,1,0,0,2,,,,,,['mlflow']
Azure Machine Learning - authorization error despite having maximum permissions,"<p>I am trying to build a machine learning model on Azure for my company. The IT team at the company I work at has given me maximum permissions for our Azure Machine Learning account since I am doing all the setup part (we started using it only last month). However, I checked the portal and realized that I am not authorized to access any of the modules within Azure ML, namely Experiment, Models, Endpoints, Datasets, etc. Is there something I am missing that is giving me this error? The error message has this <a href=""https://go.microsoft.com/fwlink/?linkid=2161335"" rel=""nofollow noreferrer"">link</a> but I am not sure it serves the purpose.</p>
<p><strong>Note:</strong> I am new to Azure so please forgive me if this is a very basic doubt.</p>
<p><a href=""https://i.stack.imgur.com/oPvpP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/oPvpP.png"" alt=""enter image description here"" /></a></p>",1,1,2021-06-09 08:32:39.873000 UTC,,,1,azure|azure-machine-learning-service,251,2017-12-13 05:40:29.087000 UTC,2022-09-25 04:16:20.110000 UTC,"Seattle, Washington",53,108,0,21,,,,,,['azure-machine-learning-service']
"Error in giving inputs to the tensorflow serving model on sagemaker. {'error': ""Missing 'inputs' or 'instances' key""}","<p>I have a custom model built-in TensorFlow. I am trying to deploy this model on amazon sagemaker for inference. The model takes three inputs and gives five outputs.
The name of the inputs are:</p>

<pre><code>1.    input_image
2.    input_image_meta
3.    input_anchors
</code></pre>

<p>and the name of outputs are:</p>

<pre><code>1    output_detections
2    output_mrcnn_class
3    output_mrcnn_bbox
4    output_mrcnn_mask
5    output_rois
</code></pre>

<p>I have successfully created the model endpoint on sagemaker and when I am trying to hit the request for the results, I am getting {'error': ""Missing 'inputs' or 'instances' key""} in return.</p>

<p>The sagemaker endpoint gets created and the tensorflow server also starts(as shown in CloudWatch logs).
On the client side, I call the predictor using follwoing code:</p>

<pre><code>request = {}
request[""img_link""] = ""image.jpg""
result = predictor.predict(request)
</code></pre>

<p>But when I print the result the following gets printed out, {'error': ""Missing 'inputs' or 'instances' key""}
All the bucket connections for loading the image are in inference.py</p>",1,1,2019-08-30 17:28:20.260000 UTC,1.0,,2,json|tensorflow|tensorflow-serving|amazon-sagemaker,798,2019-08-30 17:20:07.190000 UTC,2019-09-03 13:23:31.813000 UTC,,21,0,0,2,,,,,,['amazon-sagemaker']
Vertex AI Instance Out of Disk Space,"<p>I accidentally ran out of disk space on my Vertex AI instance. There's no way to connect to it by any means now. Using the jupyter failed as there's not enough disk space:
<code>OSError: [Errno 28] No space left on device</code></p>
<p>I tried to increase disk space for both boot and data disks using <code>gcloud compute disks resize</code>, but it still doesn't work despite disk space being shown as increased in the machine info panel.</p>
<p>Also tried connecting through ssh but got timeouts. My guess is that it's still caused by disk space.</p>
<p>So is there any ways to recover the instance without a hard reset?</p>",0,0,2022-08-13 15:12:11.073000 UTC,,2022-08-13 15:13:15.093000 UTC,0,google-cloud-platform|google-cloud-vertex-ai,71,2022-08-13 15:02:38.963000 UTC,2022-09-20 21:40:25.030000 UTC,,1,0,0,0,,,,,,['google-cloud-vertex-ai']
Allowed values shown as constant input in azure ml web service,"<p>I have created a web service using Azure ML and deployed it. It works, but when I hit the Test button to test the web service, I am not being able to enter a different set of input values in the screen which asks for input. See screenshot below. As you can see, it's not a textbox where I can enter values, but a dropdown where the values are the ones in my script.</p>

<p><a href=""https://i.stack.imgur.com/O1c4f.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/O1c4f.png"" alt=""enter image description here""></a></p>

<p>Also, note how the instructions page shows allowed values as just those values
<a href=""https://i.stack.imgur.com/62HDR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/62HDR.png"" alt=""enter image description here""></a></p>

<p>These values are from my initial script where I do the following</p>

<pre><code>## ------- User-Defined Parameters ------ ##

IDinput&lt;- data.frame(
GenderCD=""M"",
Age=""8"",
..,
..
)

# Select data.frame to be sent to the output Dataset port
maml.mapOutputPort(""IDinput"");
</code></pre>

<p>I then have a script which reads these variables using POST as</p>

<pre><code># Map 1-based optional input ports to variables# Map 1-based optional input ports to variables
POST &lt;- maml.mapInputPort(1) # class: data.frame

#getting data from POST
mytestrow = NULL
mytestrow$GenderCD=POST$GenderCD
mytestrow$Age=POST$Age

#perform logic and store in a data frame called outputdf

# Select data.frame to be sent to the output Dataset port
maml.mapOutputPort(""outputdf"");
</code></pre>

<p>My overall architecture looks as
<a href=""https://i.stack.imgur.com/F0fuo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/F0fuo.png"" alt=""enter image description here""></a></p>",1,0,2015-09-19 18:57:56.097000 UTC,,2016-03-01 16:34:04.130000 UTC,0,web-services|post|cortana-intelligence|azure-machine-learning-studio,60,2015-03-21 00:57:21.170000 UTC,2021-03-18 18:32:12.667000 UTC,,2024,76,1,298,,,,,,['azure-machine-learning-studio']
ColumnTransformer not fitted after sklearn Pipeline loaded from Mlflow,"<p>I am building a machine learning model using sklearn Pipeline which includes a ColumnTransformer as a preprocessor before the actual model. Below is the code how the pipeline is created.</p>
<pre><code>transformers = []
num_pipe = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])
transformers.append(('numerical', num_pipe, num_cols))
cat_pipe = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('ohe', OneHotEncoder(handle_unknown='ignore'))
])
transformers.append(('categorical', cat_pipe, cat_cols))
preprocessor = ColumnTransformer(transformers, remainder='passthrough')
model = Pipeline([
  ('prep', preprocessor),
  ('clf', XGBClassifier())
])
</code></pre>
<p>I am using <code>Mlflow</code> to log the model artifact as sklearn model.</p>
<pre><code>model.fit(X, y)
mlflow.sklearn.log_model(model, model_uri)
</code></pre>
<p>When I tried to load the model from mlflow for scoring though, I got the error &quot;This ColumnTransformer instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.&quot;</p>
<pre><code>run_model = mlflow.sklearn.load_model(model_uri)
run_model.predict(X_pred)
</code></pre>
<p>Only standard sklearn imputers and scalers are being used. Nothing is custom built. Why does it still complain not fitted. Is column transformer not compatible with mlflow?</p>
<p><strong>EDIT</strong>
I ran <code>check_is_fitted</code> on the second step of the Pipeline which is the xgboost model itself after loaded from mlflow and it is NOT fitted either.</p>",0,0,2021-11-02 19:46:45.073000 UTC,,2021-11-02 20:13:17.500000 UTC,0,python|scikit-learn|mlflow,139,2014-02-27 05:08:23.600000 UTC,2022-09-22 19:28:57.043000 UTC,,4303,134,1,536,,,,,,['mlflow']
specify an environment in an Inference config,"<p>I am having an issue deploying a model using a custom docker image.
    The deployment is failing because the packages needed are not in the “default” environment and I need to specify a custom one. (""/miniconda/envs/py37/bin/python"")</p>

<pre><code>I am using the same image to run the training, and with the estimator class I do have a way to specify the environment. Is it possible to something similar with the InferenceConfig?

    est = Estimator(source_directory=script_folder,
                    script_params=script_params,
                    inputs=inputs,
                    compute_target=gpu_compute_target,
                    entry_script='train.py',
                    image_registry_details = my_registry,
                    custom_docker_image='omr:latest',
                    use_gpu=True,
                    user_managed=True)
    est.run_config.environment.python.user_managed_dependencies = True
    est.run_config.environment.python.interpreter_path = ""/miniconda/envs/py37/bin/python""



    #establish container configuration using custom base image
    privateRegistry = aml_utils.getContainerRegistryDetails()
    inference_config = InferenceConfig(source_directory=""./detect"",
                                       runtime= ""python"", 
                                       entry_script=""aml_score.py"",
                                       enable_gpu=False,
                                       base_image=""amlworkspaceom3760145996.azurecr.io/omr:latest"",
                                       base_image_registry = privateRegistry)

    #deploy model via AKS deployment target
    deployment_config = AksWebservice.deploy_configuration(gpu_cores = 1, memory_gb = 1, auth_enabled = False)
    targetcluster = aml_utils.getGpuDeploymentTarget(ws)

    model_service = Model.deploy(workspace = ws,
                               name = ""model_name,
                               models = [model],
                               inference_config = inference_config, 
                               deployment_config = deployment_config,
                               deployment_target = targetcluster)
</code></pre>",1,0,2019-08-09 00:45:33.773000 UTC,,,0,azure-machine-learning-service|azure-machine-learning-workbench,449,2018-03-01 09:03:25.313000 UTC,2022-03-13 07:26:26.350000 UTC,,33,0,0,2,,,,,,"['azure-machine-learning-workbench', 'azure-machine-learning-service']"
Is it possible to access datastores from a Azure ML Service webservice?,"<p>According to the Azure ML Service <a href=""https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-access-data#access-datastores-during-training"" rel=""nofollow noreferrer"">documentation</a> it is possible to access datastores during training, but I couldn't find anything about using data from datastores inside the Webservice.</p>
<p>Even though is not necessary use external data to make an Webservice work, to use my model as I intend I need to use some datasets with features created based on historical data. For example: imagine that I'm trying to forecast if a client is going to pay a bill in the right date a good strategy is to create a feature based on previous payments of this same client.</p>
<p>The only external file that I could use in a Webservice is the 'model.pkl' which stores the ML model that I created previously.</p>
<p>How can I get an Azure ML webservice access a datastore?</p>",2,0,2019-06-28 19:04:43.060000 UTC,1.0,2022-07-27 15:58:05.407000 UTC,1,azure|azure-machine-learning-service,935,2019-06-26 17:47:39.943000 UTC,2021-08-26 03:29:09.313000 UTC,,19,1,0,7,,,,,,['azure-machine-learning-service']
embeddings layer produce cannot be interpreted as a Tensor in sagemaker?,"<p>I am trying to use python SDk with tensorflow for test classification on sagemaker. I am able to modify this <a href=""https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/tensorflow_abalone_age_predictor_using_keras/abalone.py"" rel=""nofollow noreferrer"">https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/tensorflow_abalone_age_predictor_using_keras/abalone.py</a> and run it but when I  change the arch to include embeddings layer, I get the error</p>

<blockquote>
  <p>""Fetch argument  cannot be interpreted as a Tensor. (Tensor Tensor(""first-layer/embeddings:0"", shape=(*, <em>), dtype=float32_ref) is not an element of this graph.""</em></p>
</blockquote>

<p>When I run it as a standalone model, it runs perfectly. 
here is arch for standalone model</p>

<pre><code>model = Sequential()
model.add(Embedding(len(word_index) + 1,
                        EMBEDDING_DIM,
                        weights=[embedding_matrix],
                        input_length=MAX_SEQUENCE_LENGTH,
                        trainable=False))

model.add(Conv1D(64, kernel_size=10, padding='same', activation='relu'))
model.add(Conv1D(64, kernel_size=15, padding='same', activation='selu'))
model.add(Conv1D(128, kernel_size=15, padding='same', activation='relu'))
model.add(Conv1D(64, kernel_size=25, padding='same', activation='softmax'))
model.add(Conv1D(128, kernel_size=15, padding='same', activation='relu'))
model.add(BatchNormalization())

model.add(Flatten())
model.add(Dense(2, activation='softmax'))
</code></pre>

<p>Here is my model_fn for sagemaker:</p>

<pre><code>embedding = tf.keras.layers.Embedding(len(word_index) + 1,
                            EMBEDDING_DIM,
                            weights=[embedding_matrix],
                            input_length=MAX_SEQUENCE_LENGTH,
                            trainable=False, name='first-layer')(features[INPUT_TENSOR_NAME])

first = tf.keras.layers.Conv1D(64, kernel_size=10, padding='same', activation='relu')(embedding)
second = tf.keras.layers.Conv1D(64, kernel_size=15, padding='same', activation='relu')(first)
 third = tf.keras.layers.Conv1D(128, kernel_size=15, padding='same', activation='relu')(second)
 fourth = tf.keras.layers.Conv1D(64, kernel_size=25, padding='same', activation='softmax')(third)
 fifth = tf.keras.layers.Conv1D(128, kernel_size=15, padding='same', activation='relu')(fourth)
sixth = tf.keras.layers.BatchNormalization()(fifth)

output = tf.keras.layers.Flatten()(sixth)
output_layer = tf.keras.layers.Dense(2, activation='softmax'))(output)
</code></pre>

<p>There no issue with input dimension or value, if I replace this arch with a simple arch of dense layer only, code works perfectly. </p>

<p>I have already tried solution on 
<a href=""https://stackoverflow.com/questions/44219094/tensorflow-the-tensor-is-not-the-element-of-this-graph"">TensorFlow: The tensor is not the element of this graph</a> but I get a new error </p>

<blockquote>
  <p>Input graph and Layer graph are not the same: Tensor(""random_shuffle_queue_DequeueMany:1"", shape=(128, 200), dtype=float32, device=/device:CPU:0) is not from the passed-in graph.*</p>
</blockquote>",0,1,2018-03-15 17:08:28.143000 UTC,,2018-03-15 18:20:21.170000 UTC,1,python|tensorflow|amazon-sagemaker,406,2018-03-15 16:49:46.847000 UTC,2018-04-18 16:28:07.217000 UTC,,11,0,0,3,,,,,,['amazon-sagemaker']
Azure ML Two-Class Decision Forest causes Score Model Error 1000,"<p>Can anybody explain to me why Two-Class Decision Forest causes Score Model Error 1000: AFx Library library exception: col: Invalid index: 1, expected bounds [0, 1)? My dataset is normalized, column names doesn't contains spaces...</p>

<p>Thank you!</p>",0,1,2015-05-28 07:07:21.477000 UTC,,,2,azure|azure-machine-learning-studio,174,2010-11-19 12:08:15.693000 UTC,2022-02-05 17:47:04.060000 UTC,Slovenia,189,44,0,30,,,,,,['azure-machine-learning-studio']
Azure ML: Upload File to Step Run's Output - Authentication Error,"<p>During a PythonScriptStep in an Azure ML Pipeline, I'm saving a model as joblib pickle dump to a directory in a Blob Container in the Azure Blob Storage which I've created during the setup of the Azure ML Workspace. Afterwards I'm trying to upload this model file to the step run's output directory using</p>
<pre><code>Run.upload_file (name, path_or_stream)
</code></pre>
<p>(for the function's documentation, see <a href=""https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.run(class)?view=azure-ml-py#upload-file-name--path-or-stream--datastore-name-none-"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.run(class)?view=azure-ml-py#upload-file-name--path-or-stream--datastore-name-none-</a>)</p>
<p>Some time ago when I created the script using the azureml-sdk version 1.18.0, everything worked fine. Now, I've updated the script's functionalities and upgraded the azureml-sdk to version 1.33.0 during the process and the upload function now runs into the following error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;/opt/miniconda/lib/python3.7/site-packages/azureml/_file_utils/upload.py&quot;, line 64, in upload_blob_from_stream
    validate_content=True)
  File &quot;/opt/miniconda/lib/python3.7/site-packages/azureml/_restclient/clientbase.py&quot;, line 93, in execute_func_with_reset
    return ClientBase._execute_func_internal(backoff, retries, module_logger, func, reset_func, *args, **kwargs)
  File &quot;/opt/miniconda/lib/python3.7/site-packages/azureml/_restclient/clientbase.py&quot;, line 367, in _execute_func_internal
    left_retry = cls._handle_retry(back_off, left_retry, total_retry, error, logger, func)
  File &quot;/opt/miniconda/lib/python3.7/site-packages/azureml/_restclient/clientbase.py&quot;, line 399, in _handle_retry
    raise error
  File &quot;/opt/miniconda/lib/python3.7/site-packages/azureml/_restclient/clientbase.py&quot;, line 358, in _execute_func_internal
    response = func(*args, **kwargs)
  File &quot;/opt/miniconda/lib/python3.7/site-packages/azureml/_vendor/azure_storage/blob/blockblobservice.py&quot;, line 614, in create_blob_from_stream
    initialization_vector=iv
  File &quot;/opt/miniconda/lib/python3.7/site-packages/azureml/_vendor/azure_storage/blob/_upload_chunking.py&quot;, line 98, in _upload_blob_chunks
    range_ids = [f.result() for f in futures]
  File &quot;/opt/miniconda/lib/python3.7/site-packages/azureml/_vendor/azure_storage/blob/_upload_chunking.py&quot;, line 98, in &lt;listcomp&gt;
    range_ids = [f.result() for f in futures]
  File &quot;/opt/miniconda/lib/python3.7/concurrent/futures/_base.py&quot;, line 435, in result
    return self.__get_result()
  File &quot;/opt/miniconda/lib/python3.7/concurrent/futures/_base.py&quot;, line 384, in __get_result
    raise self._exception
  File &quot;/opt/miniconda/lib/python3.7/concurrent/futures/thread.py&quot;, line 57, in run
    result = self.fn(*self.args, **self.kwargs)
  File &quot;/opt/miniconda/lib/python3.7/site-packages/azureml/_vendor/azure_storage/blob/_upload_chunking.py&quot;, line 210, in process_chunk
    return self._upload_chunk_with_progress(chunk_offset, chunk_bytes)
  File &quot;/opt/miniconda/lib/python3.7/site-packages/azureml/_vendor/azure_storage/blob/_upload_chunking.py&quot;, line 224, in _upload_chunk_with_progress
    range_id = self._upload_chunk(chunk_offset, chunk_data)
  File &quot;/opt/miniconda/lib/python3.7/site-packages/azureml/_vendor/azure_storage/blob/_upload_chunking.py&quot;, line 269, in _upload_chunk
    timeout=self.timeout,
  File &quot;/opt/miniconda/lib/python3.7/site-packages/azureml/_vendor/azure_storage/blob/blockblobservice.py&quot;, line 1013, in _put_block
    self._perform_request(request)
  File &quot;/opt/miniconda/lib/python3.7/site-packages/azureml/_vendor/azure_storage/common/storageclient.py&quot;, line 432, in _perform_request
    raise ex
  File &quot;/opt/miniconda/lib/python3.7/site-packages/azureml/_vendor/azure_storage/common/storageclient.py&quot;, line 357, in _perform_request
    raise ex
  File &quot;/opt/miniconda/lib/python3.7/site-packages/azureml/_vendor/azure_storage/common/storageclient.py&quot;, line 343, in _perform_request
    HTTPError(response.status, response.message, response.headers, response.body))
  File &quot;/opt/miniconda/lib/python3.7/site-packages/azureml/_vendor/azure_storage/common/_error.py&quot;, line 115, in _http_error_handler
    raise ex
azure.common.AzureHttpError: Server failed to authenticate the request. Make sure the value of Authorization header is formed correctly including the signature. ErrorCode: AuthenticationFailed
&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt;&lt;Error&gt;&lt;Code&gt;AuthenticationFailed&lt;/Code&gt;&lt;Message&gt;Server failed to authenticate the request. Make sure the value of Authorization header is formed correctly including the signature.
RequestId:5d4e1b7e-c01e-0070-0d47-9bf8a0000000
Time:2021-08-27T13:30:02.2685991Z&lt;/Message&gt;&lt;AuthenticationErrorDetail&gt;Signature did not match. String to sign used was rcw
2021-08-27T13:19:56Z
2021-08-28T13:29:56Z
/blob/mystorage/azureml/ExperimentRun/dcid.98d11a7b-2aac-4bc0-bd64-bb4d72e0e0be/outputs/models/Model.pkl

2019-07-07
b

&lt;/AuthenticationErrorDetail&gt;&lt;/Error&gt;

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;/mnt/batch/tasks/shared/LS_root/jobs/.../azureml-setup/context_manager_injector.py&quot;, line 243, in execute_with_context
    runpy.run_path(sys.argv[0], globals(), run_name=&quot;__main__&quot;)
  File &quot;/opt/miniconda/lib/python3.7/runpy.py&quot;, line 263, in run_path
    pkg_name=pkg_name, script_name=fname)
  File &quot;/opt/miniconda/lib/python3.7/runpy.py&quot;, line 96, in _run_module_code
    mod_name, mod_spec, pkg_name, script_name)
  File &quot;/opt/miniconda/lib/python3.7/runpy.py&quot;, line 85, in _run_code
    exec(code, run_globals)
  File &quot;401_AML_Pipeline_Time_Series_Model_Training_Azure_ML_CPU.py&quot;, line 318, in &lt;module&gt;
    main()
  File &quot;401_AML_Pipeline_Time_Series_Model_Training_Azure_ML_CPU.py&quot;, line 286, in main
    path_or_stream=model_path)
  File &quot;/opt/miniconda/lib/python3.7/site-packages/azureml/core/run.py&quot;, line 53, in wrapped
    return func(self, *args, **kwargs)
  File &quot;/opt/miniconda/lib/python3.7/site-packages/azureml/core/run.py&quot;, line 1989, in upload_file
    datastore_name=datastore_name)
  File &quot;/opt/miniconda/lib/python3.7/site-packages/azureml/_restclient/artifacts_client.py&quot;, line 114, in upload_artifact
    return self.upload_artifact_from_path(artifact, *args, **kwargs)
  File &quot;/opt/miniconda/lib/python3.7/site-packages/azureml/_restclient/artifacts_client.py&quot;, line 107, in upload_artifact_from_path
    return self.upload_artifact_from_stream(stream, *args, **kwargs)
  File &quot;/opt/miniconda/lib/python3.7/site-packages/azureml/_restclient/artifacts_client.py&quot;, line 99, in upload_artifact_from_stream
    content_type=content_type, session=session)
  File &quot;/opt/miniconda/lib/python3.7/site-packages/azureml/_restclient/artifacts_client.py&quot;, line 88, in upload_stream_to_existing_artifact
    timeout=TIMEOUT, backoff=BACKOFF_START, retries=RETRY_LIMIT)
  File &quot;/opt/miniconda/lib/python3.7/site-packages/azureml/_file_utils/upload.py&quot;, line 71, in upload_blob_from_stream
    raise AzureMLException._with_error(azureml_error, inner_exception=e)
azureml._common.exceptions.AzureMLException: AzureMLException:
    Message: Encountered authorization error while uploading to blob storage. Please check the storage account attached to your workspace. Make sure that the current user is authorized to access the storage account and that the request is not blocked by a firewall, virtual network, or other security setting.
    StorageAccount: mystorage
    ContainerName: azureml
    StatusCode: 403
    InnerException Server failed to authenticate the request. Make sure the value of Authorization header is formed correctly including the signature. ErrorCode: AuthenticationFailed
&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt;&lt;Error&gt;&lt;Code&gt;AuthenticationFailed&lt;/Code&gt;&lt;Message&gt;Server failed to authenticate the request. Make sure the value of Authorization header is formed correctly including the signature.
RequestId:5d4e1b7e-c01e-0070-0d47-9bf8a0000000
Time:2021-08-27T13:30:02.2685991Z&lt;/Message&gt;&lt;AuthenticationErrorDetail&gt;Signature did not match. String to sign used was rcw
2021-08-27T13:19:56Z
2021-08-28T13:29:56Z
/blob/mystorage/azureml/ExperimentRun/dcid.98d11a7b-2aac-4bc0-bd64-bb4d72e0e0be/outputs/models/Model.pkl

2019-07-07
b

&lt;/AuthenticationErrorDetail&gt;&lt;/Error&gt;
    ErrorResponse 
{
    &quot;error&quot;: {
        &quot;code&quot;: &quot;UserError&quot;,
        &quot;message&quot;: &quot;Encountered authorization error while uploading to blob storage. Please check the storage account attached to your workspace. Make sure that the current user is authorized to access the storage account and that the request is not blocked by a firewall, virtual network, or other security setting.\n\tStorageAccount: mystorage\n\tContainerName: azureml\n\tStatusCode: 403&quot;,
        &quot;inner_error&quot;: {
            &quot;code&quot;: &quot;Auth&quot;,
            &quot;inner_error&quot;: {
                &quot;code&quot;: &quot;Authorization&quot;
            }
        }
    }
}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;401_AML_Pipeline_Time_Series_Model_Training_Azure_ML_CPU.py&quot;, line 318, in &lt;module&gt;
    main()
  File &quot;401_AML_Pipeline_Time_Series_Model_Training_Azure_ML_CPU.py&quot;, line 286, in main
    path_or_stream=model_path)
  File &quot;/opt/miniconda/lib/python3.7/site-packages/azureml/core/run.py&quot;, line 53, in wrapped
    return func(self, *args, **kwargs)
  File &quot;/opt/miniconda/lib/python3.7/site-packages/azureml/core/run.py&quot;, line 1989, in upload_file
    datastore_name=datastore_name)
  File &quot;/opt/miniconda/lib/python3.7/site-packages/azureml/_restclient/artifacts_client.py&quot;, line 114, in upload_artifact
    return self.upload_artifact_from_path(artifact, *args, **kwargs)
  File &quot;/opt/miniconda/lib/python3.7/site-packages/azureml/_restclient/artifacts_client.py&quot;, line 107, in upload_artifact_from_path
    return self.upload_artifact_from_stream(stream, *args, **kwargs)
  File &quot;/opt/miniconda/lib/python3.7/site-packages/azureml/_restclient/artifacts_client.py&quot;, line 99, in upload_artifact_from_stream
    content_type=content_type, session=session)
  File &quot;/opt/miniconda/lib/python3.7/site-packages/azureml/_restclient/artifacts_client.py&quot;, line 88, in upload_stream_to_existing_artifact
    timeout=TIMEOUT, backoff=BACKOFF_START, retries=RETRY_LIMIT)
  File &quot;/opt/miniconda/lib/python3.7/site-packages/azureml/_file_utils/upload.py&quot;, line 71, in upload_blob_from_stream
    raise AzureMLException._with_error(azureml_error, inner_exception=e)
UserScriptException: UserScriptException:
    Message: Encountered authorization error while uploading to blob storage. Please check the storage account attached to your workspace. Make sure that the current user is authorized to access the storage account and that the request is not blocked by a firewall, virtual network, or other security setting.
    StorageAccount: mystorage
    ContainerName: azureml
    StatusCode: 403
    InnerException AzureMLException:
    Message: Encountered authorization error while uploading to blob storage. Please check the storage account attached to your workspace. Make sure that the current user is authorized to access the storage account and that the request is not blocked by a firewall, virtual network, or other security setting.
    StorageAccount: mystorage
    ContainerName: azureml
    StatusCode: 403
    InnerException Server failed to authenticate the request. Make sure the value of Authorization header is formed correctly including the signature. ErrorCode: AuthenticationFailed
&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt;&lt;Error&gt;&lt;Code&gt;AuthenticationFailed&lt;/Code&gt;&lt;Message&gt;Server failed to authenticate the request. Make sure the value of Authorization header is formed correctly including the signature.
RequestId:5d4e1b7e-c01e-0070-0d47-9bf8a0000000
Time:2021-08-27T13:30:02.2685991Z&lt;/Message&gt;&lt;AuthenticationErrorDetail&gt;Signature did not match. String to sign used was rcw
2021-08-27T13:19:56Z
2021-08-28T13:29:56Z
/blob/mystorage/azureml/ExperimentRun/dcid.98d11a7b-2aac-4bc0-bd64-bb4d72e0e0be/outputs/models/Model.pkl

2019-07-07
b

&lt;/AuthenticationErrorDetail&gt;&lt;/Error&gt;
    ErrorResponse 
{
    &quot;error&quot;: {
        &quot;code&quot;: &quot;UserError&quot;,
        &quot;message&quot;: &quot;Encountered authorization error while uploading to blob storage. Please check the storage account attached to your workspace. Make sure that the current user is authorized to access the storage account and that the request is not blocked by a firewall, virtual network, or other security setting.\n\tStorageAccount: verovisionstorage\n\tContainerName: azureml\n\tStatusCode: 403&quot;,
        &quot;inner_error&quot;: {
            &quot;code&quot;: &quot;Auth&quot;,
            &quot;inner_error&quot;: {
                &quot;code&quot;: &quot;Authorization&quot;
            }
        }
    }
}
    ErrorResponse 
{
    &quot;error&quot;: {
        &quot;code&quot;: &quot;UserError&quot;,
        &quot;message&quot;: &quot;Encountered authorization error while uploading to blob storage. Please check the storage account attached to your workspace. Make sure that the current user is authorized to access the storage account and that the request is not blocked by a firewall, virtual network, or other security setting.\n\tStorageAccount: mystorage\n\tContainerName: azureml\n\tStatusCode: 403&quot;
    }
}
</code></pre>
<p>As far as I can tell from the code of the azureml.core.Run class and the subsequent function calls, the Run object tries to upload the file to the step run's output directory using SAS-Token-Authentication (which fails). This documentation article is linked in the code (but I don't know if this relates to the issue): <a href=""https://docs.microsoft.com/en-us/rest/api/storageservices/create-service-sas#service-sas-example"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/rest/api/storageservices/create-service-sas#service-sas-example</a></p>
<p>Did anybody encounter this error as well and knows what causes it or how it can be resolved?</p>
<p>Best,
Jonas</p>",1,1,2021-08-27 14:33:45.080000 UTC,1.0,,2,python|file-upload|azure-blob-storage|azure-machine-learning-service|sas-token,215,2020-07-16 05:39:33.727000 UTC,2022-01-24 09:55:29.407000 UTC,Germany,51,0,0,2,,,,,,['azure-machine-learning-service']
Livy logs location(in S3) for an EMR cluster(debuging Neither SparkSession nor HiveContext/SqlContext is available),"<p>I'm using AWS SageMaker connected to an EMR cluster via Livy, with a &quot;normal&quot; session(default session config) the connection is created, and spark context works fine. but when adding</p>
<pre><code>spark.pyspark.python&quot;:&quot;./ANACONDA/env_name/bin/python3&quot;,
&quot;spark.yarn.dist.archives&quot;:&quot;s3://&lt;path&gt;/env_name.tar.gz#ANACONDA&quot;
</code></pre>
<p>The session is not created and an error is thrown:</p>
<blockquote>
<p>Neither SparkSession nor HiveContext/SqlContext is available</p>
</blockquote>
<p>If I remove the spark.pyspark.python line, it takes some time(because it is distributing the .tar.gz file to executors) but it works, session and spark context arre created(but I cannot use the environment in the .tar.gz), so I guess it has something to do with <strong>spark.pyspark.python</strong></p>
<p>Given that context: I'm trying to debug what's happening and for that, I want to check the Livy logs, but I cannot find them, I know they should be in S3 <a href=""https://aws.amazon.com/premiumsupport/knowledge-center/spark-driver-logs-emr-cluster/"" rel=""nofollow noreferrer"">https://aws.amazon.com/premiumsupport/knowledge-center/spark-driver-logs-emr-cluster/</a> but I cannot find them anywhere, can anyone guide me to the logs location? or any idea on how to debug the issue?</p>",0,0,2022-06-18 07:07:52.393000 UTC,,,0,python|apache-spark|amazon-emr|amazon-sagemaker|livy,127,2016-06-15 18:26:00.217000 UTC,2022-09-20 17:52:45.723000 UTC,"Guatemala City, Guatemala Department, Guatemala",3287,99,24,435,,,,,,['amazon-sagemaker']
Clean Up Azure Machine Learning Blob Storage,"<p>I manage a frequently used Azure Machine Learning workspace. With several Experiments and active pipelines. Everything is working good so far. My problem is to get rid of old data from runs, experiments and pipelines. Over the last year the blob storage grew to enourmus size, because every pipeline data is stored.</p>
<p>I have deleted older runs from experimnents by using the gui, but the actual pipeline data on the blob store is not deleted. Is there a smart way to clean up data on the blob store from runs which have been deleted ?</p>
<p>On one of the countless Microsoft support pages, I found the following not very helpfull post:</p>
<p>*Azure does not automatically delete intermediate data written with OutputFileDatasetConfig. To avoid storage charges for large amounts of unneeded data, you should either:</p>
<ol>
<li>Programmatically delete intermediate data at the end of a pipeline
run, when it is no longer needed</li>
<li>Use blob storage with a short-term storage policy for intermediate data (see Optimize costs by automating Azure Blob Storage access tiers)</li>
<li>Regularly review and delete no-longer-needed data*</li>
</ol>
<p><a href=""https://docs.microsoft.com/en-us/azure/machine-learning/how-to-move-data-in-out-of-pipelines#delete-outputfiledatasetconfig-contents-when-no-longer-needed"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/machine-learning/how-to-move-data-in-out-of-pipelines#delete-outputfiledatasetconfig-contents-when-no-longer-needed</a></p>
<p>Any idea is welcome.</p>",1,0,2022-01-21 13:23:46.213000 UTC,1.0,,4,azure-blob-storage|azure-machine-learning-service,368,2021-10-28 13:49:28.927000 UTC,2022-08-05 09:59:32.257000 UTC,Germany,61,0,0,3,,,,,,['azure-machine-learning-service']
How to plot confidence intervals for different training samples,"<p>I am working on running training with different divisions of a training set. The plots that I get (using wandb) are fine, but not quite informative in my opinion and high in variance.
<a href=""https://i.stack.imgur.com/kux5o.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kux5o.png"" alt=""enter image description here"" /></a></p>
<p>Is there a way to plot the mean of the plots, and then confidence intervals around it? Something similar to the picture below. Alternatively, is there a way to plot variance during training?</p>
<p><a href=""https://i.stack.imgur.com/veoWc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/veoWc.png"" alt=""enter image description here"" /></a></p>",1,0,2021-09-01 05:16:50.597000 UTC,,,1,python|visualization|confidence-interval|variance|wandb,232,2019-10-11 12:36:01.193000 UTC,2022-05-13 07:52:48.017000 UTC,,61,4,0,84,,,,,,['wandb']
How to read XML file into Sagemaker Notebook Instance?,"<p>I have files in <code>XML format</code> saved in <code>S3 bucket</code> and I want to parse them with <code>xml.etree</code> inside a <code>Sagemaker's Notebook</code> Instance. I tried the following but it produces <code>FileNotFoundError</code>:</p>
<pre><code>data_location = `s3://data-bucket/1.xml`
xml.etree.ElementTree.parse(data_location)
</code></pre>",0,4,2020-06-29 09:35:15.817000 UTC,,2020-06-29 09:38:10.150000 UTC,0,python|xml|amazon-web-services|amazon-s3|amazon-sagemaker,94,2016-02-21 11:52:03.583000 UTC,2022-04-21 18:28:31.730000 UTC,"Warsaw, Poland",965,182,3,92,,,,,,['amazon-sagemaker']
Mlflow tutorial issue,"<p>I am trying <a href=""https://mlflow.org/docs/latest/quickstart.html"" rel=""nofollow noreferrer"">this quickstart tutorial</a> but I have troubles with the serving phase</p>

<p>I issued the command</p>

<pre><code>         mlflow models serve -m runs:/c94c36bb6c0c48a2a7c895d93ec1866f/model
</code></pre>

<p>I get the following</p>

<pre><code>2020/05/06 14:04:33 INFO mlflow.models.cli: Selected backend for flavor 'python_function'

2020/05/06 14:04:35 INFO mlflow.projects: === Creating conda environment mlflow 4133253245e67fa1d3bbd74212a1eb60c04fbc0a ===

Collecting package metadata (repodata.json): done

Solving environment: failed

ResolvePackageNotFound:

scikit-learn=0.22.2.post1
</code></pre>

<p>It seems conda cannot find this version of scikit-learn</p>

<p>I have installed <a href=""https://repo.anaconda.com/archive/Anaconda3-2020.02-Linux-x86_64.sh"" rel=""nofollow noreferrer"">https://repo.anaconda.com/archive/Anaconda3-2020.02-Linux-x86_64.sh</a></p>

<p>Please help me,</p>

<p>Aurel</p>",0,14,2020-05-06 12:42:38.953000 UTC,0.0,2020-05-06 12:43:36.247000 UTC,0,python|anaconda|mlflow,295,2011-04-20 12:45:41.213000 UTC,2022-09-21 09:13:14.577000 UTC,,161,0,0,27,,,,,,['mlflow']
I am trying to serve a custom function as a model using ML Flow in Databricks,"<p>Below is my source code:-</p>
<pre><code>import mlflow
import mlflow.sklearn
import numpy as np
import pandas as pd
from sklearn.datasets import load_iris
from sklearn import tree
from sklearn.preprocessing import FunctionTransformer
from sklearn.pipeline import Pipeline
from mlflow.models.signature import infer_signature

import mlflow.pyfunc


class AddN(mlflow.pyfunc.PythonModel):

    def __init__(self, n):
        self.n = n
    def fit(self, X, y=None):
        return self
    def transform(self, X, y=None):
        return self
    def predict(model_input):
        return model_input.apply(lambda column: column + self.n)


model_path = &quot;add_n_model15&quot;
add5_model = AddN(n=5)
#mlflow.pyfunc.save_model(path=model_path, python_model=add5_model)


loaded_model = mlflow.pyfunc.load_model(model_path)


import pandas as pd
#model_input = pd.DataFrame([range(10)])
#model_output = loaded_model.predict(model_input)


data={'a':1,'b':2,'c':3,'d':4}
model_input = pd.DataFrame(data, index=[0])
loaded_model.predict(model_input)




pipeline = Pipeline(steps=[(&quot;model&quot;, AddN)])

with mlflow.start_run():
    
    # log model
    signature = infer_signature(model_input, model_output)
    mlflow.sklearn.log_model(pipeline, &quot;trial&quot;, signature=signature)
</code></pre>
<p>`
After registering the model and serving it in databricks, when i call the model through model serving interface of databricks using the below input Request, i get an error:-</p>
<p>Input Request:-
[{&quot;a&quot;:20,&quot;b&quot;:21,&quot;c&quot;:22,&quot;d&quot;:23}]</p>
<p>Error:-</p>
<p>BAD_REQUEST: Encountered an unexpected error while evaluating the model. Verify that the serialized input Dataframe is compatible with the model for inference.</p>
<p>Traceback (most recent call last):
File &quot;/databricks/conda/envs/model-1/lib/python3.8/site-packages/mlflow/pyfunc/scoring_server/<strong>init</strong>.py&quot;, line 306, in transformation
raw_predictions = model.predict(data)
File &quot;/databricks/conda/envs/model-1/lib/python3.8/site-packages/mlflow/pyfunc/<strong>init</strong>.py&quot;, line 612, in predict
return self._model_impl.predict(data)
File &quot;/databricks/conda/envs/model-1/lib/python3.8/site-packages/sklearn/utils/metaestimators.py&quot;, line 120, in 
out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)
File &quot;/databricks/conda/envs/model-1/lib/python3.8/site-packages/sklearn/pipeline.py&quot;, line 419, in predict
return self.steps[-1][-1].predict(Xt, **predict_params)
TypeError: predict() missing 2 required positional arguments: 'context' and 'model_input'
`</p>",1,0,2022-03-29 23:09:04.073000 UTC,,,1,python|scikit-learn|databricks|mlflow,544,2022-03-29 22:59:11.337000 UTC,2022-08-16 04:06:57.970000 UTC,,11,0,0,0,,,,,,['mlflow']
Problem running a Docker container in Gitlab CI/CD,"<p>I am trying to build and run my Docker image using Gitlab CI/CD, but there is one issue I can't fix even though locally everything works well.</p>
<p>Here's my Dockerfile:</p>
<pre><code>FROM &lt;internal_docker_repo_image&gt;
RUN apt update &amp;&amp; \
    apt install --no-install-recommends -y build-essential gcc
COPY requirements.txt /requirements.txt

RUN pip install --no-cache-dir --user -r /requirements.txt

COPY . /src
WORKDIR /src
ENTRYPOINT [&quot;python&quot;, &quot;-m&quot;, &quot;dvc&quot;, &quot;repro&quot;]
</code></pre>
<p>This is how I run the container:</p>
<p><code>docker run --volume ${PWD}:/src --env=GOOGLE_APPLICATION_CREDENTIALS=&lt;path_to_json&gt; &lt;image_name&gt; ./dvc_configs/free/dvc.yaml --force</code></p>
<p>Everything works great when running this locally, but it fails when run on Gitlab CI/CD.</p>
<pre><code>stages:
  - build_image

build_image:
  stage: build_image
  image: &lt;internal_docker_repo_image&gt;
  script:
    - echo &quot;Building Docker image...&quot;
    - mkdir ~/.docker
    - cat $GOOGLE_CREDENTIALS &gt; ${CI_PROJECT_DIR}/key.json
    - docker build . -t &lt;image_name&gt;
    - docker run --volume ${PWD}:/src --env=GOOGLE_APPLICATION_CREDENTIALS=&lt;path_to_json&gt; &lt;image_name&gt; ./dvc_configs/free/dvc.yaml --force
  artifacts:
        paths:
          - &quot;./data/*csv&quot;
        expire_in: 1 week

</code></pre>
<p>This results in the following error:
<code>ERROR: you are not inside of a DVC repository (checked up to mount point '/src')</code></p>
<p>Just in case you don't know what DVC is, this is a tool used in machine learning for versioning your models, datasets, metrics, and, in addition, setting up your pipelines, which I use it for in my case.</p>
<p>Essentially, it requires two folders <code>.dvc</code> and <code>.git</code> in the directory from which <code>dvc repro</code> is executed.</p>
<p>In this particular case, I have no idea why it's not able to run this command given that the contents of the folders are exactly the same and both <code>.dvc</code> and <code>.git</code> exist.</p>
<p>Thanks in advance!</p>",1,0,2021-05-12 14:44:55.847000 UTC,,,4,docker|continuous-integration|gitlab-ci|dvc,746,2016-01-07 12:19:30.337000 UTC,2022-09-24 05:18:01.010000 UTC,,576,431,4,68,,,,,,['dvc']
How can I invoke AWS SageMaker endpoint to get inferences?,<p>I want to get real time predictions using my machine learning model with the help of SageMaker. I want to directly get inferences on my website. How can I use the deployed model for predictions?</p>,4,0,2018-11-21 04:57:06.677000 UTC,2.0,,8,amazon-web-services|machine-learning|data-science|amazon-sagemaker,7900,2018-06-04 07:06:10.837000 UTC,2021-01-28 14:49:41.423000 UTC,"Bengaluru, Karnataka, India",91,1,0,13,,,,,,['amazon-sagemaker']
SageMaker linear-learner results not exact?,"<p>I have issues with the results i get from the AWS (SageMaker) linear-learner. </p>

<p>Namely I was trying to replicate the results I got from R, SAS or Knime (using linear regression) but unfortunately what I get from the linear-learner is different from the mentioned 3 other ways of calculating it.</p>

<p>I tried different hyperparameters and configurations but I get inexact regression results even in the very trivial case of synthetically generated data satisfying the relationship</p>

<p>Y=X1+2*X2+3</p>

<p>In this case there are exact regression coefficients equal to 1,2 and intercept 3. Unlike the mentioned other software the SageMaker linear-learner is returning me values not even close to the right values
E.g. in one example run I get [0.91547656 1.9826275 3.023757] which is simply not satisfactory.
You can see here the relevant part of my code!</p>

<pre><code>study=((1.0,3.0,10.0),(2.0,3.0,11.0),(3.0,2.0,10.0),(4.0,7.0,21.0),(5.0,4.0,16.0))
a = np.array(study).astype('float32')
other_columns=a[:,[0,1]]
labels = a[:,2]
buf = io.BytesIO()
smac.write_numpy_to_dense_tensor(buf, other_columns, labels)
buf.seek(0)
key = 'my-training-data'
boto3.resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train', key)).upload_fileobj(buf)
s3_train_data = 's3://{}/{}/train/{}'.format(bucket, prefix, key)
output_location = 's3://{}/{}/output'.format(bucket, prefix)

container = get_image_uri(boto3.Session().region_name, 'linear-learner')

import boto3
sess = sagemaker.Session()
linear = sagemaker.estimator.Estimator(container,
                                       role, 
                                       train_instance_count=1, 
                                       train_instance_type='ml.c4.xlarge',                                       
                                       output_path=output_location,
                                       sagemaker_session=sess)
linear.set_hyperparameters(feature_dim=2,
                           predictor_type='regressor',
                           loss='squared_loss',
                           epochs=50,
                           early_stopping_patience=100,
                           mini_batch_size=4)
linear.fit({'train': s3_train_data})
</code></pre>

<p>Do you have some explanation for the observed not exact results? </p>

<p>Thanks
Nikolas</p>",1,0,2018-12-08 22:48:20.887000 UTC,,2018-12-11 06:23:47.787000 UTC,2,python|amazon-web-services|machine-learning|linear-regression|amazon-sagemaker,602,2018-12-08 22:43:01.910000 UTC,2021-05-24 22:58:26.767000 UTC,,21,0,0,17,,,,,,['amazon-sagemaker']
How to login as human labeler on GCP Vertex AI,"<p>I set up a Labeling Task in Vertex-AI, and assigned a team.
The manager of that team received an email to manage the <a href=""https://datacompute.google.com/"" rel=""nofollow noreferrer"">https://datacompute.google.com/</a> console.
None of the human labelers received such an email.
What do they have to do to start labeling? Is there a console for them?</p>
<p>Any advice would be amazing!
Thanks!</p>",0,0,2022-09-22 16:02:04.327000 UTC,,,0,google-cloud-vertex-ai,8,2022-09-22 15:59:05.683000 UTC,2022-09-22 16:39:31.053000 UTC,,1,0,0,0,,,,,,['google-cloud-vertex-ai']
Installation of Azure Machine Learning Workbench fails on Windows 10,"<p>I tried to install the azure machine learning workbench from <a href=""https://docs.microsoft.com/en-us/azure/machine-learning/preview/quickstart-installation"" rel=""nofollow noreferrer"">here</a>. Once I double click on the downloaded MSI file, it shows the first screen about licensing terms. Once I click on Continue, it shows dependencies. When I click Install, it starts installation. It downloads Miniconda with Python 3.5.2. While trying to install asn1crypto 0.23.0, it suddenly stops and displays 'Installation fails'. I tried running the MSI file with log option but no error is reported in the log.</p>

<p>Here are my machine details:
Windows 10
Version 1709 (OS Build 17017.1000)</p>

<p>How can I troubleshoot this?</p>",1,0,2017-10-24 04:54:27.387000 UTC,,2017-11-22 00:32:00.170000 UTC,1,azure|azure-machine-learning-studio|azure-machine-learning-workbench,497,2011-08-11 06:01:44.323000 UTC,2022-09-23 18:22:07.947000 UTC,,2727,45,1,227,,,,,,"['azure-machine-learning-workbench', 'azure-machine-learning-studio']"
MLflow conda_env offline environment,"<p>We want to use the <a href=""https://mlflow.org/docs/latest/projects.html#mlproject-file"" rel=""nofollow noreferrer"">MLproject <code>conda_env</code></a> feature in an offline environment, in order to reproduce the models in various computers.</p>
<p>When running on an internet connected computer we see that the <code>conda environment</code> is created by downloading the packages from anaconda.</p>
<ol>
<li>Is there an option to run <code>MLflow run .</code> with an <code>MLproject</code> file that will use an <strong>existing conda environment</strong>? (without creating a new one based on the <code>conda.yaml</code> file). Setting the <a href=""https://mlflow.org/docs/latest/projects.html#project-environments"" rel=""nofollow noreferrer"">MLFLOW_CONDA_HOME</a> environment just points to the location folder where to create the environment. But I can't find how to specify an existing environment.</li>
<li>If Is there an option to point to a <a href=""https://conda.github.io/conda-pack/"" rel=""nofollow noreferrer"">conda pack</a> file?</li>
</ol>
<p><strong>UPDATE</strong><br />
I guess this <a href=""https://github.com/mlflow/mlflow/issues/2984"" rel=""nofollow noreferrer"">github issue</a> is related</p>",1,0,2020-08-13 12:26:11.003000 UTC,,2020-08-13 13:37:44.827000 UTC,0,conda|mlflow,537,2016-01-20 20:19:09.903000 UTC,2022-09-11 19:40:57.297000 UTC,Israel,1153,113,14,168,,,,,,['mlflow']
"Vertex AI AutoML getting data about Model, Dataset, Training Job","<p>I am using Vertex AI for AutoML Video classification and I would like to get some data that I'm seeing in Web UI (Cloud Console) (Model/Dataset detail).
I'm using AI platform Python SDK or REST API.</p>
<p>For example Model API returns 'training videos' but not test videos (web Model detail, tab EVALUATE)</p>
<p><a href=""https://i.stack.imgur.com/Y7Um9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Y7Um9.png"" alt=""Vertex AI Model evaluation"" /></a></p>
<p>then for example in tab Model Properties on the web I can't obtain Training time, Total items, Algorithm, Objective, Total Items</p>
<p><a href=""https://i.stack.imgur.com/5Pi96.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5Pi96.png"" alt=""Vertex AI Model properties"" /></a></p>
<p>For Dataset detail, I would like to get number of labeled/unlabeled videos, labels and correspoding number</p>
<p><a href=""https://i.stack.imgur.com/xr8Gw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xr8Gw.png"" alt=""Dataset detail, labels"" /></a></p>
<p>This is code I'm using to get the data (as component in Vertex AI Pipeline):</p>
<pre class=""lang-py prettyprint-override""><code>def get_metadata(project_id, region, model_id):
    import requests

    import google.auth
    import google.cloud.aiplatform as aip
    from google.cloud import aiplatform_v1
    from google.protobuf import json_format
    from google.auth.transport import requests as grequests

    aip.init(project=project_id, location=region)
    API_ENDPOINT = &quot;{}-aiplatform.googleapis.com&quot;.format(region)
    model = aip.Model(model_id)
    model_dict = model.to_dict()
    model_metadata = model_dict['metadata']

    model_name = model_dict['displayName']
    model_creation_date = model_dict['createTime']
    model_type = model_metadata['modelType']
    number_training = model_metadata['trainingDataItemsCount']

    client_options = {
        &quot;api_endpoint&quot;: API_ENDPOINT
    }
    model_path = model.resource_name
    client_model = aiplatform_v1.services.model_service.ModelServiceClient(client_options=client_options)
    list_eval_request = aiplatform_v1.types.ListModelEvaluationsRequest(parent=model_path)
    list_eval = client_model.list_model_evaluations(request=list_eval_request)

    eval_name = ''
    for val in list_eval:
        eval_name = val.name
    get_eval_request = aiplatform_v1.types.GetModelEvaluationRequest(name=eval_name)
    model_eval = client_model.get_model_evaluation(request=get_eval_request)
    model_eval_data = json_format.MessageToDict(model_eval._pb)

    model_metrics = model_eval_data['metrics']
    average_precision = model_metrics.get('auPrc')
    confidence_metrics = model_metrics['confidenceMetrics']
    confidence_threshold = -1
    f1_score = -1
    precision = -1
    recall = -1

    for item in confidence_metrics:
        confidence_threshold_temp = item['confidenceThreshold']
        if confidence_threshold_temp &gt;= 0.5:
            confidence_threshold = confidence_threshold_temp
            f1_score = item['f1Score']
            precision = item['precision']
            recall = item['recall']
            break
    # auc_precision = precision
    # auc_recall = recall

    credentials, _ = google.auth.default()
    r = grequests.Request()
    credentials.refresh(r)
    training_pipeline_resource_name = model_dict['trainingPipeline']

    training_pipeline_url = f'https://{API_ENDPOINT}/v1beta1/{training_pipeline_resource_name}'
    headers = {
        'Authorization': f'Bearer {credentials.token}'
    }
    r = requests.get(training_pipeline_url, headers=headers)
    training_pipeline_detail = r.json()
    input_data_config = training_pipeline_detail.get('inputDataConfig', {})
    dataset_id = input_data_config.get('datasetId', '')
    fraction_split = input_data_config.get('fractionSplit', {})
    test_fraction = fraction_split.get('testFraction')
    training_fraction = fraction_split.get('trainingFraction')
    data_split = f'{training_fraction}/{test_fraction}'

    dataset = aip.VideoDataset(dataset_id)
    dataset_resource = json_format.MessageToDict(dataset.gca_resource._pb)
    dataset_name = dataset_resource.get('displayName')
    dataset_creation_date = dataset_resource.get('createTime')
    labels = dataset_resource['labels']
    dataset_type = labels.get('aiplatform.googleapis.com/dataset_metadata_schema')

    data = {
        'model_id': model_id,
        'model_name': model_name,
        'model_creation_date': model_creation_date,
        'model_type': model_type,
        'number_training': number_training,
        'average_precision': average_precision,
        'precision': precision,
        'recall': recall,
        'data_split': data_split,
        'dataset_name': dataset_name,
        'dataset_type': dataset_type,
        'dataset_id': dataset_id,
        'dataset_creation_date': dataset_creation_date,        
    }

</code></pre>
<p>Also for example what I found is that on training job when I created dataset, training model via WebUI I can obtain data split (training/testing ratio), but when I'm doing this in Vertex AI Pipelines, I'm not explicitly setting data split for AutoMLVideoTrainingJobRunOp, I can't get data split from Training job detail, so it seems that it saved only when it's explicitly set.</p>
<p>Other thing I noticed is when API requests are made for Cloud Console (inspecting Chrome Dev Tools) it returns more (richer) data (items) then for public Vertex AI APIs.</p>
<p>I'm not sure if this is temporary or intentional/permanent behaviour.</p>
<p>I would appreciate thoughts/comments/help with this.</p>",0,3,2021-12-21 12:29:50.567000 UTC,1.0,,0,google-cloud-automl|google-cloud-vertex-ai|google-cloud-ai|google-cloud-ai-platform-pipelines,395,2013-06-25 21:45:24.860000 UTC,2022-09-24 17:35:26.440000 UTC,"Prague, Czech Republic",276,46,0,47,,,,,,['google-cloud-vertex-ai']
Vertex AI custom container batch prediction,"<p>I have created a custom container for prediction and successfully uploaded the model to Vertex AI. I was also able to deploy the model to an endpoint and successfully request predictions from the endpoint. Within the custom container code, I use the <code>parameters</code> field as described <a href=""https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#prediction"" rel=""nofollow noreferrer"">here</a>, which I then supply later on when making an online prediction request.
My questions are regarding requesting batch predictions from a custom container for prediction.</p>
<ol>
<li><p>I cannot find any documentation that describes what happens when I request a batch prediction. Say, for example, I use the <code>my_model.batch_predict</code> function from the Python SDK and set the <code>instances_format</code> to &quot;csv&quot; and provide the <code>gcs_source</code>. Now, I have setup my custom container to expect prediction requests at <code>/predict</code> as described in this <a href=""https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements"" rel=""nofollow noreferrer"">documentation</a>. Does Vertex AI make a POST request to this path, converting the cvs data into the appropriate POST body?</p>
</li>
<li><p>How do I specify the <code>parameters</code> field for batch prediction as I did for online prediction?</p>
</li>
</ol>",1,2,2021-11-11 23:44:50.160000 UTC,,,3,google-cloud-platform|google-ai-platform|google-cloud-vertex-ai,808,2016-08-15 20:29:46.790000 UTC,2022-09-24 22:22:50.413000 UTC,,700,17,1,90,,,,,,['google-cloud-vertex-ai']
MLflow UI not loading with Nginx after authentication,"<p>I have a RHEL CENTOS7 EC2 instance with MLflow installed. The MLflow application is running on localhost port 5000 (127.0.0.1:5000). I have set up Nginx on port 443 with PAM Authentication and the proxy_redirect is to where the MLflow server is running on localhost (127.0.0.1:5000). I have included the server directive of Nginx below. After successful authentication the webpage for MLflow just hangs. Looking at the network tab I can see that some of the source files are just in the pending status.</p>
<pre><code>server {
    listen 443 ssl;
    server_name _;
    ssl_certificate /ssl/mlflow.crt;
    ssl_certificate_key /ssl/mlflow.key;

    location / {
         auth_pam &quot;Secure area&quot;;
         auth_pam_service_name &quot;mlflow&quot;;
         proxy_pass http://127.0.0.1:5000/;
    }
}
</code></pre>
<p>Any help is appreciated on why this is happening, and please let me know if any further clarification is needed.</p>",0,0,2021-12-02 20:06:46.127000 UTC,,,0,nginx|pam|mlflow,223,2021-08-28 18:35:44.600000 UTC,2021-12-03 03:59:44.843000 UTC,,1,0,0,2,,,,,,['mlflow']
Getting the Azure ML environment build status,"<p>I am trying to set up a ML pipeline on Azure ML using the Python SDK.
I have scripted the creation of a custom environment from a DockerFile as follows</p>
<pre><code>from azureml.core import Environment
from azureml.core.environment import ImageBuildDetails
from other_modules import workspace, env_name, dockerfile

custom_env : Environment = Environment.from_dockerfile(name=env_name, dockerfile=dockerfile)
                      
custom_env.register(workspace=workspace)

build : ImageBuildDetails = custom_env.build(workspace=workspace)

build.wait_for_completion()
</code></pre>
<p>However, the <code>ImageBuildDetails</code> object that the <a href=""https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.environment.environment?view=azure-ml-py#build-workspace--image-build-compute-none-"" rel=""nofollow noreferrer""><code>build</code></a> method returns invariably times out while executing the last <code>wait_for_completion()</code> line, ... likely due to network constraints that I cannot change.</p>
<p>So, how can I possibly check the build status via the SDK in a way that doesn't exclusively depend on the returned <code>ImageBuildDetails</code> object?</p>",1,0,2021-12-15 16:35:16.650000 UTC,,,0,python-3.x|azure-machine-learning-service|azureml-python-sdk|azuremlsdk,167,2015-12-21 00:13:47.220000 UTC,2022-09-23 09:09:23.043000 UTC,"Oslo, Norway",372,188,1,35,,,,,,['azure-machine-learning-service']
Load Amazon Sagemaker NTM model locally for inference,"<p>I have trained a Sagemaker <a href=""https://aws.amazon.com/blogs/machine-learning/introduction-to-the-amazon-sagemaker-neural-topic-model/"" rel=""nofollow noreferrer"">NTM</a> model which is a neural topic model, directly on the AWS sagemaker platform. Once training is complete you are able to download the <code>mxnet</code> model files. Once unpacked the files contain:</p>
<ul>
<li>params</li>
<li>symbol.json</li>
<li>meta.json</li>
</ul>
<p>I have followed the docs on mxnet to load the model and have the following code:</p>
<pre class=""lang-py prettyprint-override""><code>sym, arg_params, aux_params = mx.model.load_checkpoint('model_algo-1', 0)
module_model = mx.mod.Module(symbol=sym, label_names=None, context=mx.cpu())

module_model.bind(
    for_training=False,
    data_shapes=[('data', (1, VOCAB_SIZE))]
)

module_model.set_params(arg_params=arg_params, aux_params=aux_params, allow_missing=True) # must set allow missing true here or receive an error for a missing n_epoch var
</code></pre>
<p>I now try and use the model for inference using:</p>
<pre class=""lang-py prettyprint-override""><code>module_model.predict(x) # where x is a numpy array of size (1, VOCAB_SIZE)
</code></pre>
<p>The code runs, but the result is just a single value, where I expect a distribution over topics:</p>
<pre><code>[11.060672]
&lt;NDArray 1 @cpu(0)&gt;
</code></pre>
<p>EDIT:</p>
<p>I have tried to load it using the Symbol API, but still no luck:</p>
<pre class=""lang-py prettyprint-override""><code>import warnings
with warnings.catch_warnings():
    warnings.simplefilter('ignore')
    deserialized_net = gluon.nn.SymbolBlock.imports('model_algo-1-symbol.json', ['data'], 'model_algo-1-0000.params', ctx=mx.cpu())
</code></pre>
<p>Error:</p>
<pre><code>AssertionError: Parameter 'n_epoch' is missing in file: model_algo-1-0000.params, which contains parameters: 'logsigma_bias', 'enc_0_bias', 'projection_bias', ..., 'enc_1_weight', 'enc_0_weight', 'mean_bias', 'logsigma_weight'. Please make sure source and target networks have the same prefix.
</code></pre>
<p>Any help would be great!</p>",1,0,2020-09-29 19:25:42.533000 UTC,,2020-10-01 15:31:53.100000 UTC,1,python|amazon-web-services|nlp|amazon-sagemaker|mxnet,149,2015-05-13 15:21:55.873000 UTC,2022-09-22 08:59:46.380000 UTC,"London, United Kingdom",2763,203,35,264,,,,,,['amazon-sagemaker']
Comparators in Object2Vec,"<p>Currently, Object2Vec supports these <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/object2vec-hyperparameters.html"" rel=""nofollow noreferrer"">comparators</a>: &quot;hadamard, concat, abs_diff&quot;. I am trying to build a user-item recommendation system. I am wondering which comparator to pick for this particular use case. I would love to learn more about the effect of comparators in general.</p>",0,0,2020-07-11 15:12:41.810000 UTC,,,1,amazon-sagemaker,28,2014-05-16 20:51:29.113000 UTC,2020-07-16 00:32:42.813000 UTC,,45,2,0,5,,,,,,['amazon-sagemaker']
Azure ML Studio: How to change input value with Python before it goes through data process,"<p>I am currently attempting to change the value of input as it goes through data process in Azure ML. However, I cannot find a clue about how to access to the input data with python.</p>

<p>For example, if you were to use python, you can access to the column of data with</p>

<pre><code>print(dataframe1[""Hello World""])
</code></pre>

<p>I tried to change the name of Web Service Input and tried to do it like how I did for other dataframe (e.g. sample)</p>

<pre><code>print(dataframe[""sample""])
</code></pre>

<p>But it returns an error with no luck, and from what I read from an error, it's not compatible to dataframe:</p>

<pre><code>object of type 'NoneType' has no len()
</code></pre>

<p>I tried to look up a solution with Nonetype error, but there is no good solution.
The whole error message:</p>

<pre><code>requestId = 1f0f621f1d8841baa7862d5c05154942 errorComponent=Module. taskStatusCode=400. {""Exception"":{""ErrorId"":""FailedToEvaluateScript"",""ErrorCode"":""0085"",""ExceptionType"":""ModuleException"",""Message"":""Error 0085: The following error occurred during script evaluation, please view the output log for more information:\r\n---------- Start of error message from Python interpreter ----------\r\nCaught exception while executing function: Traceback (most recent call last):\r\n File \""C:\\server\\invokepy.py\"", line 211, in batch\r\n xdrutils.XDRUtils.DataFrameToRFile(outlist[i], outfiles[i], True)\r\n File \""C:\\server\\XDRReader\\xdrutils.py\"", line 51, in DataFrameToRFile\r\n attributes = XDRBridge.DataFrameToRObject(dataframe)\r\n File \""C:\\server\\XDRReader\\xdrbridge.py\"", line 40, in DataFrameToRObject\r\n if (len(dataframe) == 1 and type(dataframe[0]) is pd.DataFrame):\r\nTypeError: object of type 'NoneType' has no len()\r\nProcess returned with non-zero exit code 1\r\n\r\n---------- End of error message from Python interpreter ----------""}}Error: Error 0085: The following error occurred during script evaluation, please view the output log for more information:---------- Start of error message from Python interpreter ----------Caught exception while executing function: Traceback (most recent call last): File ""C:\server\invokepy.py"", line 211, in batch xdrutils.XDRUtils.DataFrameToRFile(outlist[i], outfiles[i], True) File ""C:\server\XDRReader\xdrutils.py"", line 51, in DataFrameToRFile attributes = XDRBridge.DataFrameToRObject(dataframe) File ""C:\server\XDRReader\xdrbridge.py"", line 40, in DataFrameToRObject if (len(dataframe) == 1 and type(dataframe[0]) is pd.DataFrame):TypeError: object of type 'NoneType' has no len()Process returned with non-zero exit code 1---------- End of error message from Python interpreter ---------- Process exited with error code -2
</code></pre>

<p>I have also tried to <a href=""https://i.stack.imgur.com/DWZK6.png"" rel=""nofollow noreferrer"">a way to pass python script in data</a>, but it is not able to make any change to Web Service Input value as I want it to be.</p>

<p>I have tried to look on forums like msdn or SO, but it's been difficult to find any information about it. Please let me know if you need any more information if needed. I would greatly appreciate your help!</p>",1,0,2018-08-01 22:38:50.393000 UTC,,,0,python|web-services|input|azure-machine-learning-studio,309,2017-10-17 22:57:27.960000 UTC,2019-05-07 20:40:51.740000 UTC,,25,0,0,22,,,,,,['azure-machine-learning-studio']
Azure ML studio really slow,"<p>I had been using Azure ML studio for a while now and it was really fast but now when I try to unzip folders containing images around 3000 images using</p>
<pre><code>!unzip &quot;file.zip&quot; -d &quot;to unzip directory&quot;
</code></pre>
<p>it took more than 30 minutes and other activities(longer concatenation methods) also seem to take a long time even using numpy arrays. Wondering if it is something with configuration or other problems. I have tried switching locations, creating new resource groups, workspaces, changing computes(Both CPU and GPU).</p>
<p>Compute and other set of current configurations can be seen on the image</p>
<p><a href=""https://i.stack.imgur.com/GNZao.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GNZao.png"" alt=""enter image description here"" /></a></p>",0,2,2021-04-12 11:45:31.020000 UTC,,,1,azure|azure-machine-learning-studio,185,2020-03-05 10:31:00.763000 UTC,2022-09-23 07:09:58.617000 UTC,Ethiopia,412,127,1,43,,,,,,['azure-machine-learning-studio']
"Mlflow ""invocations"" prefix","<p>We are deploying some MLflow models using docker and Kubernetes and
We are using Ingress load balancer in K8S is mandatory for security reasons, but right now we need to deploy more than one mlflow image in the same cluster.
When we run the container the model serving start the application using  &quot;/invocations&quot; path for the POST requests.
That means that we cann´t differentiate the model using the prefix, cause every container is using the same prefix.</p>
<p>My question is,is there any way to change &quot;/invocations&quot; prefix on model Mlflow images?</p>",0,0,2022-08-02 20:21:59.170000 UTC,,2022-08-03 21:46:53.487000 UTC,0,python|databricks|mlflow,44,2022-06-10 19:20:31.170000 UTC,2022-09-23 16:37:58.107000 UTC,,21,0,0,2,,,,,,['mlflow']
"Sagemaker Batch Transform Error ""Model container failed to respond to ping; Ensure /ping endpoint is implemented and responds with an HTTP 200 status""","<p>My task is to do large scale inference via Sagemaker Batch Transform.</p>
<p>I have been following the tutorial: bring your own container, <a href=""https://github.com/aws/amazon-sagemaker-examples/blob/master/advanced_functionality/scikit_bring_your_own/scikit_bring_your_own.ipynb"" rel=""nofollow noreferrer"">https://github.com/aws/amazon-sagemaker-examples/blob/master/advanced_functionality/scikit_bring_your_own/scikit_bring_your_own.ipynb</a></p>
<p>I have encountered many problems and solved them by searching stack overflow. However there is one problem that still causes the trouble.</p>
<p>When I run the same code and same dataset using 20 EC2 instances simultaneously, sometimes I get the error &quot;Model container failed to respond to ping; Please ensure /ping endpoint is implemented and responds with an HTTP 200 status&quot;, and sometimes I don't.</p>
<p>What I find most frustrating is that, I have already do nothing for /ping (see code below)</p>
<pre><code>@app.route(&quot;/ping&quot;, methods=[&quot;GET&quot;])
def ping():
    &quot;&quot;&quot;Determine if the container is working and healthy. In this sample container, we declare it healthy if we can load the model successfully.&quot;&quot;&quot;
    # health = ScoringService.get_model() is not None  # You can insert a health check here
    # status = 200 if health else 404
    
    status = 200
    return flask.Response(response=&quot;\n&quot;, status=status, mimetype=&quot;text/csv&quot;)
</code></pre>
<p>How could the error still happen?</p>
<p>I read from some posts (e.g., <a href=""https://stackoverflow.com/questions/48558057/how-can-i-add-a-health-check-to-a-sagemaker-endpoint"">How can I add a health check to a Sagemaker Endpoint?</a>
) saying that &quot;ping response should return within 2 seconds timeout&quot;.</p>
<p>How can I increase the ping response timeout? And in general, what can I do to prevent the error from happening?</p>",1,0,2021-08-17 13:17:10.517000 UTC,1.0,,1,amazon-web-services|docker|nginx|amazon-ecs|amazon-sagemaker,757,2015-04-14 11:41:26.673000 UTC,2022-09-23 18:11:03.843000 UTC,,111,5,0,8,,,,,,['amazon-sagemaker']
Deploying a Random Forest Model on Amazon Sagemaker always getting a UnexpectedStatusException with Reason: AlgorithmError,"<p>Hey I am trying to deploy my RandomForest Classifier on Amazon Sagemaker but get a StatusException Error even though the script worked fine before:</p>
<p>The script runs fine and prints out the confusion matrix and accuracy as expected. When I try to deploy the model to amazon Sagemaker using the script it does not work.</p>
<pre><code>! python script.py --n-estimators 100 \
                   --max_depth 2 \
                   --model-dir ./ \
                   --train ./ \
                   --test ./ \ 
</code></pre>
<p>Confusion Matrix:
[[13  8]
[ 1 17]]
Accuracy:
0.7692307692307693</p>
<p>I used the Estimator from Sagemaker Python SDK</p>
<pre><code>from sagemaker.sklearn.estimator import SKLearn
sklearn_estimator = SKLearn(
    entry_point='script.py',
    role = get_execution_role(),
    instance_count=1,
    instance_type='ml.m4.xlarge',
    framework_version='0.20.0',
    base_job_name='rf-scikit')
</code></pre>
<p>I launched the training job as follows</p>
<pre><code>sklearn_estimator.fit({'train':trainpath, 'test': testpath}, wait=False)
</code></pre>
<p>Here I am trying to deploy the model which leads to the StatusExceptionError that I cannot seem to fix</p>
<pre><code>sklearn_estimator.latest_training_job.wait(logs='None')
artifact = m_boto3.describe_training_job(
    TrainingJobName=sklearn_estimator.latest_training_job.name)['ModelArtifacts']['S3ModelArtifacts']

print('Model artifact persisted at ' + artifact)
</code></pre>
<pre><code>
2022-08-25 12:03:27 Starting - Starting the training job....
2022-08-25 12:03:52 Starting - Preparing the instances for training............
2022-08-25 12:04:55 Downloading - Downloading input data......
2022-08-25 12:05:31 Training - Downloading the training image.........
2022-08-25 12:06:22 Training - Training image download completed. Training in progress..
2022-08-25 12:06:32 Uploading - Uploading generated training model.
2022-08-25 12:06:43 Failed - Training job failed
---------------------------------------------------------------------------
UnexpectedStatusException                 Traceback (most recent call last)
&lt;ipython-input-37-628f942a78d3&gt; in &lt;module&gt;
----&gt; 1 sklearn_estimator.latest_training_job.wait(logs='None')
      2 artifact = m_boto3.describe_training_job(
      3     TrainingJobName=sklearn_estimator.latest_training_job.name)['ModelArtifacts']['S3ModelArtifacts']
      4 
      5 print('Model artifact persisted at ' + artifact)

~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/estimator.py in wait(self, logs)
   2109             self.sagemaker_session.logs_for_job(self.job_name, wait=True, log_type=logs)
   2110         else:
-&gt; 2111             self.sagemaker_session.wait_for_job(self.job_name)
   2112 
   2113     def describe(self):

~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/session.py in wait_for_job(self, job, poll)
   3226             lambda last_desc: _train_done(self.sagemaker_client, job, last_desc), None, poll
   3227         )
-&gt; 3228         self._check_job_status(job, desc, &quot;TrainingJobStatus&quot;)
   3229         return desc
   3230 

~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/session.py in _check_job_status(self, job, desc, status_key_name)
   3390                 message=message,
   3391                 allowed_statuses=[&quot;Completed&quot;, &quot;Stopped&quot;],
-&gt; 3392                 actual_status=status,
   3393             )
   3394 

UnexpectedStatusException: Error for Training job rf-scikit-2022-08-25-12-03-25-931: Failed. Reason: AlgorithmError: framework error: 
Traceback (most recent call last):
  File &quot;/miniconda3/lib/python3.7/site-packages/sagemaker_containers/_trainer.py&quot;, line 84, in train
    entrypoint()
  File &quot;/miniconda3/lib/python3.7/site-packages/sagemaker_sklearn_container/training.py&quot;, line 39, in main
    train(environment.Environment())
  File &quot;/miniconda3/lib/python3.7/site-packages/sagemaker_sklearn_container/training.py&quot;, line 35, in train
    runner_type=runner.ProcessRunnerType)
  File &quot;/miniconda3/lib/python3.7/site-packages/sagemaker_training/entry_point.py&quot;, line 100, in run
    wait, capture_error
  File &quot;/miniconda3/lib/python3.7/site-packages/sagemaker_training/process.py&quot;, line 291, in run
    cwd=environment.code_dir,
  File &quot;/miniconda3/lib/python3.7/site-packages/sagemaker_training/process.py&quot;, line 208, in check_error
    info=extra_info,
sagemaker_training.errors.ExecuteUserScriptError: ExecuteUserScriptError:
ExitCode 1
ErrorMessage &quot;&quot;
Command &quot;/miniconda3/bin/python script.py&quot;

ExecuteUserScriptErr
</code></pre>
<p>I am happy for some help</p>",1,0,2022-08-25 12:23:07.797000 UTC,,,0,python|machine-learning|scikit-learn|random-forest|amazon-sagemaker,44,2021-11-26 08:25:52.293000 UTC,2022-09-22 11:50:10.577000 UTC,,33,0,0,4,,,,,,['amazon-sagemaker']
How do I enable_logging for ModelMonitoringAlertConfig in GCP?,"<p>I am trying to <code>enable_logging</code> in <code>ModelMonitoringAlertConfig</code> I have tried:</p>
<pre><code>from google.cloud import aiplatform_v1 as vertex_ai_beta
...
    alerting_config = vertex_ai_beta.ModelMonitoringAlertConfig(
    enable_logging=True,
    email_alert_config=vertex_ai_beta.ModelMonitoringAlertConfig.EmailAlertConfig(
        user_emails=NOTIFY_EMAILS
    )
)
</code></pre>
<p>gives:</p>
<pre><code> Unknown field for ModelMonitoringAlertConfig: enable_logging
</code></pre>
<p>but <a href=""https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform_v1.types.ModelMonitoringAlertConfig"" rel=""nofollow noreferrer"">this</a> suggests it should work. What am I missing?</p>
<p>(I have also tried <code>aiplatform_v1beta1</code>.)</p>",1,0,2022-08-23 11:38:48.657000 UTC,,,1,google-cloud-platform|google-cloud-vertex-ai,19,2012-10-25 08:48:34.717000 UTC,2022-09-23 10:10:32.783000 UTC,,2564,304,8,451,,,,,,['google-cloud-vertex-ai']
aws sagemker to aws s3 file saving without first saving localy PYTHON?,"<p>I am trying to save processed files to amazon s3 but first i have to save them locally. The size of processed files is quiet big that it won't even allow me to save it locally and gives error, &quot;No space left on device&quot;
Is there anyway I can save the processed directly on s3 cloud.<a href=""https://datapilotcom-my.sharepoint.com/:i:/g/personal/syed_hamza_data-pilot_com/EfpmRbvYJv9GhciACy_mk48Bdq93JESo1w92lH9164EojA?e=fLseyu"" rel=""nofollow noreferrer"">google drive link to error image</a></p>",0,1,2021-11-05 06:42:45.753000 UTC,,,0,python|amazon-web-services|amazon-s3|amazon-sagemaker,30,2021-11-05 05:58:44.933000 UTC,2022-06-23 05:41:07.563000 UTC,"Lahore, Pakistan",11,0,0,1,,,,,,['amazon-sagemaker']
boto3 how to get the logstream form a sagemaker transform job?,"<p>i am able to crete the job and it fail, using boto3</p>
<pre><code>import boto3
session = boto3.session.Session()


client = session.client('sagemaker')
descibe = client.describe_transform_job(TransformJobName=&quot;my_transform_job_name&quot;)
</code></pre>
<p>in the ui i can see the button to go to the logs, i can use boto3 to retrive the logs if hardcode the group name and the log-stream.</p>
<p>but how can i get the Log stream from the batch transfrom job?  shouldnt be a field with logstream or something like that in the &quot;.describe_transform_job&quot;?</p>",1,0,2021-04-09 22:02:00.010000 UTC,,,1,python|amazon-web-services|boto3|amazon-sagemaker,167,2012-09-27 18:26:03.723000 UTC,2022-09-22 02:14:04.407000 UTC,austin,1584,139,1,192,,,,,,['amazon-sagemaker']
How do I get embeddings from Huggingface(in sagemaker) instead of features?,"<p>I have a text classifier model that depends on embeddings from a certain huggingface model</p>
<pre class=""lang-py prettyprint-override""><code>from sentence_transformers import SentenceTransformer
model = SentenceTransformer('T-Systems-onsite/cross-en-de-roberta-sentence-transformer')
encodings = model.encode(&quot;guckst du bundesliga&quot;)
</code></pre>
<p>this has a shape of (768,)</p>
<p>tldr: is there a clean simple way to do this on sagemaker (hopefully using the images it provides) ?</p>
<p>context:
looking at docs of this <a href=""https://huggingface.co/T-Systems-onsite/cross-en-de-roberta-sentence-transformer?text=guckst%20du%20bundesliga"" rel=""nofollow noreferrer"">huggingface model</a> the only sagemaker option I see is feature extraction</p>
<pre class=""lang-py prettyprint-override""><code>from sagemaker.huggingface import HuggingFaceModel
import sagemaker

role = sagemaker.get_execution_role()
# Hub Model configuration. https://huggingface.co/models
hub = {
    'HF_MODEL_ID':'T-Systems-onsite/cross-en-de-roberta-sentence-transformer',
    'HF_TASK':'feature-extraction'
}

# create Hugging Face Model Class
huggingface_model = HuggingFaceModel(
    transformers_version='4.6.1',
    pytorch_version='1.7.1',
    py_version='py36',
    env=hub,
    role=role, 
)

# deploy model to SageMaker Inference
predictor = huggingface_model.deploy(
    initial_instance_count=1, # number of instances
    instance_type='ml.m5.xlarge' # ec2 instance type
)

predictor.predict({
    'inputs': &quot;Today is a sunny day and I'll get some ice cream.&quot;
})
</code></pre>
<p>this gives my the features which has a shape (9, 768)</p>
<p>there is a connection between these two values, which is seen from a another code sample</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoTokenizer, AutoModel
import torch


#Mean Pooling - Take attention mask into account for correct averaging
def embeddings(feature_envelope, attention_mask):
    features = feature_envelope[0] #First element of model_output contains all token embeddings
    input_mask_expanded = attention_mask.unsqueeze(-1).expand(features.size()).float()
    sum_embeddings = torch.sum(features * input_mask_expanded, 1)
    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)
    return sum_embeddings / sum_mask

#Sentences we want sentence embeddings for
sentences = ['guckst du bundesliga']

#Load AutoModel from huggingface model repository
tokenizer = AutoTokenizer.from_pretrained('T-Systems-onsite/cross-en-de-roberta-sentence-transformer')
model = AutoModel.from_pretrained('T-Systems-onsite/cross-en-de-roberta-sentence-transformer')

#Tokenize sentences
encoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=128, return_tensors='pt')

#Compute token embeddings
with torch.no_grad():
    model_output = model(**encoded_input)
#     print(model_output)

#Perform pooling. In this case, mean pooling
sentence_embeddings = embeddings(model_output, encoded_input['attention_mask'])
sentence_embeddings.shape, sentence_embeddings
</code></pre>
<p>But as you can see cant derive embedding given only features</p>",1,0,2022-02-18 19:32:28.743000 UTC,,,0,amazon-sagemaker|huggingface-transformers|sentence-transformers,415,2016-11-21 18:54:42.240000 UTC,2022-09-13 10:44:42.097000 UTC,,349,23,1,65,,,,,,['amazon-sagemaker']
How to get run id from run name in MLflow,"<p>To download artifacts from a run, you need run id. I get the run id from the UI as shown below.</p>
<p><a href=""https://i.stack.imgur.com/FJtAe.png"" rel=""nofollow noreferrer"">Run id from the UI</a></p>
<p>But when I set the run name parameter, run id is not visible in the UI. How to find the run Id of a particular run in MLflow ?</p>",1,2,2021-06-14 10:39:08.760000 UTC,,,2,mlflow,1492,2020-04-03 13:16:30.787000 UTC,2022-01-06 13:26:49.990000 UTC,,29,0,0,1,,,,,,['mlflow']
Amazon SageMaker Ground Truth Custom Labeling Jobs Error: Cannot read property 'taskInput' of null,"<p>When creating a custom labeling job for Amazon SageMaker Ground Truth Custom 
Am getting the following error:</p>

<p><code>Cannot read property 'taskInput' of null</code></p>",1,0,2019-12-29 02:07:12.053000 UTC,,,1,label|amazon-sagemaker,608,2013-08-22 03:12:19.087000 UTC,2022-09-22 05:41:01.377000 UTC,"Tokyo, Japan",6358,4611,1,229,,,,,,['amazon-sagemaker']
"Connecting to Azure SQL database from ""Execute R Script"" module in ""Azure Machine Learning Studio""","<p>I have already set up an <strong>Azure SQL Database</strong> and loaded results into it form my local machine via <strong>R (RODBC)</strong> successfully.  I can do queries in R Studio with no problem. </p>

<p>However when I use the same code in <strong>Execute R script</strong> module in the  ML studio, I get an error that the connection is not open. </p>

<p>What do I need to change? Have tried different strings for the driver with no avail.   </p>

<p><em>The reason Reader or Import Data module is not working for my case is that I am creating an API that provides me with the information to query the database before doing analytics. The database is very big and I do not want to load whole table and then use project columns, etc.</em></p>

<p>Any help is really appreciated</p>

<p>Thanks all</p>",1,1,2016-05-19 17:16:35.617000 UTC,1.0,,2,sql|r|azure|azure-sql-database|azure-machine-learning-studio,1136,2012-03-23 14:45:26.593000 UTC,2019-08-06 17:21:07.603000 UTC,"Boston, MA, USA",45,0,0,15,,,,,,['azure-machine-learning-studio']
AttributeError: module 'tensorflow_core.python.keras.api._v2.keras.activations' has no attribute 'swish',"<p>I'm using Azure ML studio to train a question answering ALBERT model with the SQuAD dataset. I'm getting the following error. Here is the code I execute.</p>
<pre class=""lang-py prettyprint-override""><code># Clone transformers github repo
!git clone https://github.com/huggingface/transformers \
&amp;&amp; cd transformers \
&amp;&amp; git checkout a3085020ed0d81d4903c50967687192e3101e770 

# Install libraries
# !pip install ./transformers
!pip install transformers
!pip install tensorboardX

# Get data
! mkdir dataset \
&amp;&amp; cd dataset \
&amp;&amp; wget https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json \
&amp;&amp; wget https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json

# Train model
!export SQUAD_DIR=/content/dataset \
&amp;&amp; python transformers/examples/run_squad.py \
    --model_type albert \
    --model_name_or_path albert-base-v2 \
    --do_train \
    --do_eval \
    --do_lower_case \
    --train_file $SQUAD_DIR/train-v2.0.json \
    --predict_file $SQUAD_DIR/dev-v2.0.json \
    --per_gpu_train_batch_size 12 \
    --learning_rate 3e-5 \
    --num_train_epochs 1.0 \
    --max_seq_length 384 \
    --doc_stride 128 \
    --output_dir /content/model_output \
    --save_steps 1000 \
    --threads 4 \
    --version_2_with_negative 
</code></pre>
<p>I'm using an NVIDIA Tesla K80 GPU. When I execute the cell above to train the model, I get the following error:</p>
<pre class=""lang-py prettyprint-override""><code>2020-10-31 01:31:45.732913: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory
2020-10-31 01:31:45.733023: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory
2020-10-31 01:31:45.733043: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
Traceback (most recent call last):
    File &quot;transformers/examples/run_squad.py&quot;, line 32, in &lt;module&gt;
    from transformers import (
    File &quot;/anaconda/envs/azureml_py36/lib/python3.6/site-packages/transformers/__init__.py&quot;, line 135, in &lt;module&gt;
    from .pipelines import (
    File &quot;/anaconda/envs/azureml_py36/lib/python3.6/site-packages/transformers/pipelines.py&quot;, line 47, in &lt;module&gt;
    from .modeling_tf_auto import (
    File &quot;/anaconda/envs/azureml_py36/lib/python3.6/site-packages/transformers/modeling_tf_auto.py&quot;, line 45, in &lt;module&gt;
    from .modeling_tf_albert import (
    File &quot;/anaconda/envs/azureml_py36/lib/python3.6/site-packages/transformers/modeling_tf_albert.py&quot;, line 24, in &lt;module&gt;
    from .activations_tf import get_tf_activation
    File &quot;/anaconda/envs/azureml_py36/lib/python3.6/site-packages/transformers/activations_tf.py&quot;, line 53, in &lt;module&gt;
    &quot;swish&quot;: tf.keras.activations.swish,
AttributeError: module 'tensorflow_core.python.keras.api._v2.keras.activations' has no attribute 'swish'
</code></pre>",1,0,2020-10-31 01:40:23.140000 UTC,1.0,2020-11-01 10:41:29.517000 UTC,4,python|azure|jupyter-notebook|azure-machine-learning-studio|azure-machine-learning-service,9925,2020-10-10 22:20:44.450000 UTC,2020-11-19 01:42:31.690000 UTC,,153,0,0,5,,,,,,"['azure-machine-learning-service', 'azure-machine-learning-studio']"
Outputting multiple csv files from a SageMaker batch prediction job,"<p>I am working on a AWS SageMaker (SKlearn) batch transform job, in which the prediction data is big and therefore I'm required to use mini-batches (where the input .csv is split up into smaller .csv files).</p>

<p>I have this working and outputting a .csv file with the ids and the predictions. However I am attempting to implement a way in which I can have a total of three output files from the batch transform job - which are different .csv files each aggregated in a slightly different way.</p>

<p>My issue is I am not sure how to instruct SageMaker to output multiple files. I have tried the following code as the prediction method submitted in the <code>entry_point</code> file:</p>

<pre><code>def output_fn(prediction, accept):
    output_one = prepare_one(prediction)
    output_two, output_three = prepare_others(output_one)
    return output_one, output_two, output_three
</code></pre>

<p>Couple ideas/issues I am currently working with:</p>

<ul>
<li>I think the batching strategy will cause issue. As the additional outputs are aggregations on the total predictions but SageMaker will treat each mini-batch separately (I assume?) </li>
<li>Can I simply use <code>boto3</code> and save extra files using that and treat the SageMaker output as only <code>output_one</code></li>
</ul>

<p>Any help would be much appreciated</p>",1,0,2019-08-29 15:34:15.690000 UTC,,,0,python|amazon-web-services|csv|amazon-sagemaker,1650,2015-09-22 14:32:49.407000 UTC,2022-09-04 14:53:27.090000 UTC,Belfast,1682,165,4,164,,,,,,['amazon-sagemaker']
Vertex AI Executor gives NoSuchKernel,"<p>I have a simple hello-world ipynb file in a Vertex AI notebook instance that looks like this:</p>
<pre><code>print(&quot;hello world&quot;)
</code></pre>
<p>When setting up an executor for this as shown below I receive the following error in the executor logs: <em>jupyter_client.kernelspec.NoSuchKernel: No such kernel named local-python3
error</em>
<a href=""https://i.stack.imgur.com/NcuKk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NcuKk.png"" alt=""enter image description here"" /></a></p>
<p>The notebook has the following metadata</p>
<pre><code>{
    &quot;kernelspec&quot;: {
        &quot;display_name&quot;: &quot;Python 3 (Local)&quot;,
        &quot;language&quot;: &quot;python&quot;,
        &quot;name&quot;: &quot;local-python3&quot;
    },
    &quot;language_info&quot;: {
        &quot;codemirror_mode&quot;: {
            &quot;name&quot;: &quot;ipython&quot;,
            &quot;version&quot;: 3
        },
        &quot;file_extension&quot;: &quot;.py&quot;,
        &quot;mimetype&quot;: &quot;text/x-python&quot;,
        &quot;name&quot;: &quot;python&quot;,
        &quot;nbconvert_exporter&quot;: &quot;python&quot;,
        &quot;pygments_lexer&quot;: &quot;ipython3&quot;,
        &quot;version&quot;: &quot;3.7.12&quot;
    }
}
</code></pre>
<p>What would require to run this notebook successfully? I looked into the possibility of customer containers but that should be to much of a complex solution for such.</p>",0,2,2022-08-02 15:51:43.420000 UTC,,,0,google-cloud-platform|google-cloud-vertex-ai,47,2020-04-21 08:20:19.480000 UTC,2022-09-21 12:47:49.243000 UTC,,25,0,0,3,,,,,,['google-cloud-vertex-ai']
Azure-ML-R SDK in R Studio ScriptRunConfig not recognized function error after a deprecated estimator replacement,"<p>I am trying to use Azure-ML-SDK in R Studio and used Estimator but got error stating estimator deprecated and advised to use ScriptRunConfig and when used it, it not being recognized as a function and fails to run. See the errors below. Please advise.</p>
<p>Already loaded library(azuremlsdk) which should include azureml.core to recognize the ScriptRunConfig function. Is it version compatibility issue? if so, which version should i use for ScriptRunConfig and how to load specific R version in Azure ML Compute (R Studio web interface and not R Studio Desktop)</p>
<p>First Error and code</p>
<pre><code>est &lt;- estimator(source_directory = &quot;train-and-deploy-first-model&quot;,
                 entry_script = &quot;accidents.R&quot;,
                 script_params = list(&quot;--data_folder&quot; = ds$path(target_path)),
                 compute_target = compute_target
                 )
</code></pre>
<p>cran_packages, github_packages, custom_url_packages, custom_docker_image, image_registry_details, use_gpu, environment_variables, and shm_size parameters will be deprecated. Please create an environment object with them using r_environment() and pass the environment object to the estimator().'enabled' is deprecated. Please use the azureml.core.runconfig.DockerConfiguration object with the 'use_docker' param instead.
'Estimator' is deprecated. Please use 'ScriptRunConfig' from 'azureml.core.script_run_config' with your own defined environment or an Azure ML curated environment.</p>
<p>Second Code snippet trying to fix above and it's error</p>
<pre><code>config &lt;- ScriptRunConfig(source_directory = &quot;.&quot;,
                 script = &quot;accidents.R&quot;,
                 compute_target = compute_target
                 )
</code></pre>
<p>Error in ScriptRunConfig(source_directory = &quot;.&quot;, script = &quot;accidents.R&quot;,  :
could not find function &quot;ScriptRunConfig&quot;</p>",0,3,2021-04-27 16:30:27.510000 UTC,,,1,r|azure-machine-learning-service,159,2014-04-15 12:18:57.880000 UTC,2021-05-11 16:14:42.530000 UTC,,41,0,0,3,,,,,,['azure-machine-learning-service']
Mounting a FileDatasets in Azure ML Services,"<p>I am facing a problem with the Dataset module in Azure Machine Learning Services. I created a FileDataset with a bunch of images to train a model in TensorFlow. I’m mounting the dataset in the target compute and then passing the mounting point to the train script as described <a href=""https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/work-with-data/datasets/datasets-tutorial/train-with-datasets.ipynb"" rel=""nofollow noreferrer"">in the sample notebook we have on GitHub</a>. </p>

<p>I tried two approaches: to pass the path as an script parameter (as suggested on GitHub) and as a named input, but none of them seem to pass the mounting point correctly.  Anyone does know which is the correct way to make it work? (I can make it work with Data Sources btw)</p>

<p>As script parameter</p>

<pre><code>script_params = {
    '--data-folder': dset.as_named_input('dogscats_train').as_mount('tmp/dataset'),
} 

src = TensorFlow(source_directory =  r'Tensorflow',
              framework_version = '1.13',
              entry_script = 'train.py',
              script_params=script_params,
              compute_target='amlcompute', 
              vm_size='Standard_NC6', 
              use_gpu = True, 
              pip_packages = ['matplotlib', 'pillow', 'numpy', 'azureml-sdk'])
</code></pre>

<p><strong>Mounted path:</strong></p>

<p>/mnt/batch/tasks/shared/LS_root/jobs/aa-ml-aml-workspace/azureml/cats-vs-dogs-tensorflow_1570799752_014bea9f/mounts/workspaceblobstore/azureml/cats-vs-dogs-tensorflow_1570799752_014bea9f/tmp/dataset</p>

<p><strong>Actual path received in the script:</strong></p>

<p>/tmp/dataset</p>

<p>As named input</p>

<pre><code>src = TensorFlow(source_directory =  r'Tensorflow',
              framework_version = '1.13',
              entry_script = 'train.py',
              inputs=[dset.as_named_input('dogscats_train')],
              compute_target='amlcompute', 
              vm_size='Standard_NC6', 
              use_gpu = True, 
              pip_packages = ['matplotlib', 'pillow', 'numpy', 'azureml-sdk'])
</code></pre>

<p><strong>Mounted Path:</strong></p>

<p>/mnt/batch/tasks/shared/LS_root/jobs/aa-ml-aml-workspace/azureml/cats-vs-dogs-tensorflow_1570804147_39168dcf/mounts/workspaceblobstore</p>

<p><strong>Path retrieved by run.input_datasets['dogscats_train'].mount('tmp/dataset').mount_point:</strong></p>

<p>/mnt/batch/tasks/shared/LS_root/jobs/aa-ml-aml-workspace/azureml/cats-vs-dogs-tensorflow_1570804147_39168dcf/mounts/workspaceblobstore/azureml/cats-vs-dogs-tensorflow_1570804147_39168dcf/tmp/dataset</p>",1,0,2019-10-11 15:16:18.500000 UTC,,2019-10-11 15:59:18.813000 UTC,2,azure|tensorflow|azure-machine-learning-service,2898,2019-10-11 15:06:49.227000 UTC,2022-09-20 16:33:19.193000 UTC,Argentina,41,3,0,6,,,,,,['azure-machine-learning-service']
How to save models in MLFlow with R and get Stages of them in Azure Databricks?,"<p>I would like to save a model in MLFlow with Azure Databricks. In Python, I can use the following code to save a model with a name automatically:</p>
<pre class=""lang-py prettyprint-override""><code>mlflow.spark.log_model(
        model,
        artifact_path = 'model_prueba',
        registered_model_name = 'model_prueba'
    )
</code></pre>
<p><a href=""https://i.stack.imgur.com/tTaNw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tTaNw.png"" alt=""Registered models window"" /></a></p>
<p>But I am trying to do the same with <strong>R</strong> with the following code:</p>
<pre class=""lang-r prettyprint-override""><code>mlflow_log_model(
          model,
          artifact_path = 'model_prueba_R',
          registered_model_name = 'model_prueba_R'
    )
</code></pre>
<p>But it does not register any model in the Models section. It only saves the model with the artifact path in the run section.</p>
<p><a href=""https://i.stack.imgur.com/zfAza.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zfAza.png"" alt=""Artifact location"" /></a></p>
<p>Anyone could tell me the way to save the model for staging automatically with code in R?</p>
<p>Thank you very much!</p>",0,0,2020-10-21 11:51:32.693000 UTC,,,1,python|r|model|azure-databricks|mlflow,201,2019-09-09 07:55:57.070000 UTC,2022-01-17 12:39:33.287000 UTC,,61,1,0,13,,,,,,['mlflow']
"status ""Preparing"" for more than 2 hours for 350MB file","<p>I have submitted autoML run on remote compute (Standard_D12_v2 - 4 node cluster 28GB, 4 cores each)</p>
<p>My input file is roughly 350 MB.</p>
<p>the status is &quot;Preparing&quot; for more than 2 hours. And then it fails.</p>
<pre><code>User error: Run timed out. No model completed training in the specified time. Possible solutions: 
1) Please check if there are enough compute resources to run the experiment. 
2) Increase experiment timeout when creating a run. 
3) Subsample your dataset to decrease featurization/training time. 
</code></pre>
<p>below is my python-Notebook code, please help.</p>
<pre><code>import azureml.core
from azureml.core.experiment import Experiment
from azureml.core.workspace import Workspace
from azureml.core.dataset import Dataset
from azureml.core.compute import ComputeTarget
from azureml.train.automl import AutoMLConfig



ws = Workspace.from_config()
experiment=Experiment(ws, 'nyc-taxi')




cpu_cluster_name = &quot;low-cluster&quot;
compute_target = ComputeTarget(workspace=ws, name=cpu_cluster_name)


data = &quot;https://betaml4543906917.blob.core.windows.net/betadata/2015_08.csv&quot;
dataset = Dataset.Tabular.from_delimited_files(data)
training_data, validation_data = dataset.random_split(percentage=0.8, seed=223)
label_column_name = 'totalAmount'



automl_settings = {
    &quot;n_cross_validations&quot;: 3,
    &quot;primary_metric&quot;: 'normalized_root_mean_squared_error',
    &quot;enable_early_stopping&quot;: True,
    &quot;max_concurrent_iterations&quot;: 2, # This is a limit for testing purpose, please increase it as per cluster size
    &quot;experiment_timeout_hours&quot;: 2, # This is a time limit for testing purposes, remove it for real use cases, this will drastically limit ablity to find the best model possible
    &quot;verbosity&quot;: logging.INFO,
}

automl_config = AutoMLConfig(task = 'regression',
                             debug_log = 'automl_errors.log',
                             compute_target = compute_target,
                             training_data = training_data,
                             label_column_name = label_column_name,
                             **automl_settings
                            )




remote_run = experiment.submit(automl_config, show_output = False)
</code></pre>",0,4,2020-08-20 12:47:06.660000 UTC,,,0,automl|azure-machine-learning-service,241,2013-10-29 19:37:02.167000 UTC,2022-09-18 10:20:06.600000 UTC,Zurich,1330,31,0,57,,,,,,['azure-machine-learning-service']
"Proper way to make a request to a model, deployed via Azure ML Designer","<p>I am trying to make the POST request to the Azure ML Designer endpoint (model, I have deployed).
Here is my code:</p>
<pre><code>import requests

scoring_uri = 'http:some-url/score'
key = 'someKey'

headers = {'Content-Type': 'application/json'}
headers['Authorization'] = f'Bearer {key}'

response = requests.get('https://www.okino.ua/media/var/news/2019/12/04/Quentin_Tarantino.jpg')

input_data = &quot;{\&quot;data\&quot;: [&quot; + str(response.content) + &quot;]}&quot;
resp = requests.post(scoring_uri, data=response.content, headers=headers)
print(resp.text)
</code></pre>
<p>And I receive and error:</p>
<pre><code>{&quot;error&quot;: {&quot;code&quot;: 400, &quot;message&quot;: &quot;Input Data Error. Input data are inconsistent with schema.\nSchema: {'WebServiceInput0': {'columnAttributes': [{'name': 'image', 'type': 'Bytes', 'isFeature': True, 'elementType': {'typeName': 'bytes', 'isNullable': False}, 'properties': {'mime_type': 'image/png', 'image_ref': 'image_info'}}, {'name': 'id', 'type': 'Numeri\nData: b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x01,\\x01,\\x00\\x00\\xff\\xfe\\x00[Copyright Shutterstock 2019;82139424;3600;2400;1563865756;Tue, 23 Jul 2019 07:09:16 GMT;0\\xff\\xed\\x04\\x16Photoshop 3.0\\x008BIM\\x04\\x04\\x00\\x00\\x00\\x00\\x03\\xf9\\x1c\\x02\\x05\\x00\\n103\nTraceback (most recent call last):\n  File \&quot;/azureml-envs/azureml_c1330288c44b762b0282b6f129c5292f/lib/python3.6/site-packages/azureml/designer/serving/dagengine/processor.py\&quot;, line 18, in run\n    webservice_input, global_parameters = self.pre_process(raw_data)\n  File \&quot;/azureml-envs/azureml_c1330288c44b762b0282b6f129c5292f/lib/python3.6/site-packages/azureml/designer/serving/dagengine/processor.py\&quot;, line 45, in pre_process\n    json_data = json.loads(raw_data)\n  File \&quot;/azureml-envs/azureml_c1330288c44b762b0282b6f129c5292f/lib/python3.6/json/__init__.py\&quot;, line 349, in loads\n    s = s.decode(detect_encoding(s), 'surrogatepass')\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n&quot;, &quot;details&quot;: &quot;&quot;}}

</code></pre>
<p>Is anyone aware of how I should pass image data to the exposed by Azure ML endpoint?</p>",1,1,2020-11-27 22:56:08.417000 UTC,,2020-12-01 01:17:47.360000 UTC,1,python|azure|computer-vision|classification|azure-machine-learning-service,264,2019-08-05 20:53:55.857000 UTC,2020-11-29 08:50:37.553000 UTC,,23,0,0,6,,,,,,['azure-machine-learning-service']
Troubles with BlazingText jsonlines Batch Transform,"<p>I have a jsonlines file that looks like this:</p>

<pre><code>{""id"":123,""source"":""this is a text string""}
{""id"":456,""source"":""this is another text string""}
{""id"":789,""source"":""yet another string""}
</code></pre>

<p>When I run a BlazingText Batch Transform job on a file that just contains the source, it works. When trying to join the inputs and outputs, I get <code>Customer Error: Unable to decode payload: Incorrect data format. (caused by AttributeError)</code>.</p>

<p>Any suggestions?</p>

<p>Code:</p>

<pre><code>bt_transformer = bt_model.transformer(
    instance_count = 1,
    instance_type = ""ml.m4.xlarge"",
    assemble_with = ""Line"",
    output_path = s3_batch_out_data,
    accept = ""application/jsonlines""
)

bt_transformer.transform(
    s3_batch_in_data, 
    content_type = ""application/jsonlines"",
    split_type = ""Line"", 
    input_filter = ""$.source"", 
    join_source = ""Input"", 
    output_filter = ""$['id', 'SageMakerOutput']""
)

bt_transformer.wait()
</code></pre>",1,0,2019-09-30 10:57:58.837000 UTC,0.0,,3,amazon-sagemaker,611,2016-02-11 12:05:26.587000 UTC,2021-03-26 22:39:49.673000 UTC,,81,13,0,15,,,,,,['amazon-sagemaker']
Training Model for Each Individual in AzureML,"<p>I want to train an ANN model for each individual, in azure ml. For example, there is an application which wants to learn the behavior of each individual separately. How is this possible in azure-ml? Any suggestion?</p>

<p>As I know, I can create a model and train it with some data, but I don't know how can I train it specifically for each user. I should mention that I am seeking for a scalable idea which is applicable for a real situation (might be for 100 thousands users).</p>",1,4,2017-01-05 12:56:29.100000 UTC,1.0,2017-01-26 14:02:20.663000 UTC,2,multi-tenant|azure-machine-learning-studio,205,2014-06-23 20:02:17.940000 UTC,2022-09-24 13:03:15.667000 UTC,Belgium,17835,636,1612,2203,,,,,,['azure-machine-learning-studio']
AWS SageMaker Pipelines not being triggered by EventBridge,"<p>I've created a new SageMaker pipeline using AWS Python SDK, and everything is working fine, I can trigger my pipeline and it works perfectly using the SDK with these simples commands:</p>
<pre><code>pipeline.upsert(role_arn=get_execution_role())
execution = pipeline.start()
</code></pre>
<p><a href=""https://i.stack.imgur.com/KlP9W.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/KlP9W.png"" alt=""enter image description here"" /></a></p>
<p>Now, I would like to schedule the pipeline execution to run every day during the morning (let's say 8 a.m for example). And here's my problem. I configured the EventBridge as shown in this tutorial: <a href=""https://github.com/aws-samples/scheduling-sagemaker-processing-with-sagemaker-pipelines"" rel=""nofollow noreferrer"">https://github.com/aws-samples/scheduling-sagemaker-processing-with-sagemaker-pipelines</a>, but instead of creating a new role, I used an existing one (the same returned from the command get_execution_role() above). My event is triggered in the correct hour (every day at 8 am), but the pipeline doesn't execute. When checking the logs on Cloud Watch, It shows that I got a FailedInvocations for the event, but I don't know how to get the logs from this failed execution. I tried to search on cloud trail but don't found nothing.</p>
<p>Anyone could help me?</p>",1,1,2022-04-11 15:27:24.973000 UTC,,,2,amazon-web-services|amazon-sagemaker|aws-event-bridge,470,2017-08-02 12:56:04.607000 UTC,2022-09-25 04:34:07.277000 UTC,"Tupã, SP, Brasil",121,149,0,34,,,,,,['amazon-sagemaker']
"CreatePredictor at AWS Forecast disregards ""ForecastDimensions""","<p>I defined a TARGET dataset with 4 attributes (item_id, store, date, demand) and need to forecast each item per day per shop. The model trains well, but when casting a prediction, the aws forecast service aggregates all items per day with disregards to the store attribute.</p>

<p>According to the Developer Guide, additional dimensions are taken into account if they are described in the TARGET dataset. This does not work for me.</p>

<p><a href=""https://docs.aws.amazon.com/forecast/latest/dg/forecast.dg.pdf"" rel=""nofollow noreferrer"">AWS Forecast Developer Guide</a></p>

<p>See the documentary: ForecastDimensions </p>

<blockquote>
  <p>If you want the sales forecast for each item by store, you would
  specify store_id as the dimension. All forecast dimensions specified
  in the TARGET_TIME_SERIES dataset <strong>don't need to be specified</strong> in
  the CreatePredictor request</p>
</blockquote>

<p>This is my schema of the TARGET dataset:</p>

<pre><code>ts_schema_val = [{""AttributeName"": ""item_id"", ""AttributeType"": ""string""},
                 {""AttributeName"": ""store"", ""AttributeType"": ""string""},
                 {""AttributeName"": ""timestamp"", ""AttributeType"": ""timestamp""},
                 {""AttributeName"": ""demand"", ""AttributeType"": ""float""}]
</code></pre>

<p>As soon as I create a forecast, the dimension ""store"" is missing in the predictions and items <strong>are not grouped</strong> by store.</p>

<p>What can I do?</p>",1,0,2020-05-09 22:20:35.033000 UTC,,2020-05-09 22:37:54.813000 UTC,0,amazon-web-services|amazon-sagemaker|dimension|forecast,55,2020-05-09 21:45:35.127000 UTC,2020-05-10 13:04:02.193000 UTC,"Hamburg, Germany",26,0,0,5,,,,,,['amazon-sagemaker']
Custom Docker file for Azure ML Environment that contains COPY statements errors with COPY failed: /path no such file or directory,"<p>I'm trying to submit an experiment to Azure ML using a Python script.</p>
<p>The Environment being initialised uses a custom Dockerfile.</p>
<pre class=""lang-py prettyprint-override""><code>env = Environment(name=&quot;test&quot;)
env.docker.base_image = None
env.docker.base_dockerfile = './Docker/Dockerfile'
env.docker.enabled = True
</code></pre>
<p>However the DockerFile needs a few <code>COPY</code> statements but those fail as follow:</p>
<pre><code>Step 9/23 : COPY requirements-azure.txt /tmp/requirements-azure.txt
COPY failed: stat /var/lib/docker/tmp/docker-builder701026190/requirements-azure.txt: no such file or directory
</code></pre>
<p>The Azure host environment responsible to build the image does not contain the files the Dockerfile requires, those exist in my local development machine from where I initiate the python script.</p>
<p>I've been searching for the whole day of a way to add to the environment these files but without success.</p>
<p>Below an excerpt from the Dockerfile and the python script that submits the experiment.</p>
<pre><code>FROM mcr.microsoft.com/azureml/base:intelmpi2018.3-ubuntu16.04 as base
COPY ./Docker/requirements-azure.txt /tmp/requirements-azure.txt # &lt;- breaks here

[...]

</code></pre>
<p>Here is how I'm submitting the experiment:</p>
<pre><code>from azureml.core.environment import Environment
from azureml.core import Workspace
from azureml.core.model import Model
from azureml.core import Workspace, Experiment
from azureml.core.compute import ComputeTarget

from azureml.core import Experiment, Workspace
from azureml.train.estimator import Estimator
import os

ws = Workspace.from_config(path='/mnt/azure/config/workspace-config.json')
env = Environment(name=&quot;test&quot;)
env.docker.base_image = None
env.docker.base_dockerfile = './Docker/Dockerfile'
env.docker.enabled = True
compute_target = ComputeTarget(workspace=ws, name='GRComputeInstance')
estimator = Estimator(
    source_directory='/workspace/',
    compute_target=compute_target,
    entry_script=&quot;./src/ml/train/main.py&quot;,
    environment_definition=env
)
experiment = Experiment(workspace=ws, name=&quot;estimator-test&quot;)
run = experiment.submit(estimator)
run.wait_for_completion(show_output=True, wait_post_processing=True)
</code></pre>
<p>Any idea?</p>",2,1,2020-07-29 16:37:12.287000 UTC,2.0,2020-08-06 08:06:29.560000 UTC,7,python|azure|docker|dockerfile|azure-machine-learning-service,927,2009-07-24 16:26:11.430000 UTC,2022-09-24 08:02:55.380000 UTC,"London, United Kingdom",3317,466,8,296,,,,,,['azure-machine-learning-service']
'AzureBlobDatastore' object is not subscriptable,"<p>I'm trying to create an azure ml pipeline with python steps and I encounter the following issue: &quot;'AzureBlobDatastore' object is not subscriptable&quot; and I run it using a notebook and all cells are running well beside the last one which is fairly a standard one:</p>
<pre><code>from azureml.pipeline.core import Pipeline
from azureml.core import Experiment

# create the pipeline
pipeline = Pipeline(ws, steps=[data_prep_step, train_step])

# create the experiment and submit and pipeline run
run1 = Experiment(workspace=ws, name=&quot;Skylabs-first-model&quot;).submit(pipeline)
</code></pre>",0,1,2022-06-02 08:56:11.390000 UTC,,,0,azure-machine-learning-service|azureml-python-sdk,73,2016-12-16 09:28:07.187000 UTC,2022-09-22 11:19:09.150000 UTC,,137,12,0,56,,,,,,['azure-machine-learning-service']
MLFlow -> ModuleNotFoundError: No module named 'sqlalchemy.future',"<p>It seems to use MLFlow Model Registry locally, one option is to build my own backend database with SQLite.</p>
<p>I've found a site, which advised to run:</p>
<pre><code>mlflow server --backend-store-uri sqlite:///mlflow.db --default-artifact-root ./artifacts --host 0.0.0.0 --port 5000
</code></pre>
<p>When running the command above, I get the following error message:</p>
<pre><code>2022/05/22 23:08:58 ERROR mlflow.cli: Error initializing backend store
2022/05/22 23:08:58 ERROR mlflow.cli: No module named 'sqlalchemy.future'
Traceback (most recent call last):
  File &quot;/home/username/.local/lib/python3.8/site-packages/mlflow/cli.py&quot;, line 426, in server
    initialize_backend_stores(backend_store_uri, default_artifact_root)
  File &quot;/home/username/.local/lib/python3.8/site-packages/mlflow/server/handlers.py&quot;, line 259, in initialize_backend_stores
    _get_tracking_store(backend_store_uri, default_artifact_root)
  File &quot;/home/username/.local/lib/python3.8/site-packages/mlflow/server/handlers.py&quot;, line 244, in _get_tracking_store
    _tracking_store = _tracking_store_registry.get_store(store_uri, artifact_root)
  File &quot;/home/username/.local/lib/python3.8/site-packages/mlflow/tracking/_tracking_service/registry.py&quot;, line 39, in get_store
    return self._get_store_with_resolved_uri(resolved_store_uri, artifact_uri)
  File &quot;/home/username/.local/lib/python3.8/site-packages/mlflow/tracking/_tracking_service/registry.py&quot;, line 49, in _get_store_with_resolved_uri
    return builder(store_uri=resolved_store_uri, artifact_uri=artifact_uri)
  File &quot;/home/username/.local/lib/python3.8/site-packages/mlflow/server/handlers.py&quot;, line 110, in _get_sqlalchemy_store
    from mlflow.store.tracking.sqlalchemy_store import SqlAlchemyStore
  File &quot;/home/username/.local/lib/python3.8/site-packages/mlflow/store/tracking/sqlalchemy_store.py&quot;, line 11, in &lt;module&gt;
    from sqlalchemy.future import select
ModuleNotFoundError: No module named 'sqlalchemy.future'
</code></pre>
<p>This seems odd, because if I run <code>pip freeze</code>, the sqlalchemy shows up, or if I do <code>from sqlalchemy.future import select</code> in a notebook, I get no error.</p>
<p>I think this may related to using a virtual environment. The current one I'm using is in <code>/home/username/folder/mlflow/.mlflow</code> but mlflow seems to be looking elsewhere for the file...</p>",1,1,2022-05-22 22:15:49.153000 UTC,,,1,python|sqlalchemy|mlflow,274,2014-03-31 18:02:38.297000 UTC,2022-09-24 21:03:30.610000 UTC,,714,128,6,251,,,,,,['mlflow']
Amazon SageMaker Downloading Files from S3,"<p>I'm storing midi files in an S3 bucket and am trying to download them into the SageMake jupyter notebook.  I am using this code</p>

<pre><code>import os
import boto3  # Python library for Amazon API 
import botocore
from botocore.exceptions import ClientError
def download_from_s3(url):
    """"""ex: url = s3://sagemakerbucketname/data/validation.tfrecords""""""
    url_parts = url.split(""/"")  # =&gt; ['s3:', '', 'sagemakerbucketname', 'data', ...
    bucket_name = url_parts[2]
    key = os.path.join(*url_parts[3:])
    filename = url_parts[-1]
    if not os.path.exists(filename):
        try:
            # Create an S3 client
            s3 = boto3.resource('s3')
            print('Downloading {} to {}'.format(url, filename))
            s3.Bucket(bucket_name).download_file(key, filename)
        except botocore.exceptions.ClientError as e:
            if e.response['Error']['Code'] == ""404"":
                print('The object {} does not exist in bucket {}'.format(
                    key, bucket_name))
            else:
                raise
</code></pre>

<p>however I am getting An error occurred (403) when calling the HeadObject operation: Forbidden</p>

<p>Here are the permissions attached for the S3:</p>

<pre><code>{
    ""Version"": ""2012-10-17"",
    ""Statement"": [
        {
            ""Effect"": ""Allow"",
            ""Action"": [
                ""s3:GetObject"",
                ""s3:PutObject"",
                ""s3:DeleteObject"",
                ""s3:ListBucket""
            ],
            ""Resource"": [
                ""arn:aws:s3:::*""
            ]
        }
    ]
}
</code></pre>",1,0,2018-06-21 15:11:27.460000 UTC,2.0,2018-07-05 13:22:58.847000 UTC,6,amazon-web-services|amazon-s3|amazon-sagemaker,8331,2017-06-06 19:07:22.410000 UTC,2022-09-23 19:51:52.233000 UTC,"Boca Raton, FL, United States",305,2,0,21,,,,,,['amazon-sagemaker']
add several rights to IAM role using Terraform,"<p>I am trying to adapt an existing TF file so that an IAM role can now have 2 roles/rights: AmazonSageMakerFullAccess + AmazonEC2FullAccess. I have 2 files terraform.tfvars and iam.tf. The former contains:</p>
<pre><code>iam_policy_arn = [
    &quot;arn:aws:iam::aws:policy/AmazonSageMakerFullAccess&quot;, 
    &quot;arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryFullAccess&quot;
]
</code></pre>
<p>and the latter:</p>
<pre><code>data &quot;aws_iam_policy_document&quot; &quot;sm_assume_role_policy&quot; {
  statement {
    actions = [&quot;sts:AssumeRole&quot;]

    principals {
      type        = &quot;Service&quot;
      identifiers = [&quot;sagemaker.amazonaws.com&quot;]
    }
  } 
}

# so that sagemaker can push docker image(s) to ECR
# https://stackoverflow.com/questions/45486041/how-to-attach-multiple-iam-policies-to-iam-roles-using-terraform
# Define policy ARNs as list
variable &quot;iam_policy_arn&quot; {
  description = &quot;IAM Policy to be attached to role&quot;
  type = list(string)
}

# Then parse through the list using count
resource &quot;aws_iam_role_policy_attachment&quot; &quot;role-policy-attachment&quot; {
  role       = &quot;${var.iam_role_name}&quot;
  count      = &quot;${length(var.iam_policy_arn)}&quot;
  policy_arn = &quot;${var.iam_policy_arn[count.index]}&quot;
}
</code></pre>
<p>My github action produces now:</p>
<pre><code>  on iam.tf line 19:
│  19: variable &quot;iam_policy_arn&quot; {
│
│The root module input variable &quot;iam_policy_arn&quot; is not set, and has no
│default value. Use a -var or -var-file command line argument to provide a
│value for this variable.
╵
Enter a value:
Error: Process completed with exit code 1.
</code></pre>
<p>Any idea? Thanks!</p>",2,0,2022-06-03 15:11:11.113000 UTC,,2022-06-03 16:38:27.200000 UTC,0,amazon-web-services|terraform|amazon-sagemaker,163,2010-03-01 10:53:04.443000 UTC,2022-09-24 18:56:19.313000 UTC,Somewhere,15705,2171,91,2150,,,,,,['amazon-sagemaker']
Error : cc.fr.300.bin cannot be opened for loading,"<p>I am using Azure Machine Learning and Azure Databricks.
In Azure Databricks I have a script.py written by %% command (%%write script.py).</p>
<p>In this script I tried to load cc.fr.300.bin that is saved as a model in Azure Machine Learning.</p>
<p>I did this:</p>
<pre><code>import fasttext
fr_model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'cc.fr.300.bin')
fr_model = fasttext.load_model(fr_model_path)
</code></pre>
<p>But I have the error :</p>
<pre><code>File &quot;/structure/azureml-app/script.py&quot;, line 134, in init
    fr_model = fasttext.load_model(fr_model_path)
  File &quot;/azureml-envs/azureml_d7.../lib/python3.6/site-packages/fasttext/FastText.py&quot;, line 441, in load_model
    return _FastText(model_path=path)
  File &quot;/azureml-envs/azureml_d7.../lib/python3.6/site-packages/fasttext/FastText.py&quot;, line 98, in __init__
    self.f.loadModel(model_path)
ValueError: /var/azureml-app/azureml-models/test/1/cc.fr.300.bin cannot be opened for loading!
</code></pre>
<p>What can I do ?</p>",1,0,2022-02-08 07:57:11.253000 UTC,,,-2,azure-machine-learning-service|fasttext,548,2021-11-29 12:42:01.320000 UTC,2022-09-21 11:05:53.383000 UTC,,151,54,0,17,,,,,,['azure-machine-learning-service']
I expose ports in docker but it does not connect,"<p>I have a simple docker image (I post the Dockerfile at the end) and I run it with</p>
<pre><code>docker run -p 8888:8888 -p 5000:5000 -v $(pwd):/workfolder -it --rm stockpred
</code></pre>
<p>I am expecting to expose the ports 8888 and 5000.</p>
<p>Inside the container I do:</p>
<pre><code>(base) root@41131b74043f:/workfolder# mlflow ui
[2022-06-06 10:59:24 +0000] [26] [INFO] Starting gunicorn 20.1.0
[2022-06-06 10:59:24 +0000] [26] [INFO] Listening at: http://127.0.0.1:5000 (26)
[2022-06-06 10:59:24 +0000] [26] [INFO] Using worker: sync
[2022-06-06 10:59:24 +0000] [27] [INFO] Booting worker with pid: 27
</code></pre>
<p>so I go and open that address in my browser but I got</p>
<p>The connection was reset</p>
<blockquote>
<p>The connection to the server was reset while the page was loading.</p>
<pre><code>The site could be temporarily unavailable or too busy. Try again in a few moments.
If you are unable to load any pages, check your computer’s network connection.
If your computer or network is protected by a firewall or proxy, make sure that Firefox is permitted to access the Web.
</code></pre>
</blockquote>
<p>I thought that I could open the page externally. It must be something I am missing but what am I doing wrong?</p>
<pre><code>FROM continuumio/miniconda3

RUN pip install mlflow&gt;=1.18.0 \
    &amp;&amp; pip install numpy \
    &amp;&amp; pip install scipy \
    &amp;&amp; pip install pandas \
    &amp;&amp; pip install scikit-learn \
    &amp;&amp; pip install cloudpickle \
    &amp;&amp; pip install pandas_datareader==0.10.0 \
    &amp;&amp; pip install yfinance
</code></pre>
<p>EDIT:</p>
<p>It worked when I did</p>
<pre><code>docker run --network=&quot;host&quot; -p 8888:8888 -p 5000:5000 -v $(pwd):/workfolder -it --rm stockpred
</code></pre>
<p>Notice that I did not expose the ports in the Dockerfile.</p>
<p>Can someone explain me why this is working like this?</p>
<p>(I also tried exposing the ports in the Dockerfile and running like originally but it didn't work)</p>",0,3,2022-06-06 11:09:46.370000 UTC,,2022-06-06 11:56:50.190000 UTC,0,docker|mlflow,121,2015-01-14 01:17:49.333000 UTC,2022-09-24 09:09:14.427000 UTC,,5585,792,53,1350,,,,,,['mlflow']
Is there any way to upload a file to Microsoft QnA Maker KB from visual studio?,<p>I am trying to change the knowledge base of my QnA maker service in Microsoft QnA maker site. Is it possible to upload a file to this service from my code?</p>,2,0,2017-08-14 18:09:02.850000 UTC,,2017-08-16 22:36:24.353000 UTC,0,microsoft-cognitive|azure-machine-learning-studio|azure-language-understanding,1223,2017-08-14 18:04:27.173000 UTC,2017-08-30 06:33:14.777000 UTC,,1,0,0,2,,,,,,['azure-machine-learning-studio']
Need help in finding classification algorithms in Azure Machine Learning Experiment,"<p>Need help in finding classification algorithms which can classify the different scored label based on the combination of multiple input parameter in Azure machine learning experiment.</p>

<p>Ex :  In the below example we are taking the combination of input parameters Lic, Rel, type, country and flag and predicting the matching records and returning the Scored labels and its probabilities.<br>
Here we are having multiple scored label value which will vary based on combination of all five parameter.</p>

<p>We require classification algorithm which identifies the scored label and its probabilities based on different set of input parameters in Azure Machine learning experiments</p>

<pre><code>Lic Rel        type country flag    Scored label    Scored Probabilities    
C1  indi       HD   USA     Yes     1               0.477611
C2  indi       HD   USA     Yes     2               0.477611
C3  indi       HD   USA     Yes     3               0.477611
C1  indi       HD   USA     Yes     1               0.477611
C5  indi limi  HD   USA     Yes     4               0.477611    
C6  indi limi  HD   CARIB   Yes     5               0.477611    
C7  indi limi  HD   CARIB   Yes     6               0.477611    
C6  indi limi  HD   CARIB   Yes     5               0.477611    
</code></pre>",0,3,2017-12-22 05:43:20.763000 UTC,,2017-12-22 05:48:19.447000 UTC,1,azure|azure-machine-learning-studio,100,2017-09-08 08:51:50.663000 UTC,2019-07-01 11:42:21.513000 UTC,,31,0,0,20,,,,,,['azure-machine-learning-studio']
"'all images for the production variant AllTraffic exist, the execution role used to create the model has permissions to access them'","<p>I saw this link: <a href=""https://github.com/aws/sagemaker-python-sdk/issues/912"" rel=""nofollow noreferrer"">https://github.com/aws/sagemaker-python-sdk/issues/912</a>, it's similar to my question but not the same, my error is:</p>
<pre><code>Please make sure all images included in the model for the production variant AllTraffic exist, and that the execution role used to create the model has permissions to access them.
</code></pre>
<p>Because of the security setting of my AWS account, I can't attach SageMaker fullaccess to the execution role, but I did add :</p>
<pre><code>{
            &quot;Effect&quot;: &quot;Allow&quot;,
            &quot;Action&quot;: [
                &quot;ecr:SetRepositoryPolicy&quot;,
                &quot;ecr:CompleteLayerUpload&quot;,
                &quot;ecr:BatchGetImage&quot;,                
                &quot;ecr:BatchDeleteImage&quot;,
                &quot;ecr:UploadLayerPart&quot;,
                &quot;ecr:DeleteRepositoryPolicy&quot;,
                &quot;ecr:InitiateLayerUpload&quot;,
                &quot;ecr:DeleteRepository&quot;,
                &quot;ecr:PutImage&quot;
            ],
            &quot;Resource&quot;: &quot;arn:aws:ecr:*:*:repository/*sagemaker*&quot;
        },
</code></pre>
<p>in the policies and attached to this execution role, I'm new to SageMaker, does anyone know why I got this error?</p>
<p>Here's some of the codes:</p>
<pre><code>from sagemaker.tensorflow.serving import Model

model = Model(model_data=model_data,
              role=role,
              framework_version='1.15.2',
              sagemaker_session=sagemaker_session,
              name=name)

predictor = model.deploy(initial_instance_count=1,
    instance_type='ml.m4.2xlarge',
    endpoint_name=name,
    update_endpoint=False)
</code></pre>",0,0,2020-11-24 14:52:02.640000 UTC,,2020-11-24 15:07:05.110000 UTC,1,amazon-web-services|tensorflow|amazon-sagemaker|tensorflow-serving|aws-policies,342,2018-10-30 17:35:56.270000 UTC,2022-09-22 19:30:36.883000 UTC,United Kingdom,2385,1007,16,585,,,,,,['amazon-sagemaker']
Logging Artifacts from MlFlow on GCS Bucket,"<p>I have a running MlFlow server on GCS VM instance. I have created a bucket to log the artifacts.
This is the command I'm running to start the server and for specifying bucket path-</p>

<pre><code>mlflow server --default-artifact-root gs://gcs_bucket/artifacts --host x.x.x.x
</code></pre>

<p>But facing this error:</p>

<pre><code>TypeError: stat: path should be string, bytes, os.PathLike or integer, not ElasticNet
</code></pre>

<p>Note- The mlflow server is running fine with the specified host alone. The problem is in the way when I'm specifying the storage bucket path.
I have given permission of storage api by using these commands:</p>

<pre><code>gcloud auth application-default login
gcloud auth login
</code></pre>

<p>Also, on printing the artifact URI, this is what I'm getting:</p>

<pre><code>mlflow.get_artifact_uri()
</code></pre>

<p>Output:</p>

<pre><code>gs://gcs_bucket/artifacts/0/122481bf990xxxxxxxxxxxxxxxxxxxxx/artifacts
</code></pre>

<p>So in the above path from where this is coming <code>0/122481bf990xxxxxxxxxxxxxxxxxxxxx/artifacts</code> and why it's not getting auto-created at <code>gs://gcs_bucket/artifacts</code></p>

<p>After debugging more, why it's not able to get the local path from VM:
<a href=""https://i.stack.imgur.com/ubDU0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ubDU0.png"" alt=""enter image description here""></a></p>

<p>And this error I'm getting on VM:</p>

<pre><code>ARNING:root:Malformed experiment 'mlruns'. Detailed error Yaml file './mlruns/mlruns/meta.yaml' does not exist.
Traceback (most recent call last):
 File ""/usr/local/lib/python3.6/dist-packages/mlflow/store/tracking/file_store.py"", line 197, in list_experiments
   experiment = self._get_experiment(exp_id, view_type)
 File ""/usr/local/lib/python3.6/dist-packages/mlflow/store/tracking/file_store.py"", line 256, in _get_experiment
   meta = read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)
 File ""/usr/local/lib/python3.6/dist-packages/mlflow/utils/file_utils.py"", line 160, in read_yaml
   raise MissingConfigException(""Yaml file '%s' does not exist."" % file_path)
mlflow.exceptions.MissingConfigException: Yaml file './mlruns/mlruns/meta.yaml' does not exist.
</code></pre>

<p>Can I get a solution to this and what I'm missing?</p>",1,7,2020-03-10 11:40:20.677000 UTC,1.0,2020-03-11 10:26:11.777000 UTC,4,python|google-cloud-platform|google-cloud-storage|mlflow,2153,2015-12-26 10:00:57.623000 UTC,2022-09-24 13:39:24.720000 UTC,India,736,69,2,234,,,,,,['mlflow']
trigger azure ml experiment from powerbi,"<p>I have created an azure ml experiment which fetches data from API and updates it in sql azure database. My power bi report picks data from this database and displays the report. The data from the source is changing frequently. So I need something like a checkbox in power bi which when checked will trigger the azure ml experiment and update the database with latest data.</p>

<p>I know that we can schedule it to run in Rstudio pipeline but we are not thinking of this approach as it is not financially viable.
Thanks in Advance.</p>",2,0,2015-12-15 18:38:02.987000 UTC,,2016-03-01 16:32:21.007000 UTC,0,azure|powerbi|cortana-intelligence|azure-machine-learning-studio,292,2015-12-08 16:51:25.033000 UTC,2020-10-12 14:17:25.503000 UTC,,101,4,0,16,,,,,,['azure-machine-learning-studio']
Assume IAM Role to store MLFlow Artifact on S3 bucket in another account,"<p>I have to save my MLFlow artifacts (using Databricks Unified Analytics) to a S3 bucket, with service-side encrpytion using a KMS key.</p>

<p>My instances are into an AWS account A, my S3 bucket and my KMS key into an account B. I can't have my KMS Key into my account A.</p>

<p>I don't want to use DBFS to mount S3 buckets, for security reasons (buckets can contains sensitive data and I don't want to share this between users).</p>

<p>I have to assume an IAM role in order to access the bucket, as I did to access it through s3a (with <code>spark.hadoop.fs.s3a.credentialsType</code> and <code>spark.hadoop.fs.s3a.stsAssumeRole.arn</code> parameters).</p>

<p>When I create an experiment with s3 and try to log a model like this : </p>

<pre class=""lang-py prettyprint-override""><code>import mlflow
import mlflow.sklearn
id_exp = mlflow.create_experiment(""/Users/first.last@company.org/Experiment"",'s3://s3-bucket-name/')
with mlflow.start_run(experiment_id=id_exp):
  clf_mlf = tree.DecisionTreeClassifier()
  clf_mlf = clf_mlf.fit(X_train, y_train)
  y_pred = clf_mlf.predict(X_test)
  mlflow.sklearn.log_model(clf_mlf, ""model"", serialization_format='pickle')
</code></pre>

<p>I have this error : </p>

<pre><code>S3UploadFailedError: Failed to upload /tmp/tmp2yl2olhi/model/conda.yaml to s3-bucket-name//05c17a33a33d46a5ad3cc811a9faf35a/artifacts/model/conda.yaml: An error occurred (KMS.NotFoundException) when calling the PutObject operation: Key 'arn:aws:kms:eu-central-1:account_a_id:key/key_id' does not exist
</code></pre>

<p>How can I told MLFlow to assume a role before accessing to S3 ?</p>",0,0,2019-07-24 09:27:00.500000 UTC,,,3,amazon-s3|amazon-iam|databricks|mlflow,693,2018-03-13 17:58:54.917000 UTC,2020-11-03 18:13:09.453000 UTC,,31,0,0,4,,,,,,['mlflow']
Iterate folders in Datastore in Azure Machine Learning service,"<p>We have training pictures coming in to Azure blob storage. They are places in folders like
ProductA/yyyy-mm-dd, where the date is when the material was put in blob storage.</p>
<p>We have registered the blob container as a Datastore. When we run the training, we want to use the python sdk to create a Dataset, or a new version of the dataset, called ProductA and that should reference all date folders existing at that moment.</p>
<p>This is described here: <a href=""https://docs.microsoft.com/en-us/azure/machine-learning/how-to-version-track-datasets#versioning-best-practice"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/machine-learning/how-to-version-track-datasets#versioning-best-practice</a></p>
<p>In that example data is in folders called &quot;Week xx&quot;. What is not described is how to know which all folders to reference when creating the dataset.</p>
<p>How can we access the blob storage to find the relevant folders to include? Can we use the Datastore to access files and folders directly?</p>",0,2,2022-02-03 06:58:12.443000 UTC,,,1,azure|azure-machine-learning-service,608,2010-02-04 11:21:17.390000 UTC,2022-09-22 08:00:13.620000 UTC,Finland,3462,241,4,409,,,,,,['azure-machine-learning-service']
How to get url of mlflow logged artifacts?,"<p>I am running an ML pipeline, at the end of which I am logging certain information using mlflow. I was mostly going through Databricks' official mlflow tracking tutorial.</p>
<pre><code>import mlflow
import mlflow.sklearn
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error

with mlflow.start_run():
  n_estimators = 100
  max_depth = 6
  max_features = 3
  # Create and train model
  rf = RandomForestRegressor(n_estimators = n_estimators, max_depth = max_depth, max_features = max_features)
  rf.fit(X_train, y_train)
  # Make predictions
  predictions = rf.predict(X_test)
  
  # Log parameters
  mlflow.log_param(&quot;num_trees&quot;, n_estimators)
  mlflow.log_param(&quot;maxdepth&quot;, max_depth)
  mlflow.log_param(&quot;max_feat&quot;, max_features)
  
  # Log model
  mlflow.sklearn.log_model(rf, &quot;random-forest-model&quot;)
  
  # Create metrics
  mse = mean_squared_error(y_test, predictions)
    
  # Log metrics
  mlflow.log_metric(&quot;mse&quot;, mse)
</code></pre>
<p>When I run the above block of code in Databricks notebook, the below status message shows:</p>
<pre><code>(1) MLflow run
Logged 1 run to an experiment in MLflow. Learn more
</code></pre>
<p>And I can view the logged information by clicking on &quot;1 run.&quot;</p>
<p>However, I would like to automatically retrieve this link. In particular, I need the link to the mlflow uri where the artifacts are stored. This link is in the following format:</p>
<pre><code>https://mycompany-dev.cloud.databricks.com/?o=&lt;ID_1&gt;#mlflow/experiments/&lt;ID_2&gt;/runs/&lt;ID_3&gt;
</code></pre>
<p>I tried investigating the url and finding the various id codes that are present in it by printing the following information:</p>
<pre><code>print(&quot;Tracking URI: &quot;, mlflow.get_tracking_uri())
print(&quot;Run id:&quot;, run.info.run_id)
print(&quot;Experiment:&quot;, run.info.experiment_id)
</code></pre>
<p>I figured out that <code>&lt;ID_2&gt;</code> in the link above is the <code>experiment_id</code> and <code>&lt;ID_3&gt;</code> is the <code>run_id</code>. But I have no idea what <code>&lt;ID_1&gt;</code> stands for. Also, I believe there should be a built-in functionality to retrieve the link of saved artifacts, instead of manually having to build up the link from sections. However, I haven't found such a funcitonality in the documentation so far.</p>
<p><strong>Edit:</strong> Now I discovered that <code>&lt;ID_1&gt;</code> is the Databricks workplace id. But it is still a question how I can access it programatically.</p>",0,0,2022-09-22 13:46:21.563000 UTC,,2022-09-22 14:03:46.853000 UTC,0,python|machine-learning|databricks|azure-databricks|mlflow,27,2015-07-16 10:20:07.427000 UTC,2022-09-24 16:13:50.543000 UTC,,968,512,10,241,,,,,,['mlflow']
How does autologging work in MLOps platforms like Comet or MLFlow?,<p>I was wondering how the implementation of logging is done where you just need to create an experiment object from comet_ml and it auto-detects and gives out all the statistics of the trained experiment. Is there some sort of logging system used?</p>,1,0,2022-01-03 09:14:21.397000 UTC,,,0,comet|mlflow|mlops,55,2020-08-05 13:21:06.543000 UTC,2022-03-22 05:41:49.233000 UTC,"Nashik, Maharashtra, India",18,5,0,3,,,,,,['mlflow']
MLFlow in docker - unable to store artifacts in sftp server (atmoz),"<p>I would like to run MLflow &quot;entirely offline&quot; using docker (i.e. no cloud storage like S3 or blob). So I followed <a href=""https://towardsdatascience.com/deploy-mlflow-with-docker-compose-8059f16b6039"" rel=""nofollow noreferrer"">this guide</a> and tried to set the artifact store to the <a href=""https://github.com/atmoz/sftp"" rel=""nofollow noreferrer"">atmoz</a> sftp server running inside another docker container. As suggested in the <a href=""https://www.mlflow.org/docs/latest/tracking.html#sftp-server"" rel=""nofollow noreferrer"">MLFlow docs</a>, I try to auth with host keys, however, when I try to register my artifact I receive the following error <code>pysftp.exceptions.CredentialException: No password or key specified.</code></p>
<p>I guess, there's something wrong with my hostkey setup. I also tried to follow <a href=""https://towardsdatascience.com/setup-mlflow-in-production-d72aecde7fef"" rel=""nofollow noreferrer"">this guide</a> (mentioned in <a href=""https://towardsdatascience.com/setup-mlflow-in-production-d72aecde7fef"" rel=""nofollow noreferrer"">this question</a>), but unfortunately it didn't have enough details for my - limited - knowledge of containers, sftp servers and pub-priv-key setups. My docker-compose looks like this...</p>
<pre><code>services:
db:
    restart: always
    image: mysql/mysql-server:5.7.28
    container_name: mlflow_db
    expose:
        - &quot;3306&quot;
    networks:
        - backend
    environment:
        - MYSQL_DATABASE=${MYSQL_DATABASE}
        - MYSQL_USER=${MYSQL_USER}
        - MYSQL_PASSWORD=${MYSQL_PASSWORD}
        - MYSQL_ROOT_PASSWORD=${MYSQL_ROOT_PASSWORD}
    volumes:
        - dbdata:/var/lib/mysql

mlflow-sftp:
    image: atmoz/sftp
    container_name: mlflow-sftp
    ports:
        - &quot;2222:22&quot;
    volumes:
        - ./storage/sftp:/home/foo/storage
        - ./ssh_host_ed25519_key:/home/foo/.ssh/ssh_host_ed25519_key.pub:ro
        - ./ssh_host_rsa_key:/home/foo/.ssh/ssh_host_rsa_key.pub:ro
    command: foo::1001
    networks:
        - backend
    
web:
    restart: always
    build: ./mlflow
    depends_on:
        - mlflow-sftp
    image: mlflow_server
    container_name: mlflow_server
    expose:
        - &quot;5000&quot;
    networks:
        - frontend
        - backend
    volumes:
        - ./ssh_host_ed25519_key:/root/.ssh/ssh_host_ed25519_key:ro
        - ./ssh_host_rsa_key:/root/.ssh/ssh_host_rsa_key:ro
    command: &gt;
        bash -c &quot;sleep 3
        &amp;&amp; ssh-keyscan -H mlflow-sftp &gt;&gt; ~/.ssh/known_hosts
        &amp;&amp; mlflow server --backend-store-uri mysql+pymysql://${MYSQL_USER}:${MYSQL_PASSWORD}@db:3306/${MYSQL_DATABASE} --default-artifact-root sftp://foo@localhost:2222/storage --host 0.0.0.0&quot;
    
nginx:
    restart: always
    build: ./nginx
    image: mlflow_nginx
    container_name: mlflow_nginx
    ports:
        - &quot;80:80&quot;
    networks:
        - frontend
    depends_on:
        - web
</code></pre>
<p>networks:
frontend:
driver: bridge
backend:
driver: bridge</p>
<p>volumes:
dbdata:</p>
<p>... and in my python script I create a new mlflow experiment as follows.</p>
<pre><code>remote_server_uri = &quot;http://localhost:80&quot; 
mlflow.set_tracking_uri(remote_server_uri)
EXPERIMENT_NAME = &quot;test43&quot;
mlflow.create_experiment(EXPERIMENT_NAME) #, artifact_location=ARTIFACT_URI)
mlflow.set_experiment(EXPERIMENT_NAME)
EXPERIMENT_NAME = &quot;test43&quot;
mlflow.create_experiment(EXPERIMENT_NAME) #, artifact_location=ARTIFACT_URI)
mlflow.set_experiment(EXPERIMENT_NAME)
with mlflow.start_run():
    print(mlflow.get_artifact_uri())
    print(mlflow.get_registry_uri())
    lr = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=42)
    lr.fit(train_x, train_y)

    predicted_qualities = lr.predict(test_x)

    (rmse, mae, r2) = eval_metrics(test_y, predicted_qualities)

    print(&quot;Elasticnet model (alpha=%f, l1_ratio=%f):&quot; % (alpha, l1_ratio))
    print(&quot;  RMSE: %s&quot; % rmse)
    print(&quot;  MAE: %s&quot; % mae)
    print(&quot;  R2: %s&quot; % r2)

    mlflow.log_param(&quot;alpha&quot;, alpha)
    mlflow.log_param(&quot;l1_ratio&quot;, l1_ratio)
    mlflow.log_metric(&quot;rmse&quot;, rmse)
    mlflow.log_metric(&quot;r2&quot;, r2)
    mlflow.log_metric(&quot;mae&quot;, mae)

    tracking_url_type_store = urlparse(mlflow.get_tracking_uri()).scheme

    if tracking_url_type_store != &quot;file&quot;:
        mlflow.sklearn.log_model(lr, &quot;model&quot;, registered_model_name=&quot;ElasticnetWineModel&quot;)
    else:
        mlflow.sklearn.log_model(lr, &quot;model&quot;)
</code></pre>
<p>I haven't modified the dockerfiles of the first mentioned guide i.e. you'll be able to see them <a href=""https://towardsdatascience.com/deploy-mlflow-with-docker-compose-8059f16b6039"" rel=""nofollow noreferrer"">here</a>. My guess is that I messed something up with the host keys, maybe put them in a wrong directory, but after several hours of brute-force experimenting I hope someone can help me with a pointer in the right direction. Let me know if there's anything missing to reproduce the error.</p>",0,0,2020-09-28 09:15:08.343000 UTC,,,4,python|docker|public-key|mlflow,874,2018-02-06 16:01:55.310000 UTC,2022-09-09 13:01:43.583000 UTC,Germany,41,3,0,3,,,,,,['mlflow']
OverflowError: size does not fit in an int,"<p>I am writing a python script to use in AzureML. My dataset is quite big. I have a dataset with columns called ID(int) and DataType(text). I would like to concatenate these values to just have one column with text that has both the ID and the DataType seperated by a comma. </p>

<p>How can I avoid getting an error when I do this. Do I have any mistakes in my code?</p>

<p>When i run this code I get the following error: </p>

<pre><code>Error 0085: The following error occurred during script evaluation, please view the output log for more information:
---------- Start of error message from Python interpreter ----------
data:text/plain,Caught exception while executing function: Traceback (most recent call last):
File ""C:\server\invokepy.py"", line 167, in batch
idfs.append(rutils.RUtils.RFileToDataFrame(infile))
File ""C:\server\RReader\rutils.py"", line 15, in RFileToDataFrame
rreader = RReaderFactory.construct_from_file(filename, compressed)
File ""C:\server\RReader\rreaderfactory.py"", line 25, in construct_from_file
return _RReaderFactory.construct_from_stream(stream)
File ""C:\server\RReader\rreaderfactory.py"", line 46, in construct_from_stream
return RReader(BinaryReader(RFactoryConstants.big_endian, stream.read()))
File ""C:\pyhome\lib\gzip.py"", line 254, in read
self._read(readsize)
File ""C:\pyhome\lib\gzip.py"", line 313, in _read
self._add_read_data( uncompress )
File ""C:\pyhome\lib\gzip.py"", line 329, in _add_read_data
self.crc = zlib.crc32(data, self.crc) &amp; 0xffffffffL
OverflowError: size does not fit in an int
</code></pre>

<p>My code is as below:</p>

<pre><code># The script MUST contain a function named azureml_main
# which is the entry point for this module.
#
# The entry point function can contain up to two input arguments:
#   Param&lt;dataframe1&gt;: a pandas.DataFrame
#   Param&lt;dataframe2&gt;: a pandas.DataFrame

def azureml_main(dataframe1):
import pandas as pd
dataframe1['SignalID,DataType'] = dataframe1['ID'] + "" , "" + dataframe1['DataType']
dataframe1 = dataframe1.drop('DataType')
dataframe1 = dataframe1.drop('ID')
# Return value must be of a sequence of pandas.DataFrame
return dataframe1
</code></pre>

<p>I get the same error when I run the default python code in AzureML. So I am pretty sure my data just does not fit in the data frame.</p>

<p>The default script is the following:</p>

<pre><code># The script MUST contain a function named azureml_main
# which is the entry point for this module.
#
# The entry point function can contain up to two input arguments:
#   Param&lt;dataframe1&gt;: a pandas.DataFrame
#   Param&lt;dataframe2&gt;: a pandas.DataFrame
def azureml_main(dataframe1 = None, dataframe2 = None):

    # Execution logic goes here
    print('Input pandas.DataFrame #1:\r\n\r\n{0}'.format(dataframe1))

    # If a zip file is connected to the third input port is connected,
    # it is unzipped under "".\Script Bundle"". This directory is added
    # to sys.path. Therefore, if your zip file contains a Python file
    # mymodule.py you can import it using:
    # import mymodule

    # Return value must be of a sequence of pandas.DataFrame
    return dataframe1,
</code></pre>",1,1,2016-02-04 10:05:05.847000 UTC,,2016-02-04 12:44:47.193000 UTC,1,python|pandas|dataframe|azure-machine-learning-studio,2391,2014-05-13 11:59:58.040000 UTC,2022-09-14 13:50:23.127000 UTC,,611,233,0,106,,,,,,['azure-machine-learning-studio']
How to generate an alert if deployment becomes 'Unhealthy' in Azure Machine Learning?,"<p>I deployed an Azure Machine Learning model to AKS, and would like to know how to set an alert if the deployment status changes to any value other than 'Healhty'.  I looked at the monitoring metrics in the workspace, but it looks like they are more related to the training process (Model and Run) and Quotas.  Please let me know if you have any suggestions</p>

<p>Thanks!</p>",1,0,2019-10-04 21:51:37.923000 UTC,,,0,azure-machine-learning-service,396,2019-10-04 21:39:44.120000 UTC,2021-06-02 21:49:42.870000 UTC,,11,0,0,3,,,,,,['azure-machine-learning-service']
AzureML webservice deployment with custom Environment - /var/runit does not exist,"<p>I'm struggling to deploy a model with a custom environment through the azureml SDK.</p>
<p>I have built a docker image locally and pushed it to azure container registry to use it for environment instantiating. This is how my dockerfile looks like:</p>
<pre><code>FROM mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04
FROM python:3.9.12
        
# Keeps Python from generating .pyc files in the container
ENV PYTHONDONTWRITEBYTECODE=1
        
# Turns off buffering for easier container logging
ENV PYTHONUNBUFFERED=1
        
# Install requirement for deploying the service
RUN apt-get update
RUN apt-get install -y runit
        
# Install pip requirements
RUN pip install --upgrade pip
COPY requirements.txt .
RUN pip install azureml-defaults
RUN pip install -r requirements.txt
</code></pre>
<p>I want to deploy the webservice locally for testing, so I am following the steps according to official documentation:</p>
<pre><code>ws = Workspace(
    subscription_id='mysub_id', 
    resource_group='myresource_group', 
    workspace_name='myworkspace'
)
        
model = Model.register(
    ws, 
    model_name='mymodel', 
    model_path='./Azure_Deployment/mymodel_path'
)
        
container = ContainerRegistry()
container.address = 'myaddress'
myenv = Environment.from_docker_image('myenv_name', 'img/img_name:v1', container)
        
inference_config = InferenceConfig(
    environment=myenv, 
    source_directory='./Azure_Deployment', 
    entry_script='echo_score.py',
)
        
deployment_config = LocalWebservice.deploy_configuration(port=6789)
        
service = Model.deploy(
    ws, 
    &quot;myservice&quot;, 
    [model], 
    inference_config, 
    deployment_config, 
    overwrite=True,
)
service.wait_for_deployment(show_output=True)
</code></pre>
<p>This is what I get from the logs:</p>
<p><a href=""https://i.stack.imgur.com/GLafe.png"" rel=""nofollow noreferrer"">service container logs</a></p>
<p>Checking into the resulting container for the service I can see indeed there is no /runit folder inside /var. There is also no other folders created for the service besides the azureml-app containing my model's files.</p>
<p>I would really appreciate any insights to what's going on here as I have no clue at this point.</p>",2,0,2022-03-29 22:12:03.567000 UTC,,,1,azure-machine-learning-service|azureml-python-sdk,156,2022-03-29 20:21:35.487000 UTC,2022-04-20 05:37:40.933000 UTC,,11,0,0,0,,,,,,['azure-machine-learning-service']
SageMaker fails when trying to add Lifecycle Configuration for keeping custom environments persistent after restart,"<p>I want to create environment in SageMaker on AWS with miniconda, and make it available as kernels in Jupyter when I restart the session. But the SageMaker keep failing.</p>
<p>I followed the instructions found in here:<br />
<a href=""https://aws.amazon.com/premiumsupport/knowledge-center/sagemaker-lifecycle-script-timeout/"" rel=""nofollow noreferrer"">https://aws.amazon.com/premiumsupport/knowledge-center/sagemaker-lifecycle-script-timeout/</a></p>
<p>basically it says:<br />
<em>&quot;Create a custom, persistent Conda installation on the notebook instance's Amazon Elastic Block Store (Amazon EBS) volume: Run the on-create script in the terminal of an existing notebook instance. This script uses Miniconda to create a separate Conda installation on the EBS volume (/home/ec2-user/SageMaker/). Then, run the on-start script as a lifecycle configuration to make the custom environment available as a kernel in Jupyter. This method is recommended for more technical users, and it is a better long-term solution.&quot;</em></p>
<p>I run this on-create.sh script on the terminal on Jupyter:</p>
<p>on-create.sh:</p>
<pre><code>#!/bin/bash

set -e

sudo -u ec2-user -i &lt;&lt;'EOF'
unset SUDO_UID
# Install a separate conda installation via Miniconda
WORKING_DIR=/home/ec2-user/SageMaker/custom-environments
mkdir -p &quot;$WORKING_DIR&quot;
wget https://repo.anaconda.com/miniconda/Miniconda3-4.6.14-Linux-x86_64.sh -O &quot;$WORKING_DIR/miniconda.sh&quot;
bash &quot;$WORKING_DIR/miniconda.sh&quot; -b -u -p &quot;$WORKING_DIR/miniconda&quot; 
rm -rf &quot;$WORKING_DIR/miniconda.sh&quot;

# Create a custom conda environment
source &quot;$WORKING_DIR/miniconda/bin/activate&quot;
KERNEL_NAME=&quot;conda-test-env&quot;
PYTHON=&quot;3.6&quot;
conda create --yes --name &quot;$KERNEL_NAME&quot; python=&quot;$PYTHON&quot;
conda activate &quot;$KERNEL_NAME&quot;
pip install --quiet ipykernel
# Customize these lines as necessary to install the required packages
conda install --yes numpy
pip install --quiet boto3
EOF
</code></pre>
<p>and it creates the &quot;conda-test-env&quot; environment as expected.</p>
<p>Then I add the on-start.sh as lifestyle configuration:</p>
<pre><code>#!/bin/bash

set -e

sudo -u ec2-user -i &lt;&lt;'EOF'
unset SUDO_UID
source &quot;/home/ec2-user/SageMaker/custom-environments/miniconda/bin/activate&quot;
conda activate conda-test-env
python -m ipykernel install --user --name &quot;conda-test-env&quot; --display-name &quot;conda-test-env&quot;

# Optionally, uncomment these lines to disable SageMaker-provided Conda functionality.
# echo &quot;c.EnvironmentKernelSpecManager.use_conda_directly = False&quot; &gt;&gt; /home/ec2-user/.jupyter/jupyter_notebook_config.py
# rm /home/ec2-user/.condarc
EOF
</code></pre>
<p>then I update the instance with the new configuration,
and when I start my notebook instance, after few minutes it fails.</p>
<p>I'll appreciate any help.</p>",0,0,2021-03-10 07:58:05.243000 UTC,,,3,jupyter-notebook|amazon-sagemaker,481,2021-03-10 07:24:17.027000 UTC,2021-05-06 13:14:59.780000 UTC,"Tel Aviv, ישראל",31,0,0,0,,,,,,['amazon-sagemaker']
Failure on importing mlflow to Azure Databricks 7.3 LTS ML Runtime,"<p>I am having trouble trying to import mlflow to Azure databricks. I'm currently using 7.3 LTS ML Runtime, which already have mlflow==1.11.0. I am a developing data scientist and I have no clue how to solve this issue. Have already tried to reinstall and didn't suceed. Any thoughts?</p>
<p>This is the error message:</p>
<pre><code>Failure while loading azureml_run_type_providers. Failed to load entrypoint hyperdrive = azureml.train.hyperdrive:HyperDriveRun._from_run_dto with exception (azureml-core 1.8.0.post1 (/local_disk0/.ephemeral_nfs/envs/pythonEnv-818c9387-555f-4b61-a142-d3c244a68503/lib/python3.7/site-packages), Requirement.parse('azureml-core~=1.19.0'), {'azureml-telemetry'}).
Failure while loading azureml_run_type_providers. Failed to load entrypoint automl = azureml.train.automl.run:AutoMLRun._from_run_dto with exception (azureml-core 1.8.0.post1 (/local_disk0/.ephemeral_nfs/envs/pythonEnv-818c9387-555f-4b61-a142-d3c244a68503/lib/python3.7/site-packages), Requirement.parse('azureml-core~=1.19.0'), {'azureml-telemetry'}).
Failure while loading azureml_run_type_providers. Failed to load entrypoint azureml.PipelineRun = azureml.pipeline.core.run:PipelineRun._from_dto with exception (azureml-core 1.8.0.post1 (/local_disk0/.ephemeral_nfs/envs/pythonEnv-818c9387-555f-4b61-a142-d3c244a68503/lib/python3.7/site-packages), Requirement.parse('azureml-core~=1.19.0')).
Failure while loading azureml_run_type_providers. Failed to load entrypoint azureml.ReusedStepRun = azureml.pipeline.core.run:StepRun._from_reused_dto with exception (azureml-core 1.8.0.post1 (/local_disk0/.ephemeral_nfs/envs/pythonEnv-818c9387-555f-4b61-a142-d3c244a68503/lib/python3.7/site-packages), Requirement.parse('azureml-core~=1.19.0')).
Failure while loading azureml_run_type_providers. Failed to load entrypoint azureml.StepRun = azureml.pipeline.core.run:StepRun._from_dto with exception (azureml-core 1.8.0.post1 (/local_disk0/.ephemeral_nfs/envs/pythonEnv-818c9387-555f-4b61-a142-d3c244a68503/lib/python3.7/site-packages), Requirement.parse('azureml-core~=1.19.0')).
Failure while loading azureml_run_type_providers. Failed to load entrypoint azureml.scriptrun = azureml.core.script_run:ScriptRun._from_run_dto with exception (msrest 0.6.18 (/local_disk0/.ephemeral_nfs/envs/pythonEnv-818c9387-555f-4b61-a142-d3c244a68503/lib/python3.7/site-packages), Requirement.parse('msrest&gt;=0.6.21'), {'azure-mgmt-containerregistry'}).
Could not import from mlflow. Please upgrade to Mlflow 1.4.0 or higher.
/local_disk0/.ephemeral_nfs/envs/pythonEnv-818c9387-555f-4b61-a142-d3c244a68503/lib/python3.7/site-packages/mlflow/tracking/_tracking_service/utils.py:123: UserWarning: Failure attempting to register store for scheme &quot;adbazureml&quot;: No module named 'mlflow.store.rest_store'
  _tracking_store_registry.register_entrypoints()
Could not import from mlflow. Please upgrade to Mlflow 1.4.0 or higher.
/local_disk0/.ephemeral_nfs/envs/pythonEnv-818c9387-555f-4b61-a142-d3c244a68503/lib/python3.7/site-packages/mlflow/tracking/_tracking_service/utils.py:123: UserWarning: Failure attempting to register store for scheme &quot;azureml&quot;: No module named 'mlflow.store.rest_store'
  _tracking_store_registry.register_entrypoints()
Could not import from mlflow. Please upgrade to Mlflow 1.4.0 or higher.
/local_disk0/.ephemeral_nfs/envs/pythonEnv-818c9387-555f-4b61-a142-d3c244a68503/lib/python3.7/site-packages/mlflow/tracking/_model_registry/utils.py:106: UserWarning: Failure attempting to register store for scheme &quot;azureml&quot;: No module named 'mlflow.store.rest_store'
  _model_registry_store_registry.register_entrypoints()
Could not import from mlflow. Please upgrade to Mlflow 1.4.0 or higher.
/local_disk0/.ephemeral_nfs/envs/pythonEnv-818c9387-555f-4b61-a142-d3c244a68503/lib/python3.7/site-packages/mlflow/store/artifact/artifact_repository_registry.py:89: UserWarning: Failure attempting to register artifact repository for scheme &quot;adbazureml&quot;: No module named 'mlflow.store.rest_store'
  _artifact_repository_registry.register_entrypoints()
Could not import from mlflow. Please upgrade to Mlflow 1.4.0 or higher.
/local_disk0/.ephemeral_nfs/envs/pythonEnv-818c9387-555f-4b61-a142-d3c244a68503/lib/python3.7/site-packages/mlflow/store/artifact/artifact_repository_registry.py:89: UserWarning: Failure attempting to register artifact repository for scheme &quot;azureml&quot;: No module named 'mlflow.store.rest_store'
  _artifact_repository_registry.register_entrypoints()
/local_disk0/.ephemeral_nfs/envs/pythonEnv-818c9387-555f-4b61-a142-d3c244a68503/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py:546: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working
  class IteratorBase(collections.Iterator, trackable.Trackable,
/local_disk0/.ephemeral_nfs/envs/pythonEnv-818c9387-555f-4b61-a142-d3c244a68503/lib/python3.7/site-packages/tensorflow/python/autograph/utils/testing.py:21: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp

pip list

azure-common              1.1.28           
azure-core                1.24.1           
azure-graphrbac           0.61.1           
azure-identity            1.4.1            
azure-mgmt-authorization  0.61.0           
azure-mgmt-containerregistry 10.0.0        
azure-mgmt-core           1.3.1            
azure-mgmt-keyvault       2.2.0            
azure-mgmt-network        10.2.0           
azure-mgmt-resource       11.0.0           
azure-mgmt-storage        11.2.0           
azure-storage-blob        12.4.0           
azureml-automl-core       1.19.0           
azureml-core              1.8.0.post1      
azureml-dataprep          2.6.6            
azureml-dataprep-native   26.0.0           
azureml-dataprep-rslex    1.4.0            
azureml-dataset-runtime   1.19.0.post1     
azureml-mlflow            1.8.0            
azureml-pipeline          1.19.0           
azureml-pipeline-core     1.19.0           
azureml-pipeline-steps    1.19.0           
azureml-sdk               1.19.0           
azureml-telemetry         1.19.0           
azureml-train             1.19.0           
azureml-train-automl-client 1.19.0         
azureml-train-core        1.19.0           
azureml-train-restclients-hyperdrive 1.19.0
mlflow                    1.11.0
pip                       20.0.2
</code></pre>",1,0,2022-06-17 19:22:08.417000 UTC,,,0,python|azure-databricks|mlflow,102,2022-03-14 13:59:32.287000 UTC,2022-06-18 19:13:14.010000 UTC,,1,0,0,3,,,,,,['mlflow']
SageMaker fails to create sagemaker_data_wrangler database cause Lake Formation permissions,"<p>The Glue DataCatalog access is managed by Lake Formation. But when trying to add a new SageMaker Data Wrangler flow that queries an Athena table, it gives the following error:</p>
<blockquote>
<p>CustomerError: An error occurred when trying to create
sagemaker_data_wrangler database in the Glue data catalog: An error
occurred (AccessDeniedException) when calling the CreateDatabase
operation: Insufficient Lake Formation permission(s): Required Create
Database on Catalog</p>
</blockquote>
<p>The database sagemaker_data_wrangler does not exist, but we have add the default S3 bucket that uses SageMaker (sagemaker-{region}-{account}), to Lake Formation Data Location, in order to give the SageMaker execution role CreateDatabase privileges:</p>
<p><a href=""https://i.stack.imgur.com/S4c1A.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/S4c1A.png"" alt=""Lake Formation Data Location"" /></a></p>
<p>The error persists even if we manually create the database (sagemaker_data_wrangler) and give privileges to the Data Wrangler execution role.</p>",1,0,2021-07-09 08:49:46.677000 UTC,,,1,amazon-sagemaker|aws-lake-formation,211,2017-09-14 19:07:09.770000 UTC,2022-09-22 14:06:10.067000 UTC,"Barcelona, Spain",98,42,0,17,,,,,,['amazon-sagemaker']
Information regarding Amazon Sagemaker groundtruth,"<p>I'm trying to run a simple GroundTruth labeling job with a private workforce for text classification. Since I'm new to AWS GroundTruth, I have some questions:</p>

<ol>
<li><p>If I use private workforce what is the maximum number of persons that I can allocate to the labeling job? Does the pricing cost depend on number of persons used in private workforce.</p></li>
<li><p>I have a labeled dataset (text classication), and I upload it to S3 bucket, if I upload another unlabeled datas to it, will AutoML label the provided raw data? If not, how can I use already labelled dataset to label new raw datas/</p></li>
<li><p>Groundtruth documentation says that it needs atleast 1000 objects to be labeled by humans. Does it mean 1000 objects of all classes, or 1000 objects for individual class? If I manually label 1000+ objects, how many more objects will AutoML label or what is the maximum number of objects can AutoML label?</p></li>
</ol>",1,0,2019-09-09 11:01:36.960000 UTC,,,0,amazon-web-services|amazon-sagemaker,844,2019-01-10 12:10:54.537000 UTC,2021-12-21 10:54:15.360000 UTC,"Chennai, Tamil Nadu, India",415,7,4,82,,,,,,['amazon-sagemaker']
Azure machine learning specify input size,"<p>I just started using Azure ML and I'm trying to figure out how to specify an input size for the models. Specifically, I have a big training set of data, but I want to input only 250 records at a time into the PCA algorithm. It seems like all I can do is hook the entire data set into the PCA module.</p>

<p>I know how to partition the data for X-validation, but I want a partition (say 10000 records) to only feed 250 records at a time to the model.</p>",1,0,2016-04-16 15:59:45.593000 UTC,,,0,azure|machine-learning|azure-machine-learning-studio,81,2012-02-18 04:56:05.833000 UTC,2020-03-22 20:56:27.307000 UTC,,477,38,0,55,,,,,,['azure-machine-learning-studio']
how to call sagemaker endpoint from another server,"<p>I try to call aws sagemaker endpoint. but I don't use lambda function. only, I want to acess endpoint ARN, URL.</p>

<p>if impossible method, I want to know lambda function</p>

<p>my endpoint based keras model. I don't know json.dumps </p>

<pre class=""lang-py prettyprint-override""><code>import json 
import boto3 
client = boto3.client('runtime.sagemaker')

import numpy as np
test = np.zeros((1, 1, 4325))
test[0][0][1] = 1

data = {""instances"": test.tolist()} 
response = client.invoke_endpoint(EndpointName=endpoint_name,
                                  Body=json.dumps(data))
response_body = response['Body'] 
print(response_body.read())
</code></pre>",2,1,2019-07-24 01:48:17.447000 UTC,,2019-07-24 05:17:08.770000 UTC,0,amazon-web-services|keras|amazon-sagemaker,597,2019-07-24 01:24:52.393000 UTC,2020-11-24 04:06:17.753000 UTC,Korea,1,0,0,2,,,,,,['amazon-sagemaker']
SageMaker end point deployment keeps failing trying to find a dependency file,"<p>SageMaker end point deployment keeps failing trying to find a dependency file.</p>

<p>In the below example, I'm using the ""lables.txt"" in a function <code>load_lables()</code> that gets called in the <code>model_fn()</code> function</p>

<p><strong>project folder structure</strong></p>

<pre><code>--model (directory)
|--code (directory)
   |--requirements.txt
   |--train.py (entry point)
   |--labels.txt
|--notebook_train_deploy.ipynb
</code></pre>

<p><strong>train.py</strong></p>

<pre class=""lang-py prettyprint-override""><code>def load_labels(file_name_category='labels.txt'):

    labels = list()
    with open(file_name_category) as label_file:
        for line in label_file:
            labels.append(line.strip().split(' ')[0][:])
    _out_labels = tuple(labels)

    return _out_labels

def model_fn(model_dir):

    labels = load_labels()
    num_labels = len(labels)    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    model_ft = models.resnet18(pretrained=True)
    .
    .
    .  
    with open(os.path.join(model_dir, 'model.pth'), 'rb') as f:
        model_ft .load_state_dict(torch.load(f))
    model_ft .eval()

    return model_ft .to(device)
</code></pre>

<p><strong>notebook_train_deploy.ipynb</strong></p>

<pre><code>pytorch_model = PyTorchModel(model_data='s3://sagemaker-poc/model.tar.gz', 
                             role=role,
                             source_dir='code',
                             entry_point='train.py',
                             framework_version='1.0.0',
                             dependencies=['./code/labels.txt']
                            )

predictor = pytorch_model.deploy(
    instance_type='ml.t2.medium', 
    initial_instance_count=1)
</code></pre>

<p><strong>ERROR</strong></p>

<pre><code>algo-1-esw8d_1  | [2020-04-29 19:33:40 +0000] [22] [ERROR] Error handling request /ping
algo-1-esw8d_1  | Traceback (most recent call last):
algo-1-esw8d_1  |   File ""/usr/local/lib/python3.6/dist-packages/sagemaker_containers/_functions.py"", line 85, in wrapper
algo-1-esw8d_1  |     return fn(*args, **kwargs)
algo-1-esw8d_1  |   File ""/usr/local/lib/python3.6/dist-packages/train.py"", line 49, in model_fn
algo-1-esw8d_1  |     labels = load_labels()
algo-1-esw8d_1  |   File ""/usr/local/lib/python3.6/dist-packages/train.py"", line 25, in load_labels
algo-1-esw8d_1  |     with open(os.path.join(file_name_category)) as label_file:
algo-1-esw8d_1  | FileNotFoundError: [Errno 2] No such file or directory: 'labels.txt'
</code></pre>",0,0,2020-04-29 19:48:32.783000 UTC,,,1,amazon-web-services|amazon-sagemaker,62,2019-02-26 21:26:09.200000 UTC,2021-09-04 18:51:36.037000 UTC,,389,6,0,21,,,,,,['amazon-sagemaker']
What is the best practice to develop CD/CI when you use ML studio APIs?,"<p>In our backend development process, we have two environments: testing and production. We develop our code, and then we push the code into the testing repository. Then on the release date, we push everything into production. </p>

<p>Now that we are going to use ML studio, I'm struggling with setting up testing and production environments for my ML studio experiments.</p>

<p>I created two identical experiments with independent APIs; one experiment for testing and the other experiment is used by the production. When it comes to moving the trained experiment from testing to production, I make all the changes I made in the testing environment to the production environment, which is a very time demanding process. </p>

<p>Do you know any better solution so we can deploy and test our changes and then deploy the latest changes to the production? How people use ML studio in their CD/CI process?</p>

<p>The attached image shows the design that I have now. I'd appreciate if you can help me in improving this process. Maybe ML studio has some features to manage this scenario that I don't know.</p>

<p><img src=""https://i.stack.imgur.com/eBcuP.jpg"" alt=""""></p>",1,0,2018-08-21 00:33:31.857000 UTC,,2018-08-21 03:35:00.183000 UTC,0,azure-machine-learning-studio|ml-studio,51,2017-07-27 00:12:26.137000 UTC,2020-11-19 13:36:00.497000 UTC,Australia,37,2,0,8,,,,,,['azure-machine-learning-studio']
Track input transformation for keras-flavored mlflow models,"<p><strong>TL/DR:</strong> How to track and serve the input transformation for keras-flavored mlflow models?</p>
<p>Neural network training usually involves preprocessing steps in which</p>
<ul>
<li>continuous variables are scaled and shifted to have unit width and zero mean,</li>
<li>categorical variables (integer or string) are transformed to one-hot encoding.</li>
</ul>
<p>When the model is applied to new data, the scaling weights and the category-to-index association needs to be known.</p>
<p>In keras there are generally two options to perform preprocessing:</p>
<ul>
<li><strong>Option 1:</strong> Using <a href=""https://keras.io/guides/preprocessing_layers/"" rel=""nofollow noreferrer"">preprocessing layers</a>, or</li>
<li><strong>Option 2:</strong> perform the transformation before the training when the dataset is loaded.</li>
</ul>
<p>With <strong>Option 1</strong>, the transformation is part of the model and will be automatically applied when the network is used and served as a <a href=""https://www.mlflow.org/docs/latest/models.html"" rel=""nofollow noreferrer"">mlflow model</a>.</p>
<p>My question concerns <strong>Option 2</strong>: What is the recommended way</p>
<ul>
<li>to keep track of the input transformation in mlflow for different experiments,</li>
<li>and how to apply the same transformations when the model is served, e.g. with <code>mlflow model serve</code>?</li>
</ul>",0,0,2022-08-18 08:18:00.010000 UTC,,2022-08-20 07:11:36.130000 UTC,1,keras|mlflow,49,2011-04-06 04:40:33.600000 UTC,2022-09-22 18:26:40.727000 UTC,,3635,460,7,424,,,,,,['mlflow']
How to import modules in Azure Machine Learning run script?,"<p>I am new to Azure Machine Learning and have been struggling with importing modules into my run script. I am using the AzureML SDK for Python. I think I somehow have to append the script location to PYTHONPATH, but have been unable to do so.</p>
<p>To illustrate the problem, assume I have the following project directory:</p>
<pre><code>project/
   src/
      utilities.py
      test.py
   run.py
   requirements.txt
</code></pre>
<p>I want to run test.py on a compute instance on AzureML and I submit the run via run.py.
A simple version of run.py looks as follows:</p>
<pre><code>from azureml.core import Workspace, Experiment, ScriptRunConfig
from azureml.core.compute import ComputeInstance
ws = Workspace.get(...) # my credentials here
env = Environment.from_pip_requirements(name='test-env', file_path='requirements.txt')
instance = ComputeInstance(ws, '&lt;instance-name&gt;')
config = ScriptRunConfig(source_directory='./src', script='test.py', environment=env, compute_target=instance)
run = exp.submit(config)
run.wait_for_completion()
</code></pre>
<p>Now, test.py imports functions from utilities.py, e.g.:</p>
<pre><code>from src.utilities import test_func
test_func()
</code></pre>
<p>Then, when I submit a run, I get the error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;src/test.py&quot;, line 13, in &lt;module&gt;
    from src.utilities import test_func
ModuleNotFoundError: No module named 'src.utilities'; 'src' is not a package
</code></pre>
<p>This looks like a standard error where the directory is not appended to the Python path. I tried two things to get rid of it:</p>
<ol>
<li>include an <code>__init__.py</code> file in src. This didn't work and I would also for various reasons prefer not to use <code>__init__.py</code> files anyways.</li>
<li>fiddle with the environment_variables passed to AzureML like so
<code>env.environment_variables={'PYTHONPATH': f'./src:${{PYTHONPATH}}'</code> but that didn't really work either and I assume that is simply not the correct way to append the PYTHONPATH</li>
</ol>
<p>I would greatly appreciate any suggestions on extending PYTHONPATH or any other ways to import modules when running a script in AzureML.</p>",1,0,2022-05-17 19:34:11.403000 UTC,,2022-05-17 19:56:03.573000 UTC,2,python|azure-machine-learning-service|azureml-python-sdk,204,2015-08-10 14:51:09.390000 UTC,2022-09-23 18:15:01.693000 UTC,,73,3,0,6,,,,,,['azure-machine-learning-service']
AWS SageMaker ML DevOps tooling / architecture - Kubeflow?,"<p>I'm tasked with defining AWS tools for ML development at a medium-sized company. Assume about a dozen ML engineers plus other DevOps staff familiar with serverless ( lambdas and the framework ). The main questions are: a) what is an architecture that allows for the main tasks related to ML development (creating, training, fitting models, data pre-processing, hyper parameter optimization, job management, wrapping serverless services, gathering model metrics, etc ), b) what are the main tools that can be used for packaging and deploying things and c) what are the development tools (IDEs, SDKs, 'frameworks' ) used for it?
I just want to set Jupyter notebooks aside for a second. Jupyter notebooks are great for proof-of-concepts and the closest thing to PowerPoint for management... But I have a problem with notebooks when thinking about deployable units of code.<br />
My intuition points to a preliminary target architecture with 5 parts:</p>
<p>1 - A 'core' with ML models supporting basic model operations (create blank, create pre-trained, train, test/fit, etc). I foresee core Python scripts here - no problem.</p>
<p>2- (optional) A 'containerized-set-of-things' that performs hyper parameter optimization and/or model versioning</p>
<p>3- A 'contained-unit-of-Python-scripts-around-models' that exposes an API and that does job management and incorporates data pre-processing. This also reads and writes to S3 buckets.</p>
<p>4-  A 'serverless layer' with high level API ( in Python ). It talks to #3 and/or #1 above.</p>
<p>5- Some container or bundling thing that will unpack files from Git and deploy them onto various AWS services creating things from the previous 3 points.</p>
<p>As you can see, my terms are rather fuzzy:)  If someone can be specific with terms that will be helpful.
My intuition and my preliminary readings say that the answer will likely include a local IDE like PyCharm or Anaconda or a cloud-based IDE (what can these be? - don't mention notebooks please).
The point that I'm not really clear about is #5. Candidates include Amazon SageMaker Components for Kubeflow Pipelines and/or Amazon SageMaker Components for Kubeflow Pipelines and/or AWS Step Functions DS SDK For SageMaker. It's unclear to me how they can perform #5, however. Kubeflow looks very interesting but does it have enough adoption or will it die in 2 years? Are Amazon SageMaker Components for Kubeflow Pipelines, Amazon SageMaker Components for Kubeflow Pipelines and AWS Step Functions DS SDK For SageMaker mutually exclusive? How can each of them help with 'containerizing things' and with basic provisioning and deployment tasks?</p>",1,0,2020-12-07 23:40:25.877000 UTC,,,0,machine-learning|amazon-sagemaker|kubeflow-pipelines,762,2018-02-26 05:01:59.587000 UTC,2022-09-22 18:07:44.033000 UTC,,123,7,0,0,,,,,,['amazon-sagemaker']
What is the correct way to read AWS Glue output files into a TensorFlow batch transform on SageMaker,"<p>I have a Glue job that outputs a .out file into S3. The format of this file is fine for training a TensorFlow model on SageMaker (using script mode), but I am struggling to parse this data when running a batch transform.</p>

<p>I'm using the input_handler and output_handler functions as per the preferred inference.py scripting approach that is recommended, but I'm not exactly sure if I should treat the .out file as application/json, or text/csv, or something else entirely.</p>

<p>Example of the inference.py file: <a href=""https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker_batch_transform/tensorflow_cifar-10_with_inference_script/code/inference.py"" rel=""nofollow noreferrer"">https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker_batch_transform/tensorflow_cifar-10_with_inference_script/code/inference.py</a></p>",1,0,2020-04-07 10:31:37.577000 UTC,1.0,,0,python|tensorflow|tensorflow-serving|amazon-sagemaker,415,2015-07-17 14:41:05.690000 UTC,2021-08-15 10:39:11.857000 UTC,"Cape Town, South Africa",530,106,0,89,,,,,,['amazon-sagemaker']
Can't connect to Azure ML Web Service in Azure Data Factory,"<p>I deployed an automated ML in Azure Machine Learning Studio and I have my endpoint url (that is a http, not a https ?) and my primary key.</p>

<p>I got in Azure Data Factory, try to create a Azure ML Studio linked services to use the Azure Batch Execution, I put my http endpoint in https and my primary key but every time i try, if i put http, it's 404 error and if i put https, i got a timeout.</p>

<p>Anyone can help me ?</p>",0,2,2020-04-25 01:17:37.410000 UTC,,,0,azure-data-factory|azure-machine-learning-studio,175,2019-07-23 13:07:37.490000 UTC,2020-06-01 23:20:50.110000 UTC,"Paris, France",143,3,0,8,,,,,,['azure-machine-learning-studio']
What DVC does when git merge is executed?,<p>I have two git branches (master and develop). DVC maps a data folder in both of them. When I go into master and merging with develop is correct that DVC does not add any new file inside the data folder created in the develop branch but leaves the folder as it is unchanged?</p>,1,0,2022-09-01 10:15:31.777000 UTC,,2022-09-02 05:40:50.203000 UTC,1,git|dvc,40,2020-02-02 18:40:04.397000 UTC,2022-09-24 14:57:14.117000 UTC,,498,453,9,66,,,,,,['dvc']
can mlflow.spark's saved model loaded as Spark/Scala Pipeline?,<p>Our algorithm engineer is developing machine learning model using pyspark &amp; mlflow. He's trying to save the model using <code>mlflow.spark</code> API &amp; the model format is the native <code>spark MLlib</code> format. Could the model be loaded from <code>Spark Scala</code> code? It seems that mlflow is quite restricted for cross-language usage.</p>,1,0,2020-06-19 09:04:39.793000 UTC,1.0,,1,scala|pyspark|mlflow,630,2019-03-25 09:33:34.607000 UTC,2021-03-04 14:00:13.497000 UTC,,129,0,0,47,,,,,,['mlflow']
create and run notebooks in azure ML studio by terraform or powershell,"<p>I am trying to automate the process of creation of ML Studio along with compute instance and notebook and to run the notebook. I have a code that creates ML studio workspace but not finding any leads on creating the notebook in it to run.</p>
<pre><code># retrive current connection data
data &quot;azurerm_client_config&quot; &quot;current&quot; {}

    # Create App Insight
    resource &quot;azurerm_application_insights&quot; &quot;AML&quot; {
      name                = &quot;${var.prefix}iotMLInsights&quot;
      location            = var.resource_group_location
      resource_group_name = var.resource_group_name
      application_type    = &quot;web&quot;
    }
    
    # Create Azure Key Vault
    resource &quot;azurerm_key_vault&quot; &quot;AML&quot; {
      name                = &quot;${var.prefix}iotKeyVault&quot;
      location            = var.resource_group_location
      resource_group_name = var.resource_group_name
      tenant_id           = data.azurerm_client_config.current.tenant_id
      sku_name            = &quot;premium&quot;
    }
    
    # Create Azure ML Service
    resource &quot;azurerm_machine_learning_workspace&quot; &quot;AML&quot; {
      name                    = &quot;${var.prefix}iotMLStudio&quot;
      location                = var.resource_group_location
      resource_group_name     = var.resource_group_name
      application_insights_id = azurerm_application_insights.AML.id
      key_vault_id            = azurerm_key_vault.AML.id
      storage_account_id      = var.storage_account_id
    
      identity {
        type = &quot;SystemAssigned&quot;
      }
    }
</code></pre>
<p>need some suggestions to take this forward, either in terraform or powershell.</p>",0,0,2022-08-20 17:16:01.113000 UTC,,,0,powershell|terraform|azure-machine-learning-service|rnotebook,50,2022-07-20 10:15:16.010000 UTC,2022-09-13 11:42:26.987000 UTC,,17,0,0,28,,,,,,['azure-machine-learning-service']
Features extraction in Real-time prediction in sagemaker,"<p>i want to deploy a real time prediction machine learning model for fraud detection using sagemaker.</p>
<p>i used sagemaker jupyter instance to:</p>
<pre><code>-load my training data from s3 contains transactions
-preprocessing data and features engineering (i use category_encoders to encode the categorical value)
-training the model and configure the endpoint
</code></pre>
<p>For the inference step , i used a lambda function which  invoke my endpoint to get the prediction for each real time transaction.</p>
<pre><code>should i calculte again all the features for this real time transactions in lambda function ?

for the features when i use category_encoders with fit_transform() function to transform my categorical feature to numerical one, what should I do because the result will not be the same as training set?

is there another method not to redo the calculation of the features in the inference step?
</code></pre>",1,1,2021-05-06 16:37:27.827000 UTC,,,0,amazon-web-services|machine-learning|lambda|amazon-sagemaker|fraud-prevention,85,2020-11-24 12:41:16.633000 UTC,2022-08-15 22:26:53.737000 UTC,,53,0,0,11,,,,,,['amazon-sagemaker']
Can't use wildcard with Azure Data Lake Gen2 files,"<p>I was able to properly connect my Data Lake Gen2 Storage Account with my Azure ML Workspace. When trying to read a specific set of Parquet files from the Datastore, it will take forever and will not load it.</p>
<p>The code looks like:</p>
<pre><code>from azureml.core import Workspace, Datastore, Dataset
from azureml.data.datapath import DataPath

ws = Workspace(subscription_id, resource_group, workspace_name)

datastore = Datastore.get(ws, 'my-datastore')

files_path = 'Brazil/CommandCenter/Invoices/dt_folder=2020-05-11/*.parquet'

dataset = Dataset.Tabular.from_parquet_files(path=[DataPath(datastore, files_path)], validate=False)
df = dataset.take(1000)

df.to_pandas_dataframe()
</code></pre>
<p>Each of these Parquet files have approx. 300kB. There are 200 of them on the folder - generic and straight out of Databricks. Strange is that when I try and read one single parquet file from the exact same folder, it runs smoothly.</p>
<p>Second is that other folders that contain less than say 20 files, will also run smoothly, so I eliminated the possibility that this was due to some connectivity issue. And even stranger is that I tried the wildcard like the following:</p>
<pre><code># files_path = 'Brazil/CommandCenter/Invoices/dt_folder=2020-05-11/part-00000-*.parquet'
</code></pre>
<p>And theoretically this will only direct me to the <code>00000</code> file, but it will also not load. Super weird.</p>
<p>To try to overcome this, I have tried to connect to the Data Lake through ADLFS with Dask, and it just works. I know this can be a workaround for processing &quot;large&quot; datasets/files, but it would be super nice to do it straight from the Dataset class methods.</p>
<p>Any thoughts?</p>
<p>EDIT: typo</p>",1,4,2020-10-29 21:03:17.523000 UTC,,,1,azure|machine-learning|azure-machine-learning-service,403,2017-10-25 09:33:44.027000 UTC,2022-09-23 18:31:48.303000 UTC,,118,6,0,17,,,,,,['azure-machine-learning-service']
How to fine tune a model from hugging face?,"<p>I want to download a pretrained a model and fine tune the model with my own data. I have downloaded a bert-large-NER model artifacts from hugging face,I have listed the contents below . being new to this, I want to know what files or artifacts do i need and from the looks of it the pytorch_model.bin is the trained model, but what are these others file and their purpose like tokenizer files and vocab.txt ....</p>
<pre><code>config.json
pytorch_model.bin
special_tokens_map.json
tokenizer_config.json
vocab.txt
</code></pre>",1,0,2022-09-03 22:10:02.923000 UTC,,,-1,machine-learning|amazon-sagemaker|huggingface-tokenizers|huggingface,43,2021-06-22 17:06:55.117000 UTC,2022-09-24 17:08:41.977000 UTC,,95,10,0,16,,,,,,['amazon-sagemaker']
How does AWS SageMaker Feature Store know what transformations to perform when ingesting data?,"<p>When you create feature groups in SageMaker Feature Store you take the following steps:</p>
<ol>
<li>Setup SageMaker Python SDK and boto client</li>
<li>Inspect data we want to use, and apply transformations (e.g. remove NAs, round numbers, etc.)</li>
<li>Ingest transformed data into feature store</li>
<li>Build training data by running Athena query on Feature Groups</li>
<li>Select columns for training</li>
<li>Save training dataset to S3 bucket</li>
<li>Train and deploy model</li>
<li>Use GetRecord functionality to make prediction on recent data from feature store.</li>
</ol>
<p>You can see a detailed example of the steps <a href=""https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-featurestore/sagemaker_featurestore_fraud_detection_python_sdk.html"" rel=""nofollow noreferrer"">here</a>.</p>
<p>But how does the feature store apply the transformations to the data prior to making a prediction? Obviously the newly ingested data must be transformed (so it's the same as in training), but we only made these transformations in step 2, BEFORE anything was added to a feature group. It doesn't appear as though following these steps allows the feature store to have any knowledge of the transformations.</p>
<p>For example, in the linked example they add the transformed data to the <code>transaction_feature_group</code> as follows:</p>
<pre><code>transaction_feature_group.ingest(data_frame=transformed_transaction_data, max_workers=5, wait=True)
</code></pre>
<p>So we can see that the transformed data is what gets loaded into the feature group. But what about new data added over time? How is this new data getting automatically transformed?</p>",1,0,2021-07-12 15:01:38.950000 UTC,,2021-07-12 15:07:41.710000 UTC,0,python|amazon-web-services|amazon-sagemaker|feature-store|aws-feature-store,343,2012-08-31 20:08:40.090000 UTC,2022-09-25 04:17:41.297000 UTC,,11650,6318,21,977,,,,,,['amazon-sagemaker']
How to run a jupyter notebook programmatically (inside a Sagemaker notebook) from a local environment,"<p>I can start/stop Sagemaker notebooks with boto3, but how do run the jupyter notebooks or <code>.py</code> scripts inside?</p>

<p>This is something I'll run from a local environment or lambda (but that's no problem).</p>

<p><strong>Start Sagemaker notebook instance:</strong></p>

<pre><code>import boto3

client = boto3.client('sagemaker')

client.start_notebook_instance(
    NotebookInstanceName='sagemaker-notebook-name'
)
</code></pre>

<p><a href=""https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.start_notebook_instance"" rel=""noreferrer"">docs</a></p>

<p>In the UI I would just click ""Open Jupyter"", then run a notebook or a <code>.py</code> script inside it.</p>

<p><a href=""https://i.stack.imgur.com/tGVGx.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/tGVGx.png"" alt=""enter image description here""></a></p>

<p>But I want to do it programmatically with boto3 or other.</p>

<p>My file inside is called <code>lemmatize-input-data.ipynb</code>.</p>

<p>This must be possible but I'm unsure how?</p>

<p><strong>I also tried:</strong></p>

<p>In a ""start notebook"" lifecycle configuration script, after creating a simpler test file called <code>test_script.ipynb</code> to be certain it wasn't something advanced in my jupyter notebook that caused the error.</p>

<pre><code>set -e

jupyter nbconvert --execute test_script.ipynb
</code></pre>

<p>But got the error:</p>

<blockquote>
  <p>[NbConvertApp] WARNING | pattern 'test_script.ipynb' matched no files</p>
</blockquote>",3,1,2019-05-13 23:26:15.813000 UTC,0.0,2019-05-14 22:08:59.820000 UTC,5,python|amazon-web-services|jupyter-notebook|boto3|amazon-sagemaker,6693,2015-05-17 14:29:15.547000 UTC,2022-09-13 18:51:39.033000 UTC,Unknown,9923,288,0,530,,,,,,['amazon-sagemaker']
Why I got an invalid bucket name error using dvc mlflow on macos,"<p>Could anyone tell what's the reason for error:</p>
<p>botocore.exceptions.ParamValidationError: Parameter validation failed:
Invalid bucket name &quot;&quot;: Bucket name must match the regex &quot;^[a-zA-Z0-9.-_]{1,255}$&quot; or be an ARN matching the regex &quot;^arn:(aws).<em>:(s3|s3-object-lambda):[a-z-0-9]</em>:[0-9]{12}:accesspoint[/:][a-zA-Z0-9-.]{1,63}$|^arn:(aws).*:s3-outposts:[a-z-0-9]+:[0-9]{12}:outpost[/:][a-zA-Z0-9-]{1,63}[/:]accesspoint[/:][a-zA-Z0-9-]{1,63}$&quot;</p>
<p>I try to use mlflow with docker.
.env file contains:</p>
<pre><code>AWS_ACCESS_KEY_ID=...
AWS_SECRET_ACCESS_KEY=...
AWS_S3_BUCKET=vla...rts
MLFLOW_S3_ENDPOINT_URL=http://localhost:9000
MLFLOW_TRACKING_URI=http://127.0.0.1:5000
POSTGRES_USER=...
POSTGRES_PASSWORD=...
POSTGRES_DB=test_db
</code></pre>
<p>Also tried to use:</p>
<pre><code>AWS_ACCESS_KEY_ID=...
AWS_SECRET_ACCESS_KEY=...
AWS_S3_BUCKET=vla...rts
MLFLOW_S3_ENDPOINT_URL=http://localhost:9000
MLFLOW_TRACKING_URI=http://localhost:5000
POSTGRES_USER=...
POSTGRES_PASSWORD=...
POSTGRES_DB=test_db
</code></pre>
<p>docker-compose contains:</p>
<pre><code>... 
   mlflow:
        restart: always
        image: mlflow_server
        container_name: mlflow_server
        ports:
          - &quot;5000:5000&quot;
        networks:
          - postgres
          - s3
        environment:
          - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
          - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
          - MLFLOW_S3_ENDPOINT_URL=http://nginx:9000
        command: mlflow server --backend-store-uri postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@db/${POSTGRES_DB} --default-artifact-root s3://${AWS_S3_BUCKET}/ --host 0.0.0.0
...
</code></pre>
<p>As I understood, I get an exception cause bucket name is empty (&quot;&quot;). But in .env file I set bucket name as <code>vla...rts</code></p>",2,4,2022-05-21 21:10:53.863000 UTC,,2022-05-24 06:16:27.987000 UTC,0,docker|mlflow|mlops|dvc,117,2019-12-29 15:05:47.330000 UTC,2022-09-20 12:21:31.067000 UTC,Saint Petersburg,21,0,0,8,,,,,,"['mlflow', 'dvc']"
Error 400 Bad Request Post Request to MLFLow API of dockerized image processing onnx-model,"<p>For testing purposes i try to deploy the <a href=""https://github.com/google/mediapipe/blob/master/mediapipe/modules/hand_landmark/hand_landmark_full.tflite"" rel=""nofollow noreferrer"">MediaPipe Hands</a> model with MLFlow in docker.<br />
The model excpects the input <code>{'input_1': img}</code>. <code>img</code> is an 4dim numpy array <code>(1,224,224,3)</code> in <code>float32</code>
Everytime I send it, i get an <code>http 400 Error</code>.<br />
I think its the wrong input Format, as the MLFlow API only supports</p>
<ol>
<li>&quot;JSON-serialized pandas DataFrames in the split orientation&quot;</li>
<li>&quot;JSON-serialized pandas DataFrames in the records orientation&quot;</li>
<li>&quot;CSV-serialized pandas DataFrames&quot;</li>
<li>&quot;Tensor input formatted as described in <a href=""https://www.tensorflow.org/tfx/serving/api_rest#request_format_2"" rel=""nofollow noreferrer"">TF Serving’s API docs</a> where the provided inputs will be cast to Numpy arrays&quot;</li>
</ol>
<p>In my opinion I have to convert it to a Tensor Input as pandas only support 2 dimensions and my np array has 4dim. I tried the wohle day, but the result is still the same 400 http error:  <code>requests.exceptions.HTTPError: 400 Client Error: BAD REQUEST for url: http://127.0.0.1:5001/invocations</code>  Whole request in<strong>5.</strong></p>
<p><strong>To reproduce:</strong><br />
<strong>1. Convert Model to ONNX</strong><br />
As MLFlow doesn't support tflite models, I used python and tf2onnx
<code>!pip install tensorflow onnxruntime tf2onnx</code></p>
<pre><code>import tf2onnx
tf2onnx.convert.from_tflite(&quot;hand_model/hand_landmark_full.tflite&quot;, output_path=&quot;hand_model/hand_landmark_full2.onnx&quot;);
</code></pre>
<p><strong>2. Log the model in MLFlow with python</strong>
<code>pip install mlflow onnx</code></p>
<pre><code>import mlflow
import onnx
import os
from mlflow.tracking import MlflowClient

mlflow_client = MlflowClient()
EXPERIMENT_NAME = &quot;ONNX_Hand&quot;
experiment_details = mlflow_client.get_experiment_by_name(EXPERIMENT_NAME)

if experiment_details is not None:
    experiment_id = experiment_details.experiment_id
else:
    experiment_id = mlflow.create_experiment(EXPERIMENT_NAME)
with mlflow.start_run(experiment_id=experiment_id, run_name=&quot;handdatasetrfrun&quot;) as run:
    model = onnx.load(&quot;./onnx/hand_landmark_full2.onnx&quot;)
    mlflow.onnx.log_model(model,artifact_path=&quot;model&quot;)
run_id = run.info.run_id
print('Run ID: {}'.format(run_id))
</code></pre>
<p><strong>3. I tried to access the model</strong><br />
I took a random picture of my hand an formated it the way, the model expects:   <a href=""https://drive.google.com/file/d/1-rmIgTfuCbBPW_IFHkh3f0-U_lnGrWpg/preview"" rel=""nofollow noreferrer"">Model card</a></p>
<pre><code>import mlflow
import numpy as np
import cv2

##read image and process it:
img = cv2.imread(&quot;hand.JPG&quot;)
#RBG-&gt;RGB
RGB_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
#224x224
resized_img=cv2.resize(RGB_img,dsize=(224,224))
#flip around y-Achsis
image_data=cv2.flip(resized_img, 1)
float_image = image_data.astype(np.float32)
im=np.array([np.divide(float_image,255)])
print(im.shape)
logged_model = 'C:/Workspace/MLFrameworks/mpHanddetection/mlruns/1/9a44d5671a6140988fbafaa939c6f9d9/artifacts/model'
loaded_model = mlflow.pyfunc.load_model(logged_model)
data={'input_1': im}

#random numpy array works fine too
#data={'input_1':(np.array(np.random.random_sample(input_shape), dtype=np.float32))}

#predict
predictions = loaded_model.predict(data)
print(predictions)
</code></pre>
<p>until here, everything works fine.<br />
<strong>4. build a docker image of the Model</strong><br />
<code>mlflow models build-docker -m &quot;C:/Workspace/MLFrameworks/mpHanddetection/mlruns/1/9a44d5671a6140988fbafaa939c6f9d9/artifacts/model&quot; -n &quot;handmodel&quot;  </code><br />
<code>docker run -p 5001:8080 &quot;handmodel&quot;</code><br />
here I get some User warnings:</p>
<pre><code>/miniconda/envs/custom_env/lib/python3.10/site-packages/numpy/core/getlimits.py:500: UserWarning: The value of the smallest subnormal for &lt;class 'numpy.float64'&gt; type is zero.
  setattr(self, word, getattr(machar, word).flat[0])
/miniconda/envs/custom_env/lib/python3.10/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for 
&lt;class 'numpy.float64'&gt; type is zero.
</code></pre>
<p><strong>5.Error 400: Access the model via python requests:</strong></p>
<pre><code>import cv2
import requests
import numpy as np

img = cv2.imread(&quot;C:/Workspace/MLFrameworks/mpHanddetection/hand.JPG&quot;)
#RBG-&gt;RGB
RGB_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
#224x224
resized_img=cv2.resize(RGB_img,dsize=(224,224))
#flip around y-Achsis
image_data=cv2.flip(resized_img, 1)
float_image = image_data.astype(np.float32)
im=np.divide([float_image],255)
headers = {&quot;content-type&quot;: &quot;application/json&quot;}
response = requests.post(url=&quot;http://127.0.0.1:5001/invocations&quot;,   
                            data={&quot;inputs&quot;:{'input_1': im}},
                            headers=headers)
print(response.raise_for_status())
print(response.reason)
print(response.status_code)
</code></pre>
<p>log:</p>
<pre><code>File &quot;c:\Workspace\MLFrameworks\mpHanddetection\Mlflow_hand.py&quot;, line 36, in &lt;module&gt;
   print(response.raise_for_status())
 File &quot;C:\Python310\lib\site-packages\requests\models.py&quot;, line 953, in raise_for_status
   raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 400 Client Error: BAD REQUEST for url: http://127.0.0.1:5001/invocations
</code></pre>",0,0,2022-08-12 11:48:41.913000 UTC,,2022-08-14 14:21:22.537000 UTC,0,docker|numpy|machine-learning|onnx|mlflow,55,2022-08-12 10:22:04.070000 UTC,2022-09-22 08:17:44.353000 UTC,,1,0,0,0,,,,,,['mlflow']
How to load an .RData file to AWS sagemaker notebook?,"<p>I just started using the AWS sagemaker and I have an xgboost model saved in my personal laptop using save as .Rdata, saveRDS, xgb.save commands. I have uploaded those files in my Sagemaker notebook instance where my different notebooks are. However, I am unable to load it to my environment and predict for test data by using the following commands:</p>
<pre><code>load(&quot;Model.RData&quot;)
model=xgb.load('model')
model &lt;- readRDS(&quot;Model.rds&quot;)
</code></pre>
<p>When I predict, I get NAs as my prediction. These commands work fine on Rstudio but not on sagemaker notebook.Please help</p>",0,4,2020-08-18 22:23:55.370000 UTC,,2020-08-18 23:28:39.477000 UTC,0,amazon-web-services|amazon-sagemaker,180,2019-11-30 13:08:31.447000 UTC,2021-11-12 18:03:37.127000 UTC,,3,0,0,7,,,,,,['amazon-sagemaker']
MLflow FileNotFound when calling spark_udf from pycharm with databricks-connect,"<p>I am saving a scikit-learn trained model to MLflow's model registry in my Windows laptop. I am using databricks-connect to connect to an Azure Databricks cluster and train models there from my local Pycharm, but for development I launch the model registry in my laptopt and save the trained models in it - to avoid having to set up accessing DBFS remotely.</p>
<p>I am having trouble with the spark_udf function. <strong>I can read the model from my laptop's model registry but cannot use it as a UDF in my Databricks cluster</strong>:</p>
<pre><code>model_udf = mlflow.pyfunc.spark_udf(spark, &quot;models:/mymodel/production&quot;)  # this works fine
struct_col = F.struct(*df.columns)
predictions = df.withColumn(&quot;pred_spark&quot;, model_udf(struct_col))  
predictions.show()                                        # throws an exception(see below)
</code></pre>
<p>pyspark.sql.utils.PythonException: An exception was thrown from a UDF: 'FileNotFoundError:</p>
<p>[Errno 2] No such file or directory:</p>
<p>'/local_disk0/spark-1fa39b20-9d2c-4697-957c-392d80326dee/executor-57b039d8-7405-47c4-b072-612e9b87b3dd/spark-e442241d-4007-4c6e-8acd-bf2a35b1a455/isolatedSparkFiles/044cd765-f5f7-46b3-9efb-0944cc91ef4d/c:\temp\tmpsl4hpeyt.zip'</p>
<p>The last part is weird as it mixes linux-style routes with Windows route (like a route in my Windows laptop's local dir). I thought that the driver would read the model from the model registry and broadcast it to the workers to call the UDF, but looks like the workers are trying to fetch it directly from the remote model registry, is this right? Is there a solution that does not require saving the model in the remote model registry - or at least, configuring the security to access DBFS?</p>
<p><strong>EDIT:</strong> After having set up registering the model into the remote registry (<a href=""https://docs.databricks.com/applications/mlflow/access-hosted-tracking-server.html"" rel=""nofollow noreferrer"">not that difficult!</a>) I can now download the registered model as a sklearn model and do predictions with it, but I cannot do the same via spark_udf. I get either</p>
<ul>
<li>The same FileNotFound error if I specify the model via runs:/.... in function spark_udf</li>
<li>An SSL error (unverified self-signed certificate) if I specify the model via model:/... in the spark_udf, even though I have explicitly added a new line <code>insecure = True</code> in my .databrickscfg - and because that was not working, also added <code>os.environ[&quot;MLFLOW_TRACKING_INSECURE_TLS&quot;] = &quot;true&quot;</code> but has no effect. I guess it is doing something because at least I can download the model from the registry when it is not a spark_udf, but looks like &quot;the workers?&quot; are having issues when they try to do the same (that's my guess, no idea if it makes sense)</li>
</ul>",1,0,2021-11-07 14:35:05.043000 UTC,,2021-11-08 21:13:35.477000 UTC,1,apache-spark|pyspark|mlflow|databricks-connect,180,2015-12-29 08:10:18.090000 UTC,2022-09-21 18:17:26.213000 UTC,"Madrid, Spain",308,10,1,75,,,,,,['mlflow']
Amazon SageMaker Model Monitor for Batch Transform jobs,"<p>Couldn't find the right place to ask this, so doing it here.</p>
<p>Does Model Monitor support monitoring Batch Transform jobs, or only endpoints? The documentation seems to only reference endpoints...</p>",1,2,2021-10-28 02:17:35.380000 UTC,3.0,,3,amazon-sagemaker,572,2016-10-08 02:58:32.717000 UTC,2022-09-03 00:57:16.447000 UTC,"Melbourne VIC, Australia",2304,277,33,166,,,,,,['amazon-sagemaker']
Python Notebook Invoke Endpoint Sagemaker from Local,"<p>I am trying to invoke an Amazon Sagemaker Endpoint from a local python notebook. This is the code I am using.  </p>

<pre><code>import boto3

aws_access_key_id = '...............'
aws_secret_access_key = '................'
tkn = '..........'
region_name = '............'

amz = boto3.client('sagemaker-runtime',
                   aws_access_key_id=aws_access_key_id,
                   aws_secret_access_key=aws_secret_access_key,
                   aws_session_token=tkn,
                   region_name=region_name)


response = amz.invoke_endpoint(
    EndpointName='mymodel',
    Body=b'bytes'
)               
</code></pre>

<p>However, this doesn't work. Do I have to specify something else in <em>Body</em> ?   </p>",2,0,2018-06-15 08:19:27.617000 UTC,,,1,python|amazon-sagemaker,3972,2015-05-26 22:53:10.120000 UTC,2019-06-03 07:51:39.510000 UTC,,455,7,0,58,,,,,,['amazon-sagemaker']
Sagemaker charge from ListBucket,"<p>Looking at the breakdown of charges from AWS Sagemaker, I noticed only about 30% of total cost is from actually running the instances, surprisingly <strong>~50 percent</strong> come from S3 (shows as ListBucket) and 20% for other overhead. I wonder if there is a way to decrease this massive extra charge from S3.</p>
<p>To give more background, I run hundreds of training jobs each roughly 3 hours long, and the data is hundreds of pickle files zipped into a tar.gz file of size ~10G (gets unzipped in the instance).
So If I run 1000 jobs on instances with pricing $0.1/hr, I expect to see around $300 charge (1000 jobs * 3 hours * $0.1), however it ends up being close to $1000 with around $500 coming from &quot;ListBucket&quot;!!
I wonder where this comes from, since the s3 folder with training data is simply a single zipped file, why would ListBucket cost so much?</p>",0,4,2022-05-06 16:03:36.210000 UTC,,2022-05-06 16:28:29.077000 UTC,0,amazon-web-services|amazon-s3|amazon-sagemaker,88,2021-05-30 15:23:01.327000 UTC,2022-06-06 14:05:53.057000 UTC,,103,1,0,0,,,,,,['amazon-sagemaker']
Convert Sting to DateTime in Python using Azure Machine Learning Studio,"<p>I am working on a Machine Learning model and am trying convert a string column into a Date one. I guess it has to be a Date and Time column.</p>

<p>To be more specific this is the Date column, after being uploaded to Azure ML Studio it automatically became a string. I have tried to use the Edit Metadata option but it is not letting me make this change. I have also tried just to amend the format of that column on the CSV file before uploading it but it is not working. I have also checked the forum and found that some people have experienced the same problem. However I tried to apply the same solution and did not work.</p>

<pre><code>import pandas as pd

def azureml_main(dataframe1 = None, dataframe2 = None):
    dataframe1.Date=pd.to_date(dataframe1.Date, error='coerce')
    return dataframe1
</code></pre>

<p>Any help is welcome.</p>

<p>Thank you</p>",1,0,2018-09-22 13:00:51.290000 UTC,,2018-09-22 13:55:12.710000 UTC,-1,python|pandas|azure|azure-machine-learning-studio,295,2018-09-22 12:43:15.620000 UTC,2018-10-06 21:57:09.783000 UTC,"London, UK",1,0,0,5,,,,,,['azure-machine-learning-studio']
About Amazon SageMaker Ground Truth save in S3 after label,"<p>I am setting up the automatic data labelling pipeline for my colleague.</p>

<p>First, I define the ground truth request based on API (bucket, manifests, etc).</p>

<p>Second, I create this labelling job, and all files are uploaded in S3 immediately.</p>

<p>After that my colleague will receive an email saying it is ready to label it, then he will label the data and submit.</p>

<p>Until now, everything is well and quick. Then I check the SageMaker labelling job dashboard, it shows the task is in progress, and it takes very very long time to know it is completed or failed. I don't know the reason. Yesterday, it saved the results at 4 am, took around 6 hours. But if I create label job on website instead of sending requests, it will save the results quickly.</p>

<p>Can anyone explain it? Or maybe I need to set up a time sync or other configuration?</p>

<p>This is my config:</p>

<pre><code>{
  ""InputConfig"": {
    ""DataSource"": {
      ""S3DataSource"": {
        ""ManifestS3Uri"": """"s3://{bucket_name}/{JOB_ID}/{manifest_name}-{JOB_ID}.manifest""""
      }
    },
    ""DataAttributes"": {
      ""ContentClassifiers"": [
        ""FreeOfPersonallyIdentifiableInformation"",
        ""FreeOfAdultContent""
      ]
    }
  },
  ""OutputConfig"": {
    ""S3OutputPath"": ""s3://{bucket_name}/{JOB_ID}/output-{manifest_name}/""
  },
  ""HumanTaskConfig"": {
    ""AnnotationConsolidationConfig"": {
      ""AnnotationConsolidationLambdaArn"": ""arn:aws:lambda:us-east-2:266458841044:function:PRE-TextMultiClass""
    },
    ""PreHumanTaskLambdaArn"": ""arn:aws:lambda:us-east-2:266458841044:function:PRE-TextMultiClass"",
    ""NumberOfHumanWorkersPerDataObject"": 2,
    ""TaskDescription"": ""Dear Annotator, please label it according to instructions. Thank you!"",
    ""TaskKeywords"": [
      ""text"",
      ""label""
    ],
    ""TaskTimeLimitInSeconds"": 600,
    ""TaskTitle"": ""Label Text"",
    ""UiConfig"": {
      ""UiTemplateS3Uri"": ""s3://{bucket_name}/instructions.template""
    },
    ""WorkteamArn"": ""work team arn""
  },
  ""LabelingJobName"": ""Label"",
  ""RoleArn"": ""my role arn"",
  ""LabelAttributeName"": ""category"",
  ""LabelCategoryConfigS3Uri"": """"s3://{bucket_name}/labels.json""""
}

</code></pre>

<p>I think my Lambda function is wrong, when I change to aws arn (preHuman and Annotation) everything works fine.</p>

<p>This is my afterLabeling Lambda:</p>

<pre><code>import json
import boto3
from urllib.parse import urlparse

def lambda_handler(event, context):
    consolidated_labels = []

    parsed_url = urlparse(event['payload']['s3Uri']);
    s3 = boto3.client('s3')
    textFile = s3.get_object(Bucket = parsed_url.netloc, Key = parsed_url.path[1:])
    filecont = textFile['Body'].read()
    annotations = json.loads(filecont);

    for dataset in annotations:
        for annotation in dataset['annotations']:
            new_annotation = json.loads(annotation['annotationData']['content'])
            label = {
                'datasetObjectId': dataset['datasetObjectId'],
                'consolidatedAnnotation' : {
                'content': {
                    event['labelAttributeName']: {
                        'workerId': annotation['workerId'],
                        'result': new_annotation,
                        'labeledContent': dataset['dataObject']
                        }
                    }
                }
            }
            consolidated_labels.append(label)

    return consolidated_labels
</code></pre>

<p>Are there any reasons?</p>",0,4,2019-09-19 21:44:40.923000 UTC,,2019-09-22 06:46:22.627000 UTC,0,python-3.x|amazon-web-services|amazon-s3|amazon-sagemaker,515,2019-04-15 15:16:13.253000 UTC,2019-12-20 17:11:37.727000 UTC,,1,0,0,2,,,,,,['amazon-sagemaker']
How to create partitioned Athena table with Sagemaker Feature Store,"<p>I'm using Sagemaker Feature Store and trying to create an Offline Feature Store. During the process, Sagemaker creates an Athena table. However, I notice that this table is not partitioned, and when I create a query, it takes forever.</p>
<p>How can I use Sagemaker Feature Store to create a Athena table with partition?</p>",0,0,2021-12-15 11:51:49.507000 UTC,,,1,amazon-sagemaker|feature-store|aws-feature-store,69,2016-10-17 11:39:35.777000 UTC,2022-07-28 17:26:21.417000 UTC,,109,1,0,21,,,,,,['amazon-sagemaker']
How to set a seed for hyperparameter tuning jobs in AWS SageMaker for reproductibility,"<p>I would like to run REPRODUCTIBLE experiments on AWS SageMaker.  Hyper-parameter tuning jobs are source of randomness either with Bayesian or Random strategies: each time you run them with the same data you obtain slightly different results since the search in the hyper-parameter space is pseudo-random. The variance of the result is significative as explained here: <a href=""https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-automatic-model-tuning-now-supports-random-search-and-hyperparameter-scaling/"" rel=""nofollow noreferrer"">https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-automatic-model-tuning-now-supports-random-search-and-hyperparameter-scaling/</a></p>
<p>Is there an way of setting a seed for these jobs? From the doc it does not seem to: <a href=""https://sagemaker.readthedocs.io/en/stable/api/training/tuner.html"" rel=""nofollow noreferrer"">https://sagemaker.readthedocs.io/en/stable/api/training/tuner.html</a></p>
<p>The only way I see to mitigate the effect of randomness is to run enough training jobs per hyperparameter tuning jobs.</p>",0,0,2020-12-22 12:05:22.843000 UTC,,,1,amazon-sagemaker|random-seed|hyperparameters,235,2014-07-30 10:25:22.230000 UTC,2022-09-22 19:20:13.617000 UTC,Belgium,1714,133,6,307,,,,,,['amazon-sagemaker']
Using ipyleaflet within a Vertex AI Managed Notebook running on a Docker image,"<p>TL;DR How does one get ipyleaflet to work in a Vertex AI Managed Notebook booted from a custom Docker image?</p>
<p><strong>Objective</strong></p>
<p>Following on <a href=""https://stackoverflow.com/questions/73360734/programmatically-enable-installed-extensions-in-vertex-ai-managed-notebook-insta/73369289?noredirect=1#comment129617711_73369289"">this thread</a>, I am working in JupyterLab within a Managed Notebook instance, accessed through the Vertex AI workbench, as part of a Google Cloud Project. I am trying to supply a custom Docker image, such that when a Jupyter Lab notebook is launched from the running instance, it (a) contains some modules for performing and visualizing cartographic analysis, and, importantly, (b) permits visualization with ipyleaflet and associated modules.</p>
<p><strong>What I've tried</strong></p>
<p>Thus far, I have succeed in creating a Docker image (derivative of <a href=""https://cloud.google.com/deep-learning-containers/docs/choosing-container"" rel=""nofollow noreferrer"">Google image</a>) that supplies, in the running Jupyter Lab, a dedicated environment (explicitly exposed kernel) with the correct modules (in particular, geemap, earthengine-api, ipyleaflet, ipywidgets). The modules are all importable and appear sound. However, so far as I can tell, supplying a custom Docker image during the build process, effectively breaks the ipyleaflet (and presumably widgets, events, etc) connection that Google's Jupyter Lab base image has if one creates a Managed Notebook <em>without</em> supplying a Docker image. Attempts to create map visualizations returns, &quot;Error displaying widget: model not found&quot;, discussed at <a href=""https://github.com/jupyter-widgets/ipyleaflet/issues/504"" rel=""nofollow noreferrer"">1</a>, <a href=""https://github.com/jupyter-widgets/ipyleaflet/issues/889"" rel=""nofollow noreferrer"">2</a>, <a href=""https://github.com/jupyter-widgets/ipyleaflet/issues/547"" rel=""nofollow noreferrer"">3</a>, <a href=""https://leafmap.org/faq/"" rel=""nofollow noreferrer"">4</a>. In other words, if one creates a Managed Notebook <em>without</em> a Docker image, starts the notebook instance, launches Jupyter Lab, opens a notebook, and then uses <code>%pip install xyz</code> for the modules of interest, ipyleaflet-based mapping works fine. I suspect that the nuance of difference here, is that the latter method (<code>%pip install</code>ing from within the notebook), is being layered on top of a fully formed base Jupyter Lab container (per @gogasca's comment <a href=""https://stackoverflow.com/questions/73360734/programmatically-enable-installed-extensions-in-vertex-ai-managed-notebook-insta/73369289?noredirect=1#comment129617711_73369289"">here</a>, that the Google Managed Jupyter Lab runs as a container that is not customizable).</p>
<p><strong>Questions</strong></p>
<p>So, what I would love to know is:</p>
<p><strong>(1)</strong> How does one retain ipyleaflet (and associated modules) functionality in a Managed Notebook that is based on a user supplied Docker image?</p>
<p><strong>(2)</strong> Is there a way to effectively replicate the <code>%pip install</code> approach when using a custom Docker image, such that commands specified in the Docker file are layered on top of a fully formed base Google image.</p>
<p>To question (2), I suspect that when the gcloud sdk for Managed Notebooks is available (currently under the impression that this is a work in progress), it will be possible to provide a post-startup-script, as in <a href=""https://medium.com/@gogasca_/ai-platform-notebooks-with-voila-c3c57d4e8e"" rel=""nofollow noreferrer"">this example</a>. I am aware that there are REST and Terraform build options available that might satisfy my Managed Notebook needs. These require quite a bit more legwork though, so I am interested in simpler solutions, if they exist.</p>
<p><strong>EDIT</strong></p>
<p>Adding content for partial reproducibility. Steps to reproduce:</p>
<ol>
<li>A custom Docker image was created using the interface at <a href=""https://shell.cloud.google.com/"" rel=""nofollow noreferrer"">https://shell.cloud.google.com/</a>.</li>
<li>Within the cloud shell, from the terminal, set gcloud configuration parameters with <code>gcloud config set project yourProjectIDHere</code></li>
<li>Create a file named Dockerfile with the following content:</li>
</ol>
<blockquote>
<pre><code>FROM python:3.7.4-buster 
ENV VIRTUAL_ENV=/env/testEnvironment 
RUN python3 -m venv $VIRTUAL_ENV --system-site-packages 
ENV PATH=&quot;$VIRTUAL_ENV/bin:$PATH&quot; 
# Install dependencies: 
COPY requirements.txt . 
RUN $VIRTUAL_ENV/bin/pip install -r requirements.txt
</code></pre>
</blockquote>
<ol start=""4"">
<li>Create a file named requirements.txt with the following content</li>
</ol>
<blockquote>
<pre><code>ipython
ipykernel
geemap
earthengine-api
ipyleaflet
folium
voila
ipywidgets
</code></pre>
</blockquote>
<ol start=""5"">
<li>From the cloud shell terminal, build your Docker image (this takes a long time to build. We will use a Google deep learning library derivative once we get our workflow sorted):
<code>docker build . -f Dockerfile -t &quot;gcr.io/yourProjectIDHere/test-image:latest&quot;</code></li>
<li>Push container to GCP Container Registry
<code>docker push &quot;gcr.io/yourProjectIDHere/test-image:latest&quot;</code></li>
<li>Move to Google cloud console and create a new Managed Notebook within your GCP.</li>
<li>Select the defaults except for (a) use Service account permissions; (b) use &quot;Networks shared with me&quot; associated with the host project, including both a Network and Shared subnetwork (presuming that this isn't going to change installation components, so probably doesn't matter); (c) uncheck &quot;Enable external IPs&quot;; (d) check &quot;Enable terminal&quot;; and (e) check &quot;Provide custom docker images&quot;.</li>
<li>From the &quot;Provide custom docker images&quot; dialog, select the image you just created.</li>
<li>Create the Managed Notebook, and once created, open Jupyter Lab.</li>
<li>The Docker image should expose a kernel in the Jupyter Lab environment. Open a new notebook using the kernel.</li>
<li>Load and test modules within the notebook</li>
</ol>
<blockquote>
<pre><code># Import and initialize the earthengine-api by whatever means fit your use case
import ee
ee.Initialize( ... )
# Test authentication pathways
print(ee.Image(&quot;NASA/NASADEM_HGT/001&quot;).get(&quot;title&quot;).getInfo())
# Provided you have your authentication set-up correctly...

# Test import of various other modules   
import os
import sys
import pandas as pd
import numpy as np
import geemap
from IPython.display import display, HTML, Image
import ipywidgets as widgets
from ipywidgets import Layout
from ipywidgets import interact, interactive, fixed, interact_manual, Button, HBox, VBox
from ipywidgets import HTML
import voila

# Test inline mapping using ipyleaflet-based map     
Map = geemap.Map(center=(-0.2557968807155925, 119.46629773460036), zoom=5)
Map

# Receive error.

# This is also replicable without earthengine-api or geemap, by just trying to make a basic ipyleaflet map
from ipyleaflet import Map, basemaps, basemap_to_tiles
m = Map(basemap=basemap_to_tiles(basemaps.OpenStreetMap.Mapnik), center=(48.204793, 350.121558), zoom=3)
m
</code></pre>
</blockquote>",0,5,2022-08-18 12:21:40.527000 UTC,,2022-09-01 18:59:54.213000 UTC,1,google-cloud-platform|jupyter-lab|google-cloud-vertex-ai|gcp-ai-platform-notebook,148,2016-03-30 15:15:05.810000 UTC,2022-09-22 14:32:04.693000 UTC,,55,2,0,5,,,,,,['google-cloud-vertex-ai']
"AzureML endpoint deploy to AKS, the export parameter, deprecated since v1.14,is no longer supported","<p>I'm trying to deploy a model in AzureML and publish it as a endpoint in a Azure Kubernetes environment. This action gives after some time the following error:</p>
<blockquote>
<p>{
&quot;code&quot;: &quot;KubernetesError&quot;,
&quot;message&quot;: &quot;Kubernetes error: Bad Request. Reason: {&quot; kind &quot;:&quot; Status &quot;,&quot; apiVersion &quot;:&quot; v1 &quot;,&quot; metadata &quot;:{},&quot; status &quot;:&quot; Failure &quot;,&quot; message &quot;:&quot; the export parameter, deprecated since v1.14,is no longer supported &quot;,&quot; reason &quot;:&quot; BadRequest &quot;,&quot; code &quot;:400}&quot;
}</p>
</blockquote>
<p>This Kubernetes change seems to be mentioned in: <a href=""https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.14.md#deprecations"" rel=""nofollow noreferrer"">https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.14.md#deprecations</a> and I'm using Kubernetes 1.21.2</p>
<p>The code is based on <a href=""https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-and-where?tabs=python#register-a-model-from-a-local-file-1"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-and-where?tabs=python#register-a-model-from-a-local-file-1</a> and <a href=""https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-azure-kubernetes-service?tabs=python#deploy-to-aks"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-azure-kubernetes-service?tabs=python#deploy-to-aks</a></p>
<p>Here is the script:</p>
<pre><code>from azureml.core import Workspace, Environment
from azureml.core import Environment
from azureml.core.model import InferenceConfig
import urllib.request
from azureml.core.model import Model
from azureml.core.webservice.aks import AksWebservice
from azureml.core.compute import ComputeTarget

ws=Workspace(&quot;xyz-subscription&quot;, &quot;xyz-resourcegroup&quot;, &quot;xyz-azure-ml-name&quot;)
env=Environment('example-env')

urllib.request.urlretrieve(&quot;https://aka.ms/bidaf-9-model&quot;, &quot;model.onnx&quot;)

model = Model.register(ws, model_name=&quot;bidaf_onnx&quot;, model_path=&quot;./model.onnx&quot;)

dummy_inference_config = InferenceConfig(
    environment=env,
    source_directory=&quot;./source_dir&quot;,
    entry_script=&quot;./echo_score.py&quot;,
)

compute=ComputeTarget(ws, &quot;k8s-example&quot;)

deployment_config=AksWebservice.deploy_configuration(
    autoscale_enabled=True, 
    cpu_cores=0.1, 
    memory_gb=0.5, 
    auth_enabled=True, 
    enable_app_insights=True, 
    max_request_wait_time=4000, 
    namespace=&quot;example-namespace&quot;) 

service = Model.deploy(
    ws,
    &quot;examplemodel&quot;,
    [model],
    dummy_inference_config,
    deployment_config,
    compute,
    overwrite=True,
)
service.wait_for_deployment(show_output=True)

print(service.get_logs())

</code></pre>
<p>Does somebody knows what is going wrong here?</p>",2,0,2021-08-25 08:56:49.147000 UTC,1.0,2021-08-25 09:10:34.087000 UTC,0,azure-aks|azure-machine-learning-service,445,2021-08-25 08:40:41.637000 UTC,2022-03-30 14:57:45.423000 UTC,,11,0,0,4,,,,,,['azure-machine-learning-service']
Sagemaker Torch Installation Failing,"<p>I try to install torch on Sagemaker with the shebang-command in python.</p>
<pre><code>!pip install torch==1.6.0
</code></pre>
<p>However, I can only run a notebook once on that specific version. The next time i install that torch version using the notebook, it fails.</p>
<p>The exact error message is:</p>
<pre><code>Collecting torch==1.6.0
Killed
</code></pre>
<p>The only workaround would be to install slightly different versions for next notebook runs.</p>",0,0,2021-12-25 17:54:29.567000 UTC,,,1,python|pytorch|amazon-sagemaker,125,2021-03-03 16:38:57.417000 UTC,2022-09-19 12:03:55.947000 UTC,,11,0,0,1,,,,,,['amazon-sagemaker']
Connecting sagemaker using java sdk,"<p>Can anyone tell me how to connect to sagemaker using aws java sdk and invoke a endpoint which is arleady created using jupyter notebook?</p>

<p>Link -<a href=""https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/sagemaker/AmazonSageMaker.html#createNotebookInstance-com.amazonaws.services.sagemaker.model.CreateNotebookInstanceRequest-"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/sagemaker/AmazonSageMaker.html#createNotebookInstance-com.amazonaws.services.sagemaker.model.CreateNotebookInstanceRequest-</a></p>",1,0,2019-06-09 09:22:36.127000 UTC,,,0,amazon-sagemaker,1112,2018-10-15 09:33:27.860000 UTC,2022-09-22 13:55:04.627000 UTC,,33,1,0,39,,,,,,['amazon-sagemaker']
How do you start using MLflow SQL storage instead of the file system storage?,"<p>If I were getting started with MLflow, then how would I set up a database store? Is it sufficient to create a new MySQL database or a SQLite database and point MLflow to that?</p>

<p>I tried to set the tracking URI, but that didn't create a database if it didn't exist.</p>",2,0,2019-09-10 21:13:31.390000 UTC,,,1,mlflow,882,2013-04-02 19:38:08.083000 UTC,2022-09-23 18:27:42.590000 UTC,"San Francisco, CA, USA",31,4,0,17,,,,,,['mlflow']
Can SageMaker dynamically allocate resources based on the load? (i.e. run 5000 models in parallel with parameter tuning),"<p>Cam Sagemaker dynamically allocate resources based on the load? For example, how easy would it be to run 5000 models in parallel? How does parameter tuning come into play?</p>",1,2,2021-12-09 16:53:13.240000 UTC,,,2,amazon-web-services|amazon-sagemaker,59,2012-08-31 20:08:40.090000 UTC,2022-09-25 04:17:41.297000 UTC,,11650,6318,21,977,,,,,,['amazon-sagemaker']
How to save parquet in S3 from AWS SageMaker?,"<p>I would like to save a Spark DataFrame from AWS SageMaker to S3. In Notebook, I ran</p>

<p><code>myDF.write.mode('overwrite').parquet(""s3a://my-bucket/dir/dir2/"")</code></p>

<p>I get</p>

<blockquote>
  <p>Py4JJavaError: An error occurred while calling o326.parquet. :
  java.lang.RuntimeException: java.lang.ClassNotFoundException: Class
  org.apache.hadoop.fs.s3native.NativeS3FileSystem not found    at
  org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2195)
    at
  org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2654)
    at
  org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2667)
    at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)   at
  org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)
    at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)
    at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)     at
  org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)    at
  org.apache.spark.sql.execution.datasources.DataSource.writeInFileFormat(DataSource.scala:394)
    at
  org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:471)
    at
  org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:50)
    at
  org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)
    at
  org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)
    at
  org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)
    at
  org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)
    at
  org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)
    at
  org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)
    at
  org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
    at
  org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)
    at
  org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)
    at
  org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)
    at
  org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)
    at
  org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:609)
    at
  org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:233)
    at
  org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:217)
    at
  org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:508)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)  at
  sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at
  sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)     at
  py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)  at
  py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)    at
  py4j.Gateway.invoke(Gateway.java:280)     at
  py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
    at py4j.commands.CallCommand.execute(CallCommand.java:79)   at
  py4j.GatewayConnection.run(GatewayConnection.java:214)    at
  java.lang.Thread.run(Thread.java:745) Caused by:
  java.lang.ClassNotFoundException: Class
  org.apache.hadoop.fs.s3native.NativeS3FileSystem not found    at
  org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2101)
    at
  org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2193)</p>
</blockquote>

<p>How should I do it correctly in Notebook? Many thanks!</p>",1,0,2018-03-30 18:28:26.127000 UTC,,,0,amazon-web-services|apache-spark|hadoop|amazon-s3|amazon-sagemaker,1986,2014-12-31 04:45:02.893000 UTC,2021-11-29 22:53:47.930000 UTC,,173,15,0,25,,,,,,['amazon-sagemaker']
Error when deploying Azure ML model to modelmanagement,"<p>When executing the command below: </p>

<p><code>az ml model register -m &lt;pkl name&gt;.pkl -n &lt;model name&gt; -d ""dummy model"" --debug --verbose</code></p>

<p>I get the error stating that the URL cannot be connected to. The verbose message does not show any error before the one below. I can confirm that the model management account and environments have been set. </p>

<p>I am using the Visual Studio subscription to test out some functionality. Any help is appreciated!</p>

<pre><code>{
    ""Azure-cli-ml Version"": ""0.1.0a27.post3"",
    ""Error"": ""Error connecting to https://australiaeast.modelmanagement.azureml.net/api/subscriptions/ad19a4a2-ed65-4574-aec3-e247c4d96efd/resourceGroups/rcity-rg-bi-001-azureml-3797f/accounts/rcity-bi-mlexpmgmt-002/models.""
}
</code></pre>",1,0,2018-05-23 01:00:22.223000 UTC,,2018-05-23 08:34:02.137000 UTC,0,azure|azure-machine-learning-studio,129,2017-12-22 08:33:21.027000 UTC,2021-03-10 06:45:27.530000 UTC,,11,0,0,3,,,,,,['azure-machine-learning-studio']
How to select a subset of Avro files from Azure Data Lake Gen2 by data content,"<p>I have lots of Avro files in an Azure Data Lake Gen2 storage sent by an Event Hub service with capture enabled. These Avro files contain data from different sensors and engines. The structure of the directory is organized in folders with the following path format (typical of Azure Blobs): </p>

<p><code>namespace/eventhub/partition/year/month/day/hour/minute/file.avro</code></p>

<p>I need to access to some of these files, in order to get data to 1) pre-process and 2) train or re-train a machine learning model. I'd like to know what procedure could I follow to download or mount just the files containing data of a particular engine and/or sensor, given that not data from all of them are present in all Avro files. Let's assume I'm interested just in files containing data from:</p>

<pre><code>Engine = engine_ID_4012
Sensor = sensor_engine_4012_ID_0114
</code></pre>

<p>I'm aware that Spark offers some advantages working with Avro files, so I could consider to carry out this task using Databricks. Otherwise the option is Azure Machine Learning service, but maybe there are other possiblities, for instance a combination. The goal is to speed up the data ingestion process, avoiding to read files with no needed data.</p>

<p>Thanks.</p>",1,0,2020-04-10 16:45:07.233000 UTC,1.0,,0,avro|azure-data-lake|azure-eventhub|azure-databricks|azure-machine-learning-service,248,2018-09-07 11:08:23.563000 UTC,2022-09-20 13:21:15.580000 UTC,,150,64,0,16,,,,,,['azure-machine-learning-service']
Mlflow model in Heroku,"<p>I built an MLflow model and call a prediction on a streamlit dashboard, in it work fine in local.
In Heroku, the app which works fine locally failed to send the request online, what am I missing to such deployment?</p>
<p>Below the error code raised.</p>
<p>Procfile :</p>
<pre><code>web: mlflow sagemaker deploy -m mlflow_model/
web: sh setup.sh &amp;&amp; streamlit run app.py
</code></pre>
<p>Heroku logs:</p>
<pre><code>2022-01-07T10:14:15.759252+00:00 app[web.1]: Traceback (most recent call last):
2022-01-07T10:14:15.759252+00:00 app[web.1]: File &quot;/app/.heroku/python/lib/python3.9/site-packages/streamlit/script_runner.py&quot;, line 354, in _run_script
2022-01-07T10:14:15.759252+00:00 app[web.1]: exec(code, module.__dict__)
2022-01-07T10:14:15.759252+00:00 app[web.1]: File &quot;/app/app.py&quot;, line 204, in &lt;module&gt;
2022-01-07T10:14:15.759252+00:00 app[web.1]: main()
2022-01-07T10:14:15.759252+00:00 app[web.1]: File &quot;/app/app.py&quot;, line 119, in main
2022-01-07T10:14:15.759253+00:00 app[web.1]: pred = request_prediction(MLFLOW_URI, ml_data)[0]
2022-01-07T10:14:15.759253+00:00 app[web.1]: File &quot;/app/app.py&quot;, line 63, in request_prediction
2022-01-07T10:14:15.759253+00:00 app[web.1]: response = requests.request(
2022-01-07T10:14:15.759253+00:00 app[web.1]: File &quot;/app/.heroku/python/lib/python3.9/site-packages/requests/api.py&quot;, line 61, in request
2022-01-07T10:14:15.759253+00:00 app[web.1]: return session.request(method=method, url=url, **kwargs)
2022-01-07T10:14:15.759254+00:00 app[web.1]: File &quot;/app/.heroku/python/lib/python3.9/site-packages/requests/sessions.py&quot;, line 529, in request
2022-01-07T10:14:15.759254+00:00 app[web.1]: resp = self.send(prep, **send_kwargs)
2022-01-07T10:14:15.759254+00:00 app[web.1]: File &quot;/app/.heroku/python/lib/python3.9/site-packages/requests/sessions.py&quot;, line 645, in send
2022-01-07T10:14:15.759254+00:00 app[web.1]: r = adapter.send(request, **kwargs)
2022-01-07T10:14:15.759254+00:00 app[web.1]: File &quot;/app/.heroku/python/lib/python3.9/site-packages/requests/adapters.py&quot;, line 519, in send
2022-01-07T10:14:15.759254+00:00 app[web.1]: raise ConnectionError(e, request=request)
2022-01-07T10:14:15.759259+00:00 app[web.1]: requests.exceptions.ConnectionError: HTTPConnectionPool(host='127.0.0.1', port=5000): Max retries exceeded with url: /invocations (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object at 0x7f2452d23790&gt;: Failed to establish a new connection: [Errno 111] Connection refused'))
</code></pre>",1,0,2022-01-07 08:15:18.273000 UTC,,2022-01-07 11:19:14.363000 UTC,0,heroku|streamlit|mlflow,148,2022-01-03 11:45:30.270000 UTC,2022-01-31 05:12:53.300000 UTC,,1,0,0,0,,,,,,['mlflow']
Handle different runs concurrently on multiple tracking servers?,"<p>Are there any resources or insights on handling multiple tracking servers concurrently? We're trying to deploy some RESTful APIs (with FastAPI) that basically launch, potentially concurrently, multiple runs on different Tracking Servers using the MLflow Python API. We've seen that there's no clear way to explicitly assign the Tracking URI during the <code>mlflow.projects.run</code> function and so we're obliged to use <code>set_tracking_uri</code> everytime before launching the new run (which I quote &quot;does not affect the currently active run (if one exists), but takes effect for successive runs.&quot;). Problem is that it may happens that multiple runs go in conflict between each other and some random errors like <code>mlflow.exceptions.RestException: RESOURCE_DOES_NOT_EXIST</code> may occur.</p>
<p>Is there a way to handle this use case scenario or MLflow is still too unripe to be handling multiple tracking servers on a single endpoint?</p>",0,0,2022-05-08 15:15:31.417000 UTC,,,0,fastapi|mlflow,52,2015-01-11 13:16:26.627000 UTC,2022-09-24 16:47:22.317000 UTC,"Florence, Italy",1065,35,0,73,,,,,,['mlflow']
"What does ""Number of points"" mean when you select the Parameter Range option","<p>What does ""Number of points"" mean on the various models when you select the Parameter Range option for Create Trainer Mode.  Can anyone shed light on what this parameter means.</p>

<p>The Azure ML Studio documentation does not mention this parameter, either in <a href=""https://docs.microsoft.com/en-us/azure/machine-learning/studio-module-reference/two-class-logistic-regression"" rel=""nofollow noreferrer"">the documentation for the model</a> or in the <a href=""https://docs.microsoft.com/en-us/azure/machine-learning/studio-module-reference/tune-model-hyperparameters"" rel=""nofollow noreferrer"">documentation to tune hyperparameters</a>.</p>

<p><a href=""https://i.stack.imgur.com/6De7r.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6De7r.png"" alt=""enter image description here""></a></p>",1,1,2018-12-04 03:07:05.220000 UTC,,2018-12-06 07:16:03.543000 UTC,0,machine-learning|logistic-regression|azure-machine-learning-studio,72,2018-09-30 02:23:26.613000 UTC,2021-08-13 13:02:53.707000 UTC,,798,21,2,64,,,,,,['azure-machine-learning-studio']
"How to configure artifact store of mlflow tracking service to connect to minio S3 using minio STS generated acces_key, secret_key and session_token","<ul>
<li><p>Minio is configured with LDAP and am generating credentials of user
with AssumeRoleWithLDAPIdentity using STS API (<a href=""https://docs.min.io/minio/baremetal/security/ad-ldap-external-identity-management/AssumeRoleWithLDAPIdentity.html#assumerolewithldapidentity"" rel=""nofollow noreferrer"">reference</a>)</p>
</li>
<li><p>From above values, I'm setting the variables AWS_ACCESS_KEY, AWS_SECRET_KEY, AWS_SESSION_TOKEN (<a href=""https://www.mlflow.org/docs/latest/tracking.html#amazon-s3-and-s3-compatible-storage"" rel=""nofollow noreferrer"">reference</a>)</p>
</li>
</ul>
<p>I'm getting error when am trying to push model to mlflow to store in minio artifact</p>
<pre><code>S3UploadFailedError: Failed to upload /tmp/tmph68xubhm/model/MLmodel to mlflow/1/xyz/artifacts/model/MLmodel: An error occurred (InvalidTokenId) when calling the PutObject operation: The security token included in the request is invalid
</code></pre>",0,0,2021-10-14 15:38:30.190000 UTC,,,2,amazon-s3|minio|mlflow|mlops,255,2017-05-31 04:12:26.490000 UTC,2022-09-24 08:59:44.060000 UTC,,838,89,2,33,,,,,,['mlflow']
"Kedro-mlflow usage - when to use it from notebooks, and when from kedro pipeline?","<p>I'm a bit confused - what is the common practice for kedro-mlflow usage? It's seems slightly uncomfortable to use it only from kedro pipelines, but kedro intention is fully reproducible research.</p>
<p>At the same time rather rare tutorials on kedro-mlflow usage describe experiments creation from Jupiter notebooks, which seems natural, but then full reproducibility without the full pipeline is broken.</p>
<p>Question - what are common patterns on kedro-mlflow usage, as well as subexperiments creation (in scope of CrossValidation or HyperOptimisation)? When kedro pipelines should be used, and when code (and which code) should be placed in notebooks?</p>",0,2,2021-05-15 12:32:57.757000 UTC,,,1,mlflow|kedro,160,2018-01-02 12:23:30.993000 UTC,2021-07-08 08:48:54.770000 UTC,"Riga, Latvia",11,0,0,7,,,,,,['mlflow']
How can I save loss and accuracy metrics in mlflow after each epoch?,"<p>I would like to see metrics like loss and accuracy as a graph by storing each value for the corresponding metrics after each epoch during training/testing phase of a keras model.</p>
<p>PS: I know that we can do it by using autolog feature of mlflow for keras like below, but I dont want to use that.</p>
<pre><code>mlflow.keras.autolog()
</code></pre>",1,0,2022-06-02 08:46:47.043000 UTC,,,1,keras|mlflow,278,2016-09-09 08:04:39.707000 UTC,2022-09-22 15:20:23.897000 UTC,Germany,620,49,1,101,,,,,,['mlflow']
Azure Machine Learning studio: Failed to parse parameter,"<p>I keep getting Error 002: Failed to parse parameter in the ""Score Matchbox Recommender"" box. </p>

<p>I'm using Azure's default ""Recommender: Movie recommendation"" sample project. </p>

<p>I've tried referring to an online video too, followed same steps but to no avail. Still getting error 002.</p>

<p><a href=""https://www.youtube.com/watch?v=3cNd_YRAsdk"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=3cNd_YRAsdk</a></p>

<p>Full error code below.</p>

<blockquote>
  <p>requestId = 8ce5b273ae124cd5a5d872e5a509fe45 errorComponent=Module. taskStatusCode=400. {""Exception"":{""ErrorId"":""ParameterParsing"",""ErrorCode"":""0002"",""ExceptionType"":""ModuleException"",""Message"":""Error 0002: Failed to parse parameter""}}Error: Error 0002: Failed to parse parameter Process exited with error code -2</p>
</blockquote>

<p>Does anyone have any idea what's wrong?</p>",0,3,2018-04-29 10:43:46.470000 UTC,,2018-05-11 14:54:14.763000 UTC,0,azure|data-science|azure-machine-learning-studio|azure-machine-learning-workbench,153,2017-12-31 12:45:30.473000 UTC,2022-09-21 04:30:14.037000 UTC,,3,0,0,9,,,,,,"['azure-machine-learning-workbench', 'azure-machine-learning-studio']"
Is it good or necessary to use Blobs when running machine learning algorithms with big data,"<p>I know what I can either upload my data files to the azure ml (as new datasets) or I can use Blobs (and read data within ML experiment). I wonder if particularly one of them is recommended when training machine learning models and creating prediction-related ML solutions.</p>

<p>My goal of using Azure is to cluster users based on a various of features. I have a large dataset (~ 50GB). I wonder if you have any recommendations.</p>

<p>I appreciate any help!</p>",1,0,2016-11-20 10:02:21.783000 UTC,,2016-11-20 10:10:36.943000 UTC,0,azure|azure-storage|azure-blob-storage|azure-machine-learning-studio,135,2012-11-22 14:59:22.917000 UTC,2022-09-10 15:02:49.860000 UTC,USA,8007,1304,38,792,,,,,,['azure-machine-learning-studio']
I am wondering about limitations of BQML. What are some of the biggest limitations for data science projects when using BQML?,"<p>I am  dealing with an unbalanced panel dataset. So, I have a timestamp column as well as an orders column, a categories columns, product name,  office location column and selling price column and a few other columns. If I am trying to forecast how much I would be selling for both every category and product name for the next year given three years worth of data, Would BQML be a good place to start? Any recommended tutorials to get me started?</p>",0,1,2022-09-16 14:29:05.493000 UTC,,,0,google-bigquery|google-cloud-automl|google-cloud-vertex-ai|gcp-ai-platform-training|multivariate-time-series,22,2022-09-07 21:07:02.113000 UTC,2022-09-24 19:41:56.250000 UTC,,1,0,0,3,,,,,,['google-cloud-vertex-ai']
Connect Jupyter Notebook (locally) to AWS s3 without SageMaker,<p>Is it possible to connect jupyter notebook that is running locally to one of the buckets on AWS S3 without using SageMaker and involving <strong>no or with</strong> access and secret keys?</p>,2,0,2019-10-15 20:37:45.887000 UTC,,2019-10-15 21:12:25.217000 UTC,4,amazon-web-services|amazon-s3|jupyter-notebook|jupyter|amazon-sagemaker,7218,2017-07-18 02:02:41.200000 UTC,2022-07-30 14:44:42.427000 UTC,,187,26,0,30,,,,,,['amazon-sagemaker']
"How can I pass request body for sklearn model, decision tree classifier in AWS Sagemaker while invoking endpoint?","<p>I am trying to invoke a multi-model endpoint in AWS Sagemaker and I am getting a ModelError -<strong>ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from model with message &quot;invalid literal for int() with base 10: 'Iris-virginica'&quot;</strong>. I have used Sklearn model with IRIS dataset and Decision Tree Algorithm to predict the output.</p>
<pre><code>import boto3
import json

runtime_client = boto3.client('sagemaker')
content_type = &quot;application/json&quot;
request_body = {&quot;Input&quot;: [[1.2,0.5, 1.7, 2.6, 2.7,1.0]]}
data = json.loads(json.dumps(request_body))
payload = json.dumps(data)
endpoint_name = &quot;sklearn-endpoint&quot;
print(&quot;payload &quot;,type(payload))
response = runtime_client.invoke_endpoint(
    EndpointName=endpoint_name,
    ContentType=content_type,
    TargetModel=&quot;iris.tar.gz&quot;,
    Body=payload)
</code></pre>",0,3,2022-06-01 10:29:11.687000 UTC,,,0,python|machine-learning|scikit-learn|amazon-sagemaker|mlops,78,2019-01-31 06:43:12.977000 UTC,2022-09-23 10:19:17.710000 UTC,,81,41,0,476,,,,,,['amazon-sagemaker']
How to log model using mlflow REST api? Does mlflow REST APIs support it?,"<p>I'm writing a library using mlflow REST APIs.
I'm looking for mlflow REST api for logging different mlflow models.</p>
<p>In the doc, <a href=""https://www.mlflow.org/docs/latest/rest-api.html#log-model"" rel=""nofollow noreferrer"">https://www.mlflow.org/docs/latest/rest-api.html#log-model</a> it says the api will be removed in future and doesn't have description about model_json request body.</p>
<p>If I see github, <a href=""https://github.com/mlflow/mlflow/blob/master/docs/source/rest-api.rst"" rel=""nofollow noreferrer"">https://github.com/mlflow/mlflow/blob/master/docs/source/rest-api.rst</a> mlflow REST API for Log model is missing.</p>",1,6,2022-03-30 11:14:03.580000 UTC,,2022-03-30 12:06:26.563000 UTC,3,machine-learning|mlflow|mlops,390,2017-05-31 04:12:26.490000 UTC,2022-09-24 08:59:44.060000 UTC,,838,89,2,33,,,,,,['mlflow']
How can I build a multi model endpoint for ensemble modeling with using my own model containers?,"<p>I'm trying to deploy a multi model endpoint on Amazon Sagemaker, and am working with my own model containers which I created using <a href=""https://github.com/aws/amazon-sagemaker-examples/tree/main/advanced_functionality/scikit_bring_your_own"" rel=""nofollow noreferrer"">scikit_bring_your_own</a> example. I can train and create endpoint for each of them separately but for example when I try to collect mlp and cart together in multi model endpoint, I get an error which says &quot;The cart,mlp for production variant AllTraffic did not pass the ping health check. Please check CloudWatch logs for this endpoint&quot;. When I check CloudWatch logs I cannot see anything unusual. <strong>Should I change the container structure for multi model endpoints ?</strong></p>
<pre><code>from time import gmtime, strftime
import os
import boto3
import time
import re
import sagemaker

model_name = &quot;efe-test-model-ensemble-modeling-&quot; + strftime(&quot;%Y-%m-%d-%H-%M-%S&quot;, gmtime())

cart_hosting_container = {
    &quot;Image&quot;: &quot;097916623002.dkr.ecr.eu-central-1.amazonaws.com/snop-mm-cart:latest&quot;,
    &quot;ContainerHostname&quot;: &quot;cart&quot;,
    &quot;ModelDataUrl&quot;: &quot;s3://sagemaker-eu-central-1-097916623002/output/snop-mm-cart-2022-03-09-12-47-04-881/output/model.tar.gz&quot;,
}

mlp_hosting_container = {
    &quot;Image&quot;: &quot;097916623002.dkr.ecr.eu-central-1.amazonaws.com/snop-mm-mlp:latest&quot;,
    &quot;ContainerHostname&quot;: &quot;mlp&quot;,
    &quot;ModelDataUrl&quot;: &quot;s3://sagemaker-eu-central-1-097916623002/output/snop-mm-mlp-2022-03-09-12-52-09-267/output/model.tar.gz&quot;,
}

role = sagemaker.get_execution_role()
sm = boto3.client(&quot;sagemaker&quot;)

inferenceExecutionConfig = {&quot;Mode&quot;: &quot;Direct&quot;}

create_model_response = sm.create_model(
    ModelName=model_name,
    InferenceExecutionConfig=inferenceExecutionConfig,
    ExecutionRoleArn=role,
    Containers=[cart_hosting_container, mlp_hosting_container],
)

endpoint_config_name = &quot;TEST-config-ensemble-modelling-&quot; + strftime(
    &quot;%Y-%m-%d-%H-%M-%S&quot;, gmtime()
)
print(endpoint_config_name)
create_endpoint_config_response = sm.create_endpoint_config(
    EndpointConfigName=endpoint_config_name,
    ProductionVariants=[
        {
            &quot;InstanceType&quot;: &quot;ml.m5.large&quot;,
            &quot;InitialInstanceCount&quot;: 1,
            &quot;InitialVariantWeight&quot;: 1,
            &quot;ModelName&quot;: model_name,
            &quot;VariantName&quot;: &quot;AllTraffic&quot;,
        }
    ],
)

print(&quot;Endpoint Config Arn: &quot; + create_endpoint_config_response[&quot;EndpointConfigArn&quot;])

%%time
import time

endpoint_name = &quot;TEST-endpoint-ensemble-modelling-&quot; + strftime(&quot;%Y-%m-%d-%H-%M-%S&quot;, gmtime())
print(endpoint_name)
create_endpoint_response = sm.create_endpoint(
    EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name
)
print(create_endpoint_response[&quot;EndpointArn&quot;])

resp = sm.describe_endpoint(EndpointName=endpoint_name)
status = resp[&quot;EndpointStatus&quot;]
print(&quot;Status: &quot; + status)

while status == &quot;Creating&quot;:
    time.sleep(60)
    resp = sm.describe_endpoint(EndpointName=endpoint_name)
    status = resp[&quot;EndpointStatus&quot;]
    print(&quot;Status: &quot; + status)

print(&quot;Arn: &quot; + resp[&quot;EndpointArn&quot;])
print(&quot;Status: &quot; + status)
</code></pre>
<p>It creates two folders in CloudWatch</p>
<p><a href=""https://i.stack.imgur.com/wCNOC.png"" rel=""nofollow noreferrer"">log groups</a></p>
<p>mlp is:
<a href=""https://i.stack.imgur.com/O7qeA.png"" rel=""nofollow noreferrer"">mlp log</a></p>
<p>cart is:
<a href=""https://i.stack.imgur.com/XhiDC.png"" rel=""nofollow noreferrer"">cart log</a></p>",1,0,2022-03-09 14:16:53.127000 UTC,,,0,python|amazon-web-services|machine-learning|amazon-sagemaker,87,2022-02-24 19:47:27.140000 UTC,2022-06-10 13:17:44.967000 UTC,,1,0,0,2,,,,,,['amazon-sagemaker']
Vertex AI Custom Container Training Job python SDK - InvalidArgument 400 error,"<p>I'm attempting to run a Vertex AI custom training job using the python SDK, following the general instructions laid out in <a href=""https://github.com/googleapis/python-aiplatform"" rel=""nofollow noreferrer"">this readme</a>. My code is as follows (sensitive data removed):</p>
<pre><code>job = aiplatform.CustomContainerTrainingJob(
    display_name='python_api_test',
    container_uri='{URI FOR CUSTOM CONTAINER IN GOOGLE ARTIFACT REGISTRY}',
    staging_bucket='{GCS BUCKET PATH IN 'gs://' FORMAT}',
    model_serving_container_image_uri='us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-4:latest',
)

job.run(
    model_display_name='python_api_model',
    args='{ARG PASSED TO CONTAINER ENTRYPOINT}',
    replica_count=1,
    machine_type='n1-standard-4',
    accelerator_type='NVIDIA_TESLA_T4',
    accelerator_count=2,
    environment_variables={
        {A COUPLE OF SECRETS PASSED TO CONTAINER IN DICTIONARY FORMAT}
    }
)
</code></pre>
<p>When I execute <code>job.run()</code>, I get the following error:</p>
<pre><code>InvalidArgument: 400 Unable to parse `training_pipeline.training_task_inputs` into custom task `inputs` defined in the file: gs://google-cloud-aiplatform/schema/trainingjob/definition/custom_task_1.0.0.yaml
</code></pre>
<p>The full traceback does not show where it is unhappy with any specific inputs. I've successfully run jobs in the same container using the Vertex CLI.I'm confident that there is nothing wrong with my <code>aiplatform.init()</code> (I'm running the job from a Vertex workbench machine in the same project).</p>",0,2,2022-01-05 16:53:55.217000 UTC,,,0,python|google-cloud-platform|google-cloud-ml|google-cloud-vertex-ai|google-cloud-ai,79,2016-05-24 16:22:09.610000 UTC,2022-09-13 16:37:07.037000 UTC,,457,12,0,37,,,,,,['google-cloud-vertex-ai']
azure machine learning and R using azuremlsdk - supported R version and custom_docker_image,"<p>So we have to move away from using <a href=""https://docs.microsoft.com/en-us/sql/machine-learning/sql-server-machine-learning-services?view=sql-server-ver15"" rel=""nofollow noreferrer"">SQL Server Machine Learning services</a> as it only supports R 3.5.2 even for SQL Server 2019!</p>
<p>I am trying hard to go all 21st century and deploy some of our on prem R trained models as web service as described by one of the <a href=""https://azure.github.io/azureml-sdk-for-r/articles/train-and-deploy-first-model.html"" rel=""nofollow noreferrer"">Microsoft evangelists David Smith</a> (see code below).</p>
<p>Looking at <a href=""https://azure.github.io/azureml-sdk-for-r/reference/r_environment.html"" rel=""nofollow noreferrer"">r_environment</a> I noticed to my horror that, if I do not use a custom docker image, predefine images only support R 3.6?! Is this correct? If so, how do I create a custom docker image and why does Microsoft suggest using Azure ML where there are also restrictions in terms of the R version!</p>
<p>PS:</p>
<p>Some code to possibly replicate my issues:</p>
<p>Train model locally:</p>
<pre><code>library(datasets)
library(caret)

data(iris)

setwd(&quot;C:/Data&quot;)

index &lt;- createDataPartition(iris$Species, p=0.80, list=FALSE)
testset &lt;- iris[-index,]
trainset &lt;- iris[index,]

model = train(Species ~ ., 
                  data=trainset, 
                  method=&quot;rpart&quot;, 
                  trControl = trainControl(method = &quot;cv&quot;))

saveRDS(model, &quot;model.rds&quot;)
</code></pre>
<p>I can deploy this model in Azure ML:</p>
<p><a href=""https://i.stack.imgur.com/bas6D.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bas6D.png"" alt=""enter image description here"" /></a></p>
<p>Scoring script score.r</p>
<pre><code>library(jsonlite)

init &lt;- function()
{
  model_path &lt;- Sys.getenv(&quot;AZUREML_MODEL_DIR&quot;)
  model &lt;- readRDS(file.path(model_path, &quot;model.rds&quot;))
  message(&quot;iris classfication model loaded&quot;)
  
  function(data)
  {
    vars &lt;- as.data.frame(fromJSON(data))
    prediction &lt;- predict(model, newdata=vars)
    toJSON(prediction)
  }
}
</code></pre>
<p>Failing code:</p>
<pre><code>library(azuremlsdk)

interactive_auth &lt;- interactive_login_authentication(tenant_id=&quot;xxx&quot;)

ws &lt;- get_workspace(
        name = &quot;amazing_work_space&quot;, 
        subscription_id = &quot;xxx&quot;, 
        resource_group =&quot;xxx&quot;, 
        auth = interactive_auth
)

model &lt;- get_model(ws, name = &quot;iris_classification&quot;)

r_env &lt;- r_environment(name = 'myr_env',
                       version = '1')

inference_config &lt;- inference_config(
  entry_script = &quot;score.R&quot;,
  source_directory = &quot;.&quot;,
  environment = r_env)

aci_config &lt;- aci_webservice_deployment_config(cpu_cores = 1, memory_gb = 0.5)

aci_service &lt;- deploy_model(ws, 
                            'xxx', 
                            list(model), 
                            inference_config, 
                            aci_config)

wait_for_deployment(aci_service, show_output = TRUE)
</code></pre>",0,3,2021-05-17 13:05:34.127000 UTC,,,2,r|azure-machine-learning-studio|azure-machine-learning-service|azuremlsdk,158,2010-03-01 10:53:04.443000 UTC,2022-09-24 18:56:19.313000 UTC,Somewhere,15705,2171,91,2150,,,,,,"['azure-machine-learning-studio', 'azure-machine-learning-service']"
SageMaker PyTorchModel passing custom variables,"<p>When deploying a model with SageMaker through the <code>PyTorchModel</code> class, is it possible to pass a custom environmental variable or kwargs? </p>

<p>I'd like to be able to switch the functionality of the serving code via a custom argument rather than needing to write multiple <code>serve.py</code> to handle different training model export methods. </p>

<pre><code>model = PyTorchModel(name='my_model',
                     model_data=estimator.model_data,
                     role=role,
                     framework_version='1.0.0',
                     entry_point='serve.py',
                     source_dir='src',
                     sagemaker_session=sess,
                     predictor_cls=ImagePredictor,
                     &lt;custom_argument?&gt;
                    )
</code></pre>",2,0,2019-06-05 17:51:54.157000 UTC,,,0,pytorch|amazon-sagemaker|keyword-argument,369,2013-02-20 05:47:52.693000 UTC,2022-09-23 20:45:28.400000 UTC,NYC,6281,430,17,958,,,,,,['amazon-sagemaker']
Sagemaker nbextensions,"<p>I am running the following script in my Sagemaker notebook's lifecycle configuration:</p>
<pre><code>#!/bin/bash

set -e

# OVERVIEW
# This script installs a single pip package in a single SageMaker conda environments.

sudo -u ec2-user -i &lt;&lt;'EOF'
# PARAMETERS
ENVIRONMENT=python3

source /home/ec2-user/anaconda3/bin/activate &quot;$ENVIRONMENT&quot;

pip install &quot;PyAthena&quot;
pip install &quot;jupyter_nbextensions_configurator&quot;
jupyter nbextensions_configurator enable --sys-prefix
pip install &quot;jupyter_contrib_nbextensions&quot;
jupyter nbextensions_configurator enable --sys-prefix

source /home/ec2-user/anaconda3/bin/deactivate

EOF
</code></pre>
<p>When I open the notebook, I do not see the nbextensions tab. I have stopped and restart the notebook.</p>
<p><a href=""https://i.stack.imgur.com/ibxFX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ibxFX.png"" alt=""enter image description here"" /></a></p>
<p>I have tried the subcommand lines with --user as well...</p>
<pre><code>pip install &quot;jupyter_nbextensions_configurator&quot;
jupyter nbextensions_configurator enable --user
pip install &quot;jupyter_contrib_nbextensions&quot;
jupyter nbextensions_configurator enable --user
</code></pre>
<p>Am I missing something else?</p>",1,0,2022-04-29 05:13:28.943000 UTC,,,0,amazon-web-services|jupyter-notebook|amazon-sagemaker|jupyter-contrib-nbextensions,179,2015-08-17 20:28:43.250000 UTC,2022-09-24 20:02:38.657000 UTC,,137,3,0,35,,,,,,['amazon-sagemaker']
"Max retries exceeded with url, Failed to establish a new connect [Errno 60] Operation timed out'","<p>I used nginx to build mlflow server with its proxy_pass and integrated simple HTTP auth in nginx.  However, when I ran the experiment for a while, the mlflow client met this exception. And I have no idea how to fix it.</p>

<p>Here is the error messages:</p>

<pre><code>Traceback (most recent call last):
  File ""/usr/local/opt/python/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/urllib3/connection.py"", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File ""/usr/local/opt/python/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/urllib3/util/connection.py"", line 80, in create_connection
    raise err
  File ""/usr/local/opt/python/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/urllib3/util/connection.py"", line 70, in create_connection
    sock.connect(sa)
TimeoutError: [Errno 60] Operation timed out
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File ""/usr/local/opt/python/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 600, in urlopen
    chunked=chunked)
  File ""/usr/local/opt/python/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File ""/usr/local/opt/python/Frameworks/Python.framework/Versions/3.6/lib/python3.6/http/client.py"", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File ""/usr/local/opt/python/Frameworks/Python.framework/Versions/3.6/lib/python3.6/http/client.py"", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File ""/usr/local/opt/python/Frameworks/Python.framework/Versions/3.6/lib/python3.6/http/client.py"", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File ""/usr/local/opt/python/Frameworks/Python.framework/Versions/3.6/lib/python3.6/http/client.py"", line 1026, in _send_output
    self.send(msg)
  File ""/usr/local/opt/python/Frameworks/Python.framework/Versions/3.6/lib/python3.6/http/client.py"", line 964, in send
    self.connect()
  File ""/usr/local/opt/python/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/urllib3/connection.py"", line 181, in connect
    conn = self._new_conn()
  File ""/usr/local/opt/python/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/urllib3/connection.py"", line 168, in _new_conn
    self, ""Failed to establish a new connection: %s"" % e)
urllib3.exceptions.NewConnectionError: &lt;urllib3.connection.HTTPConnection object at 0x1280a8438&gt;: Failed to establish a new connection: [Errno 60] Operation timed out
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File ""/usr/local/opt/python/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/requests/adapters.py"", line 449, in send
    timeout=timeout
  File ""/usr/local/opt/python/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File ""/usr/local/opt/python/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/urllib3/util/retry.py"", line 398, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host=&lt;host_ip&gt;, port=&lt;port&gt;): Max retries exceeded with url: /api/2.0/mlflow/experiments/get-by-name?experiment_name=&lt;exp_name&gt; (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object at 0x1280a8438&gt;: Failed to establish a new connection: [Errno 60] Operation timed out',))
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File ""tmp_experiment_entry.py"", line 4, in &lt;module&gt;
    mlflow.set_experiment(&lt;exp_name&gt;)
  File ""/usr/local/opt/python/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/mlflow/tracking/fluent.py"", line 47, in set_experiment
    experiment = client.get_experiment_by_name(experiment_name)
  File ""/usr/local/opt/python/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/mlflow/tracking/client.py"", line 151, in get_experiment_by_name
    return self._tracking_client.get_experiment_by_name(name)
  File ""/usr/local/opt/python/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/mlflow/tracking/_tracking_service/client.py"", line 114, in get_experiment_by_name
    return self.store.get_experiment_by_name(name)
  File ""/usr/local/opt/python/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/mlflow/store/tracking/rest_store.py"", line 219, in get_experiment_by_name
    response_proto = self._call_endpoint(GetExperimentByName, req_body)
  File ""/usr/local/opt/python/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/mlflow/store/tracking/rest_store.py"", line 32, in _call_endpoint
    return call_endpoint(self.get_host_creds(), endpoint, method, json_body, response_proto)
  File ""/usr/local/opt/python/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/mlflow/utils/rest_utils.py"", line 133, in call_endpoint
    host_creds=host_creds, endpoint=endpoint, method=method, params=json_body)
  File ""/usr/local/opt/python/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/mlflow/utils/rest_utils.py"", line 70, in http_request
    url=url, headers=headers, verify=verify, **kwargs)
  File ""/usr/local/opt/python/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/mlflow/utils/rest_utils.py"", line 51, in request_with_ratelimit_retries
    response = requests.request(**kwargs)
  File ""/usr/local/opt/python/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/requests/api.py"", line 60, in request
    return session.request(method=method, url=url, **kwargs)
  File ""/usr/local/opt/python/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/requests/sessions.py"", line 533, in request
    resp = self.send(prep, **send_kwargs)
  File ""/usr/local/opt/python/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/requests/sessions.py"", line 646, in send
    r = adapter.send(request, **kwargs)
  File ""/usr/local/opt/python/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/requests/adapters.py"", line 516, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host=&lt;host_ip&gt;, port=&lt;port&gt;): Max retries exceeded with url: /api/2.0/mlflow/experiments/get-by-name?experiment_name=&lt;exp_name&gt; (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object at 0x1280a8438&gt;: Failed to establish a new connection: [Errno 60] Operation timed out',))
</code></pre>

<p>In the client, I use mlflow log by the following format and log_params, log_metrics in main function</p>

<pre><code>with mlflow.start_run():
    main(params)
</code></pre>",0,3,2020-03-14 07:06:08.240000 UTC,,2020-03-15 17:30:39.973000 UTC,3,python-3.x|nginx|python-requests|nginx-config|mlflow,7609,2015-05-05 19:13:59.170000 UTC,2022-08-21 14:45:40.787000 UTC,"Taipei City, Taiwan",108,35,0,17,,,,,,['mlflow']
How does Amazon Sagemaker Ground Truth work?,<p>Is there a publication that explains how they evaluate how &quot;sure&quot; the automatic system is for the label it assigns? I understand part of the labelling process is done by humans but I'm interested in how they evaluate the confidence of the prediction.</p>,1,0,2021-03-25 15:37:49.253000 UTC,,,0,amazon-web-services|amazon-sagemaker|amazon-ground-truth,174,2021-03-25 15:32:19.320000 UTC,2022-07-04 13:40:33.737000 UTC,,1,0,0,1,,,,,,['amazon-sagemaker']
AWS SageMaker: Use S3 pickled models instead of hosting on sagemaker,"<p>I am working on a use-case for which I have to use Amazon SageMaker notebook instances. Amazon SM resources are filled with material that works well for single model i.e. you do your thing locally on NB Instance and then deploy the model as an endpoint. My use-case on the other hand has multiple models for multiple customers and this needs to be automated. i.e. once a customer uploads a file, a model needs to be automatically created and stored.</p>
<p>Current approach is to automate SageMaker instances through lambda for picking up the train data, training the data and saving the model back to S3 before closing the instance.</p>
<p>My question is, is this the right approach? Or should I create an endpoint for each model for each customer? Somehow since the data size is going to be small and I am working with SageMaker for the first time, I am more comfortable with saving the models in S3 than deploying many many endpoints.</p>",1,0,2020-07-31 08:20:17.173000 UTC,,2020-07-31 12:10:28.707000 UTC,1,python|amazon-web-services|scikit-learn|xgboost|amazon-sagemaker,162,2015-09-25 10:16:37.327000 UTC,2020-11-25 09:09:00.757000 UTC,"Dallas, TX, USA",19,0,0,3,,,,,,['amazon-sagemaker']
"Sagemaker Model Deployment Error, ClientError: An error occurred (ValidationException) when calling the CreateModel operation","<p>I am trying to deploy a model with AWS Sagemaker using SKlearn, and getting this error:</p>
<pre><code>---------------------------------------------------------------------------
ClientError                               Traceback (most recent call last)
&lt;ipython-input-145-29a1d3175b01&gt; in &lt;module&gt;
----&gt; 1 deployment = model.deploy(initial_instance_count=1, instance_type=&quot;ml.m4.xlarge&quot;)

~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/estimator.py in deploy(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, use_compiled_model, wait, model_name, kms_key, data_capture_config, tags, serverless_inference_config, async_inference_config, **kwargs)
   1254             kms_key=kms_key,
   1255             data_capture_config=data_capture_config,
-&gt; 1256             serverless_inference_config=serverless_inference_config,
   1257             async_inference_config=async_inference_config,
   1258         )

~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/model.py in deploy(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, tags, kms_key, wait, data_capture_config, async_inference_config, serverless_inference_config, **kwargs)
   1001                 self._base_name = &quot;-&quot;.join((self._base_name, compiled_model_suffix))
   1002 
-&gt; 1003         self._create_sagemaker_model(
   1004             instance_type, accelerator_type, tags, serverless_inference_config
   1005         )

~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/model.py in _create_sagemaker_model(self, instance_type, accelerator_type, tags, serverless_inference_config)
    548             container_def,
    549             vpc_config=self.vpc_config,
--&gt; 550             enable_network_isolation=enable_network_isolation,
    551             tags=tags,
    552         )

~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/session.py in create_model(self, name, role, container_defs, vpc_config, enable_network_isolation, primary_container, tags)
   2670 
   2671         try:
-&gt; 2672             self.sagemaker_client.create_model(**create_model_request)
   2673         except ClientError as e:
   2674             error_code = e.response[&quot;Error&quot;][&quot;Code&quot;]

~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/client.py in _api_call(self, *args, **kwargs)
    413                     &quot;%s() only accepts keyword arguments.&quot; % py_operation_name)
    414             # The &quot;self&quot; in this scope is referring to the BaseClient.
--&gt; 415             return self._make_api_call(operation_name, kwargs)
    416 
    417         _api_call.__name__ = str(py_operation_name)

~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/client.py in _make_api_call(self, operation_name, api_params)
    743             error_code = parsed_response.get(&quot;Error&quot;, {}).get(&quot;Code&quot;)
    744             error_class = self.exceptions.from_code(error_code)
--&gt; 745             raise error_class(parsed_response, operation_name)
    746         else:
    747             return parsed_response

ClientError: An error occurred (ValidationException) when calling the CreateModel operation: Could not find model data at s3://sagemaker-us-east-2-978433479050/sagemaker-scikit-learn-2022-04-28-22-33-14-817/output/model.tar.gz.
</code></pre>
<hr />
<p>The code I am running is:</p>
<pre><code>from sagemaker import Session, get_execution_role
from sagemaker.sklearn.estimator import SKLearn

sagemaker_session = Session()
role = get_execution_role()

train_input = sagemaker_session.upload_data(&quot;TSLA.csv&quot;)

model = SKLearn(entry_point='lr.py',
                      train_instance_type='ml.m4.xlarge',
                      role=role, framework_version='0.231',
                      sagemaker_session=sagemaker_session)

model.fit({'train': train_input})

deployment = model.deploy(initial_instance_count=1, instance_type=&quot;ml.m4.xlarge&quot;)
</code></pre>
<p>And train_input is: s3://sagemaker-us-east-2-978433479050/data/TSLA.csv</p>
<p>The training job is completed, but for some reason the model is not deploying.</p>
<p>Please advise,
thank you</p>",1,0,2022-04-28 22:54:41.083000 UTC,,2022-04-29 03:13:11.640000 UTC,0,python|scikit-learn|amazon-sagemaker,241,2021-08-23 21:06:51.590000 UTC,2022-09-21 23:38:35.943000 UTC,,1,0,0,2,,,,,,['amazon-sagemaker']
how to deploy the custom model in amazon sageMaker,"<p>I am newbie to AWS sagemaker, I am trying to  deploy the time series custom  lstm model in sagemaker , please help me out and  how to perpare the script mode.
this  my script file <strong>timer_series.py</strong> code. </p>

<pre><code>import sagemaker
import boto3
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import tensorflow 
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import LSTM
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator

from sklearn.metrics import mean_squared_error


if __name__ =='__main__':

    parser.add_argument('--epochs', type=int, default=50)
    parser.add_argument('--batch_size', type=int, default=72)
    parser.add_argument('--n_train_hours', type=int, default=24*365*2)
    parser.add_argument('--n_validation_hours', type=int, default=24*365*4)

    # input data and model directories
    parser.add_argument('--model_dir', type=str)

    args, _ = parser.parse_known_args()

    train_dataset_dir = os.environ.get('SM_INPUT_DIR') + '/data/training/' 
    output_model_dir = os.environ.get('SM_MODEL_DIR')
    output_object_dir = os.environ.get('SM_OUTPUT_DATA_DIR')

    epochs = args.epochs
    batch_size = args.batch_size
    input_data = {args.input_data}
    dataset = read_csv( train_dataset_dir + 'dataset.csv', header=0, index_col='Date')
    dataset.sort_index(inplace=True)
    train = dataset.iloc[:109]
    test= dataset.iloc[109:]  
    scaler = MinMaxScaler()
    scaled_train = scaler.fit_transform(train)
    scaled_test=scaler.fit_transform(test)
    n_input = 12
    n_feature = 1

    train_generator = TimeseriesGenerator(scaled_train,scaled_train,length=n_input, batch_size=1)

    model = Sequential()

    model.add(LSTM(128,activation = 'relu', input_shape= (n_input, n_feature), return_sequences=True))
    model.add(LSTM(128, activation='relu', return_sequences=True))
    model.add(LSTM(128, activation='relu', return_sequences=False))
    model.add(Dense(1))
    model.compile(optimizer='adam', loss='mse')
    history =model.fit_generator(train_generator,epochs=50, batch_size=1,verbose=1)

# Get a SageMaker-compatible role used by this Notebook Instance.
    role = get_execution_role()
    with open(output_model_dir + '/history.json', 'w') as f:
         json.dump(history.history, f)
    #Save the Scaler
    dump(scaler, output_model_dir + '/scaler.model', protocol=2) 
    #Save the trained model and weights
    model_json = model.to_json()
    with open(output_model_dir + ""/model.json"", ""w"") as json_file:
        json_file.write(model_json)
    model.save_weights(output_model_dir + ""/model.h5"")
</code></pre>

<p>here it showing some error:</p>

<pre><code> train_instance_type = ""ml.m4.xlarg""

tf_estimator = TensorFlow(entry_point='time_series.py', role=get_execution_role(),
                          train_instance_count=1, train_instance_type=train_instance_type,
                          framework_version='1.12', py_version='py3', script_mode=True,
                          output_path = 's3://' + s3Bucket, base_job_name = ""sales-forecasting-lstm"",
                         hyperparameters={'batch_size': 2,
                                           'epochs': 50})

tf_estimator.fit(uploaded_data_path)
</code></pre>

<p>Here I got the error. what this error , I didn't understand this error.</p>

<pre><code>UnexpectedStatusException: Error for Training job sales-forecasting-lstm-2020-04-13-10-17-34-919: Failed. Reason: AlgorithmError: ExecuteUserScriptError:
Command ""/usr/bin/python time_series.py --batch_size 2 --epochs 50 --model_dir s3://sagemaker12/sales-forecasting-lstm-2020-04-13-10-17-34-919/model""

​
</code></pre>

<p>Hi, I am newbie to AWS sagemaker, I am trying to  deploy the time series custom  lstm model in sagemaker , please help me out and  how to perpare the script mode , python script  for deployment.
this  my script file <strong>timer_series.py</strong> code. </p>

<pre><code>import sagemaker
import boto3
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import tensorflow 
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import LSTM
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator

from sklearn.metrics import mean_squared_error


if __name__ =='__main__':

    parser.add_argument('--epochs', type=int, default=50)
    parser.add_argument('--batch_size', type=int, default=72)
    parser.add_argument('--n_train_hours', type=int, default=24*365*2)
    parser.add_argument('--n_validation_hours', type=int, default=24*365*4)

    # input data and model directories
    parser.add_argument('--model_dir', type=str)

    args, _ = parser.parse_known_args()

    train_dataset_dir = os.environ.get('SM_INPUT_DIR') + '/data/training/' 
    output_model_dir = os.environ.get('SM_MODEL_DIR')
    output_object_dir = os.environ.get('SM_OUTPUT_DATA_DIR')

    epochs = args.epochs
    batch_size = args.batch_size
    input_data = {args.input_data}
    dataset = read_csv( input_data + 'dataset.csv', header=0, index_col='Date')
    dataset.sort_index(inplace=True)
    train = dataset.iloc[:109]
    test= dataset.iloc[109:]  
    scaler = MinMaxScaler()
    scaled_train = scaler.fit_transform(train)
    scaled_test=scaler.fit_transform(test)
    n_input = 12
    n_feature = 1

    train_generator = TimeseriesGenerator(scaled_train,scaled_train,length=n_input, batch_size=1)

    model = Sequential()

    model.add(LSTM(128,activation = 'relu', input_shape= (n_input, n_feature), return_sequences=True))
    model.add(LSTM(128, activation='relu', return_sequences=True))
    model.add(LSTM(128, activation='relu', return_sequences=False))
    model.add(Dense(1))
    model.compile(optimizer='adam', loss='mse')
    history =model.fit_generator(train_generator,epochs=50, batch_size=1,verbose=1)

# Get a SageMaker-compatible role used by this Notebook Instance.
    role = get_execution_role()
    with open(output_model_dir + '/history.json', 'w') as f:
         json.dump(history.history, f)
    #Save the Scaler
    dump(scaler, output_model_dir + '/scaler.model', protocol=2) 
    #Save the trained model and weights
    model_json = model.to_json()
    with open(output_model_dir + ""/model.json"", ""w"") as json_file:
        json_file.write(model_json)
    model.save_weights(output_model_dir + ""/model.h5"")
</code></pre>

<p>here it showing some error:</p>

<pre><code> train_instance_type = ""ml.m4.xlarg""

tf_estimator = TensorFlow(entry_point='time_series.py', role=get_execution_role(),
                          train_instance_count=1, train_instance_type=train_instance_type,
                          framework_version='1.12', py_version='py3', script_mode=True,
                          output_path = 's3://' + s3Bucket, base_job_name = ""sales-forecasting-lstm"",
                         hyperparameters={'batch_size': 2,
                                           'epochs': 50})

tf_estimator.fit(uploaded_data_path)
</code></pre>

<p>Here I got the error. what this error , I didn't understand this error.</p>

<pre><code>UnexpectedStatusException: Error for Training job sales-forecasting-lstm-2020-04-13-10-17-34-919: Failed. Reason: AlgorithmError: ExecuteUserScriptError:
Command ""/usr/bin/python time_series.py --batch_size 2 --epochs 50 --model_dir s3://sagemaker12/sales-forecasting-lstm-2020-04-13-10-17-34-919/model""

​
</code></pre>",1,7,2020-04-16 10:33:35.170000 UTC,,2020-04-17 13:40:25.143000 UTC,0,amazon-web-services|amazon-sagemaker,710,2020-03-05 13:30:50.007000 UTC,2022-03-08 13:21:45.963000 UTC,"Bangalore, Karnataka, India",1,0,0,15,,,,,,['amazon-sagemaker']
What happens if no `entry_points` are specified in MLproject file?,"<p>If we take the example from <a href=""https://github.com/mlflow/mlflow-example"" rel=""nofollow noreferrer"">here</a>.</p>
<p>What happens if no <code>entry_points</code> are specified in MLproject file?<br />
In this case, <strong>MLproject</strong> file would look like:</p>
<pre class=""lang-yaml prettyprint-override""><code>name: tutorial

conda_env: conda.yaml
</code></pre>
<p>As described in <a href=""https://www.mlflow.org/docs/latest/python_api/mlflow.projects.html#mlflow.projects.run"" rel=""nofollow noreferrer"">docs</a>:</p>
<blockquote>
<p>entry_point – Entry point to run within the project. If no entry point with the specified name is found, runs the project file entry_point as a script, using “python” to run .py files and the default shell (specified by environment variable $SHELL) to run .sh files.</p>
</blockquote>
<p>So, if no <code>entry_points</code> are specified, in our example, it will run something like: <code>python train.py</code>?</p>
<p>Another question:</p>
<ol>
<li>&quot;runs the <strong><em>project file entry_point</em></strong> as a script&quot;.<br />
Where can I find <code>project file entry_point</code> or where is it specified?</li>
</ol>",0,0,2022-02-16 10:46:01.853000 UTC,,,0,mlflow,90,2020-06-16 11:50:13.433000 UTC,2022-09-24 20:47:34.783000 UTC,,775,188,1,11,,,,,,['mlflow']
Setting array of tags to MLFlow registered model,"<p>I have a model registered in ML Flow and would like associate a list of tags to that model.
But when i looked at the reference APIs, it looks like we can add only one tag at a time with a single http request.</p>
<pre><code>https://www.mlflow.org/docs/latest/rest-api.html#set-registered-model-tag
</code></pre>
<p>Is it possible to create an array of tags and associate that with model in a single http call ?
Like how we do during model creation API ?</p>
<pre><code>https://www.mlflow.org/docs/latest/rest-api.html#create-registeredmodel
</code></pre>",0,2,2021-12-07 11:48:04.190000 UTC,,2021-12-07 11:55:14.697000 UTC,0,databricks|azure-databricks|mlflow,173,2019-09-20 08:55:45.383000 UTC,2022-09-23 06:49:38.593000 UTC,,344,39,0,72,,,,,,['mlflow']
mlflow models serve -m mlflow_model,"<p>I have a problem with serve MLflow. When I execute this code :</p>
<pre><code>mlflow models serve -m mlflow_model/
</code></pre>
<p>I have this error:</p>
<pre><code>2022/07/24 14:11:15 INFO mlflow.pyfunc.backend: === Running command 'source /opt/anaconda3/bin/../etc/profile.d/conda.sh &amp;&amp; conda activate mlflow-a116dac3ec81be8ec538fa60f4402b7d813c2192 1&gt;&amp;2 &amp;&amp; exec gunicorn --timeout=60 -b 127.0.0.1:5000 -w 1 ${GUNICORN_CMD_ARGS} -- mlflow.pyfunc.scoring_server.wsgi:app'
[2022-07-24 14:11:16 +0200] [7534] [INFO] Starting gunicorn 20.1.0
[2022-07-24 14:11:16 +0200] [7534] [ERROR] Connection in use: ('127.0.0.1', 5000)
[2022-07-24 14:11:16 +0200] [7534] [ERROR] Retrying in 1 second.
[2022-07-24 14:11:17 +0200] [7534] [ERROR] Connection in use: ('127.0.0.1', 5000)
[2022-07-24 14:11:17 +0200] [7534] [ERROR] Retrying in 1 second.
[2022-07-24 14:11:18 +0200] [7534] [ERROR] Connection in use: ('127.0.0.1', 5000)
[2022-07-24 14:11:18 +0200] [7534] [ERROR] Retrying in 1 second.
[2022-07-24 14:11:19 +0200] [7534] [ERROR] Connection in use: ('127.0.0.1', 5000)
[2022-07-24 14:11:19 +0200] [7534] [ERROR] Retrying in 1 second.
[2022-07-24 14:11:20 +0200] [7534] [ERROR] Connection in use: ('127.0.0.1', 5000)
[2022-07-24 14:11:20 +0200] [7534] [ERROR] Retrying in 1 second.
[2022-07-24 14:11:21 +0200] [7534] [ERROR] Can't connect to ('127.0.0.1', 5000)
Traceback (most recent call last):
 File &quot;/opt/anaconda3/bin/mlflow&quot;, line 8, in &lt;module&gt;
  sys.exit(cli())
 File &quot;/opt/anaconda3/lib/python3.9/site-packages/click/core.py&quot;, line 1128, in __call__
  return self.main(*args, **kwargs)
 File &quot;/opt/anaconda3/lib/python3.9/site-packages/click/core.py&quot;, line 1053, in main
  rv = self.invoke(ctx)
 File &quot;/opt/anaconda3/lib/python3.9/site-packages/click/core.py&quot;, line 1659, in invoke
  return _process_result(sub_ctx.command.invoke(sub_ctx))
 File &quot;/opt/anaconda3/lib/python3.9/site-packages/click/core.py&quot;, line 1659, in invoke
  return _process_result(sub_ctx.command.invoke(sub_ctx))
 File &quot;/opt/anaconda3/lib/python3.9/site-packages/click/core.py&quot;, line 1395, in invoke
  return ctx.invoke(self.callback, **ctx.params)
 File &quot;/opt/anaconda3/lib/python3.9/site-packages/click/core.py&quot;, line 754, in invoke
  return __callback(*args, **kwargs)
 File &quot;/opt/anaconda3/lib/python3.9/site-packages/mlflow/models/cli.py&quot;, line 68, in serve
  return _get_flavor_backend(
 File &quot;/opt/anaconda3/lib/python3.9/site-packages/mlflow/pyfunc/backend.py&quot;, line 261, in serve
  raise Exception(
Exception: Command 'exec gunicorn --timeout=60 -b 127.0.0.1:5000 -w 1 ${GUNICORN_CMD_ARGS} -- mlflow.pyfunc.scoring_server.wsgi:app' returned non zero return code. Return code = 1
</code></pre>
<p>conda.yaml
channels:</p>
<ul>
<li>defaults</li>
<li>anaconda
dependencies:</li>
<li>python=3.9.12</li>
<li>pip&lt;=22.1.2</li>
<li>pip:
<ul>
<li>mlflow</li>
<li>cloudpickle==2.0.0</li>
<li>scikit-learn==1.0.2
name: mlflow-env</li>
</ul>
</li>
</ul>
<p>I have changed the port to 8502 but in vain. I have this message:</p>
<pre><code>[2022-07-26 02:28:06 +0200] [45925] [INFO] Listening at: http://127.0.0.1:8502 (45925)
</code></pre>
<p>[2022-07-26 02:28:06 +0200] [45925] [INFO] Using worker: sync
[2022-07-26 02:28:06 +0200] [45933] [INFO] Booting worker with pid: 45933
/opt/anaconda3/envs/mlflow-278b1bb076f93c0a5f2665638173d22fba5482d5/lib/python3.9/site-packages/lightgbm/<strong>init</strong>.py:40: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.3) compiler.
This means that in case of installing LightGBM from PyPI via the <code>pip install lightgbm</code> command, you don't need to install the gcc compiler anymore.
Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.
You can install the OpenMP library by the following command: <code>brew install libomp</code>.
warnings.warn(&quot;Starting from version 2.2.1, the library file in distribution wheels for macOS &quot;</p>
<p>I show you too my ports already used:</p>
<p>COMMAND     PID             USER   FD   TYPE            DEVICE SIZE/OFF NODE NAME
loginwind   170 mohamedads    8u  IPv4 0xd7d0f4b08a76d93      0t0  UDP <em>:</em>
ControlCe   435 mohamedads   20u  IPv4 0xd7d0f4b08a6a8d3      0t0  UDP <em>:</em>
rapportd    480 mohamedads    3u  IPv4 0xd7d0f4b08a76773      0t0  UDP <em>:</em>
rapportd    480 mohamedads    4u  IPv4 0xd7d0f4b08a76a83      0t0  UDP <em>:</em>
rapportd    480 mohamedads    7u  IPv4 0xd7d0f4b08a78923      0t0  UDP <em>:</em>
identitys   485 mohamedads   26u  IPv4 0xd7d0f4b08a629d3      0t0  UDP <em>:</em>
identitys   485 mohamedads   28u  IPv4 0xd7d0f4b08a49c33      0t0  UDP <em>:</em>
sharingd    493 mohamedads    4u  IPv4 0xd7d0f4b08a42a83      0t0  UDP <em>:</em>
sharingd    493 mohamedads    8u  IPv4 0xd7d0f4b08a42463      0t0  UDP <em>:</em>
sharingd    493 mohamedads    9u  IPv4 0xd7d0f4b08a42153      0t0  UDP <em>:</em>
sharingd    493 mohamedads   10u  IPv4 0xd7d0f4b08a436c3      0t0  UDP <em>:</em>
sharingd    493 mohamedads   14u  IPv4 0xd7d0f4b08a76153      0t0  UDP <em>:</em>
sharingd    493 mohamedads   33u  IPv4 0xd7d0f4b08a6ca83      0t0  UDP <em>:</em>
sharingd    493 mohamedads   34u  IPv4 0xd7d0f4b08a6d0a3      0t0  UDP <em>:</em>
WiFiAgent   511 mohamedads    5u  IPv4 0xd7d0f4b08a6f8d3      0t0  UDP <em>:</em>
Dropbox     548 mohamedads   71u  IPv4 0xd7d0f4fd5823123      0t0  TCP 192.168.0.14:49185-&gt;162.125.6.20:https (ESTABLISHED)
Dropbox     548 mohamedads  107u  IPv6 0xd7d0f596050d7c3      0t0  TCP *:17500 (LISTEN)
Dropbox     548 mohamedads  108u  IPv4 0xd7d0f4fd4f8d67b      0t0  TCP *:17500 (LISTEN)
Dropbox     548 mohamedads  109u  IPv4 0xd7d0f4b08a75b33      0t0  UDP *:17500
Dropbox     548 mohamedads  147u  IPv4 0xd7d0f4fd4f8abcb      0t0  TCP localhost:17603 (LISTEN)
Dropbox     548 mohamedads  148u  IPv4 0xd7d0f4fd4f8ebcb      0t0  TCP localhost:17600 (LISTEN)
Dropbox     548 mohamedads  155u  IPv4 0xd7d0f4fd5825bcb      0t0  TCP 192.168.0.14:65500-&gt;162.125.19.9:https (ESTABLISHED)
Dropbox     548 mohamedads  167u  IPv4 0xd7d0f4fd582467b      0t0  TCP 192.168.0.14:65483-&gt;162.125.19.131:https (ESTABLISHED)
assistant   564 mohamedads   45u  IPv4 0xd7d0f4b08a74ef3      0t0  UDP <em>:</em>
com.apple   793 mohamedads   43u  IPv6 0xd7d0f596050bbc3      0t0  TCP localhost:65493-&gt;localhost:8501 (ESTABLISHED)
Notes      1226 mohamedads   13u  IPv4 0xd7d0f4fd582267b      0t0  TCP 192.168.0.14:65232-&gt;imap.1and1.fr:imaps (ESTABLISHED)
Notes      1226 mohamedads   14u  IPv4 0xd7d0f4fd582267b      0t0  TCP 192.168.0.14:65232-&gt;imap.1and1.fr:imaps (ESTABLISHED)
Notes      1226 mohamedads   37u  IPv6 0xd7d0f59605052c3      0t0  TCP [2a01:e34:ecb0:2610:e1d7:5d2e:8475:4d33]:65084-&gt;wo-in-x6d.1e100.net:imaps (ESTABLISHED)
Notes      1226 mohamedads   38u  IPv6 0xd7d0f59605052c3      0t0  TCP [2a01:e34:ecb0:2610:e1d7:5d2e:8475:4d33]:65084-&gt;wo-in-x6d.1e100.net:imaps (ESTABLISHED)
python3.9  1314 mohamedads    5u  IPv4 0xd7d0f4fd5808bcb      0t0  TCP localhost:commplex-main (LISTEN)
Google    14186 mohamedads   20u  IPv4 0xd7d0f4fd5b41bcb      0t0  TCP 192.168.0.14:65486-&gt;192.168.0.17:8009 (ESTABLISHED)
Google    14186 mohamedads   24u  IPv4 0xd7d0f4fd582abcb      0t0  TCP localhost:65495-&gt;localhost:commplex-link (ESTABLISHED)
Google    14186 mohamedads   27u  IPv6 0xd7d0f59605060c3      0t0  TCP [2a01:e34:ecb0:2610:e1d7:5d2e:8475:4d33]:49156-&gt;[2606:4700::6812:1a91]:https (ESTABLISHED)
Google    14186 mohamedads   30u  IPv4 0xd7d0f4fd4f8967b      0t0  TCP 192.168.0.14:49159-&gt;249.195.120.34.bc.googleusercontent.com:https (ESTABLISHED)
Google    14186 mohamedads   31u  IPv6 0xd7d0f4b08a78c33      0t0  UDP [2a01:e34:ecb0:2610:e1d7:5d2e:8475:4d33]:55745-&gt;par21s22-in-x0a.1e100.net:https
Google    14186 mohamedads   33u  IPv6 0xd7d0f5960507cc3      0t0  TCP [2a01:e34:ecb0:2610:e1d7:5d2e:8475:4d33]:49158-&gt;par10s34-in-x0e.1e100.net:https (ESTABLISHED)
Google    14186 mohamedads   34u  IPv4 0xd7d0f4fd4f8e123      0t0  TCP 192.168.0.14:49155-&gt;249.195.120.34.bc.googleusercontent.com:https (ESTABLISHED)
Google    14186 mohamedads   35u  IPv6 0xd7d0f59605098c3      0t0  TCP [2a01:e34:ecb0:2610:e1d7:5d2e:8475:4d33]:65533-&gt;[2606:4700:4400::ac40:929e]:https (ESTABLISHED)
Google    14186 mohamedads   37u  IPv6 0xd7d0f596050adc3      0t0  TCP [2a01:e34:ecb0:2610:e1d7:5d2e:8475:4d33]:49154-&gt;par21s05-in-x01.1e100.net:https (ESTABLISHED)
Google    14186 mohamedads   38u  IPv4 0xd7d0f4fd581e67b      0t0  TCP 192.168.0.14:49182-&gt;server-52-84-174-19.cdg50.r.cloudfront.net:https (ESTABLISHED)
Google    14186 mohamedads   39u  IPv6 0xd7d0f5960506ec3      0t0  TCP [2a01:e34:ecb0:2610:e1d7:5d2e:8475:4d33]:65507-&gt;[2606:4700:20::681a:cf5]:https (ESTABLISHED)
Google    14186 mohamedads   40u  IPv4 0xd7d0f4fd583667b      0t0  TCP 192.168.0.14:65506-&gt;ec2-52-202-168-65.compute-1.amazonaws.com:https (CLOSED)
Google    14186 mohamedads   41u  IPv6 0xd7d0f596050b4c3      0t0  TCP [2a01:e34:ecb0:2610:e1d7:5d2e:8475:4d33]:49161-&gt;[2600:9000:218d:a000:b:67f0:7600:93a1]:https (ESTABLISHED)
Google    14186 mohamedads   42u  IPv6 0xd7d0f5960508ac3      0t0  TCP [2a01:e34:ecb0:2610:e1d7:5d2e:8475:4d33]:49160-&gt;[2a04:4e42:1d::282]:https (ESTABLISHED)
Google    14186 mohamedads   44u  IPv4 0xd7d0f4fd580c123      0t0  TCP 192.168.0.14:65509-&gt;sledge-cdg.slb.sfdcsvc.net:https (ESTABLISHED)
Google    14186 mohamedads   45u  IPv4 0xd7d0f4fd580b67b      0t0  TCP 192.168.0.14:65508-&gt;sledge-cdg.slb.sfdcsvc.net:https (ESTABLISHED)
Google    14186 mohamedads   48u  IPv6 0xd7d0f596050e5c3      0t0  TCP [2a01:e34:ecb0:2610:e1d7:5d2e:8475:4d33]:65348-&gt;wo-in-f188.1e100.net:5228 (ESTABLISHED)
Google    14186 mohamedads   51u  IPv4 0xd7d0f4fd581dbcb      0t0  TCP 192.168.0.14:49183-&gt;server-52-84-174-118.cdg50.r.cloudfront.net:https (ESTABLISHED)
Google    14186 mohamedads   53u  IPv4 0xd7d0f4fd5825123      0t0  TCP 192.168.0.14:65510-&gt;sledge-cdg.slb.sfdcsvc.net:https (ESTABLISHED)
Google    14186 mohamedads   54u  IPv4 0xd7d0f4fd5b3cbcb      0t0  TCP 192.168.0.14:49173-&gt;par21s19-in-f2.1e100.net:https (ESTABLISHED)
Google    14186 mohamedads   55u  IPv4 0xd7d0f4fd5b41123      0t0  TCP 192.168.0.14:65511-&gt;sledge-cdg.slb.sfdcsvc.net:https (ESTABLISHED)
Google    14186 mohamedads   58u  IPv6 0xd7d0f596050ecc3      0t0  TCP [2a01:e34:ecb0:2610:e1d7:5d2e:8475:4d33]:49163-&gt;[2a04:4e42:1d::720]:https (ESTABLISHED)
Google    14186 mohamedads   59u  IPv6 0xd7d0f59605059c3      0t0  TCP [2a01:e34:ecb0:2610:e1d7:5d2e:8475:4d33]:65527-&gt;[2606:4700:4400::ac40:929e]:https (ESTABLISHED)
Google    14186 mohamedads   62u  IPv6 0xd7d0f5960511dc3      0t0  TCP [2a01:e34:ecb0:2610:e1d7:5d2e:8475:4d33]:49181-&gt;[2001:4860:4802:36::36]:https (ESTABLISHED)
Google    14186 mohamedads   63u  IPv4 0xd7d0f4fd583267b      0t0  TCP 192.168.0.14:49166-&gt;a104-124-109-108.deploy.static.akamaitechnologies.com:https (ESTABLISHED)
Google    14186 mohamedads   65u  IPv6 0xd7d0f59605083c3      0t0  TCP [2a01:e34:ecb0:2610:e1d7:5d2e:8475:4d33]:65522-&gt;[2606:4700::6810:9440]:https (ESTABLISHED)
Google    14186 mohamedads   66u  IPv6 0xd7d0f59605091c3      0t0  TCP [2a01:e34:ecb0:2610:e1d7:5d2e:8475:4d33]:65525-&gt;[2606:4700::6810:9440]:https (ESTABLISHED)
Google    14186 mohamedads   68u  IPv4 0xd7d0f4fd582a123      0t0  TCP 192.168.0.14:65523-&gt;a23-220-25-199.deploy.static.akamaitechnologies.com:https (ESTABLISHED)
Google    14186 mohamedads   71u  IPv4 0xd7d0f4fd5833bcb      0t0  TCP 192.168.0.14:49169-&gt;server-99-86-91-75.cdg50.r.cloudfront.net:https (ESTABLISHED)
Google    14186 mohamedads   72u  IPv6 0xd7d0f59605108c3      0t0  TCP [2a01:e34:ecb0:2610:e1d7:5d2e:8475:4d33]:49172-&gt;[2600:9000:218c:4200:1f:f723:6fc0:93a1]:https (ESTABLISHED)
Google    14186 mohamedads   73u  IPv4 0xd7d0f4fd5808123      0t0  TCP 192.168.0.14:65530-&gt;sledge-cdg.slb.sfdcsvc.net:https (ESTABLISHED)
Google    14186 mohamedads   74u  IPv4 0xd7d0f4fd580767b      0t0  TCP 192.168.0.14:65531-&gt;sledge-cdg.slb.sfdcsvc.net:https (ESTABLISHED)
Google    14186 mohamedads   75u  IPv4 0xd7d0f4fd5837bcb      0t0  TCP 192.168.0.14:49170-&gt;80.142.244.35.bc.googleusercontent.com:https (ESTABLISHED)
Google    14186 mohamedads   76u  IPv6 0xd7d0f59605067c3      0t0  TCP [2a01:e34:ecb0:2610:e1d7:5d2e:8475:4d33]:49167-&gt;[2a04:4e42:54::396]:https (ESTABLISHED)
Google    14186 mohamedads   77u  IPv4 0xd7d0f4fd5833123      0t0  TCP 192.168.0.14:49171-&gt;72.249.186.35.bc.googleusercontent.com:https (ESTABLISHED)
Google    14186 mohamedads   82u  IPv4 0xd7d0f4fd5b3b67b      0t0  TCP 192.168.0.14:49177-&gt;chi.outbrain.com:https (CLOSE_WAIT)
Google    14186 mohamedads   84u  IPv4 0xd7d0f4fd5831bcb      0t0  TCP 192.168.0.14:49178-&gt;chi.outbrain.com:https (CLOSE_WAIT)
Google    14186 mohamedads   85u  IPv4 0xd7d0f4fd5831123      0t0  TCP 192.168.0.14:49179-&gt;151.101.9.140:https (ESTABLISHED)
Google    14186 mohamedads   87u  IPv6 0xd7d0f5960512bc3      0t0  TCP [2a01:e34:ecb0:2610:e1d7:5d2e:8475:4d33]:49174-&gt;[2a04:4e42:1d::720]:https (ESTABLISHED)
Google    14186 mohamedads   93u  IPv4 0xd7d0f4fd581fbcb      0t0  TCP 192.168.0.14:49180-&gt;server-13-32-145-15.cdg50.r.cloudfront.net:https (ESTABLISHED)
pycharm   37549 mohamedads   10u  IPv6 0xd7d0f596050f3c3      0t0  TCP localhost:6942 (LISTEN)
pycharm   37549 mohamedads   46u  IPv6 0xd7d0f5960510fc3      0t0  TCP localhost:63342 (LISTEN)
python3.9 39875 mohamedads   19u  IPv4 0xd7d0f4fd5823bcb      0t0  TCP *:8501 (LISTEN)
python3.9 39875 mohamedads   20u  IPv6 0xd7d0f59605116c3      0t0  TCP *:8501 (LISTEN)
python3.9 39875 mohamedads   29u  IPv6 0xd7d0f596050c2c3      0t0  TCP localhost:8501-&gt;localhost:65493 (ESTABLISHED)
python3.9 43312 mohamedads    5u  IPv4 0xd7d0f4fd581c67b      0t0  TCP localhost:commplex-link (LISTEN)
python3.9 44086 mohamedads   10u  IPv4 0xd7d0f4fd581f123      0t0  TCP localhost:ddi-tcp-2 (LISTEN)
python3.9 44086 mohamedads   11u  IPv6 0xd7d0f59605101c3      0t0  TCP localhost:ddi-tcp-2 (LISTEN)</p>",1,0,2022-07-24 12:22:45.687000 UTC,,2022-07-26 00:36:11.530000 UTC,0,mlflow,85,2022-07-24 12:15:55.250000 UTC,2022-07-26 02:17:46.430000 UTC,,1,0,0,0,,,,,,['mlflow']
Azure ML v2 Pipeline Yaml : Steps not running in the specified Conda Environment,"<p>I am trying to build an azure ML pipeline using the Azureml cli v2 but the steps in the  pipeline are not running in the specified conda environment because I am getting a dependency error (which is already installed in the specified environment). Here is the yaml I am writing to generate the pipeline:</p>
<pre><code>$schema: https://azuremlschemas.azureedge.net/latest/pipelineJob.schema.json
type: pipeline
experiment_name: my_training
description: Training Pipeline to train a model for binary classification
inputs:
 training_images:
    type: uri_folder
    mode: download # pick ro_mount, rw_mount or download
    path: azureml://datastores/mydatastore/paths/my_data/dummy_dataset/**

outputs:
  step_output_train:
    type: uri_folder
settings:
  default_datastore: azureml:mydatastore
  continue_on_step_failure: false

jobs:
  train:
    name: training
    display_name: Model-training
    environment: azureml:training_env@latest
    code: ../../my_code/training
    command: &gt;-
      python train.py
      --step_output ${{outputs.step_output}}
      --epochs ${{inputs.epochs}}
    inputs:
      epochs: 1  
    outputs:
      step_output: ${{parent.outputs.step_output_train}}
    compute: azureml:mycomputeclust
    resources:
      instance_count: 1 

</code></pre>
<p>Please guide me how we can tackle this issue.</p>",0,1,2022-07-05 05:48:40.017000 UTC,,,0,azure|azure-pipelines|azure-machine-learning-service,86,2018-08-19 06:02:24.467000 UTC,2022-09-01 13:39:30.257000 UTC,,41,1,0,15,,,,,,['azure-machine-learning-service']
Azure Machine Learning Computes - Template properties - Required properties for attach operation,"<p>As described in <a href=""https://docs.microsoft.com/en-us/azure/templates/microsoft.machinelearningservices/workspaces/computes?tabs=bicep"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/templates/microsoft.machinelearningservices/workspaces/computes?tabs=bicep</a> there are the properties <code>location</code>, <code>sku</code>, <code>tags</code> and <code>identity</code>.</p>
<p>For me it is not clear whether these properties relate to the parent workspace or to the compute resource (e.g. as the there is also <code>computeLocation</code> or <code>sku</code> as far as I can see should have the same value as the workspace)...</p>
<p>It would be great when someone can clarify to which resource these properties and related values belong to (workspace vs. compute resource).</p>
<p><strong>EDIT:</strong>
Also: which properties are actually required for attach versus create? E.g. do I need <code>identity</code> or <code>computeLocation</code> for attach, and if yes what is the purpose of it as the compute resource is being created in another context?</p>
<p>I also figured out that <code>location</code> as well as <code>disableLocalAuth</code> are required for the attach operation - why when the resource is being deployed in another context and only attached?</p>
<p>And why do I get <code>unsupported compute type</code> when checking for the compute resources via Azure CLI for the attached AKS?</p>
<pre><code>{
    &quot;description&quot;: &quot;Default AKS Cluster&quot;,
    &quot;id&quot;: &quot;/subscriptions/xxx/resourceGroups/xxx/providers/Microsoft.MachineLearningServices/workspaces/xxx/computes/DefaultAKS&quot;,
    &quot;location&quot;: &quot;westeurope&quot;,
    &quot;name&quot;: &quot;DefaultAKS&quot;,
    &quot;provisioning_state&quot;: &quot;Succeeded&quot;,
    &quot;resourceGroup&quot;: &quot;xxx&quot;,
    &quot;resource_id&quot;: &quot;/subscriptions/xxx/resourcegroups/xxx/providers/Microsoft.ContainerService/managedClusters/xxx&quot;,
    &quot;type&quot;: &quot;*** unsupported compute type ***&quot;
}
</code></pre>
<p><strong>EDIT-2:</strong></p>
<p>So based on the response from @SairamTadepalli-MT all the properties actually relate to the compute resource - what makes sense. Still, I don't understand the purpose of a few of these properties. For instance why is there a &quot;location&quot; and a &quot;computeLocation&quot; or what is the meaning of &quot;sku&quot; (e.g. I tried &quot;AmlCompute&quot; and provided the value &quot;Basic&quot; - but &quot;Basic&quot; is the &quot;sku&quot; of the workspace and for &quot;AmlCompute&quot; the size is actually defined by the &quot;vmSize&quot; or?...).</p>
<p>What brings me to the next point: the current documentation currently lacks a detailed description in which scenarios which properties can have which values respectively need to be provided (beside &quot;properties&quot;).</p>
<p>This is also true for attach (i.e. providing a &quot;resourceId&quot;) vs. create (i.e. providing &quot;properties&quot;): which properties are actually required for attach? For what I figured out it requires &quot;location&quot; and &quot;disableLocalAuth&quot; - why do I need these properties as I would assume &quot;name&quot; and &quot;resourceId&quot; (and maybe &quot;computeType&quot;) should be sufficient to attach a compute resource? What is the purpose of properties like &quot;sku&quot;, &quot;tags&quot; or &quot;identity&quot; when I attach an existing compute resource?</p>
<p>Finally regarding &quot;unsupported compute type&quot;: not sure if your response really helps me. The AKS is successfully attached, so I don't understand why I get &quot;unsupported compute type&quot;. This should be fixed.</p>",1,0,2022-04-16 22:14:33.330000 UTC,,2022-04-19 07:22:58.357000 UTC,0,azure-machine-learning-service|azure-bicep,96,2016-10-12 19:46:58.963000 UTC,2022-09-15 10:44:58.510000 UTC,,8729,128,0,251,,,,,,['azure-machine-learning-service']
SageMaker cannot create first user,"<p>I need to create first user to getting start, but Amazon won't let me.</p>
<blockquote>
<p>ResourceLimitExceeded The account-level service limit 'Maximum number
of user profiles per domain' is 0 UserProfiles, with current
utilization of 0 UserProfiles and a request delta of 1 UserProfiles.
Please contact AWS support to request an increase for this limit.</p>
</blockquote>
<p>I cannot get pass the quickstart guide.</p>
<p><a href=""https://i.stack.imgur.com/Twa8Q.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Twa8Q.png"" alt=""enter image description here"" /></a></p>
<p>How should I fix ? What did I miss ?</p>",1,0,2021-01-11 09:39:48.797000 UTC,,,1,amazon-sagemaker,378,2015-08-21 06:48:55.517000 UTC,2022-09-25 03:29:15.820000 UTC,"Ho Chi Minh City, Vietnam",4748,4859,15,596,,,,,,['amazon-sagemaker']
sagemaker-tensorflow-serving-container - is there a way to configure underlying tensorflow_model_server REST API timeout,"<p>TensorFlow Model Server has a command-line option '--rest_api_timeout_in_ms' that controls the timeout for its rest api. I believe by default this is 30 seconds. I am serving a (slow) TF model with sagemaker-tensorflow-container (<a href=""https://github.com/aws/sagemaker-tensorflow-serving-container"" rel=""nofollow noreferrer"">https://github.com/aws/sagemaker-tensorflow-serving-container</a>) and getting timeouts from the underlying tensorflow model server process (which is started by the sagemaker container, see here: <a href=""https://github.com/aws/sagemaker-tensorflow-serving-container/blob/3952606048615297e5629b2b27dfa6557616b986/docker/build_artifacts/sagemaker/serve.py#L178"" rel=""nofollow noreferrer"">https://github.com/aws/sagemaker-tensorflow-serving-container/blob/3952606048615297e5629b2b27dfa6557616b986/docker/build_artifacts/sagemaker/serve.py#L178</a></p>
<p>Looking at the sagemaker-tensorflow-container source I do not see a way to supply this '--rest_api_timeout_in_ms' option :-(.</p>
<p>If anyone faced this or similar problem, I would really appreciate any hints or possible workarounds. Thanks!</p>",0,0,2020-07-02 14:25:38.290000 UTC,,,1,tensorflow-serving|amazon-sagemaker,183,2020-07-02 14:11:05.470000 UTC,2020-11-13 17:10:10.680000 UTC,,11,0,0,2,,,,,,['amazon-sagemaker']
AWS SageMaker GroundTruth permissions issue (can't read manifest),"<p>I'm trying to run a simple GroundTruth labeling job with a public workforce. I upload my images to S3, start creating the labeling job, generate the manifest using their tool automatically, and explicitly specify a role that most certainly has permissions on both S3 bucket (input and output) as well as full access to SageMaker. Then I create the job (standard rest of stuff -- I just wanted to be clear that I'm doing all of that).</p>

<p>At first, everything looks fine. All green lights, it says it's in progress, and the images are properly showing up in the bottom where the dataset is. However, after a few minutes, the status changes to Failure and I get this: <code>ClientError: Access Denied. Cannot access manifest file: arn:aws:sagemaker:us-east-1:&lt;account number&gt;:labeling-job/&lt;job name&gt; using roleArn: null</code> in the reason for failure.</p>

<p>I also get the error underneath (where there used to be images but now there are none):</p>

<p><code>The specified key &lt;job name&gt;/manifests/output/output.manifest isn't present in the S3 bucket &lt;output bucket&gt;</code>.</p>

<p>I'm very confused for a couple of reasons. First of all, this is a super simple job. I'm just trying to do the most basic bounding box example I can think of. So this should be a very well-tested path. Second, I'm explicitly specifying a role arn, so I have no idea why it's saying it's null in the error message. Is this an Amazon glitch or could I be doing something wrong?</p>",4,0,2019-02-13 01:01:54.550000 UTC,0.0,2019-02-13 01:31:24.527000 UTC,5,amazon-web-services|amazon-sagemaker,3282,2012-01-04 07:55:26.970000 UTC,2022-09-24 23:53:29.847000 UTC,,2906,467,16,335,,,,,,['amazon-sagemaker']
Error when deploying cross account Sagemaker Endpoints,"<p>I am using cdk to deploy a Sagemaker Endpoint in a cross-account context.</p>
<p>The following error appears when creating the Sagemaker Endpoint:
Failed to download model data for container &quot;container_1&quot; from URL: &quot;s3://.../model.tar.gz&quot;. Please ensure that there is an object located at the URL and that the role passed to CreateModel has permissions to download the object.</p>
<p>Here are some useful details.</p>
<p>I have two accounts:</p>
<ul>
<li>Account A: includes the encrypted s3 bucket in which the model artifact has been saved, the Sagemaker model package group with the latest approved version and a CodePipeline that deploys the endpoint in the account A itself and account B.</li>
<li>Account B: includes the endpoint deployed by CodePipeline in Account A.</li>
</ul>
<p>In AccountA:</p>
<ul>
<li>The cross account permissions are set both for the bucket and the kms key used to encrypt that bucket</li>
</ul>
<pre class=""lang-js prettyprint-override""><code>// Create bucket and kms key to be used by Sagemaker Pipeline

        //KMS
        const sagemakerKmsKey = new Key(
            this,
            &quot;SagemakerBucketKMSKey&quot;,
            {
                description: &quot;key used for encryption of data in Amazon S3&quot;,
                enableKeyRotation: true,
                policy: new PolicyDocument(
                    {
                        statements:[
                            new PolicyStatement(
                                {
                                    actions:[&quot;kms:*&quot;],
                                    effect: Effect.ALLOW,
                                    resources:[&quot;*&quot;],
                                    principals: [new AccountRootPrincipal()]
                                }
                            ),
                            new PolicyStatement(
                                {
                                    actions:[
                                        &quot;kms:*&quot;
                                    ],
                                    effect: Effect.ALLOW,
                                    resources:[&quot;*&quot;],
                                    principals: [
                                        new ArnPrincipal(`arn:${Aws.PARTITION}:iam::${AccountA}:root`),
                                        new ArnPrincipal(`arn:${Aws.PARTITION}:iam::${AccountB}:root`),
                                    ]
                                }
                            )
                        ]
                    }
                )
            }
        )

        // S3 Bucket
        const sagemakerArtifactBucket = new Bucket(
            this,
            &quot;SagemakerArtifactBucket&quot;,
            {
                bucketName:`mlops-${projectName}-${Aws.REGION}`,
                encryptionKey:sagemakerKmsKey,
                versioned:false,
                removalPolicy: RemovalPolicy.DESTROY
            }
        )
        
        sagemakerArtifactBucket.addToResourcePolicy(
            new PolicyStatement(
                {
                    actions: [
                        &quot;s3:*&quot;,
                    ],
                    resources: [
                        sagemakerArtifactBucket.bucketArn,
                        `${sagemakerArtifactBucket.bucketArn}/*`
                    ],
                    principals: [
                        new ArnPrincipal(`arn:${Aws.PARTITION}:iam::${AccountA}:root`),
                        new ArnPrincipal(`arn:${Aws.PARTITION}:iam::${AccountB}:root`),
                    ]
                }
            )
        )
</code></pre>
<ul>
<li>A CodeDeploy Action is used to deploy the Sagemaker Endpoint in AccountA and AccountB.</li>
</ul>
<pre class=""lang-js prettyprint-override""><code>// Define Code Build Deploy Staging Action
        const deployStagingAction = new CloudFormationCreateUpdateStackAction(
            {
                actionName: &quot;DeployStagingAction&quot;,
                runOrder: 1,
                adminPermissions: false,
                stackName: `${projectName}EndpointStaging`,
                templatePath: cdKSynthArtifact.atPath(&quot;staging.template.json&quot;),
                replaceOnFailure: true,
                role: Role.fromRoleArn(
                    this,
                    &quot;StagingActionRole&quot;,
                    `arn:${Aws.PARTITION}:iam::${AccountB}:role/cdk-hnb659fds-deploy-role-${AccountB}-${Aws.REGION}`,
                ),
                deploymentRole: Role.fromRoleArn(
                    this,
                    &quot;StagingDeploymentRole&quot;,
                    `arn:${Aws.PARTITION}:iam::${AccountB}:role/cdk-hnb659fds-cfn-exec-role-${AccountB}-${Aws.REGION}`
                ),
                cfnCapabilities: [
                    CfnCapabilities.AUTO_EXPAND,
                    CfnCapabilities.NAMED_IAM
                ]
            }
        )
</code></pre>
<p>Specifically, the role that creates the Sagemaker Model and Sagemaker Endpoints should be cdk-hnb659fds-cfn-exec-role, as seen on CloudTrail, but for testing purposes I've granted to both of them Administrator privileges (the error still appears).</p>
<p>The deployment in AccountA is correctly executed, thus it means that the bucket location is correct.</p>
<p>NOTE: everything is deployed correctly up to the Sagemaker Endpoint.</p>",1,3,2022-07-18 18:50:17.303000 UTC,,,0,typescript|amazon-web-services|amazon-cloudformation|aws-cdk|amazon-sagemaker,110,2017-07-06 12:59:04.303000 UTC,2022-08-20 15:29:04.513000 UTC,"Isola d'Ischia, NA, Italia",137,17,0,58,,,,,,['amazon-sagemaker']
MLflow Artifacts Storing artifacts(google cloud storage) but not displaying them in MLFlow UI,"<p>I am working on a docker environment(docker-compose) with a jupyter notebook docker image and a postgres docker image for running ML models and using google cloud storage to store the model artifacts. Storing the models on the cloud storage works fine but i can't get to show them within the MLFlow UI. I have seen similar problems but non of the solutions used google cloud storage as the storage location for artifacts. The error message says the following <code>Unable to list artifacts stored under &lt;gs-location&gt; for the current run. Please contact your tracking server administrator to notify them of this error, which can happen when the tracking server lacks permission to list artifacts under the current run's root artifact directory.</code>What could possibly be causing this problem?</p>",2,5,2020-09-03 15:47:21.450000 UTC,,,0,docker-compose|google-cloud-storage|artifacts|mlflow,1087,2016-10-17 21:17:27.223000 UTC,2022-07-05 17:50:06.730000 UTC,"Cape Town, South Africa",11,0,0,9,,,,,,['mlflow']
How to detect near duplicate rows in Azure Machine Learning?,<p>I am new to azure machine learning. We are trying to implement <strong>questions similarity algorithm using azure machine</strong> learning. We have large set of questions and answers. Our objective is to identify whether newly added questions are duplicates or not? Just like Stackoverflow suggests existing questions when we ask new questions?Can we use azure machine learning services to solve this? Can someone guide us in the right direction? </p>,2,2,2016-02-12 08:56:02.477000 UTC,,,0,azure|azure-machine-learning-studio,1751,2016-01-04 07:16:54.093000 UTC,2016-05-13 05:47:44.760000 UTC,,41,0,0,6,,,,,,['azure-machine-learning-studio']
SageMaker - clarification on SageMaker entities in CloudFormation,"<h1>Question</h1>

<p>Would like to clarify the entities in AWS::SageMaker. </p>

<h3>SageMaker Model</h3>

<p>When looked at the diagram in the <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-hosting.html"" rel=""nofollow noreferrer"">Deploy a Model on Amazon SageMaker Hosting Services</a>, the <strong>Model</strong> artifacts in SageMaker is the data generated by a ML algorithm docker container in the Model training phase, and stored in a S3 bucket.</p>

<p><a href=""https://i.stack.imgur.com/8k12v.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8k12v.jpg"" alt=""enter image description here""></a></p>

<p>However, AWS::SageMaker::Model seems to have captured a docker image to run the inference code in a SageMaker endpoint instance. There is no reference to the model data in a S3 bucket. Hence wonder why it is called AWS::SageMaker::<strong>Model</strong> and why not called such as AWS::SageMaker::<strong>InferenceImage</strong>.</p>

<p>1-1. What is <strong>Model</strong> in AWS SageMaker? </p>

<p>1-2. Is it a docker image (algorithm) to do the prediction/inference, not the data to run the algorithm on? </p>

<p>1-3. Does AWS call the runtime (docker runtime + docker image for inference) as Model?</p>

<p><a href=""https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-sagemaker-model.html"" rel=""nofollow noreferrer"">AWS::SageMaker::Model</a></p>

<pre><code>Type: AWS::SageMaker::Model
Properties: 
  Containers: 
    - ContainerDefinition
  ExecutionRoleArn: String
  ModelName: String
  PrimaryContainer: 
    ContainerDefinition
  Tags: 
    - Tag
  VpcConfig: 
    VpcConfig
</code></pre>

<h3>SageMaker Endpoint or SageMaker Estimator from a model data in S3</h3>

<p>The SageMaker Estimator has an argument <strong>output_path</strong> as in <a href=""https://sagemaker.readthedocs.io/en/stable/estimators.html"" rel=""nofollow noreferrer"">Python SDK Estimators</a>.</p>

<blockquote>
  <p>S3 location for saving the training result (model artifacts and output files). If not specified, results are stored to a default bucket. If the bucket with the specific name does not exist, the estimator creates the bucket during the fit() method execution.</p>
</blockquote>

<p>For Python ML environment, we can use <a href=""https://docs.python.org/2/library/pickle.html"" rel=""nofollow noreferrer"">pickle</a> to export the data and reload back into a model as in <a href=""https://scikit-learn.org/stable/modules/model_persistence.html"" rel=""nofollow noreferrer"">3.4. Model persistence</a>. We can do the same for <a href=""https://spark.apache.org/docs/latest/ml-pipeline.html#ml-persistence-saving-and-loading-pipelines"" rel=""nofollow noreferrer"">Spark ML</a>.</p>

<p>2-1. What is the equivalent in SageMaker as AWS::SageMaker::Model has no argument to refer to a data in a S3 bucket?</p>

<p>2-2. Can SageMaker Estimator be re-created using the model data in S3 bucket?</p>

<h3>SageMaker Estimator</h3>

<p>I thought there would be a resource to define a SageMaker Estimator in CloudFormation, but looks there is none. </p>

<p>3-1. Please help understand if there is a reason.</p>",1,0,2020-03-27 08:30:19.493000 UTC,,,1,amazon-web-services|amazon-sagemaker,357,2014-11-22 09:22:35.470000 UTC,2022-09-24 22:13:03.237000 UTC,,14749,641,62,968,,,,,,['amazon-sagemaker']
Import PKL in sagemaker AWS to publish API,"<p>I have a .pkl file that is the result of a trained model. And I want to create an endpoint from sage maker to be able to consume the predictions, and I have already managed to read the file from s3 but I can't find exact documentation on how to expose the &quot;compiled&quot; as API</p>
<pre><code>s3 = boto3.resource('s3')
bucket = s3.Bucket(&quot;sagemake-models-workshop&quot;).Object(&quot;pikle- 
file/contatos/xgb_contratos_mensual_RandomizedSearchLinux.pkl&quot;).get()['Body'].read()

bucket_pickle = pickle.loads(bucket)
</code></pre>
<p>output :</p>
<pre><code>bucket_pickle

XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
         colsample_bynode=1, colsample_bytree=0.1, gamma=0, gpu_id=-1,
         importance_type='gain', interaction_constraints='',
         learning_rate=0.33, max_delta_step=0, max_depth=3,
         min_child_weight=1, missing=nan, monotone_constraints='()',
         n_estimators=150, n_jobs=0, num_parallel_tree=1, random_state=0,
         reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
         tree_method='exact', validate_parameters=1, verbosity=None)
</code></pre>",1,0,2022-02-17 04:19:06.387000 UTC,,2022-02-17 06:11:22.740000 UTC,0,python-3.x|amazon-web-services|xgboost|amazon-sagemaker,112,2017-08-30 17:12:37.020000 UTC,2022-07-12 20:31:44.507000 UTC,Perú,17,0,0,5,,,,,,['amazon-sagemaker']
Can I set Azure ML output to real JSON using Azure API Management?,"<p>I have started using Azure ML to deploy ML service, but it sent results as raw text. I see Azure API Management can use to set outbound body. Can I use it to convert raw text to JSON? and how?</p>
<p>This is an example result from Azure ML WebService.</p>
<pre><code>&quot;{\&quot;transcript\&quot;: \&quot;\\u0e27\\u0e31\\u0e19\&quot;}&quot;
</code></pre>
<p>Another question, Can I decode UTF-8 in set-body policy?</p>",1,0,2022-02-09 22:17:50.460000 UTC,,2022-02-10 04:41:59.117000 UTC,0,azure|azure-api-management|azure-machine-learning-studio|azure-machine-learning-service,76,2020-04-19 09:04:21.137000 UTC,2022-08-18 12:43:12.697000 UTC,"Bangkok, Thailand",23,4,0,6,,,,,,"['azure-machine-learning-studio', 'azure-machine-learning-service']"
Automate AI model training and create endpoint whenever the train data is updated using aws Sagemaker,"<p>I am searching for good references(videos or texts) to automate my AI (custom NLP model) training job whenever I update my data in s3 bucket i..e, whenever my pipeline detects change in data, it starts training automatically and create the endpoint. I am currently using aws API gateway and lambda to communicate with the endpoint. However, for each time I update my train data, I need to run the training job manually to create new endpoint. I want to automate the later part.
Is there anyone in this group who can help me with the relevant resources to achieve this?
Thank You!</p>",1,0,2022-09-16 05:59:06.833000 UTC,,,0,amazon-web-services|pytorch|amazon-sagemaker,8,2021-12-22 19:56:53.417000 UTC,2022-09-22 06:55:49.833000 UTC,,1,0,0,2,,,,,,['amazon-sagemaker']
"Why is MLFLow unable to log metrics, artifacts while using MLFlow Project in Docker environment?","<p>I am trying to store metrics and artifacts on host after running MLProject in a docker environment.I am expecting that when the experiment completes successfully, artifacts, metrics folders in mlruns/ folder should have values and be shown on mlflow ui but artifacts, metrics folders in mlruns/ folder are empty. mlflow ui is also not reflecting the new experiment.</p>
<p>/home/mlflow_demo/mlflow-demo.py -</p>
<pre><code>import mlflow
from mlflow.tracking import MlflowClient
from random import random
import pickle

client = MlflowClient()
experiment_id = client.create_experiment(name='first experiment')
run = client.create_run(experiment_id=experiment_id)
for i in range(1000):
 client.log_metric(run.info.run_id,&quot;foo&quot;,random(),step=i)
with open(&quot;test.txt&quot;,&quot;w&quot;) as f:
 f.write(&quot;This is an artifact file&quot;)
client.log_artifact(run.info.run_id,&quot;test.txt&quot;)
client.set_terminated(run.info.run_id)
</code></pre>
<p>/home/mlflow_demo/MLProject -</p>
<pre><code>name: test-project
docker_env:
 image: kusur/apex-pytorch-image:latest
entry_points:
 main:
  command: &quot;python mlflow-demo.py&quot;
</code></pre>
<p>command (executed in /home/mlflow_demo): - <code>mlflow run .</code></p>
<p>After running the above code, I get the following log -</p>
<pre><code>2021/07/06 12:22:28 INFO mlflow.projects.docker: === Building docker image test-project ===
2021/07/06 12:22:28 INFO mlflow.projects.utils: === Created directory /home/mlflow_demo/mlruns/tmpwa8ydc5j for downloading remote URIs passed to arguments of type 'path' ===
2021/07/06 12:22:28 INFO mlflow.projects.backend.local: === Running command 'docker run --rm -v /home/mlflow_demo/mlruns:/mlflow/tmp/mlruns -v /home/mlflow_demo/mlruns/0/0978fdd89ba44bf7b49975ab84838e82/artifacts:/home/mlflow_demo/mlruns/0/0978fdd89ba44bf7b49975ab84838e82/artifacts -e MLFLOW_RUN_ID=0978fdd89ba44bf7b49975ab84838e82 -e MLFLOW_TRACKING_URI=file:///mlflow/tmp/mlruns -e MLFLOW_EXPERIMENT_ID=0 test-project:latest python mlflow-demo.py' in run with ID '0978fdd89ba44bf7b49975ab84838e82' ===

...

2021/07/06 12:22:33 INFO mlflow.projects: === Run (ID '0978fdd89ba44bf7b49975ab84838e82') succeeded ===
</code></pre>
<p>Still the folders mlruns/0/0978fdd89ba44bf7b49975ab84838e82/artifacts and mlruns/0/0978fdd89ba44bf7b49975ab84838e82/metrics are empty.</p>
<p>Can someone please provide the pointers. Please let me know if the question isn't well framed.</p>",0,0,2021-07-07 14:38:10.113000 UTC,,2021-07-16 15:18:22.083000 UTC,1,python-3.x|mlflow|mlops,275,2012-08-04 05:33:50.707000 UTC,2022-09-08 13:52:47.757000 UTC,"Mumbai, Maharashtra, India",568,97,2,127,,,,,,['mlflow']
Include Scala 2.12 in SageMaker Jupyter Notebook,"<p>I have Scala project build with scala verison 2.12 and I can't change scala version of the project.   I have create jar file of this project let's call it scalaproject.jar and now I want to include this jar file in spark session. When I create spark session by doing :</p>
<pre><code>spark = SparkSession.builder.config(&quot;spark.driver.extraClassPath&quot;, &quot;scalaproject.jar&quot;).getOrCreate()
</code></pre>
<p>I get below error:</p>
<pre><code>Py4JJavaError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.
: java.lang.NoSuchMethodError: scala.Product.$init$(Lscala/Product;)V
    at scala.xml.Null$.&lt;init&gt;(Null.scala:23)
    at scala.xml.Null$.&lt;clinit&gt;(Null.scala)
    at org.apache.spark.ui.jobs.AllJobsPage.&lt;init&gt;(AllJobsPage.scala:43)
    at org.apache.spark.ui.jobs.JobsTab.&lt;init&gt;(JobsTab.scala:45)
    at org.apache.spark.ui.SparkUI.initialize(SparkUI.scala:61)
    at org.apache.spark.ui.SparkUI.&lt;init&gt;(SparkUI.scala:80)
    at org.apache.spark.ui.SparkUI$.create(SparkUI.scala:175)
    at org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:444)
    at org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:58)
    at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
    at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
    at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
    at py4j.Gateway.invoke(Gateway.java:238)
    at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
    at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
    at py4j.GatewayConnection.run(GatewayConnection.java:238)
    at java.base/java.lang.Thread.run(Thread.java:834)
</code></pre>
<p>From <a href=""https://stackoverflow.com/questions/46293697/exception-in-thread-main-java-lang-nosuchmethoderror-scala-product-initlsc"">this </a>post, I know that I am getting this issue because of version mismatch. My project is using scala 2.12 and SageMaker notebook is using scala 2.11.</p>
<p>I have scala2.12.jar that I want to send to sparksession so when spark session is create, it can use scala2.12.jar to run scalaproject.jar.</p>
<p>I tried adding scala2.12.jar by doing:</p>
<pre><code>spark = SparkSession.builder.config(&quot;spark.driver.extraLibraryPath&quot;, &quot;scala2.12.jar&quot;).config(&quot;spark.driver.extraClassPath&quot;, &quot;scalaproject.jar&quot;).getOrCreate()
</code></pre>
<p>But error is still the same. Can someone please guide me as to how can I add scala2.12.jar in to sparksession in SageMaker notebook?</p>",0,0,2022-04-03 06:49:13.830000 UTC,,,1,scala|apache-spark|pyspark|amazon-sagemaker,131,2022-03-30 14:13:10.150000 UTC,2022-09-19 23:04:55.960000 UTC,,13,0,0,2,,,,,,['amazon-sagemaker']
"Unable to proceed with ""Automate model retraining with Amazon SageMaker Pipelines when drift is detected""","<p>Has anyone worked on &quot;Automate model retraining with Amazon SageMaker Pipelines when drift is detected&quot;? I am unable to create pipeline in &quot;Run the training pipeline&quot; sub-section by following it's official blogpost. I am trying to create continual learning pipeline so that whenever a new training data will be available model will get re-trained only on that incoming data and becomes more intelligent over time.</p>
<p>Article link - <a href=""https://aws.amazon.com/blogs/machine-learning/automate-model-retraining-with-amazon-sagemaker-pipelines-when-drift-is-detected/"" rel=""nofollow noreferrer"">https://aws.amazon.com/blogs/machine-learning/automate-model-retraining-with-amazon-sagemaker-pipelines-when-drift-is-detected/</a></p>",0,1,2022-09-05 15:43:53.327000 UTC,,2022-09-05 20:51:34.930000 UTC,-3,amazon-web-services|machine-learning|pipeline|amazon-sagemaker,15,2017-05-26 16:55:23.953000 UTC,2022-09-23 05:23:42.283000 UTC,"Madhubani, Bihar, India",1,0,0,1,,,,,,['amazon-sagemaker']
Getting error while connecting ADLS to Notebook in AML,"<p>I am getting below error while connecting dataset created and registered in AML notebook and which is based on ADLS. When I connect this dataset in designer I am able to visualize the same. Below is the code that I am using. Please let me know the solution if anyone have faced the same error.</p>
<h3>Examle 1 Import dataset to notebbok</h3>
<pre class=""lang-py prettyprint-override""><code>from azureml.core import Workspace, Dataset

subscription_id = 'abcd'
resource_group = 'RGB'
workspace_name = 'DSG'

workspace = Workspace(subscription_id, resource_group, workspace_name)
dataset = Dataset.get_by_name(workspace, name='abc')
dataset.to_pandas_dataframe()
</code></pre>
<h3>Error 1</h3>
<pre><code>ExecutionError: Could not execute the specified transform.
(Error in getting metadata for path /local/top.txt.
Operation: GETFILESTATUS failed with Unknown Error: The operation has timed out..
Last encountered exception thrown after 5 tries.
[The operation has timed out.,The operation has timed out.,The operation has timed out.,The operation has timed out.,The operation has timed out.]
[ServerRequestId:])|session_id=2d67
</code></pre>
<h3>Example 2 Import data from datastore to notebook</h3>
<pre class=""lang-py prettyprint-override""><code>from azureml.core import Workspace, Datastore, Dataset

datastore_name = 'abc'
workspace = Workspace.from_config()

datastore = Datastore.get(workspace, datastore_name)
datastore_paths = [(datastore, '/local/top.txt')]
df_ds = Dataset.Tabular.from_delimited_files(
    path=datastore_paths, validate=True,
    include_path=False, infer_column_types=True,
    set_column_types=None, separator='\t',
    header=True, partition_format=None
    )

df = df_ds.to_pandas_dataframe()
</code></pre>
<h3>Error 2</h3>
<pre><code>Cannot load any data from the specified path. Make sure the path is accessible.
</code></pre>",2,1,2020-04-01 05:45:22.157000 UTC,,2020-06-20 09:12:55.060000 UTC,1,azure-data-lake|azure-machine-learning-service,277,2020-04-01 05:43:43.390000 UTC,2021-11-20 18:00:08.373000 UTC,,11,0,0,0,,,,,,['azure-machine-learning-service']
Can we print the configurations on which the MLflow server has started?,"<p>I am using the following command to start the MLflow server:</p>
<pre><code>mlflow server --backend-store-uri postgresql://mlflow_user:mlflow@localhost/mlflow  --artifacts-destination &lt;S3 bucket location&gt; --serve-artifacts  -h 0.0.0.0 -p 8000
</code></pre>
<p>Before production deployment, we have a requirement that we need to print or fetch the under what configurations the server is running. For example, the above command uses localhost postgres connection and S3 bucket.</p>
<p>Is there a way to achieve this?</p>
<p>Also, how do I set the server's environment as &quot;production&quot;? So finally I should see a log like this:</p>
<pre><code>[LOG] Started MLflow server:
Env: production
postgres: localhost:5432
S3: &lt;S3 bucket path&gt;
</code></pre>",1,1,2022-08-24 07:37:33.987000 UTC,,2022-08-24 07:44:47.603000 UTC,0,machine-learning|mlflow|model-management,17,2017-01-15 18:29:12.247000 UTC,2022-09-24 17:54:49.473000 UTC,"Mumbai, India",4433,121,31,885,,,,,,['mlflow']
Run multiple ipynb and R notebooks in Sagameker Instance,"<p>Question on running multiple ipynb and r notebooks inside sagemaker instance using life cycle config or other way.</p>
<p>So I have four notebooks, 3 ipynb and 1 r. These should run in order and should wait for first one to finish. Question is how should we achieve that?</p>
<p><code>Lambda can start the instance-&gt; Life cycle policy will execute the notebooks BUT how the run order can be accomplished? </code></p>
<p>Thoughts? Not sure if step function can run these notebooks 1 by 1. Thanks.</p>",0,0,2022-05-22 05:01:30.813000 UTC,,,0,amazon-web-services|amazon-sagemaker,25,2017-11-05 18:04:36.277000 UTC,2022-09-03 01:17:57.907000 UTC,,97,7,0,10,,,,,,['amazon-sagemaker']
Microsoft packages revoscalepy and microsoftml source code,"<p>Does anybody know how to find the source code for Microsoft packages for R/Python called <code>revoscalepy</code> and <code>microsoftml</code> (also <code>azure-machine-learning-sdk</code> would be great). These packages contain implementation of different machine learning algos and could be installed by SQL Server installer. Model serialized by <code>revoscalepy</code> could be used directly in SQL script.</p>
<p>Packages seem to be proprietary, but the documentation is confusing because parameters in different methods are not used, algorithms versions are not known definetely. Moreover, results differ too much with well-known <code>scikit-learn</code> library.</p>",0,0,2022-09-02 08:13:24.983000 UTC,,,0,python|sql-server|machine-learning|azure-machine-learning-studio,23,2017-08-27 14:11:32.620000 UTC,2022-09-22 10:47:17.127000 UTC,"Kazán, Россия",1683,155,9,116,,,,,,['azure-machine-learning-studio']
MLflow - How the model storage aborescence works behind the model registry APIs?,"<p>I want to understand how the model storage aborescence works behind the model registry APIs.</p>
<p>I wanted to know if the stageworks like a tag,
if the . pickel model is moved to different branches depending on the stage.
same question for versions ?</p>
<p>I'm working with python on Databricks.
sincerely,</p>
<p>nicolas</p>",0,1,2022-06-29 09:03:39.827000 UTC,,,0,python|model|databricks|mlflow,15,2019-06-03 13:12:59.187000 UTC,2022-06-29 09:32:51.713000 UTC,,1,0,0,0,,,,,,['mlflow']
deploy Huggingface model to SageMaker endpoint,"<p>I need to deploy a large language model (t0pp) on a SageMaker endpoint. I modified the official example to look like this:</p>
<pre><code>from sagemaker.huggingface import HuggingFaceModel
import sagemaker

role = sagemaker.get_execution_role()

hub = {
  'HF_MODEL_ID':'bigscience/T0', # model_id from hf.co/models
  'HF_TASK':'text2text-generation' # NLP task you want to use for predictions
}

# create Hugging Face Model Class
huggingface_model = HuggingFaceModel(
   env=hub,
   role=role, # iam role with permissions to create an Endpoint
   transformers_version=&quot;4.6&quot;, # transformers version used
   pytorch_version=&quot;1.7&quot;, # pytorch version used
   py_version=&quot;py36&quot;, # python version of the DLC
)

# deploy model to SageMaker Inference
predictor = huggingface_model.deploy(
   initial_instance_count=1,
   instance_type=&quot;ml.m5.xlarge&quot;
)
</code></pre>
<p>but I'm getting this error</p>
<pre><code>UnexpectedStatusException: Error hosting endpoint huggingface-pytorch-inference-2022-09-21-15-44-30-116: Failed. Reason: The primary container for production variant AllTraffic did not pass the ping health check.
</code></pre>
<p>Any idea what is going wrong here?</p>",0,2,2022-09-21 18:03:45.707000 UTC,,,0,amazon-sagemaker|huggingface-transformers,13,2016-06-24 15:48:30.737000 UTC,2022-09-23 18:59:59.153000 UTC,"New York, NY, USA",2701,598,6,260,,,,,,['amazon-sagemaker']
AzureML: Engine process terminated. This is most likely due to system running out of memory. Please retry with increased memory,"<p>I am trying to run an experiment on AzureML through the notebook. I get the above error on trying to read a dataset created in previous step.</p>
<p>I checked the memory usage through command - <code>df -h</code> and it looks ok. I checked git links with same error, but that doesn't appear to have been resolved.</p>
<p>Github issues <a href=""https://github.com/Azure/MachineLearningNotebooks/issues/1143"" rel=""nofollow noreferrer"">link</a></p>
<p>What is going wrong here?</p>
<p>Below line of code gives the error. This line had run successfully just a day ago on same workspace, using same compute.</p>
<p><a href=""https://i.stack.imgur.com/vynQe.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vynQe.png"" alt=""enter image description here"" /></a></p>
<p>Below is the screen of memory:</p>
<p><a href=""https://i.stack.imgur.com/Mv3M4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Mv3M4.png"" alt=""enter image description here"" /></a></p>",3,1,2021-09-26 06:36:01.787000 UTC,,2021-09-26 06:42:06.013000 UTC,2,azure-machine-learning-service|azureml-python-sdk,513,2015-03-10 01:48:12.993000 UTC,2022-07-14 11:29:03.527000 UTC,"Bangalore, Karnataka, India",427,28,1,77,,,,,,['azure-machine-learning-service']
Using AWS Elastic Inference without making changes to the client code,"<p>I have an endpoint deployed in SageMaker with a Tensorflow model, and I make calls to it using the Scala SDK like this:</p>

<pre><code>    runtime = AmazonSageMakerRuntimeClientBuilder
        .standard()
        .withCredentials(credentialsProvider)
        .build()
   ...
   val invokeEndpointResult = runtime.invokeEndpoint(request);
</code></pre>

<p>Can i use Sagemaker's Elastic Inferece with this code as it is and gain the performance enhancement of EI? 
I have tried running an endpoint with a configuration of 8 ml.m5d.xlarge instances vs a configuration with 8 ml.m5d.xlarge instances with added EI of ml.eia2.xlarge but looking at the cloud watch metrics I get the same number of invocations per minute, and the total run time (on the same input) is the same.   </p>",0,1,2020-04-24 10:10:18.257000 UTC,,2020-04-25 16:33:14.500000 UTC,1,tensorflow|amazon-sagemaker,58,2016-03-21 08:49:39.920000 UTC,2022-07-17 11:53:14.840000 UTC,,383,10,0,19,,,,,,['amazon-sagemaker']
Sagemaker Neo Compilation witn Dynamic Input Data Size,"<p>I'm trying to compile a <code>PyTorch</code> model to a raspberry pie 3. The model that I am using does not have any limitation in regards to the width and the height. Sagemaker Neo takes in a <code>Data input configuration</code> parameter which looks like it needs to be fixed from the documentation and the examples. </p>

<pre><code>Data Input Configuration:  
Amazon SageMaker needs to know the what the shape of the data matrix is
{""data"": [1, 3, 224, 224]}
</code></pre>

<p>The above example can be found <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/neo-job-compilation-console.html"" rel=""nofollow noreferrer"">here</a>. </p>

<p>I need something like the following: </p>

<pre><code>Data Input Configuration:  
Amazon SageMaker needs to know the what the shape of the data matrix is
{""data"": [1, 3, *, *]}
</code></pre>

<p>entering the above yields the following error message:</p>

<pre><code>JSON.parse: unexpected character at line 1 column 17 of the JSON data
</code></pre>

<p>and fixing the height and the width yields the following message:</p>

<pre><code>ClientError: InputConfiguration: Invalid PyTorch model or input-shape mismatch. Make sure that inputs are lexically ordered and of the correct dimensionality.
</code></pre>

<p>Is there anyway to specify that I want 1 images, with 3 channels, and variable height and width?</p>",0,0,2020-03-21 03:31:41.600000 UTC,,,1,amazon-web-services|pytorch|amazon-sagemaker,221,2013-07-22 01:18:53.410000 UTC,2022-09-25 03:42:45.950000 UTC,"Montreal, QC, Canada",2306,1283,3,182,,,,,,['amazon-sagemaker']
Could not connect to MLFlow model hosted on Docker,"<p>I hosted a model inside a docker container.
On running the DockerFile, It runs the following command:</p>
<p><code>mlflow models serve -m model --port 8080 --no-conda</code></p>
<p>It serves the model succesfully , And I can now make calls to it.
But, I keep getting Max retries exceeded with url</p>
<p>When I host the same model without using Docker(And follow the same steps), it works perfectly.</p>
<p>I use the following command to run the docker container
<code>docker run -it --rm --network host imagename:random</code></p>
<p>I have also tried mapping port 8080, But still not able to get a response.</p>
<p>Not able to understand what the possible issues could be.</p>
<p>Dockerfile for reference</p>
<pre><code>  
FROM ubuntu:20.04
ENV DEBIAN_FRONTEND=noninteractive
RUN apt-get update &amp;&amp; apt-get install -y \
    build-essential software-properties-common\
    libboost-dev libboost-serialization-dev libssl-dev \
    cmake vim\
    wget \
    make libbz2-dev libexpat1-dev swig python-dev
RUN add-apt-repository -y ppa:ubuntugis/ppa &amp;&amp; apt-get -q update
RUN apt-get -y install gdal-bin libgdal-dev
RUN apt-get update

RUN apt install -y python3-pip
RUN pip3 install --upgrade pip
RUN pip install mlflow
RUN pip install pandas

RUN mkdir -p /tmp
COPY ./main.py /tmp/
COPY ./run.sh /tmp/

ENV LC_ALL=C.UTF-8
ENV LANG=C.UTF-8
RUN chmod +x run.sh
CMD ./run.sh
</code></pre>
<p>Where, run.sh is</p>
<pre><code>python3 main.py
mlflow models serve -m /tmp/mlflow_model --port 8080 --no-conda
</code></pre>
<p>When I run the commands of run.sh file outside of docker container, It is able to serve the model correctly,And I get the correct response.</p>",1,0,2021-06-20 19:52:04.787000 UTC,,,1,docker|python-requests|port|mlflow,195,2018-10-08 22:27:01.213000 UTC,2022-07-27 16:40:50.953000 UTC,"New Delhi, Delhi, India",75,3,0,21,,,,,,['mlflow']
Understanding Sagemaker Neo,"<p>I have few questions for <a href=""https://aws.amazon.com/sagemaker/neo/"" rel=""nofollow noreferrer"">Sagemaker Neo</a>:</p>

<p>1) Can I take advantage of Sagemaker Neo if I have an externally trained tensorflow/mxnet model?</p>

<p>2) Sagemaker provides container image for <em>'image-classification'</em> and it has released a new image with name <em>'<a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/neo-deployment-hosting-services-cli.html"" rel=""nofollow noreferrer"">image-classification-neo</a>'</em> for the neo compilation job. What is the difference between both of them? Do I require a new Neo compatible image for each pre built sagemaker template(container) similarly?</p>

<p>Any help would be appreciated</p>

<p>Thanks!!</p>",2,0,2019-03-22 08:36:20.953000 UTC,,,1,amazon-web-services|amazon-sagemaker|amazon-machine-learning,820,2018-01-08 13:38:32.450000 UTC,2021-04-03 03:20:30.177000 UTC,,91,13,0,27,,,,,,['amazon-sagemaker']
Sagemaker: MemoryError: Unable to allocate ___for an array with shape ___ and data type float64,"<p>I am running a notebook in sagemaker and it seems like one of the arrays produced after vectorizing text is causing issues.</p>

<p>Reading other answers it seems like it is an issue with <a href=""https://www.kernel.org/doc/Documentation/vm/overcommit-accounting"" rel=""noreferrer"">overcommit</a>. And one of the solutions proposed is to set it to always overcommit with this:</p>

<pre><code>$ echo 1 &gt; /proc/sys/vm/overcommit_memory
</code></pre>

<p>Is there any documentation or do you have any suggestion on how to do the same thing in sagemaker?</p>

<p>Thank you very much.</p>",2,0,2020-05-14 07:23:24.223000 UTC,1.0,,8,arrays|python-3.x|pandas|memory|amazon-sagemaker,2821,2018-02-06 15:55:07.093000 UTC,2022-03-31 15:50:16.033000 UTC,,331,14,0,73,,,,,,['amazon-sagemaker']
"Surprising results of Ms Azure vs Google Colab BERT training performance, not sure how to explain","<p>I'm not sure if it's BERT related or not, had no chance to test other models, but did it for BERT.</p>
<p>So what I noticed recently that training algorithms and data that I used to work with in google colab for free, are seemed to work significantly slower in Azure ML workspace which we pay for.</p>
<p>I made the comparison - same data file (classification problem, sentiment analysis of 10K reviews), totally same notebook code (copy+paste), same latest ver of ktrain lib installed on both, both must be on Python 3.8, but GPU is a bit more performant on a colab side.</p>
<p>Results surprised me to say the least: google lab made its job <strong>10 times faster: 17 min vs 170 min</strong>, and it's reproducible. Tesla T4 (colab) is faster than K80 (azure) indeed, but not that much as per known benchmarks. So I wonder what else could matter. Is it virt. environment created in Azure ML performing so slow? If you have any idea what it could be, or what else I can check on both sides to reveal it, please share</p>
<p>BTW google gives you T4 in colab for your experimentations for free, while you have to pay for slower K80 at Azure.</p>
<p><strong>Google colab</strong>
execution time = 17 min
<a href=""https://i.stack.imgur.com/Y6iVN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Y6iVN.png"" alt=""enter image description here"" /></a>
<strong>Google colab hardware</strong>: cpu model: Intel(R) Xeon(R) CPU @ 2.20GHz, memory 13Gb, GPU:<br />
<a href=""https://i.stack.imgur.com/3YrWq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3YrWq.png"" alt=""enter image description here"" /></a></p>
<p><strong>Azure</strong>
execution time = 2h50m = 170min (10x of colab)
<a href=""https://i.stack.imgur.com/9SPlB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9SPlB.png"" alt=""enter image description here"" /></a>
<strong>Azure hardware information</strong>
<a href=""https://i.stack.imgur.com/wA0Se.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wA0Se.png"" alt=""enter image description here"" /></a></p>
<p>K80 and T4 comparison: <a href=""https://technical.city/en/video/Tesla-K80-vs-Tesla-T4"" rel=""nofollow noreferrer"">https://technical.city/en/video/Tesla-K80-vs-Tesla-T4</a></p>",1,0,2022-08-08 20:12:16.940000 UTC,,2022-08-08 20:17:37.227000 UTC,2,azure|google-colaboratory|azure-machine-learning-service,69,2011-05-05 00:52:42.350000 UTC,2022-09-25 04:46:30.543000 UTC,,4550,321,19,287,,,,,,['azure-machine-learning-service']
Problem connecting Livy to EMR via sagemaker,"<p>I have followed this tutorial: <a href=""https://aws.amazon.com/fr/blogs/machine-learning/build-amazon-sagemaker-notebooks-backed-by-spark-in-amazon-emr/"" rel=""nofollow noreferrer"">https://aws.amazon.com/fr/blogs/machine-learning/build-amazon-sagemaker-notebooks-backed-by-spark-in-amazon-emr/</a> in order to be able to run pyspark code on EMR via apache-livy. I have only made some little change so that EMR configuration script run as a sagemaker lifecycle configuration script.</p>

<p>When testing the connection with <code>curl &lt;EMR Master Private IP&gt;:8998/sessions</code> the result seems completely fine: <code>{""from"":0,""total"":0,""sessions"":[]}</code>. But, when I try to run an application the state go from <strong>starting</strong> directly to <strong>dead</strong> with the following message:</p>

<p><code>{'id': 0, 'appId': None, 'owner': None, 'proxyUser': None, 'state': 'dead', 'kind': 'spark', 'appInfo': {'driverLogUrl': None, 'sparkUiUrl': None}, 'log': ['19/02/27 09:23:24 INFO Client: Requesting a new application from cluster with 2 NodeManagers', '19/02/27 09:23:25 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (1024 MB per container)', '19/02/27 09:23:25 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead', '19/02/27 09:23:25 INFO Client: Setting up container launch context for our AM', '19/02/27 09:23:25INFO Client: Setting up the launch environment for our AM container', '19/02/27 09:23:25 INFO Client: Preparing resources for our AM container', '19/02/27 09:23:26 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.', '\nYARN Diagnostics: ', 'java.lang.Exception: No YARN application is found with tag livy-session-0-v9wkutit in 120 seconds. Please check your cluster status, it is may be very busy.', 'org.apache.livy.utils.SparkYarnApp.org$apache$livy$utils$SparkYarnApp$$getAppIdFromTag(SparkYarnApp.scala:182) org.apache.livy.utils.SparkYarnApp$$anonfun$1$$anonfun$4.apply(SparkYarnApp.scala:239) org.apache.livy.utils.SparkYarnApp$$anonfun$1$$anonfun$4.apply(SparkYarnApp.scala:236) scala.Option.getOrElse(Option.scala:121) org.apache.livy.utils.SparkYarnApp$$anonfun$1.apply$mcV$sp(SparkYarnApp.scala:236) org.apache.livy.Utils$$anon$1.run(Utils.scala:94)']}</code></p>

<p>I have tried to investigate, but really have no clue of what's going on here, is there by any chance someone here which have an idea on how to debug that.</p>",0,0,2019-02-27 10:56:32.440000 UTC,1.0,,3,amazon-emr|livy|amazon-sagemaker,904,2014-08-21 08:30:20.407000 UTC,2022-08-02 08:31:21.447000 UTC,,111,36,0,29,,,,,,['amazon-sagemaker']
Jupyter notebooks HR tag not rendering well,"<p>I use Brave browser for my jupyter notebook (not jupyterlab) works under Amazon Sagemaker, but I also use Chrome for my Internet activities, so last week the update symbol appeared on Chrome, so I updated it, and immediately my notebooks on Brave stopped rendering well the following tag <code>&lt;hr style=&quot;border:3px solid black&quot;&gt; &lt;/hr&gt;</code>, which shows a thick straight line that I use to separate blocks of cells in a markdown cell, is now rendered as a thin gray line that is also generated by <code>---</code>.</p>
<p>A couple of days after that Chrome update, the update symbol showed on Brave too, so I updated it and nothing happened, the line was still rendered as if it were a <code>---</code>.</p>
<p>Has anyone noticed this behaviour and found a solution? Maybe it's because of some change in how the browser renders tags or something?</p>
<p>Note: I opened the same notebooks on JupyterLab (Sagemaker), they are rendered fine there.</p>
<p>Thank you in advance!!</p>
<p>EDIT: I launched a Jupyter Notebook from my computer and it always opens in Chrome, and the <code>&lt;hr style=&quot;border:3px solid black&quot;&gt; &lt;/hr&gt;</code> tag was rendered fine, it seems the problem is with the JNs from Sagemaker.</p>",0,2,2021-08-23 23:55:09.393000 UTC,,2021-08-24 16:31:09.490000 UTC,0,python-3.x|amazon-web-services|jupyter-notebook|jupyter-lab|amazon-sagemaker,79,2019-04-01 21:33:38.117000 UTC,2022-09-09 18:56:35.410000 UTC,,89,3,0,13,,,,,,['amazon-sagemaker']
"How to export Scikit learn models for hosting in AWS Sagemaker (sklearn model artifacts, .pkl file or .tar.gz)","<p>I am looking for a solution that allows me to host my trained Sklearn model (that I am satisfied with) on SageMaker without having to retrain it before deploying to an endpoint.</p>
<p>On the one hand I have seen specific examples for bring-your-own scikit model that involve containerizing the trained model but - these guides go through the training step and dont specifically show how you can alternatively avoid retraining the model and just deploy. (<a href=""https://github.com/awslabs/amazon-sagemaker-examples/blob/27d3aeb9166a4d4dbbb0721d381329e41d431078/advanced_functionality/scikit_bring_your_own/scikit_bring_your_own.ipynb"" rel=""nofollow noreferrer"">https://github.com/awslabs/amazon-sagemaker-examples/blob/27d3aeb9166a4d4dbbb0721d381329e41d431078/advanced_functionality/scikit_bring_your_own/scikit_bring_your_own.ipynb</a>)</p>
<p>On the other hand, there are guides that show you how to BYOM only for deploying - but these are specific to MXNet and TensorFlow frameworks. I noticed that the way you export your model artifacts among frameworks differs. I need something specific to Sklearn and how to get to a good point where I have model artifacts in the correct format Sagemaker expects(<a href=""https://github.com/awslabs/amazon-sagemaker-examples/tree/27d3aeb9166a4d4dbbb0721d381329e41d431078/advanced_functionality/mxnet_mnist_byom"" rel=""nofollow noreferrer"">https://github.com/awslabs/amazon-sagemaker-examples/tree/27d3aeb9166a4d4dbbb0721d381329e41d431078/advanced_functionality/mxnet_mnist_byom</a>)</p>
<p>The closest guide I have seen that might work is this one: <a href=""https://aws.amazon.com/blogs/machine-learning/bring-your-own-pre-trained-mxnet-or-tensorflow-models-into-amazon-sagemaker/"" rel=""nofollow noreferrer"">https://aws.amazon.com/blogs/machine-learning/bring-your-own-pre-trained-mxnet-or-tensorflow-models-into-amazon-sagemaker/</a></p>
<p>However, I dont know what my sklearn &quot;model artifacts&quot; includes. I think I need a clear understanding of what sklearn model artifacts looks like and what it includes.</p>
<p>Any help is appreciated. The goal is to avoid training in Sagemaker and only deploy my already trained scikit model to an endpoint.</p>",0,0,2020-07-03 22:34:50.967000 UTC,2.0,,3,amazon-web-services|scikit-learn|amazon-sagemaker,418,2020-01-05 10:48:20.170000 UTC,2022-09-19 02:56:37.510000 UTC,"Toronto, ON, Canada",117,21,0,15,,,,,,['amazon-sagemaker']
How to ensure libraries installed using sagemaker?,"<p>I have a jupyter notebook with a standard template code like so</p>

<p>from sagemaker.tensorflow import TensorFlow</p>

<pre><code>import sagemaker
from sagemaker import get_execution_role
sagemaker_session = sagemaker.Session()
role = get_execution_role()

tf_estimator = TensorFlow(entry_point='sagemaker_predict_2.py', role=role,
                          training_steps=10000, evaluation_steps=100,
                          train_instance_count=1, train_instance_type='ml.p2.xlarge',
                          framework_version='1.10.0')
tf_estimator.fit('s3://XXX-sagemaker/XXX')
</code></pre>

<p>This kicks off fine but eventually throws an error</p>

<pre><code>2018-11-27 06:21:12 Starting - Starting the training job...
2018-11-27 06:21:15 Starting - Launching requested ML instances.........
2018-11-27 06:22:44 Starting - Preparing the instances for training...
2018-11-27 06:23:35 Downloading - Downloading input data...
2018-11-27 06:24:03 Training - Downloading the training image......
2018-11-27 06:25:12 Training - Training image download completed. Training in progress..
2018-11-27 06:25:11,813 INFO - root - running container entrypoint
2018-11-27 06:25:11,813 INFO - root - starting train task
2018-11-27 06:25:11,833 INFO - container_support.training - Training starting
2018-11-27 06:25:15,306 ERROR - container_support.training - uncaught exception during training: No module named keras
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/container_support/training.py"", line 36, in start
    fw.train()
  File ""/usr/local/lib/python2.7/dist-packages/tf_container/train_entry_point.py"", line 143, in train
    customer_script = env.import_user_module()
  File ""/usr/local/lib/python2.7/dist-packages/container_support/environment.py"", line 101, in import_user_module
    user_module = importlib.import_module(script)
  File ""/usr/lib/python2.7/importlib/__init__.py"", line 37, in import_module
    __import__(name)
  File ""/opt/ml/code/sagemaker_predict_2.py"", line 7, in &lt;module&gt;
    import keras
ImportError: No module named keras  
</code></pre>

<p>My <code>sagemaker_predict_2.py</code> needs some of these libraries:</p>

<pre><code>import pandas as pd
import numpy as np
import sys
import keras
from keras.models import Model, Input
from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional
from keras.wrappers.scikit_learn import KerasClassifier
from keras_contrib.layers import CRF
</code></pre>

<p>I suppose it has no problem importing <code>pandas</code> and <code>numpy</code>, but dies when importing <code>keras</code>. I thought <code>keras</code> was standard in the notebook. When I kick this script off, does it have some other uninitialized environment?</p>

<p>Also, I believe <code>keras_contrib</code> is not standard, so I will need a way to install that. How do I do that?</p>

<p>I tried <code>!pip install keras</code> in the cell above but it reported that <code>Requirement already satisfied</code>, so it seems my jupyter environment has the library. But kicking off the <code>sagemaker_predict_2.py</code> must be in a different environment?</p>",1,0,2018-11-27 06:30:19.930000 UTC,,,3,tensorflow|keras|amazon-sagemaker,3809,2011-10-21 21:58:08.810000 UTC,2022-09-17 00:51:12.053000 UTC,,4966,744,11,304,,,,,,['amazon-sagemaker']
Save Multiple Dataframes on excel workbook then Upload to AWS S3 Bucket,"<p>Good afternoon everyone, </p>

<p>I am trying to save multiple dataframes to an excel workbook on different sheets. Then upload that workbook to an Amazon S3 bucket. the code below works 99% of the way but the writer.save() cannot find my excel file on my S3 Bucket. Please assist if you know a way around this. thanks. </p>

<pre><code>#Exports the data back to Excel - PLEASE READ LINE BELOW THIS CODE
bucket='sagemaker-bucket-xxxx/xxxx/xxxxx'
data_key = 'Provider Data.xlsx'
data_location = 's3://{}/{}'.format(bucket, data_key)
writer = pd.ExcelWriter(data_location) #Targets the file where data is to be sent to
Comparison.to_excel(writer,'DATA') #Targets the worksheet data is to be sent too
df_current.to_excel(writer,'New Records') #Targets the worksheet data is to be sent too
df_prev.to_excel(writer,'Old Records') #Targets the worksheet data is to be sent too
df_same.to_excel(writer,'Same Records') #Targets the worksheet data is to be sent too
ALLCOUNT.to_excel(writer,'RPN Roll Up Count') #Targets the worksheet data is to be sent too
writer.save() #Saves files
</code></pre>

<p>error message is listed below. </p>

<p>FileNotFoundError: [Errno 2] No such file or directory: 's3://sagemaker-bucket-xxxx/xxxx/xxxx/Provider Data.xlsx'</p>",1,2,2019-05-08 00:09:53.143000 UTC,1.0,,1,python|excel|pandas|amazon-s3|amazon-sagemaker,1049,2018-09-20 14:35:40.907000 UTC,2022-09-05 21:31:03.393000 UTC,,181,12,0,63,,,,,,['amazon-sagemaker']
"How to solve ""Encountered unresolved custom op: edgetpu-custom-op"" exception","<p>Im trying to implement my custom ML model in a kotlin app.</p>
<p>I first make and train my model in GCP Vertex AI.
After my model was ready i've export it as tensor flow model to the edge and then upload to my Firebase Machine Learning proyect.</p>
<p>After that i follow the <a href=""https://firebase.google.com/docs/ml/android/use-custom-models?hl=es&amp;authuser=0"" rel=""nofollow noreferrer"">guide to implement a custom model of tensor flow lite on android.</a></p>
<p>Then when i execute my app it crash on this part of the code:</p>
<pre><code>val conditions = CustomModelDownloadConditions.Builder()
        .requireWifi()  // Also possible: .requireCharging() and .requireDeviceIdle()
        .build()
FirebaseModelDownloader.getInstance()
        .getModel(&quot;your_model&quot;, DownloadType.LOCAL_MODEL_UPDATE_IN_BACKGROUND,
            conditions)
        .addOnSuccessListener { model: CustomModel? -&gt;
            // Download complete. Depending on your app, you could enable the ML
            // feature, or switch from the local model to the remote model, etc.

            // The CustomModel object contains the local path of the model file,
            // which you can use to instantiate a TensorFlow Lite interpreter.
            val modelFile = model?.file
            if (modelFile != null) {
                interpreter = Interpreter(modelFile) // this line crash
            }
        }
</code></pre>
<p>More specific at the line &quot;interpreter = Interpreter(modelFile)&quot;.
I get the following exception:</p>
<blockquote>
<p>java.lang.IllegalStateException: Internal error: Unexpected failure
when preparing tensor allocations: Encountered unresolved custom op:
edgetpu-custom-op. See instructions:
<a href=""https://www.tensorflow.org/lite/guide/ops_custom"" rel=""nofollow noreferrer"">https://www.tensorflow.org/lite/guide/ops_custom</a>  Node number 0
(edgetpu-custom-op) failed to prepare.</p>
</blockquote>
<p>What is the meaning of this error?  How can i solve it?</p>",0,0,2022-09-23 18:55:49.290000 UTC,,,0,kotlin|machine-learning|google-cloud-vertex-ai|firebase-machine-learning,21,2012-08-22 16:10:22.773000 UTC,2022-09-25 02:05:58.237000 UTC,,109,13,0,21,,,,,,['google-cloud-vertex-ai']
Testing the sagemaker endpoint deployment locally,"<p>How to test the endpoint deployment in sagemaker locally using the sagemaker notebook instance?</p>
<p>The issue is that if we want to test the endpoint using the sagemaker studio notebook then it will take some time before it spins up the docker inference container depending on the instance. This can certainly hamper the development and process cycle!</p>",1,0,2021-11-29 20:20:22.293000 UTC,,,1,python-3.x|amazon-web-services|amazon-sagemaker,550,2020-10-31 12:12:09.127000 UTC,2022-09-02 15:37:19.343000 UTC,,95,57,0,7,,,,,,['amazon-sagemaker']
Set up health check for Sagemaker endpoint in Postman,"<p>I am trying to set up the health check for a model deployed in Sagemaker.</p>
<p>I am using <code>/ping</code> to set up the health check endpoint using this snippet of code:</p>
<pre><code>app = flask.Flask(__name__)

@app.route('/ping', methods=['GET'])
def ping():
    &quot;&quot;&quot;Determine if the container is working and healthy. We declare
    it healthy if we can load the model successfully.&quot;&quot;&quot;
    health = ScoringService.get_model() is not None

    status = 200 if health else 404
    return flask.Response(response='\n', status=status, mimetype='application/json')
</code></pre>
<p>I already have assumed a AWS Role and am getting predictions using <code>/invocations</code> but when I try calling the <code>GET https://runtime.sagemaker.eu-west-1.amazonaws.com/endpoints/{{endpoint}}/ping</code>, I am receiving this error:</p>
<pre><code>&lt;AccessDeniedException&gt;
  &lt;Message&gt;Unable to determine service/operation name to be authorized&lt;/Message&gt;
&lt;/AccessDeniedException&gt;
</code></pre>
<p>For Authorization, I am using Type AWS Signature, and header is Content-Type: application/json. I think I am missing something in the settings.</p>",1,0,2021-03-15 14:49:56.803000 UTC,,,0,amazon-web-services|postman|ping|amazon-sagemaker,423,2018-05-14 09:05:19.837000 UTC,2022-07-01 15:18:50.603000 UTC,"Madrid, Spain",33,1,0,11,,,,,,['amazon-sagemaker']
How to read a file during training in AWS SageMaker?,"<p>I'm trying to train a custom tensorflow model, using AWS SageMaker. Thus, in the <code>model_fn</code> method, that I should provide, I want to be able to read an external file. I've uploaded the file to S3 and try to read like below:</p>

<pre><code>BUCKET_PATH = 's3://&lt;bucket_name&gt;/data/&lt;prefix&gt;/'

def model_fn(features, labels, mode, params):
    # Load vocabulary
    vocab_path = os.path.join(BUCKET_PATH, 'vocab.pkl')
    with open(vocab_path, 'rb') as f:
        vocab = pickle.load(f)
    n_vocab = len(vocab)
    ...
</code></pre>

<p>I get an <code>IOError: [Errno 2] No such file or directory</code></p>

<p>How can I read this file during training?</p>",1,1,2018-04-11 10:12:55.947000 UTC,,,2,tensorflow|amazon-sagemaker,752,2015-02-06 14:11:04.697000 UTC,2019-04-18 16:45:40.390000 UTC,"Athens, Greece",1109,29,0,157,,,,,,['amazon-sagemaker']
%run works only once in Jupyter Notebook,"<p>I have two Jupyter notebooks open in the same folder: <code>functions.ipynb</code> and <code>preprocessing.ipynb</code>.<br />
In <code>functions.ipynb</code> I have defined a series of functions that I will call in the other one.</p>
<p>In the first cell of <code>preprocessing.ipynb</code> I execute</p>
<pre><code>%run 'functions.ipynb'
</code></pre>
<p>and it works just fine. But if I run once again the same cell I get this error:</p>
<pre><code>---------------------------------------------------------------------------
OSError                                   Traceback (most recent call last)
~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/IPython/core/magics/execution.py in run(self, parameter_s, runner, file_finder)
    696             fpath = arg_lst[0]
--&gt; 697             filename = file_finder(fpath)
    698         except IndexError:

~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/IPython/utils/path.py in get_py_filename(name, force_win32)
    108     else:
--&gt; 109         raise IOError('File `%r` not found.' % name)
    110 

OSError: File `'functions.ipynb.py'` not found.

During handling of the above exception, another exception occurred:

Exception                                 Traceback (most recent call last)
&lt;ipython-input-2-330907dc0fe2&gt; in &lt;module&gt;
----&gt; 1 get_ipython().run_line_magic('run', &quot;'functions.ipynb'&quot;)

~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py in run_line_magic(self, magic_name, line, _stack_depth)
   2324                 kwargs['local_ns'] = sys._getframe(stack_depth).f_locals
   2325             with self.builtin_trap:
-&gt; 2326                 result = fn(*args, **kwargs)
   2327             return result
   2328 

&lt;decorator-gen-59&gt; in run(self, parameter_s, runner, file_finder)

~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/IPython/core/magic.py in &lt;lambda&gt;(f, *a, **k)
    185     # but it's overkill for just that one bit of state.
    186     def magic_deco(arg):
--&gt; 187         call = lambda f, *a, **k: f(*a, **k)
    188 
    189         if callable(arg):

~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/IPython/core/magics/execution.py in run(self, parameter_s, runner, file_finder)
    706             if os.name == 'nt' and re.match(r&quot;^'.*'$&quot;,fpath):
    707                 warn('For Windows, use double quotes to wrap a filename: %run &quot;mypath\\myfile.py&quot;')
--&gt; 708             raise Exception(msg)
    709         except TypeError:
    710             if fpath in sys.meta_path:

Exception: File `'functions.ipynb.py'` not found.
</code></pre>
<p>I really can't understand why this code runs only once.<br />
I am using Python 3.6 in Amazon SageMaker</p>",1,0,2021-07-07 09:38:44.207000 UTC,,,1,python|python-3.x|jupyter-notebook|ipython|amazon-sagemaker,443,2021-07-07 09:25:25.160000 UTC,2022-09-23 13:44:59.867000 UTC,"Frankfurt, Germany",11,0,0,2,,,,,,['amazon-sagemaker']
How can I deploy a model that i trained on amazon sagemaker locally?,"<p>I trained a model using aws blazingtext algorithm on amazon sagemaker and I was able to deploy an endpoint using sagemaker. However, in my circumstance this is not cost-efficient and I would like to run it locally. I have found the documentation on this to be confusing.</p>

<p>What I have is the trained model saved as a ""model.tar.gz"" file that I have downloaded from my s3 bucket. I have read online that you can deploy models using tensorflow and docker images, but I simply want to deploy the model I have created using sagemaker using my local machine. Essentially what I want to do is :</p>

<pre class=""lang-py prettyprint-override""><code>predictor = sagemaker.deploy(initial_instance_count=1, instance_type='local_cpu')
</code></pre>

<p>I expect to be able to use the predict function to make inference calls and return responses with the prediction results. I am looking for which libraries to use, and the associated code to accomplish this task. Thank you.</p>",1,0,2019-07-01 20:29:26.707000 UTC,,,2,tensorflow|mxnet|amazon-sagemaker,688,2019-07-01 20:19:22.730000 UTC,2019-08-08 19:27:57.880000 UTC,,21,0,0,1,,,,,,['amazon-sagemaker']
How to modify default docker base image during deployment of Azure Kubernetes service,"<p>I have been using DEFAULT_GPU_IMAGE as my base image in Azure ML but now it started throwing the
<code>ImportError: libGL.so.1: cannot open shared object file: No such file or directory</code> error when importing opencv.
Some answers here on stackoverflow say i need to run apt-get update on the image. specifically:</p>
<pre><code>RUN apt-get update ##[edited] 
RUN apt-get install 'ffmpeg'\
'libsm6'\ 
'libxext6'  -y
</code></pre>
<p>Would you know where can i find the docker file to add the lines to  or is there a way to patch the image during the deployment of the AKS service? (same way as pip and conda packages are possible to be installed during the deployment)</p>",1,0,2020-10-02 06:51:07.670000 UTC,,,0,azure|docker|opencv|kubernetes|azure-machine-learning-service,148,2011-12-10 11:29:59.543000 UTC,2022-09-08 09:27:26.173000 UTC,Taipei,548,118,3,55,,,,,,['azure-machine-learning-service']
Get current workspace from inside a AzureML Pipeline step,"<p>I'm using the Python SDK.</p>
<p>I assume there is a way to get a handle to the workspace on which the <code>PythonScriptStep</code> is running from inside the <code>PythonScriptStep</code> but I can't find it.</p>
<p>Any idea how this can be achieved?</p>",1,0,2021-08-13 20:35:17.863000 UTC,,,1,azure-machine-learning-service,500,2012-01-26 14:27:40.553000 UTC,2022-09-24 16:26:41.580000 UTC,,802,288,0,91,,,,,,['azure-machine-learning-service']
"Cannot import librosa on SageMaker Jupyter notebook instance ""OSError: sndfile library not found""","<p>I am trying to import librosa on a SageMaker notebook instance but it's telling me that the sndfile library is not found. I have tried conda install -c conda-forge libsndfile but it is not working. I have been stuck on this for almost 3 hours now. Would appreciate some help. Thank you.</p>
<p>UPDATE (NOW WORKING):
This is what ended up working.. we had to compile the libsndfile from scrach using the following commands</p>
<pre><code>%%bash
wget 'https://github.com/libsndfile/libsndfile/releases/download/1.0.31/libsndfile-1.0.31.tar.bz2'
tar -xf libsndfile-1.0.31.tar.bz2
cd libsndfile-1.0.31/
./configure
make
sudo make install
</code></pre>",1,0,2022-06-20 12:37:51.537000 UTC,,2022-06-20 13:38:32.530000 UTC,0,python|python-3.x|amazon-web-services|amazon-sagemaker|librosa,180,2022-06-20 12:29:28.083000 UTC,2022-06-29 08:59:30.167000 UTC,,1,0,0,1,,,,,,['amazon-sagemaker']
How to deploy image classifier with resnet50 model on AWS endpoint to predict without worker dying?,"<p>Created a imageclassifier model built on renet50 to identify dog breeds. I created it in sagemaker studio. Tuning and training are done, I deployed it, but when I try to predict on it, it fails. I believe this is related to the pid of the worker because its first warning I see.
Getting following Cloudwatch log output says worker pid not available yet then soon after the worker dies.</p>
<pre class=""lang-sh prettyprint-override""><code>timestamp,message,logStreamName
1648240674535,&quot;2022-03-25 20:37:54,107 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...&quot;,AllTraffic/i-055c5d00e53e84b93
1648240674535,&quot;2022-03-25 20:37:54,188 [INFO ] main org.pytorch.serve.ModelServer - &quot;,AllTraffic/i-055c5d00e53e84b93
1648240674535,Torchserve version: 0.4.0,AllTraffic/i-055c5d00e53e84b93
1648240674535,TS Home: /opt/conda/lib/python3.6/site-packages,AllTraffic/i-055c5d00e53e84b93
1648240674535,Current directory: /,AllTraffic/i-055c5d00e53e84b93
1648240674535,Temp directory: /home/model-server/tmp,AllTraffic/i-055c5d00e53e84b93
1648240674535,Number of GPUs: 0,AllTraffic/i-055c5d00e53e84b93
1648240674535,Number of CPUs: 1,AllTraffic/i-055c5d00e53e84b93
1648240674535,Max heap size: 6838 M,AllTraffic/i-055c5d00e53e84b93
1648240674535,Python executable: /opt/conda/bin/python3.6,AllTraffic/i-055c5d00e53e84b93
1648240674535,Config file: /etc/sagemaker-ts.properties,AllTraffic/i-055c5d00e53e84b93
1648240674535,Inference address: http://0.0.0.0:8080,AllTraffic/i-055c5d00e53e84b93
1648240674535,Management address: http://0.0.0.0:8080,AllTraffic/i-055c5d00e53e84b93
1648240674535,Metrics address: http://127.0.0.1:8082,AllTraffic/i-055c5d00e53e84b93
1648240674535,Model Store: /.sagemaker/ts/models,AllTraffic/i-055c5d00e53e84b93
1648240674535,Initial Models: model.mar,AllTraffic/i-055c5d00e53e84b93
1648240674535,Log dir: /logs,AllTraffic/i-055c5d00e53e84b93
1648240674535,Metrics dir: /logs,AllTraffic/i-055c5d00e53e84b93
1648240674535,Netty threads: 0,AllTraffic/i-055c5d00e53e84b93
1648240674535,Netty client threads: 0,AllTraffic/i-055c5d00e53e84b93
1648240674535,Default workers per model: 1,AllTraffic/i-055c5d00e53e84b93
1648240674535,Blacklist Regex: N/A,AllTraffic/i-055c5d00e53e84b93
1648240674535,Maximum Response Size: 6553500,AllTraffic/i-055c5d00e53e84b93
1648240674536,Maximum Request Size: 6553500,AllTraffic/i-055c5d00e53e84b93
1648240674536,Prefer direct buffer: false,AllTraffic/i-055c5d00e53e84b93
1648240674536,Allowed Urls: [file://.*|http(s)?://.*],AllTraffic/i-055c5d00e53e84b93
1648240674536,Custom python dependency for model allowed: false,AllTraffic/i-055c5d00e53e84b93
1648240674536,Metrics report format: prometheus,AllTraffic/i-055c5d00e53e84b93
1648240674536,Enable metrics API: true,AllTraffic/i-055c5d00e53e84b93
1648240674536,Workflow Store: /.sagemaker/ts/models,AllTraffic/i-055c5d00e53e84b93
1648240674536,&quot;2022-03-25 20:37:54,195 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...&quot;,AllTraffic/i-055c5d00e53e84b93
1648240675536,&quot;2022-03-25 20:37:54,217 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: model.mar&quot;,AllTraffic/i-055c5d00e53e84b93
1648240675536,&quot;2022-03-25 20:37:55,505 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model model loaded.&quot;,AllTraffic/i-055c5d00e53e84b93
1648240675786,&quot;2022-03-25 20:37:55,515 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.&quot;,AllTraffic/i-055c5d00e53e84b93
1648240675786,&quot;2022-03-25 20:37:55,569 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080&quot;,AllTraffic/i-055c5d00e53e84b93
1648240675786,&quot;2022-03-25 20:37:55,569 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.&quot;,AllTraffic/i-055c5d00e53e84b93
1648240675786,&quot;2022-03-25 20:37:55,569 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082&quot;,AllTraffic/i-055c5d00e53e84b93
1648240675786,Model server started.,AllTraffic/i-055c5d00e53e84b93
1648240676036,&quot;2022-03-25 20:37:55,727 [WARN ] pool-2-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.&quot;,AllTraffic/i-055c5d00e53e84b93
1648240676036,&quot;2022-03-25 20:37:55,812 [INFO ] pool-2-thread-1 TS_METRICS - CPUUtilization.Percent:100.0|#Level:Host|#hostname:container-0.local,timestamp:1648240675&quot;,AllTraffic/i-055c5d00e53e84b93
1648240676036,&quot;2022-03-25 20:37:55,813 [INFO ] pool-2-thread-1 TS_METRICS - DiskAvailable.Gigabytes:38.02598190307617|#Level:Host|#hostname:container-0.local,timestamp:1648240675&quot;,AllTraffic/i-055c5d00e53e84b93
1648240676036,&quot;2022-03-25 20:37:55,813 [INFO ] pool-2-thread-1 TS_METRICS - DiskUsage.Gigabytes:12.715518951416016|#Level:Host|#hostname:container-0.local,timestamp:1648240675&quot;,AllTraffic/i-055c5d00e53e84b93
1648240676036,&quot;2022-03-25 20:37:55,814 [INFO ] pool-2-thread-1 TS_METRICS - DiskUtilization.Percent:25.1|#Level:Host|#hostname:container-0.local,timestamp:1648240675&quot;,AllTraffic/i-055c5d00e53e84b93
1648240676036,&quot;2022-03-25 20:37:55,815 [INFO ] pool-2-thread-1 TS_METRICS - MemoryAvailable.Megabytes:29583.98046875|#Level:Host|#hostname:container-0.local,timestamp:1648240675&quot;,AllTraffic/i-055c5d00e53e84b93
1648240676036,&quot;2022-03-25 20:37:55,815 [INFO ] pool-2-thread-1 TS_METRICS - MemoryUsed.Megabytes:1355.765625|#Level:Host|#hostname:container-0.local,timestamp:1648240675&quot;,AllTraffic/i-055c5d00e53e84b93
1648240676036,&quot;2022-03-25 20:37:55,816 [INFO ] pool-2-thread-1 TS_METRICS - MemoryUtilization.Percent:5.7|#Level:Host|#hostname:container-0.local,timestamp:1648240675&quot;,AllTraffic/i-055c5d00e53e84b93
1648240676036,&quot;2022-03-25 20:37:55,994 [INFO ] W-9000-model_1-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000&quot;,AllTraffic/i-055c5d00e53e84b93
1648240676036,&quot;2022-03-25 20:37:55,994 [INFO ] W-9000-model_1-stdout MODEL_LOG - [PID]48&quot;,AllTraffic/i-055c5d00e53e84b93
1648240676036,&quot;2022-03-25 20:37:55,994 [INFO ] W-9000-model_1-stdout MODEL_LOG - Torch worker started.&quot;,AllTraffic/i-055c5d00e53e84b93
1648240676036,&quot;2022-03-25 20:37:55,994 [INFO ] W-9000-model_1-stdout MODEL_LOG - Python runtime: 3.6.13&quot;,AllTraffic/i-055c5d00e53e84b93
1648240676036,&quot;2022-03-25 20:37:55,999 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000&quot;,AllTraffic/i-055c5d00e53e84b93
1648240676286,&quot;2022-03-25 20:37:56,006 [INFO ] W-9000-model_1-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.&quot;,AllTraffic/i-055c5d00e53e84b93
1648240676286,&quot;2022-03-25 20:37:56,111 [INFO ] W-9000-model_1-stdout MODEL_LOG - Backend worker process died.&quot;,AllTraffic/i-055c5d00e53e84b93
1648240676286,&quot;2022-03-25 20:37:56,111 [INFO ] W-9000-model_1-stdout MODEL_LOG - Traceback (most recent call last):&quot;,AllTraffic/i-055c5d00e53e84b93
1648240676286,&quot;2022-03-25 20:37:56,111 [INFO ] W-9000-model_1-stdout MODEL_LOG -   File &quot;&quot;/opt/conda/lib/python3.6/site-packages/ts/model_service_worker.py&quot;&quot;, line 182, in &lt;module&gt;&quot;,AllTraffic/i-055c5d00e53e84b93
1648240676286,&quot;2022-03-25 20:37:56,111 [INFO ] W-9000-model_1-stdout MODEL_LOG -     worker.run_server()&quot;,AllTraffic/i-055c5d00e53e84b93
1648240676286,&quot;2022-03-25 20:37:56,111 [INFO ] W-9000-model_1-stdout MODEL_LOG -   File &quot;&quot;/opt/conda/lib/python3.6/site-packages/ts/model_service_worker.py&quot;&quot;, line 154, in run_server&quot;,AllTraffic/i-055c5d00e53e84b93
1648240676286,&quot;2022-03-25 20:37:56,111 [INFO ] W-9000-model_1-stdout MODEL_LOG -     self.handle_connection(cl_socket)&quot;,AllTraffic/i-055c5d00e53e84b93
1648240676286,&quot;2022-03-25 20:37:56,112 [INFO ] W-9000-model_1-stdout MODEL_LOG -   File &quot;&quot;/opt/conda/lib/python3.6/site-packages/ts/model_service_worker.py&quot;&quot;, line 116, in handle_connection&quot;,AllTraffic/i-055c5d00e53e84b93
1648240676286,&quot;2022-03-25 20:37:56,112 [INFO ] W-9000-model_1-stdout MODEL_LOG -     service, result, code = self.load_model(msg)&quot;,AllTraffic/i-055c5d00e53e84b93
1648240676286,&quot;2022-03-25 20:37:56,112 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED&quot;,AllTraffic/i-055c5d00e53e84b93
1648240676286,&quot;2022-03-25 20:37:56,112 [INFO ] W-9000-model_1-stdout MODEL_LOG -   File &quot;&quot;/opt/conda/lib/python3.6/site-packages/ts/model_service_worker.py&quot;&quot;, line 89, in load_model&quot;,AllTraffic/i-055c5d00e53e84b93
1648240676286,&quot;2022-03-25 20:37:56,112 [INFO ] W-9000-model_1-stdout MODEL_LOG -     service = model_loader.load(model_name, model_dir, handler, gpu, batch_size, envelope)&quot;,AllTraffic/i-055c5d00e53e84b93
1648240676286,&quot;2022-03-25 20:37:56,112 [INFO ] W-9000-model_1-stdout MODEL_LOG -   File &quot;&quot;/opt/conda/lib/python3.6/site-packages/ts/model_loader.py&quot;&quot;, line 110, in load&quot;,AllTraffic/i-055c5d00e53e84b93
1648240676286,&quot;2022-03-25 20:37:56,112 [INFO ] W-9000-model_1-stdout MODEL_LOG -     initialize_fn(service.context)&quot;,AllTraffic/i-055c5d00e53e84b93
1648240676286,&quot;2022-03-25 20:37:56,113 [WARN ] W-9000-model_1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.&quot;,AllTraffic/i-055c5d00e53e84b93
1648240676286,&quot;2022-03-25 20:37:56,113 [INFO ] W-9000-model_1-stdout MODEL_LOG -   File &quot;&quot;/home/model-server/tmp/models/23b30361031647d08792d32672910688/handler_service.py&quot;&quot;, line 51, in initialize&quot;,AllTraffic/i-055c5d00e53e84b93
1648240676286,&quot;2022-03-25 20:37:56,113 [INFO ] W-9000-model_1-stdout MODEL_LOG -     super().initialize(context)&quot;,AllTraffic/i-055c5d00e53e84b93
1648240676286,&quot;2022-03-25 20:37:56,113 [WARN ] W-9000-model_1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1-stderr&quot;,AllTraffic/i-055c5d00e53e84b93
1648240676286,&quot;2022-03-25 20:37:56,113 [WARN ] W-9000-model_1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1-stdout&quot;,AllTraffic/i-055c5d00e53e84b93
1648240676286,&quot;2022-03-25 20:37:56,113 [INFO ] W-9000-model_1-stdout MODEL_LOG -   File &quot;&quot;/opt/conda/lib/python3.6/site-packages/sagemaker_inference/default_handler_service.py&quot;&quot;, line 66, in initialize&quot;,AllTraffic/i-055c5d00e53e84b93
1648240676286,&quot;2022-03-25 20:37:56,113 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1-stdout&quot;,AllTraffic/i-055c5d00e53e84b93
1648240676536,&quot;2022-03-25 20:37:56,114 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.&quot;,AllTraffic/i-055c5d00e53e84b93
1648240676536,&quot;2022-03-25 20:37:56,416 [INFO ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1-stderr&quot;,AllTraffic/i-055c5d00e53e84b93
1648240676536,&quot;2022-03-25 20:37:56,461 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.178.2:39848 &quot;&quot;GET /ping HTTP/1.1&quot;&quot; 200 9&quot;,AllTraffic/i-055c5d00e53e84b93
1648240677787,&quot;2022-03-25 20:37:56,461 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:container-0.local,timestamp:null&quot;,AllTraffic/i-055c5d00e53e84b93
1648240677787,&quot;2022-03-25 20:37:57,567 [INFO ] W-9000-model_1-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000&quot;,AllTraffic/i-055c5d00e53e84b93
1648240677787,&quot;2022-03-25 20:37:57,568 [INFO ] W-9000-model_1-stdout MODEL_LOG - [PID]86&quot;,AllTraffic/i-055c5d00e53e84b93
1648240677787,&quot;2022-03-25 20:37:57,568 [INFO ] W-9000-model_1-stdout MODEL_LOG - Torch worker started.&quot;,AllTraffic/i-055c5d00e53e84b93
1648240677787,&quot;2022-03-25 20:37:57,568 [INFO ] W-9000-model_1-stdout MODEL_LOG - Python runtime: 3.6.13&quot;,AllTraffic/i-055c5d00e53e84b93
1648240677787,&quot;2022-03-25 20:37:57,568 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000&quot;,AllTraffic/i-055c5d00e53e84b93
1648240677787,&quot;2022-03-25 20:37:57,569 [INFO ] W-9000-model_1-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.&quot;,AllTraffic/i-055c5d00e53e84b93
1648240677787,&quot;2022-03-25 20:37:57,642 [INFO ] W-9000-model_1-stdout MODEL_LOG - Backend worker process died.&quot;,AllTraffic/i-055c5d00e53e84b93
1648240677787,&quot;2022-03-25 20:37:57,642 [INFO ] W-9000-model_1-stdout MODEL_LOG - Traceback (most recent call last):&quot;,AllTraffic/i-055c5d00e53e84b93
1648240677787,&quot;2022-03-25 20:37:57,642 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED&quot;,AllTraffic/i-055c5d00e53e84b93
1648240677787,&quot;2022-03-25 20:37:57,642 [INFO ] W-9000-model_1-stdout MODEL_LOG -   File &quot;&quot;/opt/conda/lib/python3.6/site-packages/ts/model_service_worker.py&quot;&quot;, line 182, in &lt;module&gt;&quot;,AllTraffic/i-055c5d00e53e84b93
1648240677787,&quot;2022-03-25 20:37:57,643 [INFO ] W-9000-model_1-stdout MODEL_LOG -     worker.run_server()&quot;,AllTraffic/i-055c5d00e53e84b93
1648240677787,&quot;2022-03-25 20:37:57,643 [INFO ] W-9000-model_1-stdout MODEL_LOG -   File &quot;&quot;/opt/conda/lib/python3.6/site-packages/ts/model_service_worker.py&quot;&quot;, line 154, in run_server&quot;,AllTraffic/i-055c5d00e53e84b93
1648240677787,&quot;2022-03-25 20:37:57,643 [WARN ] W-9000-model_1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.&quot;,AllTraffic/i-055c5d00e53e84b93
1648240677787,&quot;2022-03-25 20:37:57,643 [INFO ] W-9000-model_1-stdout MODEL_LOG -     self.handle_connection(cl_socket)&quot;,AllTraffic/i-055c5d00e53e84b93
1648240677787,&quot;2022-03-25 20:37:57,643 [INFO ] W-9000-model_1-stdout MODEL_LOG -   File &quot;&quot;/opt/conda/lib/python3.6/site-packages/ts/model_service_worker.py&quot;&quot;, line 116, in handle_connection&quot;,AllTraffic/i-055c5d00e53e84b93
1648240677787,&quot;2022-03-25 20:37:57,643 [WARN ] W-9000-model_1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1-stderr&quot;,AllTraffic/i-055c5d00e53e84b93
1648240677787,&quot;2022-03-25 20:37:57,643 [INFO ] W-9000-model_1-stdout MODEL_LOG -     service, result, code = self.load_model(msg)&quot;,AllTraffic/i-055c5d00e53e84b93
1648240677787,&quot;2022-03-25 20:37:57,643 [WARN ] W-9000-model_1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1-stdout&quot;,AllTraffic/i-055c5d00e53e84b93
1648240677787,&quot;2022-03-25 20:37:57,643 [INFO ] W-9000-model_1-stdout MODEL_LOG -   File &quot;&quot;/opt/conda/lib/python3.6/site-packages/ts/model_service_worker.py&quot;&quot;, line 89, in load_model&quot;,AllTraffic/i-055c5d00e53e84b93
1648240677787,&quot;2022-03-25 20:37:57,643 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1-stdout&quot;,AllTraffic/i-055c5d00e53e84b93
1648240678037,&quot;2022-03-25 20:37:57,643 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.&quot;,AllTraffic/i-055c5d00e53e84b93
1648240679288,&quot;2022-03-25 20:37:57,991 [INFO ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1-stderr&quot;,AllTraffic/i-055c5d00e53e84b93
1648240679288,&quot;2022-03-25 20:37:59,096 [INFO ] W-9000-model_1-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000&quot;,AllTraffic/i-055c5d00e53e84b93
1648240679288,&quot;2022-03-25 20:37:59,097 [INFO ] W-9000-model_1-stdout MODEL_LOG - [PID]114&quot;,AllTraffic/i-055c5d00e53e84b93
</code></pre>
<p>Model tuning and training came out alright so I'm not sure why it won't predict if that is fine. Someone mentioned to me that it might be due to entry point script, but I don't know what would cause it fail in predicting after deployed if it can predict fine during training.</p>
<p>Entry point script:</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.models as models
import torchvision.transforms as transforms
import json

import copy
import argparse
import os
import logging
import sys
from tqdm import tqdm
from PIL import ImageFile
import smdebug.pytorch as smd

ImageFile.LOAD_TRUNCATED_IMAGES = True

logger=logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)
logger.addHandler(logging.StreamHandler(sys.stdout))

def test(model, test_loader, criterion, hook):
    model.eval()
    running_loss=0
    running_corrects=0
    hook.set_mode(smd.modes.EVAL)
    
    
    for inputs, labels in test_loader:
        outputs=model(inputs)
        loss=criterion(outputs, labels)
        _, preds = torch.max(outputs, 1)
        running_loss += loss.item() * inputs.size(0)
        running_corrects += torch.sum(preds == labels.data)

    ##total_loss = running_loss // len(test_loader)
    ##total_acc = running_corrects.double() // len(test_loader)
    
    ##logger.info(f&quot;Testing Loss: {total_loss}&quot;)
    ##logger.info(f&quot;Testing Accuracy: {total_acc}&quot;)
    logger.info(&quot;New test acc&quot;)
    logger.info(f'Test set: Accuracy: {running_corrects}/{len(test_loader.dataset)} = {100*(running_corrects/len(test_loader.dataset))}%)')

def train(model, train_loader, validation_loader, criterion, optimizer, hook):
    epochs=50
    best_loss=1e6
    image_dataset={'train':train_loader, 'valid':validation_loader}
    loss_counter=0
    hook.set_mode(smd.modes.TRAIN)
    
    for epoch in range(epochs):
        logger.info(f&quot;Epoch: {epoch}&quot;)
        for phase in ['train', 'valid']:
            if phase=='train':
                model.train()
                logger.info(&quot;Model Trained&quot;)
            else:
                model.eval()
            running_loss = 0.0
            running_corrects = 0

            for inputs, labels in image_dataset[phase]:
                outputs = model(inputs)
                loss = criterion(outputs, labels)

                if phase=='train':
                    optimizer.zero_grad()
                    loss.backward()
                    optimizer.step()
                    logger.info(&quot;Model Optimized&quot;)

                _, preds = torch.max(outputs, 1)
                running_loss += loss.item() * inputs.size(0)
                running_corrects += torch.sum(preds == labels.data)

            epoch_loss = running_loss // len(image_dataset[phase])
            epoch_acc = running_corrects // len(image_dataset[phase])
            
            
            if phase=='valid':
                logger.info(&quot;Model Validating&quot;)
                if epoch_loss&lt;best_loss:
                    best_loss=epoch_loss
                else:
                    loss_counter+=1

            logger.info(loss_counter)
            '''logger.info('{} loss: {:.4f}, acc: {:.4f}, best loss: {:.4f}'.format(phase,
                                                                                 epoch_loss,
                                                                                 epoch_acc,
                                                                                 best_loss))'''
            
            if phase==&quot;train&quot;:
                logger.info(&quot;New epoch acc for Train:&quot;)
                logger.info(f&quot;Epoch {epoch}: Loss {loss_counter/len(train_loader.dataset)}, Accuracy {100*(running_corrects/len(train_loader.dataset))}%&quot;)
            if phase==&quot;valid&quot;:
                logger.info(&quot;New epoch acc for Valid:&quot;)
                logger.info(f&quot;Epoch {epoch}: Loss {loss_counter/len(train_loader.dataset)}, Accuracy {100*(running_corrects/len(train_loader.dataset))}%&quot;)
            
        ##if loss_counter==1:
        ##    break
        ##if epoch==0:
        ##    break
    return model
    
def net():
    model = models.resnet50(pretrained=True)

    for param in model.parameters():
        param.requires_grad = False   

    model.fc = nn.Sequential(
                   nn.Linear(2048, 128),
                   nn.ReLU(inplace=True),
                   nn.Linear(128, 133))
    return model

def create_data_loaders(data, batch_size):
    train_data_path = os.path.join(data, 'train')
    test_data_path = os.path.join(data, 'test')
    validation_data_path=os.path.join(data, 'valid')

    train_transform = transforms.Compose([
        transforms.RandomResizedCrop((224, 224)),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        ])

    test_transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        ])

    train_data = torchvision.datasets.ImageFolder(root=train_data_path, transform=train_transform)
    train_data_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)

    test_data = torchvision.datasets.ImageFolder(root=test_data_path, transform=test_transform)
    test_data_loader  = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)

    validation_data = torchvision.datasets.ImageFolder(root=validation_data_path, transform=test_transform)
    validation_data_loader  = torch.utils.data.DataLoader(validation_data, batch_size=batch_size, shuffle=True) 
    
    return train_data_loader, test_data_loader, validation_data_loader

def main(args):
    logger.info(f'Hyperparameters are LR: {args.lr}, Batch Size: {args.batch_size}')
    logger.info(f'Data Paths: {args.data}')

    
    train_loader, test_loader, validation_loader=create_data_loaders(args.data, args.batch_size)
    model=net()
    
    hook = smd.Hook.create_from_json_file()
    hook.register_hook(model)
    
    criterion = nn.CrossEntropyLoss(ignore_index=133)
    optimizer = optim.Adam(model.fc.parameters(), lr=args.lr)
    
    logger.info(&quot;Starting Model Training&quot;)
    model=train(model, train_loader, validation_loader, criterion, optimizer, hook)
    
    logger.info(&quot;Testing Model&quot;)
    test(model, test_loader, criterion, hook)
    
    logger.info(&quot;Saving Model&quot;)
    torch.save(model.cpu().state_dict(), os.path.join(args.model_dir, &quot;model.pth&quot;))
    
if __name__=='__main__':
    parser=argparse.ArgumentParser()
    '''
    TODO: Specify any training args that you might need
    '''
    parser.add_argument(
        &quot;--batch-size&quot;,
        type=int,
        default=64,
        metavar=&quot;N&quot;,
        help=&quot;input batch size for training (default: 64)&quot;,
    )
    parser.add_argument(
        &quot;--test-batch-size&quot;,
        type=int,
        default=1000,
        metavar=&quot;N&quot;,
        help=&quot;input batch size for testing (default: 1000)&quot;,
    )
    parser.add_argument(
        &quot;--epochs&quot;,
        type=int,
        default=5,
        metavar=&quot;N&quot;,
        help=&quot;number of epochs to train (default: 10)&quot;,
    )
    parser.add_argument(
        &quot;--lr&quot;, type=float, default=0.01, metavar=&quot;LR&quot;, help=&quot;learning rate (default: 0.01)&quot;
    )
    parser.add_argument(
        &quot;--momentum&quot;, type=float, default=0.5, metavar=&quot;M&quot;, help=&quot;SGD momentum (default: 0.5)&quot;
    )

    # Container environment
    parser.add_argument(&quot;--hosts&quot;, type=list, default=json.loads(os.environ[&quot;SM_HOSTS&quot;]))
    parser.add_argument(&quot;--current-host&quot;, type=str, default=os.environ[&quot;SM_CURRENT_HOST&quot;])
    parser.add_argument(&quot;--model-dir&quot;, type=str, default=os.environ[&quot;SM_MODEL_DIR&quot;])
    parser.add_argument(&quot;--data&quot;, type=str, default=os.environ[&quot;SM_CHANNEL_TRAINING&quot;])
    parser.add_argument(&quot;--num-gpus&quot;, type=int, default=os.environ[&quot;SM_NUM_GPUS&quot;])
    args=parser.parse_args()
    
    main(args)
</code></pre>
<p>To test the model on the endpoint I sent over an image using the following code:</p>
<pre><code>from sagemaker.serializers import IdentitySerializer
import base64

predictor.serializer = IdentitySerializer(&quot;image/png&quot;)
with open(&quot;Akita_00282.jpg&quot;, &quot;rb&quot;) as f:
    payload = f.read()

    
response = predictor.predict(payload)```
</code></pre>",1,0,2022-03-28 19:32:16.573000 UTC,,2022-04-05 06:38:38.163000 UTC,0,amazon-web-services|neural-network|pytorch|amazon-sagemaker|resnet,229,2022-01-03 18:22:34.077000 UTC,2022-09-23 19:30:02.803000 UTC,,1,0,0,3,,,,,,['amazon-sagemaker']
Pick up Results From ML Studio Pipeline in Data Factory Pipeline,"<p>We currently have a Data Factory pipeline that is able to call one of our ML Studio Pipelines successfully.  After the ML Studio Pipeline completed, we wanted Azure Data Factory to pick up the results of the ML Studio Pipeline and store the results in SQL Server.</p>
<p>We found the PipelineData class stores the results in a folder in blob based on the child run id, which makes it hard for Data factory to pick up the results.  We then discovered OutputFileDatasetConfig which allows ML Studio to save the results to a static location for Data Factory.  This worked great for Data Factory except OutputFileDatasetConfig doesn't always work :( since it's experimental class.  It took us a while to figure this out and we even created a stackoverflow question for this, which we resolved, and can be found here:  <a href=""https://stackoverflow.com/questions/65240603/azure-ml-studio-ml-pipeline-exception-no-temp-file-found/65350106#65350106"">Azure ML Studio ML Pipeline - Exception: No temp file found</a></p>
<p>We returned to using PipelineData class which stores the results in a folder in blob based on the child run id, but we can't figure out how to get Data factory to find the blob based on the child run id of the ML Studio Pipeline it just ran.</p>
<p><strong>So my question is, how do you get Data Factory to pick up the results of a ML Studio Pipeline which was triggered from a Data Factory Pipeline???</strong></p>
<p>Here is a simple visual of the Data Factory pipeline we're trying to build.</p>
<pre><code>Step 1: Store Data in azure file store --&gt;
Step 2: Run ML Studio scoring Pipeline --&gt;
Step 3: Copy Results to SQL Server
</code></pre>
<p>Step 3 is the step we can't figure out.  Any help would be greatly appreciated.  Thanks and happy coding!</p>",1,0,2020-12-18 18:16:58.553000 UTC,,,1,azure-data-factory|azure-data-factory-2|azure-machine-learning-service|ml-studio|azureml-python-sdk,281,2018-06-10 03:57:32.363000 UTC,2022-09-23 22:17:15.213000 UTC,,247,637,0,53,,,,,,['azure-machine-learning-service']
Where does Kubeflow pipeline look for packages in `packages_to_install`?,"<p>I am using Kubeflow Pipelines in Vertex AI to create my ML pipeline and has beeen able to use standard packaged in Kubeflow component using the below syntax</p>
<pre><code>@component(
   # this component builds an xgboost classifier with xgboost
   packages_to_install=[&quot;google-cloud-bigquery&quot;, &quot;xgboost&quot;, &quot;pandas&quot;, &quot;sklearn&quot;, &quot;joblib&quot;, &quot;pyarrow&quot;],
   base_image=&quot;python:3.9&quot;,
   output_component_file=&quot;output_component/create_xgb_model_xgboost.yaml&quot;
)
def build_xgb_xgboost(project_id: str,
                     data_set_id: str,
                     training_view: str,
                     metrics: Output[Metrics],
                     model: Output[Model]
):
</code></pre>
<p>Now I need to add my custom python module in <code>packages_to_install</code> . Is there a way to do it? For this I need to understand where does KFP look for packages when installing those on top of base_image.
I understand this can be achieved using a custom base_image where I build the base_image with my python module in it. But it seems like an overkill for me and would prefer to specify python module where applicable in the component specification
Something like below</p>
<pre><code>@component(
   # this component builds an xgboost classifier with xgboost
   packages_to_install=[&quot;my-custom-python-module&quot;,&quot;google-cloud-bigquery&quot;, &quot;xgboost&quot;, &quot;pandas&quot;, &quot;sklearn&quot;, &quot;joblib&quot;, &quot;pyarrow&quot;],
   base_image=&quot;python:3.9&quot;,
   output_component_file=&quot;output_component/create_xgb_model_xgboost.yaml&quot;
)
def build_xgb_xgboost(project_id: str,
                     data_set_id: str,
                     training_view: str,
                     metrics: Output[Metrics],
                     model: Output[Model]
):
</code></pre>",0,2,2022-06-22 13:05:51.500000 UTC,,,1,python|kubeflow|google-cloud-vertex-ai|kubeflow-pipelines,177,2009-05-27 17:42:14.993000 UTC,2022-09-23 06:17:58.327000 UTC,"Gothenburg, Sweden",1547,28,9,212,,,,,,['google-cloud-vertex-ai']
PowerBI: Unable to parse the response from the Azure ML Web Service,"<p>I created a time series experiment and a model in MS Azure Machine Learning.</p>
<p>That model is visible in PowerBI, but when trying to deploy it i get the following error:
Unable to parse the response from the Azure ML Web Service</p>
<p>I think it is due to the timestamp. When choosing a timestamp in BowerBI, I can only use numbers. No date format.</p>
<p>Did anyone solved this issue yet?</p>",1,1,2021-05-25 09:22:13.917000 UTC,,2021-05-25 10:22:41.360000 UTC,-1,azure|powerbi|azure-machine-learning-service,199,2021-05-25 09:16:01.347000 UTC,2021-06-18 08:57:28.407000 UTC,,1,0,0,0,,,,,,['azure-machine-learning-service']
MLFlow Projects throw JSONDecode error when run,"<p>I'm trying to get MLFlow Projects to run using the MLFlow CLI and its following the tutorial leads to an error.  For any project I try to run from the CLI, I get the following error</p>

<pre><code>Traceback (most recent call last):
  File ""/home/rbc/.local/bin/mlflow"", line 11, in &lt;module&gt;
    sys.exit(cli())
  File ""/home/rbc/.local/lib/python3.6/site-packages/click/core.py"", line 764, in __call__
    return self.main(*args, **kwargs)
  File ""/home/rbc/.local/lib/python3.6/site-packages/click/core.py"", line 717, in main
    rv = self.invoke(ctx)
  File ""/home/rbc/.local/lib/python3.6/site-packages/click/core.py"", line 1137, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File ""/home/rbc/.local/lib/python3.6/site-packages/click/core.py"", line 956, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File ""/home/rbc/.local/lib/python3.6/site-packages/click/core.py"", line 555, in invoke
    return callback(*args, **kwargs)
  File ""/home/rbc/.local/lib/python3.6/site-packages/mlflow/cli.py"", line 139, in run
    run_id=run_id,
  File ""/home/rbc/.local/lib/python3.6/site-packages/mlflow/projects/__init__.py"", line 230, in run
    storage_dir=storage_dir, block=block, run_id=run_id)
  File ""/home/rbc/.local/lib/python3.6/site-packages/mlflow/projects/__init__.py"", line 88, in _run
    active_run = _create_run(uri, experiment_id, work_dir, entry_point)
  File ""/home/rbc/.local/lib/python3.6/site-packages/mlflow/projects/__init__.py"", line 579, in _create_run
    active_run = tracking.MlflowClient().create_run(experiment_id=experiment_id, tags=tags)
  File ""/home/rbc/.local/lib/python3.6/site-packages/mlflow/tracking/client.py"", line 101, in create_run
    source_version=source_version
  File ""/home/rbc/.local/lib/python3.6/site-packages/mlflow/store/rest_store.py"", line 156, in create_run
    response_proto = self._call_endpoint(CreateRun, req_body)
  File ""/home/rbc/.local/lib/python3.6/site-packages/mlflow/store/rest_store.py"", line 66, in _call_endpoint
    js_dict = json.loads(response.text)
  File ""/usr/lib/python3.6/json/__init__.py"", line 354, in loads
    return _default_decoder.decode(s)
  File ""/usr/lib/python3.6/json/decoder.py"", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File ""/usr/lib/python3.6/json/decoder.py"", line 357, in raw_decode
    raise JSONDecodeError(""Expecting value"", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
</code></pre>

<p>Here's an example of the type of command I'm using to start the run, which comes directly from the tutorial </p>

<pre><code>mlflow run https://github.com/mlflow/mlflow#examples/sklearn_elasticnet_wine -m databricks -c cluster-spec.json --experiment-id 72647065958042 -P alpha=2.0 -P l1_ratio=0.5
</code></pre>

<p>I've traced the error to something involving MLFLow returning empty when it tries to start a run but I can successfully run MLFlow experiments using the Databricks environment I'm connecting to so I'm not sure where the problem is, I'm running MLFlow 0.9.1 on Ubuntu 18.04</p>",1,2,2019-05-13 13:37:47.253000 UTC,,,3,mlflow,386,2017-10-31 17:37:18.900000 UTC,2019-10-24 20:16:31.737000 UTC,"Orlando, FL, United States",31,0,0,3,,,,,,['mlflow']
AWS Sagemaker fatal: could not read Username for 'https://gitlab.com/my/repo.git': terminal prompts disabled,"<p>I want to integrate my private gitlab repository into AWS Sagemaker.</p>
<p>I added git repository on Sagemaker using https protocol (it allows only this protocol) and saved secrets(username and password of my gitlab account) for git repo.</p>
<p>When I run notebook instance by linking git repo, it failed with following message.</p>
<p><code>fatal: could not read Username for 'https://gitlab.com/my/repo.git': terminal prompts disabled</code></p>
<p>Is there any step I am missing?</p>",0,1,2021-08-20 07:24:08.450000 UTC,,2021-08-20 22:35:26.860000 UTC,4,git|amazon-web-services|jupyter-notebook|gitlab|amazon-sagemaker,197,2016-03-16 06:06:26.687000 UTC,2022-09-24 21:55:35.777000 UTC,Europe,349,45,2,50,,,,,,['amazon-sagemaker']
Cannot see the kernel Python 3 (TensorFlow 2.3 Python 3.7 GPU Optimized) in sagemaker,"<p>Amazon sagemaker <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/notebooks-available-kernels.html"" rel=""nofollow noreferrer"">documentation</a>
states that <strong>TensorFlow 2.3 Python 3.7 GPU Optimized</strong> kernel should be available to use when a sagemaker notebook instance is used. But when I use a <em>ml.p2.xlarge</em> (us-west-2) <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/nbi.html"" rel=""nofollow noreferrer"">amazon sagemaker notebook instance</a> I cannot see the <em>TensorFlow 2.3 Python 3.7 GPU Optimized</em> kernel</p>
<p>I can see other kernles such as</p>
<ul>
<li>Python 3 (TensorFlow 2.1 Python 3.6 GPU Optimized)</li>
<li>Python 3 (MXNet 1.8 Python 3.7 GPU Optimized)</li>
</ul>
<p>Do I need to enable some particular setting to see <em>TensorFlow 2.3 Python 3.7 GPU Optimized</em> kernel</p>",1,0,2021-06-21 09:39:23.917000 UTC,,,0,python|amazon-web-services|tensorflow|amazon-sagemaker,381,2014-12-15 09:33:28.490000 UTC,2022-09-22 13:59:19.453000 UTC,Singapore,661,18,3,63,,,,,,['amazon-sagemaker']
Sagemaker xgboost regression with multi-dimansional targets,"<p>I'm using XGBoost Regression from Amazon Sagemaker and I have a dataset with categorical features where I want to have a multi-dimensional target. Is this at all possible with their APIs?</p>
<p>The features would be integers starting from 0 and the target would have to be a vector consisting of 2 float numbers ranging from -100 to 100 (e.g <code>[21.57, -64.90]</code>).</p>",1,0,2022-08-16 11:33:08.477000 UTC,,,0,amazon-web-services|regression|xgboost|amazon-sagemaker,20,2019-10-19 20:09:49.383000 UTC,2022-09-21 12:36:28.800000 UTC,,15,18,0,19,,,,,,['amazon-sagemaker']
"Mlflow log_model, not able to predict with spark_udf but with python works","<p>I was wondering to log a model on mlflow, once I do it, I'm able to predict probabilities with python loaded model but not with spark_udf. The thing is, I still need to have a preprocessing function inside the model. Here is a toy reproductible example for you to see when it fails:</p>
<pre><code>import mlflow
from mlflow.models.signature import infer_signature
from sklearn.datasets import make_classification
from sklearn.ensemble import RandomForestClassifier
import pandas as pd
import numpy as np

X, y = make_classification(n_samples=1000, n_features=10, n_informative=2, n_classes=2, shuffle=True, random_state=1995)
X, y = pd.DataFrame(X), pd.DataFrame(y,columns=[&quot;target&quot;])
# geerate column names
X.columns = [f&quot;col_{idx}&quot; for idx in range(len(X.columns))]
X[&quot;categorical_column&quot;] = np.random.choice([&quot;a&quot;,&quot;b&quot;,&quot;c&quot;], size=len(X) )


def encode_catcolumn(X):
  X = X.copy()
  # replace cat values [a,b,c] for [-10,0,35] respectively
  X['categorical_column'] = np.select([X[&quot;categorical_column&quot;] == &quot;a&quot;, X[&quot;categorical_column&quot;] == &quot;b&quot;, X[&quot;categorical_column&quot;] == &quot;c&quot;],  [-10, 0,35] ) 
  return X

# with catcolumn encoded; i need to use custom encoding , we'll do this within mlflow later
X_encoded = encode_catcolumn(X)

</code></pre>
<p>Now let's create a wrapper for the model to encode the function within the model. Please see that the function encode_catcolumn within the class and the one outside the class presented before are the same.</p>
<pre><code>class SklearnModelWrapper(mlflow.pyfunc.PythonModel):
  def __init__(self, model):
    self.model = model
  def encode_catcolumn(self,X):
    X = X.copy()
    # replace cat values [a,b,c] for [-10,0,35] respectively
    X['categorical_column'] = np.select([X[&quot;categorical_column&quot;] == &quot;a&quot;, X[&quot;categorical_column&quot;] == &quot;b&quot;, X[&quot;categorical_column&quot;] == &quot;c&quot;],  [-10, 0,35] ) 
    return X 
  def predict(self, context, model_input):
    # encode catvariable
    model_input = self.encode_catcolumn(model_input)
    # predict probabilities
    predictions = self.model.predict_proba(model_input)[:,1]
    return predictions
</code></pre>
<p>Now let's log the model</p>
<pre><code>with mlflow.start_run(run_name=&quot;reproductible_example&quot;) as run:
  clf = RandomForestClassifier()
  clf.fit(X_encoded,y)
  # wrappmodel with pyfunc, does the encoding inside the class 
  wrappedModel = SklearnModelWrapper(clf)
  # When the model is deployed, this signature will be used to validate inputs.
  mlflow.pyfunc.log_model(&quot;reproductible_example_model&quot;, python_model=wrappedModel)

model_uuid = run.info.run_uuid
model_path = f'runs:/{model_uuid}/reproductible_example_model'
</code></pre>
<p>Do the inference without spark and works perfectly:</p>
<pre><code>model_uuid = run.info.run_uuid
model_path = f'runs:/{model_uuid}/reproductible_example_model'
# Load model as a PyFuncModel.
loaded_model = mlflow.pyfunc.load_model(model_path)
# predictions without spark , encodes the variables INSIDE; this WORKS
loaded_model.predict(X)
</code></pre>
<p>Now do the inference with spark_udf and get an error:</p>
<pre><code># create spark dataframe to test it on spark
X_spark = spark.createDataFrame(X)
# Load model as a Spark UDF.
loaded_model_spark = mlflow.pyfunc.spark_udf(spark, model_uri=model_path)

# Predict on a Spark DataFrame.
columns = list(X_spark.columns)
# this does not work
X_spark.withColumn('predictions', loaded_model_spark(*columns)).collect()
</code></pre>
<p>The error is:</p>
<pre><code>PythonException: An exception was thrown from a UDF: 'KeyError: 'categorical_column'', from &lt;command-908038&gt;, line 7. Full traceback below:
</code></pre>
<p>I need to some how encode the variables and preprocess within the class. Is there any solution to this or any workaround to make this code able to woork with spark?
What I've tried so far:</p>
<ol>
<li>Incorporate the encode_catcolumn within a sklearn Pipeline (with a custom encoder sklearn) -&gt; Fails;</li>
<li>Create a function within the sklearn wrapper class (this example) -&gt; Fails
3 ) Use the log_model and then create a pandas_udf in order to do it with spark as well --&gt; works but that's not what I want. I would like to be able to run the model on spark with just calling .predict() method or something like that.</li>
<li>When a remove the preprocessing function and do it outside the class --&gt;  this actually works but this is not what</li>
</ol>",1,0,2021-12-09 20:31:49.030000 UTC,,,0,apache-spark|pyspark|scikit-learn|mlflow|mlops,730,2020-04-28 18:29:02.797000 UTC,2022-09-24 13:27:28.063000 UTC,"Buenos Aires, Argentina",391,44,2,26,,,,,,['mlflow']
Can an organizational account (office 365) be used for live/Microsoft services?,"<p>I understand that Office 365 is on separate domain and live id (Microsoft account) is used for consumer applications.</p>

<p>But can an Office 365 account get live/Microsoft services?</p>

<p>The issue is we trying to SSO Office 365 applications and Azure ML (used with Microsoft account) but as the domains are different I am unable to find any proper help or process on the web.</p>

<p>We can create a live account with our company domain but can we create a federation on Live account ? For e.g. on Office 365 we created a @.com federation and were able to SSO it, how can we do the same with a live account ?</p>",1,0,2015-03-12 01:01:17.520000 UTC,,2015-03-13 20:00:22.120000 UTC,1,azure|single-sign-on|office365|microsoft-account|azure-machine-learning-studio,292,2013-03-14 18:30:03.303000 UTC,2019-10-09 19:03:09.400000 UTC,,203,2,1,27,,,,,,['azure-machine-learning-studio']
How to dynamically pass the default_value in the PipelineParameter in Azure ML ADF interface?,"<p>I am trying to pass the value dynamically for default_value in the pipeline parameter to avoid the duplication of the entire pipeline for each parameter value.</p>
<p><img src=""https://i.stack.imgur.com/0oZY6.png"" alt=""A snapshot of the pipeline python script"" /></p>
<p>Any immediate help would be highly appreciated. Thanks.</p>",1,1,2022-05-20 09:21:38.130000 UTC,,2022-06-01 06:50:46.047000 UTC,0,azure|azure-data-factory|azure-machine-learning-service,145,2022-05-20 08:46:04.497000 UTC,2022-07-15 06:02:55.610000 UTC,India,21,0,0,3,,,,,,['azure-machine-learning-service']
Amazon Sagemaker ModelError when serving model,"<p>I have uploaded a transformer roberta model in S3 bucket. Am now trying to run inference against the model using Pytorch with SageMaker Python SDK. I specified the model directory <code>s3://snet101/sent.tar.gz</code> which is a compressed file of the model (pytorch_model.bin) and all its dependencies.  Here is the code</p>
<pre><code>model = PyTorchModel(model_data=model_artifact,
                   name=name_from_base('roberta-model'),
                   role=role, 
                   entry_point='torchserve-predictor2.py',
                   source_dir='source_dir',
                   framework_version='1.4.0',
                   py_version = 'py3',
                   predictor_cls=SentimentAnalysis)
predictor = model.deploy(initial_instance_count = 1, instance_type = 'ml.m4.xlarge')
test_data = {&quot;text&quot;: &quot;How many cows are in the farm ?&quot;}
prediction = predictor.predict(test_data)
</code></pre>
<p>I get the following error on the predict method of the predictor object:</p>
<pre><code>ModelError                                Traceback (most recent call last)
&lt;ipython-input-6-bc621eb2e056&gt; in &lt;module&gt;
----&gt; 1 prediction = predictor.predict(test_data)

~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/predictor.py in predict(self, data, initial_args, target_model, target_variant)
    123 
    124         request_args = self._create_request_args(data, initial_args, target_model, target_variant)
--&gt; 125         response = self.sagemaker_session.sagemaker_runtime_client.invoke_endpoint(**request_args)
    126         return self._handle_response(response)
    127 

~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/client.py in _api_call(self, *args, **kwargs)
    355                     &quot;%s() only accepts keyword arguments.&quot; % py_operation_name)
    356             # The &quot;self&quot; in this scope is referring to the BaseClient.
--&gt; 357             return self._make_api_call(operation_name, kwargs)
    358 
    359         _api_call.__name__ = str(py_operation_name)

~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/client.py in _make_api_call(self, operation_name, api_params)
    674             error_code = parsed_response.get(&quot;Error&quot;, {}).get(&quot;Code&quot;)
    675             error_class = self.exceptions.from_code(error_code)
--&gt; 676             raise error_class(parsed_response, operation_name)
    677         else:
    678             return parsed_response

ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from model with message &quot;Your invocation timed out while waiting for a response from container model. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.&quot;. See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/roberta-model-2020-12-16-09-42-37-479 in account 165258297056 for more information.
</code></pre>
<p>I checked the server log error</p>
<pre><code>java.lang.IllegalArgumentException: reasonPhrase contains one of the following prohibited characters: \r\n: Can't load config for '/.sagemaker/mms/models/model'. Make sure that:
'/.sagemaker/mms/models/model' is a correct model identifier listed on 'https://huggingface.co/models'
or '/.sagemaker/mms/models/model' is the correct path to a directory containing a config.json file
</code></pre>
<p>How can I fix this?</p>",1,0,2020-12-16 10:12:49.800000 UTC,1.0,,6,amazon-s3|pytorch|amazon-sagemaker|huggingface-transformers,623,2017-12-21 21:14:23.330000 UTC,2021-04-28 11:41:54.087000 UTC,,97,1,0,14,,,,,,['amazon-sagemaker']
SyntaxError (amazon-sagemaker-stock-prediction/dbg-custom-rnn.ipython),"<p>I'm running the code cell below from <a href=""https://github.com/aws-samples/amazon-sagemaker-stock-prediction/blob/master/notebooks/dbg-custom-rnn.ipynb"" rel=""nofollow noreferrer"">https://github.com/aws-samples/amazon-sagemaker-stock-prediction/blob/master/notebooks/dbg-custom-rnn.ipynb</a> , on SageMaker Notebook instance. For more information, here is the link to the error I posted on GitHub: <a href=""https://github.com/aws-samples/amazon-sagemaker-stock-prediction/issues/7"" rel=""nofollow noreferrer"">https://github.com/aws-samples/amazon-sagemaker-stock-prediction/issues/7</a>.</p>
<pre><code>%%time
Instantiate estimator with container image of artifact and backend EC2 instance(s)
rnn = Estimator(image,
role, 1, 'ml.c5.18xlarge',
output_path=output_location,
base_job_name = base_job_name,
sagemaker_session=session)

rnn.set_hyperparameters(**hyperparameters)
Train the model
rnn.fit(data_location)
estimator_job = rnn.latest_training_job.job_name
model_archive = &quot;{}/{}/output/{}/output/model.tar.gz&quot;.format(artifactname,interval,estimator_job)
print(&quot;Estimator created at completion of training job {}&quot;.format(estimator_job))
</code></pre>
<p>And I hit the error below:</p>
<pre><code>Parameter image_name will be renamed to image_uri in SageMaker Python SDK v2.
's3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.
2020-08-12 15:58:15 Starting - Starting the training job...
2020-08-12 15:58:17 Starting - Launching requested ML instances......
2020-08-12 15:59:38 Starting - Preparing the instances for training...
2020-08-12 16:00:16 Downloading - Downloading input data...
2020-08-12 16:00:22 Training - Downloading the training image...
2020-08-12 16:01:13 Uploading - Uploading generated training model.2020-08-12 16:01:08.786584: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory
2020-08-12 16:01:08.786650: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
Starting the training.
Hyperparameters file : {&quot;target_stock&quot;: &quot;BMW&quot;, &quot;lag&quot;: &quot;10&quot;, &quot;interval&quot;: &quot;D&quot;, &quot;batch_size&quot;: &quot;4096&quot;, &quot;covariate_columns&quot;: &quot;StartPrice, MinPrice, MaxPrice&quot;, &quot;percent_train&quot;: &quot;85.0&quot;, &quot;covariate_stocks&quot;: &quot;CON, DAI, PAH3, VOW3&quot;, &quot;dropout_ratio&quot;: &quot;0.1&quot;, &quot;num_epochs&quot;: &quot;1000&quot;, &quot;target_column&quot;: &quot;EndPrice&quot;, &quot;horizon&quot;: &quot;5&quot;, &quot;num_units&quot;: &quot;256&quot;}
Hyperparameters initialized
Loading data from : /opt/ml/input/data/training/resampled_stockdata.csv
Loading data from : /opt/ml/input/data/training/resampled_stockdata.csv
Training data loaded
100 Stock symbols found.
Records for 65 trading days found.
0-CON#0111-DAI#0112-PAH3#0113-VOW3
Exception during training: invalid syntax (core.py, line 314)
Traceback (most recent call last):
File &quot;/opt/program/train&quot;, line 255, in train
traindata.to_csv(os.path.join(model_path, trainfile))
File &quot;/usr/local/lib/python3.5/dist-packages/pandas/core/generic.py&quot;, line 3020, in to_csv
formatter.save()
File &quot;/usr/local/lib/python3.5/dist-packages/pandas/io/formats/csvs.py&quot;, line 157, in save
compression=self.compression)
File &quot;/usr/local/lib/python3.5/dist-packages/pandas/io/common.py&quot;, line 344, in _get_handle
from s3fs import S3File
File &quot;/usr/local/lib/python3.5/dist-packages/s3fs/__init__.py&quot;, line 1, in &lt;module&gt;
from .core import S3FileSystem, S3File
File &quot;/usr/local/lib/python3.5/dist-packages/s3fs/core.py&quot;, line 8, in &lt;module&gt;
from fsspec import AbstractFileSystem
File &quot;/usr/local/lib/python3.5/dist-packages/fsspec/__init__.py&quot;, line 10, in &lt;module&gt;
from .mapping import FSMap, get_mapper
File &quot;/usr/local/lib/python3.5/dist-packages/fsspec/mapping.py&quot;, line 2, in &lt;module&gt;
from .core import url_to_fs
File &quot;/usr/local/lib/python3.5/dist-packages/fsspec/core.py&quot;, line 314
out[0] = (f&quot;{out[0][1]}://&quot;, out[0][1], out[0][2])
^
SyntaxError: invalid syntax
2020-08-12 16:01:19 Failed - Training job failed
UnexpectedStatusException Traceback (most recent call last)
&lt;timed exec&gt; in &lt;module&gt;

~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/estimator.py in fit(self, inputs, wait, logs, job_name, experiment_config)
497 self.jobs.append(self.latest_training_job)
498 if wait:
--&gt; 499 self.latest_training_job.wait(logs=logs)
500
501 def _compilation_job_name(self):

~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/estimator.py in wait(self, logs)
1114 # If logs are requested, call logs_for_jobs.
1115 if logs != &quot;None&quot;:
-&gt; 1116 self.sagemaker_session.logs_for_job(self.job_name, wait=True, log_type=logs)
1117 else:
1118 self.sagemaker_session.wait_for_job(self.job_name)

~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/session.py in logs_for_job(self, job_name, wait, poll, log_type)
3075
3076 if wait:
-&gt; 3077 self._check_job_status(job_name, description, &quot;TrainingJobStatus&quot;)
3078 if dot:
3079 print()

~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/session.py in _check_job_status(self, job, desc, status_key_name)
2669 ),
2670 allowed_statuses=,
-&gt; 2671 actual_status=status,
2672 )
2673

UnexpectedStatusException: Error for Training job dbg-custom-rnn-D-BMW-2020-08-12-15-58-15-812: Failed. Reason: AlgorithmError: Exception during training: invalid syntax (core.py, line 314)
Traceback (most recent call last):
File &quot;/opt/program/train&quot;, line 255, in train
traindata.to_csv(os.path.join(model_path, trainfile))
File &quot;/usr/local/lib/python3.5/dist-packages/pandas/core/generic.py&quot;, line 3020, in to_csv
formatter.save()
File &quot;/usr/local/lib/python3.5/dist-packages/pandas/io/formats/csvs.py&quot;, line 157, in save
compression=self.compression)
File &quot;/usr/local/lib/python3.5/dist-packages/pandas/io/common.py&quot;, line 344, in _get_handle
from s3fs import S3File
File &quot;/usr/local/lib/python3.5/dist-packages/s3fs/__init__.py&quot;, line 1, in &lt;module&gt;
from .core import S3FileSystem, S3File
File &quot;/usr/local/lib/python3.5/dist-packages/s3fs/core.py&quot;, line 8, in &lt;module&gt;
from fsspec import AbstractFileSystem
File &quot;/usr/local/lib/python3.5/dist-packages/fsspec/__init__.py&quot;, line 10, in &lt;module&gt;
from .mapping import FSMap, get_mapper
File &quot;/usr/local/lib/python3.5/dist-packages/fsspec/map
</code></pre>
<p>Could anyone explain for me how to solve this error.
Thank you so much!</p>",1,0,2020-08-12 22:09:07.850000 UTC,,,0,amazon-sagemaker|predictive,428,2019-04-25 12:10:02.473000 UTC,2021-12-16 16:23:12.503000 UTC,"Dublin, Ireland",57,5,0,52,,,,,,['amazon-sagemaker']
Grant privileges on future objects in Unity Catalog,"<p>Similar to this question : <a href=""https://stackoverflow.com/questions/22684255/grant-privileges-on-future-tables-in-postgresql"">Grant privileges on future tables in PostgreSQL?</a></p>
<p>I would like to enable SELECT access to all the tables that will be created within an existing database, is there any way to achieve this with Unity Catalog ?</p>",1,0,2022-07-11 14:38:42.467000 UTC,1.0,,2,databricks-unity-catalog,30,2015-10-14 15:18:06.937000 UTC,2022-09-23 14:41:36.493000 UTC,Paris,1902,35,8,95,,,,,,['databricks-unity-catalog']
Is there way of gaining more root (temporary) volume on the aws sagemaker notebook instance?,"<p>Aws sagemaker notebook instances come with a fixed root volume size of ~104GB whose ~15 GB is free (available).</p>
<p>Docker uses this temporary memory (<code>/var/lib/docker</code> as far as I know).</p>
<p>When I try to build docker image to create custom training-job, temporary root volume in use  blows up and system throws &quot;no space left on the device&quot; error.</p>
<p>I tried to delete anaconda directory (~62 GB), however then, boto3 and sagemaker python libraries stopped working.</p>
<p>What is the best way to solve problem?</p>
<p>Heavy Dockerfile I try to build to push ECR :</p>
<pre><code>ARG REGION=&quot;us-east-1&quot;

FROM 763104351884.dkr.ecr.$REGION.amazonaws.com/pytorch-training:1.8.1-gpu-py36-cu111-ubuntu18.04

RUN pip3 install torch==1.8.2+cu111 torchvision==0.9.2+cu111 -f https://download.pytorch.org/whl/lts/1.8/torch_lts.html

RUN python3 -m pip install detectron2 -f \
  https://dl.fbaipublicfiles.com/detectron2/wheels/cu111/torch1.8/index.html

ENV FORCE_CUDA=&quot;1&quot;

ENV TORCH_CUDA_ARCH_LIST=&quot;Volta&quot;

ENV FVCORE_CACHE=&quot;/tmp&quot;

############# SageMaker section ##############

COPY tested_train_src/train_src /opt/ml/code
WORKDIR /opt/ml/code

ENV SAGEMAKER_SUBMIT_DIRECTORY /opt/ml/code
ENV SAGEMAKER_PROGRAM train.py

WORKDIR /

ENTRYPOINT [&quot;bash&quot;, &quot;-m&quot;, &quot;start_with_right_hostname.sh&quot;]
</code></pre>
<p>Build command:</p>
<pre><code>docker build -t image-name:tag . --build-arg REGION=&quot;us-east-1&quot;
</code></pre>
<p>Output from docker build</p>
<pre><code>Sending build context to Docker daemon  1.935GB
Step 1/12 : ARG REGION=&quot;us-east-1&quot;
Step 2/12 : FROM 763104351884.dkr.ecr.$REGION.amazonaws.com/pytorch-training:1.8.1-gpu-py36-cu111-ubuntu18.04
1.8.1-gpu-py36-cu111-ubuntu18.04: Pulling from pytorch-training

d2c87b75: Pulling fs layer 
10be24e1: Pulling fs layer 
7173dcfe: Pulling fs layer 
8de7822d: Pulling fs layer 
bf66c36b: Pulling fs layer 
c74d4d18: Pulling fs layer 
f70a70b2: Pulling fs layer 
4e2cb041: Pulling fs layer 
8ddd4da6: Pulling fs layer 
fac38f0d: Pulling fs layer 
a26fd875: Pulling fs layer 
1dca51bb: Pulling fs layer 
0d6bb6c9: Pulling fs layer 
26721764: Pulling fs layer 
956fbe7a: Pulling fs layer 
ad4fa2a5: Pulling fs layer 
20c0bd9a: Pulling fs layer 
82804870: Pulling fs layer 
1d1fdc54: Pulling fs layer 
4500c676: Pulling fs layer 
923bbc02: Pulling fs layer 
0c9d88c6: Pulling fs layer 
f5b0d167: Pulling fs layer 
2f2aa1af: Pulling fs layer 
c272e0bb: Pulling fs layer 
311661aa: Pulling fs layer 
ed3ef379: Pulling fs layer 
03c2d7ac: Pulling fs layer 
1cefc5dc: Pulling fs layer 
30fd2377: Pulling fs layer 
78d30971: Pulling fs layer 
d18f41de: Pulling fs layer 
4c2aeed5: Pulling fs layer 
f099a687: Pulling fs layer 
253573ff: Pulling fs layer 
515cab8b: Pulling fs layer 
056b70c3: Pulling fs layer 
Digest: sha256:66af111d2bd9dae500ad73a7b427103fe8379cbb24bf4ce7cb7d5770d31cd9322KExtracting  505.2MB/962.1MB
Status: Downloaded newer image for 763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-training:1.8.1-gpu-py36-cu111-ubuntu18.04
 ---&gt; b4191cf0b8c9
Step 3/12 : RUN pip3 install torch==1.8.2+cu111 torchvision==0.9.2+cu111 -f https://download.pytorch.org/whl/lts/1.8/torch_lts.html
 ---&gt; Running in 7c62740a69c6
Looking in links: https://download.pytorch.org/whl/lts/1.8/torch_lts.html
Collecting torch==1.8.2+cu111
  Downloading https://download.pytorch.org/whl/lts/1.8/cu111/torch-1.8.2%2Bcu111-cp36-cp36m-linux_x86_64.whl (1982.2 MB)
ERROR: Could not install packages due to an OSError: [Errno 28] No space left on device
</code></pre>
<p>Disk usage before build:</p>
<pre><code>sh-4.2$ df -h
Filesystem      Size  Used Avail Use% Mounted on
devtmpfs        1.9G   76K  1.9G   1% /dev
tmpfs           1.9G     0  1.9G   0% /dev/shm
/dev/nvme0n1p1  104G   89G   16G  86% /
/dev/nvme1n1     63G  1.9G   58G   4% /home/ec2-user/SageMaker
</code></pre>
<p>Disk usage after erronous build:</p>
<pre><code>sh-4.2$ df -h
Filesystem      Size  Used Avail Use% Mounted on
devtmpfs        1.9G   76K  1.9G   1% /dev
tmpfs           1.9G     0  1.9G   0% /dev/shm
/dev/nvme0n1p1  104G  101G  2.4G  98% /
/dev/nvme1n1     63G  1.9G   58G   4% /home/ec2-user/SageMaker
</code></pre>
<p>Note : I will try to mount directory <code>/var/lib/docker</code> to EBS volume at notebook start.</p>
<p>Note : I don't have any issue about attached EBS volume size. My issue is about temporary volume.</p>",1,3,2021-09-07 18:47:10.670000 UTC,,2021-09-07 18:52:17.013000 UTC,0,amazon-web-services|amazon-sagemaker,632,2020-06-01 19:54:14.117000 UTC,2022-09-25 05:13:38.733000 UTC,Turkey,36,16,0,11,,,,,,['amazon-sagemaker']
Registering model without weights with MLFLow,<p>I would like to be able to register untrained models with MLFLow to use as prototypes for instantiating models for training. I need this because we need to train thousands of models of the same type. Is this possible?</p>,1,0,2021-11-09 22:44:51.150000 UTC,,,0,mlflow,46,2021-11-09 22:39:05.873000 UTC,2021-12-30 19:51:49.690000 UTC,,1,0,0,2,,,,,,['mlflow']
Sending Azure Blob Storage csv file as an attachment to user on Microsoft Azure,"<p>I have created a Logic App in Microsoft Azure. I am having a scheduler recurrence first and then I am calling Azure ML Batch Job with Job Input and Output.  After this now, I have to send email to the user with azure blob storage .csv file generated in Azure after running Azure ML job. So, how can I send the azure blob storage file to user after running Azure ML Batch Job with Job Input and Output?  </p>",1,1,2019-03-13 16:44:54.130000 UTC,,,2,azure|email-attachments|azure-logic-apps|azure-machine-learning-studio|azure-blob-storage,3796,2019-03-11 16:56:29.193000 UTC,2020-05-15 08:41:04.897000 UTC,,19,0,0,16,,,,,,['azure-machine-learning-studio']
How to run tensor flow model locally into Mac,<p>I have ML model deployed in Sagemaker. I copied the model ( tar.gz) to my Mac and trying to write the run that tar.gz model file locally . I need to pass input as an image which will go through this model and provide me output locally.How to write the python code to run this full setup locally.</p>,1,3,2020-04-08 14:35:00.770000 UTC,,,-1,amazon-sagemaker,67,2020-04-08 14:30:45.903000 UTC,2020-08-15 22:13:00.760000 UTC,,1,0,0,1,,,,,,['amazon-sagemaker']
"Why is the Databricks notebook cell training my ML model ""Running command..."" forever?","<p>I am training PyTorch models on a Azure Databricks cluster (on a notebook), using PyTorch Lightning, and doing the tracking using mlflow.</p>
<p>I would like to store training metrics + artifacts on the Databricks-hosted tracking server.</p>
<p>To enable that, code is as follows:</p>
<pre class=""lang-py prettyprint-override""><code>mlflow.pytorch.autolog()
trainer = pl.Trainer(gpus=1, max_epochs=30, callbacks=[EarlyStopping(monitor='val_loss', patience = 6)], progress_bar_refresh_rate=0)
trainer.fit(classifier, train_dl, valid_dl)
print(&quot;Done&quot;)
</code></pre>
<p>However, the notebook cell gets stuck in a &quot;Running command...&quot; state for way too long:</p>
<p><a href=""https://i.stack.imgur.com/e3k4X.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/e3k4X.png"" alt=""Running forever"" /></a></p>
<p>even though in the driver logs execution seems to have ended:</p>
<p><a href=""https://i.stack.imgur.com/bT3Xc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bT3Xc.png"" alt=""Run is over"" /></a></p>
<p>and the experiment is marked as FINISHED in the mlflow UI:</p>
<p><a href=""https://i.stack.imgur.com/lwObH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lwObH.png"" alt=""mlflow tracking"" /></a></p>
<p>Stopping execution manually doesn't solve either, as the cell would stay in a &quot;Cancelling...&quot; state forever. So the only option left is to clear the cluster state.</p>
<p>This is a problem because I can't execute further commands that would be useful for artifact logging:</p>
<pre class=""lang-py prettyprint-override""><code>mapping.to_json(&quot;/tmp/mapping.json&quot;, orient = &quot;records&quot;)
mlflow.log_artifact(&quot;/tmp/mapping.json&quot;, &quot;mapping&quot;)

torch.save(classifier.state_dict(), &quot;/tmp/model.pt&quot;)
mlflow.log_artifact(&quot;/tmp/model.pt&quot;, &quot;model.pt&quot;)
</code></pre>
<p>This problem seems to correlate with GC problems:</p>
<blockquote>
<p>2021-04-22T07:58:58.025+0000: [GC (Allocation Failure) [PSYoungGen:
28399104K-&gt;100290K(28499456K)] 28696488K-&gt;397698K(85935104K),
0.0755720 secs] [Times: user=0.15 sys=0.06, real=0.08 secs]  2021-04-22T08:01:01.645+0000: [GC (System.gc()) [PSYoungGen:
4522724K-&gt;54360K(28561920K)] 4820132K-&gt;351776K(85997568K), 0.0237712
secs] [Times: user=0.09 sys=0.01, real=0.02 secs]
2021-04-22T08:01:01.669+0000: [Full GC (System.gc()) [PSYoungGen:
54360K-&gt;0K(28561920K)] [ParOldGen: 297416K-&gt;123173K(57435648K)]
351776K-&gt;123173K(85997568K), [Metaspace: 203356K-&gt;203325K(219136K)],
0.3513905 secs] [Times: user=0.99 sys=0.00, real=0.36 secs]</p>
</blockquote>
<p>Am I doing something wrong? Should I track experiments + artifacts in another way? Running neither single-node nor cluster works, nor reducing the size of the training set.</p>",0,3,2021-04-22 11:52:52.593000 UTC,1.0,2021-04-22 21:20:04.540000 UTC,2,python|pytorch|azure-databricks|mlflow|pytorch-lightning,387,2014-11-11 16:17:30.717000 UTC,2022-09-24 20:31:18.173000 UTC,"Verona, VR, Italy",4811,376,73,713,,,,,,['mlflow']
AzureML Dataset.File.from_files creation extremely slow even with 4 files,"<p>I have a few thousand of video files in my BlobStorage, which I set it as a datastore.
This blob storage receives new files every night and I need to split the data and register each split as a new version of AzureML Dataset.</p>
<p>This is how I do the data split, simply getting the blob paths and splitting them.</p>
<pre class=""lang-py prettyprint-override""><code>container_client = ContainerClient.from_connection_string(AZ_CONN_STR,'keymoments-clips')
blobs = container_client.list_blobs('soccer')
blobs = map(lambda x: Path(x['name']), blobs)
train_set, test_set = get_train_test(blobs, 0.75, 3, class_subset={'goal', 'hitWoodwork', 'penalty', 'redCard', 'contentiousRefereeDecision'})
valid_set, test_set = split_data(test_set, 0.5, 3)
</code></pre>
<p><code>train_set, test_set, valid_set</code> are just nx2 numpy arrays containing blob storage path and class.</p>
<p>Here is when I try to create a new version of my Dataset:</p>
<pre class=""lang-py prettyprint-override""><code>datastore = Datastore.get(workspace, 'clips_datastore')

dataset_train = Dataset.File.from_files([(datastore, b) for b, _ in train_set[:4]], validate=True, partition_format='**/{class_label}/*.mp4')
dataset_train.register(workspace, 'train_video_clips', create_new_version=True)
</code></pre>
<p>How is it possible that the Dataset creation seems to hang for an indefinite time even with only 4 paths?
I saw in the <a href=""https://docs.microsoft.com/it-it/python/api/azureml-core/azureml.data.dataset_factory.filedatasetfactory?view=azure-ml-py#from-files-path--validate-true--partition-format-none-"" rel=""noreferrer"">doc</a> that providing a list of <code>Tuple[datastore, path]</code> is perfectly fine. Do you know why?</p>
<p>Thanks</p>",3,5,2021-07-28 08:10:09.963000 UTC,,2021-07-28 13:04:29.297000 UTC,5,azure-machine-learning-studio|azure-machine-learning-service|azure-sdk-python,989,2015-07-04 10:48:10.427000 UTC,2022-09-23 15:10:29.127000 UTC,"Amsterdam, Paesi Bassi",959,136,31,204,,,,,,"['azure-machine-learning-studio', 'azure-machine-learning-service']"
How to deploy a custom model in AWS SageMaker?,"<p>I have a custom machine learning predictive model. I also have a user defined Estimator class that uses Optuna for hyperparameter tuning. I need to deploy this model to SageMaker so as to invoke it from a lambda function.</p>
<p>I'm facing trouble in the process of creating a container for the model and the Estimator.</p>
<p>I am aware that SageMaker has a scikit learn container which can be used for Optuna, but how would I leverage this to include the functions from my own Estimator class? Also, the model is one of the parameters passed to this Estimator class so how do I define it as a separate training job in order to make it an Endpoint?</p>
<p>This is how the Estimator class and the model are invoked:</p>
<pre><code>sirf_estimator = Estimator(
    SIRF, ncov_df, population_dict[countryname],
    name=countryname, places=[(countryname, None)],
    start_date=critical_country_start
    )
sirf_dict = sirf_estimator.run()
</code></pre>
<p>where:</p>
<ol>
<li>Model Name : SIRF</li>
<li>Cleaned Dataset : ncov_df</li>
</ol>
<p>Would be really helpful if anyone could look into this, thanks a ton!</p>",1,0,2020-06-25 08:16:51.700000 UTC,,2020-11-15 16:55:34.093000 UTC,5,amazon-web-services|machine-learning|deployment|scikit-learn|amazon-sagemaker,1694,2020-04-22 15:52:34.543000 UTC,2020-07-08 11:34:59.857000 UTC,,51,0,0,5,,,,,,['amazon-sagemaker']
Specifying Machine Type in Vertex AI Pipeline,"<p>I have a pipeline component defined like this:</p>
<pre><code>data_task = run_ssd_data_op(
        labels_path=input_labels,
        data_config=config_task.outputs[&quot;output_data_config&quot;],
        training_config=config_task.outputs[&quot;output_training_config&quot;],
        assets_json=dump_conversion_task.outputs[&quot;output_ssd_query&quot;]
    )
data_task.execution_options.caching_strategy.max_cache_staleness = &quot;P0D&quot;
data_task.container.add_resource_request('cpu', cpu_request)
data_task.container.add_resource_request('memory', memory_request)
</code></pre>
<p>When I run the pipeline on VertexAI the above component runs on an E2 machine type which matches the CPU and RAM requirements.</p>
<p>However, the component runs much more slowly on VertexAI than on the Kubeflow pipeline I setup using AIPlatform. I configured that cluster to use N1-highmem-32 machines for this job.</p>
<p>I would like to request that this component is run on an <code>n1-highmem-32</code> machine, how can I do that?</p>
<p>For the GPU component of the pipeline I could use the line:</p>
<pre><code>training_task.add_node_selector_constraint('cloud.google.com/gke-accelerator', 'NVIDIA_TESLA_T4').set_gpu_limit(
        gpu_request)
</code></pre>
<p>What is the equivalent <code>node_selector_constraint</code> that I need to apply to my <code>data_task</code>?</p>",1,3,2022-01-17 11:50:52.480000 UTC,,,2,google-cloud-platform|kubeflow|google-cloud-vertex-ai,603,2012-10-15 12:32:21.050000 UTC,2022-09-20 11:48:04.983000 UTC,"Morecambe, United Kingdom",3827,145,3,573,,,,,,['google-cloud-vertex-ai']
SageMaker: Unable to create network interface because subnet 'subnet-xxxx' does not have enough free addresses to satisfy the request,"<p>When you setup SageMaker you specify the VPC that it runs in, and any corresponding subnets. If no subnets are specified it uses 2 by default.</p>
<p>But during the course of architecture creation it's easy to have different resources use the same subnets, causing errors such as this:</p>
<pre><code>Failed to change to instance xx.8xlarge Failed to launch app [xxxx]: LimitExceededError: Unable to create network interface because subnet 'subnet-xxxx' does not have enough free addresses to satisfy the request. Free up addresses or add more addresses for the subnet to use, or create a new domain with a new subnet.
</code></pre>
<p>It would be nice to change the subnets that SageMaker uses without having to tear down the entire setup and start over. But the only documentation I can see on configuring the VPC/subnets for SageMakers is in the setup stage.</p>
<p>So, what is SageMaker’s relationship to subnets, <strong>where is this configured</strong>, and <strong>can this be modified after deployment</strong>?</p>",1,0,2022-03-07 19:00:55.840000 UTC,,,1,amazon-web-services|amazon-sagemaker|subnet|vpc|private-subnet,499,2012-08-31 20:08:40.090000 UTC,2022-09-25 04:17:41.297000 UTC,,11650,6318,21,977,,,,,,['amazon-sagemaker']
Reading Mp3 files from S3 to Sagemaker for feature extraction using LIBROSA,"<p>Hi I am trying to load mp3 files which are stored in S3 to Sage maker for audio feature extraction using Librosa!</p>
<p>Initially I get all the mp3 files paths</p>
<pre><code>def get_all_files():
    files = []
    #initiate s3 resource
    s3 = boto3.resource('s3')
    # select bucket
    my_bucket = s3.Bucket('songs')
    # download file into current directory
    for s3_object in my_bucket.objects.filter(Prefix = 'c/songs/'):
        if s3_object.key.endswith('mp3'):
            filename = s3_object.key
            print(filename)
            files.append(filename)
</code></pre>
<p><a href=""https://i.stack.imgur.com/xtUuQ.png"" rel=""nofollow noreferrer"">mp3 file paths</a></p>
<p>Then tried to download files using below function</p>
<pre><code>def download_all_files_music():
    #initiate s3 resource
    s3 = boto3.resource('s3')
    # select bucket
    my_bucket = s3.Bucket('songs')
    # download file into current directory
    for s3_object in my_bucket.objects.filter(Prefix = 'c/songs/'):
      if s3_object.key.endswith('mp3'):
        filename = s3_object.key
        print(filename)
        my_bucket.download_file(s3_object.key, filename)
</code></pre>
<p>But I am getting this error!</p>
<p><a href=""https://i.stack.imgur.com/FUpem.png"" rel=""nofollow noreferrer"">file not found error</a></p>
<p>Do I need to make all the songs publicly accessible to load using boto?</p>",0,1,2022-09-16 09:44:37.767000 UTC,,2022-09-17 20:19:03.813000 UTC,0,amazon-s3|amazon-sagemaker,16,2021-02-03 01:24:01.337000 UTC,2022-09-20 08:59:09.783000 UTC,,1,0,0,1,,,,,,['amazon-sagemaker']
Saving Machine Learning Logs on MongoDB,"<p>How can i save the logs of machine learing model trained in sagemaker like (Creation time, time taken for a job to learn etc.) on mongoDB</p>",1,0,2020-01-08 10:39:26.177000 UTC,,2020-01-08 21:33:35.530000 UTC,0,mongodb|amazon-web-services|amazon-sagemaker,56,2020-01-08 10:36:08.727000 UTC,2020-01-22 07:42:41.220000 UTC,,1,0,0,2,,,,,,['amazon-sagemaker']
Does Azure ML have a way of specifying dropout?,"<p>The Net# neural network <a href=""https://azure.microsoft.com/en-us/documentation/articles/machine-learning-azure-ml-netsharp-reference-guide/"" rel=""nofollow"">specification language</a> for Azure does not seem to have an option for specifying <a href=""http://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf"" rel=""nofollow"">dropout</a>. Is this available or planned? Are there alternatives or equivalent (though perhaps differently named) options I'm missing?</p>",1,0,2016-05-13 12:49:25.237000 UTC,1.0,,0,machine-learning|neural-network|azure-machine-learning-studio,115,2011-03-12 19:54:30.313000 UTC,2022-09-21 19:06:38.420000 UTC,United States,41475,1198,107,1912,,,,,,['azure-machine-learning-studio']
nbeats model logging in mlflow not working,"<p>I am trying to log nbeats model developed using darts library into mlflow ui, but I am not able to do that. I am getting the following error.</p>
<p>PS this model is developed using Darts library. If anyone knows who had developed nbetas and logged in mlflow. Kindly mention the source for my reference.</p>
<p><em>RuntimeError: [enforce fail at C:\cb\pytorch_1000000000000\work\c10\core\impl\alloc_cpu.cpp:81] data. DefaultCPUAllocator: not enough memory: you tried to allocate 16515072 bytes.</em></p>
<p>My code is as folows:</p>
<pre><code>from darts.models import LightGBMModel, RandomForest, NaiveMean, NBEATSModel
from data_preparation import darts_x_train, darts_y_train, darts_y_val, df_x

import pandas as pd
import tensorflow as tf
import mlflow
import warnings
warnings.filterwarnings(&quot;ignore&quot;)
import torch

print(torch.__version__)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

import logging

logging.basicConfig(level=logging.WARN)
logger = logging.getLogger(__name__)
lags = [-4, -24 * 4, -12 * 4, -8 * 4 * 24, -7 * 24 * 4, -10 * 4 * 24, -11 * 4 * 24, -12 * 4 * 24]
lags_future = [0, 4, 8, 12 * 4, 24 * 4, 48 * 4]
output_chunk_length = 4  # int(pd.Timedelta('2w') / (darts_x_train.time_index[1] - 
darts_x_train.time_index[0]))
print(output_chunk_length, &quot;output_chunk_length timesteps&quot;)
last_two_weeks = darts_y_val.time_index[-1] - pd.Timedelta(days=16)
test_series = darts_y_val.drop_after(last_two_weeks)

def baseline_model():
    nbeats_model = NBEATSModel(input_chunk_length=672, output_chunk_length=output_chunk_length, n_epochs=1,
                           random_state=42,
                           pl_trainer_kwargs={
                               &quot;accelerator&quot;: &quot;gpu&quot;,
                               &quot;devices&quot;: [0]
                           }, )

return nbeats_model

def mlflow_run(run_name=&quot;nbeats_model_run&quot;):

    model_uri = mlflow.get_artifact_uri(&quot;model&quot;)
    # Start MLflow run and log everyting...
    with mlflow.start_run(run_name=run_name, nested=True) as run:
    model = baseline_model()
    mlflow.keras.log_model(keras_model=model, keras_module= tf.keras, artifact_path='artifacts')
    model.fit(darts_y_train, past_covariates=darts_x_train, epochs=1)
    model.predict(n=1344, series=test_series, past_covariates=df_x)
    run_id = run.info.run_uuid
    exp_id = run.info.experiment_id

    return exp_id, run_id


 if __name__ == '__main__':
# suppress any deprecated warnings
 warnings.filterwarnings(&quot;ignore&quot;, category=DeprecationWarning)

 (exp_id, run_id) = mlflow_run()
 print(f&quot;Finished Experiment id={exp_id} and run id = {run_id}&quot;)
</code></pre>
<blockquote>
<p>C:\Users\R953682\Anaconda3\envs\MLflow_tutorial\python.exe &quot;C:/Users/R953682/Local_Energy _Consumption/MLW/run_file.py&quot;
34397 34397
11467 11467
1.12.1
4 output_chunk_length timesteps
2022/09/22 17:31:09 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\Users\R953682\AppData\Local\Temp\tmpnefahsun\model, flavor: keras), fall back to return ['tensorflow==2.9.1', 'keras==2.9.0']. Set logging level to DEBUG to see the full traceback.
2022-09-22 17:31:10 pytorch_lightning.utilities.rank_zero INFO: GPU available: True (cuda), used: True
2022-09-22 17:31:10 pytorch_lightning.utilities.rank_zero INFO: TPU available: False, using: 0 TPU cores
2022-09-22 17:31:10 pytorch_lightning.utilities.rank_zero INFO: IPU available: False, using: 0 IPUs
2022-09-22 17:31:10 pytorch_lightning.utilities.rank_zero INFO: HPU available: False, using: 0 HPUs
2022-09-22 17:31:10 pytorch_lightning.accelerators.cuda INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
2022-09-22 17:31:11 pytorch_lightning.callbacks.model_summary INFO:
| Name          | Type             | Params</p>
</blockquote>
<hr />
<h2>0 | criterion     | MSELoss          | 0
1 | train_metrics | MetricCollection | 0
2 | val_metrics   | MetricCollection | 0
3 | stacks        | ModuleList       | 69.4 M</h2>
<p>69.3 M    Trainable params
49.7 K    Non-trainable params
69.4 M    Total params
555.182   Total estimated model params size (MB)
Epoch 0: 100%|██████████| 1054/1054 [02:46&lt;00:00,  6.33it/s, loss=0.0392, train_loss=0.0496]
2022-09-22 17:33:57 pytorch_lightning.utilities.rank_zero INFO: <code>Trainer.fit</code> stopped: <code>max_epochs=1</code> reached.
Traceback (most recent call last):
File &quot;C:\Users\R953682\Anaconda3\envs\MLflow_tutorial\lib\site-packages\pytorch_lightning\trainer\trainer.py&quot;, line 650, in _call_and_handle_interrupt
return trainer_fn(*args, **kwargs)
File &quot;C:\Users\R953682\Anaconda3\envs\MLflow_tutorial\lib\site-packages\pytorch_lightning\trainer\trainer.py&quot;, line 735, in _fit_impl
results = self._run(model, ckpt_path=self.ckpt_path)
File &quot;C:\Users\R953682\Anaconda3\envs\MLflow_tutorial\lib\site-packages\pytorch_lightning\trainer\trainer.py&quot;, line 1169, in _run
self._teardown()
File &quot;C:\Users\R953682\Anaconda3\envs\MLflow_tutorial\lib\site-packages\pytorch_lightning\trainer\trainer.py&quot;, line 1229, in _teardown
self.strategy.teardown()
File &quot;C:\Users\R953682\Anaconda3\envs\MLflow_tutorial\lib\site-packages\pytorch_lightning\strategies\strategy.py&quot;, line 476, in teardown
self.lightning_module.cpu()
File &quot;C:\Users\R953682\Anaconda3\envs\MLflow_tutorial\lib\site-packages\pytorch_lightning\core\mixins\device_dtype_mixin.py&quot;, line 141, in cpu
return super().cpu()
File &quot;C:\Users\R953682\Anaconda3\envs\MLflow_tutorial\lib\site-packages\torch\nn\modules\module.py&quot;, line 738, in cpu
return self._apply(lambda t: t.cpu())
File &quot;C:\Users\R953682\Anaconda3\envs\MLflow_tutorial\lib\site-packages\torch\nn\modules\module.py&quot;, line 579, in _apply
module._apply(fn)
File &quot;C:\Users\R953682\Anaconda3\envs\MLflow_tutorial\lib\site-packages\torch\nn\modules\module.py&quot;, line 579, in _apply
module._apply(fn)
File &quot;C:\Users\R953682\Anaconda3\envs\MLflow_tutorial\lib\site-packages\torch\nn\modules\module.py&quot;, line 579, in _apply
module._apply(fn)
[Previous line repeated 3 more times]
File &quot;C:\Users\R953682\Anaconda3\envs\MLflow_tutorial\lib\site-packages\torch\nn\modules\module.py&quot;, line 615, in _apply
grad_applied = fn(param.grad)
File &quot;C:\Users\R953682\Anaconda3\envs\MLflow_tutorial\lib\site-packages\torch\nn\modules\module.py&quot;, line 738, in 
return self._apply(lambda t: t.cpu())
RuntimeError: [enforce fail at C:\cb\pytorch_1000000000000\work\c10\core\impl\alloc_cpu.cpp:81] data. DefaultCPUAllocator: not enough memory: you tried to allocate 16515072 bytes.</p>
<p>During handling of the above exception, another exception occurred:</p>
<p>Traceback (most recent call last):
File &quot;C:\Users\R953682\Local_Energy _Consumption\MLW\run_file.py&quot;, line 60, in 
(exp_id, run_id) = mlflow_run()
File &quot;C:\Users\R953682\Local_Energy _Consumption\MLW\run_file.py&quot;, line 48, in mlflow_run
model.fit(darts_y_train, past_covariates=darts_x_train, epochs=1)
File &quot;C:\Users\R953682\Anaconda3\envs\MLflow_tutorial\lib\site-packages\darts\utils\torch.py&quot;, line 112, in decorator
return decorated(self, *args, **kwargs)
File &quot;C:\Users\R953682\Anaconda3\envs\MLflow_tutorial\lib\site-packages\darts\models\forecasting\torch_forecasting_model.py&quot;, line 806, in fit
return self.fit_from_dataset(
File &quot;C:\Users\R953682\Anaconda3\envs\MLflow_tutorial\lib\site-packages\darts\utils\torch.py&quot;, line 112, in decorator
return decorated(self, *args, **kwargs)
File &quot;C:\Users\R953682\Anaconda3\envs\MLflow_tutorial\lib\site-packages\darts\models\forecasting\torch_forecasting_model.py&quot;, line 961, in fit_from_dataset
self._train(train_loader, val_loader)
File &quot;C:\Users\R953682\Anaconda3\envs\MLflow_tutorial\lib\site-packages\darts\models\forecasting\torch_forecasting_model.py&quot;, line 983, in _train
self.trainer.fit(
File &quot;C:\Users\R953682\Anaconda3\envs\MLflow_tutorial\lib\site-packages\pytorch_lightning\trainer\trainer.py&quot;, line 696, in fit
self._call_and_handle_interrupt(
File &quot;C:\Users\R953682\Anaconda3\envs\MLflow_tutorial\lib\site-packages\pytorch_lightning\trainer\trainer.py&quot;, line 664, in _call_and_handle_interrupt
self._teardown()
File &quot;C:\Users\R953682\Anaconda3\envs\MLflow_tutorial\lib\site-packages\pytorch_lightning\trainer\trainer.py&quot;, line 1229, in _teardown
self.strategy.teardown()
File &quot;C:\Users\R953682\Anaconda3\envs\MLflow_tutorial\lib\site-packages\pytorch_lightning\strategies\strategy.py&quot;, line 476, in teardown
self.lightning_module.cpu()
File &quot;C:\Users\R953682\Anaconda3\envs\MLflow_tutorial\lib\site-packages\pytorch_lightning\core\mixins\device_dtype_mixin.py&quot;, line 141, in cpu
return super().cpu()
File &quot;C:\Users\R953682\Anaconda3\envs\MLflow_tutorial\lib\site-packages\torch\nn\modules\module.py&quot;, line 738, in cpu
return self._apply(lambda t: t.cpu())
File &quot;C:\Users\R953682\Anaconda3\envs\MLflow_tutorial\lib\site-packages\torch\nn\modules\module.py&quot;, line 579, in _apply
module._apply(fn)
File &quot;C:\Users\R953682\Anaconda3\envs\MLflow_tutorial\lib\site-packages\torch\nn\modules\module.py&quot;, line 579, in _apply
module._apply(fn)
File &quot;C:\Users\R953682\Anaconda3\envs\MLflow_tutorial\lib\site-packages\torch\nn\modules\module.py&quot;, line 579, in _apply
module._apply(fn)
[Previous line repeated 3 more times]
File &quot;C:\Users\R953682\Anaconda3\envs\MLflow_tutorial\lib\site-packages\torch\nn\modules\module.py&quot;, line 615, in _apply
grad_applied = fn(param.grad)
File &quot;C:\Users\R953682\Anaconda3\envs\MLflow_tutorial\lib\site-packages\torch\nn\modules\module.py&quot;, line 738, in 
return self._apply(lambda t: t.cpu())
RuntimeError: [enforce fail at C:\cb\pytorch_1000000000000\work\c10\core\impl\alloc_cpu.cpp:81] data. DefaultCPUAllocator: not enough memory: you tried to allocate 16515072 bytes.</p>",0,0,2022-09-23 06:48:30.887000 UTC,,,0,python|python-3.x|tensorflow|mlflow,18,2018-06-01 15:13:33.743000 UTC,2022-09-23 13:49:09.390000 UTC,,23,1,0,1,,,,,,['mlflow']
Do Sagemaker ML instance types use Nitro system based on instance type pattern?,"<p>Only certain EC2 instance types are build on the <a href=""https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-types.html#ec2-nitro-instances"" rel=""nofollow noreferrer"">Nitro System</a></p>
<p>Sagemaker <a href=""https://aws.amazon.com/sagemaker/pricing/instance-types/"" rel=""nofollow noreferrer"">instance types</a> follow the pattern of <code>ml.{ec2 instance type}</code></p>
<p>If the <code>{ec2 instance type}</code> for an Amazon Sagemaker training instance was built on the <a href=""https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-types.html#ec2-nitro-instances"" rel=""nofollow noreferrer"">Nitro System</a>, can I deduce that the model training instance was <em>also</em> built on the Nitro system?</p>
<p>For example, M5 EC2 instances were built on the Nitro System. Does this mean that a Sagemaker ml.m5.xlarge instance is <em>also</em> built on the Nitro System?</p>",1,0,2020-08-20 15:56:57.983000 UTC,,,0,amazon-web-services|amazon-ec2|amazon-sagemaker,72,2012-09-28 14:12:28.500000 UTC,2022-09-23 20:25:27.910000 UTC,,5182,1902,7,315,,,,,,['amazon-sagemaker']
Install Jupytext plugin on AWS Sagemaker,"<p><a href=""https://github.com/mwouts/jupytext"" rel=""nofollow noreferrer"">Jupytext</a> allows you to save your notebook as a plain python or markdown file. One of the advantages is that you can do an easy git diff in merge requests.</p>

<p>How can you install the jupytext plugin on the jupyter/jupyterlab environment on AWS Sagemaker?</p>",1,0,2020-01-28 12:48:38.597000 UTC,2.0,2020-01-29 07:16:49.970000 UTC,2,jupyter-notebook|jupyter|amazon-sagemaker|jupyter-lab,754,2012-10-24 12:12:59.277000 UTC,2022-09-25 05:49:23.950000 UTC,"Leuven, Belgium",3126,1817,2,262,,,,,,['amazon-sagemaker']
consuming an deployed Azure ML model in PowerBI,"<p>I have issues while interfacing Azure ML with <code>PowerBI</code>. I deployed a model from <code>Auto ML</code>, and tried to consume it in <code>PowerBI</code>. I successfully completed the following tutorials <a href=""https://docs.microsoft.com/en-us/azure/machine-learning/tutorial-power-bi-automated-model"" rel=""nofollow noreferrer"">create a predictive model by using auto ML</a> and <a href=""https://docs.microsoft.com/en-us/power-bi/connect-data/service-aml-integrate"" rel=""nofollow noreferrer"">consume a model in PowerBI</a> . But when it comes to implement my proper model, I  can choose the targeted model, and use the right inputs, with the right data type, but I get this error :</p>
<pre><code>Unable to parse the response from the Azure ML Web Service
</code></pre>
<p>I have to add that my model <strong>forecasts time series</strong>. On the contrary, the model was a regression in the Microsoft tutorials. And I didn't use R or Python script, I used exactly the same method as the second tutorial about <code>PowerBI</code>.</p>
<p>Thank you very much for your help ! don't hesitate to ask me if you need more information.</p>
<p>Mary</p>",1,1,2021-01-26 10:23:13.523000 UTC,,2021-01-27 10:20:18.257000 UTC,2,azure-web-app-service|powerbi-desktop|azure-machine-learning-studio,234,2021-01-26 10:11:25.343000 UTC,2021-09-01 08:51:05.830000 UTC,,21,0,0,3,,,,,,['azure-machine-learning-studio']
Azure Machine Learning - web input output,"<p>I am new to Azure ML and I am trying to pass a web input to my Scorer , and my R script returns a Web output. </p>

<p><a href=""https://i.stack.imgur.com/ULI1V.png"" rel=""nofollow noreferrer"">Screenshot - Design of my expertment</a></p>

<p>Input Parameters : No. of predictions to make - String type
Output Parameters :  Forecast - Data frame</p>

<p>For every input I pass , I get the same output.Can you help me out?</p>

<p>Thanks in advance. </p>

<p><strong>Solution</strong></p>

<p>Web service output generally shows only one value. So by default it fetches the first row element of the frame. In order to check the entire frame , use Batch execution or excel.</p>",1,5,2017-01-25 02:00:04.857000 UTC,,2017-01-26 20:39:02.860000 UTC,0,r|azure|azure-machine-learning-studio,535,2015-12-03 06:14:28.407000 UTC,2022-08-09 17:04:50.440000 UTC,"East Newark, NJ, United States",33,3,0,43,,,,,,['azure-machine-learning-studio']
How to deploy multiple ml models with scoring file using azure ml cli,"<p>I'm trying to deploy the Multiple azure ml models from workspace with scoring script file but i'm unable to deploy 2 models with azure ml cli</p>
<pre><code> az ml model deploy --name multi-model --model '[model1:9,model2:1]' --compute-target 'aks-cpu' --ic inferenceConfig.json -e 'inferen
ce-env' --ev 6 --dc aksDeploymentConfig.json -g 'Workspace' --workspace-name 'MLWorkspace' --as true --mi 1 --ma 2 --overwrite -v
</code></pre>
<p>But got error:
<strong>{'Azure-cli-ml Version': '1.41.0', 'Error': WebserviceException:
Message: ModelNotFound: Model with id [model1:9,model2:1] not found in provided workspace
InnerException None
ErrorResponse
{
&quot;error&quot;: {
&quot;message&quot;: &quot;ModelNotFound: Model with id [model1:9,model2:1] not found in provided workspace&quot;
}
}}</strong></p>
<p>But i'm able to deploy single model without issue
For multi-models, i am able to do with python without any issue i.e.,</p>
<pre><code>aks_service_name='modelsvc' 
aks_service = Model.deploy(ws,
models=[model1,model2],
inference_config=inference_config,
deployment_config=gpu_aks_config,
deployment_target=aks_target,
name=aks_service_name,overwrite=True)

aks_service.wait_for_deployment(show_output=True)
</code></pre>
<p>Can anyone provide insight on this?</p>",1,0,2022-08-05 12:47:31.057000 UTC,,2022-08-06 13:50:03.267000 UTC,0,azure|azure-machine-learning-studio|azure-machine-learning-service|azure-ml-pipelines,92,2018-05-30 12:15:06.413000 UTC,2022-09-25 03:34:32.717000 UTC,,1,0,0,1,,,,,,"['azure-machine-learning-studio', 'azure-machine-learning-service']"
AttributeError: Can't pickle local object 'train.<locals>.create_model',"<p>I am trying to use my own ML models for creating trainings job in aws Sagemaker. When I start training process everything goes well but at the end it says that &quot;AttributeError: Can't pickle local object 'train..create_model'&quot;. I am new into this job. I did the same things for mlp, knn, cart, and svr but never encountered with that issue. I know that lstm uses too much different things to create model but I can not figure out how to solve that issue.</p>
<p>Here is my train.py file where I get the error:</p>
<pre><code>from __future__ import print_function

import json
import os
import pickle
import sys
import traceback

import pandas as pd
import numpy as np
from pandas import DataFrame
from pandas import concat
from sklearn.preprocessing import MinMaxScaler
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM

# These are the paths to where SageMaker mounts interesting things in your container.

prefix = &quot;/opt/ml/&quot;

input_path = prefix + &quot;input/data&quot;
output_path = os.path.join(prefix, &quot;output&quot;)
model_path = os.path.join(prefix, &quot;model&quot;)

# This algorithm has a single channel of input data called 'training'. Since we run in
# File mode, the input files are copied to the directory specified here.
channel_name = &quot;training&quot;
training_path = os.path.join(input_path, channel_name)

# The function to execute training.
def train():
    print(&quot;Starting the training&quot;)
    
    print(training_path)
    
    try:
        # Take the set of files and read them all into a single pandas dataframe
        input_files = [ os.path.join(training_path, file) for file in os.listdir(training_path) ]
        if len(input_files) == 0:
            raise ValueError(('There are no files in {}.\n' +
                              'This usually indicates that the channel ({}) was incorrectly specified,\n' +
                              'the data specification in S3 was incorrectly specified or the role specified\n' +
                              'does not have permission to access the data.').format(training_path, channel_name))
        raw_data = [ pd.read_csv(file, header=0, index_col=0) for file in input_files ]
        data = pd.concat(raw_data)
        
        print(data)

        # convert series to supervised learning
        def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):
            n_vars = 1 if type(data) is list else data.shape[1]
            df = DataFrame(data)
            cols, names = list(), list()
            # input sequence (t-n, ... t-1)
            for i in range(n_in, 0, -1):
                cols.append(df.shift(i))
                names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]
            # forecast sequence (t, t+1, ... t+n)
            for i in range(0, n_out):
                cols.append(df.shift(-i))
                if i == 0:
                    names += [('var%d(t)' % (j+1)) for j in range(n_vars)]
                else:
                    names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]
            # put it all together
            agg = concat(cols, axis=1)
            agg.columns = names
            # drop rows with NaN values
            if dropnan:
                agg.dropna(inplace=True)
            return agg


        values = data.values
        # ensure all data is float
        values = values.astype('float32')
        # normalize features
        scaler = MinMaxScaler()
        scaled = scaler.fit_transform(values)


        # specify the number of lag time steps
        n_timesteps = 3
        n_features = 4
        # frame as supervised learning
        reframed = series_to_supervised(scaled, n_timesteps, 1)
        print(reframed.shape)


        # drop columns we don't want to predict
        reframed.drop(reframed.columns[[4,9,14,15,16,17,18]], axis=1, inplace=True)
        print(reframed.head())


        # split into train and test sets
        values = reframed.values
        n_train_size = 403
        train = values[:n_train_size, :]
        test = values[n_train_size:, :]
        # split into input and outputs
        n_obs = n_timesteps * n_features
        train_X, train_y = train[:, :n_obs], train[:, -1]
        test_X, test_y = test[:, :n_obs], test[:, -1]
        print(train_X.shape, len(train_X), train_y.shape)
        # reshape input to be 3D [samples, timesteps, features]
        train_X = train_X.reshape((train_X.shape[0], n_timesteps, n_features))
        test_X = test_X.reshape((test_X.shape[0], n_timesteps, n_features))
        print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)


        # Function to create model
        def create_model():
            # create model
            model = Sequential()
            model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))
            model.add(Dense(1))
            # Compile model
            # optimizer = SGD(lr=learn_rate, momentum=momentum)
            model.compile(loss='mae',optimizer='adam')
            return model


        from scikeras.wrappers import KerasRegressor
        # create model
        model = KerasRegressor(model=create_model, verbose=0)


        from sklearn.model_selection import GridSearchCV
        # define the grid search parameters
        batch_size = [2,4,8,16,32]
        epochs = [10, 50, 100]
        #learn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]
        #momentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]
        param_grid = dict(batch_size=batch_size, epochs=epochs)
        grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)
        grid_result = grid.fit(train_X, train_y)
        # summarize results
        print(&quot;Best: %f using %s&quot; % (grid_result.best_score_, grid_result.best_params_))
        means = grid_result.cv_results_['mean_test_score']
        stds = grid_result.cv_results_['std_test_score']
        params = grid_result.cv_results_['params']
        for mean, stdev, param in zip(means, stds, params):
            print(&quot;%f (%f) with: %r&quot; % (mean, stdev, param))

        # save the model
        with open(os.path.join(model_path, &quot;snop-lstm.pkl&quot;), &quot;wb&quot;) as out:
            pickle.dump(grid, out)
        print(&quot;Training complete.&quot;)
    except Exception as e:
        # Write out an error file. This will be returned as the failureReason in the
        # DescribeTrainingJob result.
        trc = traceback.format_exc()
        with open(os.path.join(output_path, &quot;failure&quot;), &quot;w&quot;) as s:
            s.write(&quot;Exception during training: &quot; + str(e) + &quot;\n&quot; + trc)
        # Printing this causes the exception to be in the training job logs, as well.
        print(&quot;Exception during training: &quot; + str(e) + &quot;\n&quot; + trc, file=sys.stderr)
        # A non-zero exit code causes the training job to be marked as Failed.
        sys.exit(255)
        
if __name__ == &quot;__main__&quot;:
    train()
    
    # A zero exit code causes the job to be marked a Succeeded.
    sys.exit(0)
</code></pre>
<p>And this is the log:</p>
<pre><code>2022-02-25T10:28:16.751+03:00
Exception during training: Can't pickle local object 'train.&lt;locals&gt;.create_model'
Exception during training: Can't pickle local object 'train.&lt;locals&gt;.create_model'

2022-02-25T10:28:16.751+03:00
Traceback (most recent call last):
  File &quot;/opt/program/train&quot;, line 154, in train
    pickle.dump(grid, out)
Traceback (most recent call last): File &quot;/opt/program/train&quot;, line 154, in train pickle.dump(grid, out)

2022-02-25T10:28:16.751+03:00
AttributeError: Can't pickle local object 'train.&lt;locals&gt;.create_model'
AttributeError: Can't pickle local object 'train.&lt;locals&gt;.create_model'
</code></pre>",1,0,2022-02-26 20:52:17.447000 UTC,,,0,amazon-web-services|machine-learning|lstm|pickle|amazon-sagemaker,522,2022-02-24 19:47:27.140000 UTC,2022-06-10 13:17:44.967000 UTC,,1,0,0,2,,,,,,['amazon-sagemaker']
Gridsearch on an experiment in Sacred,"<p>I'm trying to see some ways to store my ML experiments and I came across some python libraries like Sacred, ModelChimp, MLFlow, ....</p>

<p>The one I like the most is Sacred, but I would like to know how to save the <code>GridSearchCV</code> sklearn object the way ModelChimp does, for example. Is there any way to include each of the tests that the <code>GridSearchCV</code> object does in Sacred like ModelChimp does?</p>

<p>Additionally I would like to be able to visualize an interactive map of the folium library (which I would simply export to HTML), but I haven't seen that any of these libraries accept objects to visualize beyond an image.</p>

<p>Are Sacred or ModelChimp good options? The little I've seen of MLflow or other libraries hasn't convinced me either but I'm open to suggestions. <a href=""https://www.reddit.com/r/MachineLearning/comments/bx0apm/d_how_do_you_manage_your_machine_learning/"" rel=""nofollow noreferrer"">Here</a> are a few more alternatives. Which one do you use?</p>",0,0,2019-10-09 14:28:26.603000 UTC,,2019-10-14 15:55:02.090000 UTC,1,python|machine-learning|folium|mlflow|python-sacred,205,2019-02-15 12:18:22.177000 UTC,2022-09-22 08:26:53.537000 UTC,,621,87,26,103,,,,,,['mlflow']
MLFlow run passing Google Application credentials,"<p>I want to pass my <code>GOOGLE_APPLICATION_CREDENTIALS</code> environmental variable when I run <code>mlflow run</code> using a Docker container:</p>

<p>This is my current <code>docker run</code> when using mlflow run:</p>

<pre><code> Running command 'docker run --rm -e MLFLOW_RUN_ID=f18667e37ecb486cac4631cbaf279903 -e MLFLOW_TRACKING_URI=http://3.1.1.11:5000 -e MLFLOW_EXPERIMENT_ID=0 mlflow_gcp:33156ee python -m trainer.task --job-dir /tmp/ \
    --num-epochs 10 \
    --train-steps 1000 \
    --eval-steps 1 \
    --train-files gs://cloud-samples-data/ml-engine/census/data/adult.data.csv \
    --eval-files gs://cloud-samples-data/ml-engine/census/data/adult.test.csv \
    --batch-size 128
</code></pre>

<p>This is how I would normally pass it:</p>

<pre><code>docker run \
   -p 9090:${PORT} \
   -e PORT=${PORT} \
   -e GOOGLE_APPLICATION_CREDENTIALS=/tmp/keys/[FILE_NAME].json
</code></pre>

<p>What is the best way to option to pass this value to mlflow? I'm writing files in GCS and Docker requires access to GCP.</p>

<p>MLproject contents</p>

<pre><code>name: mlflow_gcp
docker_env:
  image: mlflow-gcp-example
entry_points:
  main:
    parameters:
      job_dir:
        type: string
        default: '/tmp/'
      num_epochs:
        type: int
        default: 10
      train_steps:
        type: int
        default: 1000
      eval_steps:
        type: int
        default: 1
      batch_size:
        type: int
        default: 64
      train_files:
        type: string
        default: 'gs://cloud-samples-data/ml-engine/census/data/adult.data.csv'
      eval_files:
        type: string
        default: 'gs://cloud-samples-data/ml-engine/census/data/adult.test.csv'
      mlflow_tracking_uri:
        type: uri
        default: ''

    command: |
        python -m trainer.task --job-dir {job_dir} \
            --num-epochs {num_epochs} \
            --train-steps {train_steps} \
            --eval-steps {eval_steps} \
            --train-files {train_files} \
            --eval-files {eval_files} \
            --batch-size {batch_size} \
            --mlflow-tracking-uri {mlflow_tracking_uri}

</code></pre>

<p>I already tried in Python file and fails since Docker has no access to local file system:</p>

<pre><code>import os
    os.environ[""GOOGLE_APPLICATION_CREDENTIALS""] = ""/Users/user/key.json""
</code></pre>",0,1,2019-12-19 00:25:18.127000 UTC,,2019-12-19 00:30:56.443000 UTC,2,docker|mlflow,243,2010-01-28 09:42:15.677000 UTC,2022-09-25 05:06:35.287000 UTC,"San Francisco, CA",8619,1916,102,1286,,,,,,['mlflow']
Import ONNX model to tensorflow-ValidationError: BatchNormalization.scale in initializer but not in graph input,"<p>I have downloaded ONNX model form CustomVision.ai and now I want to import into tensorflow and I am follwing ""<a href=""https://github.com/onnx/tutorials/blob/master/tutorials/OnnxTensorflowImport.ipynb"" rel=""nofollow noreferrer"">https://github.com/onnx/tutorials/blob/master/tutorials/OnnxTensorflowImport.ipynb</a>"" for guidance.</p>

<p>I have installed all the prerequisites as discussed in the above link. I am facing an error while executing ""tf_rep = prepare(model)""---ValidationError: BatchNormalization.scale in initializer but not in graph input</p>

<pre><code>import onnx
from onnx_tf.backend import prepare
model = onnx.load('C:\\Pankaj\\XYZ\\abc.onnx')
tf_rep = prepare(model)
</code></pre>

<p>Thank you for your help and time.</p>",0,0,2018-05-23 06:49:56.637000 UTC,,2018-05-23 07:02:51.817000 UTC,2,python-3.x|image-processing|tensorflow|artificial-intelligence|azure-machine-learning-studio,434,2012-05-09 16:13:36.970000 UTC,2022-04-09 20:22:01.520000 UTC,"Hyderabad, India",209,3,0,32,,,,,,['azure-machine-learning-studio']
"DVC(Data Version Control) keeps stuck at ""dvc add xxx"" with ""Collecting stages from the workspace"" in the terminal?","<p>I used : <code>dvc[webhdfs]==2.9.3</code>, installed by <code>pip install dvc[webhdfs]</code></p>
<p>Then the repo is already cloned by git.</p>
<p>I have also typed : <code>dvc remote add -d storage webhdfs://xxx/dvc</code> and <code>git add .dvc/config</code></p>
<p>But the command <code>dvc add ./assets/xxx/*</code> was still stuck...</p>
<p>The command line window keeps showing : <code>Collecting stages from the workspace</code></p>",0,2,2022-01-13 08:24:36.767000 UTC,,2022-01-13 10:11:54.663000 UTC,1,python|deployment|continuous-integration|dvc,115,2019-03-24 03:57:13.283000 UTC,2022-07-29 12:48:42.787000 UTC,Beijing,11,0,0,2,,,,,,['dvc']
Parameter validation failed:\nInvalid type for parameter Body in endpoint,"<p><strong>error</strong></p>
<pre><code> {
      &quot;errorMessage&quot;: &quot;Parameter validation failed:\nInvalid type for parameter Body, value: [[[[-0.4588235, -0.27058822, -0.44313723], [-0.4823529, -0.47450978, -0.6], [-0.7490196, -0.70980394, -0.75686276], 
    ....
    [0.18431377, 0.19215691, 0.15294123], [0.0196079, 0.02745104, -0.03529412], [-0.0745098, -0.05882353, -0.16862744], [-0.4823529, -0.5058824, -0.62352943], [-0.38039213, -0.372549, -0.4352941], [-0.4588235, -0.41960782, -0.47450978], [-0.56078434, -0.58431375, -0.62352943], [-0.4352941, -0.41960782, -0.4588235], [-0.40392154, -0.41176468, -0.45098037], [-0.30196077, -0.34117645, -0.372549], [-0.30196077, -0.29411763, -0.34117645], [-0.26274508, -0.2235294, -0.27843136], [0.00392163, -0.01176471, 0.09019613], [0.09019613, 0.09803927, -0.01176471], [0.06666672, 0.12941182, -0.05098039], [0.03529418, 0.09019613, -0.03529412], [0.09019613, 0.10588241, 0.00392163], [-0.01960784, -0.01176471, -0.05882353], [0.03529418, 0.04313731, -0.01960784], [0.13725495, 0.15294123, 0.06666672], [0.06666672, 0.07450986, 0.02745104], [0.16078436, 0.1686275, 0.20000005], [0.43529415, 0.52156866, 0.5686275], [0.5764706, 0.64705884, 0.7019608], [0.67058825, 0.7490196, 0.75686276], [0.5764706, 0.654902, 0.6627451], [0.5921569, 0.67058825, 0.6784314]]]], type: &lt;class 'list'&gt;, valid types: &lt;class 'bytes'&gt;, &lt;class 'bytearray'&gt;, file-like object&quot;,
      &quot;errorType&quot;: &quot;ParamValidationError&quot;,
      &quot;stackTrace&quot;: [
        [
          &quot;/var/task/lambda_function.py&quot;,
          16528,
          &quot;lambda_handler&quot;,
          &quot;[ 0.5921569 ,  0.67058825,  0.6784314 ]]]])&quot;
        ],
        [
          &quot;/var/runtime/botocore/client.py&quot;,
          357,
          &quot;_api_call&quot;,
          &quot;return self._make_api_call(operation_name, kwargs)&quot;
        ],
        [
          &quot;/var/runtime/botocore/client.py&quot;,
          649,
          &quot;_make_api_call&quot;,
          &quot;api_params, operation_model, context=request_context)&quot;
        ],
        [
          &quot;/var/runtime/botocore/client.py&quot;,
          697,
          &quot;_convert_to_request_dict&quot;,
          &quot;api_params, operation_model)&quot;
        ],
        [
          &quot;/var/runtime/botocore/validate.py&quot;,
          297,
          &quot;serialize_to_request&quot;,
          &quot;raise ParamValidationError(report=report.generate_report())&quot;
        ]
      ]
    }
</code></pre>
<p><strong>lambda code</strong></p>
<pre><code>import os
import io
import boto3
import JSON

ENDPOINT_NAME = &quot;tensorflow-training-2021-01-24-03-35-44-884&quot;
runtime= boto3.client('runtime.sagemaker')

def lambda_handler(event, context):
    print(&quot;Received event: &quot; + json.dumps(event, indent=2))
    
    data = json.loads(json.dumps(event))
    payload = data['data']
    print(payload)
    
    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME,
                                       Body=payload)
    print(response)
    result = json.loads(response['Body'].read().decode())
    print(result)

    return result[0]
</code></pre>
<p>I trained the model in sagemaker using TensorFlow</p>
<p><strong>Estimator part</strong></p>
<pre><code>pets_estimator = TensorFlow(
    entry_point='train.py',
    role=role,
    train_instance_type='ml.m5.large',
    train_instance_count=1,
    framework_version='2.1.0',
    py_version='py3',
    output_path='s3://imageclassificationtest202/data',
    sagemaker_session=sess
)
</code></pre>
<p>I don't know why but I can't send data using JSON for</p>
<pre><code>{
&quot;data&quot; : [[[[90494]...]]
}
</code></pre>
<p>My model simply accept NumPy array of dimension (1,128,128,3), and I m sending that data in JSON data field, but its saying invalid format, need byte or byte array</p>",2,2,2021-01-24 18:10:05.073000 UTC,,,1,amazon-sagemaker,849,2020-06-13 13:27:37.963000 UTC,2022-09-25 05:43:36.010000 UTC,,19,2,0,6,,,,,,['amazon-sagemaker']
Sagemaker endpoint invalid when create_monitoring_schedule is called on the endpoint,"<p>I am following this github <a href=""https://github.com/aws/amazon-sagemaker-examples/tree/master/sagemaker_model_monitor"" rel=""nofollow noreferrer"">repo</a>, adopting it to a text classification problem that is built on distil bert. So given a sting of text, the model should return a label and a (probability) score.
Output from the model:</p>
<pre><code>sentiment_input = {&quot;inputs&quot;: &quot;I love using the new Inference DLC.&quot;}

# sentiment_input= &quot;I love using the new Inference DLC.&quot;

response = predictor.predict(data=sentiment_input)
print(response)
</code></pre>
<p>Output:</p>
<blockquote>
<p>[{'label': 'LABEL_80', 'score': 0.008507220074534416}]</p>
</blockquote>
<p>When I run the following</p>
<pre><code># Create an enpointInput
endpointInput = EndpointInput(
    endpoint_name=predictor.endpoint_name,
    probability_attribute=&quot;score&quot;,
    inference_attribute=&quot;label&quot;,
#     probability_threshold_attribute=0.5,
    destination=&quot;/opt/ml/processing/input_data&quot;,
)

# Create the monitoring schedule to execute every hour.
from sagemaker.model_monitor import CronExpressionGenerator

response = clinc_intent0911.create_monitoring_schedule(
    monitor_schedule_name=clincintent_monitor_schedule_name,
    endpoint_input=endpointInput,
    output_s3_uri=baseline_results_uri,
    problem_type=&quot;MulticlassClassification&quot;,
    ground_truth_input=ground_truth_upload_path,
    constraints=baseline_job.suggested_constraints(),
    schedule_cron_expression=CronExpressionGenerator.hourly(),
    enable_cloudwatch_metrics=True,
)
</code></pre>
<p>I get the following error:</p>
<pre><code>---------------------------------------------------------------------------
ClientError                               Traceback (most recent call last)
&lt;ipython-input-269-72e7049246fb&gt; in &lt;module&gt;
     10     constraints=baseline_job.suggested_constraints(),
     11     schedule_cron_expression=CronExpressionGenerator.hourly(),
---&gt; 12     enable_cloudwatch_metrics=True,
     13 )

/opt/conda/lib/python3.6/site-packages/sagemaker/model_monitor/model_monitoring.py in create_monitoring_schedule(self, endpoint_input, ground_truth_input, problem_type, record_preprocessor_script, post_analytics_processor_script, output_s3_uri, constraints, monitor_schedule_name, schedule_cron_expression, enable_cloudwatch_metrics)
   2615             network_config=self.network_config,
   2616         )
-&gt; 2617         self.sagemaker_session.sagemaker_client.create_model_quality_job_definition(**request_dict)
   2618 
   2619         # create schedule

/opt/conda/lib/python3.6/site-packages/botocore/client.py in _api_call(self, *args, **kwargs)
    355                     &quot;%s() only accepts keyword arguments.&quot; % py_operation_name)
    356             # The &quot;self&quot; in this scope is referring to the BaseClient.
--&gt; 357             return self._make_api_call(operation_name, kwargs)
    358 
    359         _api_call.__name__ = str(py_operation_name)

/opt/conda/lib/python3.6/site-packages/botocore/client.py in _make_api_call(self, operation_name, api_params)
    674             error_code = parsed_response.get(&quot;Error&quot;, {}).get(&quot;Code&quot;)
    675             error_class = self.exceptions.from_code(error_code)
--&gt; 676             raise error_class(parsed_response, operation_name)
    677         else:
    678             return parsed_response

ClientError: An error occurred (ValidationException) when calling the CreateModelQualityJobDefinition operation: Endpoint 'clinc-intent-analysis-0911' does not exist or is not valid
</code></pre>
<p>At this point my sagemaker endpoint is live and unable to debug it is not valid.</p>",1,0,2021-09-27 12:55:59.567000 UTC,,,0,amazon-web-services|amazon-sagemaker,106,2018-12-20 13:04:14.090000 UTC,2022-09-23 06:56:23.127000 UTC,,170,113,2,33,,,,,,['amazon-sagemaker']
About processing job name when executing Sagameker Processing job with step function and CloudWatch Events,"<p>I'm currently considering a configuration where I have a statement from Step Functions that executes a Sagameker's Processing Job and call it in CloudWatch Events.</p>
<p>When Event was executed with the following statement, an error occurred because the maximum number of characters in ProcessingJobname was exceeded.
Execution of statement alone, not Event execution, works fine.</p>
<p>Could you please tell me how to avoid this?</p>
<blockquote>
<p>Value 'ID AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA' at 'processingJobName' failed to satisfy constraint: Member must have length less than or equal to 63; Value</p>
</blockquote>
<p>---------statement-------------------------------------</p>
<pre><code>{
  &quot;Comment&quot;: &quot;This is your state machine&quot;,
  &quot;StartAt&quot;: &quot;SageMaker CreateProcessingJob&quot;,

  &quot;States&quot;: {
    &quot;SageMaker CreateProcessingJob&quot;: {

      &quot;Type&quot;: &quot;Task&quot;,
      &quot;Resource&quot;: &quot;arn:aws:states:::sagemaker:createProcessingJob.sync&quot;,
      &quot;Parameters&quot;: {

        &quot;AppSpecification&quot;: {

          &quot;ImageUri&quot;: &quot;354813040037.dkr.ecr.ap-northeast-1.amazonaws.com/sagemaker-scikit-learn:0.20.0-cpu-py3&quot;,
          &quot;ContainerEntrypoint&quot;: [
            &quot;python3&quot;,
            &quot;/opt/ml/processing/input/code/transform.py&quot;
          ]

        },

        &quot;ProcessingResources&quot;: {
          &quot;ClusterConfig&quot;: {
            &quot;InstanceCount&quot;: 1,
            &quot;InstanceType&quot;: &quot;ml.t3.medium&quot;,
            &quot;VolumeSizeInGB&quot;: 10
          }
        },
        &quot;ProcessingInputs&quot;: [
          {
            &quot;InputName&quot;: &quot;input-1&quot;,
            &quot;S3Input&quot;: {
              &quot;S3Uri&quot;: &quot;s3://AAAAA/step_test/test_data.csv&quot;,
              &quot;LocalPath&quot;: &quot;/opt/ml/processing/input&quot;,
              &quot;S3DataType&quot;: &quot;S3Prefix&quot;,
              &quot;S3InputMode&quot;: &quot;File&quot;,
              &quot;S3DataDistributionType&quot;: &quot;FullyReplicated&quot;,
              &quot;S3CompressionType&quot;: &quot;None&quot;
            }
          },
          {
            &quot;InputName&quot;: &quot;code&quot;,
            &quot;S3Input&quot;: {
              &quot;S3Uri&quot;: &quot;s3://AAAAA/step_test/transform.py&quot;,
              &quot;LocalPath&quot;: &quot;/opt/ml/processing/input/code&quot;,
              &quot;S3DataType&quot;: &quot;S3Prefix&quot;,
              &quot;S3InputMode&quot;: &quot;File&quot;,
              &quot;S3DataDistributionType&quot;: &quot;FullyReplicated&quot;,
              &quot;S3CompressionType&quot;: &quot;None&quot;
            }
          }
        ],
        &quot;ProcessingOutputConfig&quot;: {
          &quot;Outputs&quot;: [
            {
              &quot;OutputName&quot;: &quot;train_data&quot;,
              &quot;S3Output&quot;: {
                &quot;S3Uri&quot;: &quot;s3://AAAAA/step_test/train&quot;,
                &quot;LocalPath&quot;: &quot;/opt/ml/processing/output/train&quot;,
                &quot;S3UploadMode&quot;: &quot;EndOfJob&quot;
              }
            }
          ]
        },
        &quot;RoleArn&quot;:  &quot;[role]&quot;,
        &quot;ProcessingJobName.$&quot;: &quot;$$.Execution.Name&quot;  #&lt;-I think that this is cause
      },
      &quot;End&quot;: true
    }
  }
}
</code></pre>",1,0,2021-08-17 20:34:05.517000 UTC,1.0,2021-08-30 17:25:18.927000 UTC,0,amazon-cloudwatch|amazon-sagemaker|aws-step-functions,465,2021-08-17 20:13:18.740000 UTC,2021-09-02 07:28:54.620000 UTC,,1,0,0,1,,,,,,['amazon-sagemaker']
Facing problem in azure ml pipeline creation,"<p>When creating the container image, it is showing below error in ACI instance.
Any help is appreciated.</p>
<pre><code>Exception in worker process
Traceback (most recent call last):
  File &quot;/opt/miniconda/lib/python3.6/site-packages/gunicorn/arbiter.py&quot;, line 557, in spawn_worker
    worker.init_process()
  File &quot;/opt/miniconda/lib/python3.6/site-packages/gunicorn/workers/base.py&quot;, line 126, in init_process
    self.load_wsgi()
  File &quot;/opt/miniconda/lib/python3.6/site-packages/gunicorn/workers/base.py&quot;, line 136, in load_wsgi
    self.wsgi = self.app.wsgi()
  File &quot;/opt/miniconda/lib/python3.6/site-packages/gunicorn/app/base.py&quot;, line 67, in wsgi
    self.callable = self.load()
  File &quot;/opt/miniconda/lib/python3.6/site-packages/gunicorn/app/wsgiapp.py&quot;, line 65, in load
    return self.load_wsgiapp()
  File &quot;/opt/miniconda/lib/python3.6/site-packages/gunicorn/app/wsgiapp.py&quot;, line 52, in load_wsgiapp
    return util.import_app(self.app_uri)
  File &quot;/opt/miniconda/lib/python3.6/site-packages/gunicorn/util.py&quot;, line 357, in import_app
    __import__(module)
  File &quot;/var/azureml-server/wsgi.py&quot;, line 1, in &lt;module&gt;
    import create_app
  File &quot;/var/azureml-server/create_app.py&quot;, line 3, in &lt;module&gt;
    from app import main
  File &quot;/var/azureml-server/app.py&quot;, line 31, in &lt;module&gt;
    import main as user_main
  File &quot;/var/azureml-app/main.py&quot;, line 12, in &lt;module&gt;
    driver_module_spec.loader.exec_module(driver_module)
  File &quot;/var/azureml-app/score.py&quot;, line 30, in &lt;module&gt;
    from azureml.core.model import Model
  File &quot;/opt/miniconda/lib/python3.6/site-packages/azureml/core/__init__.py&quot;, line 13, in &lt;module&gt;
    from .workspace import Workspace
  File &quot;/opt/miniconda/lib/python3.6/site-packages/azureml/core/workspace.py&quot;, line 34, in &lt;module&gt;
    from azureml.core.image import Image
  File &quot;/opt/miniconda/lib/python3.6/site-packages/azureml/core/image/__init__.py&quot;, line 28, in &lt;module&gt;
    from .image import Image
  File &quot;/opt/miniconda/lib/python3.6/site-packages/azureml/core/image/image.py&quot;, line 19, in &lt;module&gt;
    from azureml.core.model import Model
  File &quot;/opt/miniconda/lib/python3.6/site-packages/azureml/core/model.py&quot;, line 40, in &lt;module&gt;
    from azureml.core.environment import Environment
  File &quot;/opt/miniconda/lib/python3.6/site-packages/azureml/core/environment.py&quot;, line 31, in &lt;module&gt;
    from azureml.core.conda_dependencies import CondaDependencies, PYTHON_DEFAULT_VERSION
  File &quot;/opt/miniconda/lib/python3.6/site-packages/azureml/core/conda_dependencies.py&quot;, line 11, in 
import ruamel.yaml
ModuleNotFoundError: No module named 'ruamel'
</code></pre>
<p>The image is created without any error, but it is failing in the build process.</p>",1,2,2020-12-18 16:13:28.437000 UTC,,,-1,azure|azure-devops|azure-pipelines-release-pipeline|azure-machine-learning-service|azureml-python-sdk,208,2017-03-17 17:37:58.130000 UTC,2022-04-13 17:29:25.880000 UTC,"Chennai, Tamil Nadu, India",13,0,0,6,,,,,,['azure-machine-learning-service']
Control AWS Sagemaker costs,"<p>I want to use GPU capacity for deep learning models. Sagemaker is great in its flexibility of starting on demand clusters for training. However, my department wants to have guarantees we won't overspend on the AWS budget. Is there a way to 'cap' the costs without resorting to using a dedicated machine? </p>",1,0,2020-06-15 09:06:54.720000 UTC,,,2,amazon-web-services|amazon-sagemaker,933,2011-11-20 21:12:53.477000 UTC,2022-05-25 11:12:08.817000 UTC,"Hilversum, Netherlands",572,1,0,39,,,,,,['amazon-sagemaker']
Hyperopt failed to execute mlflow.end_run() with tracking URI: databricks,"<p>I'm using Azure Databricks + Hyperopt + MLflow for some hyperparameter tuning on a small dataset.  Seem like the job is running, and I get output in MLflow, but the job ends with the following error message:</p>

<pre><code>Hyperopt failed to execute mlflow.end_run() with tracking URI: databricks
</code></pre>

<p>Here is my code code with some information redacted:</p>

<pre><code>from pyspark.sql import SparkSession

# spark session initialization
spark = (SparkSession.builder.getOrCreate())
sc = spark.sparkContext

# Data Processing
import pandas as pd
import numpy as np
# Hyperparameter Tuning
from hyperopt import fmin, tpe, hp, anneal, Trials, space_eval, SparkTrials, STATUS_OK
from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score
# Modeling
from sklearn.ensemble import RandomForestClassifier
# cleaning
import gc
# tracking
import mlflow
# track runtime
from datetime import date, datetime

mlflow.set_experiment('/user/myname/myexp')
# notebook settings \ variable settings
n_splits = #
n_repeats = #
max_evals = #

dfL = pd.read_csv(""/my/data/loc/mydata.csv"")

x_train = dfL[['f1','f2','f3']]
y_train = dfL['target']

def define_model(params):
    model = RandomForestClassifier(n_estimators=int(params['n_estimators']),
                                   criterion=params['criterion'], 
                                   max_depth=int(params['max_depth']), 
                                   min_samples_split=params['min_samples_split'], 
                                   min_samples_leaf=params['min_samples_leaf'], 
                                   min_weight_fraction_leaf=params['min_weight_fraction_leaf'], 
                                   max_features=params['max_features'], 
                                   max_leaf_nodes=None, 
                                   min_impurity_decrease=params['min_impurity_decrease'], 
                                   min_impurity_split=None, 
                                   bootstrap=params['bootstrap'], 
                                   oob_score=False, 
                                   n_jobs=-1, 
                                   random_state=int(params['random_state']), 
                                   verbose=0, 
                                   warm_start=False, 
                                   class_weight={0:params['class_0_weight'], 1:params['class_1_weight']})
        return model


space = {'n_estimators': hp.quniform('n_estimators', #, #, #),
         'criterion': hp.choice('#', ['#','#']),
         'max_depth': hp.quniform('max_depth', #, #, #),
         'min_samples_split': hp.quniform('min_samples_split', #, #, #),
         'min_samples_leaf': hp.quniform('min_samples_leaf', #, #, #),
         'min_weight_fraction_leaf': hp.quniform('min_weight_fraction_leaf', #, #, #),
         'max_features': hp.quniform('max_features', #, #, #),
         'min_impurity_decrease': hp.quniform('min_impurity_decrease', #, #, #),
         'bootstrap': hp.choice('bootstrap', [#,#]),
         'random_state': hp.quniform('random_state', #, #, #),
         'class_0_weight': hp.choice('class_0_weight', [#,#,#]),
         'class_1_weight': hp.choice('class_1_weight', [#,#,#])}

# define hyperopt objective
def objective(params, n_splits=n_splits, n_repeats=n_repeats):

    # define model
    model = define_model(params)
    # get cv splits
    kfold = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=1331)
    # define and run sklearn cv scorer
    scores = cross_val_score(model, x_train, y_train, cv=kfold, scoring='roc_auc')
    score = scores.mean()

    return {'loss': score*(-1), 'status': STATUS_OK}

spark_trials = SparkTrials(parallelism=36, spark_session=spark)
with mlflow.start_run():
  best = fmin(objective, space, algo=tpe.suggest, trials=spark_trials, max_evals=max_evals)
</code></pre>

<p>and then at the end I get..</p>

<pre><code>100%|██████████| 200/200 [1:35:28&lt;00:00, 100.49s/trial, best loss: -0.9584565527065526]

Hyperopt failed to execute mlflow.end_run() with tracking URI: databricks

Exception: 'MLFLOW_RUN_ID'

Total Trials: 200: 200 succeeded, 0 failed, 0 cancelled.
</code></pre>

<p>My Azure Databricks cluster is..</p>

<pre><code>6.6 ML (includes Apache Spark 2.4.5, Scala 2.11)
Standard_DS3_v2
min 9 max 18 nodes
</code></pre>

<p>Am I doing something wrong or is this a bug?</p>",1,4,2020-06-02 20:19:55.507000 UTC,,,2,pyspark|databricks|azure-databricks|mlflow|hyperopt,604,2018-06-10 03:57:32.363000 UTC,2022-09-23 22:17:15.213000 UTC,,247,637,0,53,,,,,,['mlflow']
Is there a way to create a graph comparing hyper-parameters vs model accuracy with TRAINS python package?,"<p>I would like to run multiple experiments, then report model accuracy per experiment.</p>

<p>I'm training a toy MNIST example with pytorch (v1.1.0), but the goal is, once I can compare performance for the toy problem, to have it integrated with the actual code base.</p>

<p>As I understand the TRAINS python package, with the ""two lines of code"" all my hyper-parameters are already logged (Command line argparse in my case). </p>

<p>What do I need to do in order to report a final scalar and then be able to sort through all the different training experiments (w/ hyper-parameters) in order to find the best one.</p>

<p>What I'd like to get, is a graph/s where on the X-axis I have hyper-parameter values and on the Y-axis I have the validation accuracy.</p>",1,0,2019-06-24 21:54:32.967000 UTC,,2021-01-05 15:40:56.560000 UTC,1,python|deep-learning|pytorch|trains|clearml,261,2012-07-21 15:18:16.543000 UTC,2019-12-26 20:10:54.037000 UTC,,81,6,0,18,,,,,,['clearml']
How to save/restore weights of a Sagemaker Estimator object,"<p>I have been trying to save weights of a Sagemaker Estimator object after a &quot;fit&quot; method. But apparently the estimator objects do not have a &quot;save_weights&quot; method like for a tensorflow model. Is there a way where we can save the weights of the said Sagemaker Estimator object in the &quot;.h5&quot; format?</p>
<p>I have seen the answer to this <a href=""https://stackoverflow.com/questions/51169661/get-weights-after-training-from-sagemaker-estimator"">Get weights after training from Sagemaker Estimator</a>, but I was not successful in getting what I wanted.</p>",0,0,2022-09-22 05:52:16.147000 UTC,,,0,amazon-web-services|amazon-s3|amazon-sagemaker|tensorflow-estimator,14,2022-04-25 08:20:14.367000 UTC,2022-09-22 11:46:27.527000 UTC,,49,3,0,8,,,,,,['amazon-sagemaker']
Serving models from mlflow registry to sagemaker,"<p>I have an mlflow server running locally and being exposed at port 80. I also have a model in the mlflow registry and I want to deploy it using the <code>mlflow sagemaker run-local</code> because after testing this locally, I am going to deploy everything to AWS and Sagemaker. My problem is that when I run:</p>
<pre><code>export MODEL_PATH=models:/churn-lgb-test/2
export LOCAL_PORT=8000
mlflow sagemaker run-local -m $MODEL_PATH -p $LOCAL_PORT -f python_function -i splicemachine/mlflow-pyfunc:1.6.0
</code></pre>
<p>it starts the container and I immediately get this error:</p>
<pre><code>2020-07-27 13:02:13 +0000] [827] [ERROR] Exception in worker process
Traceback (most recent call last):
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/arbiter.py&quot;, line 583, in spawn_worker
    worker.init_process()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/workers/ggevent.py&quot;, line 162, in init_process
    super().init_process()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/workers/base.py&quot;, line 119, in init_process
    self.load_wsgi()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/workers/base.py&quot;, line 144, in load_wsgi
    self.wsgi = self.app.wsgi()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/app/base.py&quot;, line 67, in wsgi
    self.callable = self.load()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/app/wsgiapp.py&quot;, line 49, in load
    return self.load_wsgiapp()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/app/wsgiapp.py&quot;, line 39, in load_wsgiapp
    return util.import_app(self.app_uri)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/util.py&quot;, line 358, in import_app
    mod = importlib.import_module(module)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/importlib/__init__.py&quot;, line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1006, in _gcd_import
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 983, in _find_and_load
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 967, in _find_and_load_unlocked
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 677, in _load_unlocked
  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 728, in exec_module
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/models/container/scoring_server/wsgi.py&quot;, line 3, in &lt;module&gt;
    app = scoring_server.init(pyfunc.load_model(&quot;/opt/ml/model/&quot;))
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/pyfunc/__init__.py&quot;, line 292, in load_model
    return importlib.import_module(conf[MAIN])._load_pyfunc(data_path)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/sklearn.py&quot;, line 219, in _load_pyfunc
    return _load_model_from_local_file(path)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/sklearn.py&quot;, line 206, in _load_model_from_local_file
    with open(path, &quot;rb&quot;) as f:
IsADirectoryError: [Errno 21] Is a directory: '/opt/ml/model'
[2020-07-27 13:02:13 +0000] [828] [ERROR] Exception in worker process
Traceback (most recent call last):
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/arbiter.py&quot;, line 583, in spawn_worker
    worker.init_process()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/workers/ggevent.py&quot;, line 162, in init_process
    super().init_process()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/workers/base.py&quot;, line 119, in init_process
    self.load_wsgi()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/workers/base.py&quot;, line 144, in load_wsgi
    self.wsgi = self.app.wsgi()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/app/base.py&quot;, line 67, in wsgi
    self.callable = self.load()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/app/wsgiapp.py&quot;, line 49, in load
    return self.load_wsgiapp()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/app/wsgiapp.py&quot;, line 39, in load_wsgiapp
    return util.import_app(self.app_uri)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/util.py&quot;, line 358, in import_app
    mod = importlib.import_module(module)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/importlib/__init__.py&quot;, line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1006, in _gcd_import
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 983, in _find_and_load
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 967, in _find_and_load_unlocked
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 677, in _load_unlocked
  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 728, in exec_module
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/models/container/scoring_server/wsgi.py&quot;, line 3, in &lt;module&gt;
    app = scoring_server.init(pyfunc.load_model(&quot;/opt/ml/model/&quot;))
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/pyfunc/__init__.py&quot;, line 292, in load_model
    return importlib.import_module(conf[MAIN])._load_pyfunc(data_path)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/sklearn.py&quot;, line 219, in _load_pyfunc
    return _load_model_from_local_file(path)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/sklearn.py&quot;, line 206, in _load_model_from_local_file
    with open(path, &quot;rb&quot;) as f:
IsADirectoryError: [Errno 21] Is a directory: '/opt/ml/model'
[2020-07-27 13:02:13 +0000] [828] [INFO] Worker exiting (pid: 828)
[2020-07-27 13:02:13 +0000] [827] [INFO] Worker exiting (pid: 827)
[2020-07-27 13:02:13 +0000] [829] [ERROR] Exception in worker process
Traceback (most recent call last):
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/arbiter.py&quot;, line 583, in spawn_worker
    worker.init_process()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/workers/ggevent.py&quot;, line 162, in init_process
    super().init_process()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/workers/base.py&quot;, line 119, in init_process
    self.load_wsgi()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/workers/base.py&quot;, line 144, in load_wsgi
    self.wsgi = self.app.wsgi()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/app/base.py&quot;, line 67, in wsgi
    self.callable = self.load()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/app/wsgiapp.py&quot;, line 49, in load
    return self.load_wsgiapp()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/app/wsgiapp.py&quot;, line 39, in load_wsgiapp
    return util.import_app(self.app_uri)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/util.py&quot;, line 358, in import_app
    mod = importlib.import_module(module)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/importlib/__init__.py&quot;, line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1006, in _gcd_import
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 983, in _find_and_load
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 967, in _find_and_load_unlocked
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 677, in _load_unlocked
  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 728, in exec_module
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/models/container/scoring_server/wsgi.py&quot;, line 3, in &lt;module&gt;
    app = scoring_server.init(pyfunc.load_model(&quot;/opt/ml/model/&quot;))
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/pyfunc/__init__.py&quot;, line 292, in load_model
    return importlib.import_module(conf[MAIN])._load_pyfunc(data_path)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/sklearn.py&quot;, line 219, in _load_pyfunc
    return _load_model_from_local_file(path)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/sklearn.py&quot;, line 206, in _load_model_from_local_file
    with open(path, &quot;rb&quot;) as f:
IsADirectoryError: [Errno 21] Is a directory: '/opt/ml/model'
[2020-07-27 13:02:13 +0000] [829] [INFO] Worker exiting (pid: 829)
[2020-07-27 13:02:13 +0000] [830] [ERROR] Exception in worker process
Traceback (most recent call last):
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/arbiter.py&quot;, line 583, in spawn_worker
    worker.init_process()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/workers/ggevent.py&quot;, line 162, in init_process
    super().init_process()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/workers/base.py&quot;, line 119, in init_process
    self.load_wsgi()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/workers/base.py&quot;, line 144, in load_wsgi
    self.wsgi = self.app.wsgi()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/app/base.py&quot;, line 67, in wsgi
    self.callable = self.load()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/app/wsgiapp.py&quot;, line 49, in load
    return self.load_wsgiapp()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/app/wsgiapp.py&quot;, line 39, in load_wsgiapp
    return util.import_app(self.app_uri)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/util.py&quot;, line 358, in import_app
    mod = importlib.import_module(module)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/importlib/__init__.py&quot;, line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1006, in _gcd_import
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 983, in _find_and_load
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 967, in _find_and_load_unlocked
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 677, in _load_unlocked
  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 728, in exec_module
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/models/container/scoring_server/wsgi.py&quot;, line 3, in &lt;module&gt;
    app = scoring_server.init(pyfunc.load_model(&quot;/opt/ml/model/&quot;))
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/pyfunc/__init__.py&quot;, line 292, in load_model
    return importlib.import_module(conf[MAIN])._load_pyfunc(data_path)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/sklearn.py&quot;, line 219, in _load_pyfunc
    return _load_model_from_local_file(path)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/sklearn.py&quot;, line 206, in _load_model_from_local_file
    with open(path, &quot;rb&quot;) as f:
IsADirectoryError: [Errno 21] Is a directory: '/opt/ml/model'
[2020-07-27 13:02:13 +0000] [830] [INFO] Worker exiting (pid: 830)
Traceback (most recent call last):
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/arbiter.py&quot;, line 209, in run
    self.sleep()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/arbiter.py&quot;, line 357, in sleep
    ready = select.select([self.PIPE[0]], [], [], 1.0)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/arbiter.py&quot;, line 242, in handle_chld
    self.reap_workers()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/arbiter.py&quot;, line 525, in reap_workers
    raise HaltServer(reason, self.WORKER_BOOT_ERROR)
gunicorn.errors.HaltServer: &lt;HaltServer 'Worker failed to boot.' 3&gt;

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;/miniconda/envs/custom_env/bin/gunicorn&quot;, line 8, in &lt;module&gt;
    sys.exit(run())
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/app/wsgiapp.py&quot;, line 58, in run
    WSGIApplication(&quot;%(prog)s [OPTIONS] [APP_MODULE]&quot;).run()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/app/base.py&quot;, line 228, in run
    super().run()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/app/base.py&quot;, line 72, in run
    Arbiter(self).run()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/arbiter.py&quot;, line 229, in run
    self.halt(reason=inst.reason, exit_status=inst.exit_status)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/arbiter.py&quot;, line 342, in halt
    self.stop()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/arbiter.py&quot;, line 393, in stop
    time.sleep(0.1)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/arbiter.py&quot;, line 242, in handle_chld
    self.reap_workers()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/arbiter.py&quot;, line 525, in reap_workers
    raise HaltServer(reason, self.WORKER_BOOT_ERROR)
gunicorn.errors.HaltServer: &lt;HaltServer 'Worker failed to boot.' 3&gt;
creating and activating custom environment
Got sigterm signal, exiting.
[2020-07-27 13:02:13 +0000] [831] [ERROR] Exception in worker process
Traceback (most recent call last):
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/arbiter.py&quot;, line 583, in spawn_worker
    worker.init_process()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/workers/ggevent.py&quot;, line 162, in init_process
    super().init_process()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/workers/base.py&quot;, line 119, in init_process
    self.load_wsgi()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/workers/base.py&quot;, line 144, in load_wsgi
    self.wsgi = self.app.wsgi()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/app/base.py&quot;, line 67, in wsgi
    self.callable = self.load()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/app/wsgiapp.py&quot;, line 49, in load
    return self.load_wsgiapp()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/app/wsgiapp.py&quot;, line 39, in load_wsgiapp
    return util.import_app(self.app_uri)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/util.py&quot;, line 358, in import_app
    mod = importlib.import_module(module)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/importlib/__init__.py&quot;, line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1006, in _gcd_import
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 983, in _find_and_load
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 967, in _find_and_load_unlocked
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 677, in _load_unlocked
  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 728, in exec_module
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/models/container/scoring_server/wsgi.py&quot;, line 3, in &lt;module&gt;
    app = scoring_server.init(pyfunc.load_model(&quot;/opt/ml/model/&quot;))
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/pyfunc/__init__.py&quot;, line 292, in load_model
    return importlib.import_module(conf[MAIN])._load_pyfunc(data_path)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/sklearn.py&quot;, line 219, in _load_pyfunc
    return _load_model_from_local_file(path)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/sklearn.py&quot;, line 206, in _load_model_from_local_file
    with open(path, &quot;rb&quot;) as f:
IsADirectoryError: [Errno 21] Is a directory: '/opt/ml/model'
[2020-07-27 13:02:13 +0000] [831] [INFO] Worker exiting (pid: 831)
[2020-07-27 13:02:14 +0000] [833] [ERROR] Exception in worker process
Traceback (most recent call last):
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/arbiter.py&quot;, line 583, in spawn_worker
    worker.init_process()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/workers/ggevent.py&quot;, line 162, in init_process
    super().init_process()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/workers/base.py&quot;, line 119, in init_process
    self.load_wsgi()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/workers/base.py&quot;, line 144, in load_wsgi
    self.wsgi = self.app.wsgi()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/app/base.py&quot;, line 67, in wsgi
    self.callable = self.load()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/app/wsgiapp.py&quot;, line 49, in load
    return self.load_wsgiapp()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/app/wsgiapp.py&quot;, line 39, in load_wsgiapp
    return util.import_app(self.app_uri)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/util.py&quot;, line 358, in import_app
    mod = importlib.import_module(module)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/importlib/__init__.py&quot;, line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1006, in _gcd_import
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 983, in _find_and_load
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 967, in _find_and_load_unlocked
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 677, in _load_unlocked
  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 728, in exec_module
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/models/container/scoring_server/wsgi.py&quot;, line 3, in &lt;module&gt;
    app = scoring_server.init(pyfunc.load_model(&quot;/opt/ml/model/&quot;))
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/pyfunc/__init__.py&quot;, line 292, in load_model
    return importlib.import_module(conf[MAIN])._load_pyfunc(data_path)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/sklearn.py&quot;, line 219, in _load_pyfunc
    return _load_model_from_local_file(path)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/sklearn.py&quot;, line 206, in _load_model_from_local_file
    with open(path, &quot;rb&quot;) as f:
IsADirectoryError: [Errno 21] Is a directory: '/opt/ml/model'
[2020-07-27 13:02:14 +0000] [833] [INFO] Worker exiting (pid: 833)
[2020-07-27 13:02:14 +0000] [832] [ERROR] Exception in worker process
Traceback (most recent call last):
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/arbiter.py&quot;, line 583, in spawn_worker
    worker.init_process()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/workers/ggevent.py&quot;, line 162, in init_process
    super().init_process()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/workers/base.py&quot;, line 119, in init_process
    self.load_wsgi()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/workers/base.py&quot;, line 144, in load_wsgi
    self.wsgi = self.app.wsgi()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/app/base.py&quot;, line 67, in wsgi
    self.callable = self.load()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/app/wsgiapp.py&quot;, line 49, in load
    return self.load_wsgiapp()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/app/wsgiapp.py&quot;, line 39, in load_wsgiapp
    return util.import_app(self.app_uri)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/util.py&quot;, line 358, in import_app
    mod = importlib.import_module(module)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/importlib/__init__.py&quot;, line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1006, in _gcd_import
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 983, in _find_and_load
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 967, in _find_and_load_unlocked
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 677, in _load_unlocked
  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 728, in exec_module
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/models/container/scoring_server/wsgi.py&quot;, line 3, in &lt;module&gt;
    app = scoring_server.init(pyfunc.load_model(&quot;/opt/ml/model/&quot;))
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/pyfunc/__init__.py&quot;, line 292, in load_model
    return importlib.import_module(conf[MAIN])._load_pyfunc(data_path)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/sklearn.py&quot;, line 219, in _load_pyfunc
    return _load_model_from_local_file(path)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/sklearn.py&quot;, line 206, in _load_model_from_local_file
    with open(path, &quot;rb&quot;) as f:
IsADirectoryError: [Errno 21] Is a directory: '/opt/ml/model'
[2020-07-27 13:02:14 +0000] [832] [INFO] Worker exiting (pid: 832)
</code></pre>",1,1,2020-07-27 13:26:19.060000 UTC,,,2,amazon-sagemaker|mlflow,429,2020-06-05 19:27:43.857000 UTC,2020-11-19 23:45:52.310000 UTC,,21,0,0,1,,,,,,"['mlflow', 'amazon-sagemaker']"
How to make inference with Huggingface deep learning container from Lambda using Serverless framework,"<p>This is a question from ML newbee :-)</p>
<p>I am building AWS StepFunction with Serverless framework and one of the steps is intended to deploy a Sagemaker endpoint with HuggingFace deep learning container (DLC).</p>
<p>The problem is that I could not make Lambda to work with SageMaker (to build estimator).</p>
<p>One of the solutions I have is to manually launch the endpoint using the SageMaker studio, but I really want to have everything within the code.</p>
<p>Here is what I am trying to do to get Sagemaker working</p>
<pre class=""lang-py prettyprint-override""><code>def installPack(package):
    import subprocess
    import sys
    subprocess.check_call([sys.executable, &quot;-m&quot;, &quot;pip&quot;, &quot;install&quot;, package])

installPack('sagemaker')
from sagemaker.huggingface import HuggingFaceModel
import sagemaker 

role = sagemaker.get_execution_role()

# Hub Model configuration. https://huggingface.co/models
hub = {
        'HF_MODEL_ID':'distilbert-base-uncased-distilled-squad', # model_id from hf.co/models
        'HF_TASK':'question-answering' # NLP task you want to use for predictions
        }

# create Hugging Face Model Class
huggingface_model = sagemaker.HuggingFaceModel(
            env=hub,
            role=role, # iam role with permissions to create an Endpoint
            transformers_version=&quot;4.6&quot;, # transformers version used
            pytorch_version=&quot;1.7&quot;, # pytorch version used
            py_version=&quot;py36&quot;, # python version of the DLC

..........

</code></pre>
<p>The error I'm getting is</p>
<pre><code>WARNING: The directory '/home/sbx_user1051/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.
</code></pre>
<p><em>(... then there are many lines of log like Collecting pyparsing&gt;=2.0.2
Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)...</em></p>
<pre><code>Downloading pox-0.3.0-py2.py3-none-any.whl (30 kB)
Collecting multiprocess&gt;=0.70.12
Downloading multiprocess-0.70.12.2-py38-none-any.whl (128 kB)
Using legacy 'setup.py install' for sagemaker, since package 'wheel' is not installed.
Using legacy 'setup.py install' for protobuf3-to-dict, since package 'wheel' is not installed.
Installing collected packages: dill, zipp, pytz, pyparsing, protobuf, ppft, pox, numpy, multiprocess, smdebug-rulesconfig, protobuf3-to-dict, pathos, pandas, packaging, importlib-metadata, google-pasta, attrs, sagemaker
ERROR: Could not install packages due to an OSError: [Errno 30] Read-only file system: '/home/sbx_user1051'
</code></pre>",1,0,2021-09-08 12:55:12.503000 UTC,,2021-09-08 13:38:05.067000 UTC,1,python|aws-lambda|serverless-framework|amazon-sagemaker|huggingface-transformers,174,2016-07-08 09:30:22.743000 UTC,2022-08-19 15:33:32.697000 UTC,"Litija, Slovenia",19,0,0,1,,,,,,['amazon-sagemaker']
Sagemaker Inference with Redis,"<p>I have a custom inference container on Sagemaker that runs a Flask API to handle the incoming calls. Around this, I have another API with a Lambda that calls the respective Sagemaker endpoint. The underlying model is generating vector embeddings for incoming sentences.</p>
<p>Now, I would like to enable caching and store already computed vectors with Redis.</p>
<p>My question: Does it make more sense to enable Redis on the inference container or in the Lambda API wrapped around the endpoint?</p>",1,1,2022-01-18 10:22:23.780000 UTC,,,0,amazon-web-services|redis|amazon-sagemaker,109,2014-03-13 09:33:37.223000 UTC,2022-09-15 12:15:05.390000 UTC,"Cologne, Germany",486,27,0,26,,,,,,['amazon-sagemaker']
Installing Pytorch Transformers in AWS Sagemaker,"<p>I'm trying to install the pytorch transformers package for my AWS Sagemaker notebook instance. However, it keeps giving me error of ""No Module Found"" for the package when i run my entry point script. </p>

<p>I saw in an example for TensorFlowModel which requires to set up env but for Pytorch it is not the case (<a href=""https://stackoverflow.com/questions/49665241/how-do-i-load-python-modules-which-are-not-available-in-sagemaker/49676109"">How do I load python modules which are not available in Sagemaker?</a>). Anyway, below is my code :</p>

<pre><code>estimator = PyTorch(entry_point='model.py',
                role=role,
                framework_version='1.4.0',
                train_instance_count=2,
                train_instance_type='ml.c4.xlarge',
                source_dir = 'src',
                hyperparameters={
                    'train_path': 's3://bucket-train',
                    'validation_path': 's3://bucket-val',
                    'epochs': 3,
                    'backend': 'gloo'
                })
</code></pre>",2,0,2020-04-24 18:49:26.340000 UTC,,,0,pytorch|amazon-sagemaker,731,2019-06-11 21:32:48.320000 UTC,2021-03-19 18:35:21.363000 UTC,,11,0,0,17,,,,,,['amazon-sagemaker']
How do I improve Azure ML learning performance?,"<p>Training a simple convolutional network to recognize MNIST digits on Microsoft Azure (in Machine Learning Studio) takes many many times longer than it does for (already very slow) learning of exactly the same model locally, on a CPU (MacBook Pro, with limited memory) with TensorFlow.</p>

<p>Is there a way — perhaps purchasing resources or connecting virtual GPUs — to improve performance of Azure Machine Learning?</p>",1,0,2016-05-12 19:59:41.150000 UTC,1.0,,1,performance|machine-learning|cloud|azure-machine-learning-studio,1048,2011-03-12 19:54:30.313000 UTC,2022-09-21 19:06:38.420000 UTC,United States,41475,1198,107,1912,,,,,,['azure-machine-learning-studio']
How to get output from a web service from a python script in Azure ML,"<p>I'm new to Azure ML and I'm trying to implement a python script in Azure ML. I'm trying to deploy the web services, but I'm getting only a string as output.</p>

<p>When I run the python script alone, I'm getting the result, but when implemented in Web service, i'm only getting a statement saying ""Execution ok""  . Please let me know, how to go about it. </p>

<pre><code>The output returns a Json format.
</code></pre>

<p>Following is my output from Python script:</p>

<pre><code>User Patterns
[{""Jane"": [{""Thermostat"": 20, ""Days"": [1, 2], ""Hour"": 6, ""Minute"": 43}], 
""John"": [{""Thermostat"": 18, ""Days"": [1, 2], ""Hour"": 0, ""Minute"": 15}], 
""Jen"": [{""Thermostat"": 22, ""Days"": [1, 2], ""Hour"": 10, ""Minute"": 1}]}]

Missed Patterns
[{""Jane"": [], ""John"": [], ""Jen"": []}]
Patternsssssssssss [{""Jane"": [{""Thermostat"": 20, ""Days"": [1, 2], ""Hour"": 6, 
""Minute"": 43}], ""John"": [{""Thermostat"": 18, ""Days"": [1, 2], ""Hour"": 0, 
""Minute"": 15}], ""Jen"": [{""Thermostat"": 22, ""Days"": [1, 2], ""Hour"": 10, 
""Minute"": 1}]}]
</code></pre>",1,1,2017-01-10 07:11:49.870000 UTC,,2017-01-10 09:59:06.570000 UTC,1,python|json|web-services|azure|azure-machine-learning-studio,392,2016-11-15 07:23:47.133000 UTC,2018-07-12 09:45:50.387000 UTC,,2713,26,2,358,,,,,,['azure-machine-learning-studio']
comparing the output of Automl,"<p>im getting different output for feature importance, when I run the automl in azure, google and h2o. even though the data is same and all the features are also same. what would be the reason for it.
is there any other method to compare the models</p>",1,0,2022-07-27 11:51:28.803000 UTC,,2022-07-27 13:17:59.200000 UTC,-1,azure|h2o|automl|google-cloud-vertex-ai,23,2022-07-27 10:36:51.743000 UTC,2022-07-27 11:46:31.297000 UTC,,1,0,0,0,,,,,,['google-cloud-vertex-ai']
Azure Machine Learning + R: Reading files,"<p>I am trying to run my R-code in the Azure Machine Learning environment, and I am running into a dead end trying to import data from the blob storage.</p>

<p>Locally I can easily import a file like this:</p>

<pre><code>data &lt;- read.delim(""myfile.xls"", sep = ""\t"", skip = 9)
</code></pre>

<p>When I am using Azure machine learning, just referring to file in the blob storage location, I get the error ""cannot open the connection"".</p>

<pre><code>data &lt;- read.delim(""https://knnstorage.blob.core.windows.net/knn/myfile.xls"", sep = ""\t"", skip = 9)
</code></pre>

<p>I have additionally attempted to import the file using the ""Import Data"" option, but it only allows one to import csv files or excel-files with at the most one line to be skipped.</p>

<p>In the future I will also need to import unstructured text into Azure Machine Learning. Is this really not possible using R?</p>",0,3,2017-05-29 14:47:49.853000 UTC,0.0,,3,r|import|azure-machine-learning-studio,359,2012-09-06 13:58:20.920000 UTC,2022-09-22 13:16:58.247000 UTC,,2545,339,16,381,,,,,,['azure-machine-learning-studio']
Amazon sagemaker Lifecycle configuration not working,"<p>I have the following Lifecycle configuration file in Amazon sagemaker</p>
<pre class=""lang-bash prettyprint-override""><code>#!/bin/bash
sudo -u ec2-user -i &lt;&lt;'EOF'

source activate conda_pytorch_p36

# Replace myPackage with the name of the package you want to install.
conda install -c pytorch torchtext
# You can also perform &quot;conda install&quot; here as well.

source deactivate

EOF
</code></pre>
<p>But when I try to import torchtext on a jupyter notebook inside the pytorch_conda_p36 virtual environment I get a module not found error</p>
<p>This is my code</p>
<pre class=""lang-python prettyprint-override""><code>from sagemaker import get_execution_role

role = get_execution_role()
bucket='sagemaker-us-east-2-964130302244'
data_dir = 'dataset'
data_location = 's3://{}/{}'.format(bucket, data_dir)

import torch
import torch.utils.data as tud
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from collections import Counter, defaultdict
import operator
import os, math
import numpy as np
import random
import copy
import s3fs
import torchtext
fs = s3fs.S3FileSystem()
</code></pre>
<p>The error I get is the following</p>
<pre class=""lang-python prettyprint-override""><code>ModuleNotFoundError                       Traceback (most recent call last)
&lt;ipython-input-2-c9a96a48b0b7&gt; in &lt;module&gt;
     11 import copy
     12 import s3fs
---&gt; 13 import torchtext
     14 fs = s3fs.S3FileSystem()
     15 # from nltk import word_tokenize

ModuleNotFoundError: No module named 'torchtext'
</code></pre>",0,1,2020-06-23 12:01:42.137000 UTC,,2020-06-23 13:05:59.210000 UTC,2,amazon-web-services|pytorch|amazon-sagemaker|torchtext,376,2020-01-24 01:21:01.013000 UTC,2020-09-19 22:50:22.223000 UTC,,21,0,0,2,,,,,,['amazon-sagemaker']
How do you monitor more 'standard' metrics with SageMaker?,"<p>I'm trying to visualize multiple metrics, while using only one as the objective.  I see how you can define 'custom' metrics using 'MetricDefinitions' under 'AlgorithmSpecification', but what if we just want to see more of the following and record them in CloudWatch as our HyperParameter tuning job progresses:</p>
<pre><code>validation:accuracy 
validation:auc  
validation:error        
validation:logloss  
validation:mse  
</code></pre>
<p>There are more, of course, and the exact metrics I realize might vary based on whether it's a classification or regression problem.</p>
<p>The larger question is just how do we specify the 'recording/logging' of more of these metrics using a standard container like the one for XGBoost?</p>",1,0,2022-04-16 22:59:38.663000 UTC,,,0,amazon-sagemaker,135,2012-04-05 22:09:29.110000 UTC,2022-09-25 00:46:08.230000 UTC,,2269,204,2,207,,,,,,['amazon-sagemaker']
Sagemaker invoke endpoint returned value type?,"<p>I have a classifier working with XGBoost in sagemaker, but despite the training set only having 1s and 0s in the first column (csv file, first column is assumed to be target in sagemaker xgboost), the algorithm returns a decimal. </p>

<p>First 3 records return 1.08, 0.34, and 0.91. I'd assume probabilities but 1.08? If these are rounded to 0 or 1 then they're all correct, but why is it returning non-class values?</p>

<p>Furthermore, the class only contains a predict method - is a predict probability method not possible without using your own model?</p>

<p>The code calling this is:</p>

<pre><code>from flask import Flask
from flask import request
import boto3
from sagemaker.predictor import csv_serializer
import sagemaker

app = Flask(__name__)

@app.route(""/"")
def hello():
    numbers = request.args.get('numbers')

    #session
    boto_session = boto3.Session(profile_name=""profilename"",
                          region_name='regionname')

    #sagemaker session
    sagemaker_session = sagemaker.Session(boto_session=boto_session)

    #endpoint
    predictor = sagemaker.predictor.RealTimePredictor(endpoint=""modelname"", 
        sagemaker_session=sagemaker_session)
    predictor.content_type=""text/csv""
    predictor.serializer=csv_serializer
    predictor.deserializer=None

    #result
    result=predictor.predict(numbers)
    result=result.decode(""utf-8"")
    return f'Output: {result}'

if __name__ == ""__main__"":
    app.run(debug=True, port=5000)
</code></pre>

<p>The flask section works fine, I can retrieve predictions at 127.0.0.1:5000.</p>

<p>Sagemaker Version 1.3.0. Version 1.9.0 does not work - it requires fcntrl which is mac/linux only - see <a href=""https://github.com/aws/sagemaker-python-sdk/issues/311"" rel=""nofollow noreferrer"">this on their repo</a>, apparently it's fixed on pypi but I've tried and the version doesn't change or fix the issue, so I'm stuck on 1.3.0 until they resolve it. Version 1.3.0 does not have a predict_proba method.</p>",0,6,2018-08-22 12:20:31.650000 UTC,,2018-08-22 19:14:35.237000 UTC,4,python|machine-learning|flask|xgboost|amazon-sagemaker,990,2018-07-04 10:36:42.213000 UTC,2021-05-07 21:35:34.107000 UTC,,104,0,0,8,,,,,,['amazon-sagemaker']
Call a sagemaker inference endpoint using the java sdk v2,"<p>I am trying to call a sagemaker inference endpoint from Java. I can do it without an issue from Python, running this after installing the <code>sagemaker</code> package:</p>
<pre><code>predictor = TensorFlowPredictor('my-endpoint')
data = {'foo': 'bar'}
result = predictor.predict(data)
</code></pre>
<p>How can I do this using the latest official library? I'm assuming it is <code>software.amazon.awssdk:sagemaker:2.17.167</code></p>
<p>There are similar questions on this site, but they seem to use an older version of the client. I also found a github repository with examples, but it does not show how to call an inference endpoint.</p>",1,3,2022-04-12 10:51:48.033000 UTC,,,1,java|amazon-web-services|amazon-sagemaker,193,2015-12-23 09:57:24.007000 UTC,2022-09-24 20:07:21.917000 UTC,"Berlin, Germany",6919,352,43,498,,,,,,['amazon-sagemaker']
My Azure Machine Learning Web Service has same result.How to overcome this problem?,"<p>My project is binary classification prediction.I make standardise my numeric values and apply one-hot encoding to categorical values.I trained data and get 80% accuracy.I want to create web service and it has original data input and original data output but currently score model has standardised and one-hot encoded datas.So I concat two table which are original data and my prediction table's score probability,Scored label columns.Then I linked to web service output.I created web service but when I change feature datas Web service output always has same result.Where is my fault? At below,you can see my model.
<a href=""https://i.stack.imgur.com/trQ7s.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/trQ7s.png"" alt=""My model""></a></p>",1,0,2020-06-10 10:56:47.743000 UTC,,,-1,machine-learning|classification|azure-machine-learning-studio|azure-webapps,34,2019-09-07 14:36:25.470000 UTC,2021-12-20 07:10:06.980000 UTC,,39,1,0,6,,,,,,['azure-machine-learning-studio']
How to update Sagemaker Endpoint with the newly Trained Model?,"<p>I am able to train a model on Sagemaker and then deploy a model endpoint out of it.</p>
<p>Now, I want to retrain my model every week with the new data that is coming in. My question is - when I retrain the model how do I update my existing endpoint to use the latest model. (I don't want to deploy a new endpoint)</p>
<p>From some exploration, I think I can do it in 2 ways -</p>
<ol>
<li><p>Near the end of the training job, I create a new <code>EndpointConfig</code> and later use <a href=""https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_UpdateEndpoint.html"" rel=""nofollow noreferrer"">UpdateEndpoint</a> - The downside of this would be -  I would end up with a lot of unnecessary Endpoint Configurations in my AWS Account? Or am I thinking about it wrongly?</p>
</li>
<li><p>Near the end of the training job, I deploy the trained model using <code>.deploy()</code> and set <code>update_endpoint=True</code> as illustrated in <a href=""https://sagemaker.readthedocs.io/en/stable/overview.html"" rel=""nofollow noreferrer"">Sagemaker SDK Doc</a></p>
</li>
</ol>
<p>I am not sure which is the better solution to accomplish this? Is there an even better way to do this?</p>",1,0,2022-03-23 17:00:09.023000 UTC,,,0,amazon-web-services|amazon-sagemaker,362,2014-09-16 17:30:11.067000 UTC,2022-09-22 09:59:00.377000 UTC,Singapore,435,107,12,32,,,,,,['amazon-sagemaker']
Azure ML LibraryExecutionError,"<p>I get the following error when trying to retrieve the data from Azure Machine Learning</p>

<pre><code>Error: LibraryExecutionError
Target: Score Model (AFx Library)
Message: table: The data set being scored must contain all features used during training, missing feature(s): 'NA'.
</code></pre>

<p>If I include NA within the values that get sent to Azure I get the following message </p>

<pre><code>Parsing of input vector failed. Verify the input vector has the correct number of columns and data types
</code></pre>

<p>Has anyone got any idea's on how to fix this issue?</p>

<p>James</p>",2,0,2016-12-20 14:57:21.440000 UTC,,,0,c#|azure|machine-learning|azure-machine-learning-studio,187,2012-05-15 21:40:01.493000 UTC,2022-01-21 11:54:40.987000 UTC,"Neath, Wales",97,33,0,30,,,,,,['azure-machine-learning-studio']
Deploying tensorflow model on sagemaker async endpoint and including an inference.py script,"<p>I am trying to deploy a tensorflow model to async endpoint on sagemaker.</p>
<p>I've previously deployed the same model to a real time endpoint using the following code:</p>
<pre><code>from sagemaker.tensorflow.serving import Model

tensorflow_serving_model = Model(model_data=model_artifact,
                                 entry_point = 'inference.py',
                                 source_dir = 'code',
                                 role=role,
                                 framework_version='2.3',
                                 sagemaker_session=sagemaker_session)
</code></pre>
<pre><code>predictor = tensorflow_serving_model.deploy(initial_instance_count=1, instance_type='ml.m5.xlarge')
</code></pre>
<p>Using the source_dir argument; I was able to include inference.py and requirements.txt files with my model...</p>
<p><strong>What iam  trying to do now:</strong>
iam trying to deploy the same model to an async endpoint, following<a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/async-inference-create-endpoint.html"" rel=""nofollow noreferrer"">doc</a> and <a href=""https://aws.amazon.com/blogs/machine-learning/run-computer-vision-inference-on-large-videos-with-amazon-sagemaker-asynchronous-endpoints/"" rel=""nofollow noreferrer"">this</a> blog example...
I used the following snippits:</p>
<pre><code>from sagemaker.image_uris import retrieve

deploy_instance_type = 'ml.m5.xlarge'
tensorflow_inference_image_uri = retrieve('tensorflow',
                                       region,
                                       version='2.8',
                                       py_version='py3',
                                       instance_type = deploy_instance_type,
                                       accelerator_type=None,
                                       image_scope='inference')

container = tensorflow_inference_image_uri
model_name = 'sagemaker-{0}'.format(str(int(time.time())))

# Create model
create_model_response = sm_client.create_model(
    ModelName = model_name,
    ExecutionRoleArn = role,
    PrimaryContainer = {
        'Image': container,
        'ModelDataUrl': model_artifact,
        'Environment': {
            'TS_MAX_REQUEST_SIZE': '100000000', #default max request size is 6 Mb for torchserve, need to update it to support the 70 mb input payload
            'TS_MAX_RESPONSE_SIZE': '100000000',
            'TS_DEFAULT_RESPONSE_TIMEOUT': '1000'
        }
    },    
)
</code></pre>
<pre><code># Create endpoint config
endpoint_config_name = f&quot;AsyncEndpointConfig-{strftime('%Y-%m-%d-%H-%M-%S', gmtime())}&quot;
create_endpoint_config_response = sm_client.create_endpoint_config(
    EndpointConfigName=endpoint_config_name,
    ProductionVariants=[
        {
            &quot;VariantName&quot;: &quot;variant1&quot;,
            &quot;ModelName&quot;: model_name,
            &quot;InstanceType&quot;: &quot;ml.m5.xlarge&quot;,
            &quot;InitialInstanceCount&quot;: 1
        }
    ],
    AsyncInferenceConfig={
        &quot;OutputConfig&quot;: {
            &quot;S3OutputPath&quot;: f&quot;s3://{bucket}/{bucket_prefix}/output&quot;,
            #  Optionally specify Amazon SNS topics
            &quot;NotificationConfig&quot;: {
              &quot;SuccessTopic&quot;: success_topic,
              &quot;ErrorTopic&quot;: error_topic,
            }
        },
        &quot;ClientConfig&quot;: {
            &quot;MaxConcurrentInvocationsPerInstance&quot;: 2
        }
    }
)
</code></pre>
<pre><code># Create endpoint
endpoint_name = f&quot;sm-{strftime('%Y-%m-%d-%H-%M-%S', gmtime())}&quot;
create_endpoint_response = sm_client.create_endpoint(EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name)
</code></pre>
<p><strong>The problem I am having:</strong>
I can not specify a source directory containing my inference.py and my requirements.txt when trying to deploy the model to an async endpoint.</p>
<p>I am sure i can't include the code/ directory in the .tar model file according to the docs <a href=""https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/using_tf.html#deploy-tensorflow-serving-models"" rel=""nofollow noreferrer"">here</a> the only way is through the source_dir argument in the SDK Model class initialization.</p>
<p><strong>my question:</strong>
how can i use my code/ directory containing my inference.py with my tensorflow model on async endpoint?</p>",1,0,2022-09-03 20:27:27.847000 UTC,,2022-09-03 22:48:49.827000 UTC,0,amazon-web-services|amazon-sagemaker,39,2021-09-26 02:19:45.680000 UTC,2022-09-22 17:23:33.553000 UTC,"Cairo, Egypt",1,0,0,4,,,,,,['amazon-sagemaker']
How to call sagemaker inference endpoint in Java ?,<p>Is there anyone who has successfully implemented a sample code for calling an inference endpoint using sagemaker client java sdk ? I am trying to call a endpoint with text/csv payload.</p>,1,1,2018-07-16 19:05:12.323000 UTC,,,1,amazon-sagemaker,2925,2016-02-09 00:43:38.703000 UTC,2022-09-12 20:20:04.923000 UTC,,43,0,0,38,,,,,,['amazon-sagemaker']
Time series forecasting (DeepAR): Prediction results seem to have basic flaw,"<p>I'm using the DeepAR algorithm to forecast survey response progress with time. I want the model to predict the next 20 data points in the survey progress. Each survey is a time series in my training data. The length of each time series is the # days for which the survey ran. For example, the below series indicates that the survey started on 29-June-2011 and the last response was received on 24-Jul-2011 (25 days is the length). </p>

<pre><code>{""start"":""2011-06-29 00:00:00"", ""target"": [37, 41.2, 47.3, 56.4, 60.6, 60.6, 
61.8, 63, 63, 63, 63.6, 63.6, 64.2, 65.5, 66.1, 66.1, 66.1, 66.1, 66.1, 66.1, 
66.1, 66.1, 66.1, 66.1, 66.7], ""cat"": 3}
</code></pre>

<p>As you can see the values in the time series can remain the same or increase. The training data would never indicate a downward trend. Surprisingly, when I generated predictions, I noticed that the predictions had a downward trend. When there is no trace of downward trend in the training data, I'm wondering how the model could have possibly learned this. To me, this seems to be a basic flaw in the predictions. Can someone please throw some light on why the model might behave in this way? I build the DeepAR model with the below hyper parameters. The model was tested and the RMSE is about 9. Would it help if I change any of the hyper parameters? Any recommendations for this.</p>

<pre><code>time_freq= 'D',
context_length= 30,
prediction_length= 20,
cardinality= 8,
embedding_dimension= 30,
num_cells= 40,
num_layers= 2,
likelihood= 'student-T',
epochs= 20,
mini_batch_size= 32,
learning_rate= 0.001,
dropout_rate= 0.05,
early_stopping_patience= 10 
</code></pre>",1,0,2018-07-05 05:16:12.547000 UTC,,,2,machine-learning|time-series|amazon-sagemaker,1013,2011-03-29 14:34:56.423000 UTC,2022-09-16 11:45:40.660000 UTC,"Chennai, India",454,2,0,363,,,,,,['amazon-sagemaker']
Failed ping healthcheck after deploying TF2.1 model with TF-serving-container on AWS Sagemaker,"<p>We want to deploy a trained Tensorflow Model to AWS Sagemaker for inference with a tensorflow-serving-container. Tensorflow version is 2.1. Following the guide at <a href=""https://github.com/aws/sagemaker-tensorflow-serving-container"" rel=""nofollow noreferrer"">https://github.com/aws/sagemaker-tensorflow-serving-container</a> the following steps have been taken:</p>
<ol>
<li>Build TF 2.1 AMI and publish it to AWS ECR after sucessful local testing</li>
<li>Setting Sagemaker Execution Role Permissions for S3 and ECR.</li>
<li>Pack saved TF model folder (saved_model.pb, assets, variables) into model.tar.gz</li>
<li>Created endpoint with realtime predictor:</li>
</ol>
<pre><code>import os
import sagemaker
from sagemaker.tensorflow.serving import Model
from sagemaker.tensorflow.model import TensorFlowModel
from sagemaker.predictor import json_deserializer, json_serializer, RealTimePredictor
from sagemaker.content_types import CONTENT_TYPE_JSON

def create_tfs_sagemaker_model():
    sagemaker_session = sagemaker.Session()
    role = 'arn:aws:iam::XXXXXXXXX:role/service-role/AmazonSageMaker-ExecutionRole-XXXXXXX
    bucket = 'tf-serving'
    prefix = 'sagemaker/tfs-test'
    s3_path = 's3://{}/{}'.format(bucket, prefix)
    image = 'XXXXXXXX.dkr.ecr.eu-central-1.amazonaws.com/sagemaker-tensorflow-serving:2.1.0-cpu'
    model_data = sagemaker_session.upload_data('model.tar.gz', bucket, os.path.join(prefix, 'model'))
    endpoint_name = 'tf-serving-ep-test-1'
    tensorflow_serving_model = Model(model_data=model_data, role=role, sagemaker_session=sagemaker_session, image=image, framework_version='2.1')
    tensorflow_serving_model.deploy(initial_instance_count=1, instance_type='ml.c4.xlarge', endpoint_name=endpoint_name)
    rt_predictor = RealTimePredictor(endpoint=endpoint_name, sagemaker_session=sagemaker_session, serializer=json_serializer, content_type=CONTENT_TYPE_JSON, accept=CONTENT_TYPE_JSON)
</code></pre>
<ol start=""5"">
<li>Create batch-transform job:</li>
</ol>
<pre><code>def create_tfs_sagemaker_batch_transform():
    sagemaker_session = sagemaker.Session()
    print(sagemaker_session.boto_region_name)
    role = 'arn:aws:iam::XXXXXXXXXXX:role/service-role/AmazonSageMaker-ExecutionRole-XXXXXXXX'
    bucket = 'XXXXXXX-tf-serving'
    prefix = 'sagemaker/tfs-test'
    image = 'XXXXXXXXXX.dkr.ecr.eu-central-1.amazonaws.com/sagemaker-tensorflow-serving:2.1.0-cpu'
    s3_path = 's3://{}/{}'.format(bucket, prefix)
    model_data = sagemaker_session.upload_data('model.tar.gz', bucket, os.path.join(prefix, 'model'))
    tensorflow_serving_model = Model(model_data=model_data, role=role, sagemaker_session=sagemaker_session, image=image, name='deep-net-0', framework_version='2.1')
    print(tensorflow_serving_model.model_data)
    out_path = 's3://XXXXXX-serving-out/'
    input_path = &quot;s3://XXXXXX-serving-in/&quot;    
    tensorflow_serving_transformer = tensorflow_serving_model.transformer(instance_count=1, instance_type='ml.c4.xlarge', accept='application/json', output_path=out_path)
    tensorflow_serving_transformer.transform(input_path, content_type='application/json')
</code></pre>
<p>Both steps 4 and 5 are running and in the AWS Cloudwatch logs we see successful starting of the instances, loading of the model and TF-Serving entering the event loop – see below:</p>
<blockquote>
<p>2020-07-08T17:07:16.156+02:00 INFO:<strong>main</strong>:starting services</p>
<p>2020-07-08T17:07:16.156+02:00 INFO:<strong>main</strong>:nginx config:</p>
<p>2020-07-08T17:07:16.156+02:00 load_module
modules/ngx_http_js_module.so;</p>
<p>2020-07-08T17:07:16.156+02:00 worker_processes auto;</p>
<p>2020-07-08T17:07:16.156+02:00 daemon off;</p>
<p>2020-07-08T17:07:16.156+02:00 pid /tmp/nginx.pid;</p>
<p>2020-07-08T17:07:16.157+02:00 error_log /dev/stderr error;</p>
<p>2020-07-08T17:07:16.157+02:00 worker_rlimit_nofile 4096;</p>
<p>2020-07-08T17:07:16.157+02:00 events { worker_connections 2048;</p>
<p>2020-07-08T17:07:16.157+02:00 }</p>
<p>2020-07-08T17:07:16.162+02:00 http { include /etc/nginx/mime.types;
default_type application/json; access_log /dev/stdout combined;
js_include tensorflow-serving.js; upstream tfs_upstream { server
localhost:10001; } upstream gunicorn_upstream { server
unix:/tmp/gunicorn.sock fail_timeout=1; } server { listen 8080
deferred; client_max_body_size 0; client_body_buffer_size 100m;
subrequest_output_buffer_size 100m; set $tfs_version 2.1; set
$default_tfs_model None; location /tfs { rewrite ^/tfs/(.*) /$1 break;
proxy_redirect off; proxy_pass_request_headers off; proxy_set_header
Content-Type 'application/json'; proxy_set_header Accept
'application/json'; proxy_pass http://tfs_upstream; } location /ping {
js_content ping; } location /invocations { js_content invocations; }
location /models { proxy_pass http://gunicorn_upstream/models; }
location / { return 404 '{&quot;error&quot;: &quot;Not Found&quot;}'; } keepalive_timeout
3; }</p>
<p>2020-07-08T17:07:16.162+02:00 }</p>
<p>2020-07-08T17:07:16.162+02:00 INFO:tfs_utils:using default model name:
model</p>
<p>2020-07-08T17:07:16.162+02:00 INFO:tfs_utils:tensorflow serving model
config:</p>
<p>2020-07-08T17:07:16.162+02:00 model_config_list: { config: { name:
&quot;model&quot;, base_path: &quot;/opt/ml/model&quot;, model_platform: &quot;tensorflow&quot; }</p>
<p>2020-07-08T17:07:16.162+02:00 }</p>
<p>2020-07-08T17:07:16.162+02:00 INFO:<strong>main</strong>:using default model name:
model</p>
<p>2020-07-08T17:07:16.162+02:00 INFO:<strong>main</strong>:tensorflow serving model
config:</p>
<p>2020-07-08T17:07:16.163+02:00 model_config_list: { config: { name:
&quot;model&quot;, base_path: &quot;/opt/ml/model&quot;, model_platform: &quot;tensorflow&quot; }</p>
<p>2020-07-08T17:07:16.163+02:00 }</p>
<p>2020-07-08T17:07:16.163+02:00 INFO:<strong>main</strong>:tensorflow version info:</p>
<p>2020-07-08T17:07:16.163+02:00 TensorFlow ModelServer:
2.1.0-rc1+dev.sha.075ffcf</p>
<p>2020-07-08T17:07:16.163+02:00 TensorFlow Library: 2.1.0</p>
<p>2020-07-08T17:07:16.163+02:00 INFO:<strong>main</strong>:tensorflow serving
command: tensorflow_model_server --port=10000 --rest_api_port=10001
--model_config_file=/sagemaker/model-config.cfg --max_num_load_retries=0</p>
<p>2020-07-08T17:07:16.163+02:00 INFO:<strong>main</strong>:started tensorflow serving
(pid: 13)</p>
<p>2020-07-08T17:07:16.163+02:00 INFO:<strong>main</strong>:nginx version info:</p>
<p>2020-07-08T17:07:16.163+02:00 nginx version: nginx/1.18.0</p>
<p>2020-07-08T17:07:16.163+02:00 built by gcc 7.4.0 (Ubuntu
7.4.0-1ubuntu1~18.04.1)</p>
<p>2020-07-08T17:07:16.163+02:00 built with OpenSSL 1.1.1 11 Sep 2018</p>
<p>2020-07-08T17:07:16.163+02:00 TLS SNI support enabled</p>
<p>2020-07-08T17:07:16.163+02:00 configure arguments: --prefix=/etc/nginx
--sbin-path=/usr/sbin/nginx --modules-path=/usr/lib/nginx/modules --conf-path=/etc/nginx/nginx.conf --error-log-path=/var/log/nginx/error.log --http-log-path=/var/log/nginx/access.log --pid-path=/var/run/nginx.pid --lock-path=/var/run/nginx.lock --http-client-body-temp-path=/var/cache/nginx/client_temp --http-proxy-temp-path=/var/cache/nginx/proxy_temp --http-fastcgi-temp-path=/var/cache/nginx/fastcgi_temp --http-uwsgi-temp-path=/var/cache/nginx/uwsgi_temp --http-scgi-temp-path=/var/cache/nginx/scgi_temp --user=nginx --group=nginx --with-compat --with-file-aio --with-threads --with-http_addition_module --with-http_auth_request_module --with-http_dav_module --with-http_flv_module --with-http_gunzip_module --with-http_gzip_static_module --with-http_mp4_module --with-http_random_index_module --with-http_realip_module --with-http_secure_link_module --with-http_slice_module --with-http_ssl_module --with-http_stub_status_module --with-http_sub_module --with-http_v2_module --with-mail --with-mail_ssl_module --with-stream --with-stream_realip_module --with-stream_ssl_module --with-stream_ssl_preread_module --with-cc-opt='-g -O2 -fdebug-prefix-map=/data/builder/debuild/nginx-1.18.0/debian/debuild-base/nginx-1.18.0=. -fstack-protector-strong -Wformat -Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -fPIC' --with-ld-opt='-Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-z,now -Wl,--as-needed -pie'</p>
<p>2020-07-08T17:07:16.163+02:00 INFO:<strong>main</strong>:started nginx (pid: 15)</p>
<p>2020-07-08T17:07:16.163+02:00 2020-07-08 15:07:15.075708: I
tensorflow_serving/model_servers/server_core.cc:462] Adding/updating
models.</p>
<p>2020-07-08T17:07:16.163+02:00 2020-07-08 15:07:15.075760: I
tensorflow_serving/model_servers/server_core.cc:573] (Re-)adding
model: model</p>
<p>2020-07-08T17:07:16.163+02:00 2020-07-08 15:07:15.180755: I
tensorflow_serving/util/retrier.cc:46] Retrying of Reserving resources
for servable: {name: model version: 1} exhausted max_num_retries: 0</p>
<p>2020-07-08T17:07:16.163+02:00 2020-07-08 15:07:15.180887: I
tensorflow_serving/core/basic_manager.cc:739] Successfully reserved
resources to load servable {name: model version: 1}</p>
<p>2020-07-08T17:07:16.163+02:00 2020-07-08 15:07:15.180919: I
tensorflow_serving/core/loader_harness.cc:66] Approving load for
servable version {name: model version: 1}</p>
<p>2020-07-08T17:07:16.163+02:00 2020-07-08 15:07:15.180944: I
tensorflow_serving/core/loader_harness.cc:74] Loading servable version
{name: model version: 1}</p>
<p>2020-07-08T17:07:16.163+02:00 2020-07-08 15:07:15.180995: I
external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:31]
Reading SavedModel from: /opt/ml/model/1</p>
<p>2020-07-08T17:07:16.163+02:00 2020-07-08 15:07:15.205712: I
external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:54]
Reading meta graph with tags { serve }</p>
<p>2020-07-08T17:07:16.164+02:00 2020-07-08 15:07:15.205825: I
external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:264]
Reading SavedModel debug info (if present) from: /opt/ml/model/1</p>
<p>2020-07-08T17:07:16.164+02:00 2020-07-08 15:07:15.208599: I
external/org_tensorflow/tensorflow/core/common_runtime/process_util.cc:147]
Creating new thread pool with default inter op setting: 2. Tune using
inter_op_parallelism_threads for best performance.</p>
<p>2020-07-08T17:07:16.164+02:00 2020-07-08 15:07:15.328057: I
external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:203]
Restoring SavedModel bundle.</p>
<p>2020-07-08T17:07:17.165+02:00 2020-07-08 15:07:16.578796: I
external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:152]
Running initialization op on SavedModel bundle at path:
/opt/ml/model/1</p>
<p>2020-07-08T17:07:17.165+02:00 2020-07-08 15:07:16.626494: I
external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:333]
SavedModel load for tags { serve }; Status: success: OK. Took 1445495
microseconds.</p>
<p>2020-07-08T17:07:17.165+02:00 2020-07-08 15:07:16.630443: I
tensorflow_serving/servables/tensorflow/saved_model_warmup.cc:105] No
warmup data file found at
/opt/ml/model/1/assets.extra/tf_serving_warmup_requests</p>
<p>2020-07-08T17:07:17.165+02:00 2020-07-08 15:07:16.632461: I
tensorflow_serving/util/retrier.cc:46] Retrying of Loading servable:
{name: model version: 1} exhausted max_num_retries: 0</p>
<p>2020-07-08T17:07:17.165+02:00 2020-07-08 15:07:16.632484: I
tensorflow_serving/core/loader_harness.cc:87] Successfully loaded
servable version {name: model version: 1}</p>
<p>2020-07-08T17:07:17.165+02:00 2020-07-08 15:07:16.634727: I
tensorflow_serving/model_servers/server.cc:362] Running gRPC
ModelServer at 0.0.0.0:10000 ...</p>
<p>2020-07-08T17:07:17.165+02:00 [warn] getaddrinfo: address family for
nodename not supported</p>
<p>2020-07-08T17:07:17.165+02:00 2020-07-08 15:07:16.635747: I
tensorflow_serving/model_servers/server.cc:382] Exporting HTTP/REST
API at:localhost:10001 ...</p>
<p>2020-07-08T17:07:17.165+02:00 [evhttp_server.cc : 238] NET_LOG:
Entering the event loop …</p>
</blockquote>
<p>But both (endpoint and batch transform) fail the Sagemaker Ping Health check with:</p>
<blockquote>
<p>2020-07-08T17:07:32.169+02:00 2020/07/08 15:07:31 [error] 16#16: *1
js: failed ping{ &quot;error&quot;: &quot;Could not find any versions of model None&quot;
}</p>
<p>2020-07-08T17:07:32.170+02:00
169.254.255.130 - - [08/Jul/2020:15:07:31 +0000] &quot;GET /ping HTTP/1.1&quot; 502 157 &quot;-&quot; &quot;Go-http-client/1.1&quot;</p>
</blockquote>
<p>Also, when tested locally with self built docker tf-serving-container the model is running without problems and can be queried with curl.
What could be the issue?</p>",2,0,2020-07-09 10:39:23.890000 UTC,,2020-07-09 22:28:30.260000 UTC,1,amazon-web-services|tensorflow|machine-learning|tensorflow-serving|amazon-sagemaker,1022,2017-10-26 19:45:16.883000 UTC,2022-09-23 14:24:45.990000 UTC,,19,3,0,6,,,,,,['amazon-sagemaker']
azureml how to create a webservice from docker image,"<p>Got all machine learning model that i have registered into a docker image using model package. How do I deploy this docker image to a web service</p>
<pre><code># Define the deployment configuration
aciconfig = AciWebservice.deploy_configuration(
      cpu_cores = 1,
      memory_gb = 1,
      dns_name_label = os.environ['ACI_DNS_NAME_LABEL'])


#create env 
environment = Environment('env')
environment.python.conda_dependencies = CondaDependencies.create(conda_packages=[
'pip==20.2.4'],
 pip_packages=[
 'azureml-defaults',
 'joblib',
 'numpy',
 'scikit-learn'])

    
inf_conf = InferenceConfig(entry_script=&quot;score.py&quot;,environment=environment)
#crete docker image    
docker_image = Model.package(ws,models_latest, inf_conf,image_name=&quot;imgname&quot;)
docker_image.wait_for_creation(show_output=True) 
docker_image.pull()
</code></pre>",1,0,2022-08-19 09:40:33.817000 UTC,,,0,python|azure-machine-learning-service|azureml-python-sdk,42,2021-10-27 11:28:41.057000 UTC,2022-09-22 14:14:03.980000 UTC,,128,28,0,24,,,,,,['azure-machine-learning-service']
Issues using SendGrid with Azure ML,"<p>I'm trying to send an email using SendGrid within Azure Machine Learning. This is initially just a basic test email to ensure it is working correctly.</p>
<p>The steps I have taken:</p>
<ol>
<li>Pip installed SendGrid;</li>
<li>Zipped the SendGrid download and uploaded as a dataset to AML platform;</li>
<li>Attempted to run the example SendGrid Python code (which can be seen below):</li>
</ol>
<p>I have copied steps in similar posts concerning uploading modules to AML <a href=""https://stackoverflow.com/questions/46222606/updating-pandas-to-version-0-19-in-azure-ml-studio/46232963#46232963"">here</a> and <a href=""https://stackoverflow.com/questions/46523924/adding-python-modules-to-azureml-workspace"">here</a> as well as ensuring the correct settings for the SendGrid API key were established on setup <a href=""https://stackoverflow.com/questions/51078310/sending-simple-email-on-azure-with-sendgrid"">here</a>.</p>
<pre class=""lang-py prettyprint-override""><code>def azureml_main():

    import sendgrid
    from sendgrid import SendGridAPIClient
    from sendgrid.helpers.mail import Mail


    message = Mail(
        from_email='xxx@xyz.com',
        to_emails='xxx@xyz.com',
        subject='Sending with Twilio SendGrid is Fun',
        html_content='&lt;strong&gt;and easy to do anywhere, even with Python&lt;/strong&gt;')
    try:
        sg = SendGridAPIClient(os.environ.get('SG API Code'))
        response = sg.send(message)
        print(response.status_code)
        print(response.body)
        print(response.headers)
    except Exception as e:
        print(e)
</code></pre>
<p>No error message is returned in the terminal. To me this indicates there weren't any issues with the code, yet no emails have been received/sent.</p>
<pre><code>python ScheduleRun.py 
azureuser@will1:~/cloudfiles/code/Users/will/Schedule$ 
azureuser@will1:~/cloudfiles/code/Users/will/Schedule$ python ScheduleRun.py 
azureuser@will1:~/cloudfiles/code/Users/will/Schedule$  
</code></pre>",1,0,2021-11-08 16:48:51.843000 UTC,,2021-11-08 23:23:28.700000 UTC,0,python|sendgrid|azure-machine-learning-service,37,2021-06-24 10:29:22.660000 UTC,2021-12-15 15:44:15.450000 UTC,,13,0,0,0,,,,,,['azure-machine-learning-service']
Convert bytes array format read from S3 to numpy array or tensor in AWS SageMaker,"<p>I have read some X_train and y_train and uploaded them in a form of in-memory bytes array to s3 as below:</p>
<p><code>X_train</code> and <code>y_train</code> are one dimensional arrays like:</p>
<p>X_train:</p>
<p><code>    array([[ 2. ],[12.9],[ 1.3],[ 5.1],[ 9.6],[ 8.2],</code>...</p>
<p>y_train:</p>
<p><code>    array([[ 43525.],[135675.],[ 46205.],[ 66029.],[112635.],</code>...</p>
<pre><code>    import io
    import sagemaker                               
    import sagemaker.amazon.common as smcl

    sm_session = sagemaker.Session()
    bucket = sm_session.default_bucket()

    buffer = io.BytesIO()

    # writing train data to the form of tensors:
    smcl.write_numpy_to_dense_tensor(buffer, X_train, y_train.reshape(-1))
    buffer.seek(0)


    # Uploading to s3
    file_name = 'Train_data'
    folder_name = 'Test_folder'
    path_to_train_data = os.path.join(folder_name,'train',file_name)
    boto3.resource('s3').Bucket(bucket).Object(path_to_train_data).upload_fileobj(buffer)

</code></pre>
<p>I want to read them back from s3 and convery them t their original form:</p>
<pre><code>    s3 = boto3.resource('s3')
    bucket = s3.Bucket(bucket)

    
    buf = io.BytesIO()
    bucket.download_fileobj(key_from_s3, buf)
    filecontent_bytes = buf.getvalue()
</code></pre>
<p>The output of the <code>fileconent_byte</code> is something like this:</p>
<pre><code>b'\n#\xd7\xce(\x00\x00\x00\n\x12\n\x06values\x12\x08\x12\x06\n\x04\x00\x00\x00@\x12\x12\n\...
</code></pre>
<p>How can I convert them to their original form?
Thanks.</p>",1,0,2022-03-15 17:08:17.313000 UTC,,2022-03-15 21:22:50.183000 UTC,0,python|arrays|amazon-web-services|io|amazon-sagemaker,133,2014-01-30 11:44:14.060000 UTC,2022-05-13 05:32:33.527000 UTC,,81,4,0,43,,,,,,['amazon-sagemaker']
aws sagemaker giving The model data archive is too large. Please reduce the size of the model data archive,"<p>I am using aws sagemaker to deploy a model whose generated artifacts are huge. The compressed size is about 80GB. Deploying on sage maker on a ml.m5.12xlarge instance is throwing this error while deploying to the endpoint
 <code>The model data archive is too large. Please reduce the size of the model data archive or move to an instance type with more memory.</code></p>

<p>I found that aws attaches EBS volume based on instance size(<a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/host-instance-storage.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/host-instance-storage.html</a>)  and i couldnot find anything more that 30Gb here. Should i go with a multi model endpoint here?</p>",1,1,2020-05-05 12:46:49.263000 UTC,,,0,training-data|amazon-sagemaker,353,2013-10-10 06:40:54.250000 UTC,2022-02-23 12:44:39.170000 UTC,,582,22,0,46,,,,,,['amazon-sagemaker']
Using SageMaker with Hydra,"<p>I have a question about SageMaker and Hydra.</p>
<p><strong>TL;DR</strong>
Is there a way to pass arguments from SageMaker estimator to a Hydra script? Currently it passes parameters in a very strict way.</p>
<p><strong>Full Question</strong>
I use Hydra in order to pass configs to my training script. I have many configs and it works good for my. For example, if I want to use a specific optimizer, I do:</p>
<pre><code>python train.py optimizer=adam
</code></pre>
<p>This is my training script, for instance:</p>
<pre><code>@hydra.main(version_base=None, config_path=&quot;configs/&quot;, config_name=&quot;config&quot;)
def train(config: DictConfig):
    logging.info(f&quot;Instantiating dataset &lt;{config.dataset._target_}&gt;&quot;)
    train_ds, val_ds = hydra.utils.call(config.dataset)

    logging.info(f&quot;Instantiating model &lt;{config.model._target_}&gt;&quot;)
    model = hydra.utils.call(config.model)

    logging.info(f&quot;Instantiating optimizer &lt;{config.optimizer._target_}&gt;&quot;)
    optimizer = hydra.utils.instantiate(config.optimizer)

    logging.info(f&quot;Instantiating loss &lt;{config.loss._target_}&gt;&quot;)
    loss = hydra.utils.instantiate(config.loss)

    callbacks = []
    if &quot;callbacks&quot; in config:
        for _, cb_conf in config.callbacks.items():
            if &quot;_target_&quot; in cb_conf:
                logging.info(f&quot;Instantiating callback &lt;{cb_conf._target_}&gt;&quot;)
                callbacks.append(hydra.utils.instantiate(cb_conf))

    metrics = []
    if &quot;metrics&quot; in config:
        for _, metric_conf in config.metrics.items():
            if &quot;_target_&quot; in metric_conf:
                logging.info(f&quot;Instantiating metric &lt;{metric_conf._target_}&gt;&quot;)
                metrics.append(hydra.utils.instantiate(metric_conf))

    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)

    model.fit(
        train_ds,
        validation_data=val_ds,
        epochs=config.epochs,
        callbacks=callbacks,
    )


if __name__ == &quot;__main__&quot;:
    train()
</code></pre>
<p>And I have a relevant <code>optimizer/adam.yaml</code> file.</p>
<p>Now, I started using SageMaker to run my experiments in the cloud and I noticed a problem.
It doesn't support the hydra syntax (<code>+optimizer=sgd</code>), stuff like that.</p>
<p>Is there a way to make it play nicely with Hydra syntax? If not, do you have a suggestion for refactoring my training code so that it would work nicely with Hydra/OmegaConf?</p>
<p>I saw there is a similar question in SageMaker issues page, but it doesn't have any replies:
<a href=""https://github.com/aws/sagemaker-python-sdk/issues/1837"" rel=""nofollow noreferrer"">https://github.com/aws/sagemaker-python-sdk/issues/1837</a></p>",1,0,2022-09-06 07:33:39.553000 UTC,,,0,python|yaml|amazon-sagemaker|fb-hydra|omegaconf,34,2016-03-06 17:09:27.210000 UTC,2022-09-22 12:24:12.473000 UTC,,769,20,6,148,,,,,,['amazon-sagemaker']
Calling AWS Sagemaker endpoint from Glue Job,"<p>How to invoke AWS Sagemaker Endpoint from Glue Job ?</p>
<pre><code>endpoint_name='I_created_EndPoint'
from sagemaker.predictor import csv_serializer, json_deserializer

predictor = sagemaker.predictor.RealTimePredictor(endpoint=endpoint_name)
</code></pre>
<p>However,  <code>import sagemaker</code> was giving error. Tried</p>
<pre><code>import sys
import subprocess

# implement pip as a subprocess:
subprocess.check_call([sys.executable, '-m', 'pip', 'install',
'sagemaker'])
</code></pre>
<p>That too gave some error as</p>
<pre><code>Traceback (most recent call last):
  File &quot;/tmp/DataToMl&quot;, line 6, in &lt;module&gt;
    'sagemaker'])
  File &quot;/usr/lib64/python3.7/subprocess.py&quot;, line 363, in check_call
    raise CalledProcessError(retcode, cmd)
subprocess.CalledProcessError: Command '['/usr/bin/python3', '-m', 'pip', 'install', 'sagemaker']' returned non-zero exit status 2
</code></pre>",1,7,2020-09-14 21:06:19.900000 UTC,,,1,python|amazon-web-services|aws-glue|amazon-sagemaker,395,2013-06-06 09:03:28.500000 UTC,2022-09-24 23:52:14.747000 UTC,,1573,97,12,194,,,,,,['amazon-sagemaker']
How to train MLM model XLM Roberta large on google machine specs fastly with less memory,"<p>I am fine tuning masked language model from <code>XLM Roberta large</code> on google machine specs.
I made couple of experiments and was strange to see few results.</p>
<pre><code>&quot;a2-highgpu-4g&quot; ,accelerator_count=4, accelerator_type=&quot;NVIDIA_TESLA_A100&quot; on 4,12,672 data batch size 4 Running ( 4 data*4 GPU=16 data points)
&quot;a2-highgpu-4g&quot; ,accelerator_count=4 , accelerator_type=&quot;NVIDIA_TESLA_A100&quot;on 4,12,672 data batch size 8 failed
 &quot;a2-highgpu-4g&quot; ,accelerator_count=4, accelerator_type=&quot;NVIDIA_TESLA_A100&quot; on 4,12,672 data batch size 16 failed
&quot;a2-highgpu-4g&quot; ,accelerator_count=4.,accelerator_type=&quot;NVIDIA_TESLA_A100&quot; on 4,12,672 data batch size 32 failed
</code></pre>
<p>I was not able to train model with <code>batch size </code> more than 4 on # of GPU's. It stopped in mid-way.</p>
<p>Here is the code I am using.</p>
<pre><code>training_args = tr.TrainingArguments(
#     disable_tqdm=True,
    output_dir='/home/pc/Bert_multilingual_exp_TCM/results_mlm_exp2', 
    overwrite_output_dir=True,
    num_train_epochs=2,
    per_device_train_batch_size=4,
#     per_device_train_batch_size
#     per_gpu_train_batch_size
    prediction_loss_only=True
    ,save_strategy=&quot;no&quot;
    ,run_name=&quot;MLM_Exp1&quot;
    ,learning_rate=2e-5
    ,logging_dir='/home/pc/Bert_multilingual_exp_TCM/logs_mlm_exp1'        # directory for storing logs
    ,logging_steps=40000
    ,logging_strategy='no'
)

trainer = tr.Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=train_data
    
)
</code></pre>
<p><strong>My Questions</strong></p>
<p>How can I train with larger batch size on <code>a2-highgpu-4g</code> machine?</p>
<p>Which parameters can I include in <code>TrainingArguments</code> so that training is fast and occupies small memory?</p>
<p>Thanks in advance.</p>
<h3>Versions</h3>
<pre><code>torch==1.11.0+cu113 

torchvision==0.12.0+cu113  

torchaudio==0.11.0+cu113 

transformers==4.17.0
</code></pre>",1,0,2022-03-16 15:51:24.443000 UTC,1.0,,1,python-3.x|google-cloud-platform|pytorch|huggingface-transformers|google-cloud-vertex-ai,210,2018-06-07 08:44:46.053000 UTC,2022-09-23 09:15:48.837000 UTC,,1127,526,93,283,,,,,,['google-cloud-vertex-ai']
Ingest image to GCP Vertex AI Feature Store,"<p>I want to ingest images to Vertex AI Feature Store from Cloud Storage.
In what format can I ingest images?</p>
<p>As documentation says:</p>
<p>For batch ingestion, Vertex AI Feature Store can ingest data from tables in BigQuery or files in Cloud Storage. For files in Cloud Storage, they must be in the Avro or CSV format.</p>
<p>For streaming ingestion, you provide the feature values to ingest as part of the API request. These source data requirements don't apply. For more information, see the writeFeatureValues API reference.</p>",0,0,2022-09-09 19:49:53.020000 UTC,,,0,google-cloud-platform|google-cloud-vertex-ai|feature-store,23,2022-09-09 19:46:33.243000 UTC,2022-09-15 19:23:55.247000 UTC,,1,0,0,1,,,,,,['google-cloud-vertex-ai']
Not able to log artifacts from Windows client to mlflow server running on GCP VM instance,"<p>I installed mlflow on GCP VM and started the server by running this command on VM <code>mlflow server --host x.x.x.x</code>, here x.x.x.x is the internal IP of the VM</p>

<p>Set the tracking URI using <code>mlflow.set_tracking_uri(""http://x.x.x.x:5000/"")</code>, here x.x.x.x is the external ip of the VM</p>

<p>I'm running this code now to log parameters and artifacts on GCP VM where my mlflow server is running:</p>

<pre><code>def eval_metrics(actual, pred):
        rmse = np.sqrt(mean_squared_error(actual, pred))
        mae = mean_absolute_error(actual, pred)
        r2 = r2_score(actual, pred)
        return rmse, mae, r2
</code></pre>

<pre><code>with mlflow.start_run():
        lr = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=42)
        lr.fit(train_x, train_y)
        predicted_qualities = lr.predict(test_x)
        (rmse, mae, r2) = eval_metrics(test_y, predicted_qualities)
        print(""Elasticnet model (alpha=%f, l1_ratio=%f):"" % (alpha, l1_ratio))
        print(""  RMSE: %s"" % rmse)
        print(""  MAE: %s"" % mae)
        print(""  R2: %s"" % r2)
        mlflow.log_param(""alpha"", alpha)
        mlflow.log_param(""l1_ratio"", l1_ratio)
        mlflow.log_metric(""rmse"", rmse)
        mlflow.log_metric(""r2"", r2)
        mlflow.log_metric(""mae"", mae)
        mlflow.log_artifacts(lr)
</code></pre>

<p>Parameters and Metrics I'm able to get on <a href=""https://x.x.x.x:5000"" rel=""nofollow noreferrer"">https://x.x.x.x:5000</a>, where x.x.x.x is external IP of the VM, but at the last line of the code i.e., <code>mlflow.log_artifacts(lr)</code> facing the error given below:</p>

<p><img src=""https://user-images.githubusercontent.com/10863620/76417135-517a5b80-63c2-11ea-8235-2f6c7c2d5324.png"" alt=""image""></p>

<p>When executed <code>mlflow.get_artifact_uri()</code>, the path returned is <code>./mlruns/0/6073b44bbac842e5axxxxxxxxxxxxxxxxxx/artifacts</code></p>

<p>Is there something wrong with the artifact path and any idea how can I resolve this to log artifacts on VM from code running on local jupyter notebook?</p>",0,0,2020-03-12 06:33:58.227000 UTC,,2020-03-12 06:42:54.870000 UTC,2,python|google-cloud-platform|google-cloud-storage|google-compute-engine|mlflow,154,2015-12-26 10:00:57.623000 UTC,2022-09-24 13:39:24.720000 UTC,India,736,69,2,234,,,,,,['mlflow']
Is it possible to visualize gradients in comet-ml?,"<p>It is straightforward to set up TensorBoard in Keras (it's just a <a href=""https://keras.io/callbacks/#tensorboard"" rel=""noreferrer"">callback</a>!) and then it is possible to visualize the distribution and magnitude of the weights and gradients. Is it possible to do the same with <a href=""https://www.comet.ml"" rel=""noreferrer"">comet.ml</a>? Comet.ml is easy to set up, but visualizes only the loss and accuracy evolution... Is there a way to ""restore"" all the TensorBoard features in comet.ml?</p>",0,0,2019-07-04 05:55:04.473000 UTC,2.0,,6,python-3.x|tensorflow|keras|tf.keras|comet-ml,117,2019-05-09 23:15:56.590000 UTC,2022-09-24 14:59:07.383000 UTC,,606,589,211,91,,,,,,['comet-ml']
Why do I get AttributeError: module 'google_cloud_pipeline_components.aiplatform' has no attribute 'EndpointDeleteOp' in GCP?,"<p>My code is:</p>
<pre><code>from google.cloud import aiplatform
from google_cloud_pipeline_components import aiplatform as gcc_aip
... (in a pipeline definition:)
 delete_endpoint_op = gcc_aip.EndpointDeleteOp(some_condition)
</code></pre>
<p>and when I compile the pipeline I get:</p>
<pre><code>AttributeError: module 'google_cloud_pipeline_components.aiplatform' has no attribute 'EndpointDeleteOp'
</code></pre>
<p>but <a href=""https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-1.0.0/google_cloud_pipeline_components.aiplatform.html#google_cloud_pipeline_components.aiplatform.EndpointDeleteOp"" rel=""nofollow noreferrer"">this</a> says it exists. Could this be I am importing the wrong version, and, if so, how do I check and fix? TIA!</p>",1,0,2022-09-06 11:30:23.667000 UTC,,,0,google-cloud-platform|google-cloud-vertex-ai,26,2012-10-25 08:48:34.717000 UTC,2022-09-23 10:10:32.783000 UTC,,2564,304,8,451,,,,,,['google-cloud-vertex-ai']
Azure ML Studio: Create DATASET via REST API,"<p>Please tell me how to create a Dataset via REST API.</p>
<p>There is a way to <a href=""https://docs.microsoft.com/en-us/rest/api/azureml/datastores"" rel=""nofollow noreferrer"">create a Datastore</a>, but I can't find a Dataset.</p>",1,0,2022-01-31 09:05:59.523000 UTC,,,1,azure|rest|dataset|azure-machine-learning-service,126,2013-04-12 13:32:22.310000 UTC,2022-09-21 16:44:16.053000 UTC,,131,123,1,32,,,,,,['azure-machine-learning-service']
Authentication problems,"<p>I am trying to automate the process to create a model with azure machine learning services and I get some problems with the authentication. When I run my code on my remote machine everything is fine but when I run the code on remote I get this authentication sentence:</p>

<pre><code>Make sure your code doesn't require 'az login' to have happened before using azureml-SDK, except the case when you are specifying AzureCliAuthentication in azureml-SDK.
Performing interactive authentication. Please follow the instructions on the terminal.
To sign in, use a web browser to open the page https://microsoft.com/devicelogin and enter the code CZMKCYS8B to authenticate""
</code></pre>

<p>Azure ask me for authentication and I have to make it manually.
I would like to know if there is some way to do it automatically.</p>

<p>I was looking for it and I was investigated how to do it using tokens but I couldn't find any solution</p>

<p>Someone can give me an advice?</p>

<p>Thanks in advance.</p>",0,3,2019-02-18 12:20:51.810000 UTC,,2019-02-18 12:50:45.457000 UTC,2,azure-machine-learning-studio,653,2017-08-01 17:34:44.543000 UTC,2022-01-10 11:29:26.423000 UTC,,71,15,0,11,,,,,,['azure-machine-learning-studio']
How can I deploy AWS SageMaker Linear Learner Model in a Local Environment,"<p>I have trained a AWS SageMaker Model using the in-built Linear Learner algorithm. I can download the trained model artifacts (model.tar.gz) from S3.</p>

<p>How can I deploy the model in an local environment which is independent of AWS, so I can make predictions inferences calls without internet access?</p>",1,0,2018-08-10 02:45:48.300000 UTC,,,0,amazon-sagemaker,518,2018-08-09 04:09:19.290000 UTC,2021-12-06 08:28:35.433000 UTC,,1,0,0,9,,,,,,['amazon-sagemaker']
Accelerate BERT training with HuggingFace Model Parallelism,"<p>I am currently using SageMaker to train BERT and trying to improve the BERT training time. I use PyTorch and Huggingface on AWS g4dn.12xlarge instance type.</p>
<p>However when I run parallel training it is far from achieving linear improvement. I'm looking for some hints on distributed training to improve the BERT training time in SageMaker.</p>",0,0,2022-09-23 13:47:56.633000 UTC,,,0,pytorch|amazon-sagemaker|huggingface-transformers|bert-language-model|distributed-training,12,2014-01-16 15:43:59.673000 UTC,2022-09-25 03:22:08.463000 UTC,Singapore,5854,155,70,794,,,,,,['amazon-sagemaker']
How to force a SageMaker Notebook instance to stop?,"<p>I am an AWS noob, My notebook instance has been on <strong>Pending Status</strong> for a couple hours.</p>

<p>How can I force it to STOP ? Or at least get my code back.</p>

<p>Thanks in advance</p>",1,0,2020-03-18 23:14:42.977000 UTC,,,1,jupyter-notebook|amazon-sagemaker,1104,2015-05-29 21:37:17.533000 UTC,2022-07-30 20:53:27.257000 UTC,,345,11,0,107,,,,,,['amazon-sagemaker']
How to set mlflow version field,"<p>I just started to work with <code>mlflow</code> (<code>1.24.0</code>)</p>
<p>How can I set (update) the version field ?</p>
<p><a href=""https://i.stack.imgur.com/fCNy3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fCNy3.png"" alt=""enter image description here"" /></a></p>",1,0,2022-04-01 08:24:28.933000 UTC,,,0,python|mlflow,147,2014-05-23 08:26:35.280000 UTC,2022-09-19 12:37:21.063000 UTC,,3934,1052,6,416,,,,,,['mlflow']
Does clone experiment work on sklearn functions?,"<p>I'm trying to run a script and I'm constantly getting this while cloning experiment in allegro.ai
AttributeError: 'Namespace' object has no attribute 'get'
Can anybody help?</p>",1,2,2020-06-26 09:35:25.703000 UTC,,2021-01-01 11:36:41.127000 UTC,0,trains|clearml,27,2017-09-11 18:02:08.440000 UTC,2020-06-26 09:33:06.227000 UTC,,1,0,0,1,,,,,,['clearml']
Is SageMaker Distributed Data-Parallel (SMDDP) supported for keras models?,"<p>Is SageMaker Distributed Data-Parallel (SMDDP) supported for keras models?</p>
<p>In documentation it says &quot;SageMaker distributed data parallel is adaptable to TensorFlow training scripts composed of tf core modules except tf.keras modules. SageMaker distributed data parallel does not support TensorFlow with Keras implementation.&quot; <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/data-parallel-modify-sdp.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/data-parallel-modify-sdp.html</a></p>
<p>But inside the training script and how to modify it, I can see the tf.keras and tf.keras.model is used. <a href=""https://sagemaker.readthedocs.io/en/stable/api/training/sdp_versions/v1.0.0/smd_data_parallel_tensorflow.html"" rel=""nofollow noreferrer"">https://sagemaker.readthedocs.io/en/stable/api/training/sdp_versions/v1.0.0/smd_data_parallel_tensorflow.html</a></p>",0,0,2022-09-09 05:56:11.423000 UTC,,,0,python|amazon-web-services|tensorflow|keras|amazon-sagemaker,12,2022-09-08 07:14:26.503000 UTC,2022-09-23 21:03:19.637000 UTC,,48,2,0,6,,,,,,['amazon-sagemaker']
Sagemaker Studio trial component chart not showing,"<p>I am wondering why I am unable to show the loss and accuracy curve in Sagemaker Studio, Trial components chart.</p>
<p>I am using tensorflow's keras API for training.</p>
<pre class=""lang-py prettyprint-override""><code>from sagemaker.tensorflow import TensorFlow


estimator = TensorFlow(
    entry_point=&quot;sm_entrypoint.sh&quot;,
    source_dir=&quot;.&quot;,
    role=role,
    instance_count=1,
    instance_type=&quot;ml.m5.4xlarge&quot;,
    framework_version=&quot;2.4&quot;,
    py_version=&quot;py37&quot;,
    metric_definitions=[
            {'Name':'train:loss', 'Regex':'loss: ([0-9.]+'},
            {'Name':'val:loss', 'Regex':'val_loss: ([0-9.]+'},
            {'Name':'train:accuracy', 'Regex':'accuracy: ([0-9.]+'},
            {'Name':'val:accuracy', 'Regex':'val_accuracy: ([0-9.]+'}
        ],
    enable_sagemaker_metrics=True
)
 

estimator.fit(
    inputs=&quot;s3://xxx&quot;,
    experiment_config={
        &quot;ExperimentName&quot;: &quot;urbansounds-20211027&quot;,
        &quot;TrialName&quot;: &quot;tf-classical-NN-20211027&quot;,
        &quot;TrialComponentDisplayName&quot;: &quot;Train&quot;
    }
)
</code></pre>
<p>Regex is enabled, and appears to be logging them correctly. Since under the metrics tab, it shows 12 counts for each metric, corresponding to 12 epochs cycle which I specified.</p>
<p><a href=""https://i.stack.imgur.com/G0Q4t.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/G0Q4t.png"" alt=""enter image description here"" /></a></p>
<p>However, the chart is empty. The x-axis is in time here, but it is also empty when I switched to epoch.</p>
<p><a href=""https://i.stack.imgur.com/7Q4x8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7Q4x8.png"" alt=""enter image description here"" /></a></p>",0,0,2021-10-29 09:36:39.747000 UTC,,,1,amazon-sagemaker,127,2015-07-15 04:08:58.693000 UTC,2022-09-22 06:19:06.670000 UTC,,1962,497,7,168,,,,,,['amazon-sagemaker']
SageMaker Train job is not creating /opt/ml/input/data/training directory,"<p>I am trying to create a custom algorithm by following the instruction given in <a href=""https://aws.amazon.com/blogs/machine-learning/train-and-host-scikit-learn-models-in-amazon-sagemaker-by-building-a-scikit-docker-container/"" rel=""nofollow noreferrer"">this tutorial</a>.</p>

<p>When I am running the train job it is failing with error No such file or directory: '/opt/ml/input/data/training'.</p>

<p>As per the documentation, SageMaker should create these documents and copy the data and artifacts during the runtime. But this is not happening.</p>

<p>Please share your thoughts on this.</p>

<p>My DockerFile content,</p>

<pre><code># Build an image that can do training and inference in SageMaker
 # This is a Python 2 image that uses the nginx, gunicorn, flask stack
 # for serving inferences in a stable way.

 FROM ubuntu:16.04

 MAINTAINER Amazon AI &lt;sage-learner@amazon.com


 RUN apt-get -y update &amp;&amp; apt-get install -y --no-install-recommends \
          wget \
          python \
          nginx \
          ca-certificates \
     &amp;&amp; rm -rf /var/lib/apt/lists/*

 # Here we get all python packages.
 # There's substantial overlap between scipy and numpy that we eliminate by
 # linking them together. Likewise, pip leaves the install caches populated which uses
 # a significant amount of space. These optimizations save a fair amount of space in the
 # image, which reduces start up time. RUN wget https://bootstrap.pypa.io/get-pip.py &amp;&amp; python get-pip.py &amp;&amp; \
     pip install numpy==1.16.2 scipy==1.2.1 scikit-learn==0.20.2 pandas flask gevent gunicorn &amp;&amp; \
         (cd /usr/local/lib/python2.7/dist-packages/scipy/.libs; rm *; ln ../../numpy/.libs/* .) &amp;&amp; \
         rm -rf /root/.cache

 # Set some environment variables. PYTHONUNBUFFERED keeps Python from buffering our standard
 # output stream, which means that logs can be delivered to the user quickly. PYTHONDONTWRITEBYTECODE
 # keeps Python from writing the .pyc files which are unnecessary in this case. We also update
 # PATH so that the train and serve programs are found when the container is invoked.

 ENV PYTHONUNBUFFERED=TRUE ENV PYTHONDONTWRITEBYTECODE=TRUE ENV
 PATH=""/opt/program:${PATH}""

 # Set up the program in the image COPY decision_trees /opt/program WORKDIR /opt/program
</code></pre>",1,0,2019-03-12 06:54:08.547000 UTC,1.0,,3,amazon-web-services|aws-sdk|amazon-sagemaker,4123,2015-01-26 15:18:57.857000 UTC,2019-11-08 02:36:25.727000 UTC,,31,0,0,2,,,,,,['amazon-sagemaker']
Error when opening a terminal in Microsoft Azure Machine Learning,"<p>After creating a compute instance under Microsoft Azure Machine Learning and select Jupyter to open Jupyter Notebook, I am selecting a New terminal from the menu. However, I am getting the following errors:</p>
<pre><code>terminado.js:4 WebSocket connection to 'wss://xalapa.eastus2.instances.azureml.ms/terminals/websocket/2' failed: Error during WebSocket handshake: Unexpected response code: 426
make_terminal @ terminado.js:4
index.js:5 Uncaught TypeError: Cannot read property 'parentElement' of undefined
    at proposeGeometry (index.js:5)
    at fit (index.js:30)
    at Terminal.terminalConstructor.fit (index.js:44)
    at window.onresize (main.js:54)
</code></pre>
<p><a href=""https://i.stack.imgur.com/868vU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/868vU.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/OfEQB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OfEQB.png"" alt=""enter image description here"" /></a></p>
<p>I have recreated the compute instance with no luck.</p>
<p>Do you know what is the issue related?</p>
<p>Thanks</p>",2,0,2021-03-04 05:42:23.603000 UTC,,2021-03-04 06:15:47.497000 UTC,1,azure|azure-machine-learning-studio|azure-machine-learning-service,1242,2015-02-06 20:10:34.190000 UTC,2022-08-29 22:33:23.433000 UTC,"Xalapa, Ver., Mexico",1413,129,3,219,,,,,,"['azure-machine-learning-studio', 'azure-machine-learning-service']"
Vertex AI returns a different result from the local tflite model,"<p>I uploaded my tflite model on Vertex AI and made an endpoint, and I requested inference with some input value, but it returns a different result from my local tflite model's inference result.</p>
<p>The input value is float32 array(actually sampled audio data) and I used <a href=""https://github.com/googleapis/python-aiplatform/blob/main/samples/snippets/prediction_service/predict_custom_trained_model_sample.py"" rel=""nofollow noreferrer"">this code</a> for the request. Although there was the same input array, the local tflite model and the model which is uploaded on Vertex AI returns quite big different result.</p>
<p>Is there any possibility of distortion on the value while it transfers to the Vertex AI instance?</p>",1,0,2021-11-17 11:09:27.180000 UTC,,2021-11-17 22:29:31.083000 UTC,1,google-cloud-platform|google-cloud-ml|google-cloud-vertex-ai|google-cloud-ai,175,2021-08-27 00:34:21.567000 UTC,2022-08-11 02:53:07.690000 UTC,,11,0,0,5,,,,,,['google-cloud-vertex-ai']
SageMaker ANSI escape codes,"<p>I'm using a library in a SageMaker training script that includes print statements with characters like tabs. When I look in my SM cloudwatch training logs, they're filled with ANSI escape codes like <code>#011</code> (in place of tabs). This makes the logs much more difficult to read. </p>

<p>Is there any way I can prevent this behavior? Whether that be through a modification of my <code>Dockerfile</code> or my <code>train.py</code> script?</p>",0,2,2020-02-29 20:14:26.340000 UTC,,,0,dockerfile|amazon-cloudwatch|ansi|amazon-sagemaker|ansi-escape,95,2013-02-20 05:47:52.693000 UTC,2022-09-23 20:45:28.400000 UTC,NYC,6281,430,17,958,,,,,,['amazon-sagemaker']
sagemaker endpoint content_type can't set attribute,"<p>I am trying to create a fraud detection model by following this notebook from aws:</p>
<p><a href=""https://github.com/awslabs/fraud-detection-using-machine-learning/blob/master/source/notebooks/sagemaker_fraud_detection.ipynb"" rel=""nofollow noreferrer"">https://github.com/awslabs/fraud-detection-using-machine-learning/blob/master/source/notebooks/sagemaker_fraud_detection.ipynb</a></p>
<p>In the section &quot;Host Random Cut Forest', at the line</p>
<pre><code>rcf_predictor.content_type = 'text/csv'
</code></pre>
<p>I get this error:</p>
<pre><code>   ---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-14-bd790d3851f9&gt; in &lt;module&gt;
      1 from sagemaker.predictor import csv_serializer, json_deserializer
      2 
----&gt; 3 rcf_predictor.content_type = 'text/csv'
      4 rcf_predictor.serializer = csv_serializer
      5 rcf_predictor.accept = 'application/json'

AttributeError: can't set attribute
</code></pre>
<p>What is this error and how to solve it?</p>",1,0,2021-01-05 20:25:37.517000 UTC,,,1,python-3.x|amazon-sagemaker,255,2016-05-02 02:13:15.707000 UTC,2022-09-01 18:42:03.807000 UTC,,133,2,0,17,,,,,,['amazon-sagemaker']
"SageMaker SkLearn - model is saving to the provided bucket, and not the complete provided path","<p>I am very new to SageMaker, and I am following the SkLearn example from this link:</p>
<p><a href=""https://github.com/aws/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/scikit_learn_inference_pipeline/Inference%20Pipeline%20with%20Scikit-learn%20and%20Linear%20Learner.ipynb"" rel=""nofollow noreferrer"">https://github.com/aws/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/scikit_learn_inference_pipeline/Inference%20Pipeline%20with%20Scikit-learn%20and%20Linear%20Learner.ipynb</a></p>
<p>I wanted to save my model in a specified S3, hence, I updated the estimator code as follow:</p>
<pre><code>from sagemaker.sklearn.estimator import SKLearn

FRAMEWORK_VERSION = &quot;0.23-1&quot;
script_path = &quot;sklearn_abalone_featurizer.py&quot;

sklearn_preprocessor = SKLearn(
    entry_point=script_path,
    role=role,
    framework_version=FRAMEWORK_VERSION,
    instance_type=&quot;ml.c4.xlarge&quot;,
    sagemaker_session=sagemaker_session,
    output_path = &quot;s3://aws-prod01-sandbox/sklearnModel/model/&quot;
)
</code></pre>
<p>However, the model is being saved to s3//aws-prod01-sandbox/sagemaker-scikit-learn-2021-07-08/source/sourcedir.tar.gz instead. It is not taking the key that I have provided in the estimator.</p>
<p>Any help would be appreciated.</p>
<p>Thank you!</p>
<p>**I found the solution to this problem. Just in case anyone has the same issue, update the estimator as follow:</p>
<pre><code>sklearn_preprocessor = SKLearn(
    entry_point=script_path,
    role=role,
    framework_version=FRAMEWORK_VERSION,
    instance_type=&quot;ml.c4.xlarge&quot;,
    sagemaker_session=sagemaker_session,
    output_path = &quot;s3://aws-prod01-sandbox/sklearnModel/model/&quot;,
    code_location = &quot;s3://aws-prod01-sandbox/sklearnModel/model/&quot;
)
</code></pre>",0,4,2021-07-08 18:35:27.140000 UTC,,2021-07-10 18:02:32.307000 UTC,2,amazon-web-services|scikit-learn|amazon-sagemaker,296,2021-07-08 18:23:10.703000 UTC,2021-11-24 05:03:45.880000 UTC,,21,0,0,0,,,,,,['amazon-sagemaker']
How to use an existing machine learning model with Azure Machine Learning?,"<p>I have a Keras ML model .h5 file that I would like to publish as a web-service. This model was created in databricks. I want to use Azure ML for this purpose.
I am following the steps given in this Azure documentation -
<a href=""https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-existing-model"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-existing-model</a></p>

<p>One of the prerequisites is to have ""Azure Machine Learning SDK"".</p>

<p>My question is how to install ""Azure Machine Learning SDK"" in my Azure ml workspace? Do I need to type the commands in the Cloud Shell? </p>

<p>Any pointer would be helpful. Thanks.</p>",1,0,2020-02-01 00:02:21.260000 UTC,,,1,azure|machine-learning|azure-machine-learning-service,273,2016-01-21 02:29:13.593000 UTC,2022-09-14 19:49:37.230000 UTC,"Phoenix, AZ, USA",445,44,1,66,,,,,,['azure-machine-learning-service']
Cannot use tensorboard with Vertex AI Custom job,"<p>I'm trying to launch a custom training job using Vertex AI through <a href=""https://github.com/deepmind/xmanager"" rel=""nofollow noreferrer"">XManager</a>. When running Custom jobs with tensorboard enabled I get a tensorboard instance in <code>experiments -&gt; tensorboard instances</code> and a button on the custom job page that says <code>OPEN TENSORBOARD</code>. However, this leads to an empty page that says <code>Not found: TensorboardExperiment</code>.</p>
<ul>
<li>I observed this behaviour when running my own custom job and when running XManager's example <a href=""https://github.com/deepmind/xmanager/tree/main/examples/cifar10_tensorflow"" rel=""nofollow noreferrer"">cifar10_tensorflow</a>. Note that in both cases the job runs to completion without problems.</li>
<li>I can visualise the logs locally via the standard tensorboard package and passing as <code>log_dir</code> the cloud storage directory containing the experiments logs.</li>
<li>I can upload experiment logs to Vertex AI tensorboard manually using</li>
</ul>
<pre><code>tb-gcp-uploader --tensorboard_resource_name \
  TENSORBOARD_INSTANCE_NAME \
  --logdir=LOG_DIR \
  --experiment_name=TB_EXPERIMENT_NAME --one_shot=True
</code></pre>
<ul>
<li>For more details check out the discussion: <a href=""https://github.com/deepmind/xmanager/issues/15"" rel=""nofollow noreferrer"">https://github.com/deepmind/xmanager/issues/15</a></li>
</ul>",0,1,2022-04-05 17:04:41.173000 UTC,,2022-04-05 17:39:30.237000 UTC,1,tensorboard|google-cloud-ml|google-cloud-vertex-ai,216,2016-01-04 14:55:18.690000 UTC,2022-08-25 11:51:00.770000 UTC,,11,0,0,5,,,,,,['google-cloud-vertex-ai']
I can not register a model in my Azure ml experiment using run context,"<p>I am trying to register a model  inside one of my azure ml  experiments. I am able to register it via <code>Model.register</code> but not via <code>run_context.register_model</code></p>
<p>This are the two code sentences I use. The commented one is the one that fails</p>
<pre><code>learn.path = Path('./outputs').absolute()
Model.register(run_context.experiment.workspace, &quot;outputs/login_classification.pkl&quot;,&quot;login_classification&quot;, tags=metrics)
run_context.register_model(&quot;login_classification&quot;, &quot;outputs/login_classification.pkl&quot;, tags=metrics)
</code></pre>
<p>I received the next error:</p>
<pre><code>Message: Could not locate the provided model_path outputs/login_classification.pkl
</code></pre>
<p>But model is stored in this path:</p>
<p><a href=""https://i.stack.imgur.com/MNojQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MNojQ.png"" alt=""enter image description here"" /></a></p>",1,0,2022-06-01 15:01:58.640000 UTC,,,0,azure-machine-learning-service,86,2011-12-01 10:02:27.387000 UTC,2022-09-23 12:26:18.907000 UTC,,729,44,1,85,,,,,,['azure-machine-learning-service']
Is there way to fix CSV reading in Russian Language in Azure ML Studio?,"<p>I have a large csv file containing some text in Russian language. When I upload it to Azure ML Studio as dataset, it appears like ""����"". What I can do to fix that problem?</p>

<p>I tried changing encoding of my text to UTF8, KOI8-R.</p>

<p>There is no code, but I can share part of the dataset for you to try.</p>",1,2,2019-08-13 04:09:03.940000 UTC,,,1,azure|azure-machine-learning-studio,141,2019-08-13 04:01:58.750000 UTC,2019-08-26 05:12:07.907000 UTC,"Nur-Sultan, Kazakhstan",11,0,0,6,,,,,,['azure-machine-learning-studio']
Can't edit the cluster created by mlflow model serving,"<p>I'm trying to deploy  Machine learning model into databricks production using mlflow. while in that process, I have registered the model to mlflow models. After that it created the cluster but then it was in pending state forever. when I checked the model events, I see a problem with https proxy, we have global init scripts which contain proxy information.</p>
<p><a href=""https://i.stack.imgur.com/qF9MT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qF9MT.png"" alt=""enter image description here"" /></a></p>
<p>Ref: <a href=""https://docs.databricks.com/applications/mlflow/model-serving.html"" rel=""nofollow noreferrer"">https://docs.databricks.com/applications/mlflow/model-serving.html</a></p>
<p>so the only way for us to edit the cluster and add them but in that process we are getting an error &quot;error: Cannot edit cluster created by ModelServing&quot;.</p>
<pre><code>[Errno 101] Network is unreachable',)': /simple/mlflow/ WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('&lt;pip._vendor.urllib3.connection.HTTPSConnection object at 0x7f258247f710&gt;: Failed to establish a new connection:
</code></pre>
<p>In the &quot;Model Events page&quot;, I see the above logs,</p>",1,0,2022-05-10 23:07:09.100000 UTC,,,0,databricks|azure-databricks|mlflow,125,2016-11-15 06:12:07.737000 UTC,2022-08-15 17:25:10.400000 UTC,"R G U K T , basar, Andhra Pradesh, India",2470,265,22,251,,,,,,['mlflow']
How do I resolve a SQL ParseError in PySpark?,"<p>I'm a new-bee to PySpark and AWS Sagemaker using Jupyter Notebook. I do know how to write SQL statements to answer my questions. This code piece is supposed to:
1. Extract available methods of death dispositions in my dataset (CDC Death Data -in CSV) by year
2. Count the frequency for each disposition by year</p>

<p>I was able to run the SQL statement on the same dataset in a MySQL database. But once I added the query to my PySpark code, I got a <code>ParseError</code> Please, see the error below. </p>

<p>How would I go about resolving this error? and if I want to create a graph/plot with the output, how would I go about it?
THANKS</p>

<pre><code>df.registerTempTable(""data"")
methods = sqlContext.sql(""\
    SELECT current_data_year AS Year, \
        CASE method_of_disposition \
            WHEN 'C' THEN 'Cremation' \
            WHEN 'B' THEN 'Burial' \
            WHEN 'D' THEN 'D' \
            WHEN 'E' THEN 'E' \
            WHEN 'O' THEN 'O' \
            WHEN 'R' THEN 'R' \
            WHEN 'U' THEN 'Unknown' \
            END AS 'Method of Disposition', \
        COUNT(method_of_disposition) AS Count \
        FROM data \
        GROUP BY current_data_year, method_of_disposition\
    "").show()
</code></pre>

<p>MY NEW OUTPUT</p>

<pre><code>+----+-------------------+-------+
|Year|MethodofDisposition|  Count|
+----+-------------------+-------+
|   0|               null|     10|
|2005|              Other|   2199|
|2005|           Donation|   4795|
|2005|                  E|  21247|
|2005|     RemovedFromUSA|  31954|
|2005|          Cremation| 350018|
|2005|             Burial| 553202|
|2005|            Unknown|1489091|
|2006|              Other|   2252|
|2006|           Donation|   6883|
|2006|                  E|  23412|
|2006|     RemovedFromUSA|  40870|
|2006|          Cremation| 423282|
|2006|             Burial| 667169|
|2006|            Unknown|1266857|
|2007|              Other|   3119|
|2007|           Donation|   8719|
|2007|                  E|  26139|
|2007|     RemovedFromUSA|  41411|
|2007|          Cremation| 472220|
|2007|             Burial| 725666|
|2007|            Unknown|1151069|
|2008|              Other|   5511|
|2008|           Donation|  10981|
|2008|                  E|  31913|
|2008|     RemovedFromUSA|  44713|
|2008|          Cremation| 579827|
|2008|             Burial| 866384|
|2008|            Unknown| 937482|
|2009|              Other|   3688|
|2009|           Donation|  12011|
|2009|                  E|  30344|
|2009|     RemovedFromUSA|  45451|
|2009|          Cremation| 599202|
|2009|             Burial| 802305|
|2009|            Unknown| 948218|
|2010|              Other|   3782|
|2010|           Donation|  15208|
|2010|                  E|  32807|
|2010|     RemovedFromUSA|  47899|
|2010|          Cremation| 706224|
|2010|            Unknown| 760192|
|2010|             Burial| 906430|
|2011|              Other|   5169|
|2011|           Donation|  17450|
|2011|                  E|  33847|
|2011|     RemovedFromUSA|  47199|
|2011|            Unknown| 685325|
|2011|          Cremation| 780480|
|2011|             Burial| 950372|
|2012|              Other|   6649|
|2012|           Donation|  20790|
|2012|                  E|  35110|
|2012|     RemovedFromUSA|  52896|
|2012|            Unknown| 440569|
|2012|          Cremation| 898222|
|2012|             Burial|1093628|
|2013|              Other|   6962|
|2013|           Donation|  21653|
|2013|                  E|  36949|
|2013|     RemovedFromUSA|  53678|
|2013|            Unknown| 395080|
|2013|          Cremation| 973768|
|2013|             Burial|1113362|
|2014|              Other|   7871|
|2014|           Donation|  24004|
|2014|                  E|  39321|
|2014|     RemovedFromUSA|  59884|
|2014|            Unknown| 242963|
|2014|          Cremation|1094292|
|2014|             Burial|1162836|
|2015|              Other|  11729|
|2015|           Donation|  27870|
|2015|                  E|  40880|
|2015|     RemovedFromUSA|  71744|
|2015|            Unknown|  74050|
|2015|          Cremation|1244297|
|2015|             Burial|1247628|
+----+-------------------+-------+
</code></pre>

<p>ERROR MESSAGE</p>

<pre><code>---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
~/anaconda3/envs/python3/lib/python3.6/site-packages/pyspark/sql/utils.py in deco(*a, **kw)
     62         try:
---&gt; 63             return f(*a, **kw)
     64         except py4j.protocol.Py4JJavaError as e:

~/anaconda3/envs/python3/lib/python3.6/site-packages/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    318                     ""An error occurred while calling {0}{1}{2}.\n"".
--&gt; 319                     format(target_id, ""."", name), value)
    320             else:

Py4JJavaError: An error occurred while calling o19.sql.
: org.apache.spark.sql.catalyst.parser.ParseException: 
extraneous input ''Method of Disposition'' expecting {&lt;EOF&gt;, ',', 'FROM', 'WHERE', 'GROUP', 'ORDER', 'HAVING', 'LIMIT', 'LATERAL', 'WINDOW', 'UNION', 'EXCEPT', 'MINUS', 'INTERSECT', 'SORT', 'CLUSTER', 'DISTRIBUTE'}(line 1, pos 213)

== SQL ==
SELECT current_data_year AS Year, CASE method_of_disposition WHEN 'C' THEN 'Cremation' WHEN 'B' THEN 'Burial' WHEN 'D' THEN 'D' WHEN 'E' THEN 'E' WHEN 'O' THEN 'O' WHEN 'R' THEN 'R' WHEN 'U' THEN 'Unknown' END AS 'Method of Disposition', COUNT(method_of_disposition) AS Count FROM data GROUP BY current_data_year, method_of_disposition
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------^^^

    at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:217)
    at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:114)
    at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:48)
    at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:68)
    at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:632)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
    at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
    at py4j.Gateway.invoke(Gateway.java:280)
    at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
    at py4j.commands.CallCommand.execute(CallCommand.java:79)
    at py4j.GatewayConnection.run(GatewayConnection.java:214)
    at java.lang.Thread.run(Thread.java:745)


During handling of the above exception, another exception occurred:

ParseException                            Traceback (most recent call last)
&lt;ipython-input-7-f99c8a5b941c&gt; in &lt;module&gt;()
      1 #Grouping and counting Cremation vs Burial by Year
      2 df.registerTempTable(""data"")
----&gt; 3 sqlContext.sql(""SELECT current_data_year AS Year, CASE method_of_disposition WHEN 'C' THEN 'Cremation' WHEN 'B' THEN 'Burial' WHEN 'D' THEN 'D' WHEN 'E' THEN 'E' WHEN 'O' THEN 'O' WHEN 'R' THEN 'R' WHEN 'U' THEN 'Unknown' END AS 'Method of Disposition', COUNT(method_of_disposition) AS Count FROM data GROUP BY current_data_year, method_of_disposition"").show()

~/anaconda3/envs/python3/lib/python3.6/site-packages/pyspark/sql/context.py in sql(self, sqlQuery)
    382         [Row(f1=1, f2=u'row1'), Row(f1=2, f2=u'row2'), Row(f1=3, f2=u'row3')]
    383         """"""
--&gt; 384         return self.sparkSession.sql(sqlQuery)
    385 
    386     @since(1.0)

~/anaconda3/envs/python3/lib/python3.6/site-packages/pyspark/sql/session.py in sql(self, sqlQuery)
    601         [Row(f1=1, f2=u'row1'), Row(f1=2, f2=u'row2'), Row(f1=3, f2=u'row3')]
    602         """"""
--&gt; 603         return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)
    604 
    605     @since(2.0)

~/anaconda3/envs/python3/lib/python3.6/site-packages/py4j/java_gateway.py in __call__(self, *args)
   1131         answer = self.gateway_client.send_command(command)
   1132         return_value = get_return_value(
-&gt; 1133             answer, self.gateway_client, self.target_id, self.name)
   1134 
   1135         for temp_arg in temp_args:

~/anaconda3/envs/python3/lib/python3.6/site-packages/pyspark/sql/utils.py in deco(*a, **kw)
     71                 raise AnalysisException(s.split(': ', 1)[1], stackTrace)
     72             if s.startswith('org.apache.spark.sql.catalyst.parser.ParseException: '):
---&gt; 73                 raise ParseException(s.split(': ', 1)[1], stackTrace)
     74             if s.startswith('org.apache.spark.sql.streaming.StreamingQueryException: '):
     75                 raise StreamingQueryException(s.split(': ', 1)[1], stackTrace)

ParseException: ""\nextraneous input ''Method of Disposition'' expecting {&lt;EOF&gt;, ',', 'FROM', 'WHERE', 'GROUP', 'ORDER', 'HAVING', 'LIMIT', 'LATERAL', 'WINDOW', 'UNION', 'EXCEPT', 'MINUS', 'INTERSECT', 'SORT', 'CLUSTER', 'DISTRIBUTE'}(line 1, pos 213)\n\n== SQL ==\nSELECT current_data_year AS Year, CASE method_of_disposition WHEN 'C' THEN 'Cremation' WHEN 'B' THEN 'Burial' WHEN 'D' THEN 'D' WHEN 'E' THEN 'E' WHEN 'O' THEN 'O' WHEN 'R' THEN 'R' WHEN 'U' THEN 'Unknown' END AS 'Method of Disposition', COUNT(method_of_disposition) AS Count FROM data GROUP BY current_data_year, method_of_disposition\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------^^^\n""
</code></pre>

<p>Sample MYSQL OUTPUT
<a href=""https://i.stack.imgur.com/Jqq1I.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Jqq1I.png"" alt=""enter image description here""></a></p>",0,5,2018-12-09 18:03:31.580000 UTC,,2018-12-09 23:31:13.427000 UTC,1,python|apache-spark|jupyter-notebook|pyspark-sql|amazon-sagemaker,1911,2017-06-12 15:24:21.713000 UTC,2019-07-24 07:17:14.217000 UTC,Washing DC,67,7,0,9,,,,,,['amazon-sagemaker']
Calling azure machine learning service pipeline externally without python sdk,"<p>I have published a pipeline created inside azure machine learning service workspace, and i have its rest endpoint with me.
I also have Service principal Id and secrets which have contributor access over the workspace.
I am trying to invoke this pipeline thorough its rest endpoint from adf using the SPn id and secret.
But i am getting forbidden error, is there a guideline how to schive it without using python SDKs.</p>",1,1,2019-09-26 08:14:22.490000 UTC,,,0,azure|azure-data-factory|azure-data-factory-2|azure-machine-learning-service,199,2017-10-22 09:05:10.973000 UTC,2020-11-06 16:28:37.523000 UTC,,55,1,0,16,,,,,,['azure-machine-learning-service']
AWS S3 ValueError: Integer column has NA values in column 63,"<p>When I am attempting to pull the content of the files into a CSV file, I get this error: ValueError: Integer column has NA values in column 63. The strange thing is that this error was not occurring before, i.e. I didn't not touch this code and 1 day it was working and the next it wasn't. This inconsistent behavior is the issue and I am not sure what to do.</p>
<p>I hope someone knows that the issue is.
Thanks!
<a href=""https://i.stack.imgur.com/pxbQH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pxbQH.png"" alt=""enter image description here"" /></a></p>",0,3,2020-08-31 23:01:13.180000 UTC,,,1,python|python-3.x|amazon-web-services|amazon-s3|amazon-sagemaker,78,2017-04-24 21:09:06.483000 UTC,2022-02-12 05:45:53.177000 UTC,,425,8,0,225,,,,,,['amazon-sagemaker']
What is real-time inference pipeline?,"<p>From Azure Machine Learning designer, to deploy a real-time inference pipeline as a service for others to consume, you must deploy the model to an Azure Kubernetes Service (AKS).
What is real-time inference pipeline ?</p>",0,0,2021-04-25 15:56:05.967000 UTC,1.0,,1,azure|azure-machine-learning-studio|azure-machine-learning-service,211,2021-01-15 12:54:54.930000 UTC,2022-04-03 14:59:06.363000 UTC,United States,306,200,2,53,,,,,,"['azure-machine-learning-studio', 'azure-machine-learning-service']"
Azure ML Workbench File from Blob,"<p>When trying to reference/load a dsource or dprep file generated with a data source file from blob storage, I receive the error ""No files for given path(s)"".</p>

<p>Tested with .py and .ipynb files.  Here's the code:</p>

<pre><code># Use the Azure Machine Learning data source package
from azureml.dataprep import datasource

df = datasource.load_datasource('POS.dsource') #Error generated here

# Remove this line and add code that uses the DataFrame
df.head(10)
</code></pre>

<p>Please let me know what other information would be helpful. Thanks!</p>",2,0,2017-10-30 18:09:10.993000 UTC,,2017-11-22 00:28:16.380000 UTC,4,azure|machine-learning|azure-blob-storage|azure-machine-learning-workbench,324,2016-04-19 18:30:53.860000 UTC,2019-04-30 19:00:43.910000 UTC,,41,1,0,4,,,,,,['azure-machine-learning-workbench']
How do I log a confusion matrix into Wanddb?,"<p>I'm using pytorch lightning, and at the end of each epoch, I create a confusion matrix from torchmetrics.ConfusionMatrix (see code below). I would like to log this into Wandb, but the Wandb confusion matrix logger only accepts y_targets and y_predictions. Does anyone know how to extract the updated confusion matrix y_targets and y_predictions from a confusion matrix, or alternatively give Wandb my updated confusion matrix in a way that it can be processed into eg a heatmap within wandb?</p>
<pre><code>class ClassificationTask(pl.LightningModule):
    def __init__(self, model, lr=1e-4, augmentor=augmentor):
        super().__init__()
        self.model = model
        self.lr = lr
        self.save_hyperparameters() #not being used at the moment, good to have ther in the future
        self.augmentor=augmentor
        
        self.matrix = torchmetrics.ConfusionMatrix(num_classes=9)
        
        self.y_trues=[]
        self.y_preds=[]
        
    def training_step(self, batch, batch_idx):
        x, y = batch
        x=self.augmentor(x)#.to('cuda')
        y_pred = self.model(x)
        loss = F.cross_entropy(y_pred, y,)  #weights=class_weights_tensor
        

        acc = accuracy(y_pred, y)
        metrics = {&quot;train_acc&quot;: acc, &quot;train_loss&quot;: loss}
        self.log_dict(metrics)
        return loss
    
    def validation_step(self, batch, batch_idx):
        loss, acc = self._shared_eval_step(batch, batch_idx)
        metrics = {&quot;val_acc&quot;: acc, &quot;val_loss&quot;: loss, }
        self.log_dict(metrics)
        return metrics
    
    def _shared_eval_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self.model(x)
        loss = F.cross_entropy(y_hat, y)
        acc = accuracy(y_hat, y)
        self.matrix.update(y_hat,y)
        return loss, acc
    
    def validation_epoch_end(self, outputs):
        confusion_matrix = self.matrix.compute()
        wandb.log({&quot;my_conf_mat_id&quot; : confusion_matrix})
        
    def configure_optimizers(self):
        return torch.optim.Adam((self.model.parameters()), lr=self.lr)
</code></pre>",1,0,2022-07-25 12:14:57.303000 UTC,1.0,,2,pytorch|confusion-matrix|pytorch-lightning|wandb,109,2021-07-22 12:49:39.237000 UTC,2022-09-23 08:34:07.490000 UTC,,31,1,0,2,,,,,,['wandb']
Increase the number of rows selected in Azure ML,"<p>I have created a predictive model with AML. The thing is when i execute r component only 1567 rows(result dataset) are selected from the dataset input (it contains around 85000 rows). I want to include at least 7000 for the training and scoring but i am not sure how to do it. </p>

<p>Input Dataset:
<a href=""https://i.stack.imgur.com/bmonN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bmonN.png"" alt=""Input Dataset""></a></p>

<p>Result Dataset
<a href=""https://i.stack.imgur.com/MWofY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MWofY.png"" alt=""Result Dataset""></a></p>

<p>Model</p>

<p><a href=""https://i.stack.imgur.com/Rbr0k.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Rbr0k.png"" alt=""AML model""></a></p>

<p>Thanks in Advance.</p>

<pre><code># Map 1-based optional input ports to variables
dataset &lt;- maml.mapInputPort(1) # class: data.frame

dataset$Capacidad &lt;- as.numeric(dataset$Capacidad)

resource &lt;- list()
output_forecast &lt;- data.frame()


datasource &lt;- data.frame(Fecha = character(0), Delegacion = character(0), Grupo_recurso = character(0), IDRecurso = character(0), Numero_proyectos = numeric(0), Cantidad = numeric(0), Capacidad = numeric(0), Productividad = numeric(0))


addToDatasource &lt;- function(datasource_data_frame, fecha, delegacion, grupo_recurso, id_recurso, numero_proyectos, cantidad, capacidad, productividad){

  new_data_frame &lt;- data.frame(Fecha = fecha, Delegacion = delegacion, Grupo_recurso = grupo_recurso, IDRecurso = id_recurso, Numero_proyectos = numero_proyectos, Cantidad = cantidad, Capacidad = capacidad, Productividad = productividad)

  return(rbind(datasource_data_frame, new_data_frame))
}

 resource_data &lt;- dataset[!(dataset$Grupo_recurso %in% c(""ADMIN"", 
 ""DIRECTOR"", ""EXTERNO"", ""GERENTE"", ""RESP OPERACIONES"", 

 ""MARKETING"", ""TELEMARKETING"")), ]

resource_list &lt;- unique(resource_data[(resource_data$Activo == 1), 
""IDRecurso""])
resource_list &lt;- resource_list[!(resource_list %in% c(""AFH"", ""BSS"", ""EDC"",""GLM"", ""GJ"", ""GPV""))]



for(i in 1:length(resource_list)){

  dataset_res &lt;- dataset[(dataset$IDRecurso %in% resource_list[i]),]

  dataset_res$Fecha &lt;- format(as.Date(dataset_res$Fecha), ""%m-%Y"")

  date_list &lt;- unique(dataset_res$Fecha)
    for(j in 1:length(date_list)){

    dataset_date &lt;- dataset_res[(dataset_res$Fecha == date_list[j]),]

    #Number of projects calculation
    number_projects &lt;- length(unique(dataset_date$Proyecto))

    if(sum(dataset_date$Capacidad) &gt; 0){

  #Productivity calculation
  productivity &lt;- sum(dataset_date$Cantidad_productiva)/sum(dataset_date$Capacidad)

  datasource &lt;- addToDatasource(datasource, date_list[j], unique(dataset_date$Delegacion), unique(dataset_res$Grupo_recurso), resource_list[i], number_projects, sum(dataset_date$Cantidad), sum(dataset_date$Capacidad), productivity)
}
</code></pre>

<p>}
}</p>

<h1>Select data.frame to be sent to the output Dataset port</h1>

<p>maml.mapOutputPort(""datasource"");</p>",0,8,2018-08-23 09:10:56.747000 UTC,,2018-08-23 10:32:30.713000 UTC,0,r|azure|prediction|azure-machine-learning-studio,60,2016-10-11 21:15:32.123000 UTC,2019-01-10 10:40:30.757000 UTC,"Barcelona, España",49,1,0,21,,,,,,['azure-machine-learning-studio']
How to remotely launch a Juypter Notebook within AWS Sagemaker using AWS Lambda,"<p>I have a Juypter Notebook set up within AWS Sagemaker. I wanted to find a way to launch this notebook on an autonomous trigger when a new file is uploaded to a certain folder (hence AWS Lambda). I was looking for if there was a streamlined way to trigger a Juypter Notebook with an AWS Lambda trigger.  </p>

<p>I have looked into using API and turning Sagemaker into and endpoint, but it didnt work.</p>

<p>*edit Sorry if the question was a little vague. I have allot of code written in this notebook on in Juypter. What i was ideally looking for was, when a file is uploaded to ""RandomFile"" then the code within the notebook will run. I was looking to do this with AWS Lambda by setting up a S3 based trigger. </p>",1,3,2019-08-26 18:45:01.027000 UTC,1.0,2019-08-26 18:56:21.093000 UTC,2,python-3.x|amazon-web-services|aws-lambda|amazon-sagemaker,1612,2019-06-26 19:31:33.757000 UTC,2022-08-26 13:57:10.100000 UTC,"New Jersey, USA",21,0,0,2,,,,,,['amazon-sagemaker']
How to link aws sagemaker with a web app?,"<p>I have a endpoint up and running in AWS Sagemaker. However, I'm not sure how to send data to this endpoint and get back a prediction. </p>

<p>The documentation is also not clear on this. Any help would be appreciated.</p>",3,0,2018-03-11 08:56:57.133000 UTC,,,3,amazon-machine-learning|amazon-sagemaker,2097,2015-01-25 18:26:40.603000 UTC,2022-09-15 13:02:40.407000 UTC,"Gurgaon, Haryana, India",719,31,1,172,,,,,,['amazon-sagemaker']
Reverse-lookup words based on word vectors using amazon sagemaker / blazingtext model,"<p>After training an deploying a model using blazingtext algorithm ... it is straightforward to obtain word-vectors for given words.</p>

<p>as demoed in this example -- <a href=""https://github.com/awslabs/amazon-sagemaker-examples/tree/master/introduction_to_amazon_algorithms/blazingtext_word2vec_subwords_text8"" rel=""nofollow noreferrer"">https://github.com/awslabs/amazon-sagemaker-examples/tree/master/introduction_to_amazon_algorithms/blazingtext_word2vec_subwords_text8</a></p>

<p>we can call the sagemaker.RealTimePredictor.predict function with the given word as the payload data and it will return the word vector</p>

<p>Now I want to be able to implement some basic NLP use cases like ... suggest a similar word, or Man-Woman+King = Queen </p>

<p>(for a different implementation using gensim see <a href=""https://rare-technologies.com/word2vec-tutorial/"" rel=""nofollow noreferrer"">https://rare-technologies.com/word2vec-tutorial/</a>)</p>

<p>To do this I would need to be able to call a function that takes a vector as input and returns the matching/nearest word/s from the embedding.</p>

<p>Is this even possible using the sagemaker sdk? I spent hours looking through the sagemaker api documentation (which was not easy to find or navigate) and I am still not sure if this is even possible</p>",3,1,2018-10-12 00:49:57.127000 UTC,,,0,amazon-sagemaker,592,2015-05-21 03:34:16.930000 UTC,2022-08-28 06:46:14.863000 UTC,,17,1,0,10,,,,,,['amazon-sagemaker']
Understanding AWS Sagemaker limits and thier requests,"<p>I want to know that is their any console where I can easily see, that what is my current limit of different resources,eg like ml.p2.2xlarge.
Also , I am not getting that , if I am training my model in sagemaker on instance:'ml.c4.xlarge', now if I got limit error, now I should request increase of what: EC2 instance, notebook instance.....
Please guide me thoroughly through this.</p>",2,1,2020-09-11 07:56:10.223000 UTC,,,1,amazon-web-services|amazon-sagemaker,1560,2020-04-30 04:59:25.643000 UTC,2021-06-05 00:13:26.343000 UTC,,21,0,0,3,,,,,,['amazon-sagemaker']
Azure HDInsight - Resource u'tokenizers/punkt/english.pickle' not found,"<p>I imported nltk package. I need to use nltk.sent_tokenize and nltk.word_tokenize and when I do, I get the following error no matter what:</p>

<pre><code>An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.3 in stage 1.0 (TID 7, 10.0.0.4): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File ""/usr/hdp/current/spark-client/python/lib/pyspark.zip/pyspark/worker.py"", line 111, in main
    process()
  File ""/usr/hdp/current/spark-client/python/lib/pyspark.zip/pyspark/worker.py"", line 106, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File ""/usr/hdp/current/spark-client/python/lib/pyspark.zip/pyspark/serializers.py"", line 263, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File ""&lt;stdin&gt;"", line 2, in &lt;lambda&gt;
  File ""/usr/bin/anaconda/lib/python2.7/site-packages/nltk/tokenize/__init__.py"", line 85, in sent_tokenize
    tokenizer = load('tokenizers/punkt/{0}.pickle'.format(language))
  File ""/usr/bin/anaconda/lib/python2.7/site-packages/nltk/data.py"", line 781, in load
    opened_resource = _open(resource_url)
  File ""/usr/bin/anaconda/lib/python2.7/site-packages/nltk/data.py"", line 895, in _open
    return find(path_, path + ['']).open()
  File ""/usr/bin/anaconda/lib/python2.7/site-packages/nltk/data.py"", line 624, in find
    raise LookupError(resource_not_found)
LookupError: 
**********************************************************************
  Resource u'tokenizers/punkt/english.pickle' not found.  Please
  use the NLTK Downloader to obtain the resource:  &gt;&gt;&gt;
  nltk.download()
  Searched in:
    - '/home/nltk_data'
    - '/usr/share/nltk_data'
    - '/usr/local/share/nltk_data'
    - '/usr/lib/nltk_data'
    - '/usr/local/lib/nltk_data'
    - u''
**********************************************************************
</code></pre>

<p>I've referred to many posts discussing about this topic and tried 
nltk.download(all') , -d, arranging the sub-folders as well.</p>

<p>Azure ML's python doesn't come with nltk library. There should be some other way to use nltk on this platform. Please help!!
Thank you!</p>",1,8,2016-04-19 04:37:19.480000 UTC,2.0,2017-01-04 09:47:28.820000 UTC,0,python|nltk|azure-hdinsight|azure-machine-learning-studio,945,2015-06-20 23:37:22.967000 UTC,2016-09-06 02:21:52.617000 UTC,,299,12,0,34,,,,,,['azure-machine-learning-studio']
Deploying custom preprocessing and postprocessing scripts in SageMaker,"<p>I am trying to convert some python scripts into a callable endpoint in SageMaker. My preprocessing(feature engineering) and postprocessing scripts are written in python and have a few interdependent scripts and methods in them. The preprocessing steps are also not necessarily from SKLearn, they are customized functions and need to be called from the preprocessing endpoint every time on the raw data that will then be used for prediction using a model saved as a second endpoint. The third endpoint will be for the postprocessing steps and connecting these 3 endpoints we want to get our data from the raw format to the output format every time.</p>
<p>We currently have normal python scripts that preprocesses the data using some highly customized functions( all features are ultimately derived features) and then performs some inference and then again using some highly customized postprocessing generates the final results. While the input is a CSV file, after each stage of preprocessing and post processing the dimensions of the data and also the format of the output(dataframe, list, list of lists) are likely to change.</p>
<p>For reference, we are using, <a href=""https://github.com/aws/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/scikit_learn_iris/scikit_learn_estimator_example_with_batch_transform.ipynb"" rel=""nofollow noreferrer"">https://github.com/aws/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/scikit_learn_iris/scikit_learn_estimator_example_with_batch_transform.ipynb</a> and <a href=""https://github.com/aws/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/scikit_learn_inference_pipeline/Inference%20Pipeline%20with%20Scikit-learn%20and%20Linear%20Learner.ipynb"" rel=""nofollow noreferrer"">https://github.com/aws/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/scikit_learn_inference_pipeline/Inference%20Pipeline%20with%20Scikit-learn%20and%20Linear%20Learner.ipynb</a>.</p>
<p>Please let me know if there is any better reference that caters to our specific requirements.</p>",1,0,2021-07-23 16:21:53.750000 UTC,,,0,amazon-web-services|endpoint|amazon-sagemaker,543,2021-07-13 16:28:54.377000 UTC,2022-03-21 06:24:34.500000 UTC,,21,1,0,5,,,,,,['amazon-sagemaker']
Packages R Azure Machine Learning,"<p>I'm new in Azure and I would like to run my R Code, I read how it's work but I have problem to run my packages but I always get errors like this . </p>

<blockquote>
  <p>package 'ParamHelpers' required by 'mlr' could not be found</p>
</blockquote>

<p>And when I run in Rstudio i don't have the same errors </p>

<p>I have a zip file with inside the zip packages </p>

<pre><code>install.packages(""src/Metrics_0.1.1.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/checkmate_1.7.0.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/mlr_2.7.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/xgboost_0.4-2.zip"", lib = ""."", repos = NULL, verbose = TRUE)
install.packages(""src/BBmisc_1.9.zip"", lib = ""."", repos = NULL, verbose = TRUE)

library(xgboost)
library(Metrics, lib.loc=""."", verbose=TRUE)
library(checkmate, lib.loc=""."", verbose=TRUE)
library(BBmisc, lib.loc=""."", verbose=TRUE)
library(mlr, lib.loc=""."", verbose=TRUE)
library(Hmisc)
packageVersion(""mlr"")
</code></pre>",2,0,2016-02-03 12:20:20.573000 UTC,,,1,r|azure|azure-machine-learning-studio,685,2014-02-25 13:08:01.930000 UTC,2017-10-22 13:10:27.147000 UTC,,305,5,0,34,,,,,,['azure-machine-learning-studio']
how to find difference in dates in pandas dataframe in Azure ML,"<p>Is Azure uses some other Syntax for finding difference in dates and time.<br>
or<br>
Any package is missing in Azure.<br>
how to find difference in dates in pandas data-frame in Azure ML.
<br>I have 2 columns in a dataframe and I have to find the difference of two and have to kept in third column ,the problem is this, all this runs well in python IDE but not in Microsoft Azure.<br>
My date format : <code>2015-09-25T01:45:34.372Z</code>
<br> I have to to find df['days'] = <code>df['a'] - df['b']</code><br>
I have tried almost all the syntax available on stackoverflow.<br>
Please help
<br></p>

<pre><code>mylist = ['app_lastCommunicatedAt', 'app_installAt', 'installationId']
</code></pre>

<p><br></p>

<pre><code>'def finding_dates(df, mylist):
        for i in mylist:
            if i == 'installationId':
                continue
            df[i] = [pd.to_datetime(e) for e in df[i]]
        df['days'] = abs((df[mylist[1]] - df[mylist[0]]).dt.days)
        return df'
</code></pre>

<p><br>
when I am calling this function it is giving error and not accepting lines below continue.
<br>
I had also tried many other things like converting dates to string, etc</p>",1,6,2016-06-08 08:02:16.787000 UTC,,2016-06-09 05:15:12.773000 UTC,0,python|azure|pandas|machine-learning|azure-machine-learning-studio,198,2016-06-02 05:53:24.997000 UTC,2016-07-07 13:12:26.720000 UTC,,21,0,0,11,,,,,,['azure-machine-learning-studio']
How to Deploy an auto ML application using REST API,"<p>I am working on building an api layer using flask for an auto ml application.</p>
<p>The application basically is built on DSVM sever and it is used to give the most important features affecting the issue using shap values by building xgboost model. so for every run the features are discarded automatically based on univariate, confounding and multivariate analysis and finally using remaining features xgboost determines the feature importance for those.</p>
<p>If I have to build an API layer for a normal model we would use a pickle file to deploy the model and the API is just used to get predictions from the model. Is there any way to deploy this application using REST so that the frontend can use them?</p>",0,1,2022-01-12 05:55:27.303000 UTC,,,0,flask|azure-devops-rest-api|azure-machine-learning-service|automl|azure-dsvm,96,2021-03-27 06:33:36.837000 UTC,2022-04-28 16:50:10.360000 UTC,,1,0,0,1,,,,,,['azure-machine-learning-service']
How to monitor Jupyterlab notebook in SageMaker Studio?,"<p>I am wondering if there is a way to monitor the logs of a SageMaker Studio notebook. The notebook instances (16GB of memory) are running fine but sometimes the kernel dies loading big datasets. The only output from the UI is that the kernel has changed from <code>busy</code> to <code>idle</code> but there is no way to actually see the logs of the running notebook's kernel and why it died.</p>
<p>I did the test by switching to a bigger instance (128GB of memory) and the dataset was loaded correctly. Running <code>df.info()</code> showed that the dataset was only using <strong>4.0GB</strong> of memory, which should be loaded fine with a 16GB instance.</p>
<p>So I would like to know if there is a way to monitor or see the running logs of the current notebook AND or if it is necessary to change an environment of the notebook to allow bigger datasets to be loaded without killing the kernel.</p>",1,0,2022-01-17 13:38:52.590000 UTC,,,0,python|amazon-web-services|jupyter-notebook|jupyter-lab|amazon-sagemaker,210,2016-09-20 21:05:35.647000 UTC,2022-09-23 23:14:44.783000 UTC,,329,32,5,28,,,,,,['amazon-sagemaker']
AWS Sagemaker Unable to read S3 data from certain regions,"<p>I have already uploaded my data to an AWS S3 bucket. I need to access the data in a Sagemaker notebook.</p>
<p>I made two notebook instances. One is in an &quot;East&quot; region while the other is &quot;West&quot;. When I try to access the data in the &quot;East&quot; region it pulls up okay. But when I try to access the data from the &quot;West&quot; it give me an <code>NoCredentialsError: Unable to locate credentials</code>.</p>
<p>How could one region work but the other fail on credentials?</p>
<p>In both cases this is the code I am using to access the data:</p>
<pre><code>import boto3
import pandas as pd
from sagemaker import get_execution_role
import matplotlib.pyplot as plt

bucket='my-bucket'
role = get_execution_role()

data_key = 'my_data.csv'
data_location = 's3://{}/{}'.format(bucket, data_key)
df = pd.read_csv(data_location)
</code></pre>",0,2,2020-07-06 14:50:46.827000 UTC,,,0,amazon-web-services|amazon-s3|jupyter-notebook|amazon-sagemaker,453,2020-05-13 14:21:21.357000 UTC,2020-08-19 14:59:21.137000 UTC,,1,0,0,3,,,,,,['amazon-sagemaker']
"ServerSelectionTimeoutError on connecting mongo atlas cluster, while importing mongo db data on azure machine learning studio","<p>I am trying to import data to Azure Machine Learning experiment using MongoDB database from mongoAtlas.</p>

<p>So for that I used Python script using Pymongo library to connect to my MongoDB host. I'm getting an error like:</p>

<pre><code>ServerSelectionTimeoutError: cluster-shard-00-.....mongodb.net:27017: ('The write operation timed out',)Process returned with non-zero exit code 1---------- End of error message from Python interpreter ---------- Process exited with error code -2
</code></pre>",0,0,2019-01-23 09:58:30.490000 UTC,,2019-01-23 10:12:50.143000 UTC,1,mongodb|azure|azure-machine-learning-studio,145,2019-01-23 09:48:35.880000 UTC,2021-07-22 13:34:53.553000 UTC,,21,0,0,4,,,,,,['azure-machine-learning-studio']
No module named bson even though uninstall and install again,"<p>I tried to import bson from a notebook on SageMaker with conda_python3 kernel but I have the error <code>No module named bson</code></p>

<p>I tried the following in the notebook:</p>

<pre><code>!python -m pip uninstall bson --yes
!python -m pip uninstall pymongo --yes
!pip install pymongo
</code></pre>

<p>But I still have the above error message. Any idea why? thanks!</p>",0,2,2020-05-19 17:24:19.907000 UTC,,,0,python|amazon-sagemaker,90,2018-12-18 09:28:42.213000 UTC,2021-08-26 13:36:32.667000 UTC,"Brussels, Belgium",1507,62,7,162,,,,,,['amazon-sagemaker']
Batch predictions Vertext AI,"<p>How do I create <code>JSONL</code> file which contains list of files in Google Cloud Bucket for Batch prediction in Vertex AI?
What I've tried so far.</p>
<ol>
<li>Get list of file from bucket and write it to a txt file
<code>gsutil ls gs://bucket/dir &gt; list.txt</code></li>
<li>Convert <code>list.txt</code> to <code>list.jsonl</code> following <a href=""https://cloud.google.com/vertex-ai/docs/predictions/batch-predictions?_ga=2.136818160.-1986884001.1629344951&amp;_gac=1.159283784.1630027944.Cj0KCQiAv8PyBRDMARIsAFo4wK2vd8OO5X9HFkii9knl6nSzFOK_srkD484THsck1a6r5rqonXafFkkaAgBgEALw_wcB#batch_request_input"" rel=""nofollow noreferrer"">Vertext AI docs</a>:</li>
</ol>
<pre><code>{&quot;content&quot;: &quot;gs://sourcebucket/datasets/images/source_image1.jpg&quot;, &quot;mimeType&quot;: &quot;image/jpeg&quot;}
{&quot;content&quot;: &quot;gs://sourcebucket/datasets/images/source_image2.jpg&quot;, &quot;mimeType&quot;: &quot;image/jpeg&quot;}
</code></pre>
<p>After create batch prediction, I got this error: <code>cannot be parsed as JSONL.</code>
How do I correct the format of this <code>JSONL</code> file?
Also, is there anyway to directly export list files in bucket to <code>JSONL</code> file format?</p>",1,0,2021-09-14 09:01:41.267000 UTC,,2021-09-14 09:12:49.777000 UTC,1,python|gsutil|google-cloud-automl|google-cloud-vertex-ai,247,2021-06-17 10:11:32.180000 UTC,2021-10-16 12:25:40.050000 UTC,,11,0,0,2,,,,,,['google-cloud-vertex-ai']
"""Batch execution failed with HTTP status code: BadGateway. The response from the Machine Learning service at endpoint","<p>I want to schedule my AzureML experiment by Azure Data Factory (ADF).  After passing my job from azureML pipeline, I face with this issue ""Batch execution failed with HTTP status code: BadGateway. The response from the Machine Learning service at endpoint <a href=""https://ussouthcentral.services.azureml.net/workspaces/26c276f8420a4c30aae39b0c27845134/services/7b122129b2c041f9b4399d7e4b16e927/jobs/job_id/start"" rel=""nofollow noreferrer"">https://ussouthcentral.services.azureml.net/workspaces/26c276f8420a4c30aae39b0c27845134/services/7b122129b2c041f9b4399d7e4b16e927/jobs/job_id/start</a> was 'Internal error occurred.""</p>

<p>Do you have any idea for solving this issue.
Thanks,</p>",1,0,2017-03-05 08:09:33.533000 UTC,,2017-03-05 08:20:37.913000 UTC,0,azure-data-factory|azure-machine-learning-studio,379,2017-02-02 23:51:25.687000 UTC,2019-04-10 01:12:00.667000 UTC,"Seattle, WA, United States",1,0,0,6,,,,,,['azure-machine-learning-studio']
Predictions on Xgboost DMatrix Object with XgBoostClassifier,"<p>I am working with a pretrained model on AWS SageMaker, </p>

<p>I need to provide a <code>predict_fn</code> which will do actual prediction. </p>

<pre><code>def predict_fn(request_body, model):
    prediction = model.predict(request_body)
    return prediction
</code></pre>

<p>This returns an error: <code>TypeError: no supported conversion for types: (dtype(‘O’),)</code></p>

<p>model is an object of <code>XGboostClassifier</code>, <code>requestbody</code> is an object of <code>xgboost.core.DMatrix</code></p>",1,0,2020-04-03 16:56:43.863000 UTC,,,0,xgboost|amazon-sagemaker,54,2013-08-08 12:30:44.470000 UTC,2022-09-22 12:25:59.377000 UTC,"Helsinki, Finland",476,71,1,73,,,,,,['amazon-sagemaker']
Saving artifacts on remote mlflow server,"<p>I am trying to store <code>MLflow</code> artifacts on a remote server running <code>MLflow</code>. The server I am accessing from and server running <code>MLflow</code> are both VMs on google cloud. I can see the matrices in the <code>MLflow</code> server but not the artifacts.</p>

<p>I tried the flollowing methods but nonoe of them is working:</p>

<ul>
<li><code>mlflow server     --backend-store-uri /mnt/persistent-disk     --default-artifact-root /tmp/ --host=0.0.0.0</code></li>
<li>mlflow server     --backend-store-uri /mnt/persistent-disk     --default-artifact-root /path/to/folder/with/mlrun --host=0.0.0.0</li>
</ul>

<p>I also gave <code>rwx</code> permissions to the path but still getting the same error :</p>

<pre><code>PermissionError: [Errno 13] Permission denied: '/home/user/folder'
</code></pre>",1,0,2019-08-27 14:13:07.633000 UTC,,2019-08-27 14:30:43.157000 UTC,3,python|mlflow,2002,2014-05-29 12:37:30.427000 UTC,2020-10-15 08:35:10.407000 UTC,Kuala Lumpur Federal Territory of Kuala Lumpur Malaysia,1990,49,1,268,,,,,,['mlflow']
Set TLS SSL version 1.2 on azure container instance,"<p>Please how do I set azure container instance to use TLS 1.2 and not TLS 1.0 and 1.1</p>
<p>I followed this <a href=""https://docs.microsoft.com/en-us/azure/machine-learning/how-to-secure-web-service"" rel=""nofollow noreferrer"">link</a> yet it installed TLS 1.0 and 1.2</p>
<p>Thanks</p>",1,0,2020-08-14 20:37:13.233000 UTC,,2020-08-18 15:05:05.180000 UTC,1,certificate|ssl-certificate|tls1.2|azure-machine-learning-service|azure-container-instances,344,2018-09-18 19:45:24.587000 UTC,2022-09-14 17:20:52.613000 UTC,"Laurel, MD, USA",359,3,0,41,,,,,,['azure-machine-learning-service']
MLflow: active run ID does not match environment run ID,"<p>OS: Ubuntu 18</p>

<p>Python: Python 3.6</p>

<p>MLflow: 1.4</p>

<p>I'm trying to get MLflow Projects to run. Here is my project:</p>

<ul>
<li><p>MLflow</p>

<ul>
<li><p>conda.yaml</p></li>
<li><p>main.py</p></li>
<li><p>prep_data.py</p></li>
<li><p>learn.py</p></li>
<li><p>List item</p></li>
</ul></li>
</ul>

<p>The project is heavily based up on this repo: <a href=""https://github.com/mlflow/mlflow/tree/master/examples/multistep_workflow"" rel=""nofollow noreferrer"">https://github.com/mlflow/mlflow/tree/master/examples/multistep_workflow</a>
I'm trying to run both the prep_data and learn scripts using MLflow Projects and the main.py script as an entry point.
For execution I use the following command: <code>mlflow run . -P experiment_name=testproject</code></p>

<p>But I get the following Error:</p>

<pre><code>Traceback (most recent call last):
  File ""prep_data.py"", line 126, in &lt;module&gt;
    prep_data()
  File ""/home/ubuntu/venv/lib/python3.6/site-packages/click/core.py"", line 764, in __call__
   return self.main(*args, **kwargs)
  File ""/home/ubuntu/venv/lib/python3.6/site-packages/click/core.py"", line 717, in main
    rv = self.invoke(ctx)
  File ""/home/ubuntu/venv/lib/python3.6/site-packages/click/core.py"", line 956, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File ""/home/ubuntu/venv/lib/python3.6/site-packages/click/core.py"", line 555, in invoke
    return callback(*args, **kwargs)
  File ""prep_data.py"", line 65, in prep_data
    with mlflow.start_run() as active_run:
  File ""/home/ubuntu/venv/lib/python3.6/site-packages/mlflow/tracking/fluent.py"", line 129, in start_run
    ""arguments"".format(existing_run_id))
mlflow.exceptions.MlflowException: Cannot start run with ID 405b83bbb61046afa83b8dcd71b4db14 because active run ID does not match environment run ID. Make sure --experiment-name or --experiment-id matches experiment set with set_experiment(), or just use command-line arguments
Traceback (most recent call last):
  File ""main.py"", line 75, in &lt;module&gt;
    workflow()
  File ""/home/ubuntu/venv/lib/python3.6/site-packages/click/core.py"", line 764, in __call__
    return self.main(*args, **kwargs)
  File ""/home/ubuntu/venv/lib/python3.6/site-packages/click/core.py"", line 717, in main
    rv = self.invoke(ctx)
  File ""/home/ubuntu/venv/lib/python3.6/site-packages/click/core.py"", line 956, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File ""/home/ubuntu/venv/lib/python3.6/site-packages/click/core.py"", line 555, in invoke
    return callback(*args, **kwargs)
  File ""main.py"", line 61, in workflow
    }, experiment_name)
  File ""main.py"", line 40, in _get_or_run
    submitted_run = mlflow.run('.', entry_point=entry_point, parameters=params)
  File ""/home/ubuntu/venv/lib/python3.6/site-packages/mlflow/projects/__init__.py"", line 287, in run
    _wait_for(submitted_run_obj)
  File ""/home/ubuntu/venv/lib/python3.6/site-packages/mlflow/projects/__init__.py"", line 304, in _wait_for
    raise ExecutionException(""Run (ID '%s') failed"" % run_id)
mlflow.exceptions.ExecutionException: Run (ID '405b83bbb61046afa83b8dcd71b4db14') failed
2019/11/22 18:51:59 ERROR mlflow.cli: === Run (ID '62c229b2d9194b569a7b2bfc14338800') failed ===
</code></pre>

<p>I'm not sure if I understand the error correctly but it seems like it's saying I am using multiple experiments. However I'm fairly certain I am only using 1 (testproject).
Browsing SO and Github issues suggested I'd should set the environment variable <code>MLFLOW_TRACKING_URI</code> but it wasn't stated on how to set that. Thus I tried two different ways:
1) exporting it before running the MLflow project: $ export <code>MLFLOW_TRACKING_URI='http://127.0.0.1:5099'</code>
2) setting it at the beginning of my main.py script using python: <code>os.environ['MLFLOW_TRACKING_URI'] = 'http://127.0.0.1:5099'</code>
Neither had any effect.
Here you can see my project:</p>

<p>main.py</p>

<pre><code>import os
import click
import mlflow
from mlflow.entities import RunStatus
def _already_ran(entry_point, params, experiment_name):
    # experiment = mlflow.get_experiment_by_name('{}_{}'.format(experiment_name, entry_point))
    experiment = mlflow.get_experiment_by_name(experiment_name)
    if experiment == None:
        return None
    experiment_id = experiment.experiment_id
    client = mlflow.tracking.MlflowClient()
    all_run_infos = reversed(client.list_run_infos(experiment_id))
    match_failed = False
    for run_info in all_run_infos
        full_run = client.get_run(run_info.run_id)
        for p_key, p_val in params:
            run_value = full_run.data.params.get(p_key)
            if run_value != p_val:
                match_failed = True
                break
        if match_failed:
            continue
        if run_info.to_proto().status != RunStatus.FINISHED:
            continue
        return client.get_run(run_info.run_id)
    return None


def _get_or_run(entry_point, params, experiment_name, use_cache=True):
    existing_run = _already_ran(entry_point, params, experiment_name)
    if use_cache and existing_run:
        return existing_run
    submitted_run = mlflow.run('.', entry_point=entry_point, parameters=params)
    return mlflow.tracking.MlflowClient().get_run(submitted_run.run_id)

@click.command()
@click.option(""--experiment-name"")
@click.option('--prep-data-time-avg', default='placeholder')
@click.option('--prep-data-sensor-id', default='placeholder')
@click.option('--learn-epochs', default=100, type=int)
@click.option('--learn-neurons', default=5, type=int)
@click.option('--learn-layers', default=2, type=int)
def workflow(experiment_name, prep_data_time_avg, prep_data_sensor_id, learn_epochs, learn_neurons, learn_layers):
    # mlflow.set_tracking_uri('http://127.0.0.1:5099')

    # mlflow.set_experiment(experiment_name)
    # with mlflow.start_run() as active_run:

    data_run = _get_or_run('prep_data', {
        'time_avg': prep_data_time_avg,
        'sensor_id':prep_data_sensor_id,
        'experiment_name': experiment_name
    }, experiment_name)

    learn_run = _get_or_run('learn', {
        'epochs': learn_epochs,
        'neurons': learn_neurons,
        'layers': learn_layers,
        'prep_data_run_id': data_run.run_id,
        'experiment_name': experiment_name,
    }, experiment_name)
if __name__ == '__main__':
    # os.environ['MLFLOW_TRACKING_URI'] = 'http://127.0.0.1:5099'
    workflow()


</code></pre>

<p>prep_data.py</p>

<pre><code>@click.command()
@click.option(""--experiment-name"")
@click.option('--time-avg', default='placeholder')
@click.option('--sensor-id', default='placeholder')
def prep_data(experiment_name, time_avg, sensor_id):
    mlflow.set_experiment(experiment_name)
    with mlflow.start_run() as active_run:
      # logic code of prep_data

if __name__ == '__main__':
    prep_data()

</code></pre>

<p>I'm happy about any ideas on how to fix this issue.</p>

<p>Thank you very much!</p>

<p>Cheers,
Raphael</p>",1,0,2019-11-23 10:00:29.297000 UTC,,,4,python-3.x|mlflow,2256,2016-09-10 16:12:07.980000 UTC,2022-06-30 12:40:05.403000 UTC,,4045,7,4,73,,,,,,['mlflow']
Unserialize model object on AWS SageMaker R Kernel,"<p>I have a model object that was serialized using the code (in R):</p>

<pre><code>serialize(res[[1]][['modelobj']], NULL, ascii=F)
</code></pre>

<p>and I stored it on S3 bucket.</p>

<p>However, when I am trying to source model from S3 bucket, I am unable to unserialize the model using:</p>

<pre><code>model &lt;- get_object(""s3://sagemaker/mwls.model_"")
unserialize(model)
</code></pre>

<p>It gives me the following error:</p>

<pre><code>Error in unserialize(model): unknown input format
Traceback:

1. unserialize(model)
</code></pre>

<p>It would help if someone even have suggestions that could work to unserialize the model object using only AWS (ideally storage on S3 and coding on R Kernel)?</p>",0,3,2019-07-30 05:28:59.990000 UTC,,2019-07-30 05:42:41.813000 UTC,0,r|amazon-web-services|amazon-s3|serialization|amazon-sagemaker,52,2018-12-05 18:14:27.080000 UTC,2021-04-02 21:17:55.193000 UTC,,471,25,0,55,,,,,,['amazon-sagemaker']
SageMaker ClientError: Detected non integer labels in the dataset,"<p>I have created a SageMaker training job to train on a toy, tabular, multiclass(3) dataset which has failed with the following error:</p>
<blockquote>
<p>ClientError: Detected non integer labels in the dataset. For classification tasks, the labels should be integers between 0 to (num_classes-1), exit code: 2</p>
</blockquote>
<p>It sounds like they're saying that for the classes (labels) they want to see values between 0 and 2 in this case, as I have 3 classes.</p>
<p>I have set <strong>num_classes</strong> to 3 and have validated that I only have 3 unique values in the rightmost column of my dataset: 0, 1, and 2</p>
<p>I've set <strong>feature_dim</strong> to 3. I've removed the headers from my dataset. My raw data looks like 5,000 lines of this:
<a href=""https://i.stack.imgur.com/sVmAb.png"" rel=""nofollow noreferrer"">csv snapshot</a></p>
<p>Can anyone guess as to what might be causing this error?</p>",1,0,2022-09-10 14:41:32.330000 UTC,,,0,amazon-sagemaker,14,2022-09-10 14:31:38.840000 UTC,2022-09-13 10:34:49.443000 UTC,,1,0,0,0,,,,,,['amazon-sagemaker']
AML issue with anaconda,"<p>After I upgraded AML SDK within conda virtual environment I have started getting error in my MacOS as follows:</p>

<p>print(azureml.core.<strong>version</strong>)
1.0.76</p>

<p>from azureml.train.automl import AutoMLConfig</p>

<p>ImportError: cannot import name 'AutoMLConfig'</p>",1,0,2019-12-04 23:59:53.193000 UTC,,,-1,azure-machine-learning-service,73,2015-05-04 02:39:12.817000 UTC,2020-02-11 19:45:53.247000 UTC,,1,0,0,26,,,,,,['azure-machine-learning-service']
Azure Machine Learning Studio : Deployed Service gives OutOfMemoryLimit exception,"<p>Using Azure Machine Learning Studio I have published a few models that are having OutOfMemoryLimit issues in the deployed services (not during training).</p>

<p>The model type I am using is the ""multiclass decision forest"", and I do create some decent sized forests. When stored to a blob they take up to around 150 MB in size. Closing in on 150 I get OOM every time. At around 140 Maybe 1 in 10, and even at 120 MB they happens now and then.</p>

<p>The thing is it runs fine in the studio, and when deployed as a service it is not very consistent in when it gives exceptions. I can run requests against a service and get a reply 9 out of 10 cases, but then in the 10% remaining cases I will get an exception that looks like this:</p>

<blockquote>
  <p>{""error"":{""code"":""MemoryQuotaViolation"",""message"":""The model had
  exceeded the memory quota assigned to
  it."",""details"":[{""code"":""OutOfMemoryLimit"",""message"":""The model
  consumed more memory than was appropriated for it. Maximum allowed
  memory for the model is 2560 MB. Please check your model for
  issues.""}]}}</p>
</blockquote>

<p>Now I do run this as a request-response, as opposed to a batch job, and I suspect it might do just fine as a batch job. The reason for R-R is that I do need these data in real time and batch jobs are simply too slow. </p>

<p>I suspect the ""right"" approach is to further handicap my forest by reducing tree count or increasing leaf node sizes, but obviously this will reduce the model accuracy (further). Before I do so I am looking for some advice around:</p>

<p>Is it possible to pay for MORE than the 2,5 GB limit for the Azure ML SaaS ? (If not when is that coming??)</p>

<p>Is there any way to test whether a deployed model will break this limit or not before actually deploying it? We are trying to run retraining automatically and this reduces our reliability drastically.</p>

<p>Any other advice on what to try/test/think of</p>

<p>Thanks in advance!</p>",0,0,2017-12-04 16:52:19.833000 UTC,,,3,azure|azure-resource-manager|azure-machine-learning-studio,321,2014-06-19 13:45:10.317000 UTC,2020-08-26 08:34:48.713000 UTC,,383,13,0,36,,,,,,['azure-machine-learning-studio']
How can I add a final step to a Sagemaker Pipeline that runs even if other steps fail,"<p>Is there a way to add an end step to a sagemaker pipeline that still runs at the very end (and runs code) even if other previous steps fail. Before I thought we could make it a <a href=""https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html?highlight=definition#sagemaker.workflow.fail_step.FailStep"" rel=""nofollow noreferrer"">Fail Step</a>  but that only lets you return an error message and doesn’t let you run code. If we made it a conditional step how would we make sure it ran at the very end without depending on any previous steps. I thought of adding all previous steps as a dependency so it runs at the end, but then the end step wouldn't run if any step before that failed.</p>
<p>I tried using the fail step, but I can't provide code. I tried putting it with dependencies but then it won't run if other steps fail before it. I tried putting no dependencies, but then it won't run at the end.</p>",1,0,2022-04-06 05:36:03.853000 UTC,,,1,amazon-web-services|pipeline|amazon-sagemaker,372,2022-04-06 05:31:05.523000 UTC,2022-05-25 21:44:20.240000 UTC,,11,0,0,0,,,,,,['amazon-sagemaker']
Access MLflow Artifacts in Model Registry Using Python,"<p>I am looking to access the artifacts of a model registered to the Model Registry in Databricks. However, I want to be able to do this outside of Databricks, using a Python script.</p>
<p>Specifically, I want to be able to access the <code>feature_spec.yml</code> shown in the directory structure below,</p>
<p><a href=""https://i.stack.imgur.com/7lSav.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7lSav.png"" alt=""enter image description here"" /></a></p>
<p>I came across this article in the Microsoft docs, but it is not quite clear,
<br>
<a href=""https://docs.microsoft.com/en-us/azure/databricks/applications/mlflow/access-hosted-tracking-server"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/databricks/applications/mlflow/access-hosted-tracking-server</a></p>
<p>Note that I will probably only have the name of the Model and the Version that I want to access. How can I do this using Python?</p>",1,0,2022-06-09 03:25:24.130000 UTC,,,0,python|databricks|mlflow,203,2018-03-24 01:53:05.820000 UTC,2022-09-24 16:46:35.903000 UTC,Sri Lanka,820,389,1,165,,,,,,['mlflow']
DataSpell & AWS Sagemaker Connection,"<p>Haven't been able to find this answer online, so I'm asking the stackoverflow community...</p>
<p>I'm wondering if DataSpell can connect to a SageMaker instance and use the EC2 instance hardware (i.e. virtual CPUs, GPUs, RAM, etc.) to run data transformations and machine learning model training on python and Jupyter notebook files?</p>
<p>I.e. I want all the advantages of DataSpell on my local computer (git, debugging, auto-complete, refactoring, etc.), while having all the advantages of a SageMaker instance on AWS (scalable compute hardware, fast training, etc.) to run python and Jupyter notebook files.</p>
<p>Thank you.</p>",1,0,2022-03-07 14:59:55.473000 UTC,1.0,,2,python|jupyter-notebook|pycharm|amazon-sagemaker|dataspell,301,2020-12-15 00:03:16.103000 UTC,2022-05-24 14:13:04.157000 UTC,,21,0,0,4,,,,,,['amazon-sagemaker']
Data version control (DVC) edit files in place results in cyclic dependency,"<p>we have a larger dataset and have several preprocessing scripts.
These scripts alter data in place.
It seems when I try to register it with <code>dvc run</code> it complains about cyclic dependencies (input is the same as output).
I would assume this is a very common use case.</p>

<p>What is the best practice here ?</p>

<p>Tried to google around but i did not see any solution to this (besides creating another folder for the output).</p>",0,2,2020-04-23 12:04:31.830000 UTC,,,2,dvc,45,2018-06-26 18:15:36.153000 UTC,2022-06-24 08:39:18.773000 UTC,"Berlin, Germany",121,4,0,16,,,,,,['dvc']
Visualise Option is not active on Azure machine learning service,"<p>hi I am new to Azure Machine learning Service. I uploaded imported CSV files in Data set and when i drag and drop them in Blank experiment, I am unable to visualize it or it fails to detect any columns. Is there any suggestions on how to rectify this problem</p>",1,0,2019-07-28 22:16:29.640000 UTC,,2019-07-29 09:12:26.363000 UTC,-1,azure|csv|azure-machine-learning-service,74,2019-07-28 22:08:34.860000 UTC,2020-11-21 06:14:24.470000 UTC,,1,0,0,7,,,,,,['azure-machine-learning-service']
Register on-prem db as Dataset in Azure ML Studio UI,"<p>how can I connect the new Azure Machine Learning Studio UI to an on-premise SQL DB via an on-premise data Gateway?</p>
<p>In the classic version, once the gateway was installed and registered the Import data module offered it as a dataset type. In the new UI however it doesn't seem to be available.</p>",1,0,2021-11-30 14:09:06.513000 UTC,,2021-11-30 19:41:04.397000 UTC,1,azure-machine-learning-studio,76,2020-02-02 16:59:57.867000 UTC,2021-11-30 14:25:41.327000 UTC,"Prague, Czechia",23,1,0,13,,,,,,['azure-machine-learning-studio']
"How to fix ""Kernel Unknown"" error in JupyterLab on Google Vertex AI notebooks (Python 3)","<p>When I try to open an existing or new notebook on my Google Vertex Notebooks instance (on Python 3.7.6), no code cells run and it says &quot;kernel unknown&quot; at the top left. When I try to run a code cell, an asterisk appears as if it were running, but nothing else happens. Everything else works fine (e.g. opening the terminal, editing markdown cells in a notebook, viewing files). The issue started happening immediately after I tried to update conda to fix conflicting package errors.  What can I do to fix the issue and be able to run code cells again?</p>
<p>(Please let me know if I'm leaving out important information)</p>
<p>screenshot of the issue:
<img src=""https://i.stack.imgur.com/7dwuB.png"" alt=""error screenshot"" /></p>",0,4,2021-12-13 17:43:00.063000 UTC,,2021-12-13 17:52:55.813000 UTC,1,google-cloud-platform|jupyter-notebook|conda|jupyter-lab|google-cloud-vertex-ai,1110,2021-12-13 17:33:05.647000 UTC,2022-04-08 22:36:48.013000 UTC,,11,0,0,0,,,,,,['google-cloud-vertex-ai']
ValueError: Enum ErrorCode has no value defined for name '403' in mlflow.set_experiment(),"<p>I am trying to run some code to train a model, while logging my results to MLflow on Databricks. I keep getting the following error when I try to make a call to <code>mlflow.set_experiment()</code>,</p>
<pre><code>    raise ValueError('Enum {} has no value defined for name {!r}'.format(
ValueError: Enum ErrorCode has no value defined for name '403'
</code></pre>
<p>What exactly is going on here?</p>
<p>I am using Databricks Connect to run my code and the section where the error pops up looks like this,</p>
<pre><code>    # set remote tracking server URI
    mlflow.set_tracking_uri(remote_server_uri)

    # create the MLflow client
    client = MlflowClient(remote_server_uri)

    # set experiment to log mlflow runs
    mlflow.set_experiment(experiment_name)
</code></pre>",0,0,2022-09-08 04:25:26.990000 UTC,,2022-09-08 08:21:53.963000 UTC,1,python|mlflow|databricks-connect,68,2018-03-24 01:53:05.820000 UTC,2022-09-24 16:46:35.903000 UTC,Sri Lanka,820,389,1,165,,,,,,['mlflow']
Is there a way to turn on SageMaker model endpoints only when I am receiving inference requests,"<p>I have created a model endpoint which is InService and deployed on an ml.m4.xlarge instance. I am also using API Gateway to create a RESTful API.</p>
<p>Questions:</p>
<ol>
<li><p>Is it possible to have my model endpoint only Inservice (or on standby) when I receive inference requests? Maybe by writing a lambda function or something that turns off the endpoint (so that it does not keep accumulating the per hour charges)</p>
</li>
<li><p>If q1 is possible, would this have some weird latency issues on the end users? Because it usually takes a couple of minutes for model endpoints to be created when I configure them for the first time.</p>
</li>
<li><p>If q1 is not possible, how would choosing a cheaper instance type affect the time it takes to perform inference (Say I'm only using the endpoints for an application that has a low number of users).</p>
</li>
</ol>
<p>I am aware of this site that compares different instance types (<a href=""https://aws.amazon.com/sagemaker/pricing/instance-types/"" rel=""nofollow noreferrer"">https://aws.amazon.com/sagemaker/pricing/instance-types/</a>)</p>
<p>But, does having a moderate network performance mean that the time to perform realtime inference may be longer?</p>
<p>Any recommendations are much appreciated. The goal is not to burn money when users are not requesting for predictions.</p>",2,0,2020-07-06 23:08:58.347000 UTC,,,1,amazon-web-services|aws-lambda|aws-api-gateway|amazon-sagemaker,643,2020-01-05 10:48:20.170000 UTC,2022-09-19 02:56:37.510000 UTC,"Toronto, ON, Canada",117,21,0,15,,,,,,['amazon-sagemaker']
Sagemaker sklearn custom code “ValueError: could not convert string to float”,"<p>I am using a sklearn custom script to train and deploy a model in sagemaker. When I tried to invoke the endpoint I had the following error:</p>
<p><code>ERROR - model_featurizer_training - Exception on /invocations [POST] Traceback (most recent call last): File &quot;/miniconda3/lib/python3.7/site-packages/sagemaker_containers/_functions.py&quot;, line 93, in wrapper return fn(*args, **kwargs) File &quot;/miniconda3/lib/python3.7/site-packages/sagemaker_sklearn_container/serving.py&quot;, line 60, in default_input_fn return np_array.astype(np.float32) if content_type in content_types.UTF8_TYPES else np_array ValueError: could not convert string to float: 'female'</code></p>
<p>My training custom script is as follows:</p>
<pre><code>if __name__=='__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--n_estimators', type=int, default=10)

    parser.add_argument('--output-data-dir', type=str, default=os.environ['SM_OUTPUT_DATA_DIR'])
    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])
    parser.add_argument('--train', type=str, default=os.environ['SM_CHANNEL_TRAIN'])
    
    args = parser.parse_args()

    input_file = [os.path.join(args.train, file) for file in os.listdir(args.train)]

    raw_data = [pd.read_csv(file, engine='python') for file in input_file]

    train_data = pd.concat(raw_data)
    

    X = train_data.iloc[:, 1:].values

    numerical_processing = make_pipeline(SimpleImputer(strategy=&quot;median&quot;))

    categorical_processing = make_pipeline(
        SimpleImputer(strategy=&quot;constant&quot;, fill_value=&quot;missing&quot;, add_indicator=True),
        OneHotEncoder(handle_unknown=&quot;ignore&quot;),
    )

    preprocessing = make_column_transformer(
        (numerical_processing, list(np.arange(0, 15))),
        (categorical_processing, list(np.arange(15, 17))),
    )

    n_estimators = args.n_estimators

    clf = RandomForestClassifier(n_estimators=n_estimators
                                ,random_state=42)

    full_pipeline = make_pipeline(preprocessing, clf)

    y = train_data.iloc[:, 0]

    full_pipeline.fit(X, y)

    joblib.dump(full_pipeline, os.path.join(args.model_dir, 'sklearn_full_pipeline_model.joblib'))



def input_fn(input_data):

    return np.array([i for i in input_data.split(&quot;,&quot;)], dtype=&quot;object&quot;).reshape(1, -1)

def predict_fn(input_data, model):

    return model.predict(input_data)

def model_fn(model_dir):
    
    clf = joblib.load(os.path.join(model_dir, 'sklearn_full_pipeline_model.joblib'))

    return clf
</code></pre>
<p>I am aware that I should pass a custom <code>input_fn</code> to my script so my data input is read correctly, but apparently the <code>default_input_fn</code> is being invoked instead.</p>",0,0,2021-03-09 18:17:15.117000 UTC,,2021-03-09 19:09:12.287000 UTC,1,python|scikit-learn|amazon-sagemaker,124,2017-07-17 16:15:17.120000 UTC,2022-03-09 12:01:21.033000 UTC,Valinhos,11,0,0,0,,,,,,['amazon-sagemaker']
AzureML experiment pipeline not using CUDA with PyTorch,"<p>I am running an experiment pipeline to train my model with PyTorch and CUDA.
I created the environment as follow:</p>
<pre class=""lang-py prettyprint-override""><code>    env = Environment.from_conda_specification(model, join(model, 'conda_dependencies.yml'))
    env.docker.enabled = True
    env.environment_variables = {'MODEL_NAME': model, 'BRANCH': branch, 'COMMIT': commit}
    env.docker.base_image = DEFAULT_GPU_IMAGE

    run_config = RunConfiguration()
    run_config.environment = env
    run_config.docker = DockerConfiguration(use_docker=True)
</code></pre>
<p>And here is the training step:</p>
<pre class=""lang-py prettyprint-override""><code>train_step = PythonScriptStep(
      name='Model Train',
      source_directory=training_dir,
      compute_target=cluster,
      runconfig=run_config,
      script_name='train_aml.py',
      arguments=[
        '--model', model,
        '--model_output_dir', model_output_dir,
      ],
      inputs=[train_dataset.as_mount()],
      outputs=[model, model_output_dir]
    )
</code></pre>
<p>Even though I am using a <code>Standard_NC12_Promo</code> machine when I run my training script, the GPU is not picked up by PyTorch <code>device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)</code>.<br />
If I try running my script on the same machine but not in an experiment then the GPU is used.<br />
Do you know any potential solutions to this?</p>",1,0,2021-08-06 08:34:00.413000 UTC,,2021-08-06 15:12:32.710000 UTC,1,azure|pytorch|azure-machine-learning-service|azureml-python-sdk,219,2015-07-04 10:48:10.427000 UTC,2022-09-23 15:10:29.127000 UTC,"Amsterdam, Paesi Bassi",959,136,31,204,,,,,,['azure-machine-learning-service']
Python 3 with Tensorflow on Sagemaker,"<p>I understand the Sagemaker currently does not support Python 3 with Tensorflow (according to this <a href=""https://github.com/aws/sagemaker-python-sdk/issues/19"" rel=""nofollow noreferrer"">https://github.com/aws/sagemaker-python-sdk/issues/19</a>)</p>

<p>But is it possible to create your own docker container with Python 3 and Tensorflow as is explained here? 
<a href=""https://github.com/awslabs/amazon-sagemaker-examples/blob/master/advanced_functionality/scikit_bring_your_own/scikit_bring_your_own.ipynb"" rel=""nofollow noreferrer"">https://github.com/awslabs/amazon-sagemaker-examples/blob/master/advanced_functionality/scikit_bring_your_own/scikit_bring_your_own.ipynb</a></p>",2,0,2018-06-19 12:06:23.087000 UTC,,,3,python|python-3.x|tensorflow|amazon-sagemaker,956,2009-01-23 18:21:26.757000 UTC,2020-03-16 13:57:33.880000 UTC,"Bangalore, Karnataka, India",3312,405,15,774,,,,,,['amazon-sagemaker']
"Getting error Missing required package ""azureml-dataset-runtime"" in VSCode","<p>I am trying to setup my virtual environment for Azure in VS Code. I have installed the required packages, e.g., <code>azureml-core</code> and <code>azureml-widgets</code> and <code>azureml-dataset-runtime</code>. Both <code>azureml-core</code> and <code>azureml-widget</code> work fine, however, I keep getting an error missing required package for <code>azureml-dataset-runtime</code> although I installed it.</p>
<p>I have Python 3.7 and Python 3.8 installed, both 64 bit and I tried both of them in my virtual environment, still no luck.</p>
<p>Any suggestions?</p>",0,1,2021-05-07 15:53:49.140000 UTC,,,7,azure|visual-studio-code|azure-machine-learning-service|azureml-python-sdk,433,2020-09-12 17:36:28.727000 UTC,2022-09-22 00:17:59.883000 UTC,,105,52,0,4,,,,,,['azure-machine-learning-service']
'MSSQL' encountered unexpected exception of type 'InvalidOperationException' with HResult 'x80131509' while opening connection,"<p>When I am trying to load a query into a tabular dateset (from a devops docker image) I will get the following error:</p>
<pre><code>raise DatasetValidationError(error_message + '\n' + str(e), e)
azureml.data.dataset_error_handling.DatasetValidationError: Cannot load any data from the datastore using the SQL query &quot;&lt;azureml.data.datapath.DataPath object at 0x&gt;&quot;. Please make sure the datastore and query is correct.

Error Code: ScriptExecution.DatabaseConnection.Unexpected
Failed Step: 9ad57100-4870-49d2-a32f-1c9c15c244e0
Error Message: ScriptExecutionException was caused by DatabaseConnectionException.
  DatabaseConnectionException was caused by UnexpectedException.
    'MSSQL' encountered unexpected exception of type 'InvalidOperationException' with HResult 'x80131509' while opening connection.
      Internal connection fatal error.
</code></pre>
<p>I believe that I have allowed the connection in firewall (I might not have done it quite right).</p>
<p>I don't get the error when I am running it from the notebook (on the compute instance).</p>",1,0,2020-08-18 05:42:27.383000 UTC,,,1,azure-machine-learning-service,156,2020-08-18 02:37:03.227000 UTC,2021-09-26 10:54:30.930000 UTC,,81,0,0,5,,,,,,['azure-machine-learning-service']
When is one supposed to run wandb.watch so that weights and biases tracks params and gradients properly?,"<p>I was trying out the wandb library and I run <code>wandb.watch</code> but that doesn't seem to work on my code. It's not supposed to be anything to complicated so I am puzzled why it's not working.</p>
<p>Code:</p>
<pre><code>&quot;&quot;&quot;
https://docs.wandb.ai/guides/track/advanced/distributed-training

import wandb

# 1. Start a new run
wandb.init(project='playground', entity='brando')

# 2. Save model inputs and hyperparameters
config = wandb.config
config.learning_rate = 0.01

# 3. Log gradients and model parameters
wandb.watch(model)
for batch_idx, (data, target) in enumerate(train_loader):
    ...
    if batch_idx % args.log_interval == 0:
        # 4. Log metrics to visualize performance
        wandb.log({&quot;loss&quot;: loss})


Notes:
    - call wandb.init and wandb.log only from the leader process
&quot;&quot;&quot;

from argparse import Namespace
from pathlib import Path
from typing import Union

import torch
from torch import nn
from torch.nn.functional import mse_loss
from torch.optim import Optimizer

import uutils
from uutils.torch_uu import r2_score_from_torch
from uutils.torch_uu.distributed import is_lead_worker
from uutils.torch_uu.models import get_simple_model
from uutils.torch_uu.tensorboard import log_2_tb_supervisedlearning


import wandb

def log_2_wandb_nice(it, loss, inputs, outputs, captions):
    wandb.log({&quot;loss&quot;: loss, &quot;epoch&quot;: it,
               &quot;inputs&quot;: wandb.Image(inputs),
               &quot;logits&quot;: wandb.Histogram(outputs),
               &quot;captions&quot;: wandb.HTML(captions)})

def log_2_wandb(**metrics):
    &quot;&quot;&quot; Log to wandb &quot;&quot;&quot;
    new_metrics: dict = {}
    for key, value in metrics.items():
        key = str(key).strip('_')
        new_metrics[key] = value
    wandb.log(new_metrics)


def log_train_val_stats(args: Namespace,
                        it: int,

                        train_loss: float,
                        train_acc: float,

                        valid,

                        log_freq: int = 10,
                        ckpt_freq: int = 50,
                        force_log: bool = False,  # e.g. at the final it/epoch

                        save_val_ckpt: bool = False,
                        log_to_tb: bool = False,
                        log_to_wandb: bool = False
                        ):
    &quot;&quot;&quot;

    log train and val stats.

    Note: Unlike save ckpt, this one does need it to be passed explicitly (so it can save it in the stats collector).
    &quot;&quot;&quot;
    from uutils.torch_uu.tensorboard import log_2_tb
    from matplotlib import pyplot as plt

    # - is it epoch or iteration
    it_or_epoch: str = 'epoch_num' if args.training_mode == 'epochs' else 'it'
    # if its
    total_its: int = args.num_empochs if args.training_mode == 'epochs' else args.num_its

    print(f'-- {it == total_its - 1}')
    print(f'-- {it}')
    print(f'-- {total_its}')
    if (it % log_freq == 0 or is_lead_worker(args.rank) or it == total_its - 1 or force_log) and is_lead_worker(args.rank):
        print('inside log')
        # - get eval stats
        val_loss, val_acc = valid(args, args.mdl, save_val_ckpt=save_val_ckpt)

        # - print
        args.logger.log('\n')
        args.logger.log(f&quot;{it_or_epoch}={it}: {train_loss=}, {train_acc=}&quot;)
        args.logger.log(f&quot;{it_or_epoch}={it}: {val_loss=}, {val_acc=}&quot;)

        # - record into stats collector
        args.logger.record_train_stats_stats_collector(it, train_loss, train_acc)
        args.logger.record_val_stats_stats_collector(it, val_loss, val_acc)
        args.logger.save_experiment_stats_to_json_file()
        fig = args.logger.save_current_plots_and_stats()

        # - log to wandb
        if log_to_wandb:
            # if it == 0:
            #     # -- todo why isn't this working?
            #     wandb.watch(args.mdl)
            #     print('watching model')
            # log_2_wandb(train_loss=train_loss, train_acc=train_acc)
            print('inside wandb log')
            wandb.log(data={'train loss': train_loss, 'train acc': train_acc, 'val loss': val_loss, 'val acc': val_acc}, step=it)
            wandb.log(data={'it': it}, step=it)
            if it == total_its - 1:
                print(f'logging fig at {it=}')
                wandb.log(data={'fig': fig}, step=it)
        plt.close('all')

        # - log to tensorboard
        if log_to_tb:
            log_2_tb_supervisedlearning(args.tb, args, it, train_loss, train_acc, 'train')
            log_2_tb_supervisedlearning(args.tb, args, it, train_loss, train_acc, 'val')
            # log_2_tb(args, it, val_loss, val_acc, 'train')
            # log_2_tb(args, it, val_loss, val_acc, 'val')

    # - log ckpt
    if (it % ckpt_freq == 0 or it == total_its - 1 or force_log) and is_lead_worker(args.rank):
        save_ckpt(args, args.mdl, args.optimizer)


def save_ckpt(args: Namespace, mdl: nn.Module, optimizer: torch.optim.Optimizer,
              dirname: Union[None, Path] = None, ckpt_name: str = 'ckpt.pt'):
    &quot;&quot;&quot;
    Saves checkpoint for any worker.
    Intended use is to save by worker that got a val loss that improved.


    &quot;&quot;&quot;
    import dill

    dirname = args.log_root if (dirname is None) else dirname
    # - pickle ckpt
    assert uutils.xor(args.training_mode == 'epochs', args.training_mode == 'iterations')
    pickable_args = uutils.make_args_pickable(args)
    torch.save({'state_dict': mdl.state_dict(),
                'epoch_num': args.epoch_num,
                'it': args.it,
                'optimizer': optimizer.state_dict(),
                'args': pickable_args,
                'mdl': mdl},
               pickle_module=dill,
               f=dirname / ckpt_name)  # f'mdl_{epoch_num:03}.pt'


def get_args() -&gt; Namespace:
    args = uutils.parse_args_synth_agent()
    # we can place model here...
    args = uutils.setup_args_for_experiment(args)
    return args


def valid_for_test(args: Namespace, mdl: nn.Module, save_val_ckpt: bool = False):
    import torch

    for t in range(1):
        x = torch.randn(args.batch_size, 5)
        y = (x ** 2 + x + 1).sum(dim=1)

        y_pred = mdl(x).squeeze(dim=1)
        val_loss, val_acc = mse_loss(y_pred, y), r2_score_from_torch(y_true=y, y_pred=y_pred)

    if val_loss.item() &lt; args.best_val_loss and save_val_ckpt:
        args.best_val_loss = val_loss.item()
        save_ckpt(args, args.mdl, args.optimizer, ckpt_name='ckpt_best_val.pt')
    return val_loss, val_acc


def train_for_test(args: Namespace, mdl: nn.Module, optimizer: Optimizer, scheduler=None):
    # wandb.watch(args.mdl)
    for it in range(args.num_its):
        x = torch.randn(args.batch_size, 5)
        y = (x ** 2 + x + 1).sum(dim=1)

        y_pred = mdl(x).squeeze(dim=1)
        train_loss, train_acc = mse_loss(y_pred, y), r2_score_from_torch(y_true=y, y_pred=y_pred)

        optimizer.zero_grad()
        train_loss.backward()  # each process synchronizes it's gradients in the backward pass
        optimizer.step()  # the right update is done since all procs have the right synced grads
        scheduler.step()

        log_train_val_stats(args, it, train_loss, train_acc, valid_for_test,
                            log_freq=2, ckpt_freq=10,
                            save_val_ckpt=True, log_to_tb=True, log_to_wandb=True)

    return train_loss, train_acc


def debug_test():
    args: Namespace = get_args()
    args.num_its = 12

    # - get mdl, opt, scheduler, etc
    args.mdl = get_simple_model(in_features=5, hidden_features=20, out_features=1, num_layer=2)
    wandb.watch(args.mdl)
    args.optimizer = torch.optim.Adam(args.mdl.parameters(), lr=1e-1)
    args.scheduler = torch.optim.lr_scheduler.ExponentialLR(args.optimizer, gamma=0.999, verbose=False)

    # - train
    train_loss, train_acc = train_for_test(args, args.mdl, args.optimizer, args.scheduler)
    print(f'{train_loss=}, {train_loss=}')

    # - eval
    val_loss, val_acc = valid_for_test(args, args.mdl)

    print(f'{val_loss=}, {val_acc=}')

    # - make sure wandb closes properly
    if args.log_to_wandb:
        wandb.finish()


if __name__ == '__main__':
    import os

    # print(os.environ['WANDB_API_KEY'])
    import time
    start = time.time()
    debug_test()
    duration_secs = time.time() - start
    print(f&quot;\nSuccess, time passed: hours:{duration_secs / (60 ** 2)}, minutes={duration_secs / 60}, seconds={duration_secs}&quot;)
    print('Done!\a')
</code></pre>
<p>code in github: <a href=""https://github.com/brando90/ultimate-utils/blob/master/tutorials_for_myself/my_wandb/my_wandb_basic1.py"" rel=""nofollow noreferrer"">https://github.com/brando90/ultimate-utils/blob/master/tutorials_for_myself/my_wandb/my_wandb_basic1.py</a></p>
<p>sample run: <a href=""https://wandb.ai/brando/playground/runs/wpupxvg1"" rel=""nofollow noreferrer"">https://wandb.ai/brando/playground/runs/wpupxvg1</a></p>
<p>cross posted: <a href=""https://community.wandb.ai/t/when-is-one-supposed-to-run-wandb-watch-so-that-weights-and-biases-tracks-params-and-gradients-prope/518"" rel=""nofollow noreferrer"">https://community.wandb.ai/t/when-is-one-supposed-to-run-wandb-watch-so-that-weights-and-biases-tracks-params-and-gradients-prope/518</a></p>",2,3,2021-09-11 17:14:02.697000 UTC,,2021-09-11 17:29:59.613000 UTC,3,machine-learning|deep-learning|neural-network|wandb,1927,2012-06-21 21:22:10.287000 UTC,2022-09-25 02:41:28.613000 UTC,,11435,1807,299,6472,,,,,,['wandb']
Script mode py3 and lack of output in s3 after successful training,"<p>I've created a script where I define my Tensorflow Estimator, then I pass it to AWS sagemaker sdk and run fit(), the training passes (though doesnt show anything related to training in the console) and in S3 the only output is /source/sourcedir.tar.gz and I believe there also should be at least /model/model.tar.gz which for some reason is not generated and I'm not getting any errors.</p>

<pre><code>sagemaker_session = sagemaker.Session()
role = get_execution_role()
inputs = sagemaker_session.upload_data(path='data', key_prefix='data/NamingConventions')
NamingConventions_estimator = TensorFlow(entry_point='NamingConventions.py',
                               role=role,
                               framework_version='1.12.0',
                               train_instance_count=1,
                               train_instance_type='ml.m5.xlarge',
                               py_version='py3',
                               model_dir=""s3://sagemaker-eu-west-2-218566301064/model"")
NamingConventions_estimator.fit(inputs, run_tensorboard_locally=True)
</code></pre>

<p>and my model_fn from 'NamingConventions.py'</p>

<pre><code>def model_fn(features, labels, mode, params):
    net = keras.layers.Embedding(alphabetLen + 1, 8, input_length=maxFeatureLen)(features[INPUT_TENSOR_NAME])
    net = keras.layers.LSTM(12)(net)

    logits = keras.layers.Dense(len(conventions), activation=tf.nn.softmax)(net) #output
    predictions = tf.reshape(logits, [-1])

    if mode == tf.estimator.ModeKeys.PREDICT:
        return tf.estimator.EstimatorSpec(
            mode=mode,
            predictions={""ages"": predictions},
            export_outputs={SIGNATURE_NAME: PredictOutput({""ages"": predictions})})

    loss = keras.losses.sparse_categorical_crossentropy(labels, predictions)

    train_op = tf.contrib.layers.optimize_loss(
        loss=loss,
        global_step=tf.contrib.framework.get_global_step(),
        learning_rate=params[""learning_rate""],
        optimizer=""AdamOptimizer"")

    predictions_dict = {""ages"": predictions}

    eval_metric_ops = {
        ""rmse"": tf.metrics.root_mean_squared_error(
            tf.cast(labels, tf.float32), predictions)
    }

    return tf.estimator.EstimatorSpec(
        mode=mode,
        loss=loss,
        train_op=train_op,
        eval_metric_ops=eval_metric_ops)
</code></pre>

<p>I still can't get it running, I'm trying to use script-mode, it seems like I can't import my model from the same directory.
Currently my script:</p>

<pre><code>import argparse
import os

if __name__ =='__main__':

    parser = argparse.ArgumentParser()

    # hyperparameters sent by the client are passed as command-line arguments to the script.
    parser.add_argument('--epochs', type=int, default=10)
    parser.add_argument('--batch_size', type=int, default=100)
    parser.add_argument('--learning_rate', type=float, default=0.1)

    # input data and model directories
    parser.add_argument('--model_dir', type=str)
    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))
    parser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))

    args, _ = parser.parse_known_args()

import tensorflow as tf
from NC_model import model_fn, train_input_fn, eval_input_fn

def train(args):
    print(args)
    estimator = tf.estimator.Estimator(model_fn=model_fn, model_dir=args.model_dir)
    train_spec = tf.estimator.TrainSpec(train_input_fn, max_steps=1000)
    eval_spec = tf.estimator.EvalSpec(eval_input_fn)
    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)

if __name__ == '__main__':
    train(args)
</code></pre>",3,0,2019-01-20 23:04:52.593000 UTC,,2019-01-21 12:31:36.783000 UTC,0,amazon-web-services|tensorflow|amazon-sagemaker,449,2018-11-13 21:54:35.153000 UTC,2019-04-13 20:25:53.210000 UTC,,91,1,0,27,,,,,,['amazon-sagemaker']
MLFlow & Conda: store envs in project dir instead of ~/opt/anaconda3/envs,"<p>As per conda's <a href=""https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#id3"" rel=""nofollow noreferrer"">documentation</a>:</p>
<blockquote>
<p>You can control where a conda environment lives by providing a path to a target directory when creating the environment. [...]:
<code>conda create --prefix ./envs jupyterlab=3.2 matplotlib=3.5 numpy=1.21</code></p>
</blockquote>
<p>Is it possible to modify how mlflow invoques <code>conda create</code> when generating all the components' environments, in order save those at the root of the project instead of the default <code>.../anaconda3/envs</code> ?</p>
<p>Many thanks in advance for your help,<br />
Kind regards<br />
Marc</p>",0,0,2022-09-03 09:41:14.837000 UTC,,2022-09-03 09:42:54.733000 UTC,0,conda|mlflow|anaconda3,18,2020-08-30 09:51:48.953000 UTC,2022-09-25 05:43:43.343000 UTC,,1,0,0,1,,,,,,['mlflow']
How to access Key vault from ML OPs,"<p>I have ml code running in Azure ML workspace. The same code is deployed Aks cluster deployed  vnet using Azure MLops pipeline through inference cluster.
Key-vault is created for storing variables accessed within jupyter notebook</p>
<pre><code>While testing the endpoint from Postman getting &quot;managedidentitycredential.get_token failed: managedidentitycredential authentication unavailable, no managed identity endpoint found&quot; error.

In code, trying to access key-vault variables by the below method.
credentials = DefaultAzureCredential()
client = SecretClient(vault_url=&quot;url&quot;, credential=credentials)
access_key = client.get_secret(&quot;KEY&quot;).value

Please help, how to access key vault here? 

Thanks.
</code></pre>",1,0,2021-07-06 10:29:15.510000 UTC,,,0,azure|azure-keyvault|azure-managed-identity|azure-machine-learning-service|mlops,324,2020-10-11 16:01:43.947000 UTC,2022-02-09 04:43:40.897000 UTC,,29,6,0,33,,,,,,['azure-machine-learning-service']
Azure ML problem with installation fastai with conda,"<p>I am struggling to install the fastbook package for fastai in Azure ML. Below is the error I am getting.</p>
<p>I have created a new environment, and all other fast ai packages are installed.</p>
<pre><code>Found conflicts! Looking for incompatible packages.
This can take several minutes.  Press CTRL-C to abort.
failed      
                                                                                                                                              
UnsatisfiableError: The following specifications were found to be incompatible with each other:
Output in format: Requested package -&gt; Available versions
The following specifications were found to be incompatible with your system:
  - feature:/linux-64::__glibc==2.27=0
  - python=3.9 -&gt; libgcc-ng[version='&gt;=7.5.0'] -&gt; __glibc[version='&gt;=2.17']
Your installed version is: 2.27
</code></pre>
<p>The command line code is below:</p>
<pre><code> conda install -y pip
 conda install -y ipykernel
 conda install -y -c fastai -c pytorch fastai
 conda install -y -c fastai fastbook
 conda install -y -c fastai nbdev
</code></pre>
<p>I only get the error for the fastbook package</p>",1,3,2021-10-26 06:20:13.020000 UTC,,2021-10-26 07:55:28.607000 UTC,0,python|conda|fast-ai|azure-machine-learning-service,124,2021-05-11 12:12:26.277000 UTC,2022-09-21 13:53:49.990000 UTC,South Africa,1,0,0,1,,,,,,['azure-machine-learning-service']
Problem trying to authenticate with bearer token on nginx + oauth2-proxy + docker,"<p>I'm trying to setup a Google Authentication for my MLflow application using nginx, oauth2-proxy and Docker. Everything works fine when I'm logging through web-browser, but I need to access MLflow in Python Scripts and request the MLflow API too.</p>
<p>I'm trying to request the API in the following way:</p>
<p><code>curl -X GET http://localhost/api/2.0/mlflow/experiments/list -H &quot;Authorization: Bearer $(gcloud auth print-identity-token)&quot;</code></p>
<p>Where <code>$(gcloud auth print-identity-token)</code> translates to my acess token of GCP (which I'm using as provider on oauth2-proxy). I'm logged on gcp cli with a valid account which has right access / privileges to all projects (i.e. it's not a gcp authentication problem)</p>
<p>oauth2-proxy logs returns me the following message:</p>
<pre><code>oauth2_proxy    | [2022/06/22 19:19:06] [jwt_session.go:51] Error retrieving session from token in Authorization header: [unable to verify bearer token, not implemented]
</code></pre>
<p>Which leads me to believe that's some misconfiguration in my nginx config file or in the env vars that I pass to oauth2-proxy.</p>
<p>Nginx <code>default.conf</code>:</p>
<pre><code>server {
    listen       80;
    server_name  localhost;
    
    location / {
        proxy_pass http://web:5000;
        auth_request /oauth2/auth;
        error_page 401 = /oauth2/sign_in;
        # error_page 404 = /404.html;
        # error_page 500 502 503 504 = /50x.html;

        auth_request_set $user   $upstream_http_x_auth_request_user;
        auth_request_set $email  $upstream_http_x_auth_request_email;
        proxy_set_header X-User  $user;
        proxy_set_header X-Email $email;

        auth_request_set $token  $upstream_http_x_auth_request_access_token;
        proxy_set_header X-Access-Token $token;

        auth_request_set $auth_cookie $upstream_http_set_cookie;
        add_header Set-Cookie $auth_cookie;

        auth_request_set $auth_cookie_name_upstream_1 $upstream_cookie_auth_cookie_name_1;

        if ($auth_cookie ~* &quot;(; .*)&quot;) {
            set $auth_cookie_name_0 $auth_cookie;
            set $auth_cookie_name_1 &quot;auth_cookie_name_1=$auth_cookie_name_upstream_1$1&quot;;
        }

        if ($auth_cookie_name_upstream_1) {
            add_header Set-Cookie $auth_cookie_name_0;
            add_header Set-Cookie $auth_cookie_name_1;
        }
    }

    location /oauth2 {
        proxy_pass            http://oauth2_proxy:4180;
        proxy_set_header      Host                    $host;
        proxy_set_header      X-Real-IP               $remote_addr;
        proxy_set_header      X-Scheme                $scheme;
        proxy_set_header      X-Auth-Request-Redirect $request_uri;
    }

    location = /oauth2/auth {
        proxy_pass       http://oauth2_proxy:4180;
        proxy_set_header Host             $host;
        proxy_set_header X-Real-IP        $remote_addr;
        proxy_set_header X-Scheme         $scheme;
        proxy_set_header Content-Length   &quot;&quot;;
        proxy_pass_request_body           off;
    }
}
</code></pre>
<p>My <code>docker-compose.yaml</code> file:</p>
<pre><code>version: '3'
services:

  db:
    restart: always
    image: mysql/mysql-server:5.7.28
    container_name: mlflow_db
    expose:
        - &quot;3306&quot;
    networks:
        - backend
    environment:
        - MYSQL_DATABASE=${MYSQL_DATABASE}
        - MYSQL_USER=${MYSQL_USER}
        - MYSQL_PASSWORD=${MYSQL_PASSWORD}
        - MYSQL_ROOT_PASSWORD=${MYSQL_ROOT_PASSWORD}
    volumes:
        - dbdata:/var/lib/mysql

  web:
    restart: always
    build: ./mlflow
    image: mlflow_server
    container_name: mlflow_server
    expose:
        - &quot;5000&quot;
    networks:
        - frontend
        - backend
    environment:
        - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
        - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
        - AWS_DEFAULT_REGION=${AWS_DEFAULT_REGION}
        - AWS_BUCKET_NAME=${AWS_BUCKET_NAME}
    command: mlflow server --backend-store-uri mysql+pymysql://${MYSQL_USER}:${MYSQL_PASSWORD}@db:3306/${MYSQL_DATABASE} --default-artifact-root s3://redacted/mlflow/ --host 0.0.0.0

  oauth2_proxy:
    build: ./oauth2_proxy
    container_name: oauth2_proxy
    environment:
      - OAUTH2_PROXY_HTTP_ADDRESS=http://0.0.0.0:4180
      - OAUTH2_PROXY_UPSTREAM=http://localhost
      # Restrictions (Not use in same time)
      - OAUTH2_PROXY_AUTHENTICATED_EMAILS_FILE=/home/emails.txt
      # - OAUTH2_PROXY_EMAIL_DOMAINS=*  
      # Same url in Github Callback URL
      - OAUTH2_PROXY_REDIRECT_URL=http://localhost/oauth2/callback
      # Generate secret -&gt; python -c 'import os,base64; print(base64.urlsafe_b64encode(os.urandom(32)))'
      - OAUTH2_PROXY_COOKIE_SECRET=redacted
      - OAUTH2_PROXY_COOKIE_SECURE=false
      - OAUTH2_PROXY_COOKIE_REFRESH=2h
      - OAUTH2_PROXY_PASS_ACCESS_TOKEN=true
      #- OAUTH2_PROXY_SET_AUTHORIZATION_HEADER=true
      #- OAUTH2_PROXY_SET_XAUTHREQUEST=true
      - OAUTH2_PROXY_PROVIDER=google
      - OAUTH2_PROXY_SKIP_JWT_BEARER_TOKENS=true
      - OAUTH2_EXTRA_JWT_ISSUERS=redacted
      # Github CLIENT_ID and CLIENT_SECRET
      - OAUTH2_PROXY_CLIENT_ID=redacted
      - OAUTH2_PROXY_CLIENT_SECRET=redacted
    networks:
      - frontend

  nginx:
    build: ./nginx
    container_name: nginx
    ports:
      - 80:80
    depends_on:
      - oauth2_proxy
    networks:
      - frontend

networks:
    frontend:
        driver: bridge
    backend:
        driver: bridge

volumes:
    dbdata:
</code></pre>
<p>Any thoughts about it?</p>",1,0,2022-06-22 19:34:59.467000 UTC,,,0,docker|nginx|oauth|mlflow|oauth2-proxy,346,2017-03-13 23:00:52.210000 UTC,2022-09-24 22:39:47.647000 UTC,,344,16,3,79,,,,,,['mlflow']
How to add a file to a dvc-tracked folder without pulling the whole folder's content?,"<p>Let's say I am working inside a git/dvc repo. There is a folder <code>data</code> containing 100k small files. I track it with DVC as a single element, as recommended by the doc:</p>
<pre class=""lang-sh prettyprint-override""><code>dvc add data
</code></pre>
<p>and because in my experience, DVC is kinda slow when tracking that many files one by one.</p>
<p>I clone the repo on another workspace, and now I have the <code>data.dvc</code> file locally but none of the actual files inside yet. I want to add a file named <code>newfile.txt</code> to the <code>data</code> folder and track it with DVC. Is there a way to do this <em>without pulling the whole content of <code>data</code> locally</em> ?</p>
<p>What I have tried for now:</p>
<ol>
<li><p>Adding the <code>data</code> folder again:</p>
<pre><code>mkdir data
mv path/to/newfile.txt data/newfile.txt
dvc add data
</code></pre>
<p>The <code>data.dvc</code> file is built again from the local state of <code>data</code> which only contains <code>newfile.txt</code> so this doesn't work.</p>
</li>
<li><p>Adding the file as a single element in <code>data</code> folder:</p>
<pre><code> dvc add data/newfile.txt
</code></pre>
<p>I get :</p>
<pre><code> Cannot add 'data/newfile.txt', because it is overlapping with other DVC tracked output: 'data'. 
 To include 'data/newfile.txt' in 'data', run 'dvc commit data.dvc'
</code></pre>
</li>
<li><p>Using dvc commit as suggested</p>
<pre><code> mkdir data
 mv path/to/newfile.txt data/newfile.txt
 dvc commit data.dvc
</code></pre>
<p>Similarly as 1., the <code>data.dvc</code> is rebuilt again from local state of <code>data</code>.</p>
</li>
</ol>",1,1,2021-05-06 15:25:19.220000 UTC,,2021-05-06 15:28:32.033000 UTC,3,dvc,1781,2021-05-06 14:35:20.937000 UTC,2021-07-20 09:38:49.723000 UTC,,31,0,0,1,,,,,,['dvc']
working directory changes to /tmp/ when python script runs with mlflow,"<p>I have a strange issue with python working directory when running with mlflow run -e build .
The script running successfully locally/using IDE, but when running it with mlflow the problem is that the working directory changes to /tmp folders instead of the correct working directory where the script resides (I have some path dependencies that certain folders should be present in ./* so thats why my process fails.</p>
<p>I had a feeling that something with the working directory messed up so I did os.getcwd() prints and saw the issue with temp folders.</p>
<p>I had a similar project that I configured in a similar manner before and didn't have these issues.</p>
<p>any idea what might be the issue?</p>",1,0,2020-12-16 19:39:34.247000 UTC,,,0,python|path|mlflow,286,2017-11-20 13:50:10.877000 UTC,2022-02-14 08:54:35.333000 UTC,,174,15,0,62,,,,,,['mlflow']
How to get `run_id` when using MLflow Project,"<p>When using MLflow Projects (via an <code>MLproject</code> file) I get this message at starting time:</p>
<pre><code>INFO mlflow.projects.backend.local: 
=== Running command 'source /anaconda3/bin/../etc/profile.d/conda.sh &amp;&amp; 
conda activate mlflow-4736797b8261ec1b3ab764c5060cae268b4c8ffa 1&gt;&amp;2 &amp;&amp; 
python3 main.py' in run with ID 'e2f0e8c670114c5887963cd6a1ac30f9' === 
</code></pre>
<p>I want to access the <code>run_id</code> shown above (<strong>e2f0e8c670114c5887963cd6a1ac30f9</strong>) from inside the main script.</p>
<p>I expected a run to be active but:</p>
<pre><code>mlflow.active_run()
&gt; None
</code></pre>
<p>Initiating a run inside the main script does give me access the correct <code>run_id</code>, although any subsequent runs will have a different <code>run_id</code>.</p>
<pre><code># first run inside the script - correct run_id
with mlflow.start_run():
   print(mlflow.active_run().info.run_id)
&gt; e2f0e8c670114c5887963cd6a1ac30f9

# second run inside the script - wrong run_id
with mlflow.start_run():
   print(mlflow.active_run().info.run_id)
&gt; 417065241f1946b98a4abfdd920239b1
</code></pre>
<p>Seems like a strange behavior, and I was wondering if there's another way to access the <code>run_id</code> assigned at the beginning of the <code>MLproject</code> run?</p>",1,1,2022-03-18 18:29:02.007000 UTC,,,0,mlflow,143,2018-05-30 13:24:03.970000 UTC,2022-09-16 21:48:03.530000 UTC,,1,0,0,5,,,,,,['mlflow']
Python getting model estimator object from Sagemaker model.tar.gz (for prediction instance explanation),"<p>I have been using ELI5 and LIME lately to provide some insight on what's driving specific predictions. They both work by passing the estimator object and the observed feature values to the function and then returns the prediction explanation.</p>

<p>Can this be used similarly with Sagemaker predictions? In a perfect world, I would start up an endpoint, make predictions, then retrieve the actual estimator object from the model.tar.gz outpt and pass this to ELI5. But I would love to hear if this is even doable before I go down this rabbit hole (and any specifics for the approach)...</p>",1,0,2019-08-12 19:07:38.630000 UTC,,,0,python|amazon-web-services|amazon-sagemaker,157,2015-01-15 17:43:03.700000 UTC,2022-08-23 22:54:25.603000 UTC,,1387,51,1,153,,,,,,['amazon-sagemaker']
How to log metrics to Azure ML Metrics Tab,"<p>I have the following train.py file</p>
<pre><code>import argparse
import os
import numpy as np
import glob
# import joblib
import mlflow
import logging
import azureml.core
import pandas as pd
import numpy as np
import pandas as pd 

from matplotlib import pyplot as plt
from azureml.core import Workspace, Dataset
from azureml.core.experiment import Experiment
from azureml.core.workspace import Workspace
from azureml.core.dataset import Dataset
from azureml.train.automl import AutoMLConfig
from sklearn.preprocessing import LabelEncoder,OneHotEncoder
import lightgbm as lgb
from sklearn.datasets import load_boston
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from pandas import DataFrame
from sklearn.preprocessing import LabelEncoder,OneHotEncoder
from sklearn.linear_model import LogisticRegression


# let user feed in 2 parameters, the dataset to mount or download,
# and the regularization rate of the logistic regression model
parser = argparse.ArgumentParser()
parser.add_argument(
    &quot;--tablename&quot;, type=str, dest=&quot;tablename&quot;, help=&quot;Table name&quot;
)
args = parser.parse_args()

tablename = args.tablename


subscription_id = ''
resource_group = 'mlplayground'
workspace_name = 'mlplayground'

workspace = Workspace(subscription_id, resource_group, workspace_name)

dataset = Dataset.get_by_name(workspace, name=tablename)
data = dataset.to_pandas_dataframe()

# use mlflow autologging
mlflow.autolog()

data.drop(['postal_code','Column1','province','region','lattitude','longitude'], axis=1, inplace=True)
one_hot_state_of_the_building=pd.get_dummies(data.state_of_the_building) 
one_hot_city = pd.get_dummies(data.city_name, prefix='city')

#removing categorical features 
data.drop(['city_name','state_of_the_building'],axis=1,inplace=True)  

#Merging one hot encoded features with our dataset 'data' 
data=pd.concat([data,one_hot_city,one_hot_state_of_the_building,],axis=1) 

data['pricepersqm'] = data.price / data.house_area

x=data.drop('price',axis=1) 
y=data.price 

X_df = DataFrame(x, columns= data.columns)
X_train, X_test, y_train, y_test = train_test_split(X_df, y, test_size=0.20)

#Converting the data into proper LGB Dataset Format
d_train=lgb.Dataset(X_train, label=y_train)


#Declaring the parameters
params = {
    'task': 'train', 
    'boosting': 'gbdt',
    'objective': 'regression',
    'num_leaves': 10,
    'learning_rate': 0.01,
    'metric': {'l2','l1'},
    'verbose': -1
}

print(&quot;Train a LightGBM Regression model&quot;)
clf=lgb.train(params,d_train,1000)

#model prediction on X_test
print(&quot;Predict the test set&quot;)
y_pred=clf.predict(X_test)

#using RMSE error metric
mse =mean_squared_error(y_pred,y_test)
print(&quot;RMSE: &quot;, mse**0.5)
mlflow.log_metric(&quot;RMSE&quot;, mse**0.5)
</code></pre>
<p>And then from a notebook file I use the following:</p>
<pre><code>from azureml.core import Workspace
from azureml.core import Experiment

# connect to your workspace
ws = Workspace.from_config()

experiment_name = &quot;get-started-with-jobsubmission-tutorial-andlightgbm&quot;
exp = Experiment(workspace=ws, name=experiment_name)



from azureml.core.environment import Environment

# use a curated environment that has already been built for you

env = Environment.get(workspace=ws, 
                      name=&quot;AzureML-sklearn-1.0-ubuntu20.04-py38-cpu&quot;, 
                      version=1)

from azureml.core import ScriptRunConfig

args = [&quot;--tablename&quot;, &quot;BelgiumRealEstate&quot;]

src = ScriptRunConfig(
    source_directory=&quot;&quot;,
    script=&quot;train.py&quot;,
    arguments=args,
    compute_target=&quot;local&quot;,
    environment=env,
)

run = exp.submit(config=src)
run.wait_for_completion(show_output=True)
</code></pre>
<p>As you can see in the train.py file I am logging the RMSE, however the metric does not appear on the metrics tab.</p>
<p>What should I do?</p>",1,0,2022-08-17 09:44:50.553000 UTC,,,0,python|azure-machine-learning-service|mlflow,40,2011-04-05 19:05:03.093000 UTC,2022-09-16 12:42:27.473000 UTC,"Brussels, Bélgica",30340,1667,79,2937,,,,,,"['mlflow', 'azure-machine-learning-service']"
"Second dvc push on AWS Batch using IAM role gets ""Unable to locate credentials""","<p>I'm running a job on AWS Batch, and this job prepares some data and versions it using <code>dvc</code>. Secondly, the job does some transformation generating new data, and it should save this new data using <code>dvc</code> again. Also, in this case, i'm setting a instance-profile role to enable the AWS Batch to persist on my S3 bucket.</p>
<p>The first <code>dvc push</code> works perfectly. But the second one generates the error <code>Unable to locate credentials</code></p>
<p>I have also changed the script to just touch a file, add to dvc and push, and then repeat the process in with other file, and could replicate the problem.</p>
<p>I have already solved, changing the command <code>dvc push</code> to <code>dvc push especific-file-to-push</code>, but I'm now trying to understand what is the problem with <code>dvc push</code> command without the parameter specifying the file.</p>
<p>Does anybody know?</p>
<p>I'm using dvc <code>dvc==2.9.5</code> and <code>boto3==1.21.21</code></p>",0,5,2022-04-30 14:56:26.727000 UTC,,2022-04-30 15:10:34.543000 UTC,2,amazon-s3|boto3|amazon-iam|aws-batch|dvc,152,2013-09-19 22:56:26.420000 UTC,2022-09-15 12:21:12.720000 UTC,"Rio de Janeiro, Brazil",101,8,0,20,,,,,,['dvc']
Want to Tag a model package instead of model_package group with aws sagemaker,"<p>{
&quot;statusCode&quot;: 500,
&quot;body&quot;: &quot;&quot;Failed to create model package: An error occurred (ValidationException) when calling the CreateModelPackage operation: Tags are not supported in Model Package versions. Please add them to the Model Package Group.&quot;&quot;
}</p>",1,0,2022-03-28 19:20:00.620000 UTC,,,0,amazon-sagemaker,105,2022-03-28 19:15:34.533000 UTC,2022-05-06 14:16:31.597000 UTC,,1,0,0,0,,,,,,['amazon-sagemaker']
Import data and python scripts in azure ml entry script when deploying models,"<p>I have an existing machine learning model saved on my local system. I want to deploy this model as a web service so I can consume this model as a request-response i.e. send an HTTP request to the model and get back a predicted response.</p>
<p>When attempting to deploy this model on AzureML I run into a few problems</p>
<p>The model needs to be initialized in an entry script int the init() function, but for initializing my model I have a custom class and require few txt files to be loaded.</p>
<p>below is the code to initialize the model object</p>
<pre><code>from model_file import MyModelClass  # this is the file which contains the model class

def init():
  global robert_model

  my_model = MyModelClass(vocab_path='&lt;path-to-text-files&gt;',
                          model_paths=['&lt;path-to-model-file&gt;'],
                          iterations=5,
                          min_error_probability=0.0,
                          min_probability=0.0,
                          weigths=None)
def run(json_data):
  try:
    data = json.loads(json_data)
    preds, cnt = my_model.handle_batch([sentence.split()])
    return {'output': pred, 'count': cnt}
  except Exception as e:
    error = str(e)
    return error
</code></pre>
<p>I don't know how to import those class files and text files in the entry script</p>
<p>I don't know much about azure, and I am having a hard time figuring this out. Please help.</p>",3,0,2020-10-08 07:15:48.237000 UTC,,2020-10-08 17:17:30.750000 UTC,3,python|azure|machine-learning|web-deployment|azure-machine-learning-service,2616,2020-07-25 14:11:29.650000 UTC,2022-07-16 07:43:34.863000 UTC,India,91,49,0,7,,,,,,['azure-machine-learning-service']
Sagemaker model CloudFormation stack deletion,"<p>I am attempting to deploy a ""AWS::SageMaker::Model"" that is deployed within a VPC and it stands up OK but when I delete it I get the model being deleted successfully but when it attempts to delete the security group associated with it, it fails saying ""DependencyViolation"".</p>

<p>Investigation found that the Model object is removed but there is an ENI still remaining that has the security group attached to it.</p>

<p>The stack output is as follows:</p>

<p><a href=""https://i.stack.imgur.com/9Jubu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9Jubu.png"" alt=""stack_puts""></a></p>

<p>The IAM role associated with the model has the following managed policy: ""arn:aws:iam::aws:policy/AmazonSageMakerFullAccess"".</p>

<p>I know that this happened previously with Lambda when it could run within a VPC and this was fixed, I wonder if we have the same issue with Model.</p>

<p>Also a point to note, this does not appear to happen ""AWS::SageMaker::NotebookInstance"". </p>

<p>My model definition is as follows:</p>

<pre class=""lang-yaml prettyprint-override""><code>  TESTMODEL:
    Type: ""AWS::SageMaker::Model""
    Properties:
      ExecutionRoleArn: !GetAtt ExecutionRole.Arn
      PrimaryContainer:
        Image: ""514117268639.dkr.ecr.ap-southeast-2.amazonaws.com/forecasting-deepar:1""
        ModelDataUrl: ""s3://test-sagemaker/sagemaker/DEMO-deepar/output/DEMO-deepar-2018-09-03-02-18-02-278/output/model.tar.gz""
      ModelName: ""Test""
      VpcConfig:
        Subnets:
          - subnet-457ee522
          - subnet-c0b82c89
          - subnet-2cc22074
        SecurityGroupIds:
          - !GetAtt SageMakerModelSG.GroupId

  SageMakerModelSG:
    Type: ""AWS::EC2::SecurityGroup""
    Properties:
      GroupDescription: ""SageMakerModelSG""
      VpcId: vpc-4df92b2a
      Tags:
        - Key: ""Name""
          Value: !Join [ -, [ !Ref ""AWS::StackName"", ""SageMakerModelSG"" ] ]

  SageMakerModelSGIngresshttps:
    Type: ""AWS::EC2::SecurityGroupIngress""
    Properties:
      GroupId: !Ref SageMakerModelSG
      Description: ""https""
      IpProtocol: ""tcp""
      FromPort: ""443""
      ToPort: ""443""
      CidrIp: ""0.0.0.0/0""
</code></pre>",2,1,2018-09-20 01:56:18.447000 UTC,1.0,2018-09-20 05:37:03.980000 UTC,1,amazon-web-services|amazon-cloudformation|amazon-sagemaker,479,2008-09-19 15:35:39.927000 UTC,2020-04-29 00:16:36.020000 UTC,"Brisbane, Queensland, Australia",621,60,0,123,,,,,,['amazon-sagemaker']
Deploy ML Model on Azure ACI (Container) or AKS (Kubernetes),"<p>I am exploring ways to serve my trained ML models in the most cost-effective way.</p>
<p>I currently have 4 different models, where the output of the 1st model form part of the input of the 2nd model, and so on.</p>
<p>The current user base is very small and the number of required inferences is small and sporadic. Ie. 2 - 3 times every few hours, and even some days with 0 inference.</p>
<p>First I deployed with ACI, but for some reason the container instance stayed running even when no one was accessing the endpoint. I was under the impression that the instance should stop itself to avoid billing unused hours.</p>
<p>Is this something to do with the model being deployed as a real-time endpoint? Will Kubernetes deployment be more suitable (as in will it scale down to 0 node) when the endpoint/model is unused?</p>",2,0,2020-12-06 07:02:03.413000 UTC,,,1,azure|azure-aks|azure-machine-learning-service|azure-container-instances|cost-management,519,2020-07-17 23:46:37.857000 UTC,2021-11-22 09:40:44.680000 UTC,"Melbourne VIC, Australia",68,18,0,15,,,,,,['azure-machine-learning-service']
Azure Machine Learning Web service Input Data Issue,"<p>I have created an Azure ML webservice as an example and face an unknown error when it comes to deploy a web service. The error comes without an explanation, so it's hard to trace. </p>

<p>When running the experiment within the studio, the experiment was running without any issue. However, when deploy to webservice, the test function has failed with the same input as in the studio.</p>

<p>I have also published a sample of the service to see if anyone can see what the issue is.</p>

<p><a href=""https://gallery.cortanaintelligence.com/Experiment/mywebservice-1"" rel=""nofollow"">https://gallery.cortanaintelligence.com/Experiment/mywebservice-1</a></p>

<p>Some info about the service:</p>

<p>The service takes input as a string represented for a sparse feature vector of svmlight format. It will return the predicted class for the input feature vector. The error fails when running the test function from the deployed service while the experiment within the studio is running without any issue.</p>

<p>Hope anyone has an idea how it went wrong.</p>",1,0,2016-07-01 01:16:39.167000 UTC,,2016-07-01 05:29:30.220000 UTC,0,azure|python-module|azure-machine-learning-studio|svmlight,738,2016-07-01 01:02:33.223000 UTC,2021-11-23 02:55:47.327000 UTC,,1,0,0,6,,,,,,['azure-machine-learning-studio']
Amazon Sagemaker: TypeError: can't pickle dict_keys objects,"<p>I am porting some pyspark jobs that currently run on Amazon EMR to Amazon Sagemaker. But while executing one such pyspark job, I came across the following issue:</p>
<pre><code>Traceback (most recent call last):
  File &quot;/opt/ml/processing/input/code/microsegment.py&quot;, line 297, in &lt;module&gt;
    raise exc
  File &quot;/opt/ml/processing/input/code/microsegment.py&quot;, line 282, in &lt;module&gt;
    results = calculate_microsegments(dataset_location, kpis, deli)
  File &quot;/opt/ml/processing/input/code/microsegment.py&quot;, line 100, in calculate_microsegments
    eids = rdd_splitted.flatMap(lambda x: x[len(x) - 2].split(&quot;|&quot;)).distinct().collect()
  File &quot;/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py&quot;, line 413, in distinct
  File &quot;/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py&quot;, line 1625, in reduceByKey
  File &quot;/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py&quot;, line 1865, in combineByKey
  File &quot;/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py&quot;, line 1802, in partitionBy
  File &quot;/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py&quot;, line 2532, in _jrdd
  File &quot;/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py&quot;, line 2434, in _wrap_function
  File &quot;/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py&quot;, line 2420, in _prepare_for_python_RDD
  File &quot;/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py&quot;, line 607, in dumps
_pickle.PicklingError: Could not serialize object: TypeError: can't pickle dict_keys objects
</code></pre>
<p>The following is the relevant code snippet:</p>
<pre><code>sc = SparkContext(appName='microsegment-job')
rdd = sc.textFile(dataset_path)
header = rdd.first()
cols = header.split(delimiter)

cols_dict = {cols[i]: i for i in range(len(cols))}

other_kpis = []
for i in cols:
    if i in filter_cond or i == 'CONSUMER_ID':
        continue
    if i == 'eventids':
        break
    other_kpis.append(i)

def return_row(z):
    temp = z.split(delimiter)
    row = [float(temp[cols_dict[kpi]]) if temp[cols_dict[kpi]] != '' else 0.0 for kpi in (kpi_list + other_kpis)] + [
        temp[cols_dict['eventids']]] + [temp[cols_dict['conversions']]]
    return row

rdd_splitted = rdd.filter(lambda z: header not in z).map(return_row)
eids = rdd_splitted.flatMap(lambda x: x[len(x) - 2].split(&quot;|&quot;)).distinct().collect() # Exception thrown in this line
</code></pre>
<p>The Sagemaker image runs on <code>Python 3.7</code> and <code>PySpark 2.4</code>. Also attempted with a different image that runs on <code>Python 3.7</code> and <code>PySpark 3.0</code>. Both resulting in same error.</p>
<p>The EMR cluster runs on <code>Python 2.7.16</code> and <code>PySpark 2.4.4</code></p>",0,1,2021-10-05 10:14:11.657000 UTC,,,1,python|python-3.x|python-2.7|pyspark|amazon-sagemaker,163,2012-05-07 08:41:13.727000 UTC,2022-06-05 02:47:59.150000 UTC,"Thiruvananthapuram, India",7319,226,85,1094,,,,,,['amazon-sagemaker']
AMLS compute: Notebooks on VM,"<p>Can anyone please help me understand the following:</p>

<ol>
<li>Can you run two separate notebooks powered by two separate computes/VMs? 
I've tried running two notebooks on separate VMS on AMLS workspace portal, it always selects the same VM for each notebook. Is this possible?</li>
<li>If using the same VM compute on AMLS to run multiple notebooks, does it use a separate core within the VM to run each notebook?</li>
</ol>",1,0,2020-01-07 11:49:54.513000 UTC,,2020-01-07 12:08:05.117000 UTC,0,azure|azure-machine-learning-service,63,2012-02-02 17:20:06.633000 UTC,2020-10-09 13:48:41.317000 UTC,,9,0,0,33,,,,,,['azure-machine-learning-service']
Which user settings are used by python's virtual environment?,"<p><strong>Note: The following behaviour is observed on MAC OS Monterey 12.3.1</strong></p>
<p>When I ran a command to start the mlflow server <strong>inside a newly created python venv</strong>:</p>
<blockquote>
<p>mlflow server --backend-store-uri postgresql://mlflow_user:mlflow@localhost/mlflow  --artifacts-destination S3://&lt;bucket_name&gt;/mlflow/ --serve-artifacts  -h 0.0.0.0 -p 8000</p>
</blockquote>
<p>It gave me <code>Access denied</code> error:</p>
<blockquote>
<p>botocore.exceptions.ClientError: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied</p>
</blockquote>
<p>Now, when I tried it using <code>sudo</code>, i.e.,</p>
<blockquote>
<p>sudo mlflow server --backend-store-uri postgresql://mlflow_user:mlflow@localhost/mlflow  --artifacts-destination S3://&lt;bucket_name&gt;/mlflow/ --serve-artifacts  -h 0.0.0.0 -p 8000</p>
</blockquote>
<p>I could access the bucket's artifacts.</p>
<p><strong>My Question:</strong></p>
<p>When I didn't used <code>sudo</code>, which user settings were used by the python's venv</p>",0,0,2022-09-02 07:04:16.747000 UTC,,2022-09-02 10:21:54.340000 UTC,0,python-3.x|amazon-s3|sudo|python-venv|mlflow,24,2017-01-15 18:29:12.247000 UTC,2022-09-24 17:54:49.473000 UTC,"Mumbai, India",4433,121,31,885,,,,,,['mlflow']
MLFLOW - Is there a way to override Pytorch Wrapper?,"<p>I am trying to use pytorch in mlflow. Currently, predict method allows data that is only of pd.DataFrame or np.ndarray type, is there a way to override this and write a custom predict method without writing a completely new loader_module?</p>
<p>The source code for the predict method can be found here -&gt; <a href=""https://github.com/mlflow/mlflow/blob/master/mlflow/pytorch/__init__.py#L706-L736"" rel=""nofollow noreferrer"">pytorch</a></p>",1,1,2021-07-23 11:56:20.293000 UTC,,2021-07-23 12:00:52.067000 UTC,-1,python|mlflow,98,2021-07-23 11:40:18.087000 UTC,2021-08-05 05:22:20.313000 UTC,,1,0,0,2,,,,,,['mlflow']
install python package in Azure ml,"<p>i am trying to install python package called delta-sharing. I was able to install (<strong>pip install delta-sharing</strong>) successfully in computer terminal. But when i try to <strong>import delta_sharing</strong> in the notebook, it is not found. I am wondering if i miss anything here?</p>
<p>Thanks</p>",1,0,2021-06-24 15:00:36.587000 UTC,,,0,python|azure-machine-learning-service,73,2017-10-16 03:07:21.893000 UTC,2022-04-21 14:22:50.677000 UTC,Toronto,11,0,0,4,,,,,,['azure-machine-learning-service']
Getting model metadata from SageMaker model endpoint,"<p>I have a TF model object deployed to SageMaker endpoint, and it's working fine when I invoke it to make a prediction. On its own, the model object itself contains key attributes that is accessible if I open it with <code>h5py.File()</code> like this:</p>
<pre><code>with h5py.File(model_path2, 'r') as f:
    labels = [n.decode(&quot;ascii&quot;, &quot;ignore&quot;) for n in f['labels']]
    img_norm_vec = np.array(f['norm_vector'])
</code></pre>
<p>My question is, can I access the metadata attributes from a SM endpoint? I searched through the SM documentation and didn't see anything related to this.</p>",1,0,2021-09-09 18:31:12.073000 UTC,,,0,tensorflow2.0|amazon-sagemaker|h5py,90,2016-06-11 17:04:44.867000 UTC,2022-03-14 02:33:05.080000 UTC,,101,5,0,11,,,,,,['amazon-sagemaker']
setup early stopping for Vertex automl for text classification,"<p>I'm running an automl job using google cloud SDK like this:</p>
<p>I couldn't see any parameters to set up early stopping. Is this a missing feature or I am missing something here?</p>
<pre><code>job = aiplatform.AutoMLTextTrainingJob(
    display_name=training_job_display_name,
    prediction_type=&quot;classification&quot;,
    multi_label=False,
)

model = job.run(
    dataset=text_dataset,
    model_display_name=model_display_name,
    training_fraction_split=0.1,
    validation_fraction_split=0.1,
    test_fraction_split=0.1,
    sync=True,
)
</code></pre>",0,1,2022-09-14 14:15:10.023000 UTC,,,0,google-cloud-vertex-ai,13,2016-06-24 15:48:30.737000 UTC,2022-09-23 18:59:59.153000 UTC,"New York, NY, USA",2701,598,6,260,,,,,,['google-cloud-vertex-ai']
Using SageMaker pipe mode with an s3 directory of tfrecords,"<p>My call to <code>sagemaker.tensorflow.TensorFlow.fit()</code> hangs indefinitely with no error message when I use <code>Pipe</code> instead of <code>File</code> as the <code>input_mode</code>. I correspondingly replace the TensorFlow <code>Dataset</code> with <code>Pipemodedataset</code>. The training in <code>File</code> mode completes successfully.</p>
<p>My data consists of two s3 buckets with multiple tfrecord files in each. Despite having looked extensively through the documentation, I am not confident about how to use the <code>Pipemodedataset</code> in this case - specifically, how to set the <code>channel</code>.</p>
<p>Here is my Sagemaker notebook setup:</p>
<pre><code>hyperparameters = {
    &quot;batch-size&quot;: 1,
    &quot;pipe_mode&quot;: 1,
}

estimator_config = {
    &quot;entry_point&quot;: &quot;tensorflow_train.py&quot;,
    &quot;source_dir&quot;: &quot;source&quot;,
    &quot;framework_version&quot;: &quot;2.3&quot;,
    &quot;py_version&quot;: &quot;py37&quot;,
    &quot;instance_type&quot;: &quot;ml.p3.2xlarge&quot;,
    &quot;instance_count&quot;: 1,
    &quot;role&quot;: sagemaker.get_execution_role(),
    &quot;hyperparameters&quot;: hyperparameters,
    &quot;output_path&quot;: f&quot;s3://{bucket_name}&quot;,
    &quot;input_mode&quot;: &quot;Pipe&quot;,
}

tf_estimator = TensorFlow(**estimator_config)

s3_data_channels = {
    &quot;training&quot;: f&quot;s3://{bucket_name}/data/training&quot;,
    &quot;validation&quot;: f&quot;s3://{bucket_name}/data/validation&quot;,
}

tf_estimator.fit(s3_data_channels)
</code></pre>
<p>If I were to run <code>aws s3 ls</code> on the <code>s3_data_channels</code>, I'd get a list of tfrecord files.</p>
<p>Here is the way I set up the dataset (see the if / else statement depending on whether <code>pipe_mode</code> is selected:</p>
<pre><code>import tensorflow as tf

if __name__ == &quot;__main__&quot;:

    arg_parser = argparse.ArgumentParser()
    ...
    arg_parser.add_argument(&quot;--pipe_mode&quot;, type=int, default=0)

    arg_parser.add_argument(&quot;--train_dir&quot;, type=str, default=os.environ.get(&quot;SM_CHANNEL_TRAINING&quot;))
    arg_parser.add_argument(
        &quot;--validation_dir&quot;, type=str, default=os.environ.get(&quot;SM_CHANNEL_VALIDATION&quot;)
    )
    arg_parser.add_argument(&quot;--model_dir&quot;, type=str)
    args, _ = arg_parser.parse_known_args()

    AUTOTUNE = tf.data.experimental.AUTOTUNE

    if args.pipe_mode == 1:
        from sagemaker_tensorflow import PipeModeDataset
        train_ds = PipeModeDataset(channel=&quot;training&quot;, record_format='TFRecord')
        val_ds = PipeModeDataset(channel=&quot;validation&quot;, record_format='TFRecord')

    else:
        train_files = tf.data.Dataset.list_files(args.train_dir + '/*tfrecord')
        val_files = tf.data.Dataset.list_files(args.validation_dir + '/*tfrecord')
        train_ds = tf.data.TFRecordDataset(filenames=train_files, num_parallel_reads=AUTOTUNE)
        val_ds = tf.data.TFRecordDataset(filenames=val_files, num_parallel_reads=AUTOTUNE)

    train_ds = (
        train_ds.map(tfrecord_parser, num_parallel_calls=AUTOTUNE)
        .batch(args.batch_size)
        .prefetch(AUTOTUNE)
    )

    val_ds = (
        val_ds.map(tfrecord_parser, num_parallel_calls=AUTOTUNE)
        .batch(args.batch_size)
        .prefetch(AUTOTUNE)
    )
    ...
</code></pre>",1,1,2021-11-04 17:05:24.567000 UTC,1.0,,3,tensorflow|deep-learning|amazon-sagemaker|tensorflow-datasets,368,2014-09-04 17:04:03.457000 UTC,2022-03-07 06:31:58.937000 UTC,,789,129,0,54,,,,,,['amazon-sagemaker']
Azure Devops: Model Register and package through Azure CLI in Devops pipeline Task,"<p>While Registering the model in Azure devops pipeline task through Inline script, its giving below issue:</p>
<p>Command:</p>
<pre><code>az ml model register -g $(ml.resourceGroup) -w $(ml.workspace) -n model_test --model-path ./Configuration/outputs/ -t model.json
</code></pre>
<p>Error:</p>
<blockquote>
<p>Encountered authorization while uploading to blob storage. Please
check the blob storage account attached to your workspace. Make sure
current user is authorized to access the storage account and that the
request is not blocked by firewall , virtual network and other
security setting.</p>
<p>status code: 403</p>
</blockquote>
<p>can someone help e on above issue?</p>",1,0,2021-11-03 11:32:51.583000 UTC,,2021-11-03 11:37:54.660000 UTC,2,azure|azure-devops|azure-pipelines|azure-machine-learning-service,123,2021-11-03 11:27:49.703000 UTC,2022-04-14 12:06:14.393000 UTC,,21,0,0,0,,,,,,['azure-machine-learning-service']
Read Excel file in Amazon Sage maker using R Notebook,"<p>I am having S3 bucket named &quot;Temp-Bucket&quot;. Inside that I am having folder named &quot;folder&quot;.
I want to read file named file1.xlsx. This file is present inside the S3 bucket(Temp-Bucket) under the folder (folder). How to read that file ?</p>",1,0,2022-01-04 17:05:20.903000 UTC,,,0,r|file|amazon-sagemaker,121,2020-04-24 10:34:21.987000 UTC,2022-08-13 06:30:57.293000 UTC,,95,39,0,30,,,,,,['amazon-sagemaker']
Azure Machine Learning in dynamics,"<p>I just started to work with Azure Machine Learning using Microsoft Azure Machine Learning Studio.</p>

<p>Could you please advise the proper way of using Classification Model so that the Model can analyze the window/interval with series of information before it reaches the target/class which should be predicted further on by Model?</p>

<p>Our problem could be solved only by analyzing all the previous information in it's dynamic/evolution (e.g. depending on the dynamical change of the Patient's medical results it could be found out some sickness including it's current stage and the stage which we could expect in medium term).</p>

<p>For example in an input file we do not provide in each row together with the variables the target info, it is shown only at the row/moment when the situation is matured to reach such target.  </p>

<p>If it is already available some materials/tutorials on this subject in Azure ML or somewhere else I would highly appreciate such info and links.</p>

<p>Thanks in advance for your kind support!</p>

<p>Best regards,
Berik</p>",1,0,2017-02-07 19:07:12.343000 UTC,,,2,azure-machine-learning-studio,87,2017-02-07 18:28:54.547000 UTC,2017-02-23 09:12:35.727000 UTC,,21,0,0,0,,,,,,['azure-machine-learning-studio']
VertexAI Batch Inference Failing for Custom Container Model,"<p>I'm having trouble executing VertexAI's batch inference, despite endpoint deployment and inference working perfectly. My TensorFlow model has been trained in a custom Docker container with the following arguments:</p>
<pre><code>aiplatform.CustomContainerTrainingJob(
        display_name=display_name,
        command=[&quot;python3&quot;, &quot;train.py&quot;],
        container_uri=container_uri,
        model_serving_container_image_uri=container_uri,
        model_serving_container_environment_variables=env_vars,
        model_serving_container_predict_route='/predict',
        model_serving_container_health_route='/health',
        model_serving_container_command=[
            &quot;gunicorn&quot;,
            &quot;src.inference:app&quot;,
            &quot;--bind&quot;,
            &quot;0.0.0.0:5000&quot;,
            &quot;-k&quot;,
            &quot;uvicorn.workers.UvicornWorker&quot;,
            &quot;-t&quot;,
            &quot;6000&quot;,
        ],
        model_serving_container_ports=[5000],
)
</code></pre>
<p>I have a Flask endpoint defined for predict and health essentially defined below:</p>
<pre><code>@app.get(f&quot;/health&quot;)
def health_check_batch():
    return 200

@app.post(f&quot;/predict&quot;)
def predict_batch(request_body: dict):
    pred_df = pd.DataFrame(request_body['instances'],
                           columns = request_body['parameters']['columns'])
    # do some model inference things
    return {&quot;predictions&quot;: predictions.tolist()}
</code></pre>
<p>As described, when training a model and deploying to an endpoint, I can successfully hit the API with JSON schema like:</p>
<pre><code>{&quot;instances&quot;:[[1,2], [1,3]], &quot;parameters&quot;:{&quot;columns&quot;:[&quot;first&quot;, &quot;second&quot;]}}
</code></pre>
<p>This also works when using the endpoint Python SDK and feeding in instances/parameters as functional arguments.</p>
<p>However, I've tried performing batch inference with a CSV file and a JSONL file, and every time it fails with an Error Code 3. I can't find logs on why it failed in Logs Explorer either. I've read through all the documentation I could find and have seen other's successfully invoke batch inference, but haven't been able to find a guide. Does anyone have recommendations on batch file structure or the structure of my APIs? Thank you!</p>",0,1,2022-04-24 07:29:05.693000 UTC,,,0,google-cloud-platform|google-cloud-vertex-ai,111,2016-06-28 17:55:10.360000 UTC,2022-05-23 21:53:23.470000 UTC,,33,3,0,7,,,,,,['google-cloud-vertex-ai']
Data version control (DVC) commands not working ---> TypeError: public() got an unexpected keyword argument 'SEP',"<p>All of a sudden, dvc has stopped functioning.
Any command typed fails and throws an exception.
example. dvc remote list results in -</p>
<pre><code>Traceback (most recent call last):
  File &quot;/home/dev2/.local/bin/dvc&quot;, line 5, in &lt;module&gt;
    from dvc.main import main
  File &quot;/home/dev2/.local/lib/python3.6/site-packages/dvc/main.py&quot;, line 6, in &lt;module&gt;
    from dvc import analytics
  File &quot;/home/dev2/.local/lib/python3.6/site-packages/dvc/analytics.py&quot;, line 16, in &lt;module&gt;
    from dvc.lock import Lock, LockError
  File &quot;/home/dev2/.local/lib/python3.6/site-packages/dvc/lock.py&quot;, line 8, in &lt;module&gt;
    import flufl.lock
  File &quot;/home/dev2/.local/lib/python3.6/site-packages/flufl/lock/__init__.py&quot;, line 3, in &lt;module&gt;
    from flufl.lock._lockfile import (
  File &quot;/home/dev2/.local/lib/python3.6/site-packages/flufl/lock/_lockfile.py&quot;, line 54, in &lt;module&gt;
    public(SEP=SEP)
TypeError: public() got an unexpected keyword argument 'SEP'
 
</code></pre>
<p>Any suggestions will be of great help.</p>",0,4,2020-06-23 07:48:46.370000 UTC,,,2,linux|git|dvc,307,2018-08-13 13:04:32.313000 UTC,2022-09-19 22:24:23.697000 UTC,,449,3,2,80,,,,,,['dvc']
How to access files stored in AzureDataLake and use this file as input to AzureBatchStep in azure.pipleline.step?,"<p>I registered an Azure data lake datastore as in the <a href=""https://docs.microsoft.com/zh-cn/python/api/azureml-core/azureml.core.datastore.datastore?view=azure-ml-py#register-azure-data-lake-workspace--datastore-name--store-name--tenant-id--client-id--client-secret--resource-url-none--authority-url-none--subscription-id-none--resource-group-none--overwrite-false-"" rel=""nofollow noreferrer"">documentation</a> in order to access the files stored in it. </p>

<p>I used </p>

<pre><code>DataReference(datastore, data_reference_name=None, path_on_datastore=None, mode='mount', path_on_compute=None, overwrite=False) 
</code></pre>

<p>and used it as input to azure pipeline step in <a href=""https://docs.microsoft.com/zh-cn/python/api/azureml-pipeline-steps/azureml.pipeline.steps.azurebatch_step.azurebatchstep?view=azure-ml-py"" rel=""nofollow noreferrer""><code>AzureBatchStep</code></a> method. </p>

<p>But I got an issue: that datastore name could not be fetched in input.<br>
Is Azure Data Lake not accessible in Azure ML or am I getting it wrong?</p>",1,1,2019-04-23 08:48:32.173000 UTC,,2019-04-24 05:47:09.317000 UTC,0,azure-machine-learning-service,106,2019-04-22 10:04:08.197000 UTC,2022-09-23 06:42:26.117000 UTC,,1,0,0,23,,,,,,['azure-machine-learning-service']
Is it possible to use authentication and authorization in MLFlow Server?,"<p>MLFlow does not have integrated authentication (openID, LDAP, kerberos, AAD...) or authorization (RBAC, ABAC, ACL...)</p>
<p>Is it only possible with a web proxy in MLFlow? p.e: nginx</p>
<p>Does anyone know of an option similar to Apache Sentry or Apache Ranger for MLFlow?</p>
<p>Thank you</p>",0,0,2022-08-08 11:01:18.063000 UTC,1.0,,0,mlflow,57,2016-05-22 18:52:36.183000 UTC,2022-09-22 08:14:44.890000 UTC,,1,0,0,0,,,,,,['mlflow']
AWS SageMaker hosting multiple models on the same machine (ML compute instance),"<p>I am able to host the models developed in <em>SageMaker</em> by using the deploy functionality. Currently, I see that the different models that I have developed needs to deployed on different ML compute instances.</p>

<p>Is there a way to deploy all models on the same instance, using separate instances seems to be very expensive option. If it is possible to deploy multiple models on the same instance, will that create different endpoints for the models?</p>",2,0,2018-03-22 06:36:41.540000 UTC,,2020-10-26 02:15:25.603000 UTC,3,amazon-web-services|machine-learning|hosting|endpoint|amazon-sagemaker,2487,2017-06-22 17:49:22.837000 UTC,2019-11-21 13:51:01.787000 UTC,,31,0,0,2,,,,,,['amazon-sagemaker']
Sagemaker validation error about running its pre-built random-forest algorithm,"<p>I'm trying to run some training on the Sagemaker using Random-Forest and its giving me this validation error. I'm not sure if I need to adjust hyper-parameters. I tried but still an error. Here is the full text of the error.</p>

<p>""Failure reason
ClientError: Unable to initialize the algorithm. Failed to validate input data configuration. (caused by ValidationError) Caused by: u'FullyReplicated' is not one of [u'ShardedByS3Key'] Failed validating u'enum' in schema[u'properties'][u'train'][u'properties'][u'S3DistributionType']: {u'enum': [u'ShardedByS3Key'], u'type': u'string'} On instance[u'train'][u'S3DistributionType']: u'FullyReplicated'"" </p>

<p>I have tried different parameters - but I still get the same results.</p>",1,0,2019-05-31 17:07:13.717000 UTC,,2019-05-31 20:35:53.133000 UTC,0,random-forest|amazon-sagemaker,503,2018-12-23 00:19:51.677000 UTC,2022-09-23 18:48:55.043000 UTC,"Kansas City, MO, USA",91,24,0,24,,,,,,['amazon-sagemaker']
How to use multiple ml models trained on different input data to produce one model and give prediction in Sagemaker?,"<p>I am working on a saree tags extraction problem. Tags are like Saree color, Saree type, border design type etc. There are total 176 different tags.</p>
<p>Initially I worked on it as a multi-label problem in which I had used 176 Sigmoid function in the output layer. But it did not work as expected and the accuracy I got was very poor.</p>
<p>Since all the labels in my problems are not independent e.g. If saree is of green color then It won't be red or black, If saree is of Banarasi type then it won't be of other type mentioned in my tags list. So now I am planning to use multiple ML models and each model will be multi class classification model like one model will predict color, another one will predict type, another will predict weight and so on..</p>
<p>I am using aws sagemaker to build and deploy models, but my problem is how to deploy all these models via sagemaker sothat all models will be called and at the end combined output of all should be sent.</p>
<p>I explored multimodel sagemaker endpoint deployment but in that only one model can be used for prediction. So it didn't fulfil my purpose.</p>
<p>Any suggestion or help would be highly appreciated.</p>",1,0,2022-02-16 12:34:51.407000 UTC,,,0,machine-learning|deployment|amazon-sagemaker,86,2022-02-16 12:06:17.057000 UTC,2022-09-24 16:53:17.943000 UTC,,7,0,0,1,,,,,,['amazon-sagemaker']
'DecisionTreeRegressor' object has no attribute 'n_features_': 0,"<p>i had deployed my random Forest Regressor on AzureML.when i tried to request the scoring url i got the &quot;'DecisionTreeRegressor' object has no attribute 'n_features_': 0&quot;<a href=""https://i.stack.imgur.com/TJU5a.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p><a href=""https://i.stack.imgur.com/LVwEP.png"" rel=""nofollow noreferrer"">My request code and error image</a></p>",0,0,2022-03-13 17:15:36.773000 UTC,,,0,python|azure-machine-learning-service,116,2022-03-13 17:10:05.910000 UTC,2022-09-18 06:30:01.360000 UTC,,1,0,0,0,,,,,,['azure-machine-learning-service']
"AWS Ground Truth text classification manifest using ""source-ref"" not displaying text","<h2>Background</h2>

<p>I'm trying out SageMaker Ground Truth, <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/sms.html"" rel=""nofollow noreferrer"">an AWS service to help you label your data before using it in your ML algorithms</a>.</p>

<p>The labeling job requires a manifest file which contains a JSON object per row that contains a <code>source</code> or a <code>source-ref</code>, see also the <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/sms-data-input.html"" rel=""nofollow noreferrer"">Input Data section</a> of the documentation. </p>

<h2>Setup</h2>

<p>Source-ref is a reference to where the document is located in an S3 bucket like so</p>

<pre><code>my-bucket/data/manifest.json
my-bucket/data/123.txt
my-bucket/data/124.txt

...
</code></pre>

<p>The manifest file looks like this (based on the <a href=""https://aws.amazon.com/blogs/aws/amazon-sagemaker-ground-truth-build-highly-accurate-datasets-and-reduce-labeling-costs-by-up-to-70/"" rel=""nofollow noreferrer"">blog example</a>) :</p>

<pre><code>{""source-ref"": ""s3://my-bucket/data/123.txt""}
{""source-ref"": ""s3://my-bucket/data/124.txt""}
...
</code></pre>

<h2>The problem</h2>

<p>When I create the job, all I get is the <code>source-ref</code> value: <strong>s3://my-bucket/data/123.txt</strong> as the text, the contents of the file are not displayed.</p>

<p>I have tried creating jobs using a manifest that does not contain the s3 protocol, but I get the same result.</p>

<p>Is this a bug on their end or I'm I missing something?</p>

<h2>Observations</h2>

<ul>
<li>I have tried to make all files public, thinking there may maybe permissions issue? but no</li>
<li>I ensured that the content type of the file was text (s3 -> object -> properties -> metadata)</li>
<li>If I use ""source"" and inline the text, it works properly, but I should be able to use individual documents as there is a limit on the file size specially if I have to label many or large documents!</li>
</ul>",2,0,2018-12-05 03:05:58.243000 UTC,,,2,amazon-web-services|machine-learning|amazon-sagemaker,1506,2009-08-12 17:14:36.470000 UTC,2022-09-21 03:46:43.037000 UTC,"Santa Monica, CA, USA",7013,2704,18,445,,,,,,['amazon-sagemaker']
Restore a specific checkpoint for deploying with Sagemaker and TensorFlow,"<p>I'm using SageMaker for training some custom TF model I realized. During training I naturally evaluate the model multiple times in order to understand when the NN actually starts overfitting. After training I'd like to restore the model that works best (i.e. which presents minimum validation loss) and deploy it on an endpoint. However, if I use the classic Tensorflow.attach() the model that is restored corresponds with the one stored in output/model.tar.gz, which, if I got it correctly, would be the one corresponding with the last training iteration (thus it may overfits). </p>

<p>Is there a way for specifying to SageMaker which checkpoint restoring without necessarily retraining the model with early stopping? Even forcing SM to save in model.tar.gz the model that presents minimum validation loss and not the last one would work for me, unfortunately I didn't find any immediate way to do so...</p>

<p>Thank you!</p>",1,0,2018-10-23 11:49:56.750000 UTC,,,3,python|tensorflow|amazon-sagemaker,975,2018-10-23 11:36:30.877000 UTC,2018-12-18 17:55:43.207000 UTC,"London, Regno Unito",31,0,0,2,,,,,,['amazon-sagemaker']
"Azure ML: how to change the ""cluster purpose"" of an existing inference cluster from ""dev_test"" to ""production""?",<p>I have a cluster whose &quot;cluster_purpose&quot; was set to &quot;dev_test&quot;. I want to change it to &quot;production&quot;. Couldn't find anything in the documentation.</p>,0,0,2022-05-12 15:45:22.527000 UTC,,,1,azure|azure-machine-learning-service,32,2014-12-18 15:16:56.597000 UTC,2022-09-23 16:12:30.223000 UTC,"Palermo, PA, Italia",1572,33,5,59,,,,,,['azure-machine-learning-service']
There is 403 error when I upload data through datastore using AzureML,"<p>I just want to upload data on my local laptop to datastore.
But there is an HTTP 403 error.</p>

<p>Could you please tell me how do I upload data?</p>

<p>There are error messages.</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>ds
      </p>
      
      <p>ds.upload(src_dir = './MNIST_data/', target_path='MNIST_data', overwrite=True, show_progress=True)
      Client-Request-ID=5b78750a-ee2f-11e8-bf9a-b46bfcb19fb2 Retry policy did not allow for a retry: Server-Timestamp=Thu, 22 Nov 2018 08:19:40 GMT, HTTP status code=40./BODY>ror 403. The request URL is forbidden.</p>set=us-ascii"">//www.w3.org/TR/html4/strict.dtd"">
      Traceback (most recent call last):
        File """", line 1, in 
        File ""C:\python-anaconda\lib\site-packages\azureml\data\azure_storage_datastore.py"", line 380, in upload
          self._file_share_upload
        File ""C:\python-anaconda\lib\site-packages\azureml\data\azure_storage_datastore.py"", line 235, in _start_upload_task
          task_fn = task_generator(target_file_path, src_file_path)
        File ""C:\python-anaconda\lib\site-packages\azureml\data\azure_storage_datastore.py"", line 451, in _file_share_upload
          if not self.file_service.exists(self.container_name, dirpath):
        File ""C:\python-anaconda\lib\site-packages\azureml_vendor\azure_storage\file\fileservice.py"", line 1306, in exists
          _dont_fail_not_exist(ex)
        File ""C:\python-anaconda\lib\site-packages\azureml_vendor\azure_storage\common_error.py"", line 97, in _dont_fail_not_exist
          raise error
        File ""C:\python-anaconda\lib\site-packages\azureml_vendor\azure_storage\file\fileservice.py"", line 1303, in exists
          self._perform_request(request, expected_errors=expected_errors)
        File ""C:\python-anaconda\lib\site-packages\azureml_vendor\azure_storage\common\storageclient.py"", line 381, in _perform_request
          raise ex
        File ""C:\python-anaconda\lib\site-packages\azureml_vendor\azure_storage\common\storageclient.py"", line 306, in _perform_request
          raise ex
        File ""C:\python-anaconda\lib\site-packages\azureml_vendor\azure_storage\common\storageclient.py"", line 292, in _perform_request
          HTTPError(response.status, response.message, response.headers, response.body))
        File ""C:\python-anaconda\lib\site-packages\azureml_vendor\azure_storage\common_error.py"", line 115, in _http_error_handler
          raise ex
      azure.common.AzureHttpError: Forbidden
      
      Forbidden
       <h2>Forbidden URL</h2> <hr><p>HTTP Error 403. The request URL is forbidden.</p> </p>
    </blockquote>
  </blockquote>
</blockquote>",1,4,2018-11-22 08:39:30.537000 UTC,1.0,,3,azure|http-status-code-403|datastore|azure-machine-learning-studio,644,2018-11-22 08:32:29.983000 UTC,2019-12-04 09:52:07.633000 UTC,,31,0,0,1,,,,,,['azure-machine-learning-studio']
Get Sagemaker Notebook Instance IP address in CloudFormation/CDK,"<p>Is it possible to retrieve the private IP address assigned to a Sagemaker Notebook Instance in a CloudFormation template? In CDK? Or perhaps, Terraform?</p>
<p>From the AWS <a href=""https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-sagemaker-notebookinstance.html#aws-resource-sagemaker-notebookinstance-return-values"" rel=""nofollow noreferrer"">documentation</a>: there's only 1 property being exposed, namely <code>NotebookInstanceName</code>. So I guess, it's not supported yet.</p>
<p>Is there any other means?
Any thoughts?</p>
<hr />",1,0,2021-12-07 13:34:44.760000 UTC,,2021-12-07 13:40:40.397000 UTC,1,amazon-web-services|ip-address|amazon-sagemaker,385,2013-11-15 15:40:39.387000 UTC,2022-09-24 12:00:52.630000 UTC,"Ljubljana, Slovenia",2470,910,11,285,,,,,,['amazon-sagemaker']
How to send response from Azure Databricks the UI in real-time?,"<p>I have registered my ML model using Mlflow in Azure Databricks, and have a model URL.
Now, in another notebook of Databricks, I am preparing data(retrieved from the SQL) and filtering it based on the input parameters fetched as a RestAPI from Postman.
Next, I am using this prepared data frame(max. 20 records) to get the prediction for each row. And, converting this data frame into JSON serializable format to send it as a response to Postman.
The notebook runtime is 2 secs.</p>
<p>Every time a request is got, a databricks job is run, and the response is sent. I am not sure why each job's runtime is 12 secs or more(despite the notebook runtime being 2 secs. only).
What I actually I am looking into is to send a curated response, that'll have some additional parameters along with the prediction, in form of a RestAPI in milliseconds(as it is realtime).</p>
<p>Everything is done in Databricks, itself.</p>
<p>I believe I am somewhere lacking in understanding which architecture shall help us achieve my requirement. Kindly help me understand the same. Thanks a lot, in advance.</p>",0,4,2020-12-24 08:11:32.573000 UTC,,2020-12-31 04:14:47.950000 UTC,0,azure|postman|databricks|job-scheduling|mlflow,243,2017-03-29 18:50:29.467000 UTC,2021-02-23 05:52:42.913000 UTC,"Hyderabad, Telangana, India",25,1,0,63,,,,,,['mlflow']
How to create a hyperparameter tuning step in SageMaker pipeline?,"<p>I am trying to use the latest SageMaker Python SDK (v2.23.0) to implement a SageMaker pipeline that includes a hyperparameter tuning job. However I didn't see anything in module sagemaker.workflow.steps or sagemaker.workflow.step_collections that I can use. There is a TrainingStep class but it's not for HPO.</p>
<p>Is this not supported at this time?</p>",1,0,2020-12-28 19:09:53.660000 UTC,,,0,amazon-sagemaker|hyperparameters|mlops,286,2014-01-15 20:09:46.960000 UTC,2021-09-22 15:40:52.590000 UTC,,1,0,0,1,,,,,,['amazon-sagemaker']
The current AWS identity is not a role for sagemaker?,"<p>I am getting error when i call get_execution_role() from sagemaker in python.
I have attached the error for the same.
<a href=""https://i.stack.imgur.com/z5NGd.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/z5NGd.png"" alt=""enter image description here""></a></p>

<p>I have added the SagemakerFullAccess Policy to role and user both.</p>",3,2,2017-12-08 08:43:30.010000 UTC,3.0,2017-12-13 07:02:49.783000 UTC,22,amazon-web-services|amazon-sagemaker,13622,2014-12-04 18:25:33.410000 UTC,2022-03-31 12:27:47.127000 UTC,"Bangalore, Karnataka, India",394,2,0,46,,,,,,['amazon-sagemaker']
Is there an alternative to DVC pipelines to create a DAG which is also aware of inputs/outputs to nodes to cache results?,"<p>I recently started to use DVC pipelines to create DAG in my application. I work on Machine Learning projects, and I need to experiment a lot with different nodes of my system. For example:</p>
<p><code>Data preprocessing -&gt; feature extraction -&gt; model training -&gt; model evaluation</code></p>
<p>Each node produces an output, and the output of each node is used in another node. What DVC allows me to do is to create a pipeline in which I can specify dependencies between nodes. I also use <code>.yaml</code> files to configure parameters of my application, and you can also specify these parameters as dependencies for different nodes. So, whenever a dependency changes between nodes (it can be either configuration parameters or inputs/outputs specified), DVC is able to detect this, and run the necessary parts of the pipeline. If a dependency hasn't changed for a particular node, DVC can use its cache to skip that step. This is really useful for me, since some nodes take really long time to execute, and they don't always need to be ran (if their dependencies hasn't changed).</p>
<p>I also started to use hydra to manage my config files, and to be honest, DVC doesn't work well with hydra. It expects a static config to specify parameter dependencies, and with hydra it is a bit tricky to do, and complicate things.</p>
<p>My question is: is there any alternative to DVC Pipelines which also goes well with hydra?</p>",0,1,2022-01-14 13:05:07.820000 UTC,,,1,pipeline|directed-acyclic-graphs|dvc|hydra-core,181,2019-01-21 07:32:05.357000 UTC,2022-09-24 17:03:22.623000 UTC,"Warsaw, Poland",551,9,2,35,,,,,,['dvc']
Propagate the error in sagemaker from ProcessingStep to Worflow functions,"<p>So I have a sagemaker worflow composed of multiple processing steps, training steps ...</p>
<p>I'm trying to change the <code>FailureReason</code> of the <code>describe_pipeline_execution()</code> to return useful information concerning the failure of my Sagemaker pipeline.</p>
<p>Indeed in my processing step and in my training step, having their code inside a docker container,  I want the pipeline to fail for various reasons. In the documentation <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/build-your-own-processing-container.html"" rel=""nofollow noreferrer"">here</a> for processing step and <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo-output.html"" rel=""nofollow noreferrer"">here</a> for training step, it is possible to return something by writing it in a file during the step through the functions <code>DescribeProcessingJob</code> or <code>DescribeTrainingJob</code>.</p>
<p>I was wondering if there is a path like <code>/opt/ml/failure</code> to put the output of my process so the function <code>describe_pipeline_execution()</code> can return what I want. Can't find it in the documentation!</p>
<p>Thanks in advance</p>",1,0,2022-02-02 13:03:03.170000 UTC,,,0,amazon-sagemaker,152,2022-02-02 12:51:02.047000 UTC,2022-09-23 15:29:50.040000 UTC,,1,0,0,0,,,,,,['amazon-sagemaker']
AWS secret manager access from Sagemaker ECR container,"<p>I have a secret stored in AWS Secret Manager.
I am trying to access that secret from a container which is in ECR.</p>
<p>When I execute the container, the error message I get is:</p>
<pre><code>  File &quot;/opt/program/train&quot;, line 65, in get_secret
    SecretId=secret_name
  File &quot;/usr/local/lib/python3.6/dist-packages/botocore/client.py&quot;, line 508, in _api_call
    return self._make_api_call(operation_name, kwargs)
  File &quot;/usr/local/lib/python3.6/dist-packages/botocore/client.py&quot;, line 895, in _make_api_call
    operation_model, request_dict, request_context
  File &quot;/usr/local/lib/python3.6/dist-packages/botocore/client.py&quot;, line 917, in _make_request
    return self._endpoint.make_request(operation_model, request_dict)
  File &quot;/usr/local/lib/python3.6/dist-packages/botocore/endpoint.py&quot;, line 116, in make_request
    return self._send_request(request_dict, operation_model)
  File &quot;/usr/local/lib/python3.6/dist-packages/botocore/endpoint.py&quot;, line 195, in _send_request
    request = self.create_request(request_dict, operation_model)
  File &quot;/usr/local/lib/python3.6/dist-packages/botocore/endpoint.py&quot;, line 134, in create_request
    operation_name=operation_model.name,
  File &quot;/usr/local/lib/python3.6/dist-packages/botocore/hooks.py&quot;, line 412, in emit
    return self._emitter.emit(aliased_event_name, **kwargs)
  File &quot;/usr/local/lib/python3.6/dist-packages/botocore/hooks.py&quot;, line 256, in emit
    return self._emit(event_name, kwargs)
  File &quot;/usr/local/lib/python3.6/dist-packages/botocore/hooks.py&quot;, line 239, in _emit
    response = handler(**kwargs)
  File &quot;/usr/local/lib/python3.6/dist-packages/botocore/signers.py&quot;, line 103, in handler
    return self.sign(operation_name, request)
  File &quot;/usr/local/lib/python3.6/dist-packages/botocore/signers.py&quot;, line 187, in sign
    auth.add_auth(request)
  File &quot;/usr/local/lib/python3.6/dist-packages/botocore/auth.py&quot;, line 407, in add_auth
    raise NoCredentialsError()
botocore.exceptions.NoCredentialsError: Unable to locate credentials
</code></pre>
<p>The container is using python and I am using the same function given as an example when I setup the secret manager entry for python3
The same function works well in my local environment where I am authenticated via CLI.</p>
<p>The function is:</p>
<pre><code>import boto3
import base64
from botocore.exceptions import ClientError


def get_secret():

    secret_name = &quot;secret1&quot;
    region_name = &quot;us-east-1&quot;

    # Create a Secrets Manager client
    session = boto3.session.Session()
    client = session.client(
        service_name='secretsmanager',
        region_name=region_name
    )

    # In this sample we only handle the specific exceptions for the 'GetSecretValue' API.
    # See https://docs.aws.amazon.com/secretsmanager/latest/apireference/API_GetSecretValue.html
    # We rethrow the exception by default.

    try:
        get_secret_value_response = client.get_secret_value(
            SecretId=secret_name
        )
    except ClientError as e:
        if e.response['Error']['Code'] == 'DecryptionFailureException':
            # Secrets Manager can't decrypt the protected secret text using the provided KMS key.
            # Deal with the exception here, and/or rethrow at your discretion.
            raise e
        elif e.response['Error']['Code'] == 'InternalServiceErrorException':
            # An error occurred on the server side.
            # Deal with the exception here, and/or rethrow at your discretion.
            raise e
        elif e.response['Error']['Code'] == 'InvalidParameterException':
            # You provided an invalid value for a parameter.
            # Deal with the exception here, and/or rethrow at your discretion.
            raise e
        elif e.response['Error']['Code'] == 'InvalidRequestException':
            # You provided a parameter value that is not valid for the current state of the resource.
            # Deal with the exception here, and/or rethrow at your discretion.
            raise e
        elif e.response['Error']['Code'] == 'ResourceNotFoundException':
            # We can't find the resource that you asked for.
            # Deal with the exception here, and/or rethrow at your discretion.
            raise e
    else:
        # Decrypts secret using the associated KMS key.
        # Depending on whether the secret is a string or binary, one of these fields will be populated.
        if 'SecretString' in get_secret_value_response:
            secret = get_secret_value_response['SecretString']
        else:
            decoded_binary_secret = base64.b64decode(get_secret_value_response['SecretBinary'])
            
    # Your code goes here. 
</code></pre>
<p>But in the docker container I have not performed a CLI authentication. I have given the sagemaker access role the KMS permissions for my region.
Any thoughts on how I can get a sagemaker container to access secret manager?
Thanks!</p>",0,1,2022-08-18 19:36:35.430000 UTC,,,0,amazon-web-services|amazon-sagemaker|secretsmanager,25,2016-03-17 06:30:24.610000 UTC,2022-08-28 15:19:22.880000 UTC,,981,64,1,74,,,,,,['amazon-sagemaker']
IOPub data rate exceeded in SageMaker?,"<p>Does anyone know where to find the config file to edit the <code>c.NotebookApp.iopub_data_rate_limit</code>? I am not working with the AWS CLI, but rather I am doing everything in the AWS Console. I have a SageMaker notebook running and I would like to change the data rate limit, but essentially don't have access to a terminal, unless someone could explain how to access the terminal within the console?</p>",0,0,2021-12-04 03:43:22.183000 UTC,,,0,jupyter-notebook|jupyter-lab|amazon-sagemaker,57,2018-09-08 18:30:47.580000 UTC,2022-04-26 16:40:12.660000 UTC,,113,9,0,47,,,,,,['amazon-sagemaker']
MLFLOW on Databricks - Cannot log a Keras model as a mlflow.pyfunc model. Get TypeError: cannot pickle 'weakref' object,"<p>Hi all: this is one of my first posts on Stackoverflow - so apologies in advance if i'm not conforming to certain standards!</p>
<p>I'm having trouble saving my Keras model as a <code>mlflow.pyfunc</code> model as it's giving me a &quot;cannot pickle a 'weakref' object when I try to log it.</p>
<p><strong>So why am i saving my Keras model as a pyfunc model object in the first place? This is because I want to override the default predict method and output something custom</strong>. I also want to do some pre-processing steps on the X_test or new data by encoding it with a tf.keras.StringLookup and then invert it back to get the original categorical variable class. For this reason, I was advised by Databricks that the mlflow.pyfunc flavor is the best way to go for these types of use-cases</p>
<p>The Keras model works just fine and i'm able to log it using <code>mlflow.keras.log_model</code>. But it fails when i try to wrap it inside a cutomer &quot;KerasWrapper&quot; class.</p>
<p>Here are some snippets of my code. For the purpose of debugging, the current <code>predict</code> method in the custom class is just the default. I simplified it to help debug, but obviously I haven't been able to resolve it.</p>
<p>I would be extremely grateful for any help. Thanks in advance!</p>
<p><strong>ALL CODE ON AZURE DATABRICKS</strong></p>
<p><strong>Custom mlflow.pyfunc class</strong></p>
<pre><code>class KerasWrapper(mlflow.pyfunc.PythonModel):
  
  def __init__(self, keras_model, labelEncoder, labelDecoder, n):         
    self.keras_model = keras_model
    self.labelEncoder = labelEncoder
    self.labelDecoder = labelDecoder
    self.topn = n
    
  def load_context(self, context): 
    self.keras_model = mlflow.keras.load_model(model_uri=context.artifacts[self.keras_model], compile=False)
  
  def predict(self, context, input_data):
    scores = self.keras_model.predict(input_data)
    return scores
</code></pre>
<p><strong>My Keras Deep Learning Model</strong> (this works fine by the way)</p>
<pre><code>def build_model(vocab_size, steps, drop_embed, n_dim, encoder, modelType):
  
  model = None

  i = Input(shape=(None,), dtype=&quot;int64&quot;)
  
  #embedding layer
  e = Embedding(vocab_size, 16)(i)
  s = SpatialDropout1D(drop_embed)(e)

  x = Conv1D(256, steps, activation='relu')(s)
  x = GlobalMaxPooling1D()(x)
  x = Dense(128, activation='relu')(x)
  x = Dropout(0.2)(x)

  #output layer
  x = Dense(vocab_size, activation='softmax')(x)

  model = Model(i, x)
  model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
  model.summary()
  
  return model
</code></pre>
<p><strong>MLFLOW Section</strong></p>
<pre><code>with mlflow.start_run(run_name=runName):

  mlflow.tensorflow.autolog()   

  #Build the model, compile and train on the training set
  #signature: build_model(vocab_size, steps, drop_embed, n_dim, encoder, modelType):
  keras_model = build_model((vocab_size + 1), timeSteps, drop_embed, embedding_dimensions, encoder, modelType)      


  keras_model.fit(X_train_encoded, y_train_encoded, epochs=epochs, verbose=1, batch_size=32, use_multiprocessing = True, 
                            validation_data=(X_test_encoded, y_test_encoded))

  # Log the model parameters used for this run.  
  mlflow.log_param(&quot;numofActionsinWorkflow&quot;, numofActionsinWf)
  mlflow.log_param(&quot;timeSteps&quot;, timeSteps)

  #wrap it up in a pyfunc model
  wrappedModel = KerasWrapper(keras_model, encoder, decoder, bestActionCount)

  # Create a model signature using the tensor input to store in the MLflow model registry
  signature = infer_signature(X_test_encoded, wrappedModel.predict(None, X_test_encoded))
  # Let's check out how it looks
  print(signature)

  # Create an input example to store in the MLflow model registry
  input_example = np.expand_dims(X_train[17], axis=0)
  
  # The necessary dependencies are added to a conda.yaml file which is logged along with the model.
  model_env = mlflow.pyfunc.get_default_conda_env()
  # Record specific additional dependencies required by the serving model
  model_env['dependencies'][-1]['pip'] += [
    f'tensorflow=={tf.__version__}',
    f'mlflow=={mlflow.__version__}',
    f'sklearn=={sklearn.__version__}',
    f'cloudpickle=={cloudpickle.__version__}',
  ]
  
  #log the model to experiment
  #mlflow.keras.log_model(keras_model, artifact_path = runName, signature=signature, input_example=input_example, conda_env = model_env)
  
  wrapped_model_path = runName
  
  if (os.path.exists(wrapped_model_path)):
    shutil.rmtree(wrapped_model_path)
  
  #Log model as pyfunc model
  mlflow.pyfunc.log_model(runName, python_model=wrappedModel, signature=signature, input_example=input_example, conda_env = model_env)

  #return the run ID for model registration
  run_id = mlflow.active_run().info.run_id
  
  mlflow.end_run()
</code></pre>
<p>Here is the error that i receive
<a href=""https://i.stack.imgur.com/hegaZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hegaZ.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/kZ2oy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kZ2oy.png"" alt=""enter image description here"" /></a></p>",0,2,2021-12-16 20:22:51.803000 UTC,,,1,tensorflow|machine-learning|keras|deep-learning|mlflow,595,2015-03-27 21:44:36.943000 UTC,2022-09-24 05:46:47.850000 UTC,,61,0,0,18,,,,,,['mlflow']
azure data factory update,"<p>I created two web services (Traning and predictive to use API's in the data factory.</p>

<ol>
<li>memory_train</li>
<li>memory_train [Predictive_Exp.]</li>
</ol>

<p>In data factory, I created ML Batch execution followed by ML Update Resource1. I used following API's and i/ps for my Data factory blocks.</p>

<p>ML Batch execution:</p>

<ol>
<li>I created a linked service that access API and key of my trained model</li>
<li>I created i/p and o/ps in a blob storage account.</li>
</ol>

<p>After this is a run , <code>.ilearner</code> file is stored in my blob</p>

<p>ML Update resource 1:</p>

<ol>
<li>I created patch endpoint in my memory_train [Predictive_Exp.] end point </li>
<li>I created second linked service that uses patch end point API and key in linked service, for update Resource end point, I used patch endpoint API again.</li>
<li>I tried to switch several API's for mr second linked service from default and patch, none of them could update my predictive model</li>
</ol>

<p>I get the following error for this Data factory block, can you suggest me if I am doing any mistakes</p>

<blockquote>
<pre><code>Activity ML Update Resource1 failed: UpdateResource has failed with error: {   ""error"": {
    ""code"": ""EditableResourcesNotAvailable"",
    ""message"": ""The specified resources do not exist or are not editable. Valid resource names: memory_train [trained model].""   } }.
</code></pre>
</blockquote>

<p>Diagnostic details: </p>

<blockquote>
  <p>job ID 011f3f75-8065-4835-bef5-143e7ae22111. Endpoint <a href=""https://management.azureml.net/workspaces/324b91422294411f9fa65d624cdd507c/webservices/88c2f89a72f14a539d529a319598f5aa/endpoints/patch"" rel=""nofollow noreferrer"">https://management.azureml.net/workspaces/324b91422294411f9fa65d624cdd507c/webservices/88c2f89a72f14a539d529a319598f5aa/endpoints/patch</a>.</p>
</blockquote>",1,0,2018-12-02 17:34:13.267000 UTC,,2018-12-02 19:22:47.443000 UTC,1,azure|azure-data-factory|azure-machine-learning-studio|ml-studio,104,2017-02-12 10:16:06.603000 UTC,2018-12-14 01:51:55.397000 UTC,,31,0,0,2,,,,,,['azure-machine-learning-studio']
mlflow Exception: Run with UUID is already active,"<p>Used mlflow.set_tracking_uri to set up tracking_uri and set_experiment, got an error and check back to run following code again. got an error that ""Exception: Run with UUID  is already active.""
Try to use <code>mlflow.end_run</code> to end current run, but got RestException: RESOURCE_DOES_NOT_EXIST: Run UUID not found.
Currently stuck in this infinite loop. Any suggestion?    </p>

<pre><code>    mlflow.set_experiment(""my_experiment"")
    mlflow.start_run(run_name='my_project')
    mlflow.set_tag('input_len',len(input))
    mlflow.log_param('metrics', r2)
</code></pre>",2,2,2020-02-18 17:34:07.790000 UTC,,2020-02-18 18:04:02.567000 UTC,1,mlflow,2813,2014-09-05 23:12:24.170000 UTC,2022-09-24 04:33:18.083000 UTC,"Los Angeles, CA, USA",1997,33,0,175,,,,,,['mlflow']
Invoke Sagemaker Endpoint using Spark (EMR Cluster),"<p>I am developing a spark application in an EMR cluster. The flow of the project goes like this :</p>

<p>Dataframe is  repartitioned based in a Id.</p>

<p>Sagemaker endpoint needs to be invoked on each partition and get the result.</p>

<p>But doing that i am getting this error :</p>

<pre><code>cPickle.PicklingError: Could not serialize object: TypeError: can't pickle thread.lock objects
</code></pre>

<p>The code is a follows : </p>

<pre><code>
from pyspark import SparkContext
from pyspark.sql import SparkSession
from pyspark import SparkConf
import itertools
import json
import boto3
import time
from pyspark.sql.window import Window
from pyspark.sql.functions import row_number
from pyspark.sql import functions as F
from pyspark.sql.functions import lit
from io import BytesIO as StringIO


client=boto3.client('sagemaker-runtime')

def invoke_endpoint(json_data):
    ansJson=json.dumps(json_data)
    response=client.invoke_endpoint(EndpointName=""&lt;EndpointName&gt;"",Body=ansJson,ContentType='text/csv',Accept='Accept')
    resultJson=json.loads(str(response['Body'].read().decode('ascii')))
    return resultJson

def execute(list_of_url):
    final_iterator=[]
    urlist=[]
    json_data={}
    for url in list_of_url:
        final_iterator.append((url.ID,url.Prediction))
        urlist.append(url.ID)
    json_data['URL']=urlist
    ressultjson=invoke_endpoint(json_data)
    return iter(final_iterator)

### Atributes to be added to Spark Conf
conf = (SparkConf().set(""spark.executor.extraJavaOptions"",""-Dcom.amazonaws.services.s3.enableV4=true"").set(""spark.driver.extraJavaOptions"",""-Dcom.amazonaws.services.s3.enableV4=true""))


scT=SparkContext(conf=conf)
scT.setSystemProperty(""com.amazonaws.services.s3.enableV4"",""true"")

hadoopConf=scT._jsc.hadoopConfiguration()
hadoopConf.set(""f3.s3a.awsAccessKeyId"",""&lt;AccessKeyId&gt;"")
hadoopConf.set(""f3.s3a.awsSecretAccessKeyId"",""&lt;SecretAccessKeyId&gt;"")
hadoopConf.set(""f3.s3a.endpoint"",""s3-us-east-1.amazonaws.com"")
hadoopConf.set(""com.amazonaws.services.s3.enableV4"",""true"")
hadoopConf.set(""fs.s3a.impl"",""org.apache.hadoop.fs.s3a.S3AFileSystem"")


sql=SparkSession(scT)
csv_df=sql.read.csv('s3 path to my csv file',header =True)

#print('Total count is',csv_df.count())

csv_dup_df=csv_df.dropDuplicates(['ID'])
print('Total count is',csv_dup_df.count())

windowSpec=Window.orderBy(""ID"")

result_df=csv_dup_df.withColumn(""ImageID"",F.row_number().over(windowSpec)%80)

final_df=result_df.withColumn(""Prediction"",lit(str(""UNKOWN"")))


df2 = final_df.repartition(""ImageID"")


df3=df2.rdd.mapPartitions(lambda url: execute(url)).toDF()
df3.coalesce(1).write.mode(""overwrite"").save(""s3 path to save the results in csv format"",format=""csv"")

print(df3.rdd.glom().collect())
##Ok
print(""Work is Done"")

</code></pre>

<p>Can you tell me how to rectify this issue ?</p>",0,0,2020-03-26 10:11:33.710000 UTC,1.0,,1,amazon-s3|pyspark|pyspark-sql|amazon-sagemaker|pyspark-dataframes,174,2020-02-19 11:01:15.130000 UTC,2020-04-06 06:17:17.017000 UTC,"Chennai, Tamil Nadu, India",21,0,0,3,,,,,,['amazon-sagemaker']
How to create SageMaker Studio environment from CLI?,"<p>I can create SageMaker Notebook instance from <code>aws sagemaker create-notebook-instance --notebook-instance-name test-123</code><br />
but I can't find a similiar CLI command to create a <strong>&quot;SageMaker Studio&quot;</strong> instance?</p>
<p>Thanks</p>",1,0,2021-12-29 04:58:59.010000 UTC,,,0,amazon-web-services|jupyter-notebook|amazon-sagemaker,247,2017-12-13 12:56:50.217000 UTC,2022-09-22 06:20:22.683000 UTC,,409,25,0,54,,,,,,['amazon-sagemaker']
Runtime error using XGBoostSageMakerEstimator from sagemaker_pyspark on AWS EMR Notebook,"<p>I am trying to use the SageMaker Python SDK with PySpark on EMR (Jupyter) Notebook.
When trying to use XGBoostSageMakerEstimator as shown below,</p>

<pre><code>from sagemaker_pyspark.algorithms import XGBoostSageMakerEstimator

xgboost_estimator = XGBoostSageMakerEstimator(
    sagemakerRole=IAMRole(someRoleArn),
    trainingInstanceType='ml.m4.xlarge',
    trainingInstanceCount=1,
    endpointInstanceType='ml.m4.xlarge',
    endpointInitialInstanceCount=1)
</code></pre>

<p>I am getting the following error that I have not been able to find the solution to.</p>

<pre><code>Exception ignored in: &lt;bound method JavaWrapper.__del__ of &lt;sagemaker_pyspark.wrapper.ScalaMap object at 0x7fd3d9e96240&gt;&gt;
Traceback (most recent call last):
  File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/ml/wrapper.py"", line 40, in __del__
AttributeError: 'ScalaMap' object has no attribute '_java_obj'
Exception ignored in: &lt;bound method JavaWrapper.__del__ of &lt;sagemaker_pyspark.wrapper.ScalaMap object at 0x7fd3d9e96240&gt;&gt;
Traceback (most recent call last):
  File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/ml/wrapper.py"", line 40, in __del__
AttributeError: 'ScalaMap' object has no attribute '_java_obj'
Exception ignored in: &lt;bound method JavaWrapper.__del__ of &lt;sagemaker_pyspark.wrapper.Option object at 0x7fd3d9e9d3c8&gt;&gt;
Traceback (most recent call last):
  File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/ml/wrapper.py"", line 40, in __del__
AttributeError: 'Option' object has no attribute '_java_obj'
Exception ignored in: &lt;bound method JavaWrapper.__del__ of &lt;sagemaker_pyspark.wrapper.Option object at 0x7fd3d9e9d128&gt;&gt;
Traceback (most recent call last):
  File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/ml/wrapper.py"", line 40, in __del__
AttributeError: 'Option' object has no attribute '_java_obj'
Exception ignored in: &lt;bound method JavaWrapper.__del__ of &lt;sagemaker_pyspark.wrapper.Option object at 0x7fd3d9e9d0f0&gt;&gt;
Traceback (most recent call last):
  File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/ml/wrapper.py"", line 40, in __del__
AttributeError: 'Option' object has no attribute '_java_obj'
Exception ignored in: &lt;bound method JavaWrapper.__del__ of &lt;sagemaker_pyspark.wrapper.Option object at 0x7fd3d9e9d080&gt;&gt;
Traceback (most recent call last):
  File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/ml/wrapper.py"", line 40, in __del__
AttributeError: 'Option' object has no attribute '_java_obj'
Exception ignored in: &lt;bound method JavaWrapper.__del__ of &lt;sagemaker_pyspark.wrapper.Option object at 0x7fd3d9e96ef0&gt;&gt;
Traceback (most recent call last):
  File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/ml/wrapper.py"", line 40, in __del__
AttributeError: 'Option' object has no attribute '_java_obj'
</code></pre>

<p>Any help to troubleshoot this would be greatly appreciated.</p>

<p>Using: </p>

<ul>
<li>EMR (emr-5.26.0) Cluster with Spark 2.4.3 </li>
<li>EMR Notebook attached to the cluster</li>
<li>sagemaker_pyspark comes pre-installed with emr-5.26.0</li>
</ul>",1,0,2019-09-25 19:08:57.693000 UTC,,2019-09-25 19:32:53.030000 UTC,0,python|apache-spark|pyspark|amazon-emr|amazon-sagemaker,221,2018-11-11 03:52:05.580000 UTC,2019-11-16 15:52:56.957000 UTC,"WI, USA",63,0,0,11,,,,,,['amazon-sagemaker']
"Pass serialized object created in R to Python, then unserializing the object in python script","<p>I am using Azure Machine Learning, and I would like to pass a binary file created in R script to Python script, to process it further in python.</p>

<p>To pass the binary file created in R, I serialized the data.
But how can I unserialize data in python script to recover the original data?</p>

<p>The R code is below</p>

<pre><code>serialized &lt;- as.integer(serialize(data,NULL))
dataset &lt;- data.frame(serialized,stringsAsFactors=FALSE)
maml.mapOutputPort(""dataset"");
</code></pre>",2,1,2017-06-20 17:12:04.787000 UTC,,,0,python|r|azure|azure-machine-learning-studio,679,2017-06-16 06:45:45.243000 UTC,2022-09-23 12:05:18.207000 UTC,,49,7,0,13,,,,,,['azure-machine-learning-studio']
Unable to upload a file from sagemaker notebook to S3,"<p>I am attempting to upload my cleaned (and split data using kfold) to s3 so that I can use sagemaker to create a model using it (since sagemaker wants an s3 file with training and test data). However, whenever I attempt to upload the csv to s3 it runs but I don't see the file in s3.</p>

<p>I have tried changing which folder I access in sagemaker, or trying to upload different types of files none of which work. In addition, I have tried the approaches in similar Stack Overflow posts without success.</p>

<p>Also note that I am able to manually upload my csv to s3, just not through sagemaker automatically.</p>

<p>The code below is what I currently have to upload to s3, which I have copied directly from AWS documentation for file uploading using sagemaker.</p>

<pre><code>import io
import csv
import boto3

#key = ""{}/{}/examples"".format(prefix,data_partition_name)
#url = 's3n://{}/{}'.format(bucket, key)
name = boto3.Session().resource('s3').Bucket('nc-demo-sagemaker').name
print(name)
boto3.Session().resource('s3').Bucket('nc-demo-sagemaker').upload_file('train', '/')
print('Done writing to {}'.format('sagemaker bucket'))
</code></pre>

<p>I expect that when I run that code snippet, I am able to upload the training and test data to the folder I want for use in creating sagemaker models.</p>",3,5,2019-06-28 16:03:08.733000 UTC,,2019-06-28 16:04:47.597000 UTC,5,python|amazon-web-services|amazon-s3|amazon-sagemaker,4962,2015-10-31 23:05:25.740000 UTC,2020-12-27 15:50:38.097000 UTC,Pristina,129,5,0,1,,,,,,['amazon-sagemaker']
How to log a sklearn pipeline with a Keras step using mlflow.pyfunc.log_model()? TypeError: can't pickle _thread.RLock objects,"<p>I would like to log into MlFlow a <code>sklearn</code> pipeline with a Keras step.</p>
<p>The pipeline has 2 steps: a <code>sklearn</code> StandardScale and a Keras TensorFlow model.</p>
<p>I am using mlflow.pyfunc.log_model() as possible solution, but I am having this error:</p>
<pre><code>TypeError: can't pickle _thread.RLock objects
---&gt;   mlflow.pyfunc.log_model(&quot;test1&quot;, python_model=wrappedModel, signature=signature)
</code></pre>
<p>Here is my code:</p>
<pre><code>import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import keras
from keras import layers, Input
from keras.wrappers.scikit_learn import KerasRegressor
import mlflow.pyfunc
from sklearn.pipeline import Pipeline
from mlflow.models.signature import infer_signature

#toy dataframe
df1 = pd.DataFrame([[1,2,3,4,5,6], [10,20,30,40,50,60],[100,200,300,400,500,600]] )

#create train test datasets
X_train, X_test = train_test_split(df1, random_state=42, shuffle=True)

#scale X_train
scaler = StandardScaler()
X_train_s = scaler.fit_transform(X_train)
X_train_s = pd.DataFrame(X_train_s)

#wrap the keras model to use it inside of sklearn pipeline
def create_model(optimizer='adam', loss='mean_squared_error', s = X_train.shape[1]):
  input_layer = keras.Input(shape=(s,))
  # &quot;encoded&quot; is the encoded representation of the input
  encoded = layers.Dense(25, activation='relu')(input_layer)
  encoded = layers.Dense(2, activation='relu')(encoded)

  # &quot;decoded&quot; is the lossy reconstruction of the input
  decoded = layers.Dense(2, activation='relu')(encoded)
  decoded = layers.Dense(25, activation='relu')(encoded)
  decoded = layers.Dense(s, activation='linear')(decoded)
  
  model = keras.Model(input_layer, decoded)
  model.compile(optimizer, loss)
  return model

# wrap the model
model = KerasRegressor(build_fn=create_model, verbose=1)

# create the pipeline
pipe = Pipeline(steps=[
    ('scale', StandardScaler()),
    ('model',model)
])

#function to wrap the pipeline to be logged by mlflow
class SklearnModelWrapper(mlflow.pyfunc.PythonModel):
  def __init__(self, model):
    self.model = model
    
  def predict(self, context, model_input):
    return self.model.predict(model_input)[:,1]
  
  
mlflow.end_run()
with mlflow.start_run(run_name='test1'):

  #train the pipeline
  pipe.fit(X_train, X_train_s, model__epochs=2)
  
  #wrap the model for mlflow log
  wrappedModel = SklearnModelWrapper(pipe)

  # Log the model with a signature that defines the schema of the model's inputs and outputs. 
  signature = infer_signature(X_train, wrappedModel.predict(None, X_train))
  mlflow.pyfunc.log_model(&quot;test1&quot;, python_model=wrappedModel, signature=signature)
  
</code></pre>
<p>From what I googled, it seems like this type of error is related to concurrency of threads. It could be then related to the TensorFlow, since it distributes the code during the model training phase.</p>
<p>However, the offending code line is after the training phase. If I remove this line, the rest of the code works, which makes me think that it happens after the concurrency phase of the model training. I have no idea why I am getting this error in this context.
I am a beginner? Can someone please help me?
Thanks</p>",1,1,2020-11-02 12:39:59.543000 UTC,,2020-11-02 16:25:05.113000 UTC,4,python|keras|scikit-learn|mlflow,787,2012-06-08 09:46:58.537000 UTC,2021-03-24 16:17:00.223000 UTC,,41,0,0,8,,,,,,['mlflow']
Best way to manage train/test/val splits on AzureML,"<p>I'm currently using AzureML with pretty complex workflows involving large datasets etc. and I'm wondering what is the best way to manage the splitting resulting of preprocessing steps. All my projects are built as pipelines fed by registered Datasets. I want to be able to track the splitting in order to easily retrieve, for example, test and validation sets for integration testing purposes.</p>
<p>What would be the best pattern to apply there ? Registering every intermediate set as different Dataset ? Directly retrieving the intermediate sets using the Run IDs ? ...</p>
<p>Thaanks</p>",2,1,2020-06-23 15:46:24.220000 UTC,,,2,python|azure|azure-machine-learning-service,171,2015-11-12 09:22:17.140000 UTC,2022-09-21 16:03:24.940000 UTC,,313,10,0,40,,,,,,['azure-machine-learning-service']
How to submit local jobs with dsl.pipeline,"<p>Trying to run and debug a pipeline locally. Pipeline is imeplemented with <code>azure.ml.component.dsl.pipeline</code>. When I try to set <code>default_compute_target='local'</code>, the compute target cannot be found:</p>
<pre><code>local not found in workspace, assume this is an AmlCompute
...
File &quot;/home/amirabdi/miniconda3/envs/stm/lib/python3.8/site-packages/azure/ml/component/run_settings.py&quot;, line 596, in _get_compute_type
    raise InvalidTargetSpecifiedError(message=&quot;Cannot find compute '{}' in workspace.&quot;.format(compute_name))
azure.ml.component._util._exceptions.InvalidTargetSpecifiedError: InvalidTargetSpecifiedError:
        Message: Cannot find compute 'local' in workspace.
        InnerException None
        ErrorResponse
{
    &quot;error&quot;: {
        &quot;code&quot;: &quot;UserError&quot;,
        &quot;message&quot;: &quot;Cannot find compute 'local' in workspace.&quot;
    }
}
</code></pre>
<p>The local run, for example, can be achieved with <code>azureml.core.ScriptRunConfig</code>.</p>
<pre class=""lang-py prettyprint-override""><code>src = ScriptRunConfig(script=&quot;train.py&quot;, compute_target=&quot;local&quot;, environment=myenv)
run = exp.submit(src)
</code></pre>",1,0,2022-08-02 22:28:54.120000 UTC,,,0,azure-machine-learning-service|azuremlsdk|azure-ml-pipelines|azure-ml-component,98,2015-10-20 00:27:48.317000 UTC,2022-09-21 18:49:31.233000 UTC,"Vancouver, BC, Canada",2059,50,10,252,,,,,,['azure-machine-learning-service']
"Amazon SageMaker soft max label must be in [0, num_class)","<p>I am starting on a multiclass classification problem using Xgboost and experimenting with SageMaker HyperparameterTuner. It looks like everything is configured correctly, but I keep getting failed statuses. When I check the CloudWatch logs I can see this is the problem. ""SoftmaxMultiClassObj: label must be in [0, num_class)."" I know how many classes I have in the data 5 (0,1,2,3,4) and they are in the first column of of both the training and validation data sets. </p>

<p>Below are my configurations:</p>

<pre><code>xgb = sagemaker.estimator.Estimator(container,
                                        role, 
                                        train_instance_count=1, 
                                        train_instance_type='ml.m4.xlarge',
                                        output_path='s3://{}/{}/output'.format(bucket, prefix),
                                        sagemaker_session=sess)

xgb.set_hyperparameters(eval_metric='merror', #'rmse',
                            objective = 'multi:softmax',#'binary:logistic',
                            num_round=100,
                            rate_drop=0.3,
                            num_class= 5)

hyperparameter_ranges = {'eta': ContinuousParameter(0, 1),
                        'min_child_weight': ContinuousParameter(1, 10),
                        'alpha': ContinuousParameter(0, 2),
                        'max_depth': IntegerParameter(1, 10)}

objective_metric_name = 'validation:accuracy'

tuner = HyperparameterTuner(xgb,
                            objective_metric_name,
                            hyperparameter_ranges,
                            max_jobs=20,
                            max_parallel_jobs=3)

s3_input_train = sagemaker.s3_input(s3_data='s3://{}/{}/TrainingData'.format(bucket, prefix), content_type='csv')
s3_input_validation = sagemaker.s3_input(s3_data='s3://{}/{}/ValidationData'.format(bucket, prefix), content_type='csv')

tuner.fit({'train': s3_input_train, 'validation': s3_input_validation}, include_cls_metadata=False)
</code></pre>

<p>And for good measures here is the set for the first column of the dataset to show there is indeed 5 classes. </p>

<pre><code>data=pd.read_csv('TrainingData.csv')
set(data['0'])

{0, 1, 2, 3, 4}
</code></pre>

<p>I have restarted the SageMaker training jobs multiple times and the notebook itself but continue to get the same error. What are your thoughts to try?</p>",0,7,2020-04-04 17:37:48.803000 UTC,,2020-04-04 20:42:00.710000 UTC,0,python|amazon-web-services|machine-learning|amazon-sagemaker|softmax,472,2017-10-29 17:29:05.847000 UTC,2020-11-19 17:34:55.543000 UTC,"Portland, OR, United States",63,1,0,13,,,,,,['amazon-sagemaker']
AzureML Hyperdrive. Pass data between trials,"<p>Is it possible to pass data between individual trials in HyperDrive experiment?</p>
<p>The <a href=""https://github.com/Azure/azureml-examples/blob/sdk-preview/sdk/jobs/single-step/lightgbm/iris/src/main.py"" rel=""nofollow noreferrer"">example notebook from AzureML</a> reads training data inside training script, which will be executed in every separate trial of the HyperDrive experiment. However, it would be much more efficient if we could read data only once and pass it between all trials.</p>
<p>Is it possible to configure it?</p>",0,0,2022-09-12 16:55:10.647000 UTC,,,0,python|azure|azure-machine-learning-service|hyperdrive,20,2018-07-19 09:41:42.317000 UTC,2022-09-23 16:55:26.937000 UTC,,367,52,0,11,,,,,,['azure-machine-learning-service']
"MLFlow: cannot specify base image for ""mlflow models build-docker""","<p>With mlflow models build-docker, it always creates the image from a hardcoded definition (the _DOCKERFILE_TEMPLATE string in mlflow.models.docker_utils.py), which is a pretty 'heavy' image of about 3GB.
It here a way to overrule this build-in dockerfile definition so you can use more lightweight docker images?
Furthermore, it looks like the docker image is serving the model via java which pulls in a lot of additional dependencies. Why can't the default just use the python-func concept, just like mlflow serve on a local system?</p>
<p>Thanks, and by the way, I really like mlflow!</p>
<p><strong>Update Sept 27, 2021: (in response to comment)</strong></p>
<p>How to reproduce: (from CLI)</p>
<pre><code>    mlflow models build-docker -m {model uri) -n {docker name}    
</code></pre>
<p>The docker file is always based on what has been hardcoded in</p>
<pre><code>    mlflow.models.docker_utils.py
</code></pre>
<p>in a string called</p>
<pre><code>    _DOCKERFILE_TEMPLATE
</code></pre>
<p>this string is used in the function</p>
<pre><code> _build_image(....)   (line 80)
</code></pre>
<p>where in line 103 it is actually used for creating a local docker file based on this string</p>
<pre><code>        with open(os.path.join(cwd, &quot;Dockerfile&quot;), &quot;w&quot;) as f:
            f.write(
                _DOCKERFILE_TEMPLATE.format(
</code></pre>
<p>I would have expected that it would take a user-specified Dockerfile as input instead of the hardcoded _DOCKERFILE_TEMPLATE</p>",0,2,2021-09-15 10:09:53.203000 UTC,,2021-09-27 14:43:26.270000 UTC,0,docker|mlflow,190,2018-07-26 14:50:12.930000 UTC,2022-09-20 09:28:13.903000 UTC,,11,0,0,0,,,,,,['mlflow']
SageMaker - What IAM Permission to specify for ECR?,"<h1>Question</h1>

<p>Is this the ECR IAM permission required for SageMaker to use the XGBoost of the <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html"" rel=""nofollow noreferrer"">Amazon SageMaker built-in algorithms</a> in the <strong>us-west-1</strong> region?</p>

<pre><code>""Effect"": ""Allow"",
""Action"": [
  ""ecr:GetAuthorizationToken"",
  ""ecr:BatchCheckLayerAvailability"",
  ""ecr:GetDownloadUrlForLayer"",
  ""ecr:BatchGetImage""
],
""Resource"": [
  ""arn:aws:ecr:us-west-1:632365934929:repository/632365934929.dkr.ecr.us-west-1.amazonaws.com/xgboost:1""
]
</code></pre>

<h1>Background</h1>

<p>The AWS document <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html#sagemaker-roles-createnotebookinstance-perms"" rel=""nofollow noreferrer"">Amazon SageMaker Roles</a> tells to specify <strong>TrainingImage</strong> value of the <strong>CreateTrainingJob</strong> API.</p>

<pre><code>Scope ecr permissions as follows:
- Scope to the AlgorithmSpecification.TrainingImage value that you specify in a CreateTrainingJob request.
- Scope to the PrimaryContainer.Image value that you specify in a CreateModel request:


""Effect"": ""Allow"",
""Action"": [
  ""ecr:BatchCheckLayerAvailability"",
  ""ecr:GetDownloadUrlForLayer"",
  ""ecr:BatchGetImage""
],
""Resource"": [
  ""arn:aws:ecr:::repository/my-repo1"",
  ""arn:aws:ecr:::repository/my-repo2"",
  ""arn:aws:ecr:::repository/my-repo3""
]
</code></pre>

<p>The AWS SageMaker API document <a href=""https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_AlgorithmSpecification.html#sagemaker-Type-AlgorithmSpecification-TrainingImage"" rel=""nofollow noreferrer"">TrainingImage</a> tells to specify the algorithm <strong>docker image registry path</strong> as the value.</p>

<blockquote>
  <p><strong>TrainingImage</strong><br/><br/>
  The <strong>registry path of the Docker image</strong> that contains the training
  algorithm. For information about docker registry paths for built-in
  algorithms, see Algorithms Provided by Amazon SageMaker: Common
  Parameters. Amazon SageMaker supports both <strong>registry/repository[:tag]</strong>
  and registry/repository[@digest] image path formats.</p>
</blockquote>

<p>The AWS document <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html"" rel=""nofollow noreferrer"">Common parameters for built-in algorithms</a> indicates the XGBoost registry path is <code>632365934929.dkr.ecr.us-west-1.amazonaws.com/xgboost:1</code>.</p>

<blockquote>
  <p><br/>
  |Algorithm name|Training image and inference image registry path|<br/>
  |XGBoost       | <strong>ecr_path</strong>/xgboost:<strong>tag</strong>|<br/>
  <br/>
  <strong>ecr_path</strong> (Algorithms: BlazingText, ..., Seq2Seq, and XGBoost (0.72)<br/>
  | us-west-1 | 632365934929.dkr.ecr.us-west-1.amazonaws.com |  </p>
  
  <p>For the Training Image and Inference Image Registry Path column, <strong>use the :1 version tag</strong> to ensure that you are using a stable version of the algorithm. You can reliably host a model trained using an image with the :1 tag on an inference image that has the :1 tag. </p>
</blockquote>",1,0,2020-03-27 04:19:13.820000 UTC,,2020-03-27 04:26:30.403000 UTC,1,amazon-web-services|amazon-iam|amazon-sagemaker,1170,2014-11-22 09:22:35.470000 UTC,2022-09-24 22:13:03.237000 UTC,,14749,641,62,968,,,,,,['amazon-sagemaker']
Does AWS Sagemaker PySparkProcessor manage autoscaling?,"<p>I'm using Sagemaker to generate to do preprocessing and generate training data and I'm following the Sagemaker API documentation <a href=""https://sagemaker.readthedocs.io/en/stable/amazon_sagemaker_processing.html#pysparkprocessor"" rel=""nofollow noreferrer"">here</a>, but I don't see any way currently how to specify autoscaling within the EMR cluster. What should I include within the <code>configuration</code> argument that I pass to my spark_processor <code>run()</code> object? What shouldn't I include?</p>
<p>I'm aware of the <a href=""https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-configure-apps.html"" rel=""nofollow noreferrer"">this resource</a>, but it doesn't seem comprehensive.</p>
<p>Below is my code; it is very much a &quot;work-in-progress&quot;, but I would like to know if someone could provide me with or point me to a resource that shows:</p>
<ol>
<li>Whether this PySparkProcessor object will manage autoscaling automatically. Should I put AutoScaling config within the <code>configuration</code> in the <code>run()</code> object?</li>
<li>An example of the full config that I can pass to the <code>configuration</code> variable.</li>
</ol>
<p>Here's what I have so far for the configuration.</p>
<pre><code>
SPARK_CONFIG = \
    { &quot;Configurations&quot;: [
          {   &quot;Classification&quot;: &quot;spark-env&quot;,
              &quot;Configurations&quot;: [ {&quot;Classification&quot;: &quot;export&quot;} ] }
        ] 
    }

spark_processor = PySparkProcessor(
    tags=TAGS,
    role=IAM_ROLE,
    instance_count=2,
    py_version=&quot;py37&quot;,
    volume_size_in_gb=30,
    container_version=&quot;1&quot;,
    framework_version=&quot;3.0&quot;,
    network_config=sm_network,
    max_runtime_in_seconds=1800,
    instance_type=&quot;ml.m5.2xlarge&quot;,
    base_job_name=EMR_CLUSTER_NAME,
    sagemaker_session=sagemaker_session,
)

spark_processor.run(
    configuration=SPARK_CONFIG,
    submit_app=LOCAL_PYSPARK_SCRIPT_DIR,
    spark_event_logs_s3_uri=&quot;s3://{BUCKET_NAME}/{S3_PYSPARK_LOG_PREFIX}&quot;,
)
</code></pre>
<p>I'm used to interacting via Python more directly with EMR for these types of tasks. Doing that allows me to specify the entire EMR cluster config at once--including applications, autoscaling, EMR default and autoscaling roles--and then adding the steps to the cluster once it's created; however, much of this config seems to be abstracted away, and I don't know what remains or needs to be specified, specifically regarding the following config variables:  <code>AutoScalingRole</code>, <code>Applications</code>, <code>VisibleToAllUsers</code>, <code>JobFlowRole</code>/<code>ServiceRole</code> etc.</p>",1,0,2021-04-01 22:37:21.580000 UTC,,2021-04-02 16:32:33.807000 UTC,0,amazon-web-services|pyspark|sdk|amazon-sagemaker,276,2014-06-05 18:32:51.660000 UTC,2022-09-08 19:34:21.517000 UTC,"Lehi, UT, USA",443,977,5,156,,,,,,['amazon-sagemaker']
Save SKLearnProcessor transformer in sagemaker,"<p>I wanted to use the <a href=""https://sagemaker.readthedocs.io/en/stable/frameworks/sklearn/sagemaker.sklearn.html#sagemaker.sklearn.processing.SKLearnProcessor"" rel=""nofollow noreferrer"">SKLearnProcessor</a> in Sagemaker to perform some transformations on an input dataset. However, I want to save this fitted transformer into Sagemaker to reuse it later in other scripts. How can I do that?. I don't see anywhere how it can be stored in S3 at least not in the SDK.</p>",0,0,2021-09-16 09:27:49.353000 UTC,,,1,python|scikit-learn|amazon-sagemaker,26,2011-12-02 00:24:29.840000 UTC,2022-07-18 13:43:46.980000 UTC,"Cork, Ireland",482,25,1,40,,,,,,['amazon-sagemaker']
Sagemaker Studio Lab How do I configure AWS for running in Sagemaker Studio Lab?,"<p>I am currently trying to use some AWS Built-In algorithms in the new  Sagemaker Studio Lab. In order to do so, I need to configure my AWS profile to get my execution role and region etc.
This is the current message</p>
<p><code>ValueError: Must setup local AWS configuration with a region supported by SageMaker.</code></p>
<p>Locally I am using credentials file for the configuration of the AWS CLI, how can I do this in Sagemaker Studio Lab?</p>
<p>Many thanks in advance!</p>",1,2,2021-12-16 11:18:24.220000 UTC,,,0,amazon-web-services|command-line-interface|amazon-sagemaker,655,2020-04-09 07:09:05.180000 UTC,2022-07-22 19:29:50.400000 UTC,,97,6,0,18,,,,,,['amazon-sagemaker']
Problems with Image Label Adjustment Job in Amazon Sagemaker Ground Truth,"<p>I'm trying to create a Image Label Adjustment Job in Ground Truth and I'm having some trouble. The thing is that I have a dataset of images, in which there are pre-made bounding boxes. I have an external python script that creates the &quot;dataset.manifest&quot; file with the json's of each image. Here are the first four lines of that manifest file:</p>
<pre><code>{&quot;source-ref&quot;: &quot;s3://automatic-defect-detection/LM-WNB1-M-0000126254-camera_2_0022.jpg&quot;, &quot;bounding-box&quot;: {&quot;image_size&quot;: [{&quot;width&quot;: 2048, &quot;height&quot;: 1536, &quot;depth&quot;: 3}], &quot;annotations&quot;: [{&quot;class_id&quot;: 0, &quot;width&quot;: 80, &quot;height&quot;: 80, &quot;top&quot;: 747, &quot;left&quot;: 840}]}, &quot;bounding-box-metadata&quot;: {&quot;class-map&quot;: {&quot;0&quot;: &quot;KK&quot;}, &quot;type&quot;: &quot;groundtruth/object-detection&quot;, &quot;human-annotated&quot;: &quot;yes&quot;}}
{&quot;source-ref&quot;: &quot;s3://automatic-defect-detection/LM-WNB1-M-0000126259-camera_2_0028.jpg&quot;, &quot;bounding-box&quot;: {&quot;image_size&quot;: [{&quot;width&quot;: 2048, &quot;height&quot;: 1536, &quot;depth&quot;: 3}], &quot;annotations&quot;: [{&quot;class_id&quot;: 0, &quot;width&quot;: 80, &quot;height&quot;: 80, &quot;top&quot;: 1359, &quot;left&quot;: 527}]}, &quot;bounding-box-metadata&quot;: {&quot;class-map&quot;: {&quot;0&quot;: &quot;KK&quot;}, &quot;type&quot;: &quot;groundtruth/object-detection&quot;, &quot;human-annotated&quot;: &quot;yes&quot;}}
{&quot;source-ref&quot;: &quot;s3://automatic-defect-detection/LM-WNB1-M-0000126256-camera_3_0006.jpg&quot;, &quot;bounding-box&quot;: {&quot;image_size&quot;: [{&quot;width&quot;: 2048, &quot;height&quot;: 1536, &quot;depth&quot;: 3}], &quot;annotations&quot;: [{&quot;class_id&quot;: 3, &quot;width&quot;: 80, &quot;height&quot;: 80, &quot;top&quot;: 322, &quot;left&quot;: 1154}, {&quot;class_id&quot;: 3, &quot;width&quot;: 80, &quot;height&quot;: 80, &quot;top&quot;: 633, &quot;left&quot;: 968}]}, &quot;bounding-box-metadata&quot;: {&quot;class-map&quot;: {&quot;3&quot;: &quot;FF&quot;}, &quot;type&quot;: &quot;groundtruth/object-detection&quot;, &quot;human-annotated&quot;: &quot;yes&quot;}}
{&quot;source-ref&quot;: &quot;s3://automatic-defect-detection/LM-WNB1-M-0000126253-camera_2_0019.jpg&quot;, &quot;bounding-box&quot;: {&quot;image_size&quot;: [{&quot;width&quot;: 2048, &quot;height&quot;: 1536, &quot;depth&quot;: 3}], &quot;annotations&quot;: [{&quot;class_id&quot;: 2, &quot;width&quot;: 80, &quot;height&quot;: 80, &quot;top&quot;: 428, &quot;left&quot;: 1058}]}, &quot;bounding-box-metadata&quot;: {&quot;class-map&quot;: {&quot;2&quot;: &quot;DD&quot;}, &quot;type&quot;: &quot;groundtruth/object-detection&quot;, &quot;human-annotated&quot;: &quot;yes&quot;}}
</code></pre>
<p>Now the problem is that I'm creating private jobs in Amazon Sagemaker to try it out. I have the manifest file and the images in a S3 bucket, and it actually kinda works. So I select the input manifest, activate the &quot;Existing-labels display options&quot;. The existing labels for the bounding boxes do not appear automatically, so I have to enter them manually (don't know why), but if I do that and try the preview before creating the adjustment job, the bounding boxes appear perfectly and I can adjust them. The thing is that, me being the only worker invited for the job, the job never apears to start working on it, and it just auto-completes. I can see later that the images are there with my pre-made bounding boxes, but the job never appears to adjust those boxes. I don't have the &quot;Automated data labeling&quot; option activated. Is there something missing in my manifest file?</p>",1,0,2022-02-16 16:00:22.603000 UTC,,2022-02-16 20:43:33.643000 UTC,0,amazon-web-services|amazon-s3|amazon-sagemaker|bounding-box,183,2022-01-24 17:36:34.900000 UTC,2022-09-24 04:13:29.370000 UTC,Chile,1,0,0,2,,,,,,['amazon-sagemaker']
Not able to view S3 bucket created from lambda function using python,"<p>I have created an S3 bucket using below code. The bucket got created but I'm not able to see the bucket in S3 console. What could be the reason? I used root user.</p>
<pre><code>s3 = boto3.resource('s3')
try:
    if my_region == 'ap-south-1':
        s3.create_bucket(Bucket=bucket_name, CreateBucketConfiguration={'LocationConstraint': 'ap-south-1'})
    print('S3 bucket ' + bucket_name + ' created successfully')
except Exception as e:
    print('S3 error: ',e)
</code></pre>",0,4,2021-04-05 12:57:52.990000 UTC,,2021-04-05 14:00:55.930000 UTC,0,amazon-web-services|amazon-s3|aws-lambda|amazon-sagemaker,77,2017-07-14 21:33:18.060000 UTC,2021-09-09 06:43:26.800000 UTC,,49,0,0,21,,,,,,['amazon-sagemaker']
Restart Jupyter Lab server running in the background,"<p>I'm trying to restart a Jupyter Lab server (not just the kernels) running in the background of an AWS SageMaker notebook instance. I have already tried the following:</p>

<ul>
<li>Killing the server by it's process ID

<ul>
<li><code>pgrep</code> doesn't show me the process</li>
<li><code>pkill</code> can't find the process</li>
<li><code>ps aux</code> shows the process ID as constantly changing</li>
</ul></li>
<li>Stopping the server through <code>jupyter notebook stop</code>

<ul>
<li>I get an SSL error and nothing happens</li>
</ul></li>
</ul>

<p>The only thing I've been able to do is reboot the entire instance, which isn't a great option as it can take awhile to become available again.</p>

<p>Edit 1:</p>

<p>The main reason I am trying to do this is that after installing the <code>tqdm</code> package and trying to use <code>tqdm.notebook</code> in Jupyter Lab, in order for it to display correctly I need to enable/install notebook and lab extensions. In order for these to take effect the server then needs to be restarted.</p>",3,4,2020-03-04 21:28:26.390000 UTC,2.0,2020-03-04 23:29:05.603000 UTC,5,amazon-web-services|jupyter-notebook|jupyter|amazon-sagemaker|jupyter-lab,11324,2015-04-17 17:47:20.597000 UTC,2022-08-15 03:14:38.040000 UTC,United States,246,60,5,11,,,,,,['amazon-sagemaker']
How to make a prediction to a private Vertex AI endpoint with Node.js client libraries?,"<p>Documentation on this is a bit vague at the time of posting <a href=""https://cloud.google.com/vertex-ai/docs/predictions/using-private-endpoints#sending-prediction-to-private-endpoint"" rel=""nofollow noreferrer"">https://cloud.google.com/vertex-ai/docs/predictions/using-private-endpoints#sending-prediction-to-private-endpoint</a> , they only mention how to do it with curl.</p>
<p>I would like to use the node.js client library if possible, but I've only managed to find examples that don't use a private endpoint ie: <a href=""https://github.com/googleapis/nodejs-ai-platform/blob/main/samples/predict-custom-trained-model.js"" rel=""nofollow noreferrer"">https://github.com/googleapis/nodejs-ai-platform/blob/main/samples/predict-custom-trained-model.js</a> .</p>
<p>I've read through the type definitions of <code>PredictionServiceClient</code> imported from <code>@google-cloud/aiplatform</code> and didn't find a way to plug in my private endpoint. I've tried making the request anyway by simply specifying the resource name by doing <code>const endpoint = projects/${project}/locations/${location}/endpoints/${endpointId}</code> but this leads to the following error:</p>
<pre><code>Error: 13 INTERNAL: Received RST_STREAM with code 0
    at Object.callErrorFromStatus (/home/vitor/vertexai/node_modules/@grpc/grpc-js/src/call.ts:81:24)
    at Object.onReceiveStatus (/home/vitor/vertexai/node_modules/@grpc/grpc-js/src/client.ts:343:36)
    at Object.onReceiveStatus (/home/vitor/vertexai/node_modules/@grpc/grpc-js/src/client-interceptors.ts:462:34)
    at Object.onReceiveStatus (/home/vitor/vertexai/node_modules/@grpc/grpc-js/src/client-interceptors.ts:424:48)
    at /home/vitor/vertexai/node_modules/@grpc/grpc-js/src/call-stream.ts:323:24
    at processTicksAndRejections (node:internal/process/task_queues:78:11) {
  code: 13,
  details: 'Received RST_STREAM with code 0',
  metadata: Metadata { internalRepr: Map(0) {}, options: {} }
}
</code></pre>
<p>My code looks like this:</p>
<pre><code>(async () =&gt; {
        const client = new v1beta1.PredictionServiceClient();
        const location = &quot;****&quot;;
        const project = &quot;****&quot;;
        const endpointId = &quot;****&quot;
        const endpoint = `projects/${project}/locations/${location}/endpoints/${endpointId}`;

        const parameters = {
            structValue: {
                fields: {},
            },
        };

        const toInstance = (obj: any) =&gt; (
            {
                structValue: {
                    fields: {
                        ****
                    }
                }
            });

        const instance = toInstance(****);
        const instances = [instance];

        const res = await client.predict({
            instances,
            endpoint,
            parameters
        });
        console.log(res);
    })();
</code></pre>
<p>Is it possible to make this kind of request atm?</p>",1,10,2022-03-14 13:05:35.233000 UTC,,,1,node.js|google-cloud-platform|google-cloud-vertex-ai,676,2020-02-15 20:36:26.270000 UTC,2022-09-08 19:19:18.530000 UTC,,185,9,0,12,,,,,,['google-cloud-vertex-ai']
Trying to work on R using Azure ML Studio Notebook and facing challenges with ODBC package,"<p>I am trying to work on R notebook on ML Studio. Using regular python is easy and works as expected but with R i am facing challenges.</p>
<p>While trying to connect to MS SQL database using odbc() :</p>
<pre><code>library(odbc)
con &lt;- dbConnect(odbc(),
                 Driver = &quot;SQL Server&quot;,
                 Server = &quot;server&quot;,
                 Database = &quot;db&quot;,
                 UID = &quot;user&quot;,
                 PWD = &quot;password&quot;,
                 Port = 1433)



Error: nanodbc/nanodbc.cpp:1021: 00000: [unixODBC][Driver Manager]Can't open lib 'SQL Server' : file not found
</code></pre>
<p>As suggested in some posts, i have also tried replacing  Driver = &quot;SQL Server&quot;, with Driver = &quot;ODBC Driver 11 for SQL Server&quot;. But i see similar error</p>
<pre><code>Error: nanodbc/nanodbc.cpp:1021: 00000: [unixODBC][Driver Manager]Can't open lib 'ODBC Driver 11 for SQL Server' : file not found 
Traceback:
</code></pre>
<p>Please suggest a work around.</p>",1,2,2022-08-04 10:54:35.930000 UTC,,,0,r|azure|azure-machine-learning-service,52,2017-06-02 12:27:30.497000 UTC,2022-09-20 06:48:21.120000 UTC,"Bangalore, Karnataka, India",3,0,0,6,,,,,,['azure-machine-learning-service']
How to automate cleaning of data in AWS using Jupyter Notebook,"<p>I have a Jupyter Notebook file that cleans the data file (.csv) in S3. The cleaning process is taken care of...</p>
<p>However, I want to be able to automatically apply this cleaning process to every file that is uploaded to the S3 bucket. Each file will have the exact same data format. I am thinking maybe of using AWS Glue, but not sure where to start. If we can skip the upload to S3 and go straight into Glue that would be interesting to explore...</p>
<p>The end goal is to load the clean data in Quick Sight and also AWS Sage Maker for ML applications.</p>
<p>Any advice on how to approach this?</p>
<p>Thanks</p>",1,1,2022-07-09 04:46:41.077000 UTC,,,-2,amazon-web-services|jupyter-notebook|aws-glue|amazon-sagemaker,38,2022-06-24 04:36:44.433000 UTC,2022-09-24 21:40:40.873000 UTC,,1,0,0,4,,,,,,['amazon-sagemaker']
PySparkProcessor: how to configure spark.driver.memory in AWS Sagemaker ProcessingStep,"<p>I am using <a href=""https://sagemaker.readthedocs.io/en/stable/api/training/processing.html#sagemaker.spark.processing.PySparkProcessor"" rel=""nofollow noreferrer"">PySparkProcessor</a> as one of my processing steps in Sagemaker Pipeline to process the data.</p>
<pre><code>spark = SparkSession.builder.appName(&quot;Test&quot;).getOrCreate()
with open(&quot;unique-guids.json&quot;) as f:
    guids = json.load(f)
guids_bc = spark.sparkContext.broadcast(guids)
</code></pre>
<p>I am broadcasting a file (<em>unique-guids.json</em>) of around 200KB, but I see the following error.</p>
<pre><code>2021-11-02T11:55:41.314-04:00   21/11/02 15:55:40 WARN memory.MemoryStore: Not enough space to cache broadcast_12 in memory! (computed 1050.9 MB so far)
2021-11-02T11:55:41.314-04:00   21/11/02 15:55:40 WARN storage.BlockManager: Persisting block broadcast_12 to disk instead.
2021-11-02T11:56:08.323-04:00   [/var/log/yarn/userlogs/application_1635868431764_0001/container_1635868431764_0001_01_000002/stderr] 21/11/02 15:55:36 INFO executor.Executor: Finished task 1343.0#
2021-11-02T11:56:08.323-04:00   # java.lang.OutOfMemoryError: Java heap space
2021-11-02T11:56:08.323-04:00   # -XX:OnOutOfMemoryError=&quot;kill -9 %p&quot;
2021-11-02T11:56:08.323-04:00   # Executing /bin/sh -c &quot;kill -9 937&quot;...
2021-11-02T11:56:08.323-04:00   11-02 15:56 smspark-submit ERROR spark-submit command failed with exit code -9: Command 'spark-submit --master yarn --deploy-mode client /opt/ml/processing/input/code/xxx.py keycite_uri yyy json_keycite_uri zzz' died with &lt;Signals.SIGKILL: 9&gt;.
2021-11-02T11:56:08.323-04:00   Traceback (most recent call last): File &quot;/usr/local/lib/python3.7/site-packages/smspark/job.py&quot;, line 149, in run subprocess.run(spark_submit_cmd, check=True, shell=True) File &quot;/usr/lib64/python3.7/subprocess.py&quot;, line 512, in run output=stdout, stderr=stderr)
2021-11-02T11:56:08.323-04:00   subprocess.CalledProcessError: Command 'spark-submit --master yarn --deploy-mode client /opt/ml/processing/input/code/filter_keycites.py keycite_uri yyy json_keycite_uri zzz' died with &lt;Signals.SIGKILL: 9&gt;.
2021-11-02T11:56:08.323-04:00   Command 'spark-submit --master yarn --deploy-mode client /opt/ml/processing/input/code/filter_keycites.py keycite_uri xxx json_keycite_uri yyy' died with &lt;Signals.SIGKILL: 9&gt;.
2021-11-02T11:56:08.323-04:00   Traceback (most recent call last): File &quot;/usr/local/lib/python3.7/site-packages/smspark/job.py&quot;, line 149, in run subprocess.run(spark_submit_cmd, check=True, shell=True) File &quot;/usr/lib64/python3.7/subprocess.py&quot;, line 512, in run output=stdout, stderr=stderr)
2021-11-02T11:56:08.323-04:00   subprocess.CalledProcessError: Command 'spark-submit --master yarn --deploy-mode client /opt/ml/processing/input/code/xx.py keycite_uri zzz json_keycite_uri yyy' died with &lt;Signals.SIGKILL: 9&gt;.
2021-11-02T11:56:08.323-04:00   11-02 15:56 smspark-submit INFO exiting with code -9: Algorithm Error: (caused by CalledProcessError): spark failed with a non-zero exit code: Command 'spark-submit --master yarn --deploy-mode client /opt/ml/processing/input/code/xxx.py keycite_uri yyy json_keycite_uri zzz' died with &lt;Signals.SIGKILL: 9&gt;.
</code></pre>
<p>I guess the issue is related to the <em>spark.driver.memory</em> so I tried to override the default <strong>2g</strong> using the following approach but no luck.</p>
<pre><code>spark = SparkSession.builder.appName(&quot;Test&quot;).config(&quot;spark.driver.memory&quot;, &quot;4g&quot;).getOrCreate()
</code></pre>
<p>According to <a href=""https://stackoverflow.com/a/38644726/1545855"">this</a> answer, I need to use the command line option to configure <em>driver.memory</em>. However, in my case, the <a href=""https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html?highlight=ProcessingStep#sagemaker.workflow.steps.ProcessingStep"" rel=""nofollow noreferrer"">ProcesingStep</a> is launching the spark job, so I don't see any option to pass <em>driver.memory</em>. According to the <a href=""https://sagemaker.readthedocs.io/en/stable/api/training/processing.html?highlight=PySparkProcessor#sagemaker.spark.processing.PySparkProcessor.get_run_args"" rel=""nofollow noreferrer"">docs</a> it looks like <em>RunArgs</em> object is the option to pass configuration but <a href=""https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html?highlight=ProcessingStep#sagemaker.workflow.steps.ProcessingStep"" rel=""nofollow noreferrer"">ProcessingStep</a> can't take <em>RunArgs</em> or configuration.</p>",1,0,2021-11-02 21:00:36.360000 UTC,,2021-11-02 21:47:52.250000 UTC,1,apache-spark|pyspark|out-of-memory|amazon-sagemaker,743,2012-07-23 12:37:37.997000 UTC,2022-09-21 22:02:10.023000 UTC,"Toronto, ON, Canada",347,40,0,40,,,,,,['amazon-sagemaker']
Using custom trained Keras model with Sagemaker endpoint results ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation:,"<p>I am trying to predict by loading pre-trained model in sagemaker, but I am getting the below error</p>
<blockquote>
<p>ModelError: An error occurred (ModelError) when calling the
InvokeEndpoint operation:  Received client error (400) from primary
with message &quot;{ &quot;error&quot;: &quot;Session was not created with a graph before
Run()!&quot; }</p>
</blockquote>
<p>My code</p>
<pre><code>def convert_h5_to_aws(loaded_model):
    import tensorflow as tf
    if tf.executing_eagerly():
        tf.compat.v1.disable_eager_execution()
    &quot;&quot;&quot;
    given a pre-trained keras model, this function converts it to a TF protobuf format
    and saves it in the file structure which aws expects
    &quot;&quot;&quot;  
    from tensorflow.python.saved_model import builder
    from tensorflow.python.saved_model.signature_def_utils import predict_signature_def
    from tensorflow.python.saved_model import tag_constants
    
    # This is the file structure which AWS expects. Cannot be changed. 
    model_version = '1'
    export_dir = 'export/Servo/' + model_version
    
    # Build the Protocol Buffer SavedModel at 'export_dir'
    builder = builder.SavedModelBuilder(export_dir)
    
    # Create prediction signature to be used by TensorFlow Serving Predict API
    signature = predict_signature_def(
        inputs={&quot;inputs&quot;: loaded_model.input}, outputs={&quot;score&quot;:    loaded_model.output})
    
    from keras import backend as K
    with K.get_session() as sess:
        # Save the meta graph and variables
        builder.add_meta_graph_and_variables(
            sess=sess, tags=[tag_constants.SERVING], signature_def_map={&quot;serving_default&quot;: signature})
        builder.save()
    
    #create a tarball/tar file and zip it
    import tarfile
    with tarfile.open('model.tar.gz', mode='w:gz') as archive:
        archive.add('export', recursive=True)
        
convert_h5_to_aws(model)



import sagemaker

sagemaker_session = sagemaker.Session()
inputs = sagemaker_session.upload_data(path='model.tar.gz', key_prefix='model')

!touch train.py #create an empty python file
import boto3, re
from sagemaker import get_execution_role

# the (default) IAM role you created when creating this notebook
role = get_execution_role()
import boto3, re
from sagemaker import get_execution_role

# the (default) IAM role you created when creating this notebook
role = get_execution_role()

# Create a Sagemaker model (see AWS console&gt;SageMaker&gt;Models)
from sagemaker.tensorflow.model import TensorFlowModel
sagemaker_model = TensorFlowModel(model_data = 's3://' + sagemaker_session.default_bucket() + '/model/model.tar.gz',
                                  role = role,
                                  framework_version = '1.12',
                                  entry_point = 'train.py')


# Deploy a SageMaker to an endpoint
predictor = sagemaker_model.deploy(initial_instance_count=1,
                                   instance_type='ml.m4.xlarge')
                                 

# Create a predictor which uses this new endpoint
import sagemaker
from sagemaker.tensorflow.model import TensorFlowModel

#endpoint = '' #get endpoint name from SageMaker &gt; endpoints

predictor=sagemaker.tensorflow.model.TensorFlowPredictor(endpoint, sagemaker_session)
# .predict send the data to our endpoint
data = X_test #&lt;-- update this to have inputs for your model
predictor.predict(data)

</code></pre>
<p>I also tried using different versions of TensorFlowModel</p>",1,0,2022-02-28 01:36:58.193000 UTC,,2022-02-28 04:51:07.363000 UTC,0,amazon-web-services|tensorflow|amazon-s3|keras|amazon-sagemaker,261,2021-07-09 03:35:49.153000 UTC,2022-08-31 15:13:07.890000 UTC,,115,12,0,28,,,,,,['amazon-sagemaker']
Model deployment in Azure ML,"<p>I am a beginner in the Azure. I am using this tutorial <a href=""https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-and-where?tabs=python"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-and-where?tabs=python</a> of setting a dummy script for a local web service but many errors are coming up. It is strange because I am using an h5 file (model involving Keras and tensor flow) in place of onxx file. I used the code</p>
<pre><code>from azureml.core import Environment
from azureml.core.model import InferenceConfig

env = Environment(name=&quot;myenv&quot;)
conda_dep = CondaDependencies()
conda_dep.add_conda_package(&quot;tensorflow&quot;)
conda_dep.add_conda_package(&quot;pip&quot;)
conda_dep.add_pip_package(&quot;azureml-core&quot;)
conda_dep.add_pip_package(&quot;azureml-contrib-services&quot;)
env.python.conda_dependencies=conda_dep
inference_config = InferenceConfig(
    environment=env,
    source_directory=&quot;./source_dir&quot;,
    entry_script=&quot;./echo_score.py&quot;,
)
 
</code></pre>
<p>There is a Error</p>
<pre><code>ERROR: Could not find a version that satisfies the requirement pickle (from -r /azureml-environment-setup/condaenv.4f206b3i.requirements.txt (line 5)) (from versions: none)
</code></pre>
<p>I have tried many times and getting error. Can anyone help.
My azureml core is 1.34.0 and python version 3.8.11.I am also not sure if it is a pickle protocol error. Pickle version in my system is 4.</p>",0,2,2021-09-09 06:46:13.543000 UTC,,2021-09-16 12:53:10.997000 UTC,0,python|azure-web-app-service|web-deployment|azure-machine-learning-service,185,2021-05-28 05:28:29.977000 UTC,2021-09-30 06:52:30.927000 UTC,Singapore,21,0,0,1,,,,,,['azure-machine-learning-service']
Distributed Training Terminology: Micro-batch and Per-Replica batch size,"<p>I am reading through the <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/distributed-training.html"" rel=""nofollow noreferrer"">Sagemaker documentation</a> on distributed training and confused on the terminology:</p>
<p>Mini-Batch, Micro-batch and Per-replica batch size</p>
<p>I understand that in data parallelism, there would be multiple copies of the model and each copy would receive data of  size = &quot;Per Replica Batch Size&quot;</p>
<ol>
<li>Could someone ELI5 how micro-batch would fit in this context?</li>
<li>Is this a common terminology used in the terminology or is this specific to AWS Sagemaker</li>
</ol>",1,0,2022-08-04 20:24:38.150000 UTC,,,1,amazon-sagemaker|distributed-training,23,2014-01-28 04:59:31.883000 UTC,2022-09-20 23:39:17.010000 UTC,"College Station, TX, United States",426,408,1,27,,,,,,['amazon-sagemaker']
Azure ML release bug AZUREML_COMPUTE_USE COMMON_RUNTIME,"<p>On 2021-10-13 in our application in Azure ML platform we get this new error that causes failures in pipeline steps - python module import failures - <a href=""https://i.stack.imgur.com/Q7Jeb.png"" rel=""nofollow noreferrer"">warning stack &lt;- warning that leads to pipeline runtime error</a></p>
<p>we needed to set it to false. Why is it failing? What exactly are exact (and long term) consequences when opting out? Also, Azure ML users - do you think it was rolled out appropriately?</p>",3,0,2021-10-13 11:03:15.570000 UTC,,2021-10-13 17:53:38.823000 UTC,0,azure|machine-learning|azure-devops|azure-machine-learning-service|mlops,493,2016-01-28 12:12:03.393000 UTC,2022-09-05 14:11:42.977000 UTC,Antarctica,11,0,0,11,,,,,,['azure-machine-learning-service']
How to retrieve compute cluster name from ComputeTarget.list(workspace=ws) in Azure ML,"<p>I use <code>clist=</code><a href=""https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.computetarget?view=azure-ml-py#list-workspace-"" rel=""nofollow noreferrer"">ComputeTarget.list(workspace=ws)</a> to find a list of compute target in the workspace, which looks like this:</p>
<pre><code>[{
   &quot;id&quot;: &quot;/subscriptions/94e14ad4-bf97-47e8-aae0-f9b85a7befa8/resourceGroups/.../providers/Microsoft.MachineLearningServices/workspaces/.../computes/std-ds3-v2&quot;,
   &quot;name&quot;: &quot;std-ds3-v2&quot;,
   &quot;location&quot;: &quot;southcentralus&quot;,
   &quot;tags&quot;: null,
   &quot;properties&quot;: {
     &quot;description&quot;: null,
     &quot;computeType&quot;: &quot;ComputeInstance&quot;,
     &quot;computeLocation&quot;: &quot;southcentralus&quot;,
     &quot;resourceId&quot;: null,
     &quot;provisioningErrors&quot;: null,
     &quot;provisioningState&quot;: &quot;Succeeded&quot;,
     &quot;properties&quot;: {
       &quot;vmSize&quot;: &quot;STANDARD_DS3_V2&quot;,
       &quot;applications&quot;: [
         {
           &quot;displayName&quot;: &quot;Jupyter&quot;,
           &quot;endpointUri&quot;: &quot;https://std-ds3-v2.southcentralus.instances.azureml.ms&quot;
         },
         {
           &quot;displayName&quot;: &quot;Jupyter Lab&quot;,
           &quot;endpointUri&quot;: &quot;https://std-ds3-v2.southcentralus.instances.azureml.ms/lab&quot;
         },
         {
           &quot;displayName&quot;: &quot;RStudio&quot;,
           &quot;endpointUri&quot;: &quot;https://std-ds3-v2-8787.southcentralus.instances.azureml.ms&quot;
         }
       ],
     ...
     ...
   }
 }]
</code></pre>
<p>The <code>clist</code> object looks like a list of dictionary elements. I want to retrieve the dictionary element <code>&quot;name&quot;: &quot;std-ds3-v2&quot;</code> dynamically, so I tried <code>clist[0]['name']</code> but got this error:</p>
<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-101-df855b4d0107&gt; in &lt;module&gt;
----&gt; 1 clist[0]['name']

TypeError: 'ComputeInstance' object is not subscriptable
</code></pre>
<p>How to retrieve <code>&quot;name&quot;: &quot;std-ds3-v2&quot;</code> from the <code>clist</code> object?</p>
<p>Thank you.</p>",1,0,2020-12-13 12:56:31.720000 UTC,,,1,azure-machine-learning-studio|azure-machine-learning-service|azure-sdk-python,244,2017-05-24 01:09:10.030000 UTC,2020-12-16 08:24:08.143000 UTC,,127,0,0,15,,,,,,"['azure-machine-learning-studio', 'azure-machine-learning-service']"
"Why do I encounter ""INVALID_PARAMETER VALUE"" error when opening ""Models"" tab in MLFlow UI?","<p>I installed mlflow via pip and opened it in the browser through the terminal. The tab Experiments displays normally; however, when I switch to Models tab, the app crashes due to the following error:</p>
<pre><code>INVALID_PARAMETER_VALUE: Model registry functionality is unavailable; 
got unsupported URI './mlruns' for model registry data storage. 
Supported URI schemes are: ['postgresql', 'mysql', 'sqlite', 'mssql']. 
See https://www.mlflow.org/docs/latest/tracking.html#storage for how to run 
an MLflow server against one of the supported backend storage locations.
</code></pre>
<p>I would like to log model data locally without connecting to external servers or databases. I thank you in advance for any help!</p>",1,0,2022-08-08 11:20:16.140000 UTC,,2022-08-11 11:41:03.030000 UTC,1,mlflow,39,2022-05-16 14:30:04.250000 UTC,2022-09-24 07:56:53.253000 UTC,,51,2,0,4,,,,,,['mlflow']
The storage account for this workspace has been deleted,"<p>The storage account for this workspace has been deleted. which I have been using for my Machine Learning Studio. What should I do as when I try to save my experiment it shows that no workspace is found.</p>

<p><a href=""https://i.stack.imgur.com/Hsb99.png"" rel=""nofollow noreferrer"">See the Image for reference which showing storage account has been deleted</a></p>",1,0,2017-12-03 16:04:43.010000 UTC,,,0,azure|azure-storage|azure-machine-learning-studio,397,2016-06-30 19:11:27.490000 UTC,2021-10-29 06:40:01.207000 UTC,"Mumbai, Maharashtra, India",35,0,0,25,,,,,,['azure-machine-learning-studio']
Tuncate a table in Azure SQL database through azure machine learning experiment,"<p>I'm using Azure machine learning studio to create some predictions, at the end of pipeline I need to truncate previous data in a Azure sql table and then write the new prediction, this is going to happen on a daily basis. How I could truncate a table from an experiment, it seems that there were a solution with reader component in Azure ML classic, but it is not working from current ML studio?</p>",1,0,2022-03-29 12:23:50.217000 UTC,,,0,sql|azure|azure-machine-learning-studio,52,2022-03-09 13:16:11.260000 UTC,2022-04-19 13:22:29.093000 UTC,,1,0,0,0,,,,,,['azure-machine-learning-studio']
Azure ML - How to retrain a model (classic web service),"<p>I followed this guide to retrain a model (Guide: <a href=""https://docs.microsoft.com/en-us/azure/machine-learning/machine-learning-retrain-a-classic-web-service"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/machine-learning/machine-learning-retrain-a-classic-web-service</a>)</p>

<p>Still there are some questions left. So before retraining a model, do I have to upload the new dataset into my  blob container storage ? If yes how do I do that via http ?</p>

<p>Maybe is it possible to send the new dataset via the PATCH-call in the http body?</p>

<p>Thanks in Advance!</p>",1,1,2017-05-16 11:27:06.127000 UTC,,,0,azure|azure-machine-learning-studio,537,2016-06-28 16:08:04.653000 UTC,2022-09-23 09:26:32.090000 UTC,,915,19,1,33,,,,,,['azure-machine-learning-studio']
FastAPI slow in MLFlow,"<p>I have a use case where I want to deploy ML Models with low latency and high throughput.
In MLFlow, I couldn't achieve it, so I tried FastAPI and it showed quite good results.</p>
<p>Thus, I tried to replace the Flask engine with FastAPI in MLFlow. But I am getting very low throughput per process on using FastAPI in MLFlow as compared to standalone FastAPI serving for the same model.</p>
<p><a href=""https://github.com/tsenart/vegeta"" rel=""nofollow noreferrer"">Vegeta</a> benchmarking results:</p>
<p>MLFlow with FastAPI Test1:</p>
<pre><code>(base) [ec2-user@ip-172-31-43-232 testing]$ cat target_all.txt | ./vegeta attack -duration=1m -rate=100 -max-workers=20 | ./vegeta report
Requests [total, rate, throughput] 1174, 100.09, 100.07
Duration [total, attack, wait] 11.732s, 11.73s, 2.03ms
Latencies [min, mean, 50, 90, 95, 99, max] 1.71ms, 2.23ms, 1.87ms, 3.394ms, 3.911ms, 7.58ms, 9.813ms
Bytes In [total, mean] 111530, 95.00
Bytes Out [total, mean] 224234, 191.00
Success [ratio] 100.00%
Status Codes [code:count] 200:1174
</code></pre>
<p>MLFlow with FastAPI Test2:</p>
<pre><code>(base) [ec2-user@ip-172-31-43-232 testing]$ cat target_all.txt | ./vegeta attack -duration=1m -rate=200 -max-workers=20 | ./vegeta report
Requests [total, rate, throughput] 2159, 158.75, 157.30
Duration [total, attack, wait] 13.725s, 13.6s, 125.605ms
Latencies [min, mean, 50, 90, 95, 99, max] 10.256ms, 124.783ms, 125.951ms, 129.294ms, 133.891ms, 148.876ms, 293.218ms
Bytes In [total, mean] 209423, 97.00
Bytes Out [total, mean] 412369, 191.00
Success [ratio] 100.00%
Status Codes [code:count] 200:2159
</code></pre>
<p>Standalone FastAPI Test3:</p>
<pre><code>Requests [total, rate, throughput] 36000, 600, 599
Latencies [min, mean, 50, 90, 95, 99, max] 1.515ms, 2.608ms, 2.734ms, 3.386ms, 3.611ms, 4.204ms, 4.795ms
</code></pre>
<p>On using FastAPI, I am getting throughput to be more than 600 RPS per Gunicorn worker, while on using FastAPI with MLFlow, I am getting 100 RPS at max without affecting the response time of the model.
I think I am missing something in this <a href=""https://github.com/mlflow/mlflow/pull/5599"" rel=""nofollow noreferrer"">PR</a>. Can someone please help?</p>",0,1,2022-04-05 07:17:27.347000 UTC,,,0,fastapi|mlflow|mlops,296,2016-09-30 18:28:36.607000 UTC,2022-05-25 08:19:33.757000 UTC,,490,7,11,30,,,,,,['mlflow']
Datapipeline from Sagemaker to Redshift,"<p>I wanted to check with the community here if they have explore the pipeline option from Sagemaker to Redshift directly.
I want to load the predicted data from Sagemaker to a table in Redshift. I was planning to do it via S3, but was wondering if there are better ways to do this.</p>",1,0,2020-10-27 22:04:08.310000 UTC,,,0,amazon-redshift|pipeline|amazon-sagemaker,196,2018-11-06 00:15:50.273000 UTC,2021-11-14 23:28:56.073000 UTC,,106,5,0,172,,,,,,['amazon-sagemaker']
Jupyter Notebook: Git Files,"<p>I’m trying to populate my Sagemaker EC2 instance with scripts/modules from my git repo for running Jupyter notebooks.</p>
<p>When I clone the git repo using access token, I get a repo.git file in my Jupyter directory where notebooks live.</p>
<p>What do I do with this file and how do I Import the py files from the repo? My goal is to actually have the files in my local instance and navigate through them(vs a single flat .git file)</p>
<p>Thanks in advance!</p>",0,3,2021-03-15 21:58:38.817000 UTC,,,0,git|jupyter-notebook|amazon-sagemaker,23,2021-01-07 20:27:20.693000 UTC,2022-07-14 18:41:19.350000 UTC,,21,0,0,24,,,,,,['amazon-sagemaker']
Load/access/mount directory to aws sagemaker from S3,"<p>I am a newbee to aws s3/sagemaker. I am strugling to access my data [data meaning folders/directories, not any specific file/files] from S3 bucket to sagemaker jupyter notebook.</p>
<p>Say, my URI is:</p>
<p><code>s3://data/sub/dir/</code>, where <code>dir</code> may contain multiple directories with files. I need to acess the directory (e.g., dir) in such a way where I can access any sub directories/files from it. I tried-</p>
<p><code>!aws s3 cp s3://data/sub/dir tempdata --recursive</code> but did not work, getting error like-</p>
<p><code>fatal error: An error occurred (404) when calling the HeadObject operation: Key &quot;sub/dir&quot; does not exist</code>.</p>
<p>Please advice, how can I access the dirs from s3 buckets to my aws sagemaker jupyter lab.</p>
<p>Or how to mount s3 buckets to sagemaker? I also tried <a href=""https://medium.com/tensult/aws-how-to-mount-s3-bucket-using-iam-role-on-ec2-linux-instance-ad2afd4513ef"" rel=""nofollow noreferrer"">this link</a> and installed with no errors but s3fs wont show when I run <code>dh -f</code>, thus not worked as well! Thanks in advance.</p>",1,6,2021-10-25 08:21:58.017000 UTC,,,0,python-3.x|amazon-web-services|amazon-s3|jupyter-notebook|amazon-sagemaker,430,2018-09-11 01:45:31.350000 UTC,2022-05-20 16:47:28.750000 UTC,Sweden,92,18,0,44,,,,,,['amazon-sagemaker']
mlflow: problems with pip installation,"<p>I read through many threads regarding installation issues using pip. However, I could find a solution to help me fix my problem.
I installed mlflow with :</p>

<pre><code>    pip3 install mlflow
</code></pre>

<p>so mlflow is installed in /usr/local/bin/mlflow</p>

<p>Since it is not in /Users/xxxx/opt/anaconda3/lib/python3.7/site-packages, I get ""ModuleNotFoundError: No module named 'mlflow' error when I try to run code that imports mlflow module. How should I fix this?</p>",1,1,2019-12-16 20:37:11.793000 UTC,,,1,pip|python-import|python-3.7|importerror|mlflow,9843,2018-04-26 22:59:32.553000 UTC,2021-12-14 18:54:28.437000 UTC,"San Francisco, CA, USA",87,1,0,20,,,,,,['mlflow']
How to set up AWS Sagemaker multi input model with TensorFlow 2 using Pipe mode (can I use a Manifest File?),"<p>I have 20,000 images in an s3 bucket, I have figured out how to use PIPE mode with sagemaker for individual images that are labeled, but I am struggling on how to implement this when I have two images that equal a class / have a label. </p>

<p>In the 20,000 images They combine together multiple different times to create labeled data, imagine folder name is label and inside is two images. I could copy images multiple times for each combination used, but this is inefficient storage wise and will be far more expensive to store.  </p>

<p>My thought process was to use a manifest file, this would then enable me to A) select less than the whole dataset to test and B) both images could be selected for a label. </p>

<p>Looking at the AWS docs for Augmented Manifest File its seems you would be able to create something like </p>

<p><code>{""image1-ref"": ""s3 url goes here"", ""image2-ref"": ""s3 url goes here"", ""label"": ""example""}</code></p>

<p>But seems the only examples I can find show only one image ref.  I have looked for a similar question, but what I could find referenced one image.</p>

<p>Any help would be appreciated.</p>",0,0,2020-02-25 18:32:08.833000 UTC,,,1,amazon-web-services|tensorflow|amazon-s3|amazon-sagemaker,190,2013-05-12 10:22:46.767000 UTC,2020-10-27 15:01:55.293000 UTC,,65,1,0,6,,,,,,['amazon-sagemaker']
Github as artifact repo in mlflow,"<p>Can we use github as one of the options in artifact repository in additions to the object storage support.</p>

<p>Github seems to be a natural way to capture changes in code between different runs/experiments, this will also give a way of tying down the revision of code used in a registered model.</p>

<p>Model version --> runs --> github version.  Nothing golden than this.</p>

<p>Thoughts ?</p>",1,0,2020-03-05 18:55:15.517000 UTC,,,0,machine-learning|mlflow,311,2016-06-05 04:26:47.473000 UTC,2022-09-23 19:49:38.557000 UTC,,1,0,0,0,,,,,,['mlflow']
Deploy an externally trained tensorflow model artefact,"<p>I would like to host a tensorflow model (trained on my local PC) on Sagemaker and I followed this article: <a href=""https://aws.amazon.com/fr/blogs/machine-learning/bring-your-own-pre-trained-mxnet-or-tensorflow-models-into-amazon-sagemaker/"" rel=""nofollow noreferrer"">https://aws.amazon.com/fr/blogs/machine-learning/bring-your-own-pre-trained-mxnet-or-tensorflow-models-into-amazon-sagemaker/</a></p>

<p>However I'm not able to perform the inference. My model is an object detection model, locally it has generated frozen_inference_graph.pb, model.ckpt.data-00000-of-00001, model.ckpt.index and model.ckpt.meta, all this files have been converted into save_model.pb and variables.data-00000-of-00001, variable.index, then into .tar.gz by respecting the folder structure export/Servo/version/...</p>

<p>.tar.gz has been manually uploaded to s3 and I have successfully created an endpoint.</p>

<p>But when I try to perform inference, I have an error:
ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (415) from model with message ""{""error"": ""Unsupported Media Type: Unknown""}""</p>

<p>My input datas are images. I need help. </p>",0,3,2019-10-15 18:41:28.070000 UTC,,,0,amazon-sagemaker,372,2019-10-03 12:56:42.463000 UTC,2019-11-06 07:56:17.540000 UTC,"Paris, France",1,0,0,3,,,,,,['amazon-sagemaker']
"Kubeflow, passing Python dataframe across components?","<p>I am writing a Kubeflow component which reads an input query and creates a <code>dataframe</code>, roughly as:</p>
<pre><code>from kfp.v2.dsl import component 

@component(...)
def read_and_write():
    # read the input query 
    # transform to dataframe 
    sql.to_dataframe()
</code></pre>
<p>I was wondering how I can pass this dataframe to the next operation in my Kubeflow pipeline.
Is this possible? Or do I have to save the dataframe in a csv or other formats and then pass the output path of this?
Thank you</p>",1,0,2021-10-12 12:32:47.303000 UTC,,,1,python|kubeflow|kubeflow-pipelines|tfx|google-cloud-vertex-ai,879,2021-06-22 08:58:12.607000 UTC,2022-04-05 15:39:38.490000 UTC,,51,6,0,3,,,,,,['google-cloud-vertex-ai']
Cant connect to workspace,"<p>Im trying to complete the very first training module offered by MS. Something Im missing that isn't detailed on the documentation of the training.</p>

<p>These are the instructions I'm following
<a href=""https://github.com/MicrosoftDocs/mslearn-aml-labs/blob/master/labdocs/Lab01.md"" rel=""nofollow noreferrer"">https://github.com/MicrosoftDocs/mslearn-aml-labs/blob/master/labdocs/Lab01.md</a></p>

<p>All good until I have to run the second command defined on the notebook called 
""01-Getting_Started_with_Azure_ML.ipynb"". 
And yes I entered the device login code as the instructions indicate.</p>

<p>Look at the attached screenshot of the error returned after running the command of the notebook.</p>

<p><a href=""https://i.stack.imgur.com/uKSLT.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uKSLT.jpg"" alt=""Scheenshot""></a></p>",1,0,2020-04-09 18:45:09.503000 UTC,,,0,azure-machine-learning-studio,26,2018-06-21 12:32:13.470000 UTC,2021-03-05 20:01:04.397000 UTC,,26,0,0,3,,,,,,['azure-machine-learning-studio']
How to Backup/Restore TRAINS-server when moving from AMI to local machine,"<p>I recently started using TRAINS, with the server in AWS AMI. We are currently using v0.9.0.</p>

<p>I would like to move the TRAINS-server to run on our on-premises kubernetes cluster. However, I don't want to lose the data on the current server in AWS (experiments, models, logins, etc...).
Is there a way to backup the current server and restore it to the local server?</p>

<p>Thanks!</p>",1,0,2019-06-26 08:27:33.917000 UTC,,2021-01-05 15:40:59.227000 UTC,1,kubernetes|deep-learning|trains|clearml,2703,2016-11-08 19:13:17.167000 UTC,2019-06-26 11:21:41.617000 UTC,,31,1,0,6,,,,,,['clearml']
AWS SageMaker Very large Dataset,"<p>I have a csv file of 500GB and a mysql database of 1.5 TB of data and I want to run aws sagemaker classification and regression algorithm and random forest on it.</p>

<p>Can aws sagemaker support it? can model be read and trained in batches or chunks? any example for it</p>",3,0,2018-03-19 20:49:47.020000 UTC,3.0,,6,amazon-web-services|amazon-sagemaker,7729,2013-03-20 04:21:17.817000 UTC,2022-02-23 22:57:31.623000 UTC,,111,53,0,76,,,,,,['amazon-sagemaker']
Create data container via Azure Machine learning REST API,"<p>I understand API is in preview but maybe someone can help me out here .</p>
<p>I am trying to create <code>Data container</code> as its described in <a href=""https://docs.microsoft.com/en-us/rest/api/azureml/data-containers/create-or-update#datacontainer"" rel=""nofollow noreferrer"">AML REST API</a></p>
<pre><code>curl --location --request PUT 'https://management.azure.com/subscriptions/{{subscriptionId}}/resourceGroups/{{resourceGroupName}}/providers/Microsoft.MachineLearningServices/workspaces/{{workspaceName}}/data/abc?api-version=2021-03-01-preview' \
--header 'Authorization: Bearer ' \
--header 'Content-Type: application/json' \
--data-raw '{&quot;properties&quot;: { &quot;description&quot;: &quot;string&quot;,
&quot;tags&quot;: { },
&quot;properties&quot;: {}
}
}'
</code></pre>
<p>I receive <code>400</code> status code with message</p>
<pre><code>Error setting value to 'Description' on 'Microsoft.MachineLearning.ManagementFrontEnd.Contracts.V20210301Preview.Assets.DataContainer'.&quot;
</code></pre>
<p>after removing Description which should be optional</p>
<pre><code>DataContainers_CreateOrUpdate is not supported
</code></pre>
<p>How can I create Dataset with local files? I cannot use UI. There it works and I can receive it with GET list method.</p>",2,0,2021-11-26 15:39:00.327000 UTC,,,1,azure|machine-learning|curl|azure-machine-learning-service,132,2013-01-22 14:57:30.523000 UTC,2022-09-23 08:13:32.320000 UTC,Switzerland,720,438,1,133,,,,,,['azure-machine-learning-service']
No Experimentation Account found in your Azure Subscriptions in Azure Machine Learning Workbench,"<p>I created one Experiment and hosted as web service in Azure ML Stdio</p>

<p><a href=""https://studio.azureml.net/"" rel=""nofollow noreferrer"">https://studio.azureml.net/</a></p>

<p>However, I have installed Azure Machine Learning Workbench and logging into same account. It says:</p>

<p><strong>No Experimentation Account found in your Azure Subscriptions
You can create one in the Microsoft Azure Management Portal.</strong></p>",1,0,2018-09-01 13:07:04.517000 UTC,,2018-09-01 17:15:03.233000 UTC,1,azure|azure-machine-learning-workbench,113,2013-09-04 06:59:59.787000 UTC,2022-09-24 15:28:58.507000 UTC,"Hyderabad, India",2273,37,7,177,,,,,,['azure-machine-learning-workbench']
Annotation specs - AutoML (VertexAi),"<p>We're trying to build an imaged based product search for our webshop using the vertex ai image classification model (single label).</p>
<p>Currently we have around 20k products with xx images per product.
So our dataset containing 20k of labels (one for each product - product number), but on import we receive the following error message:</p>
<p><code>There are too many AnnotationSpecs in the dataset. Up to 5000 AnnotationSpecs are allowed in one Dataset. Check your csv/jsonl format with our public documentation.</code></p>
<p>Looks like not more than 5000 labels are allowed per Dataset... This quota is not really visible in the documentation - or we didn't find it.</p>
<p>Anyway, any ideas how we can make it work? Does we have to build 5 Datasets with 5 different Endpoints and than query every Enpoint for matching?</p>",1,0,2022-07-20 07:39:36.407000 UTC,,,0,google-cloud-vertex-ai,38,2022-07-20 07:24:37.233000 UTC,2022-09-19 15:58:45.803000 UTC,,1,0,0,0,,,,,,['google-cloud-vertex-ai']
azureml Submitting deployment to compute taking very long,"<p>Azureml is stuck on submitting deployment to compute for a very long time. how can I speed this up? is it because of cpu and memory or due to other reasons?</p>
<pre><code>log output 

Running
2022-08-23 14:51:11+00:00 Creating Container Registry if not exists.
2022-08-23 14:51:11+00:00 Registering the environment.
2022-08-23 14:51:13+00:00 Use the existing image.
2022-08-23 14:51:13+00:00 Generating deployment configuration.
2022-08-23 14:51:24+00:00 Submitting deployment to compute..
</code></pre>
<p>code</p>
<pre><code>#Define the deployment configuration
aciconfig = AciWebservice.deploy_configuration(
    cpu_cores = 1,
    memory_gb = 1,
    dns_name_label = os.environ['ACI_DNS_NAME_LABEL']
)

env = Environment.from_conda_specification(&quot;env&quot;, &quot;../Environments/score_env.yml&quot;)

inf_conf = InferenceConfig(entry_script=&quot;score.py&quot;,environment=env)

#deploy successful  models as a web service 
webservice_name = os.environ['WEB_SERVICE_NAME']
retries = 2
while retries &gt; 0:
    try:
        service = Model.deploy(ws, webservice_name,models_latest,inf_conf,aciconfig, overwrite=True)
        service.wait_for_deployment(True)
        print(&quot;Webservice updated&quot;)
        break

    except:
        print(service.get_logs())
        retries -= 1
        if retries == 0:
            raise


    
</code></pre>",1,0,2022-08-23 14:59:26.740000 UTC,,2022-08-25 10:40:03.943000 UTC,0,python|azure|azure-machine-learning-service|azureml-python-sdk|azure-ml-pipelines,31,2021-10-27 11:28:41.057000 UTC,2022-09-22 14:14:03.980000 UTC,,128,28,0,24,,,,,,['azure-machine-learning-service']
Using GPU Spot Instance(s) for SageMaker Distributed Training?,"<p>I have a requirement to use N 1x GPU Spot instances instead of 1x N-GPU instance for distributed training.</p>
<p>Does SageMaker Distributed Training support the use of GPU Spot instance(s)? If yes, how to enable it?</p>",1,0,2022-09-11 03:36:36.307000 UTC,,,0,amazon-web-services|amazon-sagemaker|distributed-training|amazon-machine-learning|spot-instances,21,2014-01-16 15:43:59.673000 UTC,2022-09-25 03:22:08.463000 UTC,Singapore,5854,155,70,794,,,,,,['amazon-sagemaker']
Installing additional R package (ImputeTS R Package) in Azure ML,"<p>I referred the below stack overflow query regarding installing additional R package in Azure ML. However I'am getting the error </p>

<p>Trail 1 : Installing miniCRAN package for windows (<a href=""https://cran.r-project.org/web/packages/imputeTS/index.html"" rel=""nofollow noreferrer"">https://cran.r-project.org/web/packages/imputeTS/index.html</a>)</p>

<p>Trail 2:  Installing ImputeTS package for windows (<a href=""https://cran.r-project.org/web/packages/miniCRAN/index.html"" rel=""nofollow noreferrer"">https://cran.r-project.org/web/packages/miniCRAN/index.html</a>)</p>

<p><strong>I double zipped and tried as per the below stack overflow query question. But, still facing the same issue</strong></p>

<p>R version i'm using : <code>CRAN 3.1.0</code></p>

<p>I need to use the <code>package ImputeTS.</code></p>

<p><strong>Stack overflow query link :</strong>
<a href=""https://stackoverflow.com/questions/27568624/installing-additional-r-package-on-azure-ml"">Installing additional R Package on Azure ML</a></p>

<p><strong>Error 1:</strong> </p>

<pre><code>    Error 0063: The following error occurred during evaluation of R script:

    ---------- Start of error message from R ----------

    zip file 'src/miniCRAN.zip' not found
</code></pre>

<p><strong>Error 2:</strong> </p>

<pre><code>     Error 0063: The following error occurred during evaluation of R script:

     ---------- Start of error message from R ----------

     zip file 'src/imputeTS.zip' not found
</code></pre>

<p><strong>R script :</strong></p>

<pre><code>JCI_CO2  &lt;- maml.mapInputPort(1)

library(dplyr)
library(tidyr)
library(lubridate)

#install.packages(""src/imputeTS.zip"", lib = ""."", repos = NULL, verbose = TRUE)
#(success &lt;- library(""imputeTS"", lib.loc = ""."", logical.return = TRUE, verbose = TRUE))

 #library(imputeTS)
 #library(imputeTS,lib.loc = ""."")


install.packages(""src/miniCRAN.zip"", lib = ""."", repos = NULL, verbose = TRUE)
(success &lt;- library(""miniCRAN"", lib.loc = ""."", logical.return = TRUE, verbose = TRUE))

library(miniCRAN)
library(miniCRAN,lib.loc = ""."")

library(imputeTS)

dt2 &lt;- JCI_CO2 %&gt;%
  mutate(Date.Time = mdy_hm(Date.Time)) %&gt;%
  filter(Date.Time %in% seq(min(Date.Time), max(Date.Time), by = ""15 min"")) %&gt;%
  complete(Date.Time = seq(min(Date.Time), max(Date.Time), by = ""15 min"")) %&gt;%
  mutate(RA.CO2 = na.interpolation(RA.CO2)) %&gt;%
  arrange(desc(Date.Time))


  JCI_CO2 &lt;- data.frame(dt2)

  maml.mapOutputPort(""JCI_CO2"");
</code></pre>

<p><strong>Note :</strong> <em>All the rest of the packages in the code i.e dplyr, tidyr, lubridate are already part of the azure ml R package. <strong>Except ImputeTS which i am trying to install.</em></strong></p>",1,0,2017-09-27 14:26:00.287000 UTC,,2018-01-21 16:15:03.190000 UTC,1,r|azure-machine-learning-studio|imputets,797,2016-11-15 07:23:47.133000 UTC,2018-07-12 09:45:50.387000 UTC,,2713,26,2,358,,,,,,['azure-machine-learning-studio']
is there a way to export experiments from on azure ml designer workspace to another?,"<p>is there a way to export experiments from an azure ml designer workspace to another? Like moving changes from INT to QA to PROD</p>
<p>Thanks in advance</p>
<p>Regards,
Kiran</p>",1,0,2020-11-13 11:02:57.027000 UTC,,,0,azure|azure-machine-learning-service,256,2012-09-07 11:33:10.887000 UTC,2022-09-03 07:44:05.233000 UTC,,11,0,0,2,,,,,,['azure-machine-learning-service']
how does scaling policy work with sagemaker endpoints?,"<p>based on the docs provided here , <a href=""https://github.com/aws/amazon-sagemaker-examples/blob/main/async-inference/Async-Inference-Walkthrough.ipynb"" rel=""nofollow noreferrer"">https://github.com/aws/amazon-sagemaker-examples/blob/main/async-inference/Async-Inference-Walkthrough.ipynb</a>
I'm defining an autoscaling policy for my sagemaker endpoint ( sample code below) . I have specified from 0 - 3 capacity for the scalable config. I understand, this will only scale to maximum capacity of 3 when needed and or else it will scale down to 0 after a period. from cost perspective, when it is scale down to 0 , is there any charges? also, what how does it handle where , it scales to maximum capacity and there are more requests in the queue?</p>
<pre><code>client = boto3.client(&quot;application-autoscaling&quot;)
resource_id = (&quot;endpoint/&quot; + endpoint_name + &quot;/variant/&quot; + &quot;variant1&quot;)

# Configure Autoscaling on asynchronous endpoint down to zero instances
response = client.register_scalable_target(
    ServiceNamespace=&quot;sagemaker&quot;,
    ResourceId=resource_id,
    ScalableDimension=&quot;sagemaker:variant:DesiredInstanceCount&quot;,
    MinCapacity=0,
    MaxCapacity=3,
)

response = client.put_scaling_policy(
    PolicyName=&quot;Invocations-ScalingPolicy&quot;,
    ServiceNamespace=&quot;sagemaker&quot;, 
    ResourceId=resource_id,  # Endpoint name
    ScalableDimension=&quot;sagemaker:variant:DesiredInstanceCount&quot;,  # only Instance Count
    PolicyType=&quot;TargetTrackingScaling&quot;,  
    TargetTrackingScalingPolicyConfiguration={
        &quot;TargetValue&quot;: 5.0,  # The target value for the 
        SageMakerVariantInvocationsPerInstance
        &quot;CustomizedMetricSpecification&quot;: {
            &quot;MetricName&quot;: &quot;ApproximateBacklogSizePerInstance&quot;,
            &quot;Namespace&quot;: &quot;AWS/SageMaker&quot;,
            &quot;Dimensions&quot;: [{&quot;Name&quot;: &quot;EndpointName&quot;, &quot;Value&quot;: endpoint_name}],
            &quot;Statistic&quot;: &quot;Average&quot;,
        },
        &quot;ScaleInCooldown&quot;: 600,   
        &quot;ScaleOutCooldown&quot;: 300
    },
)
</code></pre>",1,0,2022-03-03 21:50:02.527000 UTC,,,1,amazon-web-services|amazon-sagemaker|aws-auto-scaling,449,2020-05-30 00:10:41.983000 UTC,2022-09-24 19:51:20.543000 UTC,,525,69,0,98,,,,,,['amazon-sagemaker']
How to load Image data from s3 bucket to sagemaker notebook?,<p>I just started to use aws sagemaker. I tried to import images from my s3 bucket to sagemaker notebook. But I can't import images to the notebook. my image location is <strong>s3://my_bucket/train</strong> how can I import the train folder from the given path to my sagemaker notebook. I've gone through some of  the solution in here and the solutions are for CSV file. All the images in my S3 bucket are in .jpeg format.</p>,4,3,2019-04-02 17:27:14.133000 UTC,2.0,2019-04-04 11:30:05.817000 UTC,5,python|amazon-web-services|amazon-s3|amazon-sagemaker,12686,2016-01-25 19:19:21.223000 UTC,2019-08-25 21:18:23.557000 UTC,,51,0,0,9,,,,,,['amazon-sagemaker']
Import azure.core not found issue in running Notebook through MachineLearningStudio,"<p>I was trying to run a sample tutorial notebook through the ml studio. </p>

<p><a href=""https://notebooks.azure.com/azureml/projects/azureml-getting-started/html/tutorials/img-classification-part1-training.ipynb"" rel=""nofollow noreferrer"">https://notebooks.azure.com/azureml/projects/azureml-getting-started/html/tutorials/img-classification-part1-training.ipynb</a></p>

<p>But when i uploaded i used kernel python3. But when i ran it failed with the error azureml.core not found.</p>

<p>I am new to Azure Stack and ML. Should i install python 3.6 on my own through conda and have my own kernel, i noticed the current installation of python on studio is 3.4.</p>

<p>Please let me know how to proceed further ? I am blocked on it. I need help on deploying the 3.6 version of python on the notebook server. I am not using the notebook vm. I am just using whatever came with the azure notebook option in the ml studio.</p>

<p>How to alter the sys path to point to my libraries to after installation of the new version of python ?</p>

<p>Need help.</p>

<p>It works fine in my local environment as i have python3.6 installed. </p>",1,0,2019-10-21 00:48:09.040000 UTC,,,0,python-3.x|azure-machine-learning-studio|azure-machine-learning-service|azure-machine-learning-workbench,143,2017-01-12 10:07:57.137000 UTC,2022-01-31 05:44:43.400000 UTC,,41,6,0,4,,,,,,"['azure-machine-learning-workbench', 'azure-machine-learning-studio', 'azure-machine-learning-service']"
How to pass parameters to a training script in Azure Machine Learning service?,"<p>I am trying to submit an experiment in Azure Machine Learning service <em>locally</em> on an Azure VM using a <a href=""https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-set-up-training-targets#submit-the-experiment"" rel=""noreferrer""><code>ScriptRunConfig</code></a> object in my workspace <code>ws</code>, as in</p>

<pre><code>from azureml.core import ScriptRunConfig    
from azureml.core.runconfig import RunConfiguration
from azureml.core import Experiment

experiment = Experiment(ws, name='test')
run_local = RunConfiguration()

script_params = {
    '--data-folder': './data',
    '--training-data': 'train.csv'
}

src = ScriptRunConfig(source_directory = './source_dir', 
                      script = 'train.py', 
                      run_config = run_local, 
                      arguments = script_params)

run = experiment.submit(src)
</code></pre>

<p>However, this fails with </p>

<blockquote>
  <p>ExperimentExecutionException: {
      ""error_details"": {
          ""correlation"": {
              ""operation"": ""bb12f5b8bd78084b9b34f088a1d77224"",
              ""request"": ""iGfp+sjC34Q=""
          },
          ""error"": {
              ""code"": ""UserError"",
              ""message"": ""Failed to deserialize run definition""</p>
</blockquote>

<p>Worse, if I set my data folder to use a datastore (which likely I will need to)</p>

<pre><code>script_params = {
    '--data-folder': ds.path('mydatastoredir').as_mount(),
    '--training-data': 'train.csv'
}
</code></pre>

<p>the error is</p>

<blockquote>
  <p>UserErrorException: Dictionary with non-native python type values are
  not supported in runconfigs.<br>
  {'--data-folder':
  $AZUREML_DATAREFERENCE_d93269a580ec4ecf97be428cd2fe79,
  '--training-data': 'train.csv'}</p>
</blockquote>

<p>I don't quite understand how I should pass my <code>script_params</code> parameters to my <code>train.py</code> (<a href=""https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.scriptrunconfig?view=azure-ml-py"" rel=""noreferrer"">the documentation of <code>ScriptRunConfig</code></a> doesn't include a lot of details on this unfortunately).</p>

<p>Does anybody know how to properly create <code>src</code> in these two cases?</p>",2,3,2019-04-06 22:05:02.737000 UTC,,2019-04-09 09:58:23.827000 UTC,8,python|azure|azure-machine-learning-service,3924,2014-11-11 16:17:30.717000 UTC,2022-09-24 20:31:18.173000 UTC,"Verona, VR, Italy",4811,376,73,713,,,,,,['azure-machine-learning-service']
Deploying Amazon Textract application via Sagemaker,"<p>I am trying to build an application via Amazon Textract that extracts the textual information from Images and validates the text. I am searching for a way to deploy the application via Sagemaker but could not find any method to deploy the application. The models built on TensorFlow, PyTorch, Sklearn, etc. can be deployed via Sagemaker. How do we deploy the Textract application via Sagemaker?</p>",1,0,2022-08-29 07:05:55.320000 UTC,,,0,amazon-sagemaker|amazon-textract,30,2018-07-02 11:03:47.243000 UTC,2022-09-25 03:02:58.203000 UTC,,131,0,0,13,,,,,,['amazon-sagemaker']
Unable to set up sftp artifact storage for MLFlow,"<p>I want to set up a tracking MLFlow server with external metrics and artifact storage.
I have the following docker containers inside docker network: mlflow-server, postgres, sftp-mlflow and python-client.
I was able to set up postgres and connect it to the mlflow-server and the client:</p>
<pre><code>mlflow server --backend-store-uri postgresql://postgres:&lt;pass&gt;@mlflow_db:5432/mlflow_db --default-artifact-root sftp://sftp:&lt;pass&gt;@sftp-mlflow:22 -h 0.0.0.0 -p 8000
</code></pre>
<p>However I cannot do anything about artifact storage. Tried the following sftp images</p>
<ul>
<li><a href=""https://hub.docker.com/r/atmoz/sftp"" rel=""nofollow noreferrer"">https://hub.docker.com/r/atmoz/sftp</a></li>
<li><a href=""https://github.com/madnificent/root-sftp-docker"" rel=""nofollow noreferrer"">https://github.com/madnificent/root-sftp-docker</a></li>
</ul>
<p>Also followed this <a href=""https://medium.com/@gyani91/setup-mlflow-in-production-d72aecde7fef"" rel=""nofollow noreferrer"">guide</a>.
But still artifact storage doesn't work =(</p>
<p>On my client side I have</p>
<pre class=""lang-py prettyprint-override""><code>import mlflow
import matplotlib
import matplotlib.pyplot as plt
import numpy as np

remote_server_uri = &quot;http://mlflow-server:8000&quot; # set to your server URI
mlflow.set_tracking_uri(remote_server_uri)

# plotting
fig.savefig(&quot;test.png&quot;)

ARTIFACT_URI = &quot;sftp://sftp:&lt;pass&gt;@sftp-mlflow:22&quot;
EXPERIMENT_NAME = &quot;test&quot;
mlflow.create_experiment(EXPERIMENT_NAME, artifact_location=ARTIFACT_URI)
mlflow.set_experiment(EXPERIMENT_NAME)
with mlflow.start_run():
    mlflow.log_param(&quot;a&quot;, 1)
    mlflow.log_metric(&quot;b&quot;, 2)
    mlflow.log_artifact('test.png')

</code></pre>
<p>and when running this code I get:</p>
<pre><code>2020/08/06 01:05:19 ERROR mlflow.utils.rest_utils: API request to http://mlflow-server:8000/api/2.0/mlflow/experiments/create failed with code 500 != 200, retrying up to 0 more times. API response body: &lt;!DOCTYPE HTML PUBLIC &quot;-//W3C//DTD HTML 3.2 Final//EN&quot;&gt;
&lt;title&gt;500 Internal Server Error&lt;/title&gt;
&lt;h1&gt;Internal Server Error&lt;/h1&gt;
&lt;p&gt;The server encountered an internal error and was unable to complete your request. Either the server is overloaded or there is an error in the application.&lt;/p&gt;

Traceback (most recent call last):
  File &quot;/usr/local/lib/python3.8/runpy.py&quot;, line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File &quot;/usr/local/lib/python3.8/runpy.py&quot;, line 87, in _run_code
    exec(code, run_globals)
  File &quot;/run.py&quot;, line 24, in &lt;module&gt;
    mlflow.create_experiment(EXPERIMENT_NAME, artifact_location=ARTIFACT_URI)
  File &quot;/usr/local/lib/python3.8/site-packages/mlflow/tracking/fluent.py&quot;, line 357, in create_experiment
    return MlflowClient().create_experiment(name, artifact_location)
  File &quot;/usr/local/lib/python3.8/site-packages/mlflow/tracking/client.py&quot;, line 164, in create_experiment
    return self._tracking_client.create_experiment(name, artifact_location)
  File &quot;/usr/local/lib/python3.8/site-packages/mlflow/tracking/_tracking_service/client.py&quot;, line 126, in create_experiment
    return self.store.create_experiment(
  File &quot;/usr/local/lib/python3.8/site-packages/mlflow/store/tracking/rest_store.py&quot;, line 54, in create_experiment
    response_proto = self._call_endpoint(CreateExperiment, req_body)
  File &quot;/usr/local/lib/python3.8/site-packages/mlflow/store/tracking/rest_store.py&quot;, line 32, in _call_endpoint
    return call_endpoint(self.get_host_creds(), endpoint, method, json_body, response_proto)
  File &quot;/usr/local/lib/python3.8/site-packages/mlflow/utils/rest_utils.py&quot;, line 142, in call_endpoint
    response = http_request(
  File &quot;/usr/local/lib/python3.8/site-packages/mlflow/utils/rest_utils.py&quot;, line 86, in http_request
    raise MlflowException(&quot;API request to %s failed to return code 200 after %s tries&quot; %
mlflow.exceptions.MlflowException: API request to http://mlflow-server:8000/api/2.0/mlflow/experiments/create failed to return code 200 after 3 tries
</code></pre>
<p>I can connect to sftp storage from mlflow-server contaner and from python client using sftp:
<code>sftp -P 22 sftp@sftp-mlflow</code></p>",1,1,2020-08-06 01:14:23.440000 UTC,,,2,python|postgresql|docker|sftp|mlflow,1969,2017-01-13 15:57:38.310000 UTC,2022-05-31 08:17:03.240000 UTC,"Moscow, Россия",21,0,0,5,,,,,,['mlflow']
Token authentication failed: 'utf-8' codec can't decode byte 0xe4 in position 0: invalid continuation byte,"<p>I'm trying to send the json data from azure ml to eventhub</p>
<pre><code>import json
d = result.to_dict(orient='records')
data = json.dumps(d,ensure_ascii=False)
</code></pre>
<p>this is the screenshot of output of the variable data-<a href=""https://i.stack.imgur.com/aBU0Q.png"" rel=""nofollow noreferrer"">output</a></p>
<pre><code>import asyncio
from azure.eventhub.aio import EventHubProducerClient
from azure.eventhub import EventData
import time
conn_sting = &quot;Endpoint=***&quot;
async def run():
producer = EventHubProducerClient.from_connection_string(conn_str=conn_string)
async with producer:
    # Create a batch.
    event_data_batch = await producer.create_batch(partition_id='0')
    event_data_batch.add(EventData(data))

    # Send the batch of events to the event hub.
    await producer.send_batch(event_data_batch)
    
nest_asyncio.apply()
loop = asyncio.get_event_loop()
loop.run_until_complete(run())
print(&quot;sent to eventhub&quot;)
</code></pre>
<p>and getting following error..</p>
<pre><code>Token authentication failed: 'utf-8' codec can't decode byte 0xe4 in 
position 0: invalid continuation byte
Token authentication failed: 'utf-8' codec can't decode byte 0xe4 in 
position 0: invalid continuation byte
</code></pre>
<p>anyone could help debug the error?
thanks</p>",1,0,2021-12-22 16:55:31.913000 UTC,,2021-12-22 22:45:57.307000 UTC,0,python|jupyter-notebook|azure-eventhub|azure-machine-learning-service,487,2019-10-06 15:22:27.083000 UTC,2022-05-05 03:45:34.053000 UTC,,9,0,0,12,,,,,,['azure-machine-learning-service']
Sagemaker lifecycle config: could not find conda environment conda_python3,"<p>the below script should run a notebook called prepTimePreProcessing whenever a AWS notebook instance starts runing.
however I am getting &quot;could not find conda environment conda_python3&quot; error from the lifecycle config file.</p>
<pre class=""lang-bash prettyprint-override""><code>set -e
ENVIRONMENT=python3
NOTEBOOK_FILE=&quot;/home/ec2-user/SageMaker/prepTimePreProcessing.ipynb&quot;
echo &quot;Activating conda env&quot;
source /home/ec2-user/anaconda3/bin/activate &quot;$ENVIRONMENT&quot;
echo &quot;Starting notebook&quot;
nohup jupyter nbconvert  --to notebook --inplace --ExecutePreprocessor.timeout=600 --ExecutePreprocessor.kernel_name=python3 --execute &quot;$NOTEBOOK_FILE&quot; &amp;
</code></pre>
<p>Any help whould be appreciated.</p>",0,0,2022-09-23 08:40:25.847000 UTC,,2022-09-23 08:41:16.307000 UTC,0,amazon-web-services|amazon-sagemaker,9,2022-09-23 07:15:37.043000 UTC,2022-09-23 13:50:30.643000 UTC,,1,0,0,0,,,,,,['amazon-sagemaker']
"R Script : ""Error: (list) object cannot be coerced to type double""","<p>I am facing a issue while doing R Script in Azure MIL and error is
i.e. ""Error: (list) object cannot be coerced to type double""</p>

<p>My code is</p>

<pre><code>dataset1 &lt;-maml.mapInputPort(2)
dataset3 &lt;-maml.mapInputPort(1)
Z &lt;- as.numeric((dataset3),stringsAsFactors=TRUE)
Y &lt;- mdBinaryDesign(Z,4,dataset1)
Y.aggregate=mdBinaryToAggregateDesign(Y)
survey.design=mdDesignNames(Y.aggregate, dataset1)
data.set &lt;- as.data.frame(survey.design)
maml.mapOutputPort(""data.set"")
</code></pre>

<p>The issue in coming while assigning value to Z variable. dataset3 has simple numeric data i.e. ""5"" , which acts as a input to my model.</p>

<p><a href=""https://i.stack.imgur.com/YwVDR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YwVDR.png"" alt=""enter image description here""></a></p>",1,2,2016-11-02 10:22:23.523000 UTC,1.0,2016-11-02 10:38:55.857000 UTC,2,r|azure-machine-learning-studio,41537,2015-02-02 08:45:49.097000 UTC,2021-10-13 14:11:20.567000 UTC,,31,0,0,31,,,,,,['azure-machine-learning-studio']
NNS: Is there a way to omit certain features in a prediction?,"<p>I'd like to train a nearest-neighbor search model with something like the following -</p>

<pre><code>        | feature_A | feature_B | feature_C |
---------------------------------------------
point_1 |       0.0 |       5.1 |      94.9 |
point_2 |      80.7 |      35.3 |      64.7 |
</code></pre>

<p>And so on, with many more features and data points.</p>

<p>We can then pass a point for prediction, such as <code>[0.3, 5.0, 94.0] -&gt; returns point_1</code> as is typical in nearest-neighbor search.</p>

<p>What I'd like to do, also, is pass a value for a certain feature or subset of features and return the closest point, ignoring all other features, like so:</p>

<p><code>[None, 5.0, None] -&gt; returns point_1</code></p>

<p><code>[80.0, None, 64.7] -&gt; returns point_2</code></p>

<p>The problem is that these nulls can't be passed to the model and they can't really be inferred to be 0.0 as this is not what the prediction is asking for. The feature needs to be omitted entirely from the calculation.</p>

<p>Is there a way to do this natively using Sagemaker/Scikit-learn libraries without having to build multiple models or write a custom implementation of NNS?</p>",2,1,2020-01-29 22:13:36.427000 UTC,,2020-02-12 16:36:05.880000 UTC,3,scikit-learn|knn|amazon-sagemaker,92,2018-10-16 14:53:06.583000 UTC,2022-02-16 17:08:06.373000 UTC,,623,42,1,118,,,,,,['amazon-sagemaker']
Run failed: Unable to establish SSH connection in Azure ML Service Pipeline,"<p>I am getting <code>Run failed: Unable to establish SSH connection</code> error when I trigger my Published Azure ML Pipeline using Azure Function App while the VM is closed. Normally The Azure ML Pipeline should be able to automatically turn the virtual machine on when I trigger it and close the VM when the process done. Otherwise, it doesn't make any sense. </p>

<p>Sometimes I don't get such an error and the pipeline just works perfectly. </p>

<p>Also, the Pipeline works without a problem when I manually start the VM from AzurePortal before trigger the pipeline.</p>

<p>The Published Pipeline uses Azure Data Science Virtual Machine - Ubuntu. I am using username and password to access the VM.</p>",1,3,2020-06-12 09:44:20.000000 UTC,,2020-06-13 13:32:59.887000 UTC,1,azure|azure-machine-learning-service,253,2015-12-07 20:55:46.610000 UTC,2022-07-26 08:13:06.867000 UTC,,23,3,0,2,,,,,,['azure-machine-learning-service']
Export metrics of ClearML to Prometheus and show them in Grafana,"<p>Are there any metrics I can get from the API server? or any docker image I can point to the backend and get some metrics?
Most important is the see how many tasks running in real-time (like we can see on the worker's page) and also check how much time each task is running (also can be found on the worker's page)</p>
<p>If it does not exist, do they have an API for getting all this information?</p>",1,0,2021-12-20 15:16:30.177000 UTC,,,0,prometheus|grafana|metrics|clearml,70,2019-09-21 19:04:11.057000 UTC,2022-09-06 09:31:53.617000 UTC,,3,0,0,3,,,,,,['clearml']
400 bad request when running the pipeline,"<p>I am new to azure and I'm creating a simple hellow world pipeline. Here is how I create tasks:</p>
<pre><code>data_import_step = PythonScriptStep(
      script_name=&quot;Data_Load_Preprocessing.py&quot;,
      arguments=['--process_output',data_step_output],
      allow_reuse=False,
      outputs=[data_step_output],
      runconfig=RunConfiguration(),
      compute_target=aml_compute,
      source_directory=process_directory
   )
</code></pre>
<p>I am declaring aml_compute like this:</p>
<pre><code>aml_compute = AmlCompute(ws, clustername)
</code></pre>
<p>and getting workspace like this</p>
<pre><code>ws = Workspace.from_config(path=&quot;./config.json&quot;, auth=interactive_auth)
</code></pre>
<p>and data_load_preprocessing.py looks like this:</p>
<pre><code>def main():
    print(&quot;hellow world&quot;)
if __name__ == '__main__':
    main()
</code></pre>
<p>I get following error:
ervice invocation failed!
Request: GET ....
Status Code: 400 BadRequest
Error Code: ValidationError
Reason Phrase: Bad Request</p>",1,0,2021-08-16 08:33:24.663000 UTC,,2021-08-16 09:16:01.940000 UTC,0,python|azure|azure-machine-learning-service,316,2021-07-03 19:42:10.163000 UTC,2021-12-10 16:25:29.973000 UTC,,63,1,0,41,,,,,,['azure-machine-learning-service']
Does Mflow work only with predictive models?,"<p>I am trying to use Mlflow to manage my Bayesian Optimization Model, which has several methods other than the predict (run_optimization() for example). My doubt is that when I log my model to the tracking server the model and retrieve it, it only contains the predict() as it is wrapped as a PyFunctModel; that's a problem because I need the model also to run prescriptions (suggestion of a possible new optimum), does anyone ever tried it? Thanks</p>",0,0,2022-04-14 06:37:15.563000 UTC,,,0,mlflow,40,2019-04-03 14:27:39.233000 UTC,2022-06-28 08:32:28.750000 UTC,,1,0,0,0,,,,,,['mlflow']
Automated ML model publish in dataflow/powerbi,"<p>when I am publishing the model with designer and referring in the data-flow or power-bi it is working fine. when I am trying to get the rest endpoint from the automated ML. it is showing this screen and I am not clear what to be input in this. I tried providing columns and values. it is not working. it is throwing following error.</p>
<p>any idea what is wrong here.</p>
<p>Expression.Error: We cannot convert the value &quot;[Function]&quot; to type Function.DetailsValue = [Function]</p>
<p><a href=""https://i.stack.imgur.com/HHadB.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HHadB.jpg"" alt=""enter image description here"" /></a></p>",0,2,2022-01-21 05:18:18.017000 UTC,,,0,azure-machine-learning-service,36,2020-11-05 01:13:03.193000 UTC,2022-04-26 08:48:36.923000 UTC,,37,7,0,16,,,,,,['azure-machine-learning-service']
Is there a Java SDK for azure machine learning service?,"<p>Is there a Java SDK for Azure Machine Learning service? If not, is there a way to create Azure ML pipelines, experiments etc from Java codebase?</p>",2,0,2019-06-19 11:44:56.657000 UTC,,,0,azure|azure-java-sdk|azure-machine-learning-service,606,2015-07-20 06:18:39.813000 UTC,2021-09-22 14:43:24.710000 UTC,,132,15,0,29,,,,,,['azure-machine-learning-service']
AWS Sagemaker Studio Local Ram Overload,"<p>So weird issue,
I have been trying to work with AWS sagemaker and just run some of the examples. However, every time I open studio and get to the jupyter labs page about 10s seconds later the browser starts to eat local ram at ~0.1GB/s and one or two cores max out at 100% utilization. It will continue doing this until all local ram on <strong>my local computer</strong> has been used up and then it overloads the swap and everything freezes requiring reboot. This happens with Firefox, Chrome, and Gnome browser. I have yet to try this on windows, if possible I would like to get this working with my current environment.</p>
<p>If it helps:</p>
<pre><code>Ubuntu 20.04.1 LTS, 64 bit
Intel i7-7700HQ CPU @ 2.80GHz × 8
GeForce GTX 1060

In all cases there is a rather large spike in bandwidth corresponding with the start of the issue. 
~7+ MBs for ~2 seconds. This seems to correspond with a fetch request being made. 

The process name consuming the memory and CPU when using chrome appears to be:
chrome -type=renderer followed by a bunch of other stuff

Occasionally the page will crash before using all the ram the error message on chrome is:
Error code: SIGTRAP

If the sagemaker jupyter labs tab is killed or the browser is closed before ram maxes out.
The cores it is maxing out go to normal utilization and the ram is released. 
</code></pre>",0,0,2020-12-01 04:21:24.227000 UTC,,,3,amazon-web-services|out-of-memory|ram|jupyter-lab|amazon-sagemaker,587,2020-10-28 19:35:41.383000 UTC,2022-07-12 14:46:15.927000 UTC,,151,6,0,9,,,,,,['amazon-sagemaker']
how to specify python==3.6.8 for PyTorch Estimator (conda_packages not sufficient),"<p>I need to run my python script under Azure Machine Learning, using <strong>python=3.6.8 (not the default 3.6.2)</strong>.  I am using the AML ""<strong>PyTorch()</strong>"" Estimator, setting the ""<strong>conda_packages</strong>"" arg to <strong>[""python==3.6.8""]</strong>. </p>

<p>I am relying on this doc page for the PyTorch Estimator: </p>

<blockquote>
  <p><a href=""https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.dnn.pytorch?view=azure-ml-py"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.dnn.pytorch?view=azure-ml-py</a></p>
</blockquote>

<p>When my script runs, I print out ""<strong>sys.version</strong>"" and see that it is still set to <strong>python 3.6.2</strong>:</p>

<blockquote>
<pre><code>python: 3.6.2 | packaged by conda-forge | (default, Jul 23 2017, 22:59:30) 
[GCC 4.8.2 20140120 (Red Hat 4.8.2-15)]
</code></pre>
</blockquote>

<p>I expected to see <strong>python 3.6.8</strong>, since I specified that in the PyTorch Estimator's conda_packages arg.</p>

<p>I also tried moving the ""<strong>python==3.6.8</strong>"" from conda_packages to pip_packages, but received an error saying pip could not locate that package.  </p>

<p>FYI, I have another package specified in pip_packages, and that does get installed correctly during this process.  It seems like the value of the ""<strong>conda_packages</strong>"" arg is not being used (I can find no mention of a conda or python install error in the AML logs for my job).</p>",2,0,2019-10-16 11:36:50.950000 UTC,,2019-10-20 19:50:11.490000 UTC,0,python|pytorch|conda|azure-machine-learning-service,567,2017-03-12 21:09:44.663000 UTC,2022-09-22 21:22:08.560000 UTC,,41,62,0,13,,,,,,['azure-machine-learning-service']
"clearml-serving-triton | 2022-06-18 16:21:19,790 - clearml.Task - INFO - No repository found, storing script code instead","<p>i found this error when running this command &quot;docker-compose --env-file example.env -f docker-compose-triton.yml up&quot;.
Actually, when i run this command for the first time, it worked. And then when I try to change to my friend's workspace it suddenly give this error when I run the command again.
I've tried changing example.env file configuration and reinstall the docker, still not worked.
Can anyone help? or have the solution? Thank You :)
<a href=""https://i.stack.imgur.com/Fp8XK.png"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/Fp8XK.png</a>
[Error details on the image]</p>",1,0,2022-06-18 16:35:10.730000 UTC,,2022-06-18 16:36:19.420000 UTC,0,docker|clearml,43,2022-06-18 16:33:07.980000 UTC,2022-09-24 07:26:04.857000 UTC,,1,0,0,0,,,,,,['clearml']
AWS Sagemaker notebook intermittent 'Unable to locate credentials',"<p>I'm trying to use Dask to get multiple files (JSON) from AWS S3 into memory in a Sagemaker Jupyter Notebook.
When I submit 10 or 20 workers, everything runs smoothly. However, when I submit 100 workers, between 30% and 50% of them encounter the following error: 'Unable to locate credentials'</p>
<p>Initially I was trying with Boto3. In order to try to eliminate this issue, I switched to S3FS but the same error is occurring.</p>
<p>The workers which err out with the NoCredentialError are random if I repeat the experiment, as is the exact number of failed downloads.</p>
<p>Sagemaker is handling all the AWS credentials through its IAM role, so I have no access to key pairs or anything. The ~/.aws/config file contains only the default location - nothing about credentials.</p>
<p>It seems this is a very common use for Dask so it's obviously capable of performing such a task - where am I going wrong?</p>
<p>Any help would be much appreciated! Code and traceback below. In this example, 29 workers failed due to credentials.
Thanks,
Patrick</p>
<pre><code>import boto3
import json
import logging
import multiprocessing
from dask.distributed import Client, LocalCluster
import s3fs
import os

THREADS_PER_DASK_WORKER = 4
CPU_COUNT = multiprocessing.cpu_count()
HTTP_SUCCESSFUL_REQUEST_CODE = 200

S3_BUCKET_NAME = '-redacted-'

keys_100 = ['-redacted-']
keys_10 = ['-redacted-']

def dispatch_workers(workers):

    cluster_workers = min(len(workers), CPU_COUNT)
    cluster = LocalCluster(n_workers=cluster_workers, processes=True,
                           threads_per_worker=THREADS_PER_DASK_WORKER)
    client = Client(cluster)

    data = []
    data_futures = []

    for worker in workers:
        data_futures.append(client.submit(worker))

    for future in data_futures:
        try:
            tmp_flight_data = future.result()
            if future.status == 'finished':
                data.append(tmp_flight_data)
            else:
                logging.error(f&quot;Future status = {future.status}&quot;)
        except Exception as err:
            logging.error(err)

    del data_futures

    cluster.close()
    client.close()

    return data

def _get_object_from_bucket(key):

    s3 = s3fs.S3FileSystem(anon=False)# uses default credentials
    with s3.open(os.path.join(S3_BUCKET_NAME,key)) as f:
        return json.loads(f.read())

def get_data(keys):

    objects = dispatch_workers(
        [lambda key=key: _get_object_from_bucket(key) for key in keys]
    )
    return objects
    
data = get_data(keys_100)
</code></pre>
<p>Output:</p>
<pre><code>ERROR:root:Unable to locate credentials
ERROR:root:Unable to locate credentials
ERROR:root:Unable to locate credentials
ERROR:root:Unable to locate credentials
ERROR:root:Unable to locate credentials
ERROR:root:Unable to locate credentials
ERROR:root:Unable to locate credentials
ERROR:root:Unable to locate credentials
ERROR:root:Unable to locate credentials
ERROR:root:Unable to locate credentials
ERROR:root:Unable to locate credentials
ERROR:root:Unable to locate credentials
ERROR:root:Unable to locate credentials
ERROR:root:Unable to locate credentials
ERROR:root:Unable to locate credentials
ERROR:root:Unable to locate credentials
ERROR:root:Unable to locate credentials
ERROR:root:Unable to locate credentials
ERROR:root:Unable to locate credentials
ERROR:root:Unable to locate credentials
ERROR:root:Unable to locate credentials
ERROR:root:Unable to locate credentials
ERROR:root:Unable to locate credentials
ERROR:root:Unable to locate credentials
ERROR:root:Unable to locate credentials
ERROR:root:Unable to locate credentials
ERROR:root:Unable to locate credentials
ERROR:root:Unable to locate credentials
ERROR:root:Unable to locate credentials
</code></pre>",0,0,2021-07-01 04:37:38.370000 UTC,,,2,jupyter-notebook|boto3|amazon-sagemaker|dask-distributed|python-s3fs,309,2021-06-30 20:30:41.003000 UTC,2022-09-23 15:15:07.873000 UTC,"Toronto, ON, Canada",31,0,0,0,,,,,,['amazon-sagemaker']
How do I change the python version in Microsoft Azure Machine Learning Notebooks?,"<p>How can I change the version of Python and Tensorflow? I could not find a section in the interface or settings here that I can change. I tried to change it via console but was not successful. What is your suggestion?
<a href=""https://i.stack.imgur.com/xiPTB.png"" rel=""nofollow noreferrer"">Python and Tensorflow version in Microsoft Azure Machine Learning(Image)</a></p>",1,0,2021-03-07 20:19:22.560000 UTC,,,0,python|tensorflow|azure-machine-learning-studio,525,2020-02-23 14:21:43.637000 UTC,2021-05-25 20:39:01.150000 UTC,,11,0,0,2,,,,,,['azure-machine-learning-studio']
Vectorising categorical dataset for XGBoost in Sagemaker,"<p>When we train an XGB model using AWS built-in models
e.g. <code>(container = sagemaker.image_uris.retrieve(&quot;xgboost&quot;, region, &quot;1.2-1&quot;))</code>,</p>
<p>Based on my understanding, The training job requires numerical vectors for the train and validation.
Meaning that if you have a dataset with categorical values and strings, you need to convert them into a vector. the model only deals with float numbers,
(Outside Sagemaker, I can use TFIDF to vectorize my features and construct a DMatrix), but this approach doesn't seem to be supported by Sagemaker.</p>
<ol>
<li>Does anyone know how this data transformation is done in Sagemaker?</li>
<li>Is this a bad idea to use BlazyngText unsupervised learning to generate the vectors?</li>
<li>Should we have a preprocessing step and in that step we use TFIDF?</li>
</ol>",1,0,2021-07-21 13:30:29.773000 UTC,,,0,python|vectorization|xgboost|amazon-sagemaker,106,2012-07-11 09:35:10.263000 UTC,2022-09-21 19:58:18.557000 UTC,"London, United Kingdom",443,1252,1,83,,,,,,['amazon-sagemaker']
Running Neptune.ai in a loop,"<p>so i created a for loop so I can run various batch sizes, where each loop will open and close a neptune run. The first time runs fine, but the following runs, the accuracy doesn't record into neptune, and python does not throw an error? Can anyone think what the problem may be?</p>
<pre><code>for i in range(len(percentage)):

    run = neptune.init(
        project=&quot;xxx&quot;,
        api_token=&quot;xxx&quot;,
    )

    epochs = 600
    batch_perc = percentage[i]
    lr = 0.001
    sb = 64 #round((43249*batch_perc)*0.00185)
    params = {
        'lr': lr,
        'bs': sb,
        'epochs': epochs,
        'batch %': batch_perc
    }
    run['parameters'] = params

    torch.manual_seed(12345)
    td = 43249 * batch_perc
    vd = 0.1*(43249 - td) + td

    train_dataset = dataset[:round(td)]
    val_dataset = dataset[round(td):round(vd)]
    test_dataset = dataset[round(vd):]

    print(f'Number of training graphs: {len(train_dataset)}')
    run['train'] = len(train_dataset)
    print(f'Number of validation graphs: {len(val_dataset)}')
    run['val'] = len(val_dataset)
    print(f'Number of test graphs: {len(test_dataset)}')
    run['test'] = len(test_dataset)

    train_loader = DataLoader(train_dataset, batch_size=sb, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=sb, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)

    model = GCN(hidden_channels=64).to(device)

    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    criterion = torch.nn.CrossEntropyLoss()

    for epoch in range(1, epochs):
        train()
        train_acc = test(train_loader)
        run['training/batch/acc'].log(train_acc)
        val_acc = test(val_loader)
        run['training/batch/val'].log(val_acc)
</code></pre>",0,0,2022-09-24 00:07:54.247000 UTC,,,0,python|neptune,27,2022-03-25 13:35:17.920000 UTC,2022-09-25 03:48:09.273000 UTC,,31,2,0,0,,,,,,['neptune']
Azure Machine Learning compute cluster - avoid using docker?,"<p>I would like to use an Azure Machine Learning Compute Cluster as a compute target but do not want it to containerize my project. Is there a way to deactivate this &quot;feature&quot; ?</p>
<p>The main reasons behind this request is that :</p>
<ol>
<li>I already set up a docker-compose file that is used to specify 3 containers for Apache Airflow and want to avoid a Docker-in-Docker situation. Especially that I already tried to do so but failed so far (here's the <a href=""https://stackoverflow.com/questions/72380590/airflow-docker-compose-from-another-docker-container-on-azure-machine-learning-c?noredirect=1#comment127881455_72380590"">link</a> my other related SO question).</li>
<li>I prefer not to use a Compute Instance as it is tied to an Azure account which is not ideal for automation purposes.</li>
</ol>
<p>Thanks in advance !</p>",1,0,2022-05-30 11:30:19.710000 UTC,,2022-05-31 10:04:12.600000 UTC,0,azure|docker|docker-compose|azure-machine-learning-studio|azure-machine-learning-service,116,2020-04-10 11:23:52.390000 UTC,2022-08-12 18:19:53.330000 UTC,,127,8,0,20,,,,,,"['azure-machine-learning-service', 'azure-machine-learning-studio']"
"Call Sagemaker endpoint invoke_endpoint in lambda function, question for request body formats","<p>I have a deployed Sagemaker endpoint. When testing the endpoint using <code>Predictor.predict</code>, the endpoint works fine. I can pass down whichever Json format, it is able to process it correctly. However, I've been struggling calling endpoint from Lambda by using <code>client.invoke_endpoint</code></p>
<p>I am trying to modify my request body to follow this format in this <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/cdf-inference.html"" rel=""nofollow noreferrer"">AWS documentation</a>.</p>
<pre><code>let request = {
  // Instances might contain multiple rows that predictions are sought for.
  &quot;instances&quot;: [
    {
      // Request and algorithm specific inference parameters.
      &quot;configuration&quot;: {},
      // Data in the specific format required by the algorithm.
      &quot;data&quot;: {
         &quot;&lt;field name&gt;&quot;: dataElement
       }
    }
  ]
}
</code></pre>
<p>I am not sure what should the configuration be, so this is what my request body looks like.</p>
<pre><code>{
  &quot;instances&quot;: [
    {
      &quot;data&quot;: {
        &quot;ID&quot;: &quot;some ID&quot;,
        &quot;ACCOUNT&quot;: null,
        &quot;LEAD&quot;: some ID
        &quot;FORMNAME&quot;: &quot;some Form&quot;,
        &quot;UTMMEDIUM&quot;: &quot;some Medium&quot;,
        &quot;UTMSOURCE&quot;: &quot;some Source&quot;
      }
    },
    {
      &quot;data&quot;: {
        &quot;ID&quot;: &quot;some ID&quot;
        &quot;ACCOUNT&quot;: &quot;some ID&quot;
        &quot;LEAD&quot;: null,
        &quot;FORMNAME&quot;: &quot;some Form&quot;
        &quot;UTMMEDIUM&quot;: null,
        &quot;UTMSOURCE&quot;: null
      }
    }
  ]
}
</code></pre>
<p>This is my Lambda function</p>
<pre><code>import os
import io
import boto3
import json

# grab environment variables
ENDPOINT_NAME = 'xxxxx'

client = boto3.client('sagemaker-runtime')
def lambda_handler(event, context):
    print(&quot;Received event: &quot; + json.dumps(event, indent=2))
    data = json.loads(json.dumps(event))
    payload = str(data[&quot;instances&quot;])
    response = client.invoke_endpoint(EndpointName = ENDPOINT_NAME,
                                      Body = payload,
                                      ContentType = 'application/json',
                                      Accept = 'application/json')
    print(response)
    return 'nothing'
</code></pre>
<p>I am able to invoke the Endpoint using the code above, but the endpoint kept having trouble processing the input. The <code>input_fn</code> in the endpoint looks like this</p>
<pre><code>def input_fn(input_data, content_type):

    if content_type == 'text/csv':
        # Read the raw input data as CSV. 
        df = pd.read_csv(StringIO(input_data))

        return df
    
    elif content_type == 'application/json':
        print('input_fn (elif): input_data')
        print(input_data)
        print(type(input_data))
        print('input_fn (elif): input_data eval')
        print(eval(input_data))
        print('input_fn (elif): input_data eval type')
        print(type(eval(input_data)))
        df = pd.read_json(eval(input_data))
        print('input_fn (elif): df.columns')
        print(df.columns)
        return df
    
    else:
        raise ValueError(&quot;{} not supported by script!&quot;.format(content_type))
</code></pre>
<p>The error I got is<code>ValueError: Invalid file path or buffer object type: &lt;class 'list'&gt;</code></p>
<p>The type of <code>input_data</code> is a string, and the type of <code>eval(input_data)</code> is a list.</p>
<p>I appreciate any insight! I've tried so many different things, including removing <code>eval</code> from my <code>input_fn</code>, or change <code>pd.json_read</code> to <code>json.loads(json.dumps())</code> with <code>pd.DataFrame.from_dict</code>. I've gotten different errors like <code>json.decoder.JSONDecodeError: Expecting value: Line Column 42</code> (column 42 is where the location of null), and <code>unhashable type: 'dict'</code></p>
<p>I am really confused and not sure what to to next. Thank  you!</p>",2,0,2021-06-21 18:01:35.403000 UTC,1.0,2021-06-21 18:14:30.717000 UTC,0,python|amazon-web-services|aws-lambda|amazon-sagemaker,1811,2020-09-29 01:04:33.190000 UTC,2021-06-28 19:18:06.517000 UTC,"Boston, MA, USA",1,0,0,4,,,,,,['amazon-sagemaker']
Livy session gets corrupted,"<p>we are using a <strong>Apache Livy</strong> server to communicate with a spark cluster. We have noticed that some statements when executed error out and afterwards livy session becomes unusable. For example we are using <strong>fbprophet</strong> for timeseries models. <strong>fbprophet</strong> and <strong>pystan</strong> produce some console output. As soon as this output is logged in session the statement gives error. Although the job continues to execute on the cluster. And afterwards no statement runs correctly.
Output after running statement with fbprophet code</p>
<p><code>Unrecognized token 'Iter': was expecting 'null', 'true', 'false' or NaN</code>
<code>at [Source:     Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes ; line: 1, column: 9]</code></p>
<p>Similarly while creating docker image using <strong>mlflow build-docker</strong> command, it also produces output about the image creation process. Again as soon as this output is generated the session errors out
Following is the output for build-docker</p>
<p><code>Unexpected character ('/' (code 47)): maybe a (non-standard) comment? (not recognized as one since ``````Feature 'ALLOW_COMMENTS' not enabled for parser)</code>
<code>at [Source: /tmp/tmpq6gz1q3t/; line: 1, column: 2]</code></p>
<p>Main issue is that the session becomes unusable after such statements.
Has anyone faced such issues with Apache livy?</p>
<p>One <strong>resolution</strong> that I can think is to suppress the output from pystan and docker processes, But I could not figure that out. How can we suppress the output from being logged into livy session?</p>",0,0,2020-08-13 09:25:25.900000 UTC,,,1,apache-spark|livy|mlflow|pystan|facebook-prophet,202,2013-12-15 09:42:24.757000 UTC,2022-09-23 10:28:34.043000 UTC,,87,12,0,14,,,,,,['mlflow']
"Azure CLI - ""Hard linking failed!"" when downloading Azure DevOps Universal Packages into AzureML Compute Instances","<p>We need additional binaries such as tesseract or pretrained models on our ML computes. As those are not preprovisioned in the Azure ML compute instances and we need to restrict external access, we need to load them differently. We try now via Azure artifacts universal packages.
Unfortunately this does not work:
After having installed the azure devops extension onto the ml compute instance we can download the artifact, but I cannot install it. I get the following error:
Encountered an unexpected error.</p>
<pre><code>System.IO.IOException: Hard linking failed! 
 Status: Failed 
 Path: tesseract/tesseract-ocr_4.00~git2288-10f4998a-2_amd64.deb
   at Microsoft.VisualStudio.Services.BlobStore.WebApi.DedupStoreClient.DownloadToFileAsync(DedupNode node, String fullPath, Uri proxyUri, EdgeCache edgeCache, CancellationToken cancellationToken)
   at Microsoft.VisualStudio.Services.BlobStore.WebApi.DedupStoreClientWithDataport.DownloadToFileAsync(IDedupDataPort dataport, DedupNode node, String fullPath, Uri proxyUri, EdgeCache edgeCache, CancellationToken cancellationToken)
</code></pre>",1,0,2021-06-02 07:06:30.927000 UTC,,2021-06-22 01:11:03.280000 UTC,1,azure|azure-devops|azure-cli|artifacts|azure-machine-learning-service,333,2018-05-22 15:25:21.950000 UTC,2021-11-15 14:54:03.303000 UTC,Berlin,11,0,0,0,,,,,,['azure-machine-learning-service']
Pretraining/transfer learning with SageMaker BlazingText (word2vec)?,"<p>I have a training set consisting of a description and a binary label. From reading previous work, I know that using pretrained <a href=""https://fasttext.cc/docs/en/english-vectors.html"" rel=""nofollow noreferrer"">fasttext embeddings</a> should work well for my use case. I need to be able to make predictions on unseen words (OOV). My company is already using aws/sagemaker, so using <a href=""https://github.com/aws/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/blazingtext_word2vec_subwords_text8/blazingtext_word2vec_subwords_text8.ipynb"" rel=""nofollow noreferrer"">SageMaker Blazing text with subword embedding</a> seems like a good approach.</p>
<p>However, they are providing their own training data in the example - does this means it tries to learn word embeddings from scratch from only this training data? I was expecting to be able to pass pretraining as a parameter, and then fine tune it with my own data so those words get added to the known dictionary.</p>
<p>But the way it looks to me now, is that I either use a lookup against a hardcoded list of pretrained embeddings (like GloVe or one of the word vector datasets from fasttext), which means I'm not using my own data and also have to come up with a solution to handling OOV. Or I use Blazing text which can handle OOV, but then I don't take advantage of any pretrained model?</p>
<p>So my main question is this: can I use Blazing text to get pretrained OOV embeddings? And if so how?</p>
<p>Would be great if I could also understand how transfer learning would work in this case, so I can make use of my own classification data. However, the embeddings will be used in a downstream classification task, so I guess I could say the fine tuning happens there?</p>
<ul>
<li>Use pretrined blazing text model to get embeddings for both seen and unseen words (through subword embeddings)</li>
<li>Combine these embeddings with other features</li>
<li>Fit a model around the embeddings + features to get the classification</li>
</ul>
<p>Appreciate any help!</p>",0,0,2021-10-19 16:02:49.027000 UTC,,,0,nlp|amazon-sagemaker|embedding|pre-trained-model|fasttext,80,2015-01-15 17:43:03.700000 UTC,2022-08-23 22:54:25.603000 UTC,,1387,51,1,153,,,,,,['amazon-sagemaker']
A function/API to list out the supported hyperparameters of SageMaker built-in algorithm,"<p>I am looking for a function/API that can list out all the hyperparameters supported by the built-in algorithms on SageMaker. I fully understand that the best place to look at this list (manually though) is this link:</p>

<p><a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html</a></p>

<p>But I have a requirement in which the Python code should have the capability to fetch all available hyperparameters for an algorithm.</p>

<p>Is there a way to do this?</p>

<p>An added bonus to this would be to also get the supported data-types, Scaling type, Value / Range supported for the respective hyperparameters.</p>",0,1,2020-06-02 15:16:45.047000 UTC,1.0,,1,python|amazon-web-services|amazon-sagemaker|hyperparameters,36,2019-05-17 19:09:24.560000 UTC,2022-09-24 23:45:42.990000 UTC,,2025,86,6,94,,,,,,['amazon-sagemaker']
"Azure 504 DeploymentTimedOut ERROR - Service deployment polling reached non-successful terminal state, current service state","<p>I am trying to deploy my machine learning model in Azure's AciWebservice to expose endpoints for further usage. But, it is showing me status 504 error with DeploymentTimedOut.  Locally, My Model is running fine. This is my prediction.py</p>

<pre><code>%%writefile prediction.py
import json
import numpy as np
import os
import pickle
from sklearn.externals import joblib
from sklearn.linear_model import LogisticRegression
from azureml.core.model import InferenceConfig
from azureml.core.conda_dependencies import CondaDependencies
from azureml.core.model import Model
from azureml.core.environment import Environment
from azureml.core.webservice import LocalWebservice, Webservice

def init():
    global model
    # retrieve the path to the model file using the model name
    model_path = Model.get_model_path('prediction_model')
    model = joblib.load(model_path)

def run(raw_data):
    data = np.array(json.loads(raw_data)['data'])
    # make prediction
    y_hat = model.predict(data)
    return json.dumps(y_hat.tolist())
</code></pre>

<p>and here goes the environment</p>

<pre><code>myenv = Environment(name=""myenv"")
myenv.docker.enabled = True
myenv.docker.base_image = ""mcr.microsoft.com/azureml/o16n-sample-user-base/ubuntu-miniconda""


myenv.docker.base_image_registry.address = ""shohozds.azurecr.io""
myenv.docker.base_image_registry.username = ""farhad""
myenv.docker.base_image_registry.password = ""*********************""

myenv.inferencing_stack_version = ""latest"" 


conda_dep = CondaDependencies()

conda_dep.add_pip_package(""azureml-defaults"")
myenv.python.conda_dependencies=conda_dep
myenv.register(workspace=ws)
</code></pre>

<p>Using this environment in InferenceConfig</p>

<pre><code>inference_config = InferenceConfig(entry_script=""prediction.py"",
                                   environment=envs['myenv'])
</code></pre>

<p>AciWebservice Configuration </p>

<pre><code>deployment_config = AciWebservice.deploy_configuration(cpu_cores = 1, memory_gb = 1)
</code></pre>

<p>And now the model deployment</p>

<pre><code>service = Model.deploy(ws, ""myservice"", [model], inference_config, deployment_config)
service.wait_for_deployment(show_output = True)
print(service.state)
</code></pre>

<p>But I am facing this error</p>

<pre><code>""code"": ""DeploymentTimedOut"",
""statusCode"": 504,
</code></pre>

<p>This is the full trace</p>

<pre><code>ERROR - Service deployment polling reached non-successful terminal state, current service state: Unhealthy
Operation ID: 0e37b930-2707-4d6b-92b0-2203d1c45978
More information can be found using '.get_logs()'
Error:
{
  ""code"": ""DeploymentTimedOut"",
  ""statusCode"": 504,
  ""message"": ""The deployment operation polling has TimedOut. The service creation is taking longer than our normal time. We are still trying to achieve the desired state for the web service. Please check the webservice state for the current webservice health. You can run print(service.state) from the python SDK to retrieve the current state of the webservice.""
}
</code></pre>",1,1,2020-02-25 09:23:43.550000 UTC,,2020-03-03 21:42:53.200000 UTC,0,azure|azure-machine-learning-service,1437,2012-05-25 15:24:04.690000 UTC,2022-09-22 03:04:03.037000 UTC,"Dhaka, Dhaka Division, Bangladesh",6262,68,6,988,,,,,,['azure-machine-learning-service']
SageMaker provisioned endpoint concurrent invocation limit,"<p>On <a href=""https://docs.aws.amazon.com/general/latest/gr/sagemaker.html"" rel=""nofollow noreferrer"">Amazon SageMaker endpoints and quotas</a> I read that Maximum concurrent invocations per endpoint variant is 200 in SageMaker Serverless Inference.
Does this limit applies to provisioned endpoints? If not, what's the equivalent limit for this case?</p>
<p>We are reaching up to 605 Invocations/Second (see graph below) on our provisioned endpoint (3 ml.m5.xlarge), and yet not seeing any throttling / failure on client side. Are we missing something?</p>
<p><a href=""https://i.stack.imgur.com/RAm4g.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RAm4g.png"" alt=""enter image description here"" /></a></p>",1,0,2022-06-06 14:52:26.023000 UTC,,,0,amazon-web-services|amazon-sagemaker,187,2015-01-21 21:45:58.933000 UTC,2022-09-23 08:24:22.427000 UTC,Italy,6098,1155,8,853,,,,,,['amazon-sagemaker']
Microsoft Azure Machine Learning and Cognitive Services API,"<p>Is it possible to call Cognitive Services API in Azure ML studio when build model?” any document our sample experiment can be reference?</p>

<p>Thanks in advance.</p>",2,0,2016-04-28 03:14:00.840000 UTC,1.0,,1,azure-machine-learning-studio|microsoft-cognitive,877,2016-04-28 03:08:09.717000 UTC,2018-07-24 11:12:59.527000 UTC,,741,23,0,16,,,,,,['azure-machine-learning-studio']
Read compressed CSV (gzip) file from AWS S3 into Panda data frame in Sagemaker,<p>I am trying to read a large compressed CSV file from AWS S3 and convert it to a Panda data frame in Sagemaker. Is there any direct and clean approach to do it?</p>,1,0,2021-04-16 03:12:47.620000 UTC,,,1,amazon-web-services|amazon-s3|gzip|amazon-sagemaker,1115,2018-04-24 20:58:14.497000 UTC,2022-09-13 17:32:03.520000 UTC,,13,0,0,3,,,,,,['amazon-sagemaker']
How to make ClearML not upload annotations twice when they have the same ID?,"<p>The following uploads two annotations, though I expected there to be only one</p>
<pre><code>from typing import List
from allegroai import Dataset, DatasetVersion, SingleFrame, DataView
from allegroai.dataframe.annotation import BoundingBox2D

allegro_frame = SingleFrame(
    source=&quot;/irrelevant/source.png&quot;
)
ann_id = &quot;the_id&quot;
label = &quot;the_label&quot;
annotation = BoundingBox2D(id=ann_id)
allegro_frame.add_annotation(id=ann_id, box2d_xywh=(100, 100, 100, 100), labels=(label,))
allegro_frame.add_annotation(id=ann_id, box2d_xywh=(100, 100, 100, 100), labels=(label,))

allegro_frames: List[SingleFrame] = [
    allegro_frame
]

dataset_name = r&quot;clml_test_dataset&quot;
version_name = r&quot;clml_test_version&quot;
dataset = Dataset.create(dataset_name=dataset_name)
version = DatasetVersion.create_version(dataset_name=dataset_name, version_name=version_name)
version.add_frames(allegro_frames)
</code></pre>
<p>What's the correct way to make only one annotation be uploaded for the frame?</p>",0,0,2022-09-14 12:24:40.063000 UTC,,,0,python|clearml,5,2011-08-25 22:58:29.233000 UTC,2022-09-24 23:30:23.147000 UTC,"Technion, Israel",18777,2376,137,2000,,,,,,['clearml']
How to add new stages/rename a stage in MLflow,"<p>I was doing some experiments with MLflow using Python 3.7, and I was wondering if I can rename a stage or add a new one to the pre-existing</p>
<blockquote>
<p>None | Staging | Production | Archived</p>
</blockquote>
<p>Currently, I registered a model obtained from an experiment, but I would like to add multiple developing phases. I read the <a href=""https://mlflow.org/docs/latest/model-registry.html#concepts"" rel=""nofollow noreferrer"">docs</a> concerning that part, but the only thing I've found is the following:</p>
<blockquote>
<p>Each distinct model version can be assigned one stage at any given
time. MLflow provides predefined stages for common use-cases such as
Staging, Production or Archived. You can transition a model version
from one stage to another stage.</p>
</blockquote>",1,1,2022-01-17 13:32:13.720000 UTC,,,1,python|python-3.x|machine-learning|mlflow,212,2021-04-28 08:36:26.397000 UTC,2022-09-23 07:50:30.617000 UTC,,41,12,0,4,,,,,,['mlflow']
How to trigger AWS Sagemaker training job from a front end application (via api gateway post call)?,"<p>I have made a training job on AWS Sagemaker and it runs well - reads from an s3 location and stores model checkpoints as intended in s3. Now, I need to trigger this trigger job with specified parameters (s3 location having data for eg.) from a website
(via API gateway). The very first idea was to make a lambda function that gets called from an API call and it training job using the Sagemaker API:</p>
<pre><code>HuggingFace(entry_point='train.py',
                            source_dir='./scripts',
                            instance_type='ml.p3.2xlarge',
                            instance_count=1,
                            role=role,
                            transformers_version='4.6',
                            pytorch_version='1.7',
                            py_version='py36',
                            hyperparameters = hyperparameters)

# staarting the train job with our uploaded datasets as input
huggingface_estimator.fit({'train': training_input_path, 'test': test_input_path})
</code></pre>
<p>But, AWS lambda has a max runtime of 15 mins which is less than the training time required. I was wondering if there is a serverless way of doing the same thing? Is AWS step function any different from lambda in this regard?</p>",1,4,2022-01-30 02:27:49.630000 UTC,,,0,amazon-web-services|aws-lambda|amazon-sagemaker|huggingface-transformers|mlops,349,2018-12-26 18:41:29.957000 UTC,2022-09-25 01:49:20.550000 UTC,"Camden, NJ, USA",1,0,0,1,,,,,,['amazon-sagemaker']
ERROR:root:Line magic function `%azureml` not found?,"<p>I have created a ""Blank Jupyter Notebook"" project in Azure ML Workbench. When I try to run the Sample notebook found in the project, I get this error message:</p>

<pre><code>ERROR:root:Line magic function `%azureml` not found.
</code></pre>

<p>What is missing?</p>",1,1,2018-01-30 05:18:01.150000 UTC,,,0,azure-machine-learning-workbench,259,2016-07-21 15:10:29.743000 UTC,2022-09-21 20:32:07.613000 UTC,,2166,44,1,151,,,,,,['azure-machine-learning-workbench']
AWS Sagemaker DeepAR Validation Error Additional Properties not allowed ('training' was unexpected),"<p>I don't know what the issue is. Here is the code:</p>
<pre><code>estimator = sagemaker.estimator.Estimator(
    image_uri=image_name,
    sagemaker_session=sagemaker_session,
    role=role,
    train_instance_count=1,
    train_instance_type=&quot;ml.m5.large&quot;,
    base_job_name=&quot;deepar-stock&quot;,
    output_path=s3_output_path,
)

hyperparameters = {
    &quot;time_freq&quot;: &quot;24H&quot;,
    &quot;epochs&quot;: &quot;100&quot;,
    &quot;early_stopping_patience&quot;: &quot;10&quot;,
    &quot;mini_batch_size&quot;: &quot;64&quot;,
    &quot;learning_rate&quot;: &quot;5E-4&quot;,
    &quot;context_length&quot;: str(context_length),
    &quot;prediction_length&quot;: str(prediction_length),
    &quot;likelihood&quot;: &quot;gaussian&quot;,
}

estimator.set_hyperparameters(**hyperparameters)

%%time

estimator.fit(inputs=f&quot;{s3_data_path}/train/&quot;)
</code></pre>
<p>And when I try to train the model I get the following error (in its entirety).</p>
<pre><code>------------------------------------------------------------------------

---
UnexpectedStatusException                 Traceback (most recent call last)
&lt;timed eval&gt; in &lt;module&gt;

/opt/conda/lib/python3.7/site-packages/sagemaker/estimator.py in fit(self, inputs, wait, logs, job_name, experiment_config)
    681         self.jobs.append(self.latest_training_job)
    682         if wait:
--&gt; 683             self.latest_training_job.wait(logs=logs)
    684 
    685     def _compilation_job_name(self):

/opt/conda/lib/python3.7/site-packages/sagemaker/estimator.py in wait(self, logs)
   1626         # If logs are requested, call logs_for_jobs.
   1627         if logs != &quot;None&quot;:
-&gt; 1628             self.sagemaker_session.logs_for_job(self.job_name, wait=True, log_type=logs)
   1629         else:
   1630             self.sagemaker_session.wait_for_job(self.job_name)

/opt/conda/lib/python3.7/site-packages/sagemaker/session.py in logs_for_job(self, job_name, wait, poll, log_type)
   3658 
   3659         if wait:
-&gt; 3660             self._check_job_status(job_name, description, &quot;TrainingJobStatus&quot;)
   3661             if dot:
   3662                 print()

/opt/conda/lib/python3.7/site-packages/sagemaker/session.py in _check_job_status(self, job, desc, status_key_name)
   3218                 ),
   3219                 allowed_statuses=[&quot;Completed&quot;, &quot;Stopped&quot;],
-&gt; 3220                 actual_status=status,
   3221             )
   3222 
UnexpectedStatusException: Error for Training job deepar-2021-07-31-22-25-54-110: Failed. Reason: ClientError: Unable to initialize the algorithm. Failed to validate input data configuration. (caused by ValidationError)

Caused by: Additional properties are not allowed ('training' was unexpected)

Failed validating 'additionalProperties' in schema:
    {'$schema': 'http://json-schema.org/draft-04/schema#',
     'additionalProperties': False,
     'anyOf': [{'required': ['train']}, {'required': ['state']}],
     'definitions': {'data_channel': {'properties': {'ContentType': {'enum': ['json',
                                                                              'json.gz',
                                                                              'parquet',
                                                                              'auto'],
                                                                     'type': 'string'},
                                                     'RecordWrapperType': {'enum': ['None'],

On instance:
    {'training': {'RecordWrapperType': 'None',
                  'S3DistributionType': 'FullyReplicated',
                  'TrainingInputMode': 'File'}}
</code></pre>
<p>Here it says <code>'training' was unexpected</code>. I don't know why it says <code>'training'</code> on that last line <code>On instance:</code>. I don't know how to solve this. I've looked at other pages for help but I can't find a straight answer. I know that my data is structured right. The errors seem to be with the hyperparameters but I don't know that for sure. Please help!</p>",2,0,2021-07-31 22:39:30.133000 UTC,,2021-07-31 22:45:14.767000 UTC,1,python|amazon-sagemaker|deepar,157,2014-07-17 15:08:26.593000 UTC,2022-08-10 15:49:39.013000 UTC,"Michigan, United States",3113,145,5,317,,,,,,['amazon-sagemaker']
Import from DocumentDB to Azure Machine Learning - DateTime with milliseconds,"<p>I use the <strong>Import</strong> module from <strong>Azure Machine Learning (Azure ML)</strong> to get data from <strong>DocumentDB</strong>.</p>
<p>The import works fine.</p>
<hr />
<p>In the <strong>DocumentDB</strong> <strong>documents</strong> are <code>DateTime</code> with <code>milliseconds</code>, like:</p>
<blockquote>
<p>&quot;CurrentTime&quot;: &quot;2017-07-17T20:18:55.757316&quot;</p>
</blockquote>
<p>and in <strong>Azure ML</strong> it is recognized as <code>DateTime-Feature</code> and it is shown like this:</p>
<blockquote>
<p>2017-07-17T20:18:55</p>
</blockquote>
<p>The problem is, that the <code>milliseconds</code> are missing.</p>
<p>I have tried to get the <code>milliseconds</code> with the <strong>Edit Metadata</strong> module, but it doesn't work for me. Additionally, I have tried to use <strong>R</strong> to convert the <code>CurrentTime</code> to <strong>numeric</strong>, like this:</p>
<pre><code>time.milliseconds = as.numeric(dataset1[['CurrentTime']]);
print(time.milliseconds,digits=15)
</code></pre>
<p>But the <code>milliseconds</code> are still missing.</p>
<hr />
<p>How can I get the <strong>whole</strong> <code>DateTime</code> with <code>milliseconds</code>?</p>
<hr />
<p><strong>UPDATE:</strong></p>
<p>The complete <strong>R</strong> code:</p>
<pre><code>options(&quot;digits.secs&quot;=6)

dataset1 &lt;- maml.mapInputPort(1) # class: data.frame

time.milliseconds = as.numeric(dataset1[['CurrentTime']]);
dataset &lt;- cbind(dataset1, time.milliseconds)

maml.mapOutputPort(&quot;dataset&quot;);
</code></pre>
<p>The result:</p>
<p><a href=""https://i.stack.imgur.com/YGQOB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YGQOB.png"" alt=""enter image description here"" /></a></p>
<hr />
<p>My suggestion is, that the <strong>Import</strong> module with <strong>DocumentDB</strong> doesn't support <code>milliseconds</code>!?</p>
<hr />
<p>For now, I upload my <code>CurrentTime</code> as <code>milliseconds</code> from epoch to the <strong>DocumentDB</strong>. I hope there is a better way..</p>",1,5,2017-07-18 12:16:55.003000 UTC,,2020-06-20 09:12:55.060000 UTC,1,r|azure|datetime|azure-machine-learning-studio,48,2015-06-27 14:03:09.910000 UTC,2022-06-10 14:45:57.643000 UTC,"Leer (Ostfriesland), Deutschland",3864,2761,15,278,,,,,,['azure-machine-learning-studio']
Azure ML Designer - Unable to connect dataset (any directory type) to clean missing data module (dataframedirectory type) in designer,"<p>Unable to connect dataset (any directory type) to clean missing data module (dataframedirectory type) in designer. Please advise. Screenshot of trying to connect is below where the clean missing module connection point is not highlighted to connect both modules, which i am assuming is because of type mismatch. Dataset output is of type ""AnyDirectory"" type where as clean missing module type is ""dataframedirectory"". How to cast in designer?</p>

<p><a href=""https://i.stack.imgur.com/Qs3uV.png"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/Qs3uV.png</a></p>",1,0,2020-04-28 12:41:31.757000 UTC,,,3,azure-machine-learning-service,241,2014-04-15 12:18:57.880000 UTC,2021-05-11 16:14:42.530000 UTC,,41,0,0,3,,,,,,['azure-machine-learning-service']
how to registered the log_model in MLflow?,"<p>I have tried to load the deep learning model on mlflow, it's perfectly loaded, but the model is not stored in the model registry, can any one guide me how to register the model and its dataset for inference?<br />
Thanks</p>
<p>from mlflow import MlflowClient
experiment_name = &quot;nlp_model&quot;</p>
<pre><code>try:
    exp_id = mlflow.create_experiment(name=experiment_name) # set the experiment id 
except Exception as e:
    exp_id = mlflow.get_experiment_by_name(experiment_name).experiment_id

with mlflow.start_run(experiment_id=exp_id):
    run_id = mlflow.active_run().info.run_id
    print(run_id)
    mlflow.sklearn.autolog(log_models=True)
    print(mlflow.tracking.get_tracking_uri())
    model = Demucs(**args.demucs, sample_rate=args.sample_rate)# fitting the model
    &quot;&quot;&quot;
    loaded model on mlflow
    &quot;&quot;&quot;
    mlflow.sklearn.log_model(model, &quot;nlp_model&quot;)
    &quot;&quot;&quot;
    saved model on mlflow model registory
    &quot;&quot;&quot;
    client = MlflowClient()
    model = client.create_model_version(
        name=&quot;denoiser_nlp_model&quot;,
        source=f&quot;./mlruns/{exp_id}/{run_id}/artifacts/nlp_model&quot;,
        run_id=mlflow.active_run().info.run_id
    )
</code></pre>
<p>#Here is the error msg
[1]: <a href=""https://i.stack.imgur.com/LNAsc.png"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/LNAsc.png</a></p>",0,0,2022-09-20 18:20:39.243000 UTC,,,0,deep-learning|nlp|pytorch|mlflow,12,2020-04-02 15:17:33.983000 UTC,2022-09-23 11:59:36.240000 UTC,"Lahore, Pakistan",1,0,0,0,,,,,,['mlflow']
Error while loading MLFlow model in python 3.7,"<p>I am saving the MLFlow model using databricks. Below are the details:</p>
<pre><code>artifact_path: model
databricks_runtime: 8.4.x-gpu-ml-scala2.12
flavors:
  python_function:
    data: data
    env: conda.yaml
    loader_module: mlflow.pytorch
    pickle_module_name: mlflow.pytorch.pickle_module
    python_version: 3.8.8
  pytorch:
    model_data: data
    pytorch_version: 1.9.0+cu102
</code></pre>
<p><strong>I am not able to locally load the model using Python 3.7</strong>, whereas it works well with Python 3.9.</p>
<p>Any idea what could be the possible resolution to this?</p>
<p>Error Trace:</p>
<pre><code>  File &quot;/Users/danishm/opt/miniconda3/envs/py37/lib/python3.7/site-packages/mlflow/pytorch/__init__.py&quot;, line 714, in load_model
    return _load_model(path=torch_model_artifacts_path, **kwargs)
  File &quot;/Users/danishm/opt/miniconda3/envs/py37/lib/python3.7/site-packages/mlflow/pytorch/__init__.py&quot;, line 626, in _load_model
    return torch.load(model_path, **kwargs)
  File &quot;/Users/danishm/opt/miniconda3/envs/py37/lib/python3.7/site-packages/torch/serialization.py&quot;, line 607, in load
    return _load(opened_zipfile, map_location, pickle_module, **pickle_load_args)
  File &quot;/Users/danishm/opt/miniconda3/envs/py37/lib/python3.7/site-packages/torch/serialization.py&quot;, line 882, in _load
    result = unpickler.load()
TypeError: code() takes at most 15 arguments (16 given)
</code></pre>
<p>I have tried specifying the pickle module name explicitly as <code>mlflow.pytorch.load_model(ML_MODEL_PATH,pickle_module=mlflow.pytorch.pickle_module)</code></p>
<p>But still getting the same error.</p>",0,2,2021-11-19 06:53:12.843000 UTC,,,0,python-3.x|pickle|torch|mlflow,477,2021-11-19 06:39:10.900000 UTC,2022-09-19 10:56:54.760000 UTC,,1,0,0,2,,,,,,['mlflow']
Writing preprocessed output CSV to S3 from Scikit Learn image on Sagemaker,"<p>My problem: writing out a CSV file to S3 from inside a Sagemaker SKLearn image. I know how to write CSVs to S3 from a notebook - that is working fine. It's within the docker image that I'm unable to get it to work.</p>
<p>This is a preprocessing.py script called as an entry_point parameter to the SKLearn estimator. The purpose is to pre-process the data prior to running an inference. It's the first step in my inference pipeline.</p>
<p><strong>Everything is working as expected in my preprocessing script, except outputting the file at the end</strong>.</p>
<ol>
<li>Attempt #1 - this <strong>produces a CSV file that has strange binary-looking data at the beginning and end of the file</strong> (before the first cell and after the last cell of the CSV). It's almost a valid CSV but not quite. <em>See the image at the end.</em></li>
</ol>
<pre><code>def write_joblib(file, path):
    s3_bucket, s3_key = path.split('/')[2], path.split('/')[3:]
    s3_key = '/'.join(s3_key)
    with BytesIO() as f:
        joblib.dump(file, f)
        f.seek(0)
        boto3.client(&quot;s3&quot;).upload_fileobj(Bucket=s3_bucket, Key=s3_key, Fileobj=f)
    
predictors_csv = predictors_df.to_csv(index = False)
write_joblib(predictors_csv, predictors_s3_uri)
</code></pre>
<ol start=""2"">
<li>Attempt #2 - I used StringIO rather than BytesIO. However, this produced a zero-byte file on S3.</li>
<li>Attempt #3 - I tried boto3.client('s3').put_object(...) but got ClientError: An error occurred (AccessDenied) when calling the PutObject operation: Access Denied</li>
</ol>
<p>I believe I am almost there with Attempt #1 above. I assume it's an encoding issue. If I can fix the structure of the CSV file to remove the non-text characters at the start it will be working. A screenshot of the CSV in a Notepad++ is below.</p>
<p><a href=""https://i.stack.imgur.com/nXHMx.png"" rel=""nofollow noreferrer"">Notice the non-character text at the start of the CSV file below</a></p>",1,0,2020-12-26 02:09:04.647000 UTC,,,0,amazon-s3|amazon-sagemaker,207,2020-12-26 00:56:36.687000 UTC,2021-11-24 11:16:14.577000 UTC,,1,0,0,4,,,,,,['amazon-sagemaker']
Sales prediction in Azure ML,"<p>I am very new to Azure Machine Learning things, one of our client use to sell some fresh products to business people. They have a 'suggested buy' system, a feature will suggest some quantities to buy based on customer's sales history.</p>

<p>After client came to know about Microsoft's Azure ML, they want to use that prediction system to suggest quantities to customers.</p>

<p>We have sales data with these columns,</p>

<ul>
<li>CustomerName </li>
<li>ItemName </li>
<li>OrderDate </li>
<li>QuantityPurchased </li>
<li>QuantitySold</li>
</ul>

<p>We would like customers have suggested quantity should come from Azure ML using the Sales Data.</p>

<p>Can some one please suggest me how can I do this?</p>

<p>Thanks much in advance.</p>",1,1,2016-04-11 11:33:27.933000 UTC,1.0,,1,azure|prediction|azure-machine-learning-studio,2649,2014-08-25 04:55:03.180000 UTC,2021-06-03 01:18:34.777000 UTC,Online,722,35,5,178,,,,,,['azure-machine-learning-studio']
How to invoke the Iris endpoint in the sample-notebooks for Amazon SageMaker using SDK,"<p>I'm trying to invoke the endpoint from the <code>tensorflow_iris_dnn_classifier_using_estimators</code> provided in the sample notesbooks. In the sample, it invokes the endpoint using the same python object generated in the deployment process. In a large system, I need to know how to invoke this endpoint without this object and possibly in different languages. This is what I've tried:</p>

<pre><code>import struct
import boto3

client = boto3.client('sagemaker-runtime')

query = [6.4, 3.2, 4.5, 1.5]
buf = struct.pack('%sf' % len(query), *query)

response = client.invoke_endpoint(
    EndpointName='sagemaker-tensorflow-py2-cpu-2018-01-16-18-22-54-458',
    Body=buf
)
</code></pre>

<p>What are I doing wrong? I get the following error from cloudwatch:</p>

<pre><code>[2018-01-16 19:51:21,091] ERROR in serving: 'utf8' codec can't decode byte 0xcd in position 0: invalid continuation byte
2018-01-16 19:51:21,091 ERROR - model server - 'utf8' codec can't decode byte 0xcd in position 0: invalid continuation byte
10.32.0.2 - - [16/Jan/2018:19:51:21 +0000] ""POST /invocations HTTP/1.1"" 500 0 ""-"" ""AHC/2.0""
</code></pre>",2,0,2018-01-16 19:53:20.243000 UTC,,,0,python|amazon-sagemaker,764,2015-09-25 17:16:18.360000 UTC,2022-05-26 17:59:10.970000 UTC,,749,11,0,49,,,,,,['amazon-sagemaker']
Specify S3 location for model output to sagemaker training job duplication issue,"<p>Im a using a SageMaker training job to train an ML model, and I am attempting to output the model to a specific location on S3.</p>

<p>Code:</p>

<pre><code>model_uri = ""s3://***/model/""
script_path = 'entry_point.py'
sklearn = SKLearn(
    entry_point=script_path,
    train_instance_type=""ml.m5.large"",
    output_path=model_uri,
    role='***',
    sagemaker_session=sagemaker_session)
</code></pre>

<p>The issue I am having is that the training job will save the model <strong>twice</strong>. Once in the S3 bucket at the top level, and once in the folder specified (<code>/model</code>).</p>

<p>Top level:
<a href=""https://i.stack.imgur.com/HbSsX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HbSsX.png"" alt=""enter image description here""></a></p>

<p>Model folder:
<a href=""https://i.stack.imgur.com/59vYA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/59vYA.png"" alt=""enter image description here""></a></p>

<p>Is this expected behaviour when specifying <code>output_path</code> in the estimator? Is there a way to stop it?</p>

<p>Any help would be appreciated!</p>",1,0,2019-08-12 14:20:50.897000 UTC,3.0,2019-08-12 14:59:12.967000 UTC,0,python|amazon-s3|amazon-sagemaker,1168,2015-09-22 14:32:49.407000 UTC,2022-09-04 14:53:27.090000 UTC,Belfast,1682,165,4,164,,,,,,['amazon-sagemaker']
SageMaker Image Classification: How to get an ordered list of classes corresponding to the output of the model,"<p>I'm training a model for multi-class image classification on AWS sagemaker using a custom dataset. The dataset has around 50 classes. I'm following this notebook: <a href=""https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/imageclassification_caltech/Image-classification-transfer-learning-highlevel.ipynb"" rel=""nofollow noreferrer"">Image classification transfer learning demo</a> </p>

<p>According to my understanding, the final layer of the model outputs probabilities corresponding to each class in our dataset. Sagemaker expects the dataset to be provided in mxnet recordio's .rec format. Since I'm not manually converting the labels to one-hot-encoded, I don't know which layer is ouputing probabilities for which class. How can I get an ordered list of classes where indexes corresponds to the output of final layer of the model.</p>

<p>Even the notebook provided by AWS (Link above) has that ordered list (list: object_categories) hard-coded.</p>

<p>My dataset before converting to .rec format looks like this:</p>

<pre><code>./train/object1/
   -image1.jpg
   -image2.jpg
   -image3.jpg
   -...image500.jpg
./train/object2/
   -image1.jpg
   -image2.jpg
   -image3.jpg
   -...image500.jpg
.
.
.
./train/object50/
   -image1.jpg
   -image2.jpg
   -image3.jpg
   -...image500.jpg
</code></pre>

<p>Any help will be highly appreciated.</p>",2,0,2018-12-28 17:37:31.347000 UTC,,,3,python|deep-learning|mxnet|multilabel-classification|amazon-sagemaker,493,2018-02-27 19:56:27.610000 UTC,2022-09-23 14:11:18.503000 UTC,,429,69,7,41,,,,,,['amazon-sagemaker']
Is there an option for log models for every k epochs in MLflow autolog?,"<p>I'm wondering if there is an option to log models for every k epoch in MLFlow <strong>autolog</strong>?
When I used <code>mlflow.pytorch.autolog()</code>, the logged model is only one and I'm guessing that would be the one of the last epoch.</p>
<p>Or should I do it manually after every k epoch using <code>mlflow.log_model</code>?</p>
<p><a href=""https://i.stack.imgur.com/dbVgG.png"" rel=""nofollow noreferrer"">example</a></p>",1,0,2022-03-29 09:30:28.073000 UTC,,,0,pytorch|mlflow,216,2022-03-29 09:12:49.970000 UTC,2022-08-18 22:04:48.593000 UTC,,1,0,0,3,,,,,,['mlflow']
how to log hydra's multi-run in mlflow,"<p>I am trying to manage the results of machine learning with mlflow and hydra.
So I tried to run it using the multi-run feature of hydra.
I used the following code as a test.</p>
<pre><code>import mlflow
import hydra
from hydra import utils
from pathlib import Path
import time


@hydra.main('config.yaml')
def main(cfg):
    print(cfg)


    mlflow.set_tracking_uri('file://' + utils.get_original_cwd() + '/mlruns')
    mlflow.set_experiment(cfg.experiment_name)


    mlflow.log_param('param1',5)
    # mlflow.log_param('param1',5)
    # mlflow.log_param('param1',5)

    with mlflow.start_run() :
        mlflow.log_artifact(Path.cwd() / '.hydra/config.yaml')


if __name__ == '__main__':
    main()
</code></pre>
<p>This code will not work.
I got the following error</p>
<pre><code>Exception: Run with UUID [RUNID] is already active. To start a new run, first end the current run with mlflow.end_run(). To start a nested run, call start_run with nested=True
</code></pre>
<p>So I modified the code as follows</p>
<pre><code>import mlflow
import hydra
from hydra import utils
from pathlib import Path
import time


@hydra.main('config.yaml')
def main(cfg):
    print(cfg)


    mlflow.set_tracking_uri('file://' + utils.get_original_cwd() + '/mlruns')
    mlflow.set_experiment(cfg.experiment_name)


    mlflow.log_param('param1',5)
    # mlflow.log_param('param1',5)
    # mlflow.log_param('param1',5)

    with mlflow.start_run(nested=True) :
        mlflow.log_artifact(Path.cwd() / '.hydra/config.yaml')


if __name__ == '__main__':
    main()

</code></pre>
<p>This code works, but the artifact is not saved.
The following corrections were made to save the artifacts.</p>
<pre><code>import mlflow
import hydra
from hydra import utils
from pathlib import Path
import time


@hydra.main('config.yaml')
def main(cfg):
    print(cfg)


    mlflow.set_tracking_uri('file://' + utils.get_original_cwd() + '/mlruns')
    mlflow.set_experiment(cfg.experiment_name)


    mlflow.log_param('param1',5)
    # mlflow.log_param('param1',5)
    # mlflow.log_param('param1',5)

    
    mlflow.log_artifact(Path.cwd() / '.hydra/config.yaml')


if __name__ == '__main__':
    main()
</code></pre>
<p>As a result, the artifacts are now saved.
However, when I run the following command</p>
<pre><code>python test.py model=A,B hidden=12,212,31 -m
</code></pre>
<p>Only the artifact of the last execution condition was saved.</p>
<p>How can I modify mlflow to manage the parameters of the experiment by taking advantage of the multirun feature of hydra?</p>",3,0,2020-06-25 20:30:55.417000 UTC,,2020-06-26 07:08:01.413000 UTC,1,python|machine-learning|data-science|mlflow|fb-hydra,1109,2019-03-21 12:30:34.140000 UTC,2022-09-23 12:41:46.963000 UTC,Japan,581,26,2,37,,,,,,['mlflow']
cannot deploy YAMNet model to SageMaker,"<p>I followed <a href=""https://www.tensorflow.org/tutorials/audio/transfer_learning_audio"" rel=""nofollow noreferrer"">this tutorial</a> and had the model fine-tuned.</p>
<p>the model-saving part of serving model is like this:</p>
<pre><code>saved_model_path = 'dogs_and_cats_yamnet/yamnet-model/00000001'

input_segment = tf.keras.layers.Input(shape=(), dtype=tf.float32, name='audio')
embedding_extraction_layer = hub.KerasLayer(yamnet_model_handle,
                                            trainable=False, name='yamnet')
_, embeddings_output, _ = embedding_extraction_layer(input_segment)
serving_outputs = my_model(embeddings_output)
serving_outputs = ReduceMeanLayer(axis=0, name='classifier')(serving_outputs)
serving_model = tf.keras.Model(input_segment, serving_outputs)
serving_model.save(saved_model_path, include_optimizer=False)
</code></pre>
<p>Then followed <a href=""https://github.com/aws/amazon-sagemaker-examples/blob/main/sagemaker-python-sdk/tensorflow_serving_container/tensorflow_serving_container.ipynb"" rel=""nofollow noreferrer"">this page</a>, uploading the model to S3 and deploying the model.</p>
<pre><code>!tar -C &quot;$PWD&quot; -czf dogs_and_cats_yamnet.tar.gz dogs_and_cats_yamnet/
model_data = Session().upload_data(path=&quot;dogs_and_cats_yamnet.tar.gz&quot;, key_prefix=&quot;model&quot;)
model = TensorFlowModel(model_data=model_data, role=sagemaker_role, framework_version=&quot;2.3&quot;)
predictor = model.deploy(initial_instance_count=1, instance_type=&quot;ml.c5.xlarge&quot;)
</code></pre>
<p>Deployment seems successful, but when I try to do inference,</p>
<pre><code>waveform = np.zeros((3*48000), dtype=np.float32)
result = predictor.predict(waveform)
</code></pre>
<p>the following error occurs.</p>
<pre><code>ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from primary with message &quot;{
    &quot;error&quot;: &quot;The first dimension of paddings must be the rank of inputs[1,2] [1,144000]\n\t [[{{node yamnet_frames/tf_op_layer_Pad/Pad}}]]&quot;
</code></pre>
<p>I have no idea why this happens. I am struggling with it for hours and coming up with no clue.
YAMNet works fine when I pulled the model from tf hub directly and take inference with it.
This is kind of a minor question I guess, but I would appreciate any helpful answers.
Thank you in advance.</p>",0,0,2022-03-18 10:10:14.527000 UTC,,,0,tensorflow|amazon-sagemaker,45,2016-09-29 00:32:31.950000 UTC,2022-05-17 13:21:20.043000 UTC,,43,4,0,18,,,,,,['amazon-sagemaker']
How do you use ADF PipelineParameters with ML studio to change output location instead of input location,"<p>I want to parameterise my ML studio pipeline such that it outputs to a different blob store depending on which ADF environment it is being run from - the dev or prod ADF instance. This is so that the data engineers can have an output on their dev blob storage so they can avoid developing in live, while still using the exact same ML Studio pipeline as would be used in live.</p>
<p>I have followed the instructions in <a href=""https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-showcasing-datapath-and-pipelineparameter.ipynb"" rel=""nofollow noreferrer"">this notebook</a> to change the input data source on blob dynamically using PipelineParameters.</p>
<p>Can I / how do I do the same for output locations?</p>
<p>Below is the code I used. I have used ./outputs as the output folder. I want to change this to a location in blob storage dynamically set via a PipelineParameter that can be called from ADF. (Or another way if you have other suggestions!)</p>
<p>Thanks in advance for any suggestions!</p>
<pre class=""lang-python prettyprint-override""><code>
from azureml.data.datapath import DataPath, DataPathComputeBinding
from azureml.pipeline.core import PipelineParameter

default_data_path = DataPath(
    datastore=Datastore(workspace, &quot;prod_data_science&quot;), 
    path_on_datastore='path/to/data/')

input_datapath_pipeline_param = PipelineParameter(name=&quot;input_datapath&quot;, default_value=default_data_path)

input_data_consumption = (
    input_datapath_pipeline_param, 
    DataPathComputeBinding(
        mode='mount',
        overwrite = True
    )
)

toy_step = PythonScriptStep(
    script_name=&quot;toy_code.py&quot;,
    source_directory=&quot;./&quot;,
    arguments=[
        '--input_folder', input_data_consumption,
        '--output_folder', './outputs',
    ],
    inputs=[input_data_consumption],
    runconfig=runconfig,
)
</code></pre>",0,3,2021-07-19 08:24:34.713000 UTC,,2021-07-20 08:35:12.297000 UTC,2,python|azure-data-factory|azure-machine-learning-service,127,2021-07-15 13:16:19.493000 UTC,2022-09-23 10:51:43.910000 UTC,,21,0,0,2,,,,,,['azure-machine-learning-service']
S3 Bucket cannot be reached in GroundTruth Labeling,"<p>I am currently using GroundTruth Labeling, and have created a manifest file successfully. Right now, I am encountering the error &quot;NetworkingError: Network Failure - The S3 bucket you entered in Input dataset location cannot be reached. Either the bucket does not exist, or you do not have permission to access it. If the bucket does not exist, update Input dataset location with a new S3 URI. If the bucket exists, give the IAM entity you are using to create this labeling job permission to read and write to this S3 bucket, and try your request again.&quot;</p>
<p>I have made the contents of the S3 bucket and the bucket itself public to everybody, so I was wondering why this error occurs.</p>",0,2,2022-04-13 08:11:16.553000 UTC,,,0,amazon-web-services|amazon-s3|amazon-sagemaker|amazon-ground-truth,154,2020-07-27 06:32:35.067000 UTC,2022-09-23 19:50:51.023000 UTC,,37,26,0,27,,,,,,['amazon-sagemaker']
trying to use monotonicty constraints in XBGoost,"<p>I am using Sagemaker's XGBoost as a built-in algorithm and code along those lines (assuming for simplicity that I have <strong>3</strong> independent variables/feature/predictors):</p>
<pre><code>hyperparameters = {
        'max_depth': '10',
        'num_round': '100',
        'objective': 'count:poisson',
        'tree_method': 'exact', 
        'monotone_constraints': '(0,-1,0)'
}

output_path = 's3://{}/{}/output'.format(s3_bucket_name, s3_prefix)

estimator = sagemaker.estimator.Estimator(image_uri=sagemaker.image_uris.retrieve(&quot;xgboost&quot;, region_name, &quot;1.5-1&quot;), 
                                          hyperparameters=hyperparameters,
                                          role=role_arn,
                                          instance_count=1, 
                                          instance_type='ml.m5.2xlarge',
                                          #instance_type='local', 
                                          volume_size=1, # 1 GB 
                                          output_path=output_path)

estimator.fit({'train': s3_input_train, 'validation': s3_input_val})
</code></pre>
<p>Unfortunately, I get:</p>
<pre><code>Traceback (most recent call last):
  File &quot;c:\Repos\ds_cs_ipt2\modeling\fit_occupancy_model_in_sagemaker.py&quot;, line 67, in &lt;module&gt;
    estimator.fit({'train': s3_input_train, 'validation': s3_input_val})
  File &quot;C:\Python\Python310\lib\site-packages\sagemaker\estimator.py&quot;, line 956, in fit
    self.latest_training_job.wait(logs=logs)
  File &quot;C:\Python\Python310\lib\site-packages\sagemaker\estimator.py&quot;, line 1957, in wait
    self.sagemaker_session.logs_for_job(self.job_name, wait=True, log_type=logs)
  File &quot;C:\Python\Python310\lib\site-packages\sagemaker\session.py&quot;, line 3798, in logs_for_job
    self._check_job_status(job_name, description, &quot;TrainingJobStatus&quot;)
  File &quot;C:\Python\Python310\lib\site-packages\sagemaker\session.py&quot;, line 3336, in _check_job_status
    raise exceptions.UnexpectedStatusException(
sagemaker.exceptions.UnexpectedStatusException: Error for Training job sagemaker-xgboost-2022-05-25-12-32-37-624: Failed. Reason: AlgorithmError: framework error:
Traceback (most recent call last):
  File &quot;/miniconda3/lib/python3.7/site-packages/sagemaker_xgboost_container/algorithm_mode/train.py&quot;, line 233, in train_job
    feval=configured_feval, callbacks=callbacks, xgb_model=xgb_model, verbose_eval=False)
  File &quot;/miniconda3/lib/python3.7/site-packages/xgboost/training.py&quot;, line 196, in train
    early_stopping_rounds=early_stopping_rounds)
  File &quot;/miniconda3/lib/python3.7/site-packages/xgboost/training.py&quot;, line 51, in _train_internal
    bst = Booster(params, [dtrain] + [d[0] for d in evals])
  File &quot;/miniconda3/lib/python3.7/site-packages/xgboost/core.py&quot;, line 1334, in __init__
    params = self._configure_constraints(params)
  File &quot;/miniconda3/lib/python3.7/site-packages/xgboost/core.py&quot;, line 1400, in _configure_constraints
    ] = self._transform_monotone_constrains(value)
  File &quot;/miniconda3/lib/python3.7/site-packages/xgboost/core.py&quot;, line 1361, in _transform_monotone_constrains
    constrained_features = set(value.keys
</code></pre>
<p>which must be due to the XGBoost source code <a href=""https://github.com/dmlc/xgboost/blob/master/python-package/xgboost/core.py"" rel=""nofollow noreferrer"">here</a> and this bit:</p>
<pre><code>def _transform_monotone_constrains(
        self, value: Union[Dict[str, int], str, Tuple[int, ...]]
    ) -&gt; Union[Tuple[int, ...], str]:
        if isinstance(value, str):
            return value
        if isinstance(value, tuple):
            return value

        constrained_features = set(value.keys())
        feature_names = self.feature_names or []
        if not constrained_features.issubset(set(feature_names)):
            raise ValueError(
                &quot;Constrained features are not a subset of training data feature names&quot;
            )

        return tuple(value.get(name, 0) for name in feature_names)
</code></pre>
<p>Alas I do not fully understand what the issue may be. Here are some more details from cloud watch:</p>
<p><a href=""https://i.stack.imgur.com/3Enwp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3Enwp.png"" alt=""enter image description here"" /></a></p>",0,0,2022-05-25 13:01:03.617000 UTC,,,0,python|xgboost|amazon-sagemaker,57,2010-03-01 10:53:04.443000 UTC,2022-09-24 18:56:19.313000 UTC,Somewhere,15705,2171,91,2150,,,,,,['amazon-sagemaker']
How can an mlflow model be scaled to serve more requests?,<p>I would like to have multiple instances of my MLFlow model running in parallel but hidden behind a common the same endpoint/port so it's not visible to the user. </p>,1,0,2020-04-17 07:51:03.510000 UTC,,,0,multithreading|gunicorn|mlflow,228,2020-01-14 16:04:32.663000 UTC,2021-11-19 17:01:44.770000 UTC,,1,0,0,8,,,,,,['mlflow']
"How to fix aws region error ""ValueError: Must setup local AWS configuration with a region supported by SageMaker""","<p>I am running sagemaker for the first time from my laptop. When I try to start the session I get this error
ValueError: Must setup local AWS configuration with a region supported by SageMaker</p>

<p>Local config is set to eu-west-1 which is supported by Sagemaker.</p>

<p>I changed the region to us-west-2 and back and nothing changed. Of course I restarted the notebook kernel after each change just in case.</p>

<pre><code>import boto3

import re

import os

import numpy as np

import pandas as pd

import sagemaker as sage

boto_session = boto3.Session(profile_name=""bennu"")

session = sage.Session(boto_session=boto_session) #this is where the error appears
</code></pre>

<p>I expect the session to start and to move on to the next step. The full notebook is here <a href=""https://github.com/PacktPublishing/Hands-On-Machine-Learning-Using-Amazon-SageMaker-v-/blob/master/section_1/train_and_deploy_your_first_model_on_sagemaker.ipynb"" rel=""noreferrer"">https://github.com/PacktPublishing/Hands-On-Machine-Learning-Using-Amazon-SageMaker-v-/blob/master/section_1/train_and_deploy_your_first_model_on_sagemaker.ipynb</a></p>",3,2,2019-04-26 14:24:20.740000 UTC,2.0,2019-04-26 14:59:06.430000 UTC,13,amazon-web-services|amazon-sagemaker,14776,2013-11-05 19:04:30.550000 UTC,2021-02-27 22:10:18.450000 UTC,,131,3,0,2,,,,,,['amazon-sagemaker']
scoring R model in Python,"<p>I am having a hard time to deploy an R model and expose it as a web service using <a href=""https://azure.github.io/azureml-sdk-for-r/articles/train-and-deploy-first-model.html"" rel=""nofollow noreferrer"">azuremlsdk</a> for R. The Python side of Azure machine learning appears to be more mature as Python was deemed more important by Microsoft. Anyway, I was wondering if one score an R model, persisted as .rds file, in Python. I understand R can talk to Python via reticulate. Any input from Python experts would be very much appreciated. Thanks.</p>",0,2,2021-05-15 05:39:30.167000 UTC,,,1,python|r|azure-machine-learning-service|azuremlsdk,58,2010-03-01 10:53:04.443000 UTC,2022-09-24 18:56:19.313000 UTC,Somewhere,15705,2171,91,2150,,,,,,['azure-machine-learning-service']
Is there a way to create datasets in AuzureML Studio from a tabular model or PowerBi datasets?,"<p>My goal is to import data from a Tabular Model (via MDX or DAX) or PowerBi dataset into AzureML Studio to work with.<br />
Unfortunately I can't find a way to connect to it.</p>
<p>Another alternative would be to establish a connection with pyadomd (<a href=""https://pypi.org/project/pyadomd/"" rel=""nofollow noreferrer"">https://pypi.org/project/pyadomd/</a>). However, I am having problems installing it here as well: ERROR: Failed building wheel for pythonnet.</p>
<p>Has anyone already had experience with this or is there a different approach I'm just not aware of?</p>
<p>Thank you very much.</p>",1,0,2021-10-20 08:44:42.147000 UTC,,2021-10-20 09:01:16.580000 UTC,1,python|azure-machine-learning-studio|azure-machine-learning-service,70,2017-06-27 13:41:14.253000 UTC,2022-08-17 07:57:17.643000 UTC,,89,21,0,10,,,,,,"['azure-machine-learning-studio', 'azure-machine-learning-service']"
"TensorFlowModel deployments Error, dependencies from provided requirements.txt not installed","<p>I am trying to deploy a TensorFlowModel and provide the post processing in an inference.py file...</p>
<p>I previously managed to deploy the model and invoke it in a notebook and then do the post processing in a jupyter notebook with the following code:</p>
<pre><code>model = TensorFlowModel(
    name=name_from_base('tf-yolov4'),
    model_data=model_artifact,
    role=role,
    framework_version='2.3'
)
</code></pre>
<p>now i want to do the post processing by providing an inference.py file so i followed the docs here:
<a href=""https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/using_tf.html#sagemaker-tensorflow-docker-containers"" rel=""nofollow noreferrer"">https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/using_tf.html#sagemaker-tensorflow-docker-containers</a></p>
<p>and used this snippet:</p>
<pre><code>from sagemaker.tensorflow import TensorFlowModel

model = TensorFlowModel(entry_point='inference.py',
                        dependencies=['requirements.txt'],
                        model_data='s3://mybucket/model.tar.gz',
                        role='MySageMakerRole')
</code></pre>
<p><strong>The dependencies i added</strong>:</p>
<pre><code>numpy
tensorflow
</code></pre>
<p><strong>My problem is</strong>:
the deployment process when i call</p>
<pre><code>predictor = model.deploy(initial_instance_count=1, instance_type='ml.m5.xlarge')
</code></pre>
<p>doesn't complete, and when i checked cloud watch i found the following:</p>
<pre><code>Traceback (most recent call last):
  File &quot;/usr/local/lib/python3.7/site-packages/gunicorn/arbiter.py&quot;, line 583, in spawn_worker
    worker.init_process()
  File &quot;/usr/local/lib/python3.7/site-packages/gunicorn/workers/ggevent.py&quot;, line 162, in init_process
    super().init_process()
  File &quot;/usr/local/lib/python3.7/site-packages/gunicorn/workers/base.py&quot;, line 119, in init_process
    self.load_wsgi()
  File &quot;/usr/local/lib/python3.7/site-packages/gunicorn/workers/base.py&quot;, line 144, in load_wsgi
    self.wsgi = self.app.wsgi()
  File &quot;/usr/local/lib/python3.7/site-packages/gunicorn/app/base.py&quot;, line 67, in wsgi
    self.callable = self.load()
  File &quot;/usr/local/lib/python3.7/site-packages/gunicorn/app/wsgiapp.py&quot;, line 49, in load
    return self.load_wsgiapp()
  File &quot;/usr/local/lib/python3.7/site-packages/gunicorn/app/wsgiapp.py&quot;, line 39, in load_wsgiapp
    return util.import_app(self.app_uri)
  File &quot;/usr/local/lib/python3.7/site-packages/gunicorn/util.py&quot;, line 358, in import_app
    mod = importlib.import_module(module)
  File &quot;/usr/local/lib/python3.7/importlib/__init__.py&quot;, line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1006, in _gcd_import
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 983, in _find_and_load
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 967, in _find_and_load_unlocked
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 677, in _load_unlocked
  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 728, in exec_module
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed
  File &quot;/sagemaker/python_service.py&quot;, line 414, in &lt;module&gt;
    resources = ServiceResources()
  File &quot;/sagemaker/python_service.py&quot;, line 400, in __init__
    self._python_service_resource = PythonServiceResource()
  File &quot;/sagemaker/python_service.py&quot;, line 83, in __init__
    self._handler, self._input_handler, self._output_handler = self._import_handlers()
  File &quot;/sagemaker/python_service.py&quot;, line 278, in _import_handlers
    spec.loader.exec_module(inference)
  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 728, in exec_module
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed
  File &quot;/opt/ml/model/code/inference.py&quot;, line 2, in &lt;module&gt;
    import numpy as np
</code></pre>
<p>and</p>
<pre><code>ModuleNotFoundError: No module named 'numpy'
</code></pre>
<p>which led me to believe that my inference.py was used by the container but not the requirements.txt file i provided therefore No module named 'numpy'!</p>
<p><strong>My question:</strong>
what am i doing wrong with my code and how to make sure the dependencies to run inference.py are installed?</p>
<p>Thanks in advance!</p>",1,0,2022-08-09 20:19:07.650000 UTC,,,0,amazon-web-services|tensorflow|amazon-sagemaker,38,2021-09-26 02:19:45.680000 UTC,2022-09-22 17:23:33.553000 UTC,"Cairo, Egypt",1,0,0,4,,,,,,['amazon-sagemaker']
Amazon SageMake throwing error Building your own algorithm container execution time?,"<p>I am trying to run my own algorithm container in amazon sagemaker,at the time of deployment time ,I am getting error like below.</p>

<pre><code>predictor = tree.deploy(1, 'ml.m4.xlarge', serializer=csv_serializer)

ValueError: Error hosting endpoint decision-trees-sample-2018-03-01-09-59-06-832: Failed Reason:  The primary container for production variant AllTraffic did not pass the ping health check.
</code></pre>

<p>then I run same line of code this time i am getting  below error.</p>

<pre><code> predictor = tree.deploy(1, 'ml.m4.xlarge', serializer=csv_serializer)

ClientError: An error occurred (ValidationException) when calling the CreateEndpoint operation: Cannot create already existing endpoint ""arn:aws:sagemaker:us-east-1:69759707XXxXX:endpoint/decision-trees-sample-2018-03-01-09-59-06-832"".
</code></pre>",1,0,2018-03-01 10:26:01.097000 UTC,1.0,2018-06-01 02:49:39.003000 UTC,0,amazon-web-services|amazon-s3|amazon-ec2|amazon-sagemaker,2116,2015-04-03 14:45:43.217000 UTC,2022-01-14 00:51:53.803000 UTC,,1015,34,0,304,,,,,,['amazon-sagemaker']
API Key for Azure Machine Learning Endpoint,"<p>I am using Azure ML, I made my models and now I want to connect them to Data Factory to run some process.</p>
<p>I implement an endpoint, but I can't find the API key for the endpoints. Right now, I have the REST endpoint, but not in key-based authentication enabled, it's false. Do you know how to generate the API key?</p>",1,0,2022-05-26 08:15:24.533000 UTC,,,0,azure|machine-learning|endpoint|azure-machine-learning-service|automl,206,2022-05-26 08:05:46.057000 UTC,2022-09-23 06:57:47.507000 UTC,,11,0,0,3,,,,,,['azure-machine-learning-service']
Pygame Mixer gives Couldn't open audio device on Azure ML,"<p>I'm trying to run the code that includes pygame on Azue, and its not working. <code>pygame.init()</code> is working but <code>pygame.mixer.init()</code> gives error, how can i solve this?
<a href=""https://i.stack.imgur.com/T9LM2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/T9LM2.png"" alt=""enter image description here"" /></a></p>",0,1,2022-03-15 08:42:08.883000 UTC,,2022-03-15 21:24:17.920000 UTC,0,azure|pygame|azure-machine-learning-service|mixer,30,2018-12-24 12:29:18.690000 UTC,2022-09-23 08:40:41.187000 UTC,"Ataşehir, Küçükbakkalköy, Dudullu Osb/Ataşehir/İstanbul, Türkiye",27,8,0,31,,,,,,['azure-machine-learning-service']
SageMaker endpoint AWS Lambda inference Issue,"<p>I've deployed a category trained model and hosted the endpoint.. and trying to inference this, but have a issue.. basically we take a short description i.e &quot;laptop screen&quot; this should return a category i.e &quot;Hardware&quot; the problem i'm facing is that i just seem to get this error when inferencing this via postman.</p>
<p>i.e if i send this  <code>{&quot;data&quot;:&quot;laptop screen&quot;} </code></p>
<p>i get this error in the body</p>
<pre class=""lang-json prettyprint-override""><code>{
    &quot;errorMessage&quot;: &quot;Expecting value: line 1 column 1 (char 0)&quot;,
    &quot;errorType&quot;: &quot;JSONDecodeError&quot;,
    &quot;stackTrace&quot;: [
        [
            &quot;/var/task/lambda_function.py&quot;,
            21,
            &quot;lambda_handler&quot;,
            &quot;result = json.loads(response['Body'].read().decode())&quot;
        ],
        [
            &quot;/var/lang/lib/python3.6/json/__init__.py&quot;,
            354,
            &quot;loads&quot;,
            &quot;return _default_decoder.decode(s)&quot;
        ],
        [
            &quot;/var/lang/lib/python3.6/json/decoder.py&quot;,
            339,
            &quot;decode&quot;,
            &quot;obj, end = self.raw_decode(s, idx=_w(s, 0).end())&quot;
        ],
        [
            &quot;/var/lang/lib/python3.6/json/decoder.py&quot;,
            357,
            &quot;raw_decode&quot;,
            &quot;raise JSONDecodeError(\&quot;Expecting value\&quot;, s, err.value) from None&quot;
        ]
    ]
}
</code></pre>
<p>this is my lambda function:</p>
<pre class=""lang-py prettyprint-override""><code>import os
import io
import boto3
import json
import csv

ENDPOINT_NAME = os.environ['ENDPOINT_NAME']
runtime= boto3.client('runtime.sagemaker')

def lambda_handler(event, context):
    print(&quot;Received event: &quot; + json.dumps(event, indent=2))
    
    data = json.loads(json.dumps(event))
    payload = data['data']
    print(payload)
    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME,
                                        ContentType='text/csv',
                                        Body=payload)
    print(response)
    result = json.loads(response['Body'].read().decode())
    
    return result
</code></pre>
<p>Ive added this to admin IAM role too</p>
<pre class=""lang-json prettyprint-override""><code>{
    &quot;Sid&quot;: &quot;VisualEditor0&quot;,
    &quot;Effect&quot;: &quot;Allow&quot;,
    &quot;Action&quot;: &quot;sagemaker:InvokeEndpoint&quot;,
    &quot;Resource&quot;: &quot;*&quot;
}
</code></pre>
<p>Any assistance would be ace, i feel im pretty close it works fine when it comes to predicting priority, but need help when predicting category string.</p>",1,8,2021-07-21 09:51:48.853000 UTC,,2021-07-21 11:58:19.243000 UTC,0,python|machine-learning|aws-lambda|postman|amazon-sagemaker,523,2012-12-06 22:38:32.733000 UTC,2022-04-19 08:30:02.153000 UTC,,43,3,0,12,,,,,,['amazon-sagemaker']
Changing preprocessing in trained model on SageMaker,"<p>I have trained model on SageMaker together with prerocessing. By preprocessing I mean I added the inference.py file with input_handler and output_handler functions according to this <a href=""https://github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/tensorflow/deploying_tensorflow_serving.rst"" rel=""nofollow noreferrer"">https://github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/tensorflow/deploying_tensorflow_serving.rst</a>. </p>

<p>I works nice but the problem is that everytime I want to change something in the preprocessing I have to retrain the model. Is there maybe some other to to do this without retraining?</p>",1,0,2020-01-10 09:24:43.477000 UTC,,,0,amazon-web-services|machine-learning|preprocessor|amazon-sagemaker,206,2013-07-19 07:51:51.723000 UTC,2020-04-29 07:52:55.540000 UTC,,312,7,0,36,,,,,,['amazon-sagemaker']
"MLFlow not configured, set environment variables","<p>Step 1: Installed anaconda ( also installed R and python in it) on AWS EC2 instance with Ubuntu</p>

<p>Step 2:  Used “conda install -c conda-forge mlflow” command to install mlflow in conda(which is assumed to be used for both R and python)</p>

<p>Step 3: library(mlflow) command to use mlflow in R and import mlflow in python as per the MLFlow documentation but still unable to run the MLFlow as it gives the below error for R &amp; Python</p>

<p><a href=""https://i.stack.imgur.com/holef.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/holef.png"" alt=""Error Code in Jupyter Notebook for R""></a>
<a href=""https://i.stack.imgur.com/Wh1xv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Wh1xv.png"" alt=""R Error Code""></a>
<a href=""https://i.stack.imgur.com/w6t19.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/w6t19.png"" alt=""Python Notebook error code 1""></a>
<a href=""https://i.stack.imgur.com/IecnB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IecnB.png"" alt=""Python Notebook error code 2""></a></p>

<p>Step 4: Used wine example that’s available in R &amp; Python repository to validate MLFlow logging which is not happening. The MLFlow server is up and running </p>",0,2,2019-10-23 10:54:36.013000 UTC,,,0,python|r|mlflow,404,2015-02-16 10:02:28.377000 UTC,2022-09-18 17:43:16.870000 UTC,India,185,6,0,46,,,,,,['mlflow']
Use a model trained by Google Cloud Vertex AI accelerated with TRT on Jetson Nano,"<p>I am trying to standardize our deployment workflow for machine vision systems. So we were thinking of the following workflow.</p>
<p><a href=""https://i.stack.imgur.com/lZiQs.png"" rel=""nofollow noreferrer"">Deployment workflow</a></p>
<p>So, we want to create the prototype for the same, so we followed the workflow. So, there is no problem with GCP operation whatsoever but when we try to export models, which we train on the <code>vertexAI</code> it will give three models as mentioned in the workflow which is:</p>
<ol>
<li>SaveModel</li>
<li>TFLite</li>
<li>TFJS</li>
</ol>
<p>and we try these models to convert into the ONNX model but we failed due to different errors.</p>
<ol>
<li>SaveModel - Always getting the same error with any parameter which is as follows
<a href=""https://i.stack.imgur.com/HsBoa.png"" rel=""nofollow noreferrer"">Error in savemodel</a>
I tried to track the error and I identified that the model is not loading inside the TensorFlow only which is wired since it is exported from the <code>GCP vertexAI</code> which leverages the power of TensorFlow.</li>
<li>TFLite - Successfully converted but again the problem with the <code>opset</code> of ONNX but with 15 <code>opset</code> it gets successfully converted but then NVIDIA tensorRT ONNXparser doesn't recognize the model during ONNX to TRT conversion.</li>
<li>TFJS - yet not tried.</li>
</ol>
<p>So we are blocked here due to these problems.</p>
<p>We can run these models exported directly from the <code>vertexAI</code> on the Jetson Nano device but the problem is <code>TF-TRT</code> and TensorFlow is not memory-optimized on the GPU so the system gets frozen after 3 to 4 hours of running.</p>
<p>We try this workflow with google teachable machine once and it workout well all steps are working perfectly fine so I am really confused How I conclude this full workflow since it's working on a teachable machine which is created by Google and not working on vertexAI model which is again developed by same Company.</p>
<p>Or am I doing Something wrong in this workflow?
For the background we are developing this workflow inside C++ framework for the realtime application in industrial environment.</p>",0,0,2022-01-06 12:16:25.767000 UTC,,2022-01-12 04:39:09.420000 UTC,1,tensorflow|onnx|nvidia-jetson-nano|google-cloud-vertex-ai,189,2022-01-06 11:21:46.990000 UTC,2022-09-23 06:29:45.187000 UTC,,11,0,0,2,,,,,,['google-cloud-vertex-ai']
running OpenGL on AML gpu clusters,"<p>I am trying to run rendering code based on OpenGL (<a href=""https://glumpy.readthedocs.io/en/latest/api/app-backends.html"" rel=""nofollow noreferrer"">https://glumpy.readthedocs.io/en/latest/api/app-backends.html</a>) on NC machines as part of an AML experiment.
No matter the back-end I try to use, I get errors when rendering.</p>

<p>Is OpenGL usage supported in AML?
Did anyone had similar experiences? Where you able to solve it?</p>

<p>thanks,
Emanuel</p>",1,2,2020-02-27 19:00:21.823000 UTC,,,0,azure-machine-learning-service,57,2018-03-01 09:03:25.313000 UTC,2022-03-13 07:26:26.350000 UTC,,33,0,0,2,,,,,,['azure-machine-learning-service']
ModelNotFoundError when deploying machine learning model on ACI,"<p>Please am trying to deploy a machine learning model on ACI but am having this error below at the final stage when trying to deploy the model as a  webservice on Azure. Help advise on how to fix it.</p>
<pre><code>2020-08-10T11:01:28,103498848+00:00 - rsyslog/run 
2020-08-10T11:01:28,109724839+00:00 - nginx/run 
2020-08-10T11:01:28,111248110+00:00 - gunicorn/run 
EdgeHubConnectionString and IOTEDGE_IOTHUBHOSTNAME are not set. Exiting...
2020-08-10T11:01:28,192849628+00:00 - iot-server/finish 1 0
2020-08-10T11:01:28,194532107+00:00 - Exit code 1 is normal. Not restarting iot-server.
Starting gunicorn 19.9.0
Listening at: http://127.0.0.1:31311 (18)
Using worker: sync
worker timeout is set to 300
Booting worker with pid: 42
SPARK_HOME not set. Skipping PySpark Initialization.
Exception in worker process
Traceback (most recent call last):
  File &quot;/azureml-envs/azureml_d31cd964833447a6573171273c4f1235/lib/python3.7/site-packages/gunicorn/arbiter.py&quot;, line 583, in spawn_worker
    worker.init_process()
  File &quot;/azureml-envs/azureml_d31cd964833447a6573171273c4f1235/lib/python3.7/site-packages/gunicorn/workers/base.py&quot;, line 129, in init_process
    self.load_wsgi()
  File &quot;/azureml-envs/azureml_d31cd964833447a6573171273c4f1235/lib/python3.7/site-packages/gunicorn/workers/base.py&quot;, line 138, in load_wsgi
    self.wsgi = self.app.wsgi()
  File &quot;/azureml-envs/azureml_d31cd964833447a6573171273c4f1235/lib/python3.7/site-packages/gunicorn/app/base.py&quot;, line 67, in wsgi
    self.callable = self.load()
  File &quot;/azureml-envs/azureml_d31cd964833447a6573171273c4f1235/lib/python3.7/site-packages/gunicorn/app/wsgiapp.py&quot;, line 52, in load
    return self.load_wsgiapp()
  File &quot;/azureml-envs/azureml_d31cd964833447a6573171273c4f1235/lib/python3.7/site-packages/gunicorn/app/wsgiapp.py&quot;, line 41, in load_wsgiapp
    return util.import_app(self.app_uri)
  File &quot;/azureml-envs/azureml_d31cd964833447a6573171273c4f1235/lib/python3.7/site-packages/gunicorn/util.py&quot;, line 350, in import_app
    __import__(module)
  File &quot;/var/azureml-server/wsgi.py&quot;, line 1, in &lt;module&gt;
    import create_app
  File &quot;/var/azureml-server/create_app.py&quot;, line 3, in &lt;module&gt;
    from app import main
  File &quot;/var/azureml-server/app.py&quot;, line 31, in &lt;module&gt;
    import main as user_main
ModuleNotFoundError: No module named 'main'
Worker exiting (pid: 42)
Shutting down: Master
Reason: Worker failed to boot.
2020-08-10T11:01:28,524618650+00:00 - gunicorn/finish 3 0
2020-08-10T11:01:28,526020716+00:00 - Exit code 3 is not normal. Killing image.
</code></pre>
<p>Here is the scoring script am using. Please advise if I need to import any function. Everything seems okay to me. Find it difficult to identify any issue</p>
<pre><code>
from azureml.contrib.services.aml_request import AMLRequest, rawhttp
from azureml.contrib.services.aml_response import AMLResponse
import json


from azureml.core.model import Model
from azureml.contrib.services.aml_request import AMLRequest, rawhttp
from azureml.contrib.services.aml_response import AMLResponse

# Done - Faster
from sentence_transformers import SentenceTransformer
from sentence_transformers import models, losses
import scipy.spatial
import pickle as pkl
import pickle

import pandas as pd
import os

from collections import OrderedDict
  

import re
def clean_text(text):
  
  text = re.sub('\r\n','\n',text)
  text = re.sub('\r','',text)
  text = re.sub(r'\s+',' ',text)
  text = re.sub(r'\n',' ',text)

  return text
  
  
def init():
  
  print(&quot;This is init()&quot;)
  
  global model, on_path, ifc_inv, ifc_adv, wb_lend, wb_adv, smart_lesson, ifc_inv_data_map, ifc_adv_data_map, wb_lend_data_map, wb_adv_data_map, smart_lesson_data_map

  on_path = Model.get_model_path(model_name='Docker_custom_KP_model', version = 2)
  
  
  ## Read in Data Files Fix
  ifc_inv =  pd.read_csv(os.path.join(on_path,'Assets/data_file/ifc_data/ifc_inv.csv'),  lineterminator='\n')
  ifc_adv =  pd.read_csv(os.path.join(on_path,'Assets/data_file/ifc_data/ifc_adv.csv'), lineterminator='\n')

  # WB data
  wb_lend =  pd.read_csv(os.path.join(on_path,'Assets/data_file/wb_data/wb_lend.csv'), lineterminator='\n')
  wb_adv  =  pd.read_csv(os.path.join(on_path,'Assets/data_file/wb_data/wb_adv.csv') , lineterminator='\n')

  # Lesson data
  smart_lesson = pd.read_csv(os.path.join(on_path,'Assets/data_file/lesson_data/smart_lesson.csv'),  lineterminator='\n')

  
  
  ## Read in Data Map #Fix
  ifc_inv_data_map = pd.read_csv(os.path.join(on_path,'Assets/data_map/ifc_inv_data_map.csv'))
  ifc_adv_data_map = pd.read_csv(os.path.join(on_path,'Assets/data_map/ifc_adv_data_map.csv'))

  wb_lend_data_map = pd.read_csv(os.path.join(on_path,'Assets/data_map/wb_lend_data_map.csv'))
  wb_adv_data_map = pd.read_csv(os.path.join(on_path,'Assets/data_map/wb_adv_data_map.csv'))

  smart_lesson_data_map = pd.read_csv(os.path.join(on_path,'Assets/data_map/smart_lesson_data_map.csv'))
  
  ## Drop irrelevant columns
  ifc_inv.drop(['Unnamed: 0'], axis = 1, inplace= True)
  ifc_inv_data_map.drop(['Unnamed: 0'], axis = 1, inplace= True)
  
  ifc_adv.drop(['Unnamed: 0'], axis = 1, inplace= True)
  ifc_adv_data_map.drop(['Unnamed: 0'], axis = 1, inplace= True)
  
  wb_lend.drop(['Unnamed: 0'], axis = 1, inplace= True)
  wb_lend_data_map.drop(['Unnamed: 0'], axis = 1, inplace= True)
  
  wb_adv.drop(['Unnamed: 0'], axis = 1, inplace= True)
  wb_adv_data_map.drop(['Unnamed: 0'], axis = 1, inplace= True)
  
  smart_lesson.drop(['Unnamed: 0'], axis = 1, inplace= True)
  smart_lesson_data_map.drop(['Unnamed: 0'], axis = 1, inplace= True)
  
    
    
  ## CLean data
  # ## TOOD - After UAT
  ifc_inv['Project_Description'] = ifc_inv['Project_Description'].map(clean_text)
  ifc_adv['Project_Description'] = ifc_adv['Project_Description'].map(clean_text)

  wb_lend['proj_abstract'] = wb_lend['proj_abstract'].map(clean_text)
  wb_adv['proj_abstract'] = wb_adv['proj_abstract'].map(clean_text)

  smart_lesson['Abstracts'] = smart_lesson['Abstracts'].map(clean_text)
  
  ## end Clean data
  
  
  
  ## Read in Models
  #word_embedding_model = models.BERT('/dbfs/FileStore/tables/dev/models/BERT_model_new/') ## fix
  word_embedding_model = models.BERT(os.path.join(on_path,'model_dir/')) 

  # Applying mean pooling to get one fixed sized sentences vector
  pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),
                                 pooling_mode_mean_tokens = True,
                                 pooling_mode_cls_token = False,
                                 pooling_mode_max_tokens = False
                                )

  model = SentenceTransformer(modules=[word_embedding_model, pooling_model])

  
def text_to_embedding(model, in_text):
  
  query_embeddings = model.encode(in_text, show_progress_bar=True)
  return query_embeddings


    
@rawhttp
def run(request):
  
  print(&quot;This is run()&quot;)
  print(&quot;Request: [{0}]&quot;.format(request))
  if request.method == 'GET':
    # For this example, just return the URL for GETs.
    respBody = str.encode(request.full_path)
    return AMLResponse(respBody, 200)
  
  elif request.method == 'POST':
    
    reqBody = request.get_data(False)
    
    in_text = json.loads(reqBody)['input_query']
    in_text = [in_text]
    
    req_page = json.loads(reqBody)['request_page']
    
    n_row = json.loads(reqBody)['n_request']
    n_row = int(n_row)

    if req_page == 'IFCInvestment':
      
      relevant_file_dir = os.path.join(on_path,'Assets/data_file/ifc_data/ifc_inv.csv')
      data_map = os.path.join(on_path,'Assets/data_map/ifc_inv_data_map.csv')

      #vector_dir = './KPrequest/inference_data/' + request_page + '/faiss_inv.index'

      with open(os.path.join(on_path,'Assets/embeddings/embedding_ifc_inv.pkl'), 'rb') as f:
        corpus_vecs = pickle.load(f)

      vec = text_to_embedding(model, in_text)
      #vec = [vec]

      relevant_df = ifc_inv  #pd.read_csv(relevant_file_dir, lineterminator='\n')
      data_map_df = ifc_inv_data_map #pd.read_csv(data_map)

      #relevant_df.drop(['Unnamed: 0'], axis = 1, inplace= True)
      #data_map_df.drop(['Unnamed: 0'], axis = 1, inplace= True)


    if req_page == 'IFCAdv':

      relevant_file_dir = os.path.join(on_path,'Assets/data_file/ifc_data/ifc_adv.csv')
      data_map = os.path.join(on_path,'Assets/data_map/ifc_adv_data_map.csv')

      #vector_dir = './KPrequest/inference_data/' + request_page + '/faiss_inv.index'

      with open(os.path.join(on_path,'Assets/embeddings/embedding_ifc_adv.pkl'), 'rb') as f:
        corpus_vecs = pickle.load(f)

      vec = text_to_embedding(model, in_text)
      #vec = [vec]

      relevant_df = ifc_adv  #pd.read_csv(relevant_file_dir, lineterminator='\n')
      data_map_df = ifc_adv_data_map #pd.read_csv(data_map)

      #relevant_df.drop(['Unnamed: 0'], axis = 1, inplace= True)
      #data_map_df.drop(['Unnamed: 0'], axis = 1, inplace= True)


    if req_page == 'IFCLesson':
      
      relevant_file_dir = os.path.join(on_path,'Assets/data_file/lesson_data/smart_lesson.csv')
      data_map = os.path.join(on_path,'Assets/data_map/smart_lesson_data_map.csv')

      #vector_dir = './KPrequest/inference_data/' + request_page + '/faiss_inv.index'

      with open(os.path.join(on_path,'Assets/embeddings/embedding_smart_lesson.pkl'), 'rb') as f:
        corpus_vecs = pickle.load(f)

      vec = text_to_embedding(model, in_text)
      #vec = [vec]

      relevant_df = smart_lesson  #pd.read_csv(relevant_file_dir, lineterminator='\n')
      data_map_df = smart_lesson_data_map #pd.read_csv(data_map)

      #relevant_df.drop(['Unnamed: 0'], axis = 1, inplace= True)
      #data_map_df.drop(['Unnamed: 0'], axis = 1, inplace= True)


    if req_page == 'WBLend':

      relevant_file_dir = os.path.join(on_path,'Assets/data_file/wb_data/wb_lend.csv')
      data_map = os.path.join(on_path,'Assets/data_map/wb_lend_data_map.csv')

      #vector_dir = './KPrequest/inference_data/' + request_page + '/faiss_inv.index'

      with open(os.path.join(on_path,'Assets/embeddings/embedding_wb_lend.pkl'), 'rb') as f:
        corpus_vecs = pickle.load(f)

      vec = text_to_embedding(model, in_text)
      #vec = [vec]

      relevant_df = wb_lend  #pd.read_csv(relevant_file_dir, lineterminator='\n')
      data_map_df = wb_lend_data_map #pd.read_csv(data_map)

      #relevant_df.drop(['Unnamed: 0'], axis = 1, inplace= True)
      #data_map_df.drop(['Unnamed: 0'], axis = 1, inplace= True)


    if req_page == 'WBAdv':

      relevant_file_dir = os.path.join(on_path,'Assets/data_file/wb_data/wb_adv.csv')
      data_map = os.path.join(on_path,'Assets/data_map/wb_adv_data_map.csv')

      #vector_dir = './KPrequest/inference_data/' + request_page + '/faiss_inv.index'

      with open(os.path.join(on_path,'Assets/embeddings/embedding_wb_adv.pkl'), 'rb') as f:
        corpus_vecs = pickle.load(f)

      vec = text_to_embedding(model, in_text)
      #vec = [vec]

      relevant_df = wb_adv #pd.read_csv(relevant_file_dir, lineterminator='\n')
      data_map_df = wb_adv_data_map  #pd.read_csv(data_map)

      #relevant_df.drop(['Unnamed: 0'], axis = 1, inplace= True)
      #data_map_df.drop(['Unnamed: 0'], axis = 1, inplace= True)


    distances = scipy.spatial.distance.cdist(vec, corpus_vecs, &quot;cosine&quot;)[0]

    results = zip(range(len(distances)), distances)
    results = sorted(results, key = lambda x: x[1])
    

    result_index = list(map(lambda x: x[0],results))
    map_index = data_map_df.iloc[result_index]

    proj_index = map_index['Index'].to_list()
    unique_proj_index = list(OrderedDict.fromkeys(proj_index))

    relevant_df.fillna('', inplace=True)

    kp_dataframe = relevant_df.loc[unique_proj_index[0: n_row]]
    kp_result = kp_dataframe.to_dict(orient='records')

    resp = AMLResponse(json.dumps(kp_result), 200)
    resp.headers['Access-Control-Allow-Origin'] = '*'

    return resp
  
  else:
    return AMLResponse(&quot;bad request&quot;, 500)```
</code></pre>",0,3,2020-08-10 11:56:49.840000 UTC,,2020-08-11 09:22:16.370000 UTC,1,python|azure-web-app-service|azure-machine-learning-service|azure-container-instances,198,2018-09-18 19:45:24.587000 UTC,2022-09-14 17:20:52.613000 UTC,"Laurel, MD, USA",359,3,0,41,,,,,,['azure-machine-learning-service']
AttributeError: 'NoneType' object has no attribute '_global_run_stack',"<p><strong>Description</strong></p>
<p>I am using PTAN library with an A3C model and I am trying to work with <strong>wandb sweep</strong> but I've encountered some weird problems, and I am not sure if it's a bug regarding sweep (because if I am going to use just a simple model without any threads involving is going to work properly) or I am doing something wrong.</p>
<p><strong>How to reproduce</strong></p>
<p><em><strong>training function:</strong></em></p>
<pre><code>def train(conf):
    batch = []
    step_idx = 0
    epoch = conf['epochs']
    try:
        with commune.RewardTracker(writer, stop_reward=conf['reward_bound']) as tracker:
            with ptan.common.utils.TBMeanTracker(writer, batch_size=100) as tb_tracker:
                while True:
                    if step_idx == epoch:
                        break
                    train_entry = train_queue.get()
                    if isinstance(train_entry, TotalReward):
                        if tracker.reward(train_entry.reward, step_idx):
                            break
                        continue
                    if isinstance(train_entry, TotalProfit):
                        tracker.profits(train_entry.total_profit, train_entry.curr_profit, step_idx)
                        continue
                    step_idx += 1
                    if step_idx % 100 == 0:
                        torch.save(net.state_dict(), os.path.join(SAVING_FOLDER, PROJECT_NAME))

                    batch.append(train_entry)
                    if len(batch) &lt; conf['batch_size']:
                        continue

                    states_v, actions_t, vals_ref_v = commune.unpack_batch(batch, net,
                                                                           last_val_gamma=conf['gamma'] ** conf['reward_steps'],
                                                                           device=device)
                    batch.clear()

                    optimizer.zero_grad()
                    logits_v, value_v = net(states_v)

                    loss_value_v = F.mse_loss(value_v.squeeze(-1), vals_ref_v)

                    log_prob_v = F.log_softmax(logits_v, dim=1)
                    adv_v = vals_ref_v - value_v.detach()
                    log_prob_actions_v = adv_v * log_prob_v[range(conf['batch_size']), actions_t]
                    loss_policy_v = -log_prob_actions_v.mean()

                    prob_v = F.softmax(logits_v, dim=1)
                    entropy_loss_v = conf['entropy_beta'] * (prob_v * log_prob_v).sum(dim=1).mean()

                    loss_v = entropy_loss_v + loss_value_v + loss_policy_v
                    loss_v.backward()
                    nn_utils.clip_grad_norm_(net.parameters(), conf['clip_grad'])
                    optimizer.step()

                    tb_tracker.track(&quot;advantage&quot;, adv_v, step_idx)
                    tb_tracker.track(&quot;values&quot;, value_v, step_idx)
                    tb_tracker.track(&quot;batch_rewards&quot;, vals_ref_v, step_idx)
                    tb_tracker.track(&quot;loss_entropy&quot;, entropy_loss_v, step_idx)
                    tb_tracker.track(&quot;loss_policy&quot;, loss_policy_v, step_idx)
                    tb_tracker.track(&quot;loss_value&quot;, loss_value_v, step_idx)
                    tb_tracker.track(&quot;loss_total&quot;, loss_v, step_idx)
    finally:
        for p in data_proc_list:
            p.terminate()
            p.join()
</code></pre>
<p><strong>main function:</strong></p>
<pre><code>if __name__ == &quot;__main__&quot;:
    mp.set_start_method('fork')
    device = torch.device(&quot;cuda:0&quot; if use_cuda else &quot;cpu&quot;)

    with open(r'sweep_config.yaml') as file:
        sweep_config = yaml.load(file, Loader=yaml.FullLoader)

    logs_dir_name = &quot;a3c_stock&quot;
    wandb.tensorboard.patch(root_logdir=logs_dir_name)

    sweep_id = wandb.sweep(sweep_config, project=&quot;sweep_project&quot;, entity=&quot;vildnex&quot;)
    wandb.init(config=config_default)

    config = wandb.config

    writer = SummaryWriter(comment=logs_dir_name)

    env = make_env(config)
    net = commune.AtariA2C(env.observation_space.shape, env.action_space.n).to(device)
    net.share_memory()

    if not os.path.isdir(SAVING_FOLDER):
        os.mkdir(SAVING_FOLDER)

    if os.path.isfile(os.path.join(SAVING_FOLDER, PROJECT_NAME)):
        net.load_state_dict(torch.load(os.path.join(SAVING_FOLDER, PROJECT_NAME), map_location=device))

    optimizer = optim.RMSprop(net.parameters(), lr=config.learning_rate, eps=1e-3)

    train_queue = mp.Queue(maxsize=config.processes_count)
    data_proc_list = []
    dict_conf = dict(config)
    for _ in range(config.processes_count):
        data_proc = mp.Process(target=data_func, args=(net, device, train_queue, dict_conf))
        data_proc.start()
        data_proc_list.append(data_proc)

    wandb.agent(sweep_id, lambda: train(dict_conf))
</code></pre>
<p><strong>Error message:</strong></p>
<pre><code>Exception in thread Thread-6:
Traceback (most recent call last):
  File &quot;&lt;PATH&gt;/venv/lib/python3.9/site-packages/wandb/agents/pyagent.py&quot;, line 303, in _run_job
    self._function()
  File &quot;&lt;PATH&gt;/RL_TraningBot/EXPERIMENTS/A3C_TEST.py&quot;, line 191, in &lt;lambda&gt;
    wandb.agent(sweep_id, lambda: train(dict_conf))
  File &quot;&lt;PATH&gt;/RL_TraningBot/EXPERIMENTS/A3C_TEST.py&quot;, line 105, in train
    tracker.profits(train_entry.total_profit, train_entry.curr_profit, step_idx)
  File &quot;&lt;PATH&gt;/RL_TraningBot/EXPERIMENTS/commune.py&quot;, line 118, in profits
    self.writer.add_scalar(&quot;total_profit&quot;, total_profit, frame)
  File &quot;&lt;PATH&gt;/venv/lib/python3.9/site-packages/torch/utils/tensorboard/writer.py&quot;, line 344, in add_scalar
    self._get_file_writer().add_summary(
  File &quot;&lt;PATH&gt;/venv/lib/python3.9/site-packages/torch/utils/tensorboard/writer.py&quot;, line 250, in _get_file_writer
    self.file_writer = FileWriter(self.log_dir, self.max_queue,
  File &quot;&lt;PATH&gt;/venv/lib/python3.9/site-packages/torch/utils/tensorboard/writer.py&quot;, line 60, in __init__
    self.event_writer = EventFileWriter(
  File &quot;&lt;PATH&gt;/venv/lib/python3.9/site-packages/wandb/integration/tensorboard/monkeypatch.py&quot;, line 157, in __init__
    _notify_tensorboard_logdir(logdir, save=save, root_logdir=root_logdir_arg)
  File &quot;&lt;PATH&gt;/venv/lib/python3.9/site-packages/wandb/integration/tensorboard/monkeypatch.py&quot;, line 167, in _notify_tensorboard_logdir
    wandb.run._tensorboard_callback(logdir, save=save, root_logdir=root_logdir)
  File &quot;&lt;PATH&gt;/venv/lib/python3.9/site-packages/wandb/sdk/wandb_run.py&quot;, line 804, in _tensorboard_callback
    self._backend.interface.publish_tbdata(logdir, save, root_logdir)
  File &quot;&lt;PATH&gt;/venv/lib/python3.9/site-packages/wandb/sdk/interface/interface.py&quot;, line 202, in publish_tbdata
    self._publish(rec)
  File &quot;&lt;PATH&gt;/venv/lib/python3.9/site-packages/wandb/sdk/interface/interface.py&quot;, line 518, in _publish
    raise Exception(&quot;The wandb backend process has shutdown&quot;)
Exception: The wandb backend process has shutdown

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;/usr/lib/python3.9/threading.py&quot;, line 954, in _bootstrap_inner
    self.run()
  File &quot;/usr/lib/python3.9/threading.py&quot;, line 892, in run
    self._target(*self._args, **self._kwargs)
  File &quot;&lt;PATH&gt;/venv/lib/python3.9/site-packages/wandb/agents/pyagent.py&quot;, line 308, in _run_job
    wandb.finish(exit_code=1)
  File &quot;&lt;PATH&gt;/venv/lib/python3.9/site-packages/wandb/sdk/wandb_run.py&quot;, line 2374, in finish
    wandb.run.finish(exit_code=exit_code)
  File &quot;&lt;PATH&gt;/venv/lib/python3.9/site-packages/wandb/sdk/wandb_run.py&quot;, line 1144, in finish
    if self._wl and len(self._wl._global_run_stack) &gt; 0:
  File &quot;&lt;PATH&gt;/venv/lib/python3.9/site-packages/wandb/sdk/wandb_setup.py&quot;, line 234, in __getattr__
    return getattr(self._instance, name)
AttributeError: 'NoneType' object has no attribute '_global_run_stack'
</code></pre>
<p><strong>Environment</strong></p>
<ul>
<li>OS: Manjaro 5.21.5</li>
<li>Environment: PyCharm Local</li>
<li>Python Version: 3.9</li>
</ul>",0,1,2021-05-25 22:12:40.977000 UTC,,,0,python|pytorch|sweeper|wandb,1192,2016-11-15 22:30:43.857000 UTC,2022-09-24 21:39:25.850000 UTC,,1383,12,0,177,,,,,,['wandb']
Does the library multprocesing works on sagemaker conda-python3?,<p>Does the library multprocesing works well on sagemaker conda-python3?</p>,1,0,2019-05-22 19:49:24.157000 UTC,,,0,python-3.x|python-multiprocessing|amazon-sagemaker,723,2016-03-19 19:45:07.413000 UTC,2022-08-25 22:03:06.273000 UTC,"Ecuador, Quito",617,74,0,117,,,,,,['amazon-sagemaker']
Amazon SageMaker could not find a valid Conda environment file,"<p>I've been running Amazon SageMaker lab.
I opened SageMaker Studio Lab example notebooks, AWS Machine Learning University.
Every lab gave the error mesasge: <code>No Conda environment file found Could not find a valid Conda environment file. Please check to make sure you have provided a valid path and filename</code>.</p>
<p>Does anyone know how to fix this issue?</p>",1,0,2021-12-04 06:17:09.510000 UTC,,2021-12-10 10:53:11.077000 UTC,0,amazon-sagemaker,408,2021-06-02 00:06:14.797000 UTC,2022-03-31 06:53:48.617000 UTC,,1,0,0,2,,,,,,['amazon-sagemaker']
Azure ML Studio notebook using Python 3.6 kernel instead of 3.8 kernel for notebook magic %%bash commands?,"<p>I'm working on a Tensorflow project in Azure ML Studio right now and I'm currently following along with <a href=""https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/distribute/multi_worker_with_keras.ipynb"" rel=""nofollow noreferrer"">this Colab Notebook</a> to learn how to use multiple workers. Whenever I try to run magic %%bash commands in the notebook it seems like the notebook is using the AzureML Python 3.6 kernel instead of the AzureML Python 3.8 Kernel.</p>
<p>The 3.8 kernel is the kernel where I have installed all my necessary python packages and it is the only kernel running in my ML Studio compute. I have confirmed this by checking the kernel manager and making sure that no other kernels are running.</p>
<p>When I try to run</p>
<pre><code>%%bash --bg
python main.py &amp;&gt; job_0.log
</code></pre>
<p>I get this error in my job_0.log file...</p>
<pre><code>2021-09-29 23:00:35.170807: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory
2021-09-29 23:00:35.170899: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory
2021-09-29 23:00:35.170908: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
Traceback (most recent call last):
  File &quot;main.py&quot;, line 12, in &lt;module&gt;
    strategy = tf.distribute.MultiWorkerMirroredStrategy()
AttributeError: module 'tensorflow_core._api.v2.distribute' has no attribute 'MultiWorkerMirroredStrategy'
</code></pre>
<p>MultiWorkerMirroredStrategy was added in Tensorflow 2.6 so I made sure that I had Tensorflow 2.6 installed in my Python 3.8 kernel. When I pip show in terminal it shows that it is indeed installed there. However when I</p>
<pre><code>%%bash
pip show tensorflow
</code></pre>
<p>in my notebook, the output displays this.</p>
<pre><code>Name: tensorflow
Version: 2.1.0
Summary: TensorFlow is an open source machine learning framework for everyone.
Home-page: https://www.tensorflow.org/
Author: Google Inc.
Author-email: packages@tensorflow.org
License: Apache 2.0
Location: /anaconda/envs/azureml_py36/lib/python3.6/site-packages
Requires: astor, six, tensorflow-estimator, protobuf, scipy, keras-applications, opt-einsum, google-pasta, wheel, termcolor, keras-preprocessing, tensorboard, absl-py, numpy, gast, grpcio, wrapt
Required-by: autokeras
</code></pre>
<p>I don't know why it is showing the AzureML Python 3.6 installation location when I am running the AzureML Python 3.8 kernel. For context whenever I start a notebook in ML studio the default kernel is AzureML Python 3.6 and I change to Python 3.8 before I begin to run my code.</p>
<p>Also I'm not sure if this is related but when I pip install packages in this ML Studio Compute the default installation goes to the AzureML 3.8 kernel. So I am especially confused. If anyone has experience with this or has had a similar problem I would really appreciate some guidance.</p>",1,0,2021-09-29 23:38:17.797000 UTC,,,2,bash|tensorflow|kernel|azure-machine-learning-service,498,2021-08-19 20:13:50.813000 UTC,2022-09-22 20:21:13.943000 UTC,,63,0,0,1,,,,,,['azure-machine-learning-service']
Why require only R >= 3.3.0?,"<p>I'm building a R model in Azure machine learning with a zipped xgboost package attached to the 'execute R script'. </p>

<p>Azure machine learning uses R 3.2.2.</p>

<p>The model returns with an error saying ""This is R 3.2.2, package 'xgboost' needs >= 3.3.0"".</p>

<p>Is there any reason it insists on >= 3.3.0. If not, can I get it down to run with R 3.2.2?</p>",0,3,2017-02-13 12:46:27.357000 UTC,,,0,r|azure-machine-learning-studio,106,2012-10-29 18:22:48.770000 UTC,2022-09-21 14:01:44.350000 UTC,,823,16,4,114,,,,,,['azure-machine-learning-studio']
what is a transaction in Azure's cognitive Services text analytics API,"<p>I am looking at this:</p>

<p><a href=""https://azure.microsoft.com/en-gb/pricing/details/cognitive-services/text-analytics/"" rel=""nofollow noreferrer"">https://azure.microsoft.com/en-gb/pricing/details/cognitive-services/text-analytics/</a></p>

<p>and would like to test drive the Free - Web/Container to perform some sentiment analysis. It says that 5000 transactions are free. I understand that a record equals 1000 characters but what is a a transaction? Is it a text blob with potentially more than 1000 characters? Thanks.</p>",1,0,2019-02-19 15:33:09.107000 UTC,,,0,azure|azure-machine-learning-studio|azure-container-service|azure-cognitive-services,387,2010-03-01 10:53:04.443000 UTC,2022-09-24 18:56:19.313000 UTC,Somewhere,15705,2171,91,2150,,,,,,['azure-machine-learning-studio']
Sagemaker Train Job can't connect to ec2 instance,"<p>I have MLFlow server running on ec2 instance, port 5000.</p>
<p>This ec2 instance has security group with opened TCP connection on port 5000 to another security group designated for SageMaker.</p>
<p>ec2 instance inbound rules:
<a href=""https://i.stack.imgur.com/VXwid.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VXwid.png"" alt=""enter image description here"" /></a></p>
<p>SageMaker outbound rules:
<a href=""https://i.stack.imgur.com/ZUzek.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZUzek.png"" alt=""enter image description here"" /></a></p>
<p>These 2 security groups are in the same VPC</p>
<p>Now, I try to run SageMaker training job with designated security group, so that the training script will log metrics to ec2 server via internal IP address. (As answered <a href=""https://stackoverflow.com/questions/45416882/aws-security-group-include-another-security-group"">here</a>), but connection fails</p>
<p>SageMaker job init:</p>
<pre><code>   role = &quot;ml_sagemaker&quot;
   security_group_ids = ['sg-04868acca16e81183']
   bucket = sagemaker_session.default_bucket()  
   out_path = f&quot;s3://{bucket}/{project_name}&quot;

   estimator = PyTorch(entry_point='run_train.py',
                       source_dir='.',
                       sagemaker_session=sagemaker_session,
                       instance_type=instance_type,
                       instance_count=1,
                       framework_version='1.5.0',
                       py_version='py3',
                       role=role,
                       security_group_ids=security_group_ids,
                       hyperparameters={},
                       )
   ....

</code></pre>
<p>Inside <code>run_train.py</code>:</p>
<pre><code>import mlflow
tracking_uri = &quot;http://172.31.77.137:5000&quot;  # &lt;- this is internal ec2 IP
mlflow.set_tracking_uri(tracking_uri)
mlflow.log_param(&quot;test_param&quot;, 3)
</code></pre>
<p>Error:</p>
<pre><code>File &quot;/opt/conda/lib/python3.6/site-packages/urllib3/util/connection.py&quot;, line 74, in create_connection
    sock.connect(sa)
TimeoutError: [Errno 110] Connection timed out
</code></pre>
<p><strong>However</strong>, when when I create SageMaker Notebook instance with the same security group and the same IAM role, I am able to successfully connect to ec2 and log metrics from within the Notebook.</p>
<p><a href=""https://i.stack.imgur.com/YYHlO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YYHlO.png"" alt=""enter image description here"" /></a></p>
<p>Here is SageMaker Notebook configurations:</p>
<img src=""https://i.stack.imgur.com/bslu8.png"" width=""300"" />
<p>How can I connect to ec2 instance from SageMaker Training Job?</p>",1,0,2021-02-19 17:18:49.957000 UTC,1.0,,1,amazon-ec2|amazon-vpc|amazon-sagemaker|aws-security-group|mlflow,608,2015-07-28 15:46:05.740000 UTC,2022-09-22 13:24:29.297000 UTC,,653,216,1,76,,,,,,"['mlflow', 'amazon-sagemaker']"
Snowflake Connection to Sagemaker - CloudFormation error on StorageIntegrationStack when creating,"<p>I'm trying to associate Sagemaker with snowflake using AWS CloudFormation, by following this tutoriel : <a href=""https://quickstarts.snowflake.com/guide/vhol_snowflake_data_wrangler/index.html?index=..%2F..index#2"" rel=""nofollow noreferrer"">https://quickstarts.snowflake.com/guide/vhol_snowflake_data_wrangler/index.html?index=..%2F..index#2</a>
but while creating the stack I got this error :</p>
<p><em>StorageIntegrationStack<br />
CREATE_FAILED
Embedded stack ... was not successfully created: The following resource(s) failed to create: [KMSKey, SnowflakeSecret].</em></p>
<p>What's KMSKey and SnowflakeSecret ? How can I fix this error ?</p>",0,6,2021-12-06 10:34:02.860000 UTC,,,0,amazon-web-services|amazon-cloudformation|snowflake-cloud-data-platform|amazon-sagemaker,289,2019-05-24 13:44:49.563000 UTC,2022-09-24 19:13:53.223000 UTC,,43,2,0,11,,,,,,['amazon-sagemaker']
Error while passing test data point to sci-kit learn <endpoint>.predict() function created inside AWS-Sagemaker,"<p>I created a scikit learn model endpoint inside AWS Sagemaker. I want to make predictions on my test set using this endpoint. My endpoint creation code looks like </p>

<pre><code>predictor = sklearn.deploy(1, 'ml.m4.xlarge')
from sagemaker.predictor import csv_serializer
predictor.content_type = 'text/csv'
predictor.serializer = csv_serializer
predictor.deserializer = None
</code></pre>

<p>When I pass my test data point as a list  <code>predictor.predict(l)</code> where <code>l</code> is the list, it throws an error </p>

<pre><code>ValueError: Expected 2D array, got 1D array instead:
</code></pre>

<p>When I pass it as a numpy array of 2 dimensions(I checked dimensions using .ndim), it still throws the same error. When I tried to pass the data as a string separated by commas without any spaces, it still throws the same error. </p>

<pre><code> Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.
</code></pre>

<p>This line gets displayed every time the <code>Valueerror</code> is thrown but even after reshaping, the same error persists. </p>

<p>So I have tried the formats <code>'2,3,4,5'</code>, <code>['2,3,4,5']</code>, <code>array[[2,3,4,5]]</code>,<code>[2,3,4,5]</code> but none of these work.
Can someone please convey what the right format is for the input to the predictor function of sci-kit learn?</p>",1,5,2019-12-08 12:42:57.423000 UTC,,,0,amazon-web-services|scikit-learn|amazon-sagemaker,110,2019-09-12 20:07:41.627000 UTC,2022-09-21 14:35:39.230000 UTC,"Hyderabad, Telangana, India",486,28,2,75,,,,,,['amazon-sagemaker']
Model output folder in sagemaker,"<p>I am trying to run a training job using sagemaker sdk.
I set the <code>base_job_name</code> as <code>base-job-name</code> and model_dir as <code>s3://my-bucket/model-output/</code>,  The trained model, however, is at <code>s3://my-bucket/model-output/base-job-name-2020-10-12-21-30-42-748/output</code>.
Can I do something to remove the date-time part from the <code>base-job-name</code> folder? It is perfectly fine to overwrite files as a result.
I couldn't seem to locate any property in the documentation which can help me set this.
This is how I am creating the estimator</p>
<pre class=""lang-py prettyprint-override""><code>estimator = TensorFlow(
    base_job_name='base-job-name',
    entry_point='model.py',
    source_dir=source_dir,
    output_path='s3://my-bucket/model-output/',
    model_dir='s3://my-bucket/model-output/',
    instance_type='ml.m5.large',
    instance_count=1,
    role=my_role,
    framework_version='2.2.0',
    py_version='py37',
    subnets=subnets,
    security_group_ids=security_group_ids,
    sagemaker_session=sagemaker_sess,
    tags=tags
)
</code></pre>",1,0,2020-10-12 22:57:07.497000 UTC,,,1,amazon-sagemaker,1084,2015-05-07 06:21:56.330000 UTC,2022-09-09 00:26:12.320000 UTC,,393,8,0,9,,,,,,['amazon-sagemaker']
Difference between SageMaker instance count and Data parallelism,"<p>I can't understand the difference between SageMaker instance count and Data parallelism. As we already have a feature that can specify how many instances we train model when we write a training script using sagemaker-sdk.</p>
<p>However, in 2021 re:Invent, SageMaker team launched and demonstrated SageMaker managed Data Parallelism and this feature also provides distributed training.</p>
<p>I've searched a lot of sites for letting me know about that, but I can't find really clear demonstration. I share some stuffs explaining the concept I mentioned closely. Link : <a href=""https://godatadriven.com/blog/distributed-training-a-diy-aws-sagemaker-model/"" rel=""nofollow noreferrer"">https://godatadriven.com/blog/distributed-training-a-diy-aws-sagemaker-model/</a></p>",1,0,2022-09-09 13:03:12.430000 UTC,,,0,amazon-sagemaker,15,2014-10-07 08:13:42.830000 UTC,2022-09-23 14:45:05.230000 UTC,,26,0,0,3,,,,,,['amazon-sagemaker']
Error in the MS Azure autoML preparation - wrong file format / encoding?,"<p>I am trying to deploy the MS Azure automated machine learning as per the following Github example:</p>

<p><a href=""https://github.com/Azure/MachineLearningNotebooks/tree/master/how-to-use-azureml/automated-machine-learning/classification-bank-marketing"" rel=""nofollow noreferrer"">https://github.com/Azure/MachineLearningNotebooks/tree/master/how-to-use-azureml/automated-machine-learning/classification-bank-marketing</a></p>

<p>I changed the code there to feed it with my data, but I am getting the following error when executing the autoML run: </p>

<p>automl.client.core.common.exceptions.DataprepException: Could not execute the specified transform.</p>

<p>coming from the:
  File ""/azureml-envs/azureml_e9e27206cd19de471f4e5c7a1171037e/lib/python3.6/site-packages/azureml/automl/core/dataprep_utilities.py"", line 50, in try_retrieve_pandas_dataframe_adb</p>

<p>Now, I thought there is sth. wrong with my data, but then I performed the following experiment with the original csv file:</p>

<p>1-st execution as in the Github example, building the dataflow directly based on the http link
2-nd execution building the dataflow based on the same csv, but downloaded to my share.</p>

<p>In the second case I got the same error as with my data. This would mean, that the Azure autoML run / dataflow / preparation process accepts only specific file format, which got changed when saving to my drive. 
I am not sure if this is about encoding or anything else.
Could you please advice?</p>

<pre class=""lang-py prettyprint-override""><code>########################################
#Case 1, Error returned

data= ""\\\dwdf219\\...\\bankmarketing_train.csv""
dflow = dprep.auto_read_file(data)
dflow.get_profile()
X_train = dflow.drop_columns(columns=['y'])
y_train = dflow.keep_columns(columns=['y'], validate_column_exists=True)
dflow.head()

# Train
automl_settings = {
    ""iteration_timeout_minutes"": 10,
    ""iterations"": 5,
    ""n_cross_validations"": 2,
    ""primary_metric"": 'AUC_weighted',
    ""preprocess"": True,
    ""max_concurrent_iterations"": 5,
    ""verbosity"": logging.INFO,
}

automl_config = AutoMLConfig(task = 'classification',
                             debug_log = 'automl_errors.log',
                             path = project_folder,
                             run_configuration=conda_run_config,
                             X = X_train,
                             y = y_train,
                             **automl_settings
                            )     

remote_run = experiment.submit(automl_config, show_output = True)


########################################
#Case 2, all works fine

data = ""https://automlsamplenotebookdata.blob.core.windows.net/automl-sample-notebook-data/bankmarketing_train.csv""
dflow = dprep.auto_read_file(data)
dflow.get_profile()
X_train = dflow.drop_columns(columns=['y'])
y_train = dflow.keep_columns(columns=['y'], validate_column_exists=True)
dflow.head()

# Train ...
################################### 
</code></pre>",1,0,2019-07-18 14:04:38.393000 UTC,,2019-07-19 20:55:00.703000 UTC,0,python|azure|machine-learning|automl|azure-machine-learning-service,334,2019-02-18 10:37:02.630000 UTC,2020-02-03 10:49:34.887000 UTC,,1,0,0,13,,,,,,['azure-machine-learning-service']
MLFlow deployment example,<p>I have  models  create  I want learn to deploy   ML Flow  model on production. can I get setp by sep  tutorial  whee  I can deply model.on my PC [assuming it to production env]</p>,0,2,2019-11-26 12:41:53.833000 UTC,,,0,deep-learning|artificial-intelligence|mlflow,70,2019-04-11 19:38:54.987000 UTC,2022-03-22 12:45:03.740000 UTC,,1,0,0,9,,,,,,['mlflow']
Does mlflow support spacy model serving/ life-cycle management,"<p>How much does MLflow support for spaCy model lifecycle management?</p>
<p>SpaCy model building <a href=""https://github.com/mlflow/mlflow/tree/master/examples/spacy"" rel=""nofollow noreferrer"">example</a> is given here.</p>
<p>But model serving is failing and showing below error:
<a href=""https://i.stack.imgur.com/04InD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/04InD.png"" alt=""enter image description here"" /></a></p>",0,0,2022-05-24 19:34:18.453000 UTC,,2022-05-25 07:38:55.820000 UTC,0,spacy|cicd|mlflow,75,2017-05-23 20:48:23.257000 UTC,2022-09-24 23:59:25.120000 UTC,,11,0,0,11,,,,,,['mlflow']
"AWS Sagemaker Studio: Tensorboard fails to launch with ""500: Internal Service Error"" message","<p>While training a model on Sagemaker Studio, I wanted to track some scalars using tensorboard but ran into a jupyter error when trying to proxy to the tensorboard hosted by the sagemaker terminal. I followed these steps:</p>
<ol>
<li>Opened terminal and ran <code>pip install tensorboard</code></li>
<li>Executed tensorboard with <code>tensorboard --logdir tb_logs/</code></li>
<li>Copy pasted sagemaker URL and replaced last bit with <code>/proxy/6006/</code></li>
</ol>
<p>Why is it failing to launch tensorboard on sagemaker?</p>
<p>Python version: <code>3.7.10</code>;
Tensorboard version: <code>2.5.0</code>;
Tensorflow version: <code>2.3.2</code></p>",0,2,2021-06-05 21:44:33.167000 UTC,0.0,2021-06-10 22:17:42.920000 UTC,1,amazon-web-services|tensorboard|amazon-sagemaker,339,2020-08-25 17:40:45.400000 UTC,2022-09-23 20:39:27.060000 UTC,,111,3,0,6,,,,,,['amazon-sagemaker']
"SageMaker Studio PyTorch 1.8 kernel has no PyTorch, Numpy, or Matplotlib module","<p>I'm working with SageMaker studio with the following options:</p>
<ul>
<li>kernel: PyTorch 1.8 Python 3.6 GPU optimized.</li>
<li>instance: ml.g4dn.xlarge</li>
</ul>
<p>When running <code>import torch</code> <code>numpy</code>, <code>matplotlib</code> or <code>PIL</code>, I'm getting the <code>No module named 'X'</code> error. No matter when using <code>pip install</code> in a cell above, it will not be imported. Is this a problem only I am encountering with the new PyTorch 1.8 kernel? It also happens with the CPU-optimized version. However, PyTorch 1.6 kernel does not throw an error.</p>
<p>When running <code>conda list</code>, I get the following output:</p>
<pre><code># packages in environment at /opt/conda:
#
# Name                    Version                   Build  Channel
_libgcc_mutex             0.1                        main  
_openmp_mutex             4.5                       1_gnu  
apex                      0.1                      pypi_0    pypi
argon2-cffi               21.1.0                   pypi_0    pypi
argparse                  1.4.0                    pypi_0    pypi
async-generator           1.10                     pypi_0    pypi
attrs                     21.2.0                   pypi_0    pypi
autovizwidget             0.19.1                   pypi_0    pypi
aws-cdk-assets            1.128.0                  pypi_0    pypi
aws-cdk-aws-apigateway    1.128.0                  pypi_0    pypi
aws-cdk-aws-applicationautoscaling 1.128.0                  pypi_0    pypi
aws-cdk-aws-autoscaling   1.128.0                  pypi_0    pypi
aws-cdk-aws-autoscaling-common 1.128.0                  pypi_0    pypi
aws-cdk-aws-autoscaling-hooktargets 1.128.0                  pypi_0    pypi
aws-cdk-aws-batch         1.128.0                  pypi_0    pypi
aws-cdk-aws-certificatemanager 1.128.0                  pypi_0    pypi
aws-cdk-aws-cloudformation 1.128.0                  pypi_0    pypi
aws-cdk-aws-cloudfront    1.128.0                  pypi_0    pypi
aws-cdk-aws-cloudwatch    1.128.0                  pypi_0    pypi
aws-cdk-aws-codebuild     1.128.0                  pypi_0    pypi
aws-cdk-aws-codecommit    1.128.0                  pypi_0    pypi
aws-cdk-aws-codeguruprofiler 1.128.0                  pypi_0    pypi
aws-cdk-aws-codestarnotifications 1.128.0                  pypi_0    pypi
aws-cdk-aws-cognito       1.128.0                  pypi_0    pypi
aws-cdk-aws-dynamodb      1.128.0                  pypi_0    pypi
aws-cdk-aws-ec2           1.128.0                  pypi_0    pypi
aws-cdk-aws-ecr           1.128.0                  pypi_0    pypi
aws-cdk-aws-ecr-assets    1.128.0                  pypi_0    pypi
aws-cdk-aws-ecs           1.128.0                  pypi_0    pypi
aws-cdk-aws-efs           1.128.0                  pypi_0    pypi
aws-cdk-aws-elasticloadbalancing 1.128.0                  pypi_0    pypi
aws-cdk-aws-elasticloadbalancingv2 1.128.0                  pypi_0    pypi
aws-cdk-aws-events        1.128.0                  pypi_0    pypi
aws-cdk-aws-fsx           1.128.0                  pypi_0    pypi
aws-cdk-aws-globalaccelerator 1.128.0                  pypi_0    pypi
aws-cdk-aws-iam           1.128.0                  pypi_0    pypi
aws-cdk-aws-imagebuilder  1.128.0                  pypi_0    pypi
aws-cdk-aws-kinesis       1.128.0                  pypi_0    pypi
aws-cdk-aws-kms           1.128.0                  pypi_0    pypi
aws-cdk-aws-lambda        1.128.0                  pypi_0    pypi
aws-cdk-aws-logs          1.128.0                  pypi_0    pypi
aws-cdk-aws-route53       1.128.0                  pypi_0    pypi
aws-cdk-aws-route53-targets 1.128.0                  pypi_0    pypi
aws-cdk-aws-s3            1.128.0                  pypi_0    pypi
aws-cdk-aws-s3-assets     1.128.0                  pypi_0    pypi
aws-cdk-aws-sam           1.128.0                  pypi_0    pypi
aws-cdk-aws-secretsmanager 1.128.0                  pypi_0    pypi
aws-cdk-aws-servicediscovery 1.128.0                  pypi_0    pypi
aws-cdk-aws-signer        1.128.0                  pypi_0    pypi
aws-cdk-aws-sns           1.128.0                  pypi_0    pypi
aws-cdk-aws-sns-subscriptions 1.128.0                  pypi_0    pypi
aws-cdk-aws-sqs           1.128.0                  pypi_0    pypi
aws-cdk-aws-ssm           1.128.0                  pypi_0    pypi
aws-cdk-cloud-assembly-schema 1.128.0                  pypi_0    pypi
aws-cdk-core              1.128.0                  pypi_0    pypi
aws-cdk-custom-resources  1.128.0                  pypi_0    pypi
aws-cdk-cx-api            1.128.0                  pypi_0    pypi
aws-cdk-region-info       1.128.0                  pypi_0    pypi
aws-parallelcluster       3.0.0                    pypi_0    pypi
awscli                    1.20.63                  pypi_0    pypi
awsio                     0.0.1                    pypi_0    pypi
backcall                  0.2.0                      py_0    anaconda
bcrypt                    3.2.0                    pypi_0    pypi
beautifulsoup4            4.10.0                   pypi_0    pypi
blas                      1.0                    openblas    anaconda
bleach                    4.1.0                    pypi_0    pypi
blis                      0.7.4                    pypi_0    pypi
bokeh                     2.3.3                    pypi_0    pypi
boto3                     1.18.63                  pypi_0    pypi
botocore                  1.21.63                  pypi_0    pypi
bottleneck                1.3.2                    pypi_0    pypi
brotlipy                  0.7.0           py36h8f6f2f9_1001    conda-forge
bzip2                     1.0.8                h7f98852_4    conda-forge
ca-certificates           2021.9.30            h06a4308_1  
cairo                     1.16.0            h18b612c_1001    conda-forge
catalogue                 2.0.6                    pypi_0    pypi
cattrs                    1.0.0                    pypi_0    pypi
certifi                   2021.5.30        py36h06a4308_0  
cffi                      1.14.6           py36hc120d54_0    conda-forge
chardet                   4.0.0            py36h5fab9bb_1    conda-forge
charset-normalizer        2.0.4              pyhd3eb1b0_0  
click                     8.0.3                    pypi_0    pypi
clickclick                20.10.2                  pypi_0    pypi
cloudpickle               2.0.0                    pypi_0    pypi
cmake                     3.18.2.post1             pypi_0    pypi
colorama                  0.4.3                    pypi_0    pypi
conda                     4.10.3           py36h06a4308_0  
conda-package-handling    1.7.3            py36h8f6f2f9_0    conda-forge
connexion                 2.7.0                    pypi_0    pypi
constructs                3.3.161                  pypi_0    pypi
contextvars               2.4                      pypi_0    pypi
cryptography              35.0.0           py36hb60f036_0    conda-forge
cycler                    0.10.0                   pypi_0    pypi
cymem                     2.0.5                    pypi_0    pypi
cython                    0.29.21          py36he6710b0_0    anaconda
dataclasses               0.8                      pypi_0    pypi
decorator                 4.4.2                      py_0    anaconda
defusedxml                0.7.1                    pypi_0    pypi
dgl-cuda11.1              0.6.1                    py36_0    dglteam
dill                      0.3.4                    pypi_0    pypi
docutils                  0.15.2                   pypi_0    pypi
entrypoints               0.3                      pypi_0    pypi
fastai                    1.0.61                   pypi_0    pypi
fastprogress              1.0.0                    pypi_0    pypi
ffmpeg                    4.0                  hcdf2ecd_0  
filelock                  3.3.1                    pypi_0    pypi
flask                     2.0.2                    pypi_0    pypi
fontconfig                2.13.1            hba837de_1005    conda-forge
freeglut                  3.2.1                h9c3ff4c_2    conda-forge
freetype                  2.10.4               h0708190_1    conda-forge
fsspec                    2021.10.1                pypi_0    pypi
future                    0.18.2                   py36_1    anaconda
gevent                    21.8.0                   pypi_0    pypi
glib                      2.69.1               h5202010_0  
google-pasta              0.2.0                    pypi_0    pypi
graphite2                 1.3.13            h58526e2_1001    conda-forge
greenlet                  1.1.2                    pypi_0    pypi
h5py                      2.8.0            py36h989c5e5_3  
harfbuzz                  1.8.8                hffaf4a1_0  
hdf5                      1.10.2               hc401514_3    conda-forge
hdijupyterutils           0.19.1                   pypi_0    pypi
horovod                   0.21.3                   pypi_0    pypi
icu                       58.2              hf484d3e_1000    conda-forge
idna                      2.10               pyhd3eb1b0_0  
imageio                   2.9.0                    pypi_0    pypi
immutables                0.16                     pypi_0    pypi
importlib-metadata        4.8.1                    pypi_0    pypi
importlib-resources       5.2.2                    pypi_0    pypi
inflection                0.5.1                    pypi_0    pypi
inotify-simple            1.2.1                    pypi_0    pypi
intel-openmp              2020.2                      254    anaconda
ipykernel                 5.5.6                    pypi_0    pypi
ipython                   7.16.1           py36h5ca1d4c_0    anaconda
ipython_genutils          0.2.0                    py36_0    anaconda
ipywidgets                7.6.5                    pypi_0    pypi
isodate                   0.6.0                    pypi_0    pypi
itsdangerous              2.0.1                    pypi_0    pypi
jasper                    2.0.14               hd8c5072_2  
jedi                      0.18.0           py36h06a4308_1  
jinja2                    3.0.2                    pypi_0    pypi
jmespath                  0.10.0                   pypi_0    pypi
joblib                    1.0.1              pyhd3eb1b0_0  
jpeg                      9d                   h36c2ea0_0    conda-forge
jsii                      1.39.0                   pypi_0    pypi
jsonpatch                 1.32                     pypi_0    pypi
jsonpointer               2.1                      pypi_0    pypi
jsonschema                3.2.0                    pypi_0    pypi
jupyter                   1.0.0                    pypi_0    pypi
jupyter-client            7.0.6                    pypi_0    pypi
jupyter-console           6.4.0                    pypi_0    pypi
jupyter-core              4.8.1                    pypi_0    pypi
jupyterlab-pygments       0.1.2                    pypi_0    pypi
jupyterlab-widgets        1.0.2                    pypi_0    pypi
kiwisolver                1.3.1                    pypi_0    pypi
ld_impl_linux-64          2.35.1               h7274673_9  
libffi                    3.3                  he6710b0_2  
libgcc                    7.2.0                h69d50b8_2  
libgcc-ng                 9.3.0               h5101ec6_17  
libgfortran               3.0.0                         1    conda-forge
libgfortran-ng            7.3.0                hdf63c60_0    anaconda
libglu                    9.0.0             he1b5a44_1001    conda-forge
libgomp                   9.3.0               h5101ec6_17  
libopenblas               0.3.10               h5a2b251_0    anaconda
libopencv                 3.4.2                hb342d67_1  
libopus                   1.3.1                h7f98852_1    conda-forge
libpng                    1.6.37               h21135ba_2    conda-forge
libstdcxx-ng              9.3.0               hd4cf53a_17  
libtiff                   4.0.10            hc3755c2_1005    conda-forge
libuuid                   2.32.1            h7f98852_1000    conda-forge
libvpx                    1.7.0                h439df22_0  
libxcb                    1.13              h7f98852_1003    conda-forge
libxml2                   2.9.12               h03d6c58_0  
llvmlite                  0.36.0                   pypi_0    pypi
lz4-c                     1.9.3                h9c3ff4c_1    conda-forge
magma-cuda111             2.5.2                         1    pytorch
markupsafe                2.0.1                    pypi_0    pypi
marshmallow               3.13.0                   pypi_0    pypi
matplotlib                3.3.4                    pypi_0    pypi
mistune                   0.8.4                    pypi_0    pypi
mkl                       2020.2                      256    anaconda
mkl-include               2020.2                      256    anaconda
mock                      4.0.3                    pypi_0    pypi
mpi4py                    3.0.3                    pypi_0    pypi
multiprocess              0.70.12.2                pypi_0    pypi
murmurhash                1.0.5                    pypi_0    pypi
nbclient                  0.5.4                    pypi_0    pypi
nbconvert                 6.0.7                    pypi_0    pypi
nbformat                  5.1.3                    pypi_0    pypi
ncurses                   6.2                  he6710b0_1  
nest-asyncio              1.5.1                    pypi_0    pypi
networkx                  2.5.1              pyhd3eb1b0_0  
nose                      1.3.7                    pypi_0    pypi
notebook                  6.4.4                    pypi_0    pypi
numba                     0.53.1                   pypi_0    pypi
numexpr                   2.7.3                    pypi_0    pypi
numpy                     1.19.1           py36h30dfecb_0    anaconda
numpy-base                1.19.1           py36h75fe3a5_0    anaconda
nvidia-ml-py3             7.352.0                  pypi_0    pypi
openapi-schema-validator  0.1.5                    pypi_0    pypi
openapi-spec-validator    0.3.1                    pypi_0    pypi
opencv                    3.4.2            py36h6fd60c2_1  
opencv-python             4.5.3.56                 pypi_0    pypi
openssl                   1.1.1l               h7f8727e_0  
packaging                 21.0                     pypi_0    pypi
pandas                    1.1.5            py36ha9443f7_0  
pandocfilters             1.5.0                    pypi_0    pypi
paramiko                  2.8.0                    pypi_0    pypi
parso                     0.8.0                      py_0    anaconda
pathos                    0.2.8                    pypi_0    pypi
pathy                     0.6.0                    pypi_0    pypi
pcre                      8.45                 h9c3ff4c_0    conda-forge
pexpect                   4.8.0                    py36_0    anaconda
pickleshare               0.7.5                    py36_0    anaconda
pillow                    8.3.2                    pypi_0    pypi
pip                       21.3               pyhd8ed1ab_0    conda-forge
pixman                    0.38.0            h516909a_1003    conda-forge
plotly                    5.3.1                    pypi_0    pypi
pox                       0.3.0                    pypi_0    pypi
ppft                      1.6.6.4                  pypi_0    pypi
preshed                   3.0.5                    pypi_0    pypi
progress                  1.6                      pypi_0    pypi
prometheus-client         0.11.0                   pypi_0    pypi
prompt-toolkit            3.0.8                      py_0    anaconda
protobuf                  3.18.1                   pypi_0    pypi
protobuf3-to-dict         0.1.5                    pypi_0    pypi
psutil                    5.8.0                    pypi_0    pypi
pthread-stubs             0.4               h36c2ea0_1001    conda-forge
ptyprocess                0.6.0                    py36_0    anaconda
publication               0.0.3                    pypi_0    pypi
pure-sasl                 0.6.2                    pypi_0    pypi
py-opencv                 3.4.2            py36hb342d67_1  
pyarrow                   5.0.0                    pypi_0    pypi
pyasn1                    0.4.8                    pypi_0    pypi
pybind11                  2.8.0                    pypi_0    pypi
pycosat                   0.6.3           py36h8f6f2f9_1006    conda-forge
pycparser                 2.20                       py_2  
pydantic                  1.8.2                    pypi_0    pypi
pyfunctional              1.4.3                    pypi_0    pypi
pygments                  2.7.1                      py_0    anaconda
pyhive                    0.6.4                    pypi_0    pypi
pyinstrument              3.4.2                    pypi_0    pypi
pyinstrument-cext         0.2.4                    pypi_0    pypi
pykerberos                1.2.1                    pypi_0    pypi
pynacl                    1.4.0                    pypi_0    pypi
pyopenssl                 19.1.0                     py_1    anaconda
pyparsing                 2.4.7                    pypi_0    pypi
pyrsistent                0.18.0                   pypi_0    pypi
pysocks                   1.7.1            py36h5fab9bb_3    conda-forge
python                    3.6.13          hffdb5ce_0_cpython    conda-forge
python-dateutil           2.8.2              pyhd3eb1b0_0  
python_abi                3.6                     2_cp36m    conda-forge
pytz                      2021.3             pyhd3eb1b0_0  
pyyaml                    5.4.1                    pypi_0    pypi
pyzmq                     22.3.0                   pypi_0    pypi
qtconsole                 5.1.1                    pypi_0    pypi
qtpy                      1.11.2                   pypi_0    pypi
readline                  8.1                  h27cfd23_0  
requests                  2.26.0             pyhd3eb1b0_0  
requests-kerberos         0.12.0                   pypi_0    pypi
retrying                  1.3.3                    pypi_0    pypi
rsa                       4.7.2                    pypi_0    pypi
ruamel_yaml               0.15.100         py36h27cfd23_0  
s3fs                      0.4.2                    pypi_0    pypi
s3transfer                0.5.0                    pypi_0    pypi
sagemaker                 2.63.1                   pypi_0    pypi
sagemaker-experiments     0.1.35                   pypi_0    pypi
sagemaker-pytorch-training 2.4.0                    pypi_0    pypi
sagemaker-studio-analytics-extension 0.0.2                    pypi_0    pypi
sagemaker-studio-sparkmagic-lib 0.1.3                    pypi_0    pypi
sagemaker-training        3.9.2                    pypi_0    pypi
sasl                      0.3.1                    pypi_0    pypi
scikit-learn              0.24.2           py36ha9443f7_0  
scipy                     1.5.4                    pypi_0    pypi
seaborn                   0.11.2                   pypi_0    pypi
send2trash                1.8.0                    pypi_0    pypi
setuptools                49.6.0           py36h5fab9bb_3    conda-forge
shap                      0.39.0                   pypi_0    pypi
six                       1.16.0             pyhd3eb1b0_0  
sklearn                   0.0                      pypi_0    pypi
slicer                    0.0.7                    pypi_0    pypi
smart-open                5.2.1                    pypi_0    pypi
smclarify                 0.2                      pypi_0    pypi
smdebug                   1.0.9                    pypi_0    pypi
smdebug-rulesconfig       1.0.1                    pypi_0    pypi
smdistributed-dataparallel 1.2.0                    pypi_0    pypi
smdistributed-modelparallel 1.3.1                    pypi_0    pypi
soupsieve                 2.2.1                    pypi_0    pypi
spacy                     3.1.3                    pypi_0    pypi
spacy-legacy              3.0.8                    pypi_0    pypi
sparkmagic                0.19.1                   pypi_0    pypi
sqlite                    3.36.0               hc218d9a_0  
srsly                     2.4.1                    pypi_0    pypi
tabulate                  0.8.9                    pypi_0    pypi
tenacity                  8.0.1                    pypi_0    pypi
terminado                 0.12.1                   pypi_0    pypi
testpath                  0.5.0                    pypi_0    pypi
thinc                     8.0.10                   pypi_0    pypi
threadpoolctl             2.2.0              pyh0d69192_0  
thrift                    0.15.0                   pypi_0    pypi
thrift-sasl               0.4.3                    pypi_0    pypi
tk                        8.6.10               hbc83047_0  
torch                     1.8.1                    pypi_0    pypi
torchfile                 0.1.0                    pypi_0    pypi
torchnet                  0.0.4                    pypi_0    pypi
torchvision               0.9.1                    pypi_0    pypi
tornado                   6.1                      pypi_0    pypi
tqdm                      4.61.2             pyhd3eb1b0_1  
traitlets                 4.3.3                    py36_0    anaconda
typer                     0.4.0                    pypi_0    pypi
typing                    3.7.4.3                  py36_0    anaconda
typing-extensions         3.10.0.2                 pypi_0    pypi
tzdata                    2021a                h52ac0ba_0  
urllib3                   1.26.6             pyhd3eb1b0_1  
visdom                    0.1.8.9                  pypi_0    pypi
wasabi                    0.8.2                    pypi_0    pypi
wcwidth                   0.2.5                      py_0    anaconda
webencodings              0.5.1                    pypi_0    pypi
websocket-client          1.2.1                    pypi_0    pypi
werkzeug                  2.0.2                    pypi_0    pypi
wheel                     0.36.2             pyhd3eb1b0_0  
widgetsnbextension        3.5.1                    pypi_0    pypi
xorg-fixesproto           5.0               h7f98852_1002    conda-forge
xorg-inputproto           2.3.2             h7f98852_1002    conda-forge
xorg-kbproto              1.0.7             h7f98852_1002    conda-forge
xorg-libice               1.0.10               h7f98852_0    conda-forge
xorg-libsm                1.2.3             hd9c2040_1000    conda-forge
xorg-libx11               1.7.2                h7f98852_0    conda-forge
xorg-libxau               1.0.9                h7f98852_0    conda-forge
xorg-libxdmcp             1.1.3                h7f98852_0    conda-forge
xorg-libxext              1.3.4                h7f98852_1    conda-forge
xorg-libxfixes            5.0.3             h7f98852_1004    conda-forge
xorg-libxi                1.7.10               h7f98852_0    conda-forge
xorg-libxrender           0.9.10            h7f98852_1003    conda-forge
xorg-renderproto          0.11.1            h7f98852_1002    conda-forge
xorg-xextproto            7.3.0             h7f98852_1002    conda-forge
xorg-xproto               7.0.31            h7f98852_1007    conda-forge
xz                        5.2.5                h7b6447c_0  
yaml                      0.2.5                h7b6447c_0  
zipp                      3.6.0                    pypi_0    pypi
zlib                      1.2.11               h7b6447c_3  
zope-event                4.5.0                    pypi_0    pypi
zope-interface            5.4.0                    pypi_0    pypi
zstd                      1.4.9                ha95c52a_0    conda-forge
</code></pre>",1,1,2021-12-09 14:15:41.363000 UTC,,,2,amazon-web-services|amazon-sagemaker,309,2020-10-06 13:43:35.160000 UTC,2022-05-12 20:27:49.433000 UTC,,51,4,0,12,,,,,,['amazon-sagemaker']
"Model.get_model_path(model_name=""model"") throws an error: Model not found in cache or in root at","<p>I have a Model that I registered it with a pipeline :</p>

<pre><code>register_step = PythonScriptStep(name = ""Register Model"",
                                source_directory = training_folder,
                                script_name = ""register_model.py"",
                                arguments = ['--model_folder', model_folder],
                                inputs=[model_folder],
                                compute_target = pipeline_cluster,
                                runconfig = pipeline_run_config,
                                allow_reuse = True)
</code></pre>

<p>And here is my register_model.py:</p>

<pre><code>import argparse
import joblib
from azureml.core import Workspace, Model, Run

# Get parameters
parser = argparse.ArgumentParser()
parser.add_argument('--model_folder', type=str, dest='model_folder', default=""model"", help='model location')
args = parser.parse_args()
model_folder = args.model_folder

# Get the experiment run context
run = Run.get_context()

# Load the model
print(""Loading model from "" + model_folder)
model_file = model_folder + ""/model.pkl""
model = joblib.load(model_file)

Model.register(workspace=run.experiment.workspace,
               model_path = model_file,
               model_name = 'model',
               tags={'Training context':'Pipeline'})

run.complete() 
</code></pre>

<p>I can see the model is registered when i loop the existing models using the following:</p>

<pre><code>from azureml.core import Model

for model in Model.list(ws):
    print(model.name, 'version:', model.version)
    for tag_name in model.tags:
        tag = model.tags[tag_name]
        print ('\t',tag_name, ':', tag)
    for prop_name in model.properties:
        prop = model.properties[prop_name]
        print ('\t',prop_name, ':', prop)
    print('\n')
</code></pre>

<p>However when i try to load the model in my Score.py (below) to deploy the model as a servcie, I get the following error:</p>

<p>I believe this is where the error coming from :</p>

<pre><code>model_path = Model.get_model_path(
        model_name=""model"", version=1)
</code></pre>

<p>Error:</p>

<pre><code>ModelNotFoundException                    Traceback (most recent call last)
/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azureml/core/model.py in get_model_path(model_name, version, _workspace)
    751             try:
--&gt; 752                 return Model._get_model_path_local(model_name, version)
    753             except ModelNotFoundException as ee:

/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azureml/core/model.py in _get_model_path_local(model_name, version)
    783         if not os.path.exists(candidate_model_path):
--&gt; 784             return Model._get_model_path_local_from_root(model_name)
    785         else:

/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azureml/core/model.py in _get_model_path_local_from_root(model_name)
    826         raise ModelNotFoundException(""Model not found in cache or in root at ./{}. For more info,""
--&gt; 827                                      ""set logging level to DEBUG."".format(candidate_model_path))
    828 

ModelNotFoundException: ModelNotFoundException:
    Message: Model not found in cache or in root at ./model. For more info,set logging level to DEBUG.
    InnerException None
    ErrorResponse 
{
    ""error"": {
        ""message"": ""Model not found in cache or in root at ./model. For more info,set logging level to DEBUG.""
    }
}

During handling of the above exception, another exception occurred:

WebserviceException                       Traceback (most recent call last)
&lt;ipython-input-6-27e8df94d66f&gt; in &lt;module&gt;
      1 model_path = Model.get_model_path(
----&gt; 2         model_name=""model"", version=1)

/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azureml/core/model.py in get_model_path(model_name, version, _workspace)
    756                     module_logger.debug(""Getting model from remote"")
    757                     return Model._get_model_path_remote(model_name, version, active_workspace)
--&gt; 758                 raise WebserviceException(ee.message, logger=module_logger)
    759         else:
    760             if active_workspace is not None:

WebserviceException: WebserviceException:
    Message: Model not found in cache or in root at ./model. For more info,set logging level to DEBUG.
    InnerException None
    ErrorResponse 
{
    ""error"": {
        ""message"": ""Model not found in cache or in root at ./model. For more info,set logging level to DEBUG.""
    }
}
</code></pre>",2,0,2020-03-25 00:25:16.300000 UTC,1.0,,5,python|machine-learning|azure-machine-learning-service,2984,2015-04-15 15:30:33.927000 UTC,2021-05-19 18:39:41.463000 UTC,,261,1,0,16,,,,,,['azure-machine-learning-service']
Sagemaker MultiModel Endpoint Custom Entrypoint Script,"<p>Im working on a multimodel endpoint for a few tensorflow models leveraging this walkthrough:</p>
<p><a href=""https://github.com/aws-samples/sagemaker-multi-model-endpoint-tensorflow-computer-vision/blob/main/multi-model-endpoint-tensorflow-cv.ipynb"" rel=""nofollow noreferrer"">https://github.com/aws-samples/sagemaker-multi-model-endpoint-tensorflow-computer-vision/blob/main/multi-model-endpoint-tensorflow-cv.ipynb</a></p>
<p>My code looks something like this:</p>
<pre><code>env = {
    'SAGEMAKER_REQUIREMENTS': 'requirements.txt', # path relative to `source_dir` below.
    'CATEGORY': category
}

model_1 = TensorFlowModel(model_data=model_loc,
                          role=sagemaker_role,
                          framework_version=&quot;2.2.0&quot;,
                          source_dir=&quot;&lt;path to source dir&gt;&quot;,
                          entry_point=&quot;&lt;source dir&gt;/inference.py&quot;,
                          env=env,
                          sagemaker_session=sagemaker_session)

mme = MultiDataModel(name='test-endpoint',
                     model_data_prefix=model_prefix,
                     model=model_1,
                     sagemaker_session=sagemaker_session)

predictor = mme.deploy(instance_type=&quot;ml.r5.xlarge&quot;, 
                       initial_instance_count=1
                      )
</code></pre>
<p>But when I try to invoke this endpoint, the error returned makes it seem like my custom entry point script isn't referenced at all? This works as intended for a single model endpoint. Any examples or documentation for a multi-model endpoint + custom entry point? I can't find much.</p>",0,0,2021-10-14 19:43:28.707000 UTC,,,1,tensorflow|amazon-sagemaker,134,2016-03-04 20:36:10.467000 UTC,2022-09-22 17:34:48.727000 UTC,Denver,246,39,0,24,,,,,,['amazon-sagemaker']
Create recordio file for linear regression,"<p>I'm using AWS Sagemaker to run linear regression on a CSV dataset. I have made some tests, and with my sample dataset that is 10% of the full dataset, the csv file ends up at 1.5 GB in size.</p>

<p>Now I want to run the full dataset, but I'm facing issues with the 15 GB file. When I compress the file with Gzip, it ends up only 20 MB. However, Sagemaker only supports Gzip on ""Protobuf-Recordio"" files. I know I can make Recordio files with im2rec, but it seems to be intended for image files for image classication. I'm also not sure how to generate the protobuf file.</p>

<p>To make things even worse(?) :) I'm generating the dataset in Node.</p>

<p>I would be very grateful to get some pointers in the right direction how to do this.</p>",1,4,2018-07-26 11:45:26.123000 UTC,1.0,,0,node.js|machine-learning|amazon-sagemaker,1226,2011-12-31 09:32:48.517000 UTC,2018-08-28 16:37:34.490000 UTC,,514,3,0,22,,,,,,['amazon-sagemaker']
Azure ML - AKS Service deployment unable to handle concurrent requests despite auto scaling enabled,"<p>I have deployed around 23 models (amounting to 1.57 GB) in a Azure ML workspace using Azure Kubernetes Service. For the AKS cluster, I have used 3 D8sv3 nodes, and enabled cluster auto scaling for the cluster up to 6 nodes.
The AksWebService is configured with 4.4 cores, 16 GB memory. I have enabled pod auto scaling for the Web service, having set autoscale_max_replicas at 40:</p>
<pre><code>aks_config = AksWebservice.deploy_configuration(cpu_cores = 4.4, memory_gb = 16, autoscale_enabled = True,
                                            description = 'TEST - Configuration for Kubernetes Compute Target',
                                            enable_app_insights = True, max_request_wait_time = 25000,
                                            autoscale_target_utilization = 0.6, autoscale_max_replicas = 40)
</code></pre>
<p>I tried running load tests with 10 concurrent users (using JMeter). And I monitored the cluster application insights:
<a href=""https://i.stack.imgur.com/Aw6QK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Aw6QK.png"" alt=""enter image description here"" /></a>
<a href=""https://i.stack.imgur.com/zu8g5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zu8g5.png"" alt=""enter image description here"" /></a></p>
<p>I can see the nodes and pods scaling. However, there is no spike in CPU/memory utilization. For 10 concurrent requests, only 5 to 6 requests pass, the rest fail. When I send an individual request to the deployed endpoint, the response is generated in 7 to 9 seconds. However, in the load test logs, there are plenty requests taking more than 15 seconds to generate a response. And the requests taking more than 25 seconds, fail with status code 503. I increased the <code>max_request_wait_time</code> due to this reason, however, I don't understand why it would take so much time despite such amount of compute, and the dashboard shows that memory isn't even 30% utilized. Should I be changing the <code>replica_max_concurrent_requests</code> param? Or should I be increasing the <code>autoscale_max_replicas</code> even more? Concurrent requests load may sometimes reach 100 in production, is there any solution to this?</p>
<p>Will be grateful for any advice. Thanks.</p>",0,2,2020-10-01 11:04:48.777000 UTC,,,1,azure|kubernetes|azure-aks|azure-machine-learning-service|azureml-python-sdk,157,2019-04-16 14:13:43.330000 UTC,2022-09-22 15:58:50.683000 UTC,"Bhubaneswar, Odisha, India",328,31,4,53,,,,,,['azure-machine-learning-service']
Training & Deploying SageMaker ML Models using AWS Lambda (NodeJS),"<p>I am using AWS Lambda (NodeJS) for creating a sagemaker training job and deploy it using the Sagemaker Javascript SDK.</p>

<p>I am following the below AWS JavaScript SDK docs</p>

<p><a href=""https://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/SageMaker.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/SageMaker.html</a></p>

<p>I am using the below script for creating the Training job.</p>

<pre><code>Create Training Job:
=====================

    let TrainingJobName = 'Training-' + curr_date_time
    let TrainingImage   = 'XXXXXX.dkr.ecr.us-east-1.amazonaws.com/xxxx:latest'
    let S3Uri           = 's3://xxx.xxxx.sagemaker/csv'

    console.log(`TrainingJobName: ${TrainingJobName}`);

    let params = {
        AlgorithmSpecification: { /* required */
            TrainingInputMode: 'File', /* required */
            TrainingImage: TrainingImage
        },
        OutputDataConfig: { /* required */
            S3OutputPath: 's3://xxx.xxxx.sagemaker/xxxx/output', /* required */
        },
        ResourceConfig: { /* required */
            InstanceCount: 1, /* required */
            InstanceType: 'ml.m4.xlarge', /* required */
            VolumeSizeInGB: 1, /* required */
        },
        RoleArn: 'arn:aws:iam::xxxxx:role/service-role/AmazonSageMaker-ExecutionRole-xxxx', /* required */
        StoppingCondition: { /* required */
            MaxRuntimeInSeconds: 86400
        },
        TrainingJobName: TrainingJobName, /* required */
        InputDataConfig: [
            {
                ChannelName: 'training', /* required */
                DataSource: { /* required */
                    S3DataSource: {
                        S3DataType: 'S3Prefix', /* required */
                        S3Uri: S3Uri, /* required */
                        S3DataDistributionType: 'FullyReplicated'
                    }
                },
                CompressionType: null,
                ContentType: '',
                RecordWrapperType: null,
            }
        ]
    };

    return await sagemaker.createTrainingJob(params).promise();
</code></pre>

<p>After the training job is created, i query the job status using the sagemaker describeTrainingJob function.
I get the status as ""InProgress""</p>

<p>After that I call the sagemaker waitFor function to wait for the completion of the training job using the below method:</p>

<p><a href=""https://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/SageMaker.html#trainingJobCompletedOrStopped-waiter"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/SageMaker.html#trainingJobCompletedOrStopped-waiter</a></p>

<pre><code>let waitFor_result = await sagemaker.waitFor('trainingJobCompletedOrStopped', {TrainingJobName: training_job_name}).promise();
console.log(`waitFor_result : ${JSON.stringify(waitFor_result)}`);
</code></pre>

<p>I find the sagemaker waitFor creates the second training job before the first training job is completed, and it goes on creating subsequent training jobs with the same job name.</p>

<p><a href=""https://i.stack.imgur.com/J7H4c.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/J7H4c.png"" alt=""enter image description here""></a></p>

<p>I think this is due to the StoppingCondition parameter (MaxRuntimeInSeconds:86400) in the createTrainingJob function.</p>

<p>I want to know if there is any solution which creates a single training job and return the results after the trainining job is completed ?</p>

<p>==========================================================
Update:</p>

<p>I am following the ""Scheduling the training of a SageMaker model with a Lambda function"" <a href=""https://www.youtube.com/watch?v=FJaykbAtGTM"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=FJaykbAtGTM</a>.</p>

<p>I am able to create a training job if i am using the below code in my lambda function.</p>

<pre><code>let training_job_result = await start_model_training();
console.log(`Sagemaker training result : ${JSON.stringify(training_job_result)}`);

let training_job_arn = training_job_result[""TrainingJobArn""];
let training_job_name = training_job_arn.split(""/"")[1];


let desc_training_job = await sagemaker.describeTrainingJob({TrainingJobName: training_job_name}).promise();
let desc_status = desc_training_job[""TrainingJobStatus""];
console.log(`Training job desc_status 1 : ${JSON.stringify(desc_status)}`);
</code></pre>

<p>But I need to wait till the training job is completed and invoke the sagemaker deploy method for creating/updating the endpoint.</p>

<p>If I use the below code then it keeps on creating multiple training jobs and the lambda function never terminates.</p>

<pre><code>let waitFor_result = await sagemaker.waitFor('trainingJobCompletedOrStopped', {TrainingJobName: training_job_name}).promise();
console.log(`waitFor_result : ${JSON.stringify(waitFor_result)}`);


desc_training_job = await sagemaker.describeTrainingJob({TrainingJobName: training_job_name}).promise();
desc_status = desc_training_job[""TrainingJobStatus""];
console.log(`Training job desc_status 2 : ${JSON.stringify(desc_status)}`);
</code></pre>

<p>I want to deploy/update the endpoint once the training is completed.</p>",1,0,2019-07-11 02:38:14.627000 UTC,1.0,2019-07-12 05:36:33.607000 UTC,1,amazon-web-services|aws-lambda|amazon-sagemaker|aws-sdk-js,1288,2009-04-29 11:42:36.853000 UTC,2022-09-18 14:23:33.130000 UTC,"Kuala Lumpur, Malaysia",15794,1343,26,1032,,,,,,['amazon-sagemaker']
"Sagemaker, get spark dataframe from data image url on S3","<p>I am trying to obtain a sparkdataframe which contains the paths and image for all images in my data. The data is store as follow :
folder/image_category/image_n.jpg</p>
<p>I worked on a local jupyter notebook and got no problem with using following code:</p>
<p><code>dataframe = spark.read.format(&quot;image&quot;).load(path)</code></p>
<p>I need to do the same exercise using AWS sagemaker and S3. I created a bucket following the same pattern :
s3://my_bucket/folder/image_category/image_n.jpg</p>
<p>I've tried a lot of possible solutions i found online, based on boto3, s3fs and other stuff, but unfortunately i am still unable to make it work (and i am starting to lose faith ...).</p>
<p>Would anyone have something reliable i could base my work on ?</p>",0,0,2020-12-11 14:58:29.410000 UTC,,2020-12-11 19:20:59.630000 UTC,1,amazon-web-services|apache-spark|amazon-s3|image-processing|amazon-sagemaker,152,2020-12-11 14:40:47.720000 UTC,2021-07-12 16:49:51.613000 UTC,"Yutz, France",11,0,0,1,,,,,,['amazon-sagemaker']
Acessing Tensorboard on AzureML during training,"<p>How to use view Tensorboard during an AzureML training run on in the Cloud.</p>
<p>Followed this tutorial:
<a href=""https://docs.microsoft.com/en-us/azure/machine-learning/how-to-monitor-tensorboard#launch-tensorboard"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/machine-learning/how-to-monitor-tensorboard#launch-tensorboard</a></p>
<pre class=""lang-py prettyprint-override""><code>from azureml.tensorboard import Tensorboard

tb = Tensorboard([run])

# If successful, start() returns a string with the URI of the instance.
tb.start()

# After your job completes, be sure to stop() the streaming otherwise it will continue to run. 
tb.stop()
</code></pre>
<p>The logs contains the url <code>http://localhost:6006/</code> printed by the <code>tb.start()</code> statement.
This is somehow silly as it runs in the cloud.</p>
<p>How to get the full url to the node in the cluster where Tensorboard was launched?</p>",0,0,2020-12-06 09:06:52.760000 UTC,,,1,tensorboard|azure-machine-learning-service,122,2016-09-22 08:48:50.870000 UTC,2022-02-18 09:44:31.300000 UTC,Germany,975,52,0,122,,,,,,['azure-machine-learning-service']
Use WebService input as query parameter in reader module in Azure ML?,"<p>Is it possible to use the parameters incoming from the webservice ( named 'Query' in my case ), in the SQL in the reader module.</p>

<p>I think it's possible, but I cannot find anywhere how to template these properties in the SQL query so that it becomes dynamic.</p>

<p><a href=""https://i.stack.imgur.com/oLWmr.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/oLWmr.png"" alt=""dynamic query""></a></p>",2,0,2015-09-22 19:11:53.010000 UTC,,,4,azure-machine-learning-studio,513,2011-08-16 12:55:41.707000 UTC,2022-08-22 11:50:15.593000 UTC,"Amsterdam, Netherlands",5143,290,3,608,,,,,,['azure-machine-learning-studio']
Python AWS- Unicode error Algorithm Error Caused by: 'ascii' codec can't encode characters in position 1-2: ordinal not in range(128),"<p>I am learning to train and transform using machine learning models on AWS Sagemaker which uses Python 3.6.</p>

<p>I am able to train the Linear Learner model successfully but during transformation, I get an error:</p>

<blockquote>
  <p>UnexpectedStatusException: Error for Transform job linear-learner-2019-08-30-11-22-02-821: Failed. Reason: ClientError: See job logs for more information</p>
</blockquote>

<p>Which maps to this error in the CloudWatch logs</p>

<blockquote>
  <p>Algorithm Error: (caused by UnicodeEncodeError)<br>
   Caused by: 'ascii' codec can't encode characters in position 1-2: ordinal not in range(128)</p>
</blockquote>

<p>The code which I am using is this<br>
<em>To Create and train model</em></p>

<pre><code>import boto3
import sagemaker

sess = sagemaker.Session()

linear = sagemaker.estimator.Estimator(container,
                                       role, 
                                       train_instance_count=1, 
                                       train_instance_type='ml.c4.xlarge',
                                       output_path=output_location,
                                       sagemaker_session=sess)
linear.set_hyperparameters(feature_dim=18,
                           predictor_type='regressor')

linear.fit({'train': s3_train_data})
</code></pre>

<p>To create input and output S3 locations for test data</p>

<pre><code>batch_input ='s3://{}/{}/test/examples'.format(bucket, prefix) # The location of the test dataset   
batch_output = 's3://{}/{}/batch-inference'.format(bucket, prefix) # The location to store the results of the batch transform job

print(batch_input)
print(batch_output)
</code></pre>

<p>Transform test data</p>

<pre><code>housing_test=strat_test_set
housing_test_inputs =full_pipeline.transform (housing_test)
housing_test_inputs=np.float32(housing_test_inputs)
housing_test_labels=strat_test_set['median_house_value'].values
housing_test_labels=np.float32(housing_test_labels)
</code></pre>

<p>To check consistency of number of features</p>

<pre><code>print(housing_test_labels.shape)
print(housing_test_inputs.shape)
print(housing_labels.shape)
print(housing_inputs.shape)
</code></pre>

<p>Values returned from above are</p>

<pre><code>(4128,)
(4128, 18)
(16512,)
(16512, 18)
</code></pre>

<p>Test data upload to S3</p>

<pre><code>buf = io.BytesIO()
smac.write_numpy_to_dense_tensor(buf, housing_test_inputs, housing_test_labels)
buf.seek(0)
key = 'examples'
boto3.resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'test', key)).upload_fileobj(buf)
s3_test_data = 's3://{}/{}/test/{}'.format(bucket, prefix, key)
print('uploaded test data location: {}'.format(s3_test_data))
</code></pre>

<p>Predicting values</p>

<pre><code>transformer = linear.transformer(instance_count=1, instance_type='ml.m4.xlarge', output_path=batch_output)

transformer.transform(data=batch_input, data_type='S3Prefix', content_type='text/csv', split_type='Line')

transformer.wait()
</code></pre>

<p><strong>The error I receive at the console is this-</strong></p>

<blockquote>
  <p>UnexpectedStatusException                 Traceback (most recent call last)
   in ()
        3 transformer.transform(data=batch_input, data_type='S3Prefix', content_type='text/csv', split_type='Line')
        4 
  ----> 5 transformer.wait()</p>
  
  <p>~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/transformer.py in wait(self)
      227         """"""Placeholder docstring""""""
      228         self._ensure_last_transform_job()
  --> 229         self.latest_transform_job.wait()
      230 
      231     def _ensure_last_transform_job(self):</p>
  
  <p>~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/transformer.py in wait(self)
      344 
      345     def wait(self):
  --> 346         self.sagemaker_session.wait_for_transform_job(self.job_name)
      347 
      348     @staticmethod</p>
  
  <p>~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/session.py in wait_for_transform_job(self, job, poll)
     1050         """"""
     1051         desc = _wait_until(lambda: _transform_job_status(self.sagemaker_client, job), poll)
  -> 1052         self._check_job_status(job, desc, ""TransformJobStatus"")
     1053         return desc
     1054 </p>
  
  <p>~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/session.py in _check_job_status(self, job, desc, status_key_name)
     1077                 ),
     1078                 allowed_statuses=[""Completed"", ""Stopped""],
  -> 1079                 actual_status=status,
     1080             )
     1081 </p>
  
  <p>UnexpectedStatusException: Error for Transform job linear-learner-2019-08-30-11-22-02-821: Failed. Reason: ClientError: See job logs for more information</p>
</blockquote>

<p>Can somebody please suggest what is the root cause of this error and how to fix this?</p>",0,2,2019-08-30 11:35:05.797000 UTC,,2019-08-31 15:13:43.113000 UTC,0,python|unicode|character-encoding|amazon-sagemaker,515,2018-01-06 08:37:36.093000 UTC,2020-11-06 22:03:37.057000 UTC,,246,22,1,60,,,,,,['amazon-sagemaker']
Install Tensorflow Object Detection API without replacing existing Tensorflow package,"<p>I'm trying to build a custom container image based on AWS SageMaker 763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-training:2.3.0-gpu-py37-cu102-ubuntu18.04 image and following the instructions in <a href=""https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/install.html#tensorflow-object-detection-api-installation"" rel=""nofollow noreferrer"">https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/install.html#tensorflow-object-detection-api-installation</a></p>
<p>However, it seems that when I run the following commands, <code>pip install</code> replaces the already existing TensorFlow package with <code>tensorflow-2.3.0-cp37-cp37m-manylinux2010_x86_64.whl</code> which doesn't support AWS's CPU instructions and GPU devices.</p>
<pre><code># From within TensorFlow/models/research/
cp object_detection/packages/tf2/setup.py .
python -m pip install .
</code></pre>
<p>How can I install the Object Detection API without replacing existing TensorFlow? I tried <code>python -m pip install --ignore-installed .</code> but it doesn't seem to have any effect.</p>
<p><strong>Update 1:</strong></p>
<p>It seems that the already installed <code>tensorflow</code> for that AWS docker image isn't detected by <code>pip</code> even though it's available in <code>/usr/local/lib/python3.7/site-packages/tensorflow</code>. This is why <code>pip</code> still attempts to install it even with <code>--ignore-installed</code>.</p>
<p>As a workaround, I make a copy of the directory and then replace the newly installed one with it.</p>
<pre><code>mv /usr/local/lib/python3.7/site-packages/tensorflow/ /usr/local/lib/python3.7/site-packages/tensorflow.cpy/
# From within TensorFlow/models/research/
cp object_detection/packages/tf2/setup.py .
python -m pip install .
rm -r /usr/local/lib/python3.7/site-packages/tensorflow/
mv /usr/local/lib/python3.7/site-packages/tensorflow.cpy/ /usr/local/lib/python3.7/site-packages/tensorflow/
</code></pre>",0,1,2020-08-29 18:03:00.467000 UTC,,2020-08-30 02:05:23.360000 UTC,2,python|tensorflow|pip|amazon-sagemaker|object-detection-api,476,2014-10-06 06:06:09.680000 UTC,2022-09-23 08:56:10.567000 UTC,,679,242,11,82,,,,,,['amazon-sagemaker']
"'waitress-serve' is not recognized as an internal or external command,","<p>I try run this mlflow models serve --model-uri runs:/f3393a61d01d4289b16707ed718f23be/log_reg_model -p 1235 script but i got this error</p>
<pre><code>'waitress-serve' is not recognized as an internal or external command,
operable program or batch file.
Traceback (most recent call last):
  File &quot;C:\Users\ahmad\miniconda3\envs\mlflow\Scripts\mlflow-script.py&quot;, line 10, in &lt;module&gt;
    sys.exit(cli())
  File &quot;C:\Users\ahmad\miniconda3\envs\mlflow\lib\site-packages\click\core.py&quot;, line 1128, in __call__
    return self.main(*args, **kwargs)
  File &quot;C:\Users\ahmad\miniconda3\envs\mlflow\lib\site-packages\click\core.py&quot;, line 1053, in main
    rv = self.invoke(ctx)
  File &quot;C:\Users\ahmad\miniconda3\envs\mlflow\lib\site-packages\click\core.py&quot;, line 1659, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File &quot;C:\Users\ahmad\miniconda3\envs\mlflow\lib\site-packages\click\core.py&quot;, line 1659, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File &quot;C:\Users\ahmad\miniconda3\envs\mlflow\lib\site-packages\click\core.py&quot;, line 1395, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File &quot;C:\Users\ahmad\miniconda3\envs\mlflow\lib\site-packages\click\core.py&quot;, line 754, in invoke
    return __callback(*args, **kwargs)
  File &quot;C:\Users\ahmad\miniconda3\envs\mlflow\lib\site-packages\mlflow\models\cli.py&quot;, line 59, in serve
    ).serve(model_uri=model_uri, port=port, host=host, enable_mlserver=enable_mlserver)
  File &quot;C:\Users\ahmad\miniconda3\envs\mlflow\lib\site-packages\mlflow\pyfunc\backend.py&quot;, line 79, in serve
    conda_env_path, command, self._install_mlflow, command_env=command_env
  File &quot;C:\Users\ahmad\miniconda3\envs\mlflow\lib\site-packages\mlflow\pyfunc\backend.py&quot;, line 168, in _execute_in_conda_env
    &quot;Command '{0}' returned non zero return code. Return code = {1}&quot;.format(command, rc)
Exception: Command 'conda activate mlflow-270591f5d4ece78a187f6457a571ae1ce1e4d11f &amp; waitress-serve --host=127.0.0.1 --port=1235 --ident=mlflow mlflow.pyfunc.scoring_server.wsgi:app' returned non zero return code. Return code = 1
</code></pre>",0,1,2022-03-19 17:51:39.403000 UTC,,,0,conda|mlflow|waitress,232,2021-11-10 13:02:12.240000 UTC,2022-09-19 15:50:37.843000 UTC,,1,0,0,3,,,,,,['mlflow']
It is possible to add mlflow experiences notes using code instead of mlflow UI?,"<p>I want to add text to experiences notes without using the mlflow ui
<a href=""https://i.stack.imgur.com/9WPKN.png"" rel=""nofollow noreferrer"">mlflow UI notes example</a></p>

<p>I can't find that method in the docs <a href=""https://mlflow.org/docs/latest/tracking.html"" rel=""nofollow noreferrer"">https://mlflow.org/docs/latest/tracking.html</a>.</p>

<p>Thanks</p>",1,0,2020-03-16 13:59:46.003000 UTC,,,4,mlflow,1404,2018-09-24 11:30:21.677000 UTC,2022-09-22 14:59:01.207000 UTC,"Porto, Portugal",61,19,0,8,,,,,,['mlflow']
Is there way to install zip on sagemakder lab?,"<p>I need a way to download files off the Sagemaker lab notebook, anyone has good suggestions on how to do it? Currently only viable solution seems to be using zipfiles from Python since I can't do apt-get install on sagemaker lab</p>",2,0,2022-06-26 16:02:54.990000 UTC,,,-3,amazon-sagemaker,126,2010-10-04 00:23:33.433000 UTC,2022-06-26 16:50:47.600000 UTC,,1941,11,1,157,,,,,,['amazon-sagemaker']
The inference file that goes into the entry point of PyTorchModel to be deployed does not have an effect to the output of the predictor,"<p>I am currently running the code on AWS Sagemaker, trying to predict data using an already-trained model, accessed by MODEL_URL.</p>
<p>With the code below, the inference.py as the entry_point does not seem to have an effect on the result of the trained prediction model. Any changes in inference.py does not alter the output (the output is always correct). Is there something I am misunderstanding with how the model works? And how can I incorporate inference.py to the prediction model as the entry point?</p>
<pre><code>role = sagemaker.get_execution_role()

model = PyTorchModel(model_data = MODEL_URL, 
                            role = role,
                            framework_version = '0.4.0',
                            entry_point = '/inference.py',
                            source_dir = SOURCE_DIR)

predictor = model.deploy(instance_type = 'ml.c5.xlarge', 
                                   initial_instance_count = 1,
                                   endpoint_name = RT_ENDPOINT_NAME)

result = predictor.predict(someData)
</code></pre>",1,0,2020-07-12 21:28:28.900000 UTC,1.0,,0,pytorch|prediction|amazon-sagemaker,265,2020-07-12 21:17:55.633000 UTC,2021-03-11 21:23:11.753000 UTC,,1,0,0,3,,,,,,['amazon-sagemaker']
How is nginx used in aws sagemaker during deployment?,"<p>How are the number of AWS instances related to the Nginx and Gunicorn workers?</p>

<p>AWS Sagemaker calls the docker container stored in ECR with the serve command. The number of instances and the type are setup in the estimator(<a href=""https://sagemaker.readthedocs.io/en/stable/estimators.html#sagemaker.estimator.EstimatorBase"" rel=""noreferrer"">sage.estimator.Estimator</a>) However, each docker container has a Nginx and Gunicorn setup.</p>

<p>Is a smaller instance launched to do the nginx proxy_pass? Are the requests proxied inside each container at the thread level?</p>

<p><a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-hosting.html"" rel=""noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-hosting.html</a></p>

<p><a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/adv-bring-own-examples.html"" rel=""noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/adv-bring-own-examples.html</a></p>",0,0,2020-04-25 18:08:46.003000 UTC,0.0,2020-04-27 15:42:29.703000 UTC,6,nginx|machine-learning|gunicorn|amazon-sagemaker,596,2016-07-14 00:43:23.417000 UTC,2022-09-14 08:41:23.240000 UTC,,163,152,0,44,,,,,,['amazon-sagemaker']
sagemaker - construct model from tar.gz file,"<p>I have trained a sagemaker model successfully and the <code>model.tar.gz</code> file is on s3.</p>

<p>Now I want to ""reconstruct"" the model from that file and then deploy it. I used the following code:</p>

<pre><code>
containers = {'us-west-2': '174872318107.dkr.ecr.us-west-2.amazonaws.com/factorization-machines:latest',
              'us-east-1': '382416733822.dkr.ecr.us-east-1.amazonaws.com/factorization-machines:latest',
              'us-east-2': '404615174143.dkr.ecr.us-east-2.amazonaws.com/factorization-machines:latest',
              'eu-west-1': '438346466558.dkr.ecr.eu-west-1.amazonaws.com/factorization-machines:latest'}


fm = sagemaker.model.Model(model_s3_path, containers['eu-west-1'], role=sagemaker.get_execution_role())
</code></pre>

<p>I get back an object of type <code>sagemaker.model.Model</code>.</p>

<p>I then seek to deploy the model via</p>

<pre><code>fm_predictor = fm.deploy(instance_type='ml.c4.xlarge', initial_instance_count=1)
</code></pre>

<p>The output of this call is </p>

<pre><code>--------------------------------------------------------------------------------------!
</code></pre>

<p>But this returns a <code>NoneType</code> object that does not have a predict method. However, the model's endpoint is created. </p>

<p>What am I doing wrong?</p>",1,0,2019-12-09 08:09:01.433000 UTC,,,0,amazon-web-services|amazon-sagemaker,381,2016-02-26 11:54:14.210000 UTC,2022-09-24 11:34:28.877000 UTC,"Berlin, Germany",1464,81,9,62,,,,,,['amazon-sagemaker']
AI/ML Assisted labeling in Vertex AI,"<p>Is there a feature in Vertex AI which will allow AI/ML to assist in labeling data? This usually works by providing a small set of labeled data, followed by a model creation which assists in labeling more data. As more and more data is labeled the model keeps getting better.</p>",0,1,2022-08-04 19:48:45.143000 UTC,,,0,google-cloud-vertex-ai,58,2012-05-30 05:48:34.657000 UTC,2022-09-24 23:10:38.117000 UTC,,840,1070,1,196,,,,,,['google-cloud-vertex-ai']
How to import Basemap (mpl) in Azure ML (section Notebooks),"<p>how can I import Basemap (from mpl_toolkits.basemap) in Azure ML (in section Notebooks)? 
Is there a general way to import libraries in Azure ML? 
(current version is shown as Python 3.4.5 |Anaconda custom (64-bit)| (default, Jul  2 2016, 17:47:47) IPython: 5.1.0)</p>

<p>pip installs the GEOS package but there are missing dependencies (and I could export the GEOS_DIR)</p>

<blockquote>
  <p><i> Please install the corresponding packages using your
      systems software management system (e.g. for Debian Linux do:
      'apt-get install libgeos-3.3.3 libgeos-c1 libgeos-dev' and/or
      set the environment variable GEOS_DIR to point to the location
      where geos is installed (for example, if geos_c.h
      is in /usr/local/include, and libgeos_c is in /usr/local/lib,
      set GEOS_DIR to /usr/local), or edit the setup.py script
      manually and set the variable GEOS_dir (right after the line
      that says ""set GEOS_dir manually here"". </i></p>
</blockquote>",1,0,2017-07-25 12:13:19.823000 UTC,,2017-07-26 10:25:17.697000 UTC,0,python|azure|matplotlib-basemap|azure-machine-learning-studio,321,2016-10-28 08:02:58.503000 UTC,2022-09-19 11:29:05.033000 UTC,,2386,397,7,614,,,,,,['azure-machine-learning-studio']
Azure ML Workbench not deploying service even though its running?,"<p>When running the command:</p>

<pre><code>az ml service create realtime -f score.py --model-file model.hdf5 -s schema.json -n modelapp -r python --collect-model-data false -c aml_config\conda_dependencies.yml
</code></pre>

<p>The image and service both seem to be created. But after a while, this error comes up:</p>

<pre><code>{
""Azure-cli-ml Version"": ""0.1.0a27.post3"",
""Error"": {
    ""Error Message"": ""No response from health endpoint after multiple deploy attempts. Setting status to failed.""
},
""Response Code"": 500,
""Response Content"": {
    ""CreatedTime"": ""2018-02-16T14:40:56.358161Z"",
    ""EndTime"": ""2018-02-16T14:51:23.79374Z"",
    ""Error"": {
        ""Code"": ""DeploymentFailed"",
        ""Message"": ""No response from health endpoint after multiple deploy attempts. Setting status to failed."",
        ""StatusCode"": 500
    },
    ""OperationType"": ""Service"",
    ""State"": ""Failed""
}
}
</code></pre>

<p>When I run the <code>az ml service logs realtime</code> command, I see many messages saying </p>

<pre><code>2018-02-16T14:59:44.964990Z, INFO, 00000000-0000-0000-0000-000000000000, , 127.0.0.1 - - [16/Feb/2018:14:59:44 +0000] ""GET / HTTP/1.0"" 200 7 ""-"" ""Go-http-client/1.1""
</code></pre>

<p>I am using Azure ML Workbench version 0.1.1712.18263. This process has worked fine about a month ago when I first deployed an endpoint to this model management account. In Azure, I see the service with a status:Failed and no URL, but the primary and secondary keys are populated.</p>

<p>I tried reinstalling the software just in case it was an older version, but it did not help. What else can I do to make the endpoint active?</p>",1,0,2018-02-16 15:04:30.993000 UTC,,,0,azure|azure-machine-learning-studio|azure-machine-learning-workbench,267,2015-01-12 16:58:02.987000 UTC,2022-09-23 20:26:46.323000 UTC,,588,47,3,64,,,,,,"['azure-machine-learning-workbench', 'azure-machine-learning-studio']"
AWS sagemaker error : predict() got an unexpected keyword argument 'pred_contribs',"<p>I am using AWS sagemaker and the built-in xgboost package to train a model and deploy an inference. However, I need SHAP values as well as point predictions when I call the API.</p>
<p>I was trying to mimic the predict behavior in python and including <code>pred_contribs=True</code> in the <code>predict()</code>function but I got the following error:</p>
<pre><code>predict() got an unexpected keyword argument 'pred_contribs'

</code></pre>
<p>I understand that the built-in xgboost package in sagemaker might not have this functionality, but after reading this <a href=""https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-debugger/xgboost_census_explanations/xgboost-census-debugger-rules.html"" rel=""nofollow noreferrer"">https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-debugger/xgboost_census_explanations/xgboost-census-debugger-rules.html</a>, I found hope. The sagemaker debugger is able to create shap-values and apparently it was mentioned that they got shap values from including `pred_contribs=True' in the predict function. However, it was not mentioned how we can do it when deploying the model. It was all about the train model and train dataset.</p>
<p>I also tried using sagemaker-clarify as mentioned <a href=""https://sagemaker-examples.readthedocs.io/en/latest/sagemaker_processing/fairness_and_explainability/fairness_and_explainability.html"" rel=""nofollow noreferrer"">https://sagemaker-examples.readthedocs.io/en/latest/sagemaker_processing/fairness_and_explainability/fairness_and_explainability.html</a> but it only gives me the shap values not the predictions. I can probably call the created API separately to get point predictions but I dont want to do it twice (once for shap values and once for point predictions).</p>
<p>Also, I know we can use the open-source xgboost package in sagemaker so we can have similar functionalities as python but that is not a straightforward process so I am trying to exhaust all of my options with the built-in xgboost package first.</p>
<p>Any help is appreciated.</p>",0,1,2021-04-23 15:59:26.187000 UTC,,,1,python|amazon-web-services|xgboost|amazon-sagemaker|shap,255,2017-08-21 13:50:16.220000 UTC,2022-09-21 18:47:24.837000 UTC,,55,5,0,21,,,,,,['amazon-sagemaker']
Error using set MLFLOW_TRACKING_URI='http://0.0.0.0:5000' for serve models,"<p>Hi i need to run a command like this</p>
<pre><code>mlflow server --backend-store-uri postgresql://mlflow_user:mlflow@localhost:5433/mlflow --default-artifact-root file:D:/artifact_root --host 0.0.0.0 --port 5000
</code></pre>
<p>for start my serve and i have not problem with this but when i try to run a example
in the route of project from github python</p>
<pre><code>mlflow/examples/sklearn_elasticnet_diabetes/linux/train_diabetes.py 0.1 0.9 
</code></pre>
<p>i get this error</p>
<pre><code>  _model_registry_store_registry.register_entrypoints()
Elasticnet model (alpha=0.100000, l1_ratio=0.900000):
  RMSE: 71.98302888908191
  MAE: 60.5647520017933
  R2: 0.21655161434654602
&lt;function get_tracking_uri at 0x0000017F3AE885E8&gt;
url 'http://0.0.0.0:8001'
url2 'http|//0.0.0.0|8001'
Traceback (most recent call last):
  File &quot;train_diabetes.py&quot;, line 90, in &lt;module&gt;
    mlflow.log_param(&quot;alpha&quot;, alpha)
  File &quot;C:\Users\kevin.sanchez\Miniconda3\envs\env_mlflow\lib\site-packages\mlflow\tracking\fluent.py&quot;, line 210, in log_param
    run_id = _get_or_start_run().info.run_id
  File &quot;C:\Users\kevin.sanchez\Miniconda3\envs\env_mlflow\lib\site-packages\mlflow\tracking\fluent.py&quot;, line 508, in _get_or_start_run
    return start_run()
  File &quot;C:\Users\kevin.sanchez\Miniconda3\envs\env_mlflow\lib\site-packages\mlflow\tracking\fluent.py&quot;, line 148, in start_run
    active_run_obj = MlflowClient().create_run(
  File &quot;C:\Users\kevin.sanchez\Miniconda3\envs\env_mlflow\lib\site-packages\mlflow\tracking\client.py&quot;, line 44, in __init__
    self._tracking_client = TrackingServiceClient(final_tracking_uri)
  File &quot;C:\Users\kevin.sanchez\Miniconda3\envs\env_mlflow\lib\site-packages\mlflow\tracking\_tracking_service\client.py&quot;, line 32, in __init__       
    self.store = utils._get_store(self.tracking_uri)
  File &quot;C:\Users\kevin.sanchez\Miniconda3\envs\env_mlflow\lib\site-packages\mlflow\tracking\_tracking_service\utils.py&quot;, line 126, in _get_store     
    return _tracking_store_registry.get_store(store_uri, artifact_uri)
  File &quot;C:\Users\kevin.sanchez\Miniconda3\envs\env_mlflow\lib\site-packages\mlflow\tracking\_tracking_service\registry.py&quot;, line 37, in get_store    
    return builder(store_uri=store_uri, artifact_uri=artifact_uri)
  File &quot;C:\Users\kevin.sanchez\Miniconda3\envs\env_mlflow\lib\site-packages\mlflow\tracking\_tracking_service\utils.py&quot;, line 81, in _get_file_store 
    return FileStore(store_uri, store_uri)
  File &quot;C:\Users\kevin.sanchez\Miniconda3\envs\env_mlflow\lib\site-packages\mlflow\store\tracking\file_store.py&quot;, line 100, in __init__
    self.root_directory = local_file_uri_to_path(root_directory or _default_root_dir())
  File &quot;C:\Users\kevin.sanchez\Miniconda3\envs\env_mlflow\lib\site-packages\mlflow\utils\file_utils.py&quot;, line 387, in local_file_uri_to_path
    return urllib.request.url2pathname(path)
  File &quot;C:\Users\kevin.sanchez\Miniconda3\envs\env_mlflow\lib\nturl2path.py&quot;, line 35, in url2pathname
    raise OSError(error)
OSError: Bad URL: 'http|//0.0.0.0|8001'
</code></pre>
<p>before running the python code i run this command to set the env tracking uri for the execution set MLFLOW_TRACKING_URI='http://0.0.0.0:5000'</p>
<p>i don´t know why mlflow replace the : for | i need help. Before this option worked but now it is failing</p>",2,0,2020-08-28 20:34:28.280000 UTC,,2020-08-28 20:39:45.303000 UTC,0,python|mlflow,895,2020-08-28 20:25:07.000000 UTC,2020-10-29 00:11:58.483000 UTC,,1,0,0,1,,,,,,['mlflow']
How to render a column n datetime format from AzureML Dataset?,"<p>I have registered a dataset after an Azure Databricks ETL operation. When it is registered as an AzureML Dataset, one of the columns is rendered as a timestamp. I know the schema has been inferred properly as the Dataset-&gt;Explore blade renders it properly:
<a href=""https://i.stack.imgur.com/qQTUc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qQTUc.png"" alt=""Dataset_in_Explore_Tab"" /></a></p>
<p>However, when using <code> Dataset.get_by_name(ws,&lt;name&gt;).to_pandas_dataframe()</code>, the timestamp column is rendered as all None:</p>
<p><a href=""https://i.stack.imgur.com/5zS2V.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5zS2V.png"" alt=""Pandas_Dataframe"" /></a></p>
<p>How do I mention the schema so that it is rendered properly while Getting the <code>Dataset.get_by_name()</code></p>",1,0,2021-05-26 05:32:25.393000 UTC,,,0,azure-machine-learning-service,88,2020-10-03 12:46:02.437000 UTC,2022-09-21 15:27:45.773000 UTC,"Bengaluru, Karnataka, India",887,187,32,130,,,,,,['azure-machine-learning-service']
Installing the cuda rapids + xgboost stack through conda,"<p>I'm trying to install install the <a href=""https://developer.nvidia.com/rapids"" rel=""nofollow noreferrer"">RAPIDS</a> stack with CUDA through conda in a jupyter notebook inside an AWS Sagemaker Studio instance:</p>
<pre><code>conda install -y -c conda-forge -c rapidsai-nightly -c nvidia libgcc cudf cuml xgboost rapids-blazing
</code></pre>
<p>It tried to resolve as many conflicts in the dependencies (around 20 mins later), then it says:</p>
<pre><code>Collecting package metadata (current_repodata.json): done
Solving environment: failed with initial frozen solve. Retrying with flexible solve.
Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.
Collecting package metadata (repodata.json): done
Solving environment: failed with initial frozen solve. Retrying with flexible solve.
Solving environment: \ 
Found conflicts! Looking for incompatible packages.
This can take several minutes.  Press CTRL-C to abort.
                                                                                /failed

UnsatisfiableError: The following specifications were found to be incompatible with each other:

Output in format: Requested package -&gt; Available versions

Package _openmp_mutex conflicts for:
cudf -&gt; libgcc-ng[version='&gt;=12'] -&gt; _openmp_mutex[version='&gt;=4.5']
libgcc -&gt; libgcc-ng[version='&gt;=7.2.0'] -&gt; _openmp_mutex[version='&gt;=4.5']
python=3.8.10 -&gt; libgcc-ng[version='&gt;=9.4.0'] -&gt; _openmp_mutex[version='&gt;=4.5']
xgboost -&gt; libgcc-ng[version='&gt;=9.3.0'] -&gt; _openmp_mutex[version='&gt;=4.5']
cuml -&gt; libgcc-ng[version='&gt;=12'] -&gt; _openmp_mutex[version='&gt;=4.5']

Package libstdcxx-ng conflicts for:
cuml -&gt; libstdcxx-ng[version='&gt;=12']
python=3.8.10 -&gt; libffi[version='&gt;=3.4.2,&lt;3.5.0a0'] -&gt; libstdcxx-ng[version='&gt;=7.3.0|&gt;=7.5.0']
cudf -&gt; libstdcxx-ng[version='&gt;=12']
python=3.8.10 -&gt; libstdcxx-ng[version='&gt;=9.3.0|&gt;=9.4.0']
rapids-blazing -&gt; cudatoolkit=11.4 -&gt; libstdcxx-ng[version='&gt;=10.3.0|&gt;=9.4.0|&gt;=9.3.0|&gt;=7.3.0']
xgboost -&gt; nccl[version='&gt;=2.14.3.1,&lt;3.0a0'] -&gt; libstdcxx-ng[version='&gt;=10.3.0|&gt;=11.2.0|&gt;=12|&gt;=9.4.0|&gt;=7.2.0']
xgboost -&gt; libstdcxx-ng[version='&gt;=4.9|&gt;=7.3.0|&gt;=7.5.0|&gt;=9.3.0']
cuml -&gt; cuda-python[version='&gt;=11.5,&lt;11.7.1'] -&gt; libstdcxx-ng[version='&gt;=10.3.0|&gt;=9.4.0|&gt;=11.2.0|&gt;=9.3.0|&gt;=7.3.0|&gt;=7.5.0']
libgcc -&gt; libstdcxx-ng[version='&gt;=7.2.0']
libgcc -&gt; gmp[version='&gt;=4.2'] -&gt; libstdcxx-ng[version='&gt;=12|&gt;=4.9|&gt;=7.3.0|&gt;=7.5.0|&gt;=9.4.0']
cudf -&gt; cuda-python[version='&gt;=11.5,&lt;11.7.1'] -&gt; libstdcxx-ng[version='&gt;=10.3.0|&gt;=9.4.0|&gt;=11.2.0|&gt;=9.3.0|&gt;=7.3.0|&gt;=7.5.0|&gt;=4.9']

Package rmm conflicts for:
cuml -&gt; cudf=22.10 -&gt; rmm[version='22.08.*|22.10.*|&gt;=22.10.0a.220827,&lt;22.11.0a0|&gt;=22.10.0a.220828,&lt;22.11.0a0|&gt;=22.10.0a.220829,&lt;22.11.0a0|&gt;=22.10.0a.220830,&lt;22.11.0a0|&gt;=22.10.0a.220831,&lt;22.11.0a0|&gt;=22.10.0a.220901,&lt;22.11.0a0|&gt;=22.10.0a.220902,&lt;22.11.0a0|&gt;=22.10.0a.220903,&lt;22.11.0a0|&gt;=22.10.0a.220905,&lt;22.11.0a0|&gt;=22.10.0a.220906,&lt;22.11.0a0|&gt;=22.8.0a.220905,&lt;22.9.0a0|&gt;=22.8.0a.220903,&lt;22.9.0a0|&gt;=22.8.0a.220902,&lt;22.9.0a0|&gt;=22.8.0a.220901,&lt;22.9.0a0|&gt;=22.8.0a.220831,&lt;22.9.0a0|&gt;=22.8.0a.220830,&lt;22.9.0a0|&gt;=22.8.0a.220829,&lt;22.9.0a0|&gt;=22.8.0a.220828,&lt;22.9.0a0|&gt;=22.8.0a.220827,&lt;22.9.0a0']
cudf -&gt; rmm[version='&gt;=22.10.0a.220827,&lt;22.11.0a0|&gt;=22.10.0a.220828,&lt;22.11.0a0|&gt;=22.10.0a.220829,&lt;22.11.0a0|&gt;=22.10.0a.220830,&lt;22.11.0a0|&gt;=22.10.0a.220831,&lt;22.11.0a0|&gt;=22.10.0a.220901,&lt;22.11.0a0|&gt;=22.10.0a.220902,&lt;22.11.0a0|&gt;=22.10.0a.220903,&lt;22.11.0a0|&gt;=22.10.0a.220905,&lt;22.11.0a0|&gt;=22.10.0a.220906,&lt;22.11.0a0|&gt;=22.8.0a.220905,&lt;22.9.0a0|&gt;=22.8.0a.220903,&lt;22.9.0a0|&gt;=22.8.0a.220902,&lt;22.9.0a0|&gt;=22.8.0a.220901,&lt;22.9.0a0|&gt;=22.8.0a.220831,&lt;22.9.0a0|&gt;=22.8.0a.220830,&lt;22.9.0a0|&gt;=22.8.0a.220829,&lt;22.9.0a0|&gt;=22.8.0a.220828,&lt;22.9.0a0|&gt;=22.8.0a.220827,&lt;22.9.0a0']
xgboost -&gt; py-xgboost==1.6.2dev.rapidsai22.10=cuda_11_py39_0 -&gt; rmm[version='22.02.*|22.04.*|22.06.*|22.08.*|22.10.*']

Package nccl conflicts for:
xgboost -&gt; nccl[version='&gt;=2.10.3.1,&lt;3.0a0|&gt;=2.11.4.1,&lt;3.0a0|&gt;=2.13.4.1,&lt;3.0a0|&gt;=2.14.3.1,&lt;3.0a0|&gt;=2.12.12.1,&lt;3.0a0|&gt;=2.12.7.1,&lt;3.0a0|&gt;=2.7.8.1,&lt;3.0a0']
cuml -&gt; nccl[version='&gt;=2.9.9']
cuml -&gt; cupy[version='&gt;=7.8.0,&lt;11.0.0a0'] -&gt; nccl[version='2.7.8.1.*|&gt;=2.4.6.1,&lt;3.0a0|&gt;=2.7.8.1,&lt;3.0a0|&gt;=2.8.4.1,&lt;3.0a0|&gt;=2.8.3.1,&lt;3.0a0|&gt;=2.12.12.1,&lt;3.0a0|&gt;=2.13.4.1,&lt;3.0a0']

Package libgcc conflicts for:
xgboost -&gt; scipy -&gt; libgcc
libgcc

Package python_abi conflicts for:
xgboost -&gt; python[version='&gt;=3.8,&lt;3.9.0a0'] -&gt; python_abi[version='3.8|3.9',build='*_pypy38_pp73|*_pypy39_pp73']
cuml -&gt; cuda-python[version='&gt;=11.5,&lt;11.7.1'] -&gt; python_abi[version='3.10.*|3.7.*|3.7|3.6.*|3.6|3.9|3.8',build='*_pypy38_pp73|*_pypy39_pp73|*_pypy36_pp73|*_cp36m|*_cp310|*_cp37m|*_pypy37_pp73']
cudf -&gt; cuda-python[version='&gt;=11.5,&lt;11.7.1'] -&gt; python_abi[version='2.7.*|3.10.*|3.7.*|3.7|3.9|3.8|3.6.*|3.6',build='*_cp27mu|*_pypy36_pp73|*_cp36m|*_pypy39_pp73|*_cp310|*_cp37m|*_pypy37_pp73|*_pypy38_pp73']
xgboost -&gt; python_abi[version='3.10.*|3.7.*|3.8.*|3.9.*|3.7|3.6.*|3.6',build='*_pypy36_pp73|*_cp310|*_cp38|*_cp37m|*_cp39|*_pypy37_pp73|*_cp36m']
cuml -&gt; python_abi[version='3.8.*|3.9.*',build='*_cp38|*_cp39']
rapids-blazing -&gt; python_abi[version='3.7.*|3.8.*',build='*_cp38|*_cp37m']
cudf -&gt; python_abi[version='3.8.*|3.9.*',build='*_cp38|*_cp39']

Package ncurses conflicts for:
python=3.8.10 -&gt; readline[version='&gt;=8.1,&lt;9.0a0'] -&gt; ncurses[version='&gt;=6.1,&lt;7.0.0a0|&gt;=6.3,&lt;7.0a0|&gt;=6.1,&lt;7.0a0']
cudf -&gt; python[version='&gt;=3.8,&lt;3.9.0a0'] -&gt; ncurses[version='&gt;=6.1,&lt;7.0.0a0|&gt;=6.2,&lt;7.0.0a0|&gt;=6.3,&lt;7.0a0|&gt;=6.2,&lt;7.0a0|&gt;=6.1,&lt;7.0a0']
xgboost -&gt; python[version='&gt;=3.7,&lt;3.8.0a0'] -&gt; ncurses[version='5.9.*|5.9|&gt;=6.1,&lt;7.0.0a0|&gt;=6.2,&lt;7.0.0a0|&gt;=6.3,&lt;7.0a0|&gt;=6.2,&lt;7.0a0|&gt;=6.1,&lt;7.0a0|&gt;=6.2,&lt;6.3.0a0|&gt;=6.0,&lt;7.0a0|6.0.*']
python=3.8.10 -&gt; ncurses[version='&gt;=6.2,&lt;7.0.0a0|&gt;=6.2,&lt;7.0a0']
cuml -&gt; python[version='&gt;=3.9,&lt;3.10.0a0'] -&gt; ncurses[version='&gt;=6.1,&lt;7.0.0a0|&gt;=6.2,&lt;7.0.0a0|&gt;=6.3,&lt;7.0a0|&gt;=6.2,&lt;7.0a0|&gt;=6.1,&lt;7.0a0']

Package libsqlite conflicts for:
xgboost -&gt; python[version='&gt;=3.10,&lt;3.11.0a0'] -&gt; libsqlite[version='&gt;=3.39.2,&lt;4.0a0']
python=3.8.10 -&gt; sqlite[version='&gt;=3.36.0,&lt;4.0a0'] -&gt; libsqlite[version='3.39.2|3.39.3',build='h753d276_1|h753d276_0']

Package numpy conflicts for:
cudf -&gt; numpy
cudf -&gt; cupy[version='&gt;=9.5.0,&lt;11.0.0a0'] -&gt; numpy[version='&gt;=1.14.6,&lt;2.0a0|&gt;=1.15.4,&lt;2.0a0|&gt;=1.16.5,&lt;2.0a0|&gt;=1.16.6,&lt;2.0a0|&gt;=1.17|&gt;=1.18|&gt;=1.21.6,&lt;2.0a0|&gt;=1.19.5,&lt;2.0a0|&gt;=1.21.5,&lt;2.0a0|&gt;=1.18.5,&lt;2.0a0|&gt;=1.21.2,&lt;2.0a0|&gt;=1.21.4,&lt;2.0a0|&gt;=1.17.5,&lt;2.0a0|&gt;=1.19.4,&lt;2.0a0|&gt;=1.19.2,&lt;2.0a0|&gt;=1.18.4,&lt;2.0a0|&gt;=1.18.1,&lt;2.0a0|&gt;=1.20.3,&lt;2.0a0|&gt;=1.20.2,&lt;2.0a0|&gt;=1.16,&lt;2.0a0|&gt;=1.19']

Package cudf conflicts for:
cudf
cuml -&gt; dask-cudf=22.10 -&gt; cudf[version='22.08.00a220827.*|22.08.00a220828.*|22.08.00a220829.*|22.08.00a220830.*|22.08.00a220831.*|22.08.00a220901.*|22.08.00a220902.*|22.08.00a220903.*|22.08.00a220905.*|22.08.00a220906.*|22.10.00a.*|22.10.00a220827.*|22.10.00a220828.*|22.10.00a220829.*|22.10.00a220830.*|22.10.00a220831.*|22.10.00a220901.*|22.10.00a220902.*|22.10.00a220903.*|22.10.00a220905.*|22.10.00a220906.*']
cuml -&gt; cudf[version='22.08.*|22.10.*']

Package dask-cudf conflicts for:
cuml -&gt; dask-cudf[version='22.08.*|22.10.*']
rapids-blazing -&gt; blazingsql=21.10 -&gt; dask-cudf=21.10

Package libgcc-ng conflicts for:
cudf -&gt; cuda-python[version='&gt;=11.5,&lt;11.7.1'] -&gt; libgcc-ng[version='&gt;=10.3.0|&gt;=9.4.0|&gt;=11.2.0|&gt;=9.3.0|&gt;=7.3.0|&gt;=7.5.0|&gt;=4.9|&gt;=7.2.0']
cudf -&gt; libgcc-ng[version='&gt;=12']

Package cudatoolkit conflicts for:
cudf -&gt; cudatoolkit[version='&gt;=11,&lt;12.0a0']
cudf -&gt; cupy[version='&gt;=9.5.0,&lt;11.0.0a0'] -&gt; cudatoolkit[version='10.0|10.0.*|10.1|10.1.*|10.2|10.2.*|11.0|11.0.*|11.1|11.1.*|&gt;=11.2,&lt;12|9.2|9.2.*|&gt;=11.0,&lt;=11.7|&gt;=11.0,&lt;=11.6|&gt;=11.0,&lt;=11.5']

Package numpy-base conflicts for:
cudf -&gt; numpy -&gt; numpy-base[version='1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.14.3|1.14.3|1.14.3|1.14.3|1.14.3|1.14.3|1.14.4|1.14.4|1.14.4|1.14.4|1.14.4|1.14.4|1.14.5|1.14.5|1.14.5|1.14.5|1.14.5|1.14.5|1.14.5|1.14.5|1.14.5|1.14.5|1.14.5|1.14.5|1.14.5|1.14.5|1.14.5|1.14.5|1.14.5|1.14.5|1.14.5|1.14.5|1.14.5|1.14.5|1.14.5|1.14.5|1.14.5|1.14.5|1.14.5|1.14.5|1.14.5|1.14.5|1.14.5|1.14.6|1.14.6|1.14.6|1.14.6|1.14.6|1.14.6|1.14.6|1.14.6|1.14.6|1.14.6|1.14.6|1.14.6|1.14.6|1.14.6|1.14.6|1.14.6|1.15.0|1.15.0|1.15.0|1.15.0|1.15.0|1.15.0|1.15.0|1.15.0|1.15.1|1.15.1|1.15.1|1.15.1|1.15.1|1.15.1|1.15.1|1.15.1|1.15.1|1.15.1|1.15.1|1.15.1|1.15.2|1.15.2|1.15.2|1.15.2|1.15.2|1.15.2|1.15.2|1.15.2|1.15.2|1.15.2|1.15.2|1.15.2|1.15.2|1.15.2|1.15.3|1.15.3|1.15.3|1.15.3|1.15.3|1.15.3|1.15.4|1.15.4|1.15.4|1.15.4|1.15.4|1.15.4|1.15.4|1.15.4|1.15.4|1.16.0|1.16.0|1.16.0|1.16.0|1.16.0|1.16.0|1.16.0|1.16.0|1.16.0|1.16.0|1.16.0|1.16.0|1.16.1|1.16.1|1.16.1|1.16.1|1.16.1|1.16.1|1.16.1|1.16.1|1.16.1|1.16.1|1.16.1|1.16.1|1.16.2|1.16.2|1.16.2|1.16.2|1.16.2|1.16.2|1.16.3|1.16.3|1.16.3|1.16.3|1.16.3|1.16.3|1.16.4|1.16.4|1.16.4|1.16.4|1.16.4|1.16.4|1.16.5|1.16.5|1.16.5|1.16.5|1.16.5|1.16.5|1.16.6|1.16.6|1.16.6|1.16.6|1.16.6|1.16.6|1.16.6|1.16.6|1.16.6|1.16.6|1.16.6|1.16.6|1.16.6|1.16.6|1.16.6|1.16.6|1.16.6|1.16.6|1.16.6|1.16.6|1.16.6|1.16.6|1.16.6|1.16.6|1.17.2.*|1.17.3.*|1.17.4.*|1.18.1.*|1.18.5.*|1.19.1|1.19.1|1.19.1|1.19.1|1.19.1|1.19.1|1.19.2|1.19.2|1.19.2|1.19.2|1.19.2|1.19.2|1.19.2|1.19.2|1.19.2|1.19.2|1.19.2|1.19.2|1.19.2|1.19.2|1.20.1|1.20.1|1.20.1|1.20.1|1.20.1|1.20.1|1.20.2|1.20.2|1.20.2|1.20.2|1.20.2|1.20.2|1.20.3|1.20.3|1.20.3|1.20.3|1.20.3|1.20.3|1.21.2|1.21.2|1.21.2|1.21.2|1.21.2|1.21.2|1.21.2|1.21.2|1.21.5|1.21.5|1.21.5|1.21.5|1.21.5|1.21.5|1.21.5|1.21.5|1.21.5|1.21.5|1.21.5|1.21.5|1.21.5|1.21.5|1.21.5|1.21.5|1.21.5|1.21.5|1.21.5|1.21.5|1.21.5|1.21.5|1.21.5|1.21.5|1.22.3|1.22.3|1.22.3|1.22.3|1.22.3|1.22.3|1.23.1|1.9.3|1.9.3|1.9.3|1.9.3|1.9.3|1.9.3|1.9.3|1.9.3|1.9.3|1.9.3|1.9.3|1.9.3|1.9.3|1.9.3|&gt;=1.9.3,&lt;2.0a0|1.17.0|1.17.0|1.17.0|1.17.0',build='py37hde5b4d6_0|py36hde5b4d6_0|py27h2b20989_6|py37h2b20989_6|py37hdbf6ddf_6|py27hdbf6ddf_6|py36hdbf6ddf_6|py27h2b20989_7|py36hdbf6ddf_7|py36h2b20989_7|py35h2b20989_7|py37hdbf6ddf_7|py27h2b20989_7|py36h2b20989_7|py27hdbf6ddf_7|py37hdbf6ddf_7|py36h2b20989_8|py37hdbf6ddf_8|py27h2b20989_8|py35hdbf6ddf_8|py35h2b20989_8|py36h7cdd4dd_9|py37h7cdd4dd_9|py37h3dfced4_9|py36h3dfced4_9|py36h81de0dd_9|py27h74e8950_9|py35h74e8950_9|py36h74e8950_9|py37h74e8950_10|py27h74e8950_10|py27h81de0dd_10|py37h2f8d375_11|py27h2f8d375_11|py36h2f8d375_11|py27hde5b4d6_12|py37hde5b4d6_12|py38hde5b4d6_12|py36h0ea5e3f_1|py27h0ea5e3f_1|py35h0ea5e3f_1|py36h9be14a7_1|py27h9be14a7_1|py36h2b20989_0|py36hdbf6ddf_0|py35hdbf6ddf_0|py36h2b20989_0|py36hdbf6ddf_0|py35hdbf6ddf_0|py36h2b20989_1|py37h2b20989_1|py27hdbf6ddf_1|py27h2b20989_1|py27h2b20989_2|py27hdbf6ddf_2|py36h2b20989_3|py36hdbf6ddf_3|py27hdbf6ddf_3|py27hdbf6ddf_4|py36h2b20989_4|py36hdbf6ddf_4|py37h2f8d375_4|py27h81de0dd_4|py36h2f8d375_4|py27h2f8d375_4|py37h81de0dd_4|py36h81de0dd_4|py37h2f8d375_5|py27h2f8d375_5|py37hde5b4d6_5|py36hde5b4d6_5|py27h7cdd4dd_0|py35h7cdd4dd_0|py27h3dfced4_0|py35h74e8950_0|py27h81de0dd_0|py37h81de0dd_0|py36h2f8d375_0|py27h81de0dd_0|py35h81de0dd_0|py27h81de0dd_1|py37h81de0dd_1|py36h2f8d375_0|py27h81de0dd_0|py36h2f8d375_0|py36h81de0dd_0|py27h81de0dd_0|py36hde5b4d6_0|py37hde5b4d6_0|py27h2f8d375_0|py37h2f8d375_0|py37hde5b4d6_0|py36h2f8d375_0|py37h2f8d375_1|py36hde5b4d6_1|py27h2f8d375_0|py36h2f8d375_0|py36hde5b4d6_0|py37hde5b4d6_0|py27hde5b4d6_0|py27h2f8d375_1|py37h2f8d375_1|py36hde5b4d6_1|py27hde5b4d6_1|py37hde5b4d6_0|py36h2f8d375_0|py37hde5b4d6_0|py27hde5b4d6_0|py27h2f8d375_0|py36h2f8d375_0|py37hde5b4d6_0|py27hde5b4d6_0|py36hde5b4d6_0|py36h2f8d375_0|py27h2f8d375_0|py37hde5b4d6_0|py36h2f8d375_0|py37hde5b4d6_0|py38h2f8d375_0|py38hde5b4d6_0|py27h2f8d375_0|py27hde5b4d6_0|py37h41b4c56_3|py38hdc34a94_3|py37hdc34a94_3|py37ha8aedfd_4|py37hfa32c7d_0|py36hfa32c7d_0|py37h75fe3a5_0|py37hfa32c7d_0|py36h75fe3a5_0|py39h2ae0177_0|py38h21a3de8_1|py37h7d8b39e_0|py39h7d8b39e_0|py38he2ba247_0|py38hfae3a4d_0|py39he2ba247_0|py39hfae3a4d_0|py37h74d4b33_0|py38h74d4b33_0|py38h39b7dee_0|py39h79a1101_0|py37h2b8c604_0|py39h2b8c604_0|py38h2b8c604_0|py310h2b8c604_0|py38hb8be1f0_1|py38hf524024_1|py37hf524024_1|py310hf2716ce_2|py39hb8be1f0_2|py310h9585f30_2|py39hf524024_2|py37hf524024_2|py37h1e6e340_3|py38h1e6e340_3|py310h375b286_3|py39h1e6e340_3|py39hf524024_0|py310h9585f30_0|py39hb8be1f0_0|py310hf2716ce_0|py38h1e6e340_0|py39ha15fc14_0|py310hcba007f_0|py38ha15fc14_0|py310h375b286_0|py39h1e6e340_0|py38hb8be1f0_0|py38hf524024_0|py39ha15fc14_3|py310hcba007f_3|py38ha15fc14_3|py37ha15fc14_3|py38hf524024_2|py38hb8be1f0_2|py37hb8be1f0_2|py310h9585f30_1|py310hf2716ce_1|py37hb8be1f0_1|py39hf524024_1|py39hb8be1f0_1|py310h79a1101_0|py37h79a1101_0|py38h79a1101_0|py39h39b7dee_0|py37h39b7dee_0|py39h74d4b33_0|py37he2ba247_0|py37hfae3a4d_0|py38h7d8b39e_0|py38h34387ca_0|py39h34387ca_0|py37h34387ca_0|py39h21a3de8_1|py37h21a3de8_1|py38h4c65ebe_1|py39h4c65ebe_1|py37h4c65ebe_1|py39h0f7b65f_0|py38hfa32c7d_0|py36hfa32c7d_0|py37h75fe3a5_0|py38h75fe3a5_0|py36h75fe3a5_0|py38hfa32c7d_0|py38h75fe3a5_0|py38h73d599e_4|py39h73d599e_4|py37h73d599e_4|py38ha8aedfd_4|py39ha8aedfd_4|py39hdc34a94_3|py36hdc34a94_3|py36h41b4c56_3|py38h41b4c56_3|py39h41b4c56_3|py39h76555f2_1|py39hfb011de_1|py36hde5b4d6_0|py37h2f8d375_0|py36hde5b4d6_0|py27hde5b4d6_0|py37h2f8d375_0|py37h2f8d375_0|py36hde5b4d6_0|py37h2f8d375_0|py27h2f8d375_0|py36hde5b4d6_0|py27hde5b4d6_0|py27h2f8d375_0|py36h2f8d375_0|py37h2f8d375_0|py37hde5b4d6_1|py36h2f8d375_1|py37h2f8d375_0|py27hde5b4d6_1|py37hde5b4d6_1|py36h2f8d375_1|py27h2f8d375_1|py36hde5b4d6_0|py27hde5b4d6_0|py27hde5b4d6_0|py37h81de0dd_0|py27h2f8d375_0|py37h2f8d375_0|py37h81de0dd_0|py36h81de0dd_0|py37h2f8d375_0|py27h2f8d375_0|py37h2f8d375_1|py36h81de0dd_1|py27h2f8d375_1|py36h2f8d375_1|py36h81de0dd_0|py36h2f8d375_0|py35h2f8d375_0|py37h81de0dd_0|py27h2f8d375_0|py37h2f8d375_0|py37h2f8d375_0|py27h2f8d375_0|py35h2f8d375_0|py35h81de0dd_0|py36h81de0dd_0|py37h74e8950_0|py36h74e8950_0|py27h74e8950_0|py37h3dfced4_0|py35h3dfced4_0|py36h3dfced4_0|py37h7cdd4dd_0|py36h7cdd4dd_0|py27hde5b4d6_5|py36h2f8d375_5|py38hde5b4d6_4|py38h2f8d375_4|py35h81de0dd_4|py35h2f8d375_4|py35h2b20989_4|py35hdbf6ddf_4|py37hdbf6ddf_4|py37h2b20989_4|py27h2b20989_4|py27h2b20989_3|py37hdbf6ddf_3|py37h2b20989_3|py36hdbf6ddf_2|py37hdbf6ddf_2|py37h2b20989_2|py36h2b20989_2|py37hdbf6ddf_1|py36hdbf6ddf_1|py27hdbf6ddf_0|py27h2b20989_0|py27hdbf6ddf_0|py35h2b20989_0|py27h2b20989_0|py35h9be14a7_1|py38h2f8d375_12|py36hde5b4d6_12|py36h2f8d375_12|py27h2f8d375_12|py37h2f8d375_12|py27hde5b4d6_11|py37hde5b4d6_11|py36hde5b4d6_11|py35h2f8d375_10|py27h2f8d375_10|py36h2f8d375_10|py37h2f8d375_10|py35h81de0dd_10|py36h81de0dd_10|py37h81de0dd_10|py36h74e8950_10|py35h74e8950_10|py35h81de0dd_9|py27h81de0dd_9|py37h74e8950_9|py37h81de0dd_9|py27h3dfced4_9|py35h3dfced4_9|py27h7cdd4dd_9|py35h7cdd4dd_9|py27hdbf6ddf_8|py37h2b20989_8|py36hdbf6ddf_8|py36hdbf6ddf_7|py37h2b20989_7|py37h2b20989_7|py35hdbf6ddf_7|py27hdbf6ddf_7|py36h2b20989_6|py37h2f8d375_0|py36h2f8d375_0']
xgboost -&gt; numpy -&gt; numpy-base[version='1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.14.3|1.14.3|1.14.3|1.14.3|1.14.3|1.14.3|1.14.4|1.14.4|1.14.4|1.14.4|1.14.4|1.14.4|1.14.5|1.14.5|1.14.5|1.14.5|1.14.5|1.14.5|1.14.5|1.14.5|1.14.5|1.14.5|1.14.5|1.14.5|1.14.5|1.14.5|1.14.5|1.14.5|1.14.5|1.14.5|1.14.5|1.14.5|1.14.5|1.14.5|1.14.5|1.14.5|1.14.5|1.14.5|1.14.5|1.14.5|1.14.5|1.14.5|1.14.5|1.14.6|1.14.6|1.14.6|1.14.6|1.14.6|1.14.6|1.14.6|1.14.6|1.14.6|1.14.6|1.14.6|1.14.6|1.14.6|1.14.6|1.14.6|1.14.6|1.15.0|1.15.0|1.15.0|1.15.0|1.15.0|1.15.0|1.15.0|1.15.0|1.15.1|1.15.1|1.15.1|1.15.1|1.15.1|1.15.1|1.15.1|1.15.1|1.15.1|1.15.1|1.15.1|1.15.1|1.15.2|1.15.2|1.15.2|1.15.2|1.15.2|1.15.2|1.15.2|1.15.2|1.15.2|1.15.2|1.15.2|1.15.2|1.15.2|1.15.2|1.15.3|1.15.3|1.15.3|1.15.3|1.15.3|1.15.3|1.15.4|1.15.4|1.15.4|1.15.4|1.15.4|1.15.4|1.15.4|1.15.4|1.15.4|1.16.0|1.16.0|1.16.0|1.16.0|1.16.0|1.16.0|1.16.0|1.16.0|1.16.0|1.16.0|1.16.0|1.16.0|1.16.1|1.16.1|1.16.1|1.16.1|1.16.1|1.16.1|1.16.1|1.16.1|1.16.1|1.16.1|1.16.1|1.16.1|1.16.2|1.16.2|1.16.2|1.16.2|1.16.2|1.16.2|1.16.3|1.16.3|1.16.3|1.16.3|1.16.3|1.16.3|1.16.4|1.16.4|1.16.4|1.16.4|1.16.4|1.16.4|1.16.5|1.16.5|1.16.5|1.16.5|1.16.5|1.16.5|1.16.6|1.16.6|1.16.6|1.16.6|1.16.6|1.16.6|1.16.6|1.16.6|1.16.6|1.16.6|1.16.6|1.16.6|1.16.6|1.16.6|1.16.6|1.16.6|1.16.6|1.16.6|1.16.6|1.16.6|1.16.6|1.16.6|1.16.6|1.16.6|1.17.2.*|1.17.3.*|1.17.4.*|1.18.1.*|1.18.5.*|1.19.1|1.19.1|1.19.1|1.19.1|1.19.1|1.19.1|1.19.2|1.19.2|1.19.2|1.19.2|1.19.2|1.19.2|1.19.2|1.19.2|1.19.2|1.19.2|1.19.2|1.19.2|1.19.2|1.19.2|1.20.1|1.20.1|1.20.1|1.20.1|1.20.1|1.20.1|1.20.2|1.20.2|1.20.2|1.20.2|1.20.2|1.20.2|1.20.3|1.20.3|1.20.3|1.20.3|1.20.3|1.20.3|1.21.2|1.21.2|1.21.2|1.21.2|1.21.2|1.21.2|1.21.2|1.21.2|1.21.5|1.21.5|1.21.5|1.21.5|1.21.5|1.21.5|1.21.5|1.21.5|1.21.5|1.21.5|1.21.5|1.21.5|1.21.5|1.21.5|1.21.5|1.21.5|1.21.5|1.21.5|1.21.5|1.21.5|1.21.5|1.21.5|1.21.5|1.21.5|1.22.3|1.22.3|1.22.3|1.22.3|1.22.3|1.22.3|1.23.1|1.9.3|1.9.3|1.9.3|1.9.3|1.9.3|1.9.3|1.9.3|1.9.3|1.9.3|1.9.3|1.9.3|1.9.3|1.9.3|1.9.3|&gt;=1.9.3,&lt;2.0a0|1.17.0|1.17.0|1.17.0|1.17.0',build='py37hde5b4d6_0|py36hde5b4d6_0|py27h2b20989_6|py37h2b20989_6|py37hdbf6ddf_6|py27hdbf6ddf_6|py36hdbf6ddf_6|py27h2b20989_7|py36hdbf6ddf_7|py36h2b20989_7|py35h2b20989_7|py37hdbf6ddf_7|py27h2b20989_7|py36h2b20989_7|py27hdbf6ddf_7|py37hdbf6ddf_7|py36h2b20989_8|py37hdbf6ddf_8|py27h2b20989_8|py35hdbf6ddf_8|py35h2b20989_8|py36h7cdd4dd_9|py37h7cdd4dd_9|py37h3dfced4_9|py36h3dfced4_9|py36h81de0dd_9|py27h74e8950_9|py35h74e8950_9|py36h74e8950_9|py37h74e8950_10|py27h74e8950_10|py27h81de0dd_10|py37h2f8d375_11|py27h2f8d375_11|py36h2f8d375_11|py27hde5b4d6_12|py37hde5b4d6_12|py38hde5b4d6_12|py36h0ea5e3f_1|py27h0ea5e3f_1|py35h0ea5e3f_1|py36h9be14a7_1|py27h9be14a7_1|py36h2b20989_0|py36hdbf6ddf_0|py35hdbf6ddf_0|py36h2b20989_0|py36hdbf6ddf_0|py35hdbf6ddf_0|py36h2b20989_1|py37h2b20989_1|py27hdbf6ddf_1|py27h2b20989_1|py27h2b20989_2|py27hdbf6ddf_2|py36h2b20989_3|py36hdbf6ddf_3|py27hdbf6ddf_3|py27hdbf6ddf_4|py36h2b20989_4|py36hdbf6ddf_4|py37h2f8d375_4|py27h81de0dd_4|py36h2f8d375_4|py27h2f8d375_4|py37h81de0dd_4|py36h81de0dd_4|py37h2f8d375_5|py27h2f8d375_5|py37hde5b4d6_5|py36hde5b4d6_5|py27h7cdd4dd_0|py35h7cdd4dd_0|py27h3dfced4_0|py35h74e8950_0|py27h81de0dd_0|py37h81de0dd_0|py36h2f8d375_0|py27h81de0dd_0|py35h81de0dd_0|py27h81de0dd_1|py37h81de0dd_1|py36h2f8d375_0|py27h81de0dd_0|py36h2f8d375_0|py36h81de0dd_0|py27h81de0dd_0|py36hde5b4d6_0|py37hde5b4d6_0|py27h2f8d375_0|py37h2f8d375_0|py37hde5b4d6_0|py36h2f8d375_0|py37h2f8d375_1|py36hde5b4d6_1|py27h2f8d375_0|py36h2f8d375_0|py36hde5b4d6_0|py37hde5b4d6_0|py27hde5b4d6_0|py27h2f8d375_1|py37h2f8d375_1|py36hde5b4d6_1|py27hde5b4d6_1|py37hde5b4d6_0|py36h2f8d375_0|py37hde5b4d6_0|py27hde5b4d6_0|py27h2f8d375_0|py36h2f8d375_0|py37hde5b4d6_0|py27hde5b4d6_0|py36hde5b4d6_0|py36h2f8d375_0|py27h2f8d375_0|py37hde5b4d6_0|py36h2f8d375_0|py37hde5b4d6_0|py38h2f8d375_0|py38hde5b4d6_0|py27h2f8d375_0|py27hde5b4d6_0|py37h41b4c56_3|py38hdc34a94_3|py37hdc34a94_3|py37ha8aedfd_4|py37hfa32c7d_0|py36hfa32c7d_0|py37h75fe3a5_0|py37hfa32c7d_0|py36h75fe3a5_0|py39h2ae0177_0|py38h21a3de8_1|py37h7d8b39e_0|py39h7d8b39e_0|py38he2ba247_0|py38hfae3a4d_0|py39he2ba247_0|py39hfae3a4d_0|py37h74d4b33_0|py38h74d4b33_0|py38h39b7dee_0|py39h79a1101_0|py37h2b8c604_0|py39h2b8c604_0|py38h2b8c604_0|py310h2b8c604_0|py38hb8be1f0_1|py38hf524024_1|py37hf524024_1|py310hf2716ce_2|py39hb8be1f0_2|py310h9585f30_2|py39hf524024_2|py37hf524024_2|py37h1e6e340_3|py38h1e6e340_3|py310h375b286_3|py39h1e6e340_3|py39hf524024_0|py310h9585f30_0|py39hb8be1f0_0|py310hf2716ce_0|py38h1e6e340_0|py39ha15fc14_0|py310hcba007f_0|py38ha15fc14_0|py310h375b286_0|py39h1e6e340_0|py38hb8be1f0_0|py38hf524024_0|py39ha15fc14_3|py310hcba007f_3|py38ha15fc14_3|py37ha15fc14_3|py38hf524024_2|py38hb8be1f0_2|py37hb8be1f0_2|py310h9585f30_1|py310hf2716ce_1|py37hb8be1f0_1|py39hf524024_1|py39hb8be1f0_1|py310h79a1101_0|py37h79a1101_0|py38h79a1101_0|py39h39b7dee_0|py37h39b7dee_0|py39h74d4b33_0|py37he2ba247_0|py37hfae3a4d_0|py38h7d8b39e_0|py38h34387ca_0|py39h34387ca_0|py37h34387ca_0|py39h21a3de8_1|py37h21a3de8_1|py38h4c65ebe_1|py39h4c65ebe_1|py37h4c65ebe_1|py39h0f7b65f_0|py38hfa32c7d_0|py36hfa32c7d_0|py37h75fe3a5_0|py38h75fe3a5_0|py36h75fe3a5_0|py38hfa32c7d_0|py38h75fe3a5_0|py38h73d599e_4|py39h73d599e_4|py37h73d599e_4|py38ha8aedfd_4|py39ha8aedfd_4|py39hdc34a94_3|py36hdc34a94_3|py36h41b4c56_3|py38h41b4c56_3|py39h41b4c56_3|py39h76555f2_1|py39hfb011de_1|py36hde5b4d6_0|py37h2f8d375_0|py36hde5b4d6_0|py27hde5b4d6_0|py37h2f8d375_0|py37h2f8d375_0|py36hde5b4d6_0|py37h2f8d375_0|py27h2f8d375_0|py36hde5b4d6_0|py27hde5b4d6_0|py27h2f8d375_0|py36h2f8d375_0|py37h2f8d375_0|py37hde5b4d6_1|py36h2f8d375_1|py37h2f8d375_0|py27hde5b4d6_1|py37hde5b4d6_1|py36h2f8d375_1|py27h2f8d375_1|py36hde5b4d6_0|py27hde5b4d6_0|py27hde5b4d6_0|py37h81de0dd_0|py27h2f8d375_0|py37h2f8d375_0|py37h81de0dd_0|py36h81de0dd_0|py37h2f8d375_0|py27h2f8d375_0|py37h2f8d375_1|py36h81de0dd_1|py27h2f8d375_1|py36h2f8d375_1|py36h81de0dd_0|py36h2f8d375_0|py35h2f8d375_0|py37h81de0dd_0|py27h2f8d375_0|py37h2f8d375_0|py37h2f8d375_0|py27h2f8d375_0|py35h2f8d375_0|py35h81de0dd_0|py36h81de0dd_0|py37h74e8950_0|py36h74e8950_0|py27h74e8950_0|py37h3dfced4_0|py35h3dfced4_0|py36h3dfced4_0|py37h7cdd4dd_0|py36h7cdd4dd_0|py27hde5b4d6_5|py36h2f8d375_5|py38hde5b4d6_4|py38h2f8d375_4|py35h81de0dd_4|py35h2f8d375_4|py35h2b20989_4|py35hdbf6ddf_4|py37hdbf6ddf_4|py37h2b20989_4|py27h2b20989_4|py27h2b20989_3|py37hdbf6ddf_3|py37h2b20989_3|py36hdbf6ddf_2|py37hdbf6ddf_2|py37h2b20989_2|py36h2b20989_2|py37hdbf6ddf_1|py36hdbf6ddf_1|py27hdbf6ddf_0|py27h2b20989_0|py27hdbf6ddf_0|py35h2b20989_0|py27h2b20989_0|py35h9be14a7_1|py38h2f8d375_12|py36hde5b4d6_12|py36h2f8d375_12|py27h2f8d375_12|py37h2f8d375_12|py27hde5b4d6_11|py37hde5b4d6_11|py36hde5b4d6_11|py35h2f8d375_10|py27h2f8d375_10|py36h2f8d375_10|py37h2f8d375_10|py35h81de0dd_10|py36h81de0dd_10|py37h81de0dd_10|py36h74e8950_10|py35h74e8950_10|py35h81de0dd_9|py27h81de0dd_9|py37h74e8950_9|py37h81de0dd_9|py27h3dfced4_9|py35h3dfced4_9|py27h7cdd4dd_9|py35h7cdd4dd_9|py27hdbf6ddf_8|py37h2b20989_8|py36hdbf6ddf_8|py36hdbf6ddf_7|py37h2b20989_7|py37h2b20989_7|py35hdbf6ddf_7|py27hdbf6ddf_7|py36h2b20989_6|py37h2f8d375_0|py36h2f8d375_0']The following specifications were found to be incompatible with your system:

  - feature:/linux-64::__cuda==11.4=0
  - feature:/linux-64::__glibc==2.31=0
  - feature:|@/linux-64::__cuda==11.4=0
  - feature:|@/linux-64::__glibc==2.31=0
  - cudf -&gt; __glibc[version='&gt;=2.17,&lt;3.0.a0']
  - cudf -&gt; cupy[version='&gt;=9.5.0,&lt;11.0.0a0'] -&gt; __glibc[version='&gt;=2.17']
  - cuml -&gt; __glibc[version='&gt;=2.17,&lt;3.0.a0']
  - cuml -&gt; cupy[version='&gt;=7.8.0,&lt;11.0.0a0'] -&gt; __glibc[version='&gt;=2.17']
  - libgcc -&gt; libgcc-ng[version='&gt;=7.2.0'] -&gt; __glibc[version='&gt;=2.17']
  - rapids-blazing -&gt; cudatoolkit=11.4 -&gt; __glibc[version='&gt;=2.17,&lt;3.0.a0']
  - xgboost -&gt; __cuda
  - xgboost -&gt; __glibc[version='&gt;=2.17']
  - xgboost -&gt; cudatoolkit[version='&gt;=11,&lt;12.0a0'] -&gt; __glibc[version='&gt;=2.17,&lt;3.0.a0']

Your installed version is: 11.4
</code></pre>
<p>But when I tried to run <code>import xgboost</code>, I get an <code>ModuleNotFound</code>.</p>
<p>There is this post on the specific version numbers to install but it's rather out of date <a href=""https://towardsdatascience.com/quick-install-guide-nvidia-rapids-blazingsql-on-aws-sagemaker-cb4ddd809bf5"" rel=""nofollow noreferrer"">https://towardsdatascience.com/quick-install-guide-nvidia-rapids-blazingsql-on-aws-sagemaker-cb4ddd809bf5</a> (from 4 years ago)</p>
<p><strong>How to installing the cuda rapids + xgboost stack through conda in AWS Sagemaker?</strong></p>
<hr />
<p>Note: Somehow on Kaggle notebooks, it worked out of the box, e.g. <a href=""https://www.kaggle.com/code/alvations/numerai-baseline-2015-gpu"" rel=""nofollow noreferrer"">https://www.kaggle.com/code/alvations/numerai-baseline-2015-gpu</a></p>",1,3,2022-09-06 14:50:20.297000 UTC,,2022-09-06 15:43:54.770000 UTC,0,python|conda|xgboost|amazon-sagemaker|rapids,53,2011-02-09 23:06:54.993000 UTC,2022-09-25 01:39:20.847000 UTC,Singapore,107877,3604,244,8865,,,,,,['amazon-sagemaker']
How to run python code on AWS lambda with package dependencies >500MB?,"<p>The requirement is that I have to trigger a SageMaker endpoint on lambda to get predictions(which is easy) but have to do some extra processing for variable importance using packages such as XGBoost and SHAP.</p>

<p>I am able to hit the endpoint and get variable importance using the SageMaker Jupyter notebook. Now, I want to replicate the same thing on AWS lambda.</p>

<p>1) <strong>How to run python code on AWS lambda with package dependencies for Pandas, XGBoost and SHAP (total package size greater than 500MB)</strong>. The unzipped deployment package size is greater than 250 MB, hence lambda is not allowing to deploy. I even tried using lambda function from Cloud9 and got the same error due to size restrictions. I have also tried lambda layers, but no luck.</p>

<p>2) <strong>Is there a way for me to run the code with such big packages on or through lambda bypassing the deployment package size limitation of 250 MB</strong></p>

<p>3) <strong>Is there a way to trigger a SageMaker notebook execution through lambda which would do the calculations and return the output back to lambda?</strong></p>",4,1,2019-04-01 21:36:39.110000 UTC,1.0,,6,python|amazon-web-services|aws-lambda|xgboost|amazon-sagemaker,1558,2019-04-01 21:32:11.847000 UTC,2019-04-09 20:26:04.623000 UTC,,61,0,0,3,,,,,,['amazon-sagemaker']
RandomCutForest hyperparameter value limit is too small in Sagemaker,"<p>I'm trying to use RandomCutForest in Sagemaker with the data as below:</p>
<ul>
<li>Number of rows: 420000</li>
<li>Feature dimension: 30</li>
</ul>
<p>The problem is that RandomCutForest hyperparameters have the following restrictions (<a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/rcf_hyperparameters.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/rcf_hyperparameters.html</a>).</p>
<ul>
<li>num_samples_per_tree: min: 1, max: 2048</li>
<li>num_trees: min: 50, max: 1000</li>
</ul>
<p>I think RandomCutForest is not suitable for large dataset as described above because of that hyperparameter restrictions.
Even if you set the max values to those hyperparameters, 2048 num_samples_per_tree is too small in comparison with 420000-rows data.</p>
<p>I wonder why Sagemaker's RandomCutForest has such a restriction (due to performance issue, hardware capability or any other reason?), even though IsolationForest in sklearn has no such restrictions.
<a href=""https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html"" rel=""nofollow noreferrer"">https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html</a></p>
<p>If there's any workaround on this problem, please let me know.</p>",1,1,2021-03-01 07:43:40.003000 UTC,,2021-03-01 10:47:32.387000 UTC,1,machine-learning|amazon-sagemaker,40,2019-08-26 05:53:07.587000 UTC,2022-09-22 08:43:10.663000 UTC,"Tokyo, Japan",51,2,0,0,,,,,,['amazon-sagemaker']
sagemaker transform not filtering input as expeced,"<p>Am trying to batch transform a csv file using this code</p>
<p><strong>Processor script</strong></p>
<pre><code>text_preparation_model = SKLearnModel(
    sagemaker_session = local_session,
    entry_point='processor.py',
    role=context.role,
    framework_version=..,
    image_uri=...,
    model_data=...)
</code></pre>
<p><strong>processor.py</strong></p>
<pre><code>def input_fn(input_data, content_type):
    if content_type == 'text/csv':
        df = pd.read_csv(StringIO(input_data), names=['feature_col1'],quoting=csv.QUOTE_NONNUMERIC, escapechar='\\')
        print(df.head())

        return df
    else:
        raise ValueError(&quot;{} .Error &quot;.format(content_type))
</code></pre>
<p><strong>Transform</strong></p>
<pre><code>text_preparation_transformer.transform(
    'file://validation.txt',  
    content_type='text/csv',
    split_type='Line',
    logs=context.show_logs,
    input_filter='$[0]',
    join_source='Input'
)
</code></pre>
<p>the file is 2 columns separated with comma and values are enclosed in double quotes <code>ex</code> &quot;some feature value&quot;,&quot;label_1&quot;.
<a href=""https://i.stack.imgur.com/TKJTo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TKJTo.png"" alt=""enter image description here"" /></a>
in refernce to <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform-data-processing.html"" rel=""nofollow noreferrer"">this</a>,
I expected to receive only the feature column in the <code>processor script</code>, but I keep getting both columns</p>",1,0,2022-08-25 12:31:32.517000 UTC,,,0,amazon-web-services|amazon-sagemaker,26,2012-04-26 13:33:06.710000 UTC,2022-09-25 00:14:52.287000 UTC,Egypt,1972,701,11,547,,,,,,['amazon-sagemaker']
Can I check GPU availability prior to starting the training job?,"<p>I want to start a TrainingJob using <a href=""https://sagemaker.readthedocs.io/en/stable/"" rel=""nofollow noreferrer"">AWS Sagemaker SDK</a>. As per documentation of the <a href=""https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html"" rel=""nofollow noreferrer"">Estimator Object</a> There is a function <a href=""https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html#sagemaker.estimator.Estimator.fit"" rel=""nofollow noreferrer"">fit</a> which will start the training job.</p>
<p>Each user has a limit of the number of GPUs that they can simultaneously run. If I write</p>
<pre><code>estimator.fit()
</code></pre>
<p>It might return a <code>botocore</code> exception saying that &quot;Resource Limit is Exceeded&quot;.</p>
<p>I was wondering if there's a way to figure out if the resource limit will be exceeded without actually running <code>estimator.fit()</code> and trying to catch the exception.</p>
<p>Basically run the code in the following way:</p>
<pre><code>if not sagemaker.resources_will_be_exceeded(job_description):
    estimator.fit()

</code></pre>
<p>Looking at the documentation I could not find such a function, however maybe I am missing something.</p>",1,0,2021-06-24 08:00:39.177000 UTC,,,0,amazon-web-services|amazon-sagemaker,47,2015-07-17 19:10:35.083000 UTC,2022-08-14 18:21:57.947000 UTC,Armenia,461,35,6,153,,,,,,['amazon-sagemaker']
amazon-sagemaker-lab:: libXrender.so.1 package,"<p>I am trying to use the amazon sagemaker lab environment and the package libXrender is not installed.</p>
<p>sudo privileges are removed and it's not possible to install it with:</p>
<p><code>apt-get install libxrender1</code></p>
<p>Is there an easy fix or do I have to contact their support to install the package in their docker container?</p>
<p>Thanks in advance!</p>
<p>Error results from this piece of code:</p>
<pre><code>from rdkit.Chem.Draw import rdMolDraw2D
from rdkit.Chem.Draw.rdMolDraw2D import *

ImportError: libXrender.so.1: cannot open shared object file: No such file or directory
</code></pre>",3,0,2022-07-01 09:32:28.847000 UTC,,,0,amazon-sagemaker|rdkit,77,2016-06-14 13:45:51.740000 UTC,2022-09-22 10:07:51.313000 UTC,"Cambridge, UK",1,0,0,0,,,,,,['amazon-sagemaker']
Deploying directly from model artifacts,"<p>I want to deploy a pretrained neural network as an endpoint at Sagemaker. All my model artifacts are stored in S3.</p>
<p>In the Sagemacer documentation (<a href=""https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/deploying_tensorflow_serving.html"" rel=""nofollow noreferrer"">here</a>) I found this solution:</p>
<pre><code>from sagemaker.tensorflow import TensorFlowModel 

model = TensorFlowModel(model_data='s3://mybucket/model.tar.gz', role='MySageMakerRole')

predictor = model.deploy(initial_instance_count=1, instance_type='ml.c5.xlarge')
</code></pre>
<p>I adapted the TensorFlowModel() function like this:</p>
<pre><code>from sagemaker.tensorflow import TensorFlowModel
from sagemaker import get_execution_role

role = get_execution_role()

tensorflow_model = TensorFlowModel(model_data='s3://my-bucket-name/path_to_the_model/model.tar.gz',
                                   role=role,
                                   framework_version='2.1.0')
</code></pre>
<p>This resulted in the error:
TypeError: <strong>init</strong>() missing 1 required positional argument: 'entry_point'</p>
<p>The documentation of sagemaker.tensorflow.model.TensorFlowModel() (<a href=""https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/sagemaker.tensorflow.html"" rel=""nofollow noreferrer"">here</a>) says about the argument entry point:</p>
<blockquote>
<p>entry_point (str) – Path (absolute or relative) to the Python source file which should be executed as the entry point to model hosting. If source_dir is specified, then entry_point must point to a file located at the root of source_dir.</p>
</blockquote>
<p>Is the documentation wrong or am I missing a major difference between my code and the example from the documentation?</p>",0,0,2020-10-15 13:03:51.413000 UTC,2.0,2020-11-17 15:39:30.360000 UTC,2,python|tensorflow|keras|amazon-sagemaker,227,2020-10-06 16:10:59.123000 UTC,2021-06-16 10:58:35.157000 UTC,,21,10,0,10,,,,,,['amazon-sagemaker']
"Storage issue on Sagemaker, even when more provided","<p>I am running a sagemaker instance which always gives me an exception the same place in the cycle, even if I allocate more storage. So it might not be a storage issue, but I am at a loss for why it fails.</p>
<p>I got the error at the same spot no matter if I allocate 1024gb or 100gb storage (estimator volume_size).
The DiskUtilization sits at 23% when it crashes on 100gb allocated (however, that number does not update in real time, so it is probably higher)</p>
<pre><code>2021-06-10T13:25:32.141+02:00   terminate called after throwing an instance of 'dmlc::Error'

2021-06-10T13:25:45.144+02:00   what(): [11:25:31] src/io/local_filesys.cc:38: Check failed: std::fwrite(ptr, 1, size, fp_) == size: FileStream.Write incomplete

2021-06-10T13:25:45.144+02:00   Stack trace: [bt] (0) /usr/local/lib/python3.6/dist-packages/mxnet/libmxnet.so(+0x3c58ea9) [0x7f0280805ea9]
</code></pre>
<p>I am loading parquet files and saving them as ndarrays of 10000 rows pr. file. And around 120000 rows, it crashes.
I am doing this in order to give mxnet a dataset with random access, which I cannot do with just parquet files.</p>
<p>Any help is appreciated.</p>",2,0,2021-06-10 13:07:02.603000 UTC,,2021-06-14 08:51:35.853000 UTC,1,python|amazon-web-services|amazon-sagemaker|mxnet,224,2021-06-10 12:56:47.430000 UTC,2021-12-17 12:57:05.463000 UTC,,11,0,0,1,,,,,,['amazon-sagemaker']
AWS Sagemaker - Custom Training Job not saving Model output,"<p>I'm running a training job using AWS SageMaker and i'm using a custom Estimator based on an available docker image from AWS. I wanted to get some feedback on whether my process is correct or not prior to deployment.</p>
<p>I'm running the training job in a docker container using 'local' in a SageMaker notebook instance and the training job runs successfully. However, after the job completes and saves the model to opt/model/models within the docker image, once the docker container exits, the model saved from training is lost. Ideally, i'd like to use the model for inference, however, I'm not sure about the best way of doing it. I have also tried the training job after pushing the image to ECR, but the same thing happens.</p>
<p>It is my understanding that the docker state is lost, once the image exits, as such, is it possible to persist the model that was produced in training in the image? One option I have thought about is saving the model output to an S3 bucket once the training job is complete, then pulling that model into another docker image for inference. Is this expected behaviour and the correct way of doing it?</p>
<p>I am fairly new to using SageMaker but i'd like to do it according to best practices. I've looked at a lot of the AWS documents and followed the tutorials but it doesn't seem to mention explicitly if this is how it should be done.</p>
<p>Thanks for any feedback on this.</p>",1,3,2022-03-19 21:43:13.147000 UTC,,,2,amazon-web-services|docker|machine-learning|deployment|amazon-sagemaker,220,2022-02-05 02:09:21.167000 UTC,2022-09-13 00:30:03.273000 UTC,,21,2,0,1,,,,,,['amazon-sagemaker']
SageMaker : Sckit-learn RandomForest : REST API Value ERROR,"<p>My Lambda Code is Below.</p>
<pre><code>import os
import io
import boto3
import json
import csv

# grab environment variables
ENDPOINT_NAME = os.environ['ENDPOINT_NAME']
runtime= boto3.client('runtime.sagemaker')

def lambda_handler(event, context):
    print(&quot;Received event: &quot; + json.dumps(event, indent=2))
    
    data = json.loads(json.dumps(event))
    payload = data['data']
    print(payload)
    
    wrapper = csv.reader(payload.strip().split('\n'))
    for record in wrapper:
        print(record)
   
    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME,
                                       ContentType='text/csv',
                                       Body=wrapper)
    print(response)
    result = json.loads(response['Body'].read().decode())
    
    return result
</code></pre>
<p>My input value is</p>
<pre><code>{
  &quot;data&quot;: &quot;231, -43&quot;
}
</code></pre>
<p>Error message is</p>
<pre><code>Response
{
  &quot;errorMessage&quot;: &quot;Parameter validation failed:\nInvalid type for parameter Body, value: &lt;_csv.reader object at 0x7f638af1a6d8&gt;, type: &lt;class '_csv.reader'&gt;, valid types: &lt;class 'bytes'&gt;, &lt;class 'bytearray'&gt;, file-like object&quot;,
  &quot;errorType&quot;: &quot;ParamValidationError&quot;,
  &quot;stackTrace&quot;: [
    [
      &quot;/var/task/lambda_function.py&quot;,
      24,
      &quot;lambda_handler&quot;,
      &quot;Body=wrapper)&quot;
    ],
    [
      &quot;/var/runtime/botocore/client.py&quot;,
      357,
      &quot;_api_call&quot;,
      &quot;return self._make_api_call(operation_name, kwargs)&quot;
    ],
    [
      &quot;/var/runtime/botocore/client.py&quot;,
      649,
      &quot;_make_api_call&quot;,
      &quot;api_params, operation_model, context=request_context)&quot;
    ],
    [
      &quot;/var/runtime/botocore/client.py&quot;,
      697,
      &quot;_convert_to_request_dict&quot;,
      &quot;api_params, operation_model)&quot;
    ],
    [
      &quot;/var/runtime/botocore/validate.py&quot;,
      293,
      &quot;serialize_to_request&quot;,
      &quot;raise ParamValidationError(report=report.generate_report())&quot;
    ]
  ]
}

Function Logs
START RequestId: 0e33f157-ae77-4524-96f2-78a2fe82bf5b Version: $LATEST
Received event: {
  &quot;data&quot;: &quot;231, -43&quot;
}
231, -43
['231', ' -43']
Parameter validation failed:
Invalid type for parameter Body, value: &lt;_csv.reader object at 0x7f638af1a6d8&gt;, type: &lt;class '_csv.reader'&gt;, valid types: &lt;class 'bytes'&gt;, &lt;class 'bytearray'&gt;, file-like object: ParamValidationError
Traceback (most recent call last):
  File &quot;/var/task/lambda_function.py&quot;, line 24, in lambda_handler
    Body=wrapper)
  File &quot;/var/runtime/botocore/client.py&quot;, line 357, in _api_call
    return self._make_api_call(operation_name, kwargs)
  File &quot;/var/runtime/botocore/client.py&quot;, line 649, in _make_api_call
    api_params, operation_model, context=request_context)
  File &quot;/var/runtime/botocore/client.py&quot;, line 697, in _convert_to_request_dict
    api_params, operation_model)
  File &quot;/var/runtime/botocore/validate.py&quot;, line 293, in serialize_to_request
    raise ParamValidationError(report=report.generate_report())
botocore.exceptions.ParamValidationError: Parameter validation failed:
Invalid type for parameter Body, value: &lt;_csv.reader object at 0x7f638af1a6d8&gt;, type: &lt;class '_csv.reader'&gt;, valid types: &lt;class 'bytes'&gt;, &lt;class 'bytearray'&gt;, file-like object

END RequestId: 0e33f157-ae77-4524-96f2-78a2fe82bf5b
REPORT RequestId: 0e33f157-ae77-4524-96f2-78a2fe82bf5b  Duration: 19.80 ms  Billed Duration: 20 ms  Memory Size: 128 MB Max Memory Used: 68 MB  Init Duration: 261.73 ms

Request ID
0e33f157-ae77-4524-96f2-78a2fe82bf5b
</code></pre>
<p>I don't know the reason about the error.</p>
<p>Thank you.</p>",1,0,2021-04-12 02:02:40.303000 UTC,,,0,amazon-web-services|api|lambda|amazon-sagemaker,21,2019-10-01 00:27:21.900000 UTC,2021-05-04 05:37:11.043000 UTC,,11,0,0,2,,,,,,['amazon-sagemaker']
Logger output in AWS Sagemaker Jupyter notebook,"<p>I would like to see the custom logs that I create inside an AWS Sagemaker JupyterLab notebook (that uses a Glue development endpoint). I want to see them as the output of a notebook cell.</p>

<p>I tried with:</p>

<pre><code>import logging
logger = logging.getLogger()
logger.setLevel(logging.DEBUG)
logger.info(""I want to see it as output in my notebook cell."")
</code></pre>

<p>and also with:</p>

<pre><code>sc = SparkContext.getOrCreate()
glueContext = GlueContext(sc)
logger = glueContext.get_logger()
logger.info(""I want to see it as output in my notebook cell."")
</code></pre>

<p>but both give no output, in comparison to a <code>print(""Hi"")</code> which correctly gives the ""Hi"" output on the notebook.</p>

<p>Although I want the logs on the notebook itself, I've also checked if they are available as CloudWatch Logs, but there the logs of my notebook look like <code>[W 10:42:34.540 NotebookApp] zmq message arrived on closed channel</code> (in any case, I want them on the notebook, not in CloudWatch).</p>

<p>Many thanks </p>",1,2,2020-02-28 12:31:43.380000 UTC,,,5,python|jupyter-notebook|aws-glue|amazon-sagemaker|jupyter-lab,3355,2013-08-01 17:22:32.900000 UTC,2022-09-22 13:22:58.057000 UTC,,2041,288,3,61,,,,,,['amazon-sagemaker']
Provisioning failed with error from CloudFormation when create aws sagemaker mlops project,"<p>I am testing out MLOps using SageMaker studio and creating a project using a template for MLOps provided by SageMaker: MLOps template for model building, training, deployment and monitoring</p>
<p>I am getting this error when creating the project.(see picture 1)</p>
<p>My relation and IAM role is picture 2 and 3.</p>
<p>Could someone please help me what am I missing here?</p>
<p><a href=""https://i.stack.imgur.com/2QZIf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2QZIf.png"" alt=""picture1"" /></a>
<a href=""https://i.stack.imgur.com/D8wMG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/D8wMG.png"" alt=""picture2"" /></a>
<a href=""https://i.stack.imgur.com/hVWfH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hVWfH.png"" alt=""picture3"" /></a></p>",1,0,2022-06-16 09:31:15.283000 UTC,,,0,amazon-web-services|amazon-sagemaker,54,2017-07-11 04:06:12.730000 UTC,2022-06-30 10:21:22.480000 UTC,,105,2,0,31,,,,,,['amazon-sagemaker']
AWS Sagermaker ClientError: Data download failed:PermanentRedirect (301),<p>ClientError: Data download failed:PermanentRedirect (301): The bucket is in this region: us-west-1. Please use this region to retry the request</p>,1,0,2018-08-01 09:10:28.520000 UTC,,,1,amazon-web-services|amazon-s3|amazon-sagemaker,708,2014-06-07 09:51:43.607000 UTC,2019-08-06 21:54:47.933000 UTC,,23,0,0,4,,,,,,['amazon-sagemaker']
Not able to set DataSet Type in Data Labeling MLStudio,"<p>Not able to set DataSet Type in Data Labeling project  MLStudio.</p>
<p>Default dataset file type is displaying always. I want to work with panda_dataframe. How can I set dataset to tabuler dataset while creating the dataset.</p>
<p>Also how can I change the Filedataset to Tabular datasetI want to apply pandas_dataframe. but it is not available with file dataset.</p>
<pre><code>AttributeError                            Traceback (most recent call last)
&lt;ipython-input-10-2d5c0c116404&gt; in &lt;module&gt;
     10 
     11 
---&gt; 12 pd_fill = dsfill.to_pandas_dataframe(file_handling_option=FileHandlingOption.DOWNLOAD, target_path='./download/', overwrite_download=True)
     13 pd_fill

AttributeError: 'FileDataset' object has no attribute 'to_pandas_dataframe'
</code></pre>
<p><a href=""https://i.stack.imgur.com/um4J0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/um4J0.png"" alt=""enter image description here"" /></a></p>",1,1,2020-08-29 03:31:49.850000 UTC,,,1,dataset|azure-machine-learning-studio|azure-machine-learning-service,640,2018-05-18 14:37:12.313000 UTC,2021-10-09 15:32:17.127000 UTC,,109,2,0,47,,,,,,"['azure-machine-learning-studio', 'azure-machine-learning-service']"
"what is the difference between Duration, TT (Training Time), RunTime on ML Performance Report of mlflow","<p>I compared the performance of machine learning algorithms by applying pycaret and k-fold on a data and reported it on mlflow. There are three time columns in the report, these are duration, TT(training time) and runtime. When I look at these times, they are all different from each other. I know the training time, but what are the other times?</p>",0,1,2022-02-02 19:57:48.733000 UTC,,2022-02-02 20:54:32.060000 UTC,0,machine-learning|classification|mlflow|pycaret,54,2022-01-12 22:39:42.437000 UTC,2022-03-29 11:38:22.727000 UTC,,1,0,0,0,,,,,,['mlflow']
Machine Learning Supervised Clustering on a Field,"<p>I'm trying to build a model on labeled data that can cluster on a specific field.</p>

<p>Example data:</p>

<p><a href=""https://i.stack.imgur.com/az8Id.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/az8Id.png"" alt=""enter image description here""></a></p>

<p>The field I want to cluster on is class_id. I want to be able to give the model (class_id, date, class_time) and get an estimated time in minutes that a student stays in my class for a specific date and time. I want to cluster by class_id because each class is different in its own way. Is there a model or way that can do this? Thanks!</p>",1,0,2019-12-12 21:00:43.170000 UTC,,2019-12-12 22:29:49.513000 UTC,-2,machine-learning|cluster-analysis|amazon-sagemaker|google-cloud-automl|machine-learning-model,38,2018-05-07 17:42:25.007000 UTC,2019-12-13 17:28:14.437000 UTC,,17,0,0,23,,,,,,['amazon-sagemaker']
sagemaker xgboost output to be JSON,"<p>I am new to AWS sagemaker and trying to do a simple test, where I am trying to call the xgboost model.</p>
<pre><code>xgboost_container = sagemaker.image_uris.retrieve(&quot;xgboost&quot;, 'us-east-1', &quot;1.2-1&quot;)
</code></pre>
<p>Creating the model and endpoint:</p>
<pre><code>from sagemaker.serializers import JSONSerializer
endp_name =&quot;myendpoint&quot;
acc_model = sm_model.deploy(initial_instance_count=1, 
                instance_type='ml.m5.4xlarge',
                endpoint_name=endp_name, 
                serializer=JSONSerializer(),
                deserializer= sagemaker.deserializers.JSONDeserializer()
                            
               )
</code></pre>
<p>Creating the predictor instance:</p>
<pre><code>from sagemaker.predictor import Predictor
from sagemaker.serializers import CSVSerializer, JSONSerializer 

sess = sagemaker.Session()

payload ={
&quot;var1&quot;:1,
&quot;var2&quot;:2,
&quot;var3&quot;:3,
&quot;var4&quot;:0,
&quot;var5&quot;:4,
&quot;var6&quot;:0,
&quot;var7&quot;:5,
&quot;var8&quot;:45,

}

predictor = Predictor(
    endpoint_name=endp_name, sagemaker_session=sess, serializer=JSONSerializer(),
deserializer=JSONSerializer()  )
</code></pre>
<p>and then predicting:</p>
<pre><code>predictor.predict(payload)
</code></pre>
<p>I want the output of the predictor.predict to be JSON format, however when I run this I get this.
<a href=""https://i.stack.imgur.com/gnlxl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gnlxl.png"" alt=""enter image description here"" /></a></p>
<p>What need to be done so that I can see the output as JSON?</p>
<p>PS: If I remove the deserializer I get the output as byte:
<a href=""https://i.stack.imgur.com/0ncvK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0ncvK.png"" alt=""enter image description here"" /></a></p>
<p>and if I change it to CSVDeserializer I get it out as:
<a href=""https://i.stack.imgur.com/bv7L6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bv7L6.png"" alt=""enter image description here"" /></a></p>",1,0,2021-06-01 02:50:28.260000 UTC,,2021-06-01 02:59:01.413000 UTC,0,amazon-sagemaker,261,2016-07-31 01:48:46.907000 UTC,2022-04-10 19:27:19.153000 UTC,,101,20,0,30,,,,,,['amazon-sagemaker']
Failed to load image in Amazon Sagemaker Ground Truth,"<p>I am trying to label a custom dataset that I have but after labelling a certain number of images, Amazon Sagemaker displays an error stating 'Failed to load image, refresh if this persists'. I tried refreshing the page but still this error persists.</p>",0,1,2021-10-28 00:21:04.340000 UTC,,,1,amazon|amazon-sagemaker,108,2016-07-11 18:05:59.143000 UTC,2022-08-12 00:21:33.517000 UTC,,51,1,0,2,,,,,,['amazon-sagemaker']
WANDB run initialization,"<p>I wanted to try using wandb to log runs of my ML experiments for a project; but I am not able to initialize the run itself.
I tried:</p>
<p><code>run = wandb.init(project=&quot;name&quot;,entity=&quot;username&quot;,name=&quot;classification&quot;)</code></p>
<p>This results in:
wandb: W&amp;B API key is configured (use <code>wandb login --relogin</code> to force relogin)</p>
<p>wandb: Network error (ConnectTimeout), entering retry loop.</p>
<p>wandb: Network error (ConnectTimeout), entering retry loop.</p>
<p>What can I do to fix this? (I did login through the terminal before launching this cell idk what else to try)</p>",0,2,2022-04-18 19:58:49.900000 UTC,,2022-04-18 20:13:21.033000 UTC,0,python|jupyter-notebook|wandb,337,2022-04-18 19:51:31.997000 UTC,2022-06-02 19:16:00.160000 UTC,,1,0,0,0,,,,,,['wandb']
How to import a library “rmarkdown” using the AzureML package?,"<p>I'm trying to import <code>rmarkdown</code> into <code>AzureML</code> for one of my projects.</p>

<p>This is the function I'm trying to upload into <code>AzureML</code>. </p>

<p>The <code>R.version</code> check is because the function is evaluated in the local environment before uploaded to <code>AzureML</code>. </p>

<pre><code>fun &lt;- function (b5) {
    if (R.version[[""os""]]==""mingw32"" &amp;&amp; ! require(talection)) {
        install.packages(
            ""src/rmarkdown_0.9.6.zip"",
            lib=""."",
            type=""win.binary"",
            repos=NULL,
            verbose=TRUE)
    }
    ans &lt;- as.data.frame(c(""Finished""))
}
</code></pre>

<p><code>rmarkdown_0.9.6.zip</code> is in a <code>miniCRAN</code> library. </p>

<p>The following code, is the code that uploads <code>rmarkwodn</code> to <code>Azure ML</code>. Please note the line <code>packages</code>, which tells R to upload <code>rmarkdown</code> to <code>Azure ML</code>. </p>

<pre><code>test &lt;- as.data.frame(
    cbind(
        c(0.0,  0.3,  0.0,  0.0,  0.0),
        c(0.0,  0.0,  0.0, -0.4,  0.0),
        c(0,      0,    0,    0,    0))
)

ep &lt;- publishWebService (
  ws,
  fun = fun,
  name = ""Talection-fun"",
  inputSchema = test,
  outputSchema = list(
    ans = ""character""
  ),
  packages = c(""rmarkdown"")
)

print(consume(ep,test))
</code></pre>

<p>The code returns </p>

<blockquote>
  <p>Request failed with status 400. Waiting 12.7 seconds before retry<br>
  Request failed with status 400. Waiting 33.6 seconds before retry<br>
  Request failed with status 400. Waiting 76.7 seconds before retry<br>
  Request failed with status 400. Waiting 234.3 seconds before retry<br>
  Request failed with status 400. Waiting 123.1 seconds before retry<br>
  Error: AzureML returns error code:<br>
  HTTP status code : 400<br>
  AzureML error code  : LibraryExecutionError  </p>
</blockquote>

<p>Any and all relevant suggestions welcome. Thank you.</p>",0,1,2016-05-31 15:17:27.620000 UTC,1.0,2016-11-03 12:19:31.603000 UTC,3,r|r-markdown|azure-machine-learning-studio,147,2012-11-11 09:14:39.067000 UTC,2022-06-13 17:53:53.847000 UTC,"Levanger, Norway",445,9,0,63,,,,,,['azure-machine-learning-studio']
azure ml & Pytorch: sample conda-dependencies.yml and docker?,"<p>Could you please point me to the documentation sample showcasing how to put together pytorch dependencies for training on AzureML?
Few related questions to the scenario of running pytorch training workloads on AzureML: </p>

<ul>
<li>How can I set cuda version to 10.1? </li>
<li><p>Could you please point to sample demonstrating how to use “official” pytorch docker <a href=""https://hub.docker.com/r/pytorch/pytorch"" rel=""nofollow noreferrer"">https://hub.docker.com/r/pytorch/pytorch</a>  (which should have all cuda stuff <a href=""https://github.com/pytorch/pytorch/blob/master/docker/pytorch/Dockerfile"" rel=""nofollow noreferrer"">https://github.com/pytorch/pytorch/blob/master/docker/pytorch/Dockerfile</a>)?.  </p></li>
<li><p>I’ve found <a href=""https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/ml-frameworks/pytorch/training/distributed-pytorch-with-horovod/distributed-pytorch-with-horovod.yml"" rel=""nofollow noreferrer"">distributed-pytorch-with-horovod.ym</a>l in the docs but it does not mention any pytorch dependencies  -- am I looking in the right place?</p></li>
</ul>",1,0,2019-12-26 19:28:05.980000 UTC,,2020-02-03 22:21:16.267000 UTC,1,docker|pytorch|azure-machine-learning-service,490,2017-05-14 21:57:35.123000 UTC,2020-01-24 22:27:50.953000 UTC,"Redmond, WA, United States",31,0,0,20,,,,,,['azure-machine-learning-service']
Access to Azure Keyvault inside Azure Container Instance,"<p>I have a machine learning model deployed on azure container instance and I need to access to key vault. When i use command below</p>
<pre><code>credential = DefaultAzureCredential()
</code></pre>
<p>It can't authenticate thus i cannot reach my secrets.</p>
<p>How can i reach keyvault inside azure container instance?</p>",1,1,2021-08-04 08:16:00.597000 UTC,,,0,azure-keyvault|azure-machine-learning-studio|azure-container-instances,306,2021-03-24 10:00:57.473000 UTC,2022-09-10 14:40:32.643000 UTC,,21,0,0,9,,,,,,['azure-machine-learning-studio']
Sagemaker CPU performance slow,"<p>I have a question regarding to retriving records of a big data by using Sagemaker. I used pd.read_spl with chunk to read a big data and use for loop to go through all the sub dataframes to generate a bigger dataframe. While I used my own laptop(8cores with 8G Ram) it took 1.4 hours. And then I moved to Sagemaker and used instance (8cores,32G Ram) and it took 6hrs! I thought it should have been faster. The code is the same. I couldn't understand why. The only thing I can see difference is, the CPU performance in my laptop can go up to 70% sometimes while in Sagemaker it is only about 5%. Again, I am not sure why it is the case. I thought Sagemaker should perform much better. </p>",0,6,2020-06-16 12:39:01.450000 UTC,,,0,python|mysql|bigdata|cpu|amazon-sagemaker,546,2019-07-09 13:12:51.853000 UTC,2021-03-15 03:31:55.007000 UTC,"London, UK",1,0,0,4,,,,,,['amazon-sagemaker']
Trigger Sagemaker Inference from SQS event,"<p>I have previously created Sagemaker Inference end-point and triggered from with API Gateway, but how can I trigger it from SQS event.</p>
<p>Usecase is: We are getting events in SQS, and whenever we get an event in SQS, we need to run the model.</p>",1,2,2022-08-02 05:45:14.177000 UTC,,2022-08-02 06:21:56.790000 UTC,0,amazon-web-services|amazon-sqs|amazon-sagemaker,69,2014-01-15 05:56:19.007000 UTC,2022-09-22 16:09:40.897000 UTC,"Gurugram, Haryana, India",6357,742,504,1097,,,,,,['amazon-sagemaker']
Native logging module not printing to stdout in AzureML,"<p>I am trying to use the standard <code>logging</code> module in my AzureML run but nothing gets printed out to stdout.</p>

<p>I tried following instructions from <a href=""https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-enable-logging#python-native-logging-settings"" rel=""nofollow noreferrer"">here</a> but I have not been successful.</p>

<p>This is what I am running:</p>

<pre class=""lang-py prettyprint-override""><code>import logging

logging.basicConfig(level=logging.DEBUG)

logger = logging.getLogger(__name__)

logger.info('WHY IS THIS NOT WORKING?')
</code></pre>

<p>Am I doing something wrong?</p>",2,0,2019-11-21 01:34:21.780000 UTC,,,3,azure-machine-learning-service,526,2014-09-14 14:48:11.817000 UTC,2022-03-01 18:30:27.267000 UTC,Brazil,300,38,1,30,,,,,,['azure-machine-learning-service']
Amazon S3 files access in Sagemaker instance,"<p>How exactly is file in my S3 bucket is accessible in my Sagemaker instance?
Given that I am not adding any access provision...what exactly is happening in the backend?</p>",1,0,2021-06-29 14:31:59.120000 UTC,,,0,amazon-web-services|amazon-s3|amazon-sagemaker,1234,2021-06-16 15:15:46.760000 UTC,2021-07-05 02:44:15.657000 UTC,,1,0,0,1,,,,,,['amazon-sagemaker']
Is the Google GCP AI Platform logging the prediction output?,"<p>I'm serving a Google GCP VertexAI AutoML model to an endpoint. Through the endpoint, I could make online predictions. Now my question is, where is the response/prediction output logged? Is it configurable, or Automl serving do not log data due to security policies?</p>",0,0,2022-02-06 10:47:49.300000 UTC,,,0,google-cloud-platform|google-cloud-logging|google-cloud-automl|google-cloud-vertex-ai|gcp-ai-platform-training,60,2017-02-01 14:28:03.537000 UTC,2022-09-14 04:38:52.680000 UTC,"Chennai, Tamil Nadu, India",1,0,0,4,,,,,,['google-cloud-vertex-ai']
Is there a way to identify Azure ML vs Local PC environment using Python VS Code or JupyterLab?,"<p>I am new to Azure ML and VS Code. I'm running some projects in Azure ML, but at times I have to test it on my local PC. I was wondering if there's a way to distinguish between the two automatically so that I can load the data from the appropriate location depending on the environment. I couldn't find anything on the web search, or I couldn't understand some that I found.</p>",0,2,2022-05-26 19:04:47.643000 UTC,,,0,python|visual-studio-code|azure-machine-learning-service,42,2016-10-20 15:12:05.470000 UTC,2022-06-22 00:17:46.243000 UTC,,801,79,4,106,,,,,,['azure-machine-learning-service']
How to write if-else statements in a lifecycle configuration script,"<p>I have a sagemaker notebook instance having two jupyter notebook ipynb files. When I had one jupyter notebook, I was able to run it automatically with one lambda function trigger and lifecycle configuration.</p>
<p>Now I have two jupyter notebooks and corresponding two lambda function triggers. How can I run them based on the trigger by changing the lifecycle configuration script.</p>
<p>The trigger is file uploading into S3. Based on what location the file is added, the corresponding jupyter notebook should run</p>",1,0,2022-04-26 14:21:00.397000 UTC,,2022-04-29 13:11:51.090000 UTC,0,aws-lambda|amazon-sagemaker,76,2020-01-03 17:09:09.303000 UTC,2022-09-20 13:22:32.150000 UTC,Hyderabad India,415,105,0,44,,,,,,['amazon-sagemaker']
SageMaker Monitoring Tutorial boto3 Object Function Type Error,"<p>I am following the steps in the SageMaker Monitoring Tutorial here:
<a href=""https://sagemaker-examples.readthedocs.io/en/latest/sagemaker_model_monitor/introduction/SageMaker-ModelMonitoring.html"" rel=""nofollow noreferrer"">https://sagemaker-examples.readthedocs.io/en/latest/sagemaker_model_monitor/introduction/SageMaker-ModelMonitoring.html</a></p>
<p>And for the line:</p>
<pre><code>bucket.Object(code_prefix + &quot;/preprocessor.py&quot;).upload_file(&quot;preprocessor.py&quot;)
</code></pre>
<p>I get the error:</p>
<blockquote>
<p>TypeError: expected string or bytes-like object</p>
</blockquote>
<p>Which I dont understand, because the input to the <code>upload_file()</code> function is <code>&quot;preprocessor.py&quot;</code> which is  a string.</p>",1,2,2022-06-27 21:51:40.903000 UTC,,2022-06-28 22:11:04.457000 UTC,0,amazon-web-services|boto3|typeerror|monitoring|amazon-sagemaker,30,2018-02-25 07:22:56.310000 UTC,2022-08-29 16:07:10.467000 UTC,"Auburn, Al",447,10,0,26,,,,,,['amazon-sagemaker']
Issue with data lake mounting in custom RStudio application Azure ML,"<ol>
<li>previously while creating a compute instance  we were able to see RStudio application by default and we were able to mount/access the data lake from RStudio.</li>
</ol>
<p><a href=""https://i.stack.imgur.com/J17ne.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/J17ne.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/3l8Q4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3l8Q4.png"" alt=""enter image description here"" /></a></p>
<ol start=""2"">
<li>In current situation we are not able to access RStudio application by default.</li>
</ol>
<p><a href=""https://i.stack.imgur.com/nx5GL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nx5GL.png"" alt=""enter image description here"" /></a></p>
<ol start=""3"">
<li>with the help of below link we are able to create custom RStudio application</li>
</ol>
<p><a href=""https://docs.microsoft.com/en-us/azure/machine-learning/how-to-create-manage-compute-instance?tabs=azure-studio"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/machine-learning/how-to-create-manage-compute-instance?tabs=azure-studio</a></p>
<p><a href=""https://i.stack.imgur.com/flQyy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/flQyy.png"" alt=""enter image description here"" /></a></p>
<p>4.In custom RStudio we are not able to mount/access the data lake.</p>
<p><a href=""https://i.stack.imgur.com/2dWL9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2dWL9.png"" alt=""enter image description here"" /></a></p>
<p>Is there way to mount/access the data lake in custom RStudio app</p>",0,1,2022-08-19 04:30:31.000000 UTC,,2022-08-23 15:38:07.767000 UTC,0,r|azure|installation|azure-machine-learning-studio|rstudio-server,71,2022-06-15 09:26:58.330000 UTC,2022-09-22 12:13:16.827000 UTC,Pune,1,0,0,5,,,,,,['azure-machine-learning-studio']
Is it possible to specify MLflow project Environment through a Dockerfile (instead of an image)?,"<p>To my understanding, currently (May 2019) mlflow support running project in docker environment; however, it needs the docker image already been built. This leaves the docker image building to be a separate workflow. What is the suggested way to run a mlflow project from Dockerfile? </p>

<p>Is there plans to support targeting Dockerfile natively in mlflow? What are the considerations about using image vs Dockerfile? Thanks!</p>",2,0,2019-05-13 21:20:34.943000 UTC,,2019-05-13 23:47:12.507000 UTC,2,docker|machine-learning|artificial-intelligence|databricks|mlflow,631,2013-12-11 04:18:22.123000 UTC,2022-09-23 23:07:48.663000 UTC,,3405,641,8,1094,,,,,,['mlflow']
Endpoint in Azure Machine Learning not consumable,"<p>I have created a pipeline in Azure Machine Learning Designer. Below is the training pipeline. As you can see I have created a custom python model as I have a requirement to use an external package known as K-Modes (<code>pip install modes</code>). I have created a real-time inference pipeline off the back of the training pipeline and it ran successfully. When I deploy the inference pipeline, it is in a healthy state however I can not consume or test it.</p>
<p>In python, I get this error when trying to consume it.</p>
<pre><code>{'error': {'code': 500, 'message': 'Internal Server Error. Run: Server internal error is from Module Score Model', 'details': ''}}
</code></pre>
<p>The <code>Module Score Model</code> has completed successfully and is green in the inference pipeline.</p>
<p><a href=""https://i.stack.imgur.com/bnbVQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bnbVQ.png"" alt=""enter image description here"" /></a></p>",0,1,2020-07-08 07:22:28.917000 UTC,,,2,azure|azure-machine-learning-studio|azure-machine-learning-service,194,2013-05-20 16:44:24.257000 UTC,2022-09-21 15:00:56.043000 UTC,South Africa,2907,68,8,402,,,,,,"['azure-machine-learning-studio', 'azure-machine-learning-service']"
How can I implement iterative incremental training for xgboost? Is it worth doing?,"<p>I am currently using Xgboost version 1.3.1. There is a custom docker image created out of training scripts and uses SageMaker to run training. Training data is also present in S3. I am facing an issue recently that input data size (data frame) required is more than what the box could support (and there is no higher instance after that). And hence facing OOM issue</p>
<p>I would like to know, if there is a way to resolve this big data issue. Or is it possible to load data iteratively and train using xgb_model option? If so how?</p>
<p>Thanks in advance</p>",1,0,2021-08-27 20:45:22.250000 UTC,,,0,amazon-s3|xgboost|amazon-sagemaker,78,2020-10-14 08:29:20.230000 UTC,2022-07-25 07:02:28.140000 UTC,,21,0,0,6,,,,,,['amazon-sagemaker']
azure ml update service erro AttributeError: AttributeError: 'str' object has no attribute 'id',"<p>I am tying to update existing webservice using new Azure ML package
Its failing with error - AttributeError: 'str' object has no attribute 'id'</p>
<p>&quot;/opt/hostedtoolcache/Python/3.6.14/x64/lib/python3.6/site-packages/azureml/core/webservice/aks.py&quot;, line 678, in update</p>
<p>patch_list.append({'op': 'replace', 'path': '/imageId', 'value': image.id})</p>
<p>AttributeError: 'str' object has no attribute 'id'</p>
<p>Here is the script I am using -</p>
<pre><code>ws = Workspace.get(
        name=workspace_name,
        subscription_id=subscription_id,
        resource_group=resource_group,
        auth=cli_auth)

model = Model.register(model_path = model_path,
                   model_name = model_name,
                   #tags = {&quot;key&quot;: &quot;1&quot;},
                   description = model_description,
                   workspace = ws)

image_config = ContainerImage.image_configuration(execution_script=&quot;score.py&quot;, 
                                              runtime=&quot;python&quot;, 
                                              conda_file=&quot;packagesenv.yml&quot;)
image = 'testazureml'
service_name = 'testazureml'

# Retrieve existing service
service = Webservice(name = service_name, workspace = ws)

print(service)

service.update(image,'image.id')
</code></pre>
<p>please help
I have been trying with different methods
as - 'id', 'image_id'
its still failing</p>",2,0,2021-07-22 00:11:13.600000 UTC,,2021-07-22 02:12:59.757000 UTC,2,azure-machine-learning-studio|azure-machine-learning-service,437,2019-08-31 00:00:56.790000 UTC,2022-09-16 17:26:27.993000 UTC,,417,53,0,233,,,,,,"['azure-machine-learning-studio', 'azure-machine-learning-service']"
Requirements.txt is not packed in the model.tar.gz using Sagemaker Pytorch Estimator,"<p>I'm using SageMaker Pipeline workflow to train a model and register it. Then later I'll create an endpoint from the registered model.</p>
<p>I need to install some python packages in my inference.py file, such as gensim. I put a requirements.txt file in the same folder as train.py and inference.py.</p>
<p><strong>The problem is that the requirements.txt is not being packed in the model.tar.gz.</strong> That's why although the training and creating the endpoint work fine, but when I check the loggings of the deployed endpoint I see the following error:</p>
<pre><code>ModuleNotFoundError: No module named 'gensim'
</code></pre>
<p>This is a part of my script for training and registering the model.</p>
<pre><code>from sagemaker.pytorch.estimator import PyTorch
from sagemaker.workflow.step_collections import RegisterModel
from sagemaker.workflow.steps import (
    ProcessingStep,
    TrainingStep,
)

    train_estimator = PyTorch(entry_point= 'train.py',
                                source_dir= BASE_DIR,
                                instance_type= &quot;ml.m5.2xlarge&quot;,
                                instance_count=1,
                                role=role,
                                framework_version='1.8.0',
                                py_version='py3',
                                )
    step_train = TrainingStep(
        name=&quot;TrainStep&quot;,
        estimator=train_estimator,
        inputs={
                &quot;train&quot;: sagemaker.TrainingInput(
                            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[
                            &quot;train_data&quot;
                            ].S3Output.S3Uri,
                            content_type= 'text/csv',
                        ),
        },
    )
    step_register = RegisterModel(
        name=&quot;RegisterStep&quot;,
        estimator= train_estimator,
        model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,
        content_types=[&quot;application/json&quot;],
        response_types=[&quot;application/json&quot;],
        inference_instances=[&quot;ml.t2.medium&quot;, &quot;ml.m5.2xlarge&quot;],
        transform_instances=[&quot;ml.m5.large&quot;],
        model_package_group_name=model_package_group_name,
        approval_status=model_approval_status,
        source_dir = BASE_DIR,
        entry_point= os.path.join(BASE_DIR, &quot;inference.py&quot;),
        depends_on = [step_train]
    )
</code></pre>
<p>This is the structure of my files:</p>
<pre><code>-abalone
  - __init__.py
  - train.py
  - inference.py
  - requirements.py
  - preprocess.py
  - evaluate.py
  - pipeline.py
</code></pre>
<p>BASE_DIR refers to abalone folder.</p>
<p>In the model.tar.gz I see:</p>
<pre><code>- model.pth
- model.pth.wv.vectors_ngrams.npy
- code
  - __pycache__
  - train.py
  - _repack_model.py
  - inference.py
  - preprocess.py
  - evaluate.py
  - __init__.py
  - pipeline.py
</code></pre>
<p>You can see that it contains everything except the requirements.txt file.</p>
<p>In the sagemaker <a href=""https://sagemaker.readthedocs.io/en/v2.25.2/frameworks/pytorch/using_pytorch.html?highlight=requirements.txt"" rel=""nofollow noreferrer"">documents</a> it says:</p>
<p>&quot;The PyTorch and PyTorchModel classes repack model.tar.gz to include the inference script (and related files), as long as the framework_version is set to 1.2 or higher.&quot;</p>
<p>But you can see although my framework_version is higher than 1.2, but still it doesn't pack requirements.txt file in the model.tar.gz.</p>
<p>Can someone please help me to fix this issue?</p>",1,7,2022-01-04 09:16:38.567000 UTC,1.0,2022-01-04 16:10:06.603000 UTC,0,pytorch|amazon-sagemaker|endpoint|inference|requirements.txt,583,2015-12-23 16:48:13.150000 UTC,2022-09-17 08:32:34.823000 UTC,Finland,398,21,1,28,,,,,,['amazon-sagemaker']
How can we add complex preprocessing in AWS Sagemaker inference,"<p>I am using AWS Sagemaker to deploy my speech models trained outside of Sagemaker. I am able to convert my model into something Sagemaker would understand and have deployed it as an endpoint. Problem is that Sagemaker directly loads the model and calls .predict to get the inference. I am unable to figure out where can I add my preprocessing functions in the deployed model. It is suggested to use AWS Lambda or another server for preprocessing. Is there any way I can incorporate complex preprocessing (cannot be done by simple Scikit, Pandas like framework) in Sagemaker itself?</p>",1,0,2021-04-08 21:29:12.407000 UTC,,,1,amazon-web-services|amazon-sagemaker,132,2012-11-13 03:48:58.290000 UTC,2021-06-12 01:02:20.257000 UTC,,11,0,0,3,,,,,,['amazon-sagemaker']
Getting this ERROR Main: - Unable to infer schema for JSON. It must be specified manually,"<p>I'm trying to merge my ground truth values with data captured(for Model Quality Monitoring) using <em>sagemaker-model-monitor-groundtruth-merger</em> container image.</p>
<pre><code>def run_merge_job_processor(
        region,
        instance_type,
        role,
        bucket_name,
        groundtruth_path,
        endpoint_input,
        merge_path,
        instance_count=1,
        ):
     
    
    groundtruth_input_1 = ProcessingInput(input_name=&quot;groundtruth_input_1&quot;,
                              source=&quot;s3://{}/{}&quot;.format(bucket_name, groundtruth_path),
                              destination=f&quot;/opt/ml/processing/groundtruth{datetime.utcnow():%Y/%m/%d/%H}&quot;,
                              s3_data_type=&quot;S3Prefix&quot;,
                              s3_input_mode=&quot;File&quot;)
    
    &quot;/&quot;.join(endpoint_input.split(&quot;/&quot;)[3:])
    
    endpoint_input_1 = ProcessingInput(
                          input_name=&quot;endpoint_input_1&quot;,
                          source=&quot;s3://{}/{}&quot;.format(bucket_name, endpoint_input),
                          destination=&quot;/opt/ml/processing/input_data/{}&quot;.format(&quot;/&quot;.join(endpoint_input.split(&quot;/&quot;)[3:])),
                          s3_data_type=&quot;S3Prefix&quot;,
                          s3_input_mode=&quot;File&quot;)
     
    output = ProcessingOutput(
                          output_name=&quot;result&quot;,
                          source=&quot;/opt/ml/processing/output&quot;,
                          destination=f&quot;s3://{bucket_name}/{merge_path}&quot;)
    
    
    inputs = [ groundtruth_input_1, endpoint_input_1 ]
    outputs = [ output ]
    
    env = {
        &quot;dataset_format&quot;: &quot;{\&quot;sagemakerCaptureJson\&quot;: {\&quot;captureIndexNames\&quot;: [\&quot;endpointInput\&quot;,\&quot;endpointOutput\&quot;]}}&quot;,
        'dataset_source': '/opt/ml/processing/input_data',
        'ground_truth_source': '/opt/ml/processing/groundtruth',
        'output_path': '/opt/ml/processing/output'
    }
    
    processor = Processor(image_uri=get_model_monitor_container_uri(region),
                          instance_count=instance_count,
                          instance_type=instance_type,
                          role=role,
                          env=env,
                          )
    
    return processor.run(
        inputs=inputs,
        outputs=outputs,
        wait=True
</code></pre>
<p>The above code that I've used for creating the processing job
where my processing job is getting failed with the following error.</p>
<p><a href=""https://i.stack.imgur.com/5oxhi.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5oxhi.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/kNcrN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kNcrN.png"" alt=""enter image description here"" /></a></p>",0,1,2022-09-19 10:35:43.043000 UTC,,,0,python|python-3.x|amazon-web-services|amazon-sagemaker,26,2022-08-10 08:15:11.500000 UTC,2022-09-24 14:19:19.210000 UTC,,15,4,0,1,,,,,,['amazon-sagemaker']
K-fold cross validation in azure ML,"<p>I am currently training a model using an azure ML pipeline that i build with sdk. I am trying to add cross-validation to my ml step. I have noticed that you can add this in the parameters when you configure the autoML. My dataset consists of 30% label 0 and 70% label 1.</p>
<p>My question is, does azure autoML stratify data when performing the cross-validation? If not i would have to do the split/stratify myself before passing it to autoML.</p>",1,0,2022-09-15 14:00:19.017000 UTC,,,0,azure|azure-machine-learning-studio|automl|azure-auto-ml,22,2019-06-08 22:07:48.653000 UTC,2022-09-22 13:39:26.520000 UTC,,51,3,0,29,,,,,,['azure-machine-learning-studio']
Unable to create pyspark DataFrame from Datastore in azureml-sdk (version 1.12.0),"<p>I am trying to read contents from a CSV file into Spark DataFrame using azureml-sdk using following code but an exception is being thrown.</p>
<p><strong>Code throwing exception</strong></p>
<pre><code>import pyspark.sql as spark
from azureml.core import Dataset
dataset = Dataset.Tabular.from_delimited_files(path = [(datastore, file_path)], header = False)
sdf: spark.DataFrame = dataset.to_spark_dataframe()
sdf.show()
</code></pre>
<p><strong>Exception</strong></p>
<pre><code>---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azureml/data/dataset_error_handling.py in _try_execute(action, operation, dataset_info, **kwargs)
    100         else:
--&gt; 101             return action()
    102     except Exception as e:

/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azureml/dataprep/api/_loggerfactory.py in wrapper(*args, **kwargs)
    178                 try:
--&gt; 179                     return func(*args, **kwargs)
    180                 except Exception as e:

/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azureml/dataprep/api/dataflow.py in to_spark_dataframe(self)
    763         self._raise_if_missing_secrets()
--&gt; 764         return self._spark_executor.get_dataframe(steps_to_block_datas(self._steps), use_sampling=False)
    765 

/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azureml/dataprep/api/sparkexecution.py in get_dataframe(self, steps, use_sampling, overrides, use_first_record_schema)
    136                              overrides,
--&gt; 137                              use_first_record_schema)
    138 

/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azureml/dataprep/api/sparkexecution.py in _execute(self, blocks, export_format, use_sampling, overrides, use_first_record_schema)
    169                                           + lariat_version + '.')
--&gt; 170             raise e
    171 

/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azureml/dataprep/api/sparkexecution.py in _execute(self, blocks, export_format, use_sampling, overrides, use_first_record_schema)
    160             if export_format == ExportScriptFormat.PYSPARKDATAFRAMELOADER:
--&gt; 161                 return module.LoadData(secrets=secrets, schemaFromFirstRecord=use_first_record_schema)
    162             else:

/tmp/spark-6ce53791-c8e4-4db0-bd37-bedb53a1ef1e/userFiles-dda6cd30-5d1e-48cf-af87-9c7c2a4b8038/loaderb9bc01c2b40c4b7aa86a95d343021e0c.py in LoadData(secrets, schemaFromFirstRecord)
      8 def LoadData(secrets=dict(), schemaFromFirstRecord=False):
----&gt; 9     pex = Executor(&quot;S4ddf53ee8d5f4173bd3dcf4b51d78247&quot;, &quot;dprep_2.11&quot;, &quot;0.116.0&quot;, &quot;42315&quot;, &quot;39a925e4-9ae9-4588-93c4-5433250b7f73&quot;)
     10     jex = pex.jex

/tmp/spark-6ce53791-c8e4-4db0-bd37-bedb53a1ef1e/userFiles-dda6cd30-5d1e-48cf-af87-9c7c2a4b8038/Executor.py in __init__(self, scalaName, dprepMavenPackageName, dprepMavenPackageMatchingVersion, pythonHostChannelPort, pythonHostSecret)
     54             pythonHostChannelPort,
---&gt; 55             pythonHostSecret)
     56 

/anaconda/envs/azureml_py36/lib/python3.6/site-packages/py4j/java_gateway.py in __call__(self, *args)
   1568         return_value = get_return_value(
-&gt; 1569             answer, self._gateway_client, None, self._fqn)
   1570 

/anaconda/envs/azureml_py36/lib/python3.6/site-packages/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    327                     &quot;An error occurred while calling {0}{1}{2}.\n&quot;.
--&gt; 328                     format(target_id, &quot;.&quot;, name), value)
    329             else:

Py4JJavaError: An error occurred while calling None.com.microsoft.dprep.execution.PySparkExecutor.
: java.lang.NoClassDefFoundError: Could not initialize class com.microsoft.dprep.integration.azureml.AmlPySdkInvoker$
    at com.microsoft.dprep.execution.PySparkExecutor.&lt;init&gt;(PySparkExecutor.scala:79)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
    at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
    at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
    at py4j.Gateway.invoke(Gateway.java:238)
    at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
    at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
    at py4j.GatewayConnection.run(GatewayConnection.java:238)
    at java.lang.Thread.run(Thread.java:748)


During handling of the above exception, another exception occurred:

AzureMLException                          Traceback (most recent call last)
&lt;ipython-input-30-c546b1aded42&gt; in &lt;module&gt;
      2 from azureml.core import Dataset
      3 dataset = Dataset.Tabular.from_delimited_files(path = [(datastore, file_path)], header = False)
----&gt; 4 sdf: spark.DataFrame = dataset.to_spark_dataframe()
      5 sdf.show()
      6 

/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azureml/data/_loggerfactory.py in wrapper(*args, **kwargs)
    124             with _LoggerFactory.track_activity(logger, func.__name__, activity_type, custom_dimensions) as al:
    125                 try:
--&gt; 126                     return func(*args, **kwargs)
    127                 except Exception as e:
    128                     if hasattr(al, 'activity_info') and hasattr(e, 'error_code'):

/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azureml/data/tabular_dataset.py in to_spark_dataframe(self)
    187         return _try_execute(dataflow.to_spark_dataframe,
    188                             'to_spark_dataframe',
--&gt; 189                             None if self.id is None else {'id': self.id, 'name': self.name, 'version': self.version})
    190 
    191     @track(_get_logger, custom_dimensions={'app_name': 'TabularDataset'}, activity_type=_PUBLIC_API)

/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azureml/data/dataset_error_handling.py in _try_execute(action, operation, dataset_info, **kwargs)
    102     except Exception as e:
    103         message, is_dprep_exception = _construct_message_and_check_exception_type(e, dataset_info, operation)
--&gt; 104         _dataprep_error_handler(e, message, is_dprep_exception)
    105 
    106 

/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azureml/data/dataset_error_handling.py in _dataprep_error_handler(e, message, is_dprep_exception)
    143         raise AzureMLException(message, inner_exception=e)
    144     else:
--&gt; 145         raise AzureMLException(message, inner_exception=e)
    146 
    147 

AzureMLException: AzureMLException:
    Message: Execution failed unexpectedly due to: Py4JJavaError
    InnerException An error occurred while calling None.com.microsoft.dprep.execution.PySparkExecutor.
: java.lang.NoClassDefFoundError: Could not initialize class com.microsoft.dprep.integration.azureml.AmlPySdkInvoker$
    at com.microsoft.dprep.execution.PySparkExecutor.&lt;init&gt;(PySparkExecutor.scala:79)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
    at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
    at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
    at py4j.Gateway.invoke(Gateway.java:238)
    at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
    at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
    at py4j.GatewayConnection.run(GatewayConnection.java:238)
    at java.lang.Thread.run(Thread.java:748)

    ErrorResponse 
{
    &quot;error&quot;: {
        &quot;message&quot;: &quot;Execution failed unexpectedly due to: Py4JJavaError&quot;
    }
}
</code></pre>
<p>However, I can read and print the data with the following code i.e. create as a <code>Panda</code>'s <code>DataFrame</code>.</p>
<p><strong>Working code</strong></p>
<pre><code>dataset = Dataset.Tabular.from_delimited_files(path = [(datastore, file_path)], header = False)
#sdf: spark.DataFrame = dataset.to_spark_dataframe()
sdf: pd.DataFrame = dataset.to_pandas_dataframe()
print(sdf.head(3))
</code></pre>",1,1,2020-09-11 01:38:28.583000 UTC,,,1,azure-machine-learning-service|azureml-python-sdk,420,2009-06-23 03:11:55.290000 UTC,2022-09-25 03:45:28.337000 UTC,"Cumming, GA",77230,2724,43,6359,,,,,,['azure-machine-learning-service']
How to pre deploy the instances used by sagemaker for training?,"<p>Sagemaker takes ~3 mins for <code>preparing the instances for training</code> and around ~1 min for the actual training and artifact publishing.</p>
<p>I want to pre-deploy some ec2 instances which it can use for training. However I couldn't find an option to specify pre deployed instances in the <code>ResourceConfig</code> part of the <code>createTrainingJob</code>.</p>
<p>Is this possible?</p>",1,0,2022-01-16 06:57:58.607000 UTC,,,0,amazon-web-services|amazon-sagemaker,30,2019-03-04 12:06:56.487000 UTC,2022-09-23 07:48:34.613000 UTC,,735,167,15,106,,,,,,['amazon-sagemaker']
Is there a (easy) way to connect AWS Redshift to Microsoft Azure ML?,"<p>I'm currently trying to evaluate Microsoft Azure ML for my company. I tried it a bit with offline data and it looks promising, but for better evaluation I want to use the online data we have. Our data is stored in Amazon Redshift service, and I couldn't find a way to permanently connect Redshift to Azure ML so it would take the new data, process it and store the results back in Redshift. Is there some way to use Redshift as a data source for Azure ML? Of course, the easier the better</p>",1,0,2016-06-26 09:53:40.627000 UTC,,2017-01-04 00:06:41.170000 UTC,2,azure|amazon-web-services|machine-learning|amazon-redshift|azure-machine-learning-studio,423,2014-03-27 18:40:07.250000 UTC,2016-07-20 09:20:45.930000 UTC,Israel,31,0,0,3,,,,,,['azure-machine-learning-studio']
InternalError when trying to predict with an endpoint on AWS,"<p>I use AWS SageMaker for a ML project and my TrainingJob for image classification has successfully finished. I use a separate inference.py for the inference, that looks like this:</p>
<pre><code>import json
import logging
import sys
import os
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.models as models
import torchvision.transforms as transforms
import torchvision.transforms as T
from PIL import Image
import io
import requests
logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)
logger.addHandler(logging.StreamHandler(sys.stdout))

# define model
def Net():
    model = models.__dict__['resnet50'](pretrained=True)
    for param in model.parameters():
        param.requires_grad = False
        
    num_features = model.fc.in_features
    
    model.fc = nn.Sequential(
    nn.Linear(num_features, int(num_features/2)),
    nn.Linear(int(num_features/2), 5))
    return model

# load model parameters
def model_fn(model_dir):
    device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
    model = Net().to(device)
    
    with open(os.path.join(model_dir, &quot;model.pth&quot;), &quot;rb&quot;) as f:
        model.load_state_dict(torch.load(f))
    model.eval()
    return model
    
# deserialize input
def input_fn(request_body, content_type):
    if content_type == 'image/jpeg':
        img = Image.open(io.BytesIO(request_body))
        return img
    else:
        raise ValueError(&quot;This model only supports jpeg input&quot;)

# inference
def predict_fn(input_object, model):
    transform = T.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    ])
    input_object=transform(input_object)
    input_object=input_object.unsqueeze(0)
    
    with torch.no_grad():
        prediction = model(input_object)
    return prediction
</code></pre>
<p>I created a predictor and made a prediction like this:</p>
<pre><code>from sagemaker.pytorch import PyTorchModel
from sagemaker.predictor import Predictor

model_location=estimator.model_data
pytorch_model = PyTorchModel(model_data=model_location, 
                             role=sagemaker.get_execution_role(), 
                             entry_point='inference.py',
                             py_version='py3',
                             framework_version='1.4')
predictor = pytorch_model.deploy(initial_instance_count=1, instance_type='ml.m5.large')
</code></pre>
<pre><code>import io
with open(&quot;test1.jpg&quot;, &quot;rb&quot;) as image:
  img = image.read()

response=predictor.predict(img, initial_args={&quot;ContentType&quot;: &quot;image/jpeg&quot;})
</code></pre>
<p>I got the following error message:</p>
<pre><code>InternalFailure                           Traceback (most recent call last)
/tmp/ipykernel_7063/3976847146.py in &lt;cell line: 1&gt;()
----&gt; 1 response=predictor.predict(img, initial_args={&quot;ContentType&quot;: &quot;image/jpeg&quot;})

~/anaconda3/envs/python3/lib/python3.8/site-packages/sagemaker/predictor.py in predict(self, data, initial_args, target_model, target_variant, inference_id)
    159             data, initial_args, target_model, target_variant, inference_id
    160         )
--&gt; 161         response = self.sagemaker_session.sagemaker_runtime_client.invoke_endpoint(**request_args)
    162         return self._handle_response(response)
    163 

~/anaconda3/envs/python3/lib/python3.8/site-packages/botocore/client.py in _api_call(self, *args, **kwargs)
    393                     &quot;%s() only accepts keyword arguments.&quot; % py_operation_name)
    394             # The &quot;self&quot; in this scope is referring to the BaseClient.
--&gt; 395             return self._make_api_call(operation_name, kwargs)
    396 
    397         _api_call.__name__ = str(py_operation_name)

~/anaconda3/envs/python3/lib/python3.8/site-packages/botocore/client.py in _make_api_call(self, operation_name, api_params)
    723             error_code = parsed_response.get(&quot;Error&quot;, {}).get(&quot;Code&quot;)
    724             error_class = self.exceptions.from_code(error_code)
--&gt; 725             raise error_class(parsed_response, operation_name)
    726         else:
    727             return parsed_response

InternalFailure: An error occurred (InternalFailure) when calling the InvokeEndpoint operation (reached max retries: 4): An exception occurred while sending request to model. Please contact customer support regarding request 34c901aa-76... .
</code></pre>
<p>I also tried this code in different accounts, but the issue remained.</p>",0,1,2022-08-01 21:37:48.170000 UTC,,2022-08-01 22:33:13.577000 UTC,0,amazon-web-services|machine-learning|pytorch|amazon-sagemaker,41,2017-07-30 16:07:25.497000 UTC,2022-09-19 06:50:04.107000 UTC,,41,2,0,8,,,,,,['amazon-sagemaker']
RPackage library exception (error 1000),"<p>I have the following code in an Execute R Module.</p>

<hr>

<pre><code># Input
data1 &lt;- maml.mapInputPort(1) # Qualitative with 8 variables

install.packages(""src/graphics.zip"", lib.loc = ""."", repos = NULL, verbose = 
TRUE)
install.packages(""src/grDevices.zip"", lib.loc = ""."", repos = NULL, verbose = 
TRUE)
install.packages(""src/stats.zip"", lib.loc = ""."", repos = NULL, verbose = 
TRUE)
install.packages(""src/utils.zip"", lib.loc = ""."", repos = NULL, verbose = 
TRUE)
install.packages(""src/MASS.zip"", lib.loc = ""."", repos = NULL, verbose = 
TRUE)
success &lt;- library(""MASS"", lib.loc = ""."", logical.return = TRUE, verbose = 
TRUE)
library(MASS)

mca &lt;- mca(data1, nf = 10)

mca1 &lt;- data.frame(mca$rs)

# Output
maml.mapOutputPort(""mca1"");
</code></pre>

<hr>

<p>When I execute I am getting the following error:</p>

<p>RPackage library exception: Attempting to obtain R output before invoking execution process. (Error 1000)</p>

<p>But it is working fine in RStudio.</p>

<p>I also have a node that does the same process and it works without errors. I have executed it several times, sometimes it has worked for me and then it has returned error.</p>

<p>Please let me know what the issue is.</p>

<p>With regards,</p>

<p>Celia</p>",0,0,2017-12-20 15:03:35.720000 UTC,,,2,r|azure-machine-learning-studio,255,2017-12-20 14:56:57.750000 UTC,2017-12-20 14:56:57.750000 UTC,,21,0,0,3,,,,,,['azure-machine-learning-studio']
Using SMOTE on unbalanced dataset,"<p>I have a 2 class unbalanced dataset where the ratio is 20:1 </p>

<p>I am using SMOTE to oversample the minor class and wanted to know when using SMOTE to develop a usable model, if it was best to oversample so that the percentage of the minor class was the same as the other class (i.e 1:1) or establish through trial an error the lowest possible ratio to improve the model overall to an acceptable level (i.e F1Score >0.7) but not use too many synthetic samples if that makes sense.</p>

<p>Any thoughts/advice appreciated.</p>",2,2,2016-06-03 17:11:43.590000 UTC,,,-1,machine-learning|azure-machine-learning-studio,587,2015-04-12 06:31:14.637000 UTC,2022-09-23 17:09:47.080000 UTC,"Lexington, KY, United States",65,0,0,21,,,,,,['azure-machine-learning-studio']
How does im2rec works? I keep getting syntax error,"<p>I am trying to create lst files for aws image classification algorithm.
My main directory is train which has 20 sub-directories of 40 images each.
I want to create a train_1st which contains all the converted lst files.
But I am getting issues with the below code. I'm new to this .. So please help me.. what do i do?</p>

<p>I have tried changing the current working directory(cwd) as well. I tried setting cwd as train/ and also actual directory home/ec-2/sagemaker. Nothing helped.</p>

<pre><code>%%bash

mkdir -p  train_lst
for i in  train/*; do
    c=`basename $i`
    mkdir -p train_lst/$c
    for j in `ls $i/*.jpg | shuf | head -n 60`; do
        mv $j train_lst/$c/
    done
done

python im2rec.py --list --recursive train train_lst/
</code></pre>

<pre><code>ls: cannot access train/*/*.jpg: No such file or directory
</code></pre>",1,2,2019-07-16 00:52:12.763000 UTC,,2019-07-16 02:18:49.127000 UTC,0,python|bash|amazon-web-services|amazon-ec2|amazon-sagemaker,167,2019-01-03 15:38:43.733000 UTC,2021-01-08 11:44:28.740000 UTC,Ireland,19,0,0,21,,,,,,['amazon-sagemaker']
How to create a docker image for Sagemaker that is not part of the amazon estimator to create an endpoint?,"<p>I have built a custom model in Sagemaker and serialized the model through pickle. I want to deploy my model through Sagemaker hosting services and read through this </p>

<p><a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-hosting.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-hosting.html</a></p>

<p>But I am lost on how to build my own Docker container for a custom model with an algorithm that is currently not implemented as part of the Amazon Estimator.</p>

<p>How do I build my own docker image to load into ECR to then build the container that allows me to create an endpoint?</p>",1,0,2019-03-27 21:48:05.483000 UTC,1.0,,3,docker|machine-learning|endpoint|amazon-sagemaker,1616,2014-08-29 12:08:38.610000 UTC,2019-06-19 19:04:53.567000 UTC,"New York, NY, United States",161,2,0,15,,,,,,['amazon-sagemaker']
Mlflow not running on machine,"<p>Please I am trying to run mlflow code in R after having installed it. However, after loading the library with <code>library(mlflow)</code> and I run <code>mlflow_log_params(&quot;foo&quot;,42)</code> I get the error message below printed in my console:</p>
<pre><code>Error in rethrow_call(c_processx_exec, command, c(command, args), pty,  : 
  Command 'C:/Users/IFEANYI/AppData/Local/r-miniconda/envs/r-mlflow-1.19.0/mlflow' not found @win/processx.c:982 (processx_exec)
</code></pre>
<p>I also get the same error message when I run <code>mlflow_ui()</code>. Please was there something I ought to have done during installation failure of which is affecting its functionality? Do I need to install and load the processx library in order for mlflow to run on my Windows10 machine? I really hope I can get advice to help me because I want to use mlflow in my machine learning projects. Thanks in advance of your generous help.</p>",1,1,2021-09-13 18:48:47.983000 UTC,1.0,2021-09-14 17:21:08.230000 UTC,1,r|machine-learning|mlflow,137,2016-02-25 21:42:41.250000 UTC,2022-09-22 21:29:37.373000 UTC,,109,2,0,22,,,,,,['mlflow']
Is it possible to connect to a Postgres DB from SageMaker Data Wrangler?,"<p>I set up a regular Postgres DB in AWS using the <a href=""https://aws.amazon.com/rds/"" rel=""nofollow noreferrer"">Amazon Relational Database Service (RDS)</a>. I would like to ingest this data using <strong><a href=""https://aws.amazon.com/sagemaker/data-wrangler/"" rel=""nofollow noreferrer"">data wrangler</a></strong> for inspection and further processing.</p>
<p>Is this possible? I only see S3, Athena, Redshift and SnowFlake as the data ingestion options. Does this mean I must move the data from Postgresql to one of these 4 options before I can use Data Wrangler?</p>
<p>If it's not possible through data wrangler, can I connect to my Postgres through a Jupyter notebook, using a connection string or some kind of option like this? I'm looking to use the data for the SageMaker Feature Store.</p>",1,0,2021-07-09 20:06:49.793000 UTC,,2021-07-10 14:05:09.253000 UTC,0,postgresql|amazon-web-services|amazon-sagemaker|data-wrangling|feature-store,890,2012-08-31 20:08:40.090000 UTC,2022-09-25 04:17:41.297000 UTC,,11650,6318,21,977,,,,,,['amazon-sagemaker']
Cannot read special character in sagemaker,"<p>I'm using the R Kernel in Sagemaker. I'm using this code:</p>
<pre><code>prueba &lt;-s3_read('s3://prueba/ejemplotildes.csv',  read.csv2, encoding='Latin-1')
head(prueba)
</code></pre>
<p>And the results bring the data without special latin chars like: á, é, í, ó, so a word that must to be like &quot;opción&quot; comes &quot;opcin&quot;.</p>
<p>Do you know how can I bring my data exactly like the original?</p>",0,2,2020-07-28 16:03:25.877000 UTC,,2020-07-28 17:13:34.203000 UTC,1,r|amazon-s3|amazon-sagemaker,48,2020-07-28 15:52:03.930000 UTC,2021-04-09 15:39:54.300000 UTC,,11,0,0,1,,,,,,['amazon-sagemaker']
AWS SageMaker MXNet USE_CUDA=1,"<p>I am using the AWS ml.p2.xlarge sagemaker instance and conda_amazonei_mxnet_p36 kernel after install MXnet CUDA</p>
<pre><code>!pip install mxnet-cu101
</code></pre>
<p>when I try to run the following code</p>
<pre><code>mx_tfidf = mx.nd.sparse.array(tfidf_matrix, ctx=mx.gpu())
</code></pre>
<p>I am getting the following error</p>
<pre><code>MXNetError: [19:54:53] src/storage/storage.cc:119: 
Compile with USE_CUDA=1 to enable GPU usage
</code></pre>
<p>Please help me to resolve the issue</p>
<pre><code>nvidia-smi
</code></pre>
<p><a href=""https://i.stack.imgur.com/8MX91.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8MX91.png"" alt=""nvidia configuration as follows"" /></a></p>",1,0,2019-10-09 20:16:16.880000 UTC,,2020-06-20 09:12:55.060000 UTC,0,machine-learning|nvidia|amazon-sagemaker|mxnet,800,2015-07-14 22:09:21.000000 UTC,2020-05-29 03:50:02.223000 UTC,USA,194,7,0,38,,,,,,['amazon-sagemaker']
Using CLI to start using SageMaker in browser,"<p>In the case of using AWS only via aws cli, how can I open a sagemaker app in the browser if I already create sagemaker app and have apparn?</p>",1,0,2022-07-20 09:42:53.033000 UTC,,,0,amazon-sagemaker,64,2019-04-05 10:07:09.380000 UTC,2022-09-21 09:34:30.560000 UTC,,1,0,0,0,,,,,,['amazon-sagemaker']
Add Private Pip Wheel File to AMLS Workspace Permissions,"<p>I am working with a customer and they are having trouble getting things working using a wheel file in the Machine Learning workspace.</p>

<p>The DS is running this command </p>

<pre><code>whl_url = Environment.add_private_pip_wheel(workspace=ws, file_path='xyz-1.1.1-qwe-none-any.whl', exist_ok=True)
</code></pre>

<p>And getting this error</p>

<pre><code>AzureHttpError: This request is not authorized to perform this operation. ErrorCode: AuthorizationFailure
&lt;?xml version=""1.0"" encoding=""utf-8""?&gt;&lt;Error&gt;&lt;Code&gt;AuthorizationFailure&lt;/Code&gt;&lt;Message&gt;This request is not authorized to perform this operation.
RequestId:a166d089-901e-0126-42c1-ec6962000000
Time:2020-02-26T16:25:59.3801883Z&lt;/Message&gt;&lt;/Error&gt;
</code></pre>

<p>But the DS performing this task is listed as a <code>contributor</code> in the RBAC permissions <code>on the MLWS</code> (and to the <code>storage account under the MLWS</code> as well).</p>

<p>I don’t understand what permissions are missing – this seems setup correctly, but doesn’t appear to work for some reason. Could you please help?</p>",0,3,2020-02-26 20:29:46.377000 UTC,,,0,azure-machine-learning-studio|azure-machine-learning-service|azure-machine-learning-workbench,154,2012-02-02 17:20:06.633000 UTC,2020-10-09 13:48:41.317000 UTC,,9,0,0,33,,,,,,"['azure-machine-learning-workbench', 'azure-machine-learning-studio', 'azure-machine-learning-service']"
"Using custom trained Keras model with Sagemaker endpoint results in ""Session was not created with a graph before Run()"" error while prediction","<p>I have a trained a BERT text classification model using keras on spam vs ham dataset. I have deployed the model and got a Sagemaker endpoint. I want to use it for any prediction.</p>
<p>I am using a <code>ml.t2.medium</code> Sagemaker instance and my tensorflow version is <code>2.6.2</code> in the Sagemaker notebook</p>
<p>I am getting an error while using the Sagemaker endpoint for prediction. The error is <code>Session was not created with a graph before Run()</code></p>
<p>This is my code for training the classifier</p>
<pre><code>import tensorflow as tf
import tensorflow_hub as hub
import tensorflow_text as text


# In[2]:


import pandas as pd

df = pd.read_csv(&quot;spam.csv&quot;)
df.head(5)


# In[3]:


df.groupby('Category').describe()


# In[4]:


df['Category'].value_counts()


# In[5]:


df_spam = df[df['Category']=='spam']
df_spam.shape


# In[6]:


df_ham = df[df['Category']=='ham']
df_ham.shape


# In[7]:


df_ham_downsampled = df_ham.sample(df_spam.shape[0])
df_ham_downsampled.shape


# In[8]:


df_balanced = pd.concat([df_ham_downsampled, df_spam])
df_balanced.shape


# In[9]:


df_balanced['Category'].value_counts()


# In[10]:


df_balanced['spam']=df_balanced['Category'].apply(lambda x: 1 if x=='spam' else 0)
df_balanced.sample(5)


# In[11]:


from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(df_balanced['Message'],df_balanced['spam'], stratify=df_balanced['spam'])


# In[12]:


bert_preprocess = hub.KerasLayer(&quot;https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3&quot;)
bert_encoder = hub.KerasLayer(&quot;https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4&quot;)


# In[13]:


def get_sentence_embeding(sentences):
    preprocessed_text = bert_preprocess(sentences)
    return bert_encoder(preprocessed_text)['pooled_output']

get_sentence_embeding([
    &quot;500$ discount. hurry up&quot;, 
    &quot;Bhavin, are you up for a volleybal game tomorrow?&quot;]
)


# In[14]:


# Bert layers
text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')
preprocessed_text = bert_preprocess(text_input)
outputs = bert_encoder(preprocessed_text)

# Neural network layers
l = tf.keras.layers.Dropout(0.1, name=&quot;dropout&quot;)(outputs['pooled_output'])
l = tf.keras.layers.Dense(1, activation='sigmoid', name=&quot;output&quot;)(l)

# Use inputs and outputs to construct a final model
model = tf.keras.Model(inputs=[text_input], outputs = [l])


# In[15]:


model.summary()


# In[16]:


METRICS = [
      tf.keras.metrics.BinaryAccuracy(name='accuracy'),
      tf.keras.metrics.Precision(name='precision'),
      tf.keras.metrics.Recall(name='recall')
]

model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=METRICS)


# In[17]:


model.fit(X_train, y_train, epochs=1)
</code></pre>
<p>AND THIS PART IS USED FOR DEPLOYING THE MODEL</p>
<pre><code># In[18]:


model.save('saved_model/28dec1') 


# In[3]:


model = tf.keras.models.load_model('saved_model/28dec1')

model.predict([&quot;who is the spammer on here&quot;])

array([[0.08218178]], dtype=float32)

# Check its architecture
model.summary()


# In[18]:


tf.compat.v1.enable_eager_execution()
print(&quot;pass&quot;)


# In[5]:


def convert_h5_to_aws(loaded_model):
    &quot;&quot;&quot;
    given a pre-trained keras model, this function converts it to a TF protobuf format
    and saves it in the file structure which aws expects
    &quot;&quot;&quot;  
    from tensorflow.python.saved_model import builder
    from tensorflow.python.saved_model.signature_def_utils import predict_signature_def
    from tensorflow.python.saved_model import tag_constants
    
    # This is the file structure which AWS expects. Cannot be changed. 
    model_version = '1'
    export_dir = 'export/Servo/' + model_version
    
    # Build the Protocol Buffer SavedModel at 'export_dir'
    builder = builder.SavedModelBuilder(export_dir)
    
    # Create prediction signature to be used by TensorFlow Serving Predict API
    signature = predict_signature_def(
        inputs={&quot;inputs&quot;: loaded_model.input}, outputs={&quot;score&quot;: loaded_model.output})
    
    from keras import backend as K
    with K.get_session() as sess:
        # Save the meta graph and variables
        builder.add_meta_graph_and_variables(
            sess=sess, tags=[tag_constants.SERVING], signature_def_map={&quot;serving_default&quot;: signature})
        builder.save()
    
    #create a tarball/tar file and zip it
    import tarfile
    with tarfile.open('model.tar.gz', mode='w:gz') as archive:
        archive.add('export', recursive=True)
        
convert_h5_to_aws(model)


# In[3]:


import sagemaker

sagemaker_session = sagemaker.Session()
inputs = sagemaker_session.upload_data(path='model.tar.gz', key_prefix='model')


# In[7]:


# where did it upload to?
print(&quot;Bucket name is:&quot;)
sagemaker_session.default_bucket()


# In[9]:


import boto3, re
from sagemaker import get_execution_role

# the (default) IAM role you created when creating this notebook
role = get_execution_role()

# Create a Sagemaker model (see AWS console&gt;SageMaker&gt;Models)
from sagemaker.tensorflow.model import TensorFlowModel
sagemaker_model = TensorFlowModel(model_data = 's3://' + sagemaker_session.default_bucket() + '/model/model.tar.gz',
                                  role = role,
                                  framework_version = '1.12',
                                  entry_point = 'train.py')


# In[10]:


# Deploy a SageMaker to an endpoint
predictor = sagemaker_model.deploy(initial_instance_count=1,
                                   instance_type='ml.m4.xlarge')


# In[5]:


import numpy as np
   
import sagemaker
from sagemaker.tensorflow.model import TensorFlowModel

endpoint = 'sagemaker-tensorflow-serving-2021-10-28-11-18-34-001' #get endpoint name from SageMaker &gt; endpoints

predictor=sagemaker.tensorflow.model.TensorFlowPredictor(endpoint, sagemaker_session)
# .predict send the data to our endpoint
#data = np.asarray([&quot;what the shit&quot;]) #&lt;-- update this to have inputs for your model
predictor.predict([&quot;this is not a spam&quot;])
 
</code></pre>
<p>And I am getting this error</p>
<pre><code>ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from primary with message &quot;{ &quot;error&quot;: &quot;Session was not created with a graph before Run()!&quot; }
</code></pre>
<p>Can someone please help me.</p>",1,0,2021-12-28 12:16:23.503000 UTC,,,2,python|amazon-web-services|tensorflow|keras|amazon-sagemaker,263,2021-02-10 16:26:42.297000 UTC,2022-09-20 07:45:23.350000 UTC,,65,6,0,7,,,,,,['amazon-sagemaker']
"How to get absolute path to ""outputs"" folder in Azure ML","<p>In the <a href=""https://docs.microsoft.com/en-us/azure/machine-learning/how-to-save-write-experiment-files"" rel=""nofollow noreferrer"">documentation</a> of Azure Machine Learning, under &quot;Where to write files&quot;, it says</p>
<blockquote>
<p>Otherwise, write files to the <code>./outputs</code> and/or <code>./logs</code> folder.</p>
</blockquote>
<p>These are relative paths, i.e. relative to the folder where my script is run by the Azure ML framework. I was not able to find a function in the Azure ML SDK that would return the absolute path -- have I missed it or is there none? (Meaning that I should read the <code>cwd</code> at the beginning of my script and store it myself.)</p>",1,0,2022-07-30 19:11:39.903000 UTC,,,0,azure-machine-learning-studio|azure-machine-learning-service,111,2009-07-23 17:51:09.417000 UTC,2022-09-24 22:26:28.787000 UTC,,7506,752,11,360,,,,,,"['azure-machine-learning-studio', 'azure-machine-learning-service']"
aws sagemaker cannot import TensorFlowModel,"<p><code>from sagemaker.tensorflow import TensorFlowModel</code></p>
<p>throws</p>
<p><code>ImportError: cannot import name 'is_pipeline_variable' from 'sagemaker.workflow'</code></p>
<p>full error stack is:</p>
<pre><code>---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
&lt;ipython-input-54-e8eac6a9c02f&gt; in &lt;module&gt;
----&gt; 1 from sagemaker.tensorflow import TensorFlowModel

/opt/conda/lib/python3.7/site-packages/sagemaker/tensorflow/__init__.py in &lt;module&gt;
     14 from __future__ import absolute_import
     15 
---&gt; 16 from sagemaker.tensorflow.estimator import TensorFlow  # noqa: F401 (imported but unused)
     17 from sagemaker.tensorflow.model import TensorFlowModel, TensorFlowPredictor  # noqa: F401
     18 from sagemaker.tensorflow.processing import TensorFlowProcessor  # noqa: F401

/opt/conda/lib/python3.7/site-packages/sagemaker/tensorflow/estimator.py in &lt;module&gt;
     23 import sagemaker.fw_utils as fw
     24 from sagemaker.tensorflow import defaults
---&gt; 25 from sagemaker.tensorflow.model import TensorFlowModel
     26 from sagemaker.transformer import Transformer
     27 from sagemaker.vpc_utils import VPC_CONFIG_DEFAULT

/opt/conda/lib/python3.7/site-packages/sagemaker/tensorflow/model.py in &lt;module&gt;
     22 from sagemaker.predictor import Predictor
     23 from sagemaker.serializers import JSONSerializer
---&gt; 24 from sagemaker.workflow import is_pipeline_variable
     25 
     26 

ImportError: cannot import name 'is_pipeline_variable' from 'sagemaker.workflow' (/opt/conda/lib/python3.7/site-packages/sagemaker/workflow/__init__.py)
</code></pre>
<p>following amazon's own documentation here:
<a href=""https://aws.amazon.com/blogs/machine-learning/deploy-trained-keras-or-tensorflow-models-using-amazon-sagemaker/"" rel=""nofollow noreferrer"">https://aws.amazon.com/blogs/machine-learning/deploy-trained-keras-or-tensorflow-models-using-amazon-sagemaker/</a></p>
<p>trying to accommodate the v2 api change documented here:
<a href=""https://sagemaker.readthedocs.io/en/stable/v2.html"" rel=""nofollow noreferrer"">https://sagemaker.readthedocs.io/en/stable/v2.html</a></p>
<pre><code>TensorFlow Serving Model

sagemaker.tensorflow.serving.Model has been renamed to sagemaker.tensorflow.model.TensorFlowModel. (For the previous implementation of that class, see Remove Legacy TensorFlow).
</code></pre>",0,1,2022-04-13 20:05:53.960000 UTC,,,0,amazon-web-services|tensorflow|amazon-sagemaker,301,2014-10-29 20:37:55.427000 UTC,2022-04-20 20:17:24.690000 UTC,"Philadelphia, PA, USA",754,209,3,68,,,,,,['amazon-sagemaker']
Using the Environment Class with Pipeline Runs,"<p>I am using an estimator step for a pipeline using the Environment class, in order to have a custom Docker image as I need some <code>apt-get</code> packages to be able to install a specific pip package. It appears from the logs that it's completely ignoring, unlike the non-pipeline version of the estimator, the docker portion of the environment variable. Very simply, this seems broken : </p>

<p>I'm running on SDK v1.0.65, and my dockerfile is completely ignored, I'm using </p>

<pre><code>FROM mcr.microsoft.com/azureml/base:latest\nRUN apt-get update &amp;&amp; apt-get -y install freetds-dev freetds-bin vim gcc
</code></pre>

<p>in the base_dockerfile property of my code. 
Here's a snippet of my code : </p>

<pre class=""lang-py prettyprint-override""><code>from azureml.core import Environment
from azureml.core.environment import CondaDependencies
conda_dep = CondaDependencies()
conda_dep.add_pip_package('pymssql==2.1.1')
myenv = Environment(name=""mssqlenv"")
myenv.python.conda_dependencies=conda_dep
myenv.docker.enabled = True
myenv.docker.base_dockerfile = 'FROM mcr.microsoft.com/azureml/base:latest\nRUN apt-get update &amp;&amp; apt-get -y install freetds-dev freetds-bin vim gcc'
myenv.docker.base_image = None
</code></pre>

<p>This works well when I use an Estimator by itself, but if I insert this estimator in a Pipeline, it fails. Here's my code to launch it from a Pipeline run: </p>

<pre class=""lang-py prettyprint-override""><code>from azureml.pipeline.steps import EstimatorStep

sql_est_step = EstimatorStep(name=""sql_step"", 
                         estimator=est, 
                         estimator_entry_script_arguments=[],
                         runconfig_pipeline_params=None, 
                         compute_target=cpu_cluster)
from azureml.pipeline.core import Pipeline
from azureml.core import Experiment
pipeline = Pipeline(workspace=ws, steps=[sql_est_step])
pipeline_run = exp.submit(pipeline)
</code></pre>

<p>When launching this, the logs for the container building service reveal:</p>

<pre><code>FROM continuumio/miniconda3:4.4.10... etc.
</code></pre>

<p>Which indicates it's ignoring my <code>FROM mcr....</code> statement in the Environment class I've associated with this Estimator, and my <code>pip install</code> fails.</p>

<p>Am I missing something? Is there a workaround?</p>",3,0,2019-10-08 19:53:08.707000 UTC,,,3,azure-machine-learning-service,751,2018-09-30 02:52:40.603000 UTC,2022-07-22 02:57:21.830000 UTC,"Montreal, QC, Canada",381,75,2,50,,,,,,['azure-machine-learning-service']
geting artifacts from mlflow GridSearch run,"<p>I'm running a sklearn pipeline with hyperparameter search (let's say GridSearch). Now, I am logging artifacts such as test results and whole-dataset predictions. I'd like to retrieve these artifacts but the mlflow API is getting in the way...</p>
<pre class=""lang-py prettyprint-override""><code>import mlflow

mlflow.set_tracking_uri(&quot;sqlite:///mlruns/mlruns.db&quot;)
mlflow.set_registry_uri(&quot;./mlruns/&quot;)

run_ids = [r.run_id for r in mlflow.list_run_infos(mlflow.get_experiment_by_name(&quot;My Experiment&quot;).experiment_id)]
</code></pre>
<p>With the above code, I can retrieve all runs but I have no way of telling which one is a toplevel run with artifacts logged or a sub-run spawned by the GridSearch procedure.</p>
<p>Is there some way of querying only for <strong>parent</strong> runs, so I can retrieve these csv files in order to plot the results? I can of course go to the web api and manually select the run then copy the URI for the file, but I'd like to do it programmatically instead of opening a tab and clicking things.</p>",0,0,2022-09-06 09:16:13.517000 UTC,,,0,mlflow,10,2016-07-07 10:29:22.327000 UTC,2022-09-22 17:04:45.120000 UTC,Spain,624,56,2,76,,,,,,['mlflow']
Python Azure SDK - Incorrect datetime format inferred when reading tabular data from blobstore with from_delimited_files(),"<p>I am using the Azure Python SDK to read a tabular dataset from a Blob Store as follows:</p>
<pre class=""lang-py prettyprint-override""><code>df = Dataset.Tabular.from_delimited_files(path=[DataPath(ds, blobstore_dir + 'tabular_data.csv')],
                                          separator=',', header=True)
</code></pre>
<p>The data has four datetime columns, one of the columns reads in with no problem because there are instances where the month-day order is not ambiguous, but the other three are being inferred incorrectly as &quot;month-day&quot; instead of &quot;day-month&quot;.</p>
<p>When reading in the data I get the following warning:</p>
<blockquote>
<p>UserWarning: Ambiguous datetime formats inferred for columns ['Period Start', 'Period End', 'Extracted At'] are resolved as &quot;month-day&quot;. Desired format can be specified by <code>set_column_types</code>.</p>
</blockquote>
<p>I have attempted to set the column types as below, and have tried a few different formats but all I end up with is NULL in place of all the values.</p>
<pre><code>df = Dataset.Tabular.from_delimited_files(
        path=[DataPath(ds, blobstore_dir + 'tabular_data.csv')], separator=',', header=True,
        set_column_types={'Period Start': DataType.to_datetime(&quot;%d-%m-%Y %H:%M:%S&quot;),
                          'Period End': DataType.to_datetime(&quot;%d-%m-%Y %H:%M:%S&quot;),
                          'Extracted At': DataType.to_datetime(&quot;%d-%m-%Y %H:%M:%S&quot;)})
</code></pre>
<p>The documentation for <code>from_delimited_files()</code> is <a href=""https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.dataset_factory.tabulardatasetfactory?view=azure-ml-py#from-delimited-files-path--validate-true--include-path-false--infer-column-types-true--set-column-types-none--separator------header-true--partition-format-none--support-multi-line-false--empty-as-string-false--encoding--utf8--"" rel=""nofollow noreferrer"">here</a></p>
<p>Can anyone tell me how to force <code>from_delimited_files()</code> to resolve the ambiguous datetimes as day-month or tell me how to use <code>set_column_types</code> correctly? I've worked around it temporarily by inserting a dummy row with a non-ambiguous datetime.</p>",1,0,2022-01-05 01:25:31.550000 UTC,,2022-01-05 01:48:06.407000 UTC,1,python|azure|azure-sdk-python|azure-machine-learning-service,131,2021-05-14 02:29:49.160000 UTC,2022-06-08 04:31:31.727000 UTC,,11,0,0,2,,,,,,['azure-machine-learning-service']
Azure ML Error: AADSTS70016: OAuth 2.0 device flow error. Authorization is pending. Continue polling,"<p>Hi I am trying to run an experiment using a compute instance through Azureml SDK using the code bellow:</p>
<pre><code>from azureml.core import Workspace, Experiment, ScriptRunConfig
from azureml.core.environment import Environment

ws=Workspace.from_config(path='My path')

cluster = ws.compute_targets['My compute target']

myenv = Environment.get(workspace=ws, name=&quot;myenv&quot;)

new_experiment= Experiment(workspace=ws,
                           name='Name')

script_config =ScriptRunConfig(source_directory='.',  
                               script='Script to Run.py',
                               compute_target=cluster,
                               environment=myenv
                                )


new_run=new_experiment.submit(config=script_config)
</code></pre>
<p>After over 15 minutes of waiting I get the error 'Error: AADSTS70016: OAuth 2.0 device flow error. Authorization is pending' , if I run the same code but with the computer target as 'local' it runs right,</p>
<p>I know that it is an autetication problem, but it is not clear to me how can I solve it. Thanks for any advice</p>",1,1,2022-04-06 04:34:20.953000 UTC,,,1,python|azure|azure-machine-learning-studio,465,2022-03-25 22:49:02.930000 UTC,2022-08-03 12:00:23.083000 UTC,,11,0,0,3,,,,,,['azure-machine-learning-studio']
ALB host based routing without domain name,"<p>I'm trying to configure host based routing in AWS ALB for ClearML server using <a href=""https://allegro.ai/clearml/docs/docs/deploying_clearml/clearml_server_config.html#configuration-procedures"" rel=""nofollow noreferrer"">this tutorial</a>.
However, I don't have a domain name. So can I only use alb's dns for this routing?</p>
<p>For example, I will have the address as app.<em><strong>.ap-north-east-1.elb.amazonaws.com, api.</strong></em>.ap-north-east-1.elb.amazonaws.com.</p>",1,0,2022-03-11 00:25:08.150000 UTC,,,0,aws-application-load-balancer|clearml,190,2018-10-01 12:52:13.233000 UTC,2022-08-13 09:22:52.563000 UTC,"Ōsaka-shi, 大阪府 日本",51,0,0,6,,,,,,['clearml']
Install python package to PySpark Kernel in Sagemaker Notebooks,<p>Has somebody figured out how to install packages on AWS Sagemaker Notebook  instances so they are available in the PySpark kernel? I made several attempts now including the lifecycle scripts but it seems I just miss the right python env. Package in question is <code>joblib</code> but I guess it shouldn't matter?! </p>,1,6,2019-12-22 22:04:40.337000 UTC,,2019-12-23 06:07:42.027000 UTC,3,amazon-web-services|aws-glue|amazon-sagemaker,1878,2011-10-22 10:19:12.757000 UTC,2022-08-31 14:28:11.970000 UTC,"Zürich, Switzerland",969,108,9,107,,,,,,['amazon-sagemaker']
"When fitting a model with a test set in Sagemaker, is there a way to receive the predictions of the test set?","<p>I'm training a model using Sagemaker, specifically the DeepAR image, and giving both train and test sets as inputs for the <code>fit</code> function. 
Example code:</p>

<pre><code>image_name = sagemaker.amazon.amazon_estimator.get_image_uri(region, ""forecasting-deepar"", ""latest"")
estimator = sagemaker.estimator.Estimator(
    sagemaker_session=sagemaker_session,
    image_name=image_name,
    role=role,
    train_instance_count=1,
    train_instance_type=train_instance_type,
    base_job_name=job_name,
    output_path=s3_output_path
)

data_channels = {
    ""train"": s3_train_path,
    ""test"": s3_test_path
}

estimator.fit(inputs=data_channels, wait=True, job_name=model_name)
</code></pre>

<p>I see some test result metrics at the end of the prints of the training, but I would like to get the actual predictions to analyse. Example of test result metrics:</p>

<pre><code>[12/25/2019 13:02:26 INFO 139821722212160] #test_score (algo-1, RMSE): 819.800852342
[12/25/2019 13:02:26 INFO 139821722212160] #test_score (algo-1, mean_wQuantileLoss): 0.33004057
[12/25/2019 13:02:26 INFO 139821722212160] #test_score (algo-1, wQuantileLoss[0.1]): 0.12110487
[12/25/2019 13:02:26 INFO 139821722212160] #test_score (algo-1, wQuantileLoss[0.2]): 0.20682412
[12/25/2019 13:02:26 INFO 139821722212160] #test_score (algo-1, wQuantileLoss[0.3]): 0.2760827
[12/25/2019 13:02:26 INFO 139821722212160] #test_score (algo-1, wQuantileLoss[0.4]): 0.3326178
[12/25/2019 13:02:26 INFO 139821722212160] #test_score (algo-1, wQuantileLoss[0.5]): 0.37820518
[12/25/2019 13:02:26 INFO 139821722212160] #test_score (algo-1, wQuantileLoss[0.6]): 0.41009128
[12/25/2019 13:02:26 INFO 139821722212160] #test_score (algo-1, wQuantileLoss[0.7]): 0.42785496
[12/25/2019 13:02:26 INFO 139821722212160] #test_score (algo-1, wQuantileLoss[0.8]): 0.42626995
[12/25/2019 13:02:26 INFO 139821722212160] #test_score (algo-1, wQuantileLoss[0.9]): 0.3913141
</code></pre>

<p>The best that I have found is not uploading a test set at all, and separately running a <code>batch_transform</code> job to get the test predictions back.
<a href=""https://sagemaker.readthedocs.io/en/stable/estimators.html#sagemaker.estimator.EstimatorBase"" rel=""nofollow noreferrer"">Docs</a> are vague saying regarding estimator <code>output_path</code>:</p>

<blockquote>
  <p>S3 location for saving the training result (model artifacts and output files)</p>
</blockquote>

<p>Not sure what that includes.
Is there a way to get the predictions of the test set?
Thanks in advance!</p>",1,0,2019-12-25 13:50:21.033000 UTC,,,0,python|amazon-s3|amazon-sagemaker,487,2018-06-27 11:42:13.503000 UTC,2021-12-08 15:47:31.857000 UTC,,85,65,0,95,,,,,,['amazon-sagemaker']
Sagemaker workforce with cognito,"<p>i am trying to build the terraform for sagemaker private work force with private cognito</p>
<p><strong>Following</strong> : <a href=""https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/sagemaker_workforce"" rel=""nofollow noreferrer"">https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/sagemaker_workforce</a></p>
<p>it working fine</p>
<p><strong>main.tf</strong></p>
<pre><code>resource &quot;aws_sagemaker_workforce&quot; &quot;workforce&quot; {
  workforce_name = &quot;workforce&quot;

  cognito_config {
    client_id = aws_cognito_user_pool_client.congnito_client.id
    user_pool = aws_cognito_user_pool_domain.domain.user_pool_id
  }
}

resource &quot;aws_cognito_user_pool&quot; &quot;user_pool&quot; {
  name = &quot;sagemaker-cognito-userpool&quot;
}

resource &quot;aws_cognito_user_pool_client&quot; &quot;congnito_client&quot; {
  name            = &quot;congnito-client&quot;
  generate_secret = true
  user_pool_id    = aws_cognito_user_pool.user_pool.id
}

resource &quot;aws_cognito_user_group&quot; &quot;user_group&quot; {
  name         = &quot;user-group&quot;
  user_pool_id = aws_cognito_user_pool.user_pool.id
}

resource &quot;aws_cognito_user_pool_domain&quot; &quot;domain&quot; {
  domain       = &quot;sagemaker-user-pool-ocr-domain&quot;
  user_pool_id = aws_cognito_user_pool.user_pool.id
}

resource &quot;aws_sagemaker_workteam&quot; &quot;workteam&quot; {
  workteam_name  = &quot;worker-team&quot;
  workforce_name = aws_sagemaker_workforce.workforce.id
  description    = &quot;worker-team&quot;

  member_definition {
    cognito_member_definition {
      client_id  = aws_cognito_user_pool_client.congnito_client.id
      user_pool  = aws_cognito_user_pool_domain.domain.user_pool_id
      user_group = aws_cognito_user_group.user_group.id
    }
  }
}

resource &quot;aws_sagemaker_human_task_ui&quot; &quot;template&quot; {
  human_task_ui_name = &quot;human-task-ui-template&quot;

  ui_template {
    content = file(&quot;${path.module}/sagemaker-human-task-ui-template.html&quot;)
  }
}

resource &quot;aws_sagemaker_flow_definition&quot; &quot;definition&quot; {
  flow_definition_name = &quot;flow-definition&quot;
  role_arn             = var.aws_iam_role

  human_loop_config {
    human_task_ui_arn                     = aws_sagemaker_human_task_ui.template.arn
    task_availability_lifetime_in_seconds = 1
    task_count                            = 1
    task_description                      = &quot;Task description&quot;
    task_title                            = &quot;Please review the Key Value Pairs in this document&quot;
    workteam_arn                          = aws_sagemaker_workteam.workteam.arn
  }

  output_config {
    s3_output_path = &quot;s3://${var.s3_output_path}&quot;
  }
}
</code></pre>
<p>it's creating the cognito user pool with callback urls. These callback urls is coming from <code>aws_sagemaker_workforce.workforce.subdomain</code> and getting set in cognito automatically which is <strong>what i want</strong>.</p>
<p>But i also want to <strong>set</strong> <strong>config</strong> in <strong>cognito</strong> <strong>userpool</strong> like</p>
<pre><code>allowed_oauth_flows = [&quot;code&quot;, &quot;implicit&quot;]
  allowed_oauth_scopes = [&quot;email&quot;, &quot;openid&quot;, &quot;profile&quot;]
</code></pre>
<p>now when i add above two line we need to add <strong>callbackurl</strong> also which i dont want.</p>
<p>i tried</p>
<pre><code>allowed_oauth_flows = [&quot;code&quot;, &quot;implicit&quot;]
  allowed_oauth_scopes = [&quot;email&quot;, &quot;openid&quot;, &quot;profile&quot;]
  callback_urls = [aws_sagemaker_workforce.workforce.subdomain]
</code></pre>
<p>which is giving error :</p>
<pre><code>Cycle: module.sagemaker.aws_cognito_user_pool_client.congnito_client, module.sagemaker.aws_sagemaker_workforce.workforce
</code></pre>
<p>as both resource are dependent on each other, i want to pass those two line but it forces me to add callback url also.</p>
<p>here is the final <strong>main.tf</strong> which is failing with that <strong>three</strong> line</p>
<pre><code>resource &quot;aws_sagemaker_workforce&quot; &quot;workforce&quot; {
  workforce_name = &quot;workforce&quot;

  cognito_config {
    client_id = aws_cognito_user_pool_client.congnito_client.id
    user_pool = aws_cognito_user_pool_domain.domain.user_pool_id
  }
}

resource &quot;aws_cognito_user_pool&quot; &quot;user_pool&quot; {
  name = &quot;sagemaker-cognito-userpool&quot;
}

resource &quot;aws_cognito_user_pool_client&quot; &quot;congnito_client&quot; {
  name            = &quot;congnito-client&quot;
  generate_secret = true
  user_pool_id    = aws_cognito_user_pool.user_pool.id

  explicit_auth_flows                  = [&quot;ALLOW_REFRESH_TOKEN_AUTH&quot;, &quot;ALLOW_USER_PASSWORD_AUTH&quot;, &quot;ALLOW_CUSTOM_AUTH&quot;, &quot;ALLOW_USER_SRP_AUTH&quot;]
  allowed_oauth_flows_user_pool_client = true
  supported_identity_providers = [&quot;COGNITO&quot;]

  allowed_oauth_flows = [&quot;code&quot;, &quot;implicit&quot;]
  allowed_oauth_scopes = [&quot;email&quot;, &quot;openid&quot;, &quot;profile&quot;]
  callback_urls = [aws_sagemaker_workforce.workforce.subdomain]
}

resource &quot;aws_cognito_user_group&quot; &quot;user_group&quot; {
  name         = &quot;user-group&quot;
  user_pool_id = aws_cognito_user_pool.user_pool.id
}

resource &quot;aws_cognito_user_pool_domain&quot; &quot;domain&quot; {
  domain       = &quot;sagemaker-user-pool-ocr-domain&quot;
  user_pool_id = aws_cognito_user_pool.user_pool.id
}

resource &quot;aws_sagemaker_workteam&quot; &quot;workteam&quot; {
  workteam_name  = &quot;worker-team&quot;
  workforce_name = aws_sagemaker_workforce.workforce.id
  description    = &quot;worker-team&quot;

  member_definition {
    cognito_member_definition {
      client_id  = aws_cognito_user_pool_client.congnito_client.id
      user_pool  = aws_cognito_user_pool_domain.domain.user_pool_id
      user_group = aws_cognito_user_group.user_group.id
    }
  }
}

resource &quot;aws_sagemaker_human_task_ui&quot; &quot;template&quot; {
  human_task_ui_name = &quot;human-task-ui-template&quot;

  ui_template {
    content = file(&quot;${path.module}/sagemaker-human-task-ui-template.html&quot;)
  }
}

resource &quot;aws_sagemaker_flow_definition&quot; &quot;definition&quot; {
  flow_definition_name = &quot;flow-definition&quot;
  role_arn             = var.aws_iam_role

  human_loop_config {
    human_task_ui_arn                     = aws_sagemaker_human_task_ui.template.arn
    task_availability_lifetime_in_seconds = 1
    task_count                            = 1
    task_description                      = &quot;Task description&quot;
    task_title                            = &quot;Please review the Key Value Pairs in this document&quot;
    workteam_arn                          = aws_sagemaker_workteam.workteam.arn
  }

  output_config {
    s3_output_path = &quot;s3://${var.s3_output_path}&quot;
  }
}
</code></pre>",0,0,2021-10-21 12:29:30.847000 UTC,,,2,amazon-web-services|terraform|amazon-cognito|terraform-provider-aws|amazon-sagemaker,181,2021-06-23 15:05:59.477000 UTC,2022-08-11 08:50:44.717000 UTC,,53,10,0,7,,,,,,['amazon-sagemaker']
Getting the task input value in labeling job without using the Liquid Library,"<p>I'm writing a custom Sagemaker Ground Truth job and I can't figure out how to get the <code>taskInput</code> values without using the Liquid form of <code>{{ task.input }}</code>.</p>

<p>I have searched everywhere, looked in localStorage, js variables, and the html itself.</p>",2,0,2019-07-01 05:26:01.973000 UTC,,2019-07-01 09:12:22.137000 UTC,1,amazon-sagemaker,447,2018-05-23 22:21:34.197000 UTC,2021-10-26 20:32:34.500000 UTC,,11,0,0,2,,,,,,['amazon-sagemaker']
Why do I have so few classified labels,"<p>In my dataset I have 300k rows, I do a 70/30 split and the result seems to be an alright model, until i view the true-positive, false-negative, false-positive and true-negative numbers.</p>

<p>TP is 20, FN is 2. FP is 3 and TN is 41. </p>

<p>That's extremely low? So the results a great, but if the model were only able to classify 66 of 90 000 is rather useless.</p>

<p>What can I do to improve this? Two Class Boosted decision tree or neural net does not change the outcome that much. Any recommendations?</p>",1,1,2017-03-04 22:57:11.647000 UTC,,,-1,azure-machine-learning-studio,21,2010-03-04 13:38:58.320000 UTC,2017-10-20 15:38:01.653000 UTC,Norway,13032,516,8,1004,,,,,,['azure-machine-learning-studio']
KeyError: 'ETag' while trying to load data from S3 to Sagemaker,"<p>I Unload a file of 500 MB into S3 from Redshift, instead of saving into a single file in S3 it bifurcated into several chunks and now I am trying to access it from S3 to AWS Sagemaker. While trying to read the file using Pd.read_csv and dask.dataframe.read_csv I am getting Keyerror as 'ETag'</p>
<p>I'm a newbie to AWS, please do help me.</p>
<p><a href=""https://i.stack.imgur.com/9a78F.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9a78F.png"" alt=""enter image description here"" /></a></p>",0,1,2022-04-25 16:34:38.070000 UTC,,,1,amazon-s3|jupyter-notebook|amazon-sagemaker,215,2019-03-07 11:16:00.467000 UTC,2022-05-27 08:57:06.190000 UTC,"Bangalore, Karnataka, India",327,5,0,15,,,,,,['amazon-sagemaker']
Can I use papermill for aws sagemaker notebooks to create excel reports?,"<p>So I know I can use papermill to run jupyter notebooks in an automated way, but if I use an AWS Sagemaker notebook and I create excel reports with jupyter notebook (exporting to excel).
How do I find my excel files? Because I can only give a notebook as an output file or not?</p>
<p>I'm planning to use the solution below:
<a href=""https://github.com/aws-samples/sagemaker-run-notebook/blob/master/QuickStart.md#using-existing-aws-primitives"" rel=""nofollow noreferrer"">https://github.com/aws-samples/sagemaker-run-notebook/blob/master/QuickStart.md#using-existing-aws-primitives</a></p>",1,0,2022-01-29 17:44:51.887000 UTC,1.0,,0,excel|amazon-web-services|jupyter-notebook|amazon-sagemaker|papermill,361,2020-10-07 13:51:51.433000 UTC,2022-06-09 13:04:20.967000 UTC,,1,0,0,2,,,,,,['amazon-sagemaker']
"AWS Sagemaker : No response back from the endpoint ""HTTP 301 293""","<p>I'm trying to host custom docker container through sagemaker. 
Im using nginx, gunicorn, flask setup. 
I'm able to invoke(ping) the endpoint for my application.
input to my service is 'application/json' format and expected output from the service is json. </p>

<p>When i call the service i get following output in the client : </p>

<pre><code>'&lt;!DOCTYPE HTML PUBLIC ""-//W3C//DTD HTML 3.2Final//EN""&gt;\n&lt;title&gt;Redirecting...&lt;/title&gt;\n&lt;h1&gt;Redirecting...&lt;/h1&gt;\n&lt;p&gt;You should be redirected automatically to target URL: &lt;a href=""http://boaucpph.aws.local:8080/invocations/""&gt;http://boaucpph.aws.local:8080/invocations/&lt;/a&gt;.  If not click the link.'
</code></pre>

<p>and my endpoint logs tell : </p>

<pre><code>10.32.0.2 - - [28/Mar/2018:20:50:41 +0000] ""POST /invocations HTTP/1.1"" 301 293 ""-"" ""AHC/2.0""
</code></pre>

<p>my nginx.conf </p>

<pre><code>worker_processes 1;
daemon off; # Prevent forking

pid /tmp/nginx.pid;
error_log /var/log/nginx/error.log;

events {
  # defaults
}

 http {
  include /etc/nginx/mime.types;
  default_type application/octet-stream;
  access_log /var/log/nginx/access.log combined;

  upstream gunicorn {
    server unix:/tmp/gunicorn.sock;
  }

  server {
    listen 8080 deferred;
    client_max_body_size 100M;

    keepalive_timeout 5;

    location ~ ^/(ping|invocations) {
      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
      proxy_set_header Host $http_host;
      proxy_redirect off;
      proxy_pass http://gunicorn;
    }

    location / {
      return 404 ""{}"";
    }
  }
}
</code></pre>

<p>Has anyone faced similar problem ? any suggestions on this would be of great help. </p>",1,0,2018-03-28 22:15:36.927000 UTC,,,0,amazon-web-services|nginx|flask|gunicorn|amazon-sagemaker,739,2012-01-02 07:37:12.347000 UTC,2022-05-06 19:20:21.163000 UTC,,21,11,0,9,,,,,,['amazon-sagemaker']
Set artifact name when using kfp dsl.importer,"<p>When importing an artifact using the kfp <code>dsl.importer()</code> function, the imported artifact gets the default (display) name <code>artifact</code>. I would like to give it a custom name to make the pipeline and lineage tracking more clear. I checked the <a href=""https://kubeflow-pipelines.readthedocs.io/en/latest/source/kfp.dsl.html#kfp.dsl.importer"" rel=""nofollow noreferrer"">documentation</a>, but I can't seem to find a way to change the name of the artifact that the <code>dsl.importer()</code> function produces.</p>
<p>Example code <code>dsl.importer()</code>:</p>
<pre><code>    load_dataset_step = dsl.importer(
        artifact_uri=input_data_uri,
        artifact_class=dsl.Dataset,
        reimport=False
    ).set_display_name(&quot;Load Dataset&quot;)
</code></pre>
<p>Visualisation of the <code>dsl.importer()</code> step:</p>
<p><a href=""https://i.stack.imgur.com/b4Qx6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/b4Qx6.png"" alt=""pipelines visualisation"" /></a></p>
<p>I'm making use of Google Cloud Vertex AI Pipelines.</p>",0,3,2022-04-06 08:25:10.080000 UTC,,,1,python|kubeflow|kubeflow-pipelines|google-cloud-vertex-ai,158,2012-11-21 14:42:05.387000 UTC,2022-09-22 13:53:11.917000 UTC,Belgium,383,100,8,132,,,,,,['google-cloud-vertex-ai']
unable to identify current timezone 'C',"<p>all,</p>

<p>I am using R on the Azure machine learning, and I have some problems.</p>

<p>I want to use program R to calculate the difference between two date, for example, 2014/11/01 and 2014/11/03.</p>

<p>I using the function ""strptime"" in R to do this thing, it can work on my own computer, but when I want to run the same code on Azure ml, it came out the error.</p>

<p>The error is : </p>

<pre><code>[ModuleOutput] 1: In strptime(x, format, tz = tz) :
[ModuleOutput] 
[ModuleOutput]   unable to identify current timezone 'C':
[ModuleOutput] 
[ModuleOutput] please set environment variable 'TZ'
[ModuleOutput] 
[ModuleOutput] 2: In strptime(x, format, tz = tz) : unknown timezone 'localtime'
</code></pre>

<p>I think the problem is that it can't detect the timezone on Azure ml, but I'm not sure.</p>

<p>Is there any way to solve this problem?</p>

<p>Thanks in advance.</p>",0,2,2014-11-06 08:20:44.470000 UTC,1.0,,4,r|timezone|azure-machine-learning-studio,1456,2014-10-15 03:43:33.203000 UTC,2022-09-23 15:05:13.727000 UTC,"Taipei City, Taiwan",579,370,3,73,,,,,,['azure-machine-learning-studio']
While deploying model to AKS PipelineModel.load throwing org.apache.hadoop.mapred.InvalidInputException,"<p>I am trying to deploy model to AKS. I am using AML SDK to register the model in the aml workspace. I am using PipelineModel module to save the model. And I am trying to load the model using PipelineModel.load. My entry script looks like below:</p>

<p>` 
import os
import json
import pandas as pd</p>

<p>from azureml.core.model import Model
from pyspark.ml import PipelineModel
from mmlspark import ComputeModelStatistics</p>

<p>def init():
    import mmlspark  # this is needed to load mmlspark libraries
    import logging</p>

<pre><code># extract and load model
global model, model_path
model_path = Model.get_model_path(""{model_name}"")
print(model_path)
print(os.stat(model_path))
print(os.path.exists(model_path))
#model_path = os.path.join(os.getenv(""AZUREML_MODEL_DIR""), ""{model_name}"")
logging.basicConfig(level=logging.DEBUG)
#print(model_path)
#with ZipFile(model_path, 'r') as f:
#    f.extractall('model')
model = PipelineModel.load(model_path)
#model = PipelineModel.read().load(model_path)
</code></pre>

<p>def run(input_json):
    try:
        output_df = model.transform(pd.read_json(input_json))
        evaluator = ComputeModelStatistics().setScoredLabelsCol(""prediction"").setLabelCol(""label"").setEvaluationMetric(""AUC"")
        result = evaluator.transform(predictions)
        auc = result.select(""AUC"").collect()[0][0]
        result = auc
    except Exception as e:
        result = str(e)</p>

<pre><code>return json.dumps({{""result"": result}})
</code></pre>

<p>`</p>

<p>It's giving error like below:</p>

<p>org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/var/azureml-app/azureml-models/lightgbm.model/2/lightgbm.model/metadata\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315). </p>

<p>os.path.exists returns true the path fetched from Model.get_model_path. </p>

<p>Am I missing something here? </p>",0,0,2020-04-15 08:07:57.670000 UTC,,,1,hadoop|pyspark|azure-aks|azure-machine-learning-service,32,2012-08-23 16:58:06.593000 UTC,2022-09-15 07:26:06.053000 UTC,,63,1,0,15,,,,,,['azure-machine-learning-service']
AWS Sagemaker: Jupyter Notebook kernel keeps dying,"<p>I get disconnect every now and then when running a piece of code in Jupyter Notebooks on Sagemaker. I usually just restart my notebook and run all the cells again. However, I want to know if there is a way to reconnect to my instance without having to lose my progress. At the minute, it shows that there is &quot;No Kernel&quot; at the bottom bar, but my file seems active in the kernel sessions tab. Can I recover my notebook's variables and contents? Also, is there a way to prevent future kernel disconnections?</p>
<p>Note that I reverted back to tornado = 5.1.1, which seems to decrease the number of disconnections, but it still happens every now and then.</p>",1,3,2021-01-27 19:23:03.920000 UTC,1.0,,3,amazon-web-services|jupyter-notebook|amazon-sagemaker,1983,2018-08-08 18:39:40.750000 UTC,2022-09-23 13:21:10.983000 UTC,,344,54,1,63,,,,,,['amazon-sagemaker']
services created in ML services,"<p>At a client we have been using Azure Machine Learning Service (MLS) for a two projects. When making a new MLS Workspace, four services are automatically created:</p>

<p>•   A storage account
•   A keyvault
•   A container registry
•   En application insights</p>

<p>We’ve recently realized that the application insights service were created in the East US region instead of North West Europe, where the rest of the services were created. The client would like to have all their services running in North West Europe, so they’ve asked us to change it.</p>

<p>So my question is: Can you change “reference” in Azure Machine Learning Service from one application insights to another? I’ve been unable to find anything in this regard in the documentation. Or do we have to create a new MLS workspace in order to do it?</p>",1,0,2019-08-28 09:24:07.530000 UTC,,,1,azure-machine-learning-service,73,2015-11-14 06:54:24.853000 UTC,2021-07-27 13:59:43.153000 UTC,,53,0,0,17,,,,,,['azure-machine-learning-service']
MLflow - TypeError: Only dict and DataFrame input types are supported,"<p>I'm fairly new to the software MLflow and I'm trying to make an HTTP POST request to the served model I developed but the error up in the title appears.</p>
<p>Here's the situation.
I use as a backend storage a SQLite db and as an artifact storage a local folder.
The command to run the mlflow server is the following (the model is in the Staging stage):
<code>mlflow models serve -m &quot;models:/nuovo_modello/Staging&quot; -p 1234</code></p>
<p>I registered the model on MLflow and this is the model schema:</p>
<p><a href=""https://i.stack.imgur.com/ERL5m.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ERL5m.png"" alt=""enter image description here"" /></a></p>
<p>When I try to make a POST request as follows (as suggested in the TF serving guide: <a href=""https://www.tensorflow.org/tfx/serving/api_rest#request_format_2"" rel=""nofollow noreferrer"">https://www.tensorflow.org/tfx/serving/api_rest#request_format_2</a>)
<code>{ &quot;instances&quot;: [ [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 3, 1, 4]] }</code></p>
<p>or even in the JSON Content-Type as follows:
<code>curl http://127.0.0.1:1234/invocations -H &quot;Content-Type: application/json; format=pandas-split&quot; -d '{&quot;columns&quot;:[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99], &quot;data&quot;:[[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,1,3,1,4]]}'</code></p>
<p>I get this error and I don't really know what's causing it:</p>
<blockquote>
<p>{&quot;error_code&quot;: &quot;BAD_REQUEST&quot;, &quot;message&quot;: &quot;Encountered an unexpected error while evaluating the model. Verify that the serialized input Dataframe is compatible with the model for inference.&quot;, &quot;stack_trace&quot;: &quot;Traceback [...]</p>
</blockquote>
<blockquote>
<p>File &quot;/Path/to/the/file/venv/lib/python3.8/site-packages/mlflow/tensorflow.py&quot;, line 584, in predict\n    raise TypeError(f&quot;Only dict and DataFrame input types are supported}&quot;)\nTypeError: Only dict and DataFrame input types are supported</p>
</blockquote>
<p>The data it's causing this error is not a <code>DataFrame</code> nor a <code>dict</code> but is a <code>numpy.ndarray</code> instead (I checked it with a type(...) while debugging).</p>
<p>The shape of the inputs are correct but I really don't know how to solve this. It seems MLflow converts the data into the numpy.ndarray without any reason</p>
<p>Thanks in advance to anyone who'll help me</p>",1,0,2021-09-01 16:23:16.477000 UTC,,,0,python|dataframe|tensorflow|tensorflow-serving|mlflow,291,2020-01-15 20:34:13.467000 UTC,2022-09-22 12:56:39.437000 UTC,"Reggio Emilia, RE, Italia",49,19,0,5,,,,,,['mlflow']
stopping hyperparameter tuning (HPO) jobs after reaching metric threshold in aws sage maker,"<p>I am running HPO jobs in sage maker, and I am thinking of a way to stop my HPO job after one of the child training jobs reaches a specific metrics threshold.</p>
<p>PS: I tried sage maker early stopping, but it only works on the level of epocs within each training job, so it stops training jobs if it noticed that their learning pattern might not give as good metric as the best training jobs found already. But this does not solve my problem which in the level of HPO combinations, so regardless of what happens within the child training jobs, I want to stop the whole HPO job after one of its children reaches my desired metric threshold.</p>",0,0,2021-04-12 10:27:03.547000 UTC,,,2,amazon-sagemaker|hyperparameters|early-stopping,73,2019-02-14 07:12:07.130000 UTC,2022-02-16 11:49:09.410000 UTC,Sweden,59,7,0,14,,,,,,['amazon-sagemaker']
Conversion of facebook/nllb-200-3.3B to AWS neuron,"<p>I am trying to convert the <a href=""https://huggingface.co/facebook/nllb-200-3.3B"" rel=""nofollow noreferrer"">new translation model developed by Facebook (Meta)</a>, No Language Left Behind, to AWS's neuron model that can be used with the AWS SageMaker Inference using the Inferentia chips. However, I cannot figure out how to trace the model without errors. This <a href=""https://github.com/aws/aws-neuron-sdk/issues/420#issuecomment-1220885577"" rel=""nofollow noreferrer"">post</a> shows exactly what I am trying to do and working the AWS developers. I will copy my code to here as well for clarity:</p>
<pre><code>import copy
import itertools
from typing import List, Optional, Tuple

import torch
import torch.nn.functional as F

from transformers import M2M100Config
from transformers.generation_utils import GenerationMixin


def _convert_past_list_to_tuple(past_key_values):
    &quot;&quot;&quot;
    In Bart model, the type of past_key_values is tuple(tuple(torch.FloatTensor)) which is not
    TorchScript-compatible. To support this, we have to convert it during the export process.
    This function will convert past values from a list to tuple(tuple(torch.FloatTensor)) for
    the inner decoder.

    According to the definition of past_key_values, each inner tuple(torch.FloatTensor) has 4 tensors,
    so we convert every 4 elements in the list as a tuple(torch.FloatTensor).
    &quot;&quot;&quot;
    count_of_each_inner_tuple = 4
    results = ()
    temp_result = ()
    count_n = len(past_key_values) // count_of_each_inner_tuple
    for idx in range(count_n):
        real_idx = idx * count_of_each_inner_tuple
        temp_result = tuple(past_key_values[real_idx : real_idx + count_of_each_inner_tuple])
        results += ((temp_result),)

    return results


class EncoderForONNX(torch.nn.Module):
    def __init__(self, encoder):
        super().__init__()
        self.encoder = encoder

    def forward(self, input_ids, attention_mask):
        return self.encoder(
            input_ids=input_ids,
            attention_mask=attention_mask,
            return_dict=False,
        )


class DecoderForONNX(torch.nn.Module):
    def __init__(self, decoder):
        super().__init__()
        self.decoder = decoder

    def forward(self, input_ids, encoder_state, attention_mask, past=None):
        all_results = None
        if past is not None:
            all_results = _convert_past_list_to_tuple(past)
            input_ids = input_ids[:, -1:]

        last_hidden_state, past_key_values = self.decoder(
            input_ids=input_ids,
            encoder_hidden_states=encoder_state,
            encoder_attention_mask=attention_mask,
            past_key_values=all_results,
            return_dict=False,
        )

        past_values = []
        for past in past_key_values:
            past_values = past_values + list(past)
        return last_hidden_state, past_values


def _create_traced_encoder(encoder, input_ids, attention_mask):
    encoder_c = copy.deepcopy(encoder)
    print(&quot;shapes&quot;,input_ids.shape, attention_mask.shape)
    encoder_for_onnx = EncoderForONNX(encoder_c)
    compiler_args = ['--fp32-cast', 'matmult', '--fast-math', 'no-fast-relayout']
    inputs = (
        input_ids,
        attention_mask,
        )

    return torch_neuron.trace(encoder_for_onnx, inputs,compiler_args=compiler_args)


def _create_traced_decoder(decoder, input_ids, encoder_state, attention_mask, past=None):
    decoder_c = copy.deepcopy(decoder)
    print(input_ids.shape,encoder_state.shape,attention_mask.shape)
    decoder_for_onnx = DecoderForONNX(decoder_c)
    past_values = list(itertools.chain.from_iterable(past or ()))
    compiler_args = ['--fp32-cast', 'matmult', '--fast-math', 'no-fast-relayout']
    print(past_values)
    # Do this twice so we got 2 different decoders for further work.
    if past_values:
        inputs = (
            input_ids,
            encoder_state,
            attention_mask,
            past_values,
        )
        return torch_neuron.trace(decoder_for_onnx, inputs,compiler_args=compiler_args)
    else:
        inputs = (
            input_ids,
            encoder_state,
            attention_mask,
        )
        return torch_neuron.trace(decoder_for_onnx, inputs,compiler_args=compiler_args)


class M2M100ConfigTS(M2M100Config, torch.nn.Module):
    &quot;&quot;&quot;
    BartConfigTS is a TorchScript-compatible transformers.models.bart.configuration_bart.BartConfig.
    TorchScript only supports sub-classes of torch.nn.Module.
    &quot;&quot;&quot;

    def __init__(self, config):
        M2M100Config.__init__(self, config)
        torch.nn.Module.__init__(self)


class MinLengthLogitsProcessorTS(torch.nn.Module):
    r&quot;&quot;&quot;
    :class:`transformers.LogitsProcessor` enforcing a min-length by setting EOS probability to 0.

    Args:
        min_length (:obj:`int`):
            The minimum length below which the score of :obj:`eos_token_id` is set to :obj:`-float(&quot;Inf&quot;)`.
        eos_token_id (:obj:`int`):
            The id of the `end-of-sequence` token.
    &quot;&quot;&quot;

    def __init__(self, min_length: int, eos_token_id: int):
        super().__init__()

        if not isinstance(min_length, int) or min_length &lt; 0:
            raise ValueError(f&quot;`min_length` has to be a positive integer, but is {min_length}&quot;)

        if not isinstance(eos_token_id, int) or eos_token_id &lt; 0:
            raise ValueError(f&quot;`eos_token_id` has to be a positive integer, but is {eos_token_id}&quot;)

        self.min_length = min_length
        self.eos_token_id = eos_token_id

    def forward(self, input_ids, scores) -&gt; torch.Tensor:
        cur_len = input_ids.shape[-1]
        if cur_len &lt; self.min_length:
            scores[:, self.eos_token_id] = -float(&quot;inf&quot;)
        return scores


class NLLBGenerator(torch.nn.Module, GenerationMixin):
    def __init__(self, model):
        super().__init__()
        self.config = M2M100ConfigTS(model.config)
        self.config.force_bos_token_to_be_generated = False
        self._trace_modules(model)
        self.logits_processor = MinLengthLogitsProcessorTS(self.config.min_length, self.config.eos_token_id)
        self.final_logits_weight = model.model.shared.weight
        self.final_logits_bias = model.final_logits_bias
        self.decoder_layers = model.config.decoder_layers
        self.d_model = model.config.d_model

    def _trace_modules(self, model):
        # input_ids = torch.tensor(
        #     [
        #         [
        #             19,669,18,420,8,664,57,42,8,664,21,3028,195,4445,331,1293,34,21,10,6174,1100,6,69,104,42,32,2621,1638,144,4,6174,558,108,4419,1091,28,4,1668,9,1509,1621,279,35,867,2734,85,11,2216,2734,85,203,2244,7,6,15,8102,7,57,8629,5,
        #             model.config.eos_token_id,
        #         ]
        #     ],
        #     device=model.device,
        #     dtype=torch.long,
        # )
        # attention_mask = torch.tensor(
        #     [[True] * input_ids.shape[-1]],
        #     device=model.device,
        #     dtype=torch.bool,
        # )
        pegasus_text = &quot;PG&amp;E stated it scheduled the blackouts in response to forecasts for high winds amid dry conditions. The aim is to reduce the risk of wildfires.&quot;
        model_name = &quot;sshleifer/distilbart-cnn-12-6&quot;

        tokenizer = AutoTokenizer.from_pretrained(model_name)
        inputs = tokenizer(pegasus_text , return_tensors=&quot;pt&quot;, max_length=32, truncation=True, padding='max_length')
        input_ids = inputs[&quot;input_ids&quot;]
        attention_mask = inputs[&quot;attention_mask&quot;]

        self.encoder = _create_traced_encoder(model.get_encoder(), input_ids, attention_mask)
        encoder_outputs = model.get_encoder()(input_ids, attention_mask=attention_mask, return_dict=True)
        decoder = model.model.decoder
        decoder_outputs = decoder(input_ids, attention_mask, encoder_outputs[&quot;last_hidden_state&quot;], None, None, None)
        # print(decoder_outputs[1])
        # print(decoder_outputs[1].shape)
        self.decoder_no_past = _create_traced_decoder(
            model.model.decoder, input_ids, encoder_outputs[&quot;last_hidden_state&quot;], attention_mask
        )
        self.decoder_with_past = _create_traced_decoder(
            model.model.decoder, input_ids, encoder_outputs[&quot;last_hidden_state&quot;], attention_mask, decoder_outputs[1]
        )

    def _encoder_forward(self, input_ids, attention_mask):
        return self.encoder(input_ids, attention_mask)[0]

    @staticmethod
    def _init_sequence_length_for_generation(
        input_ids: torch.LongTensor, max_length: int
    ) -&gt; Tuple[torch.Tensor, torch.Tensor, int]:
        unfinished_sequences = torch.zeros(input_ids.shape[0], dtype=torch.long, device=input_ids.device) + 1
        sequence_lengths = torch.zeros(input_ids.shape[0], dtype=torch.long, device=input_ids.device) + max_length

        cur_len = input_ids.shape[-1]
        return sequence_lengths, unfinished_sequences, cur_len

    def _decoder_forward(self, input_ids, encoder_output, attention_mask, past: List[torch.Tensor]):
        # Update here to use different decoder for different values of past.
        if past is None or len(past) == 0:
            decoder_output, past = self.decoder_no_past(
                input_ids=input_ids, encoder_state=encoder_output, attention_mask=attention_mask
            )
        else:
            decoder_output, past = self.decoder_with_past(
                input_ids=input_ids, encoder_state=encoder_output, attention_mask=attention_mask, past=past
            )

        lm_logits = F.linear(decoder_output, self.final_logits_weight, bias=self.final_logits_bias)

        return lm_logits, past

    def greedy_search(
        self, input_ids, encoder_output, attention_mask, max_length, pad_token_id: int, eos_token_id: int
    ):
        # init sequence length tensors
        sequence_lengths, unfinished_sequences, cur_len = self._init_sequence_length_for_generation(
            input_ids, max_length
        )

        past: List[torch.Tensor] = []
        while cur_len &lt; max_length:

            logits, past = self._decoder_forward(input_ids, encoder_output, attention_mask, past)
            next_token_logits = logits[:, -1, :]

            # pre-process distribution
            scores = self.logits_processor(input_ids, next_token_logits)

            # argmax
            next_tokens = torch.argmax(scores, dim=-1)

            # add code that transfomers next_tokens to tokens_to_add
            if eos_token_id is not None:
                assert pad_token_id is not None, &quot;If eos_token_id is defined, make sure that pad_token_id is defined.&quot;
                next_tokens = next_tokens * unfinished_sequences + (pad_token_id) * (1 - unfinished_sequences)

            # add token and increase length by one
            input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)

            # update sequence length
            if eos_token_id is not None:
                sequence_lengths, unfinished_sequences = self._update_seq_length_for_generation(
                    sequence_lengths, unfinished_sequences, cur_len, next_tokens == eos_token_id
                )

            # stop when there is a &lt;/s&gt; in each sentence, or if we exceed the maximul length
            if unfinished_sequences.max() == 0:
                break

            # increase cur_len
            cur_len = cur_len + 1

        return input_ids

    def _prepare_decoder_input_ids_for_generation(
        self,
        input_ids: torch.LongTensor,
        decoder_start_token_id,
        bos_token_id: Optional[int] = None,
    ) -&gt; torch.LongTensor:

        decoder_input_ids = (
            torch.ones((input_ids.shape[0], 1), dtype=input_ids.dtype, device=input_ids.device)
            * decoder_start_token_id
        )
        return decoder_input_ids

    def forward(self, input_ids, attention_mask, max_length, decoder_start_token_id):
        pad_token_id = self.config.pad_token_id
        bos_token_id = self.config.bos_token_id
        eos_token_id = self.config.eos_token_id

        # special case if pad_token_id is not defined
        if pad_token_id is None and eos_token_id is not None:
            # Setting `pad_token_id` to `eos_token_id`:{eos_token_id} for open-end generation.
            pad_token_id = eos_token_id

        encoder_output = self._encoder_forward(input_ids, attention_mask)

        input_ids = self._prepare_decoder_input_ids_for_generation(
            input_ids,
            decoder_start_token_id=decoder_start_token_id,
            bos_token_id=bos_token_id,
        )

        return self.greedy_search(
            input_ids,
            encoder_output,
            attention_mask,
            max_length=max_length,
            pad_token_id=pad_token_id,
            eos_token_id=eos_token_id,
        )
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
model = AutoModelForSeq2SeqLM.from_pretrained(&quot;facebook/nllb-200-3.3B&quot;)
import torch
import torch_neuron


neuron_model = NLLBGenerator(model)
</code></pre>
<p>And the current error I am receiving:</p>
<pre><code>/home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p37/lib/python3.7/site-packages/transformers/models/m2m_100/modeling_m2m_100.py:326: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):
INFO:Neuron:There are 1 ops of 1 different types in the TorchScript that are not compiled by neuron-cc: aten::embedding, (For more information see https://github.com/aws/aws-neuron-sdk/blob/master/release-notes/neuron-cc-ops/neuron-cc-ops-pytorch.md)
INFO:Neuron:Number of arithmetic operators (pre-compilation) before = 1479, fused = 1456, percent fused = 98.44%
INFO:Neuron:Number of neuron graph operations 3581 did not match traced graph 3283 - using heuristic matching of hierarchical information
WARNING:Neuron:torch.neuron.trace failed on _NeuronGraph$1631; falling back to native python function call
ERROR:Neuron:Error parsing message with type 'tensorflow.GraphDef'
Traceback (most recent call last):
  File &quot;/home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p37/lib/python3.7/site-packages/torch_neuron/convert.py&quot;, line 382, in op_converter
    item, inputs, compiler_workdir=sg_workdir, **kwargs)
  File &quot;/home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p37/lib/python3.7/site-packages/torch_neuron/decorators.py&quot;, line 82, in trace
    graph_def = graph.as_graph_def()
  File &quot;/home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p37/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py&quot;, line 3238, in as_graph_def
    result, _ = self._as_graph_def(from_version, add_shapes)
  File &quot;/home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p37/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py&quot;, line 3166, in _as_graph_def
    graph.ParseFromString(compat.as_bytes(data))
google.protobuf.message.DecodeError: Error parsing message with type 'tensorflow.GraphDef'
INFO:Neuron:Number of arithmetic operators (post-compilation) before = 1479, compiled = 0, percent compiled = 0.0%
INFO:Neuron:The neuron partitioner created 1 sub-graphs
INFO:Neuron:Neuron successfully compiled 0 sub-graphs, Total fused subgraphs = 1, Percent of model sub-graphs successfully compiled = 0.0%
INFO:Neuron:Compiled these operators (and operator counts) to Neuron:
INFO:Neuron:Not compiled operators (and operator counts) to Neuron:
INFO:Neuron: =&gt; aten::Int: 414 [supported]
INFO:Neuron: =&gt; aten::add: 75 [supported]
INFO:Neuron: =&gt; aten::bmm: 48 [supported]
INFO:Neuron: =&gt; aten::contiguous: 72 [supported]
INFO:Neuron: =&gt; aten::cumsum: 1 [supported]
INFO:Neuron: =&gt; aten::detach: 1 [supported]
INFO:Neuron: =&gt; aten::dropout: 97 [supported]
INFO:Neuron: =&gt; aten::embedding: 1 [not supported]
INFO:Neuron: =&gt; aten::expand: 1 [supported]
INFO:Neuron: =&gt; aten::index_select: 1 [supported]
INFO:Neuron: =&gt; aten::layer_norm: 49 [supported]
INFO:Neuron: =&gt; aten::linear: 144 [supported]
INFO:Neuron: =&gt; aten::masked_fill: 1 [supported]
INFO:Neuron: =&gt; aten::mul: 74 [supported]
INFO:Neuron: =&gt; aten::ne: 1 [supported]
INFO:Neuron: =&gt; aten::relu: 24 [supported]
INFO:Neuron: =&gt; aten::reshape: 24 [supported]
INFO:Neuron: =&gt; aten::rsub: 1 [supported]
INFO:Neuron: =&gt; aten::size: 77 [supported]
INFO:Neuron: =&gt; aten::slice: 2 [supported]
INFO:Neuron: =&gt; aten::softmax: 24 [supported]
INFO:Neuron: =&gt; aten::to: 5 [supported]
INFO:Neuron: =&gt; aten::transpose: 120 [supported]
INFO:Neuron: =&gt; aten::type_as: 1 [supported]
INFO:Neuron: =&gt; aten::unsqueeze: 2 [supported]
INFO:Neuron: =&gt; aten::view: 219 [supported]
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
/tmp/ipykernel_4519/3952284984.py in &lt;module&gt;
    314 
    315 
--&gt; 316 neuron_model = NLLBGenerator(model)

/tmp/ipykernel_4519/3952284984.py in __init__(self, model)
    154         self.config = M2M100ConfigTS(model.config)
    155         self.config.force_bos_token_to_be_generated = False
--&gt; 156         self._trace_modules(model)
    157         self.logits_processor = MinLengthLogitsProcessorTS(self.config.min_length, self.config.eos_token_id)
    158         self.final_logits_weight = model.model.shared.weight

/tmp/ipykernel_4519/3952284984.py in _trace_modules(self, model)
    185         attention_mask = inputs[&quot;attention_mask&quot;]
    186 
--&gt; 187         self.encoder = _create_traced_encoder(model.get_encoder(), input_ids, attention_mask)
    188         encoder_outputs = model.get_encoder()(input_ids, attention_mask=attention_mask, return_dict=True)
    189         decoder = model.model.decoder

/tmp/ipykernel_4519/3952284984.py in _create_traced_encoder(encoder, input_ids, attention_mask)
     80         )
     81 
---&gt; 82     return torch_neuron.trace(encoder_for_onnx, inputs,compiler_args=compiler_args)
     83 
     84 

~/anaconda3/envs/aws_neuron_pytorch_p37/lib/python3.7/site-packages/torch_neuron/convert.py in trace(func, example_inputs, fallback, op_whitelist, minimum_segment_size, subgraph_builder_function, subgraph_inputs_pruning, skip_compiler, debug_must_trace, allow_no_ops_on_neuron, compiler_workdir, dynamic_batch_size, compiler_timeout, _neuron_trace, compiler_args, optimizations, verbose, **kwargs)
    182         logger.debug(&quot;skip_inference_context - trace with fallback at {}&quot;.format(get_file_and_line()))
    183         neuron_graph = cu.compile_fused_operators(neuron_graph, **compile_kwargs)
--&gt; 184     cu.stats_post_compiler(neuron_graph)
    185 
    186     # Wrap the compiled version of the model in a script module. Note that this is

~/anaconda3/envs/aws_neuron_pytorch_p37/lib/python3.7/site-packages/torch_neuron/convert.py in stats_post_compiler(self, neuron_graph)
    491         if succesful_compilations == 0 and not self.allow_no_ops_on_neuron:
    492             raise RuntimeError(
--&gt; 493                 &quot;No operations were successfully partitioned and compiled to neuron for this model - aborting trace!&quot;)
    494 
    495         if percent_operations_compiled &lt; 50.0:

RuntimeError: No operations were successfully partitioned and compiled to neuron for this model - aborting trace!
</code></pre>
<p>Any help would be appreciated.</p>",0,0,2022-08-23 16:29:58.903000 UTC,,,0,nlp|pytorch|translation|amazon-sagemaker|huggingface-transformers,74,2014-07-25 21:18:05.797000 UTC,2022-09-22 20:42:23.167000 UTC,"Richland, WA, USA",752,128,7,88,,,,,,['amazon-sagemaker']
"""Your invocation timed out while waiting for a response from container primary"". What does this error mean?","<p>I have a semantic segmentation model which I deployed on ml.m4.xlarge I am using invoke_endpoint from inside an AWS Lambda function using the following bit of code.</p>
<pre><code>with open('\tmp\image.jpg', 'rb') as imfile:
    imbytes = imfile.read()

response = runtime.invoke_endpoint(EndpointName = 'xyx', ContentType = 'image/jpeg',
                                   Body = imbytes)
</code></pre>
<p>This is when I get the error as mentioned above</p>
<pre><code>Your invocation timed out while waiting for a response from container primary
</code></pre>
<p>Does it mean my datapoint is reaching the model endpoint but it's taking too long to do the inference or is my data not even transferring over to the endpoint?</p>",1,1,2021-11-18 19:46:43.973000 UTC,,2021-11-18 21:02:20.063000 UTC,0,amazon-web-services|aws-lambda|deployment|amazon-sagemaker,923,2017-07-02 18:59:25.057000 UTC,2022-09-22 19:21:39.343000 UTC,"Kolkata, West Bengal, India",41,5,0,18,,,,,,['amazon-sagemaker']
mlflow static_prefix url in set_tracking_uri is not working,"<p>I am starting mlflow with below command</p>
<pre><code>mlflow server --static_prefix=/myprefix --backend-store-uri postgresql://psql_user_name:psql_password@localhost/mlflow_db --default-artifact-root s3://my-mlflow-bucket/ --host 0.0.0.0 -p 8000
</code></pre>
<p>everything worked fine and I can see mlflow UI when I open url http://localhost:8000/myprefix
but when I use mlflow.set_tracking_uri() i have to give url path as &quot;http://localhost:8000/&quot;</p>
<p>why cant we use full url , which has static prefix &quot;http://localhost:8000/myprefix&quot; ?</p>
<p>if i use full url ,I am getting request to api endpoint fail and api is experiments/list error 404 !=200
is there any way to add url with static prefix in set_tracking_uri</p>",1,1,2022-01-20 05:56:04.867000 UTC,,,0,machine-learning|artificial-intelligence|mlflow,246,2021-07-08 08:00:42.200000 UTC,2022-08-04 06:25:20.980000 UTC,,145,8,0,22,,,,,,['mlflow']
Docker file for spacy and scispacy model deployment in AWS sagemaker,"<p>Can somebody share the sample docker file for spacy, scispacy libraries and there corresponding models in AWS Sagemaker under category of bringing your own pre trained models</p>",0,2,2020-05-09 11:15:43.423000 UTC,,,1,dockerfile|spacy|amazon-sagemaker,160,2020-05-09 11:11:41.530000 UTC,2020-06-30 04:53:33.363000 UTC,,11,0,0,0,,,,,,['amazon-sagemaker']
Pandas Dataframe upload to Sagemaker to S3 bucket,"<p>I have tried to find the solution for taking my dataframe and uploading it as a csv to S3. I tried this resource but I may be confused. <a href=""https://stackoverflow.com/questions/49977679/upload-data-to-s3-with-sagemaker"">upload data to S3 with sagemaker</a></p>

<pre><code>import boto3
s3 =boto.resource('s3')
bucket = 'work'
key = 'test.csv'
doc = df.to_csv('test.csv')
s3.Bucket(bucket).put_object(Key= key, Body = doc)
</code></pre>

<p>I get the following error:</p>

<pre><code>ParamValidationError: Parameter validation failed:
Invalid type for parameter Body, value: None, type: &lt;type 'NoneType'&gt;, valid 
types: &lt;type 'str'&gt;, &lt;type 'bytearray'&gt;, file-like object
</code></pre>

<p>I am very new to s3 and using Sagemaker. I feel like what I am doing is completely wrong. </p>",1,0,2018-11-04 21:40:19.597000 UTC,,2018-11-12 19:25:29.977000 UTC,0,amazon-s3|boto3|amazon-sagemaker,2083,2017-10-18 19:08:41.193000 UTC,2020-07-21 12:28:18.533000 UTC,,45,1,0,8,,,,,,['amazon-sagemaker']
Error while deploying a model on an endpoint- Vertex AI,"<p>I'm working on a model that I need to deploy on a Vertex AI endpoint. The model is a DNN developed in Tensorflow. I've saved the model locally, loaded to GCS and imported it in the Vertex AI Model section without problems. When I'm trying to deploy it to a new endpoint Vertex responses is the following:
<a href=""https://i.stack.imgur.com/5AztD.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>I've tried to search the error <em>List of found errors: 1.Field: deployed_model.prediction_resources; Message: Required field is not set.</em> but found nothing. What could be the cause?</p>
<p>Version of tensorflow and python are the same of the pre-built environment given by Google, the model is saved through tf.keras.Model.save(model, model_path).</p>",1,3,2022-06-28 20:26:51.533000 UTC,0.0,,2,tensorflow|google-cloud-platform|gcloud|google-cloud-vertex-ai,235,2018-05-02 15:27:04.180000 UTC,2022-09-23 14:37:08.307000 UTC,,21,1,0,4,,,,,,['google-cloud-vertex-ai']
Error 400: Bad request in Amazon SageMaker Ground Truth text Labeling task,"<p>I am trying to use Amazon AWS to annotate my text data. It's a csv of 10 rows include header : &quot;orgiginalText, replyText&quot; and text data. I put my data in s3 bucker, create IAM with S3, sageMaker FullAccees. When I want to 'Create labeling job', it gave me error 400 Badrequest to connect to S3. is there anything else to be considered? I stucked 2 days in this small task and can't go forward.</p>",1,0,2022-02-21 15:36:41.307000 UTC,,,1,amazon-web-services|amazon-sagemaker|labeling,183,2015-07-11 06:11:42.337000 UTC,2022-04-21 09:13:10.720000 UTC,,21,0,0,1,,,,,,['amazon-sagemaker']
Action on error in Azure Machine Learning pipeline,"<p>I have a published and scheduled pipeline running at regular intervals. Some times, the pipeline may fail (for example if the datastore is offline for maintenance). Is there a way to specify the scheduled pipeline to perform a certain action if the pipeline fails for any reason? Actions could be to send me an email, try to run again in a few hours later or invoke a webhook. As it is now, I have to manually check the status of our production pipeline at regular intervals, and this is sub-optimal for obvious reasons. I could of course instruct every script in my pipeline to perform certain actions if they fail for whatever reason, but it would be cleaner and easier to specify it globally for the pipeline schedule (or the pipeline itself).</p>
<p>Possible sub-optimal solutions could be:</p>
<ul>
<li>Setting up an Azure Logic App to invoke the pipeline</li>
<li>Setting a cron job or Azure Scheduler</li>
<li>Setting up a second Azure Machine Learning pipeline on a schedule that triggers the pipeline, monitors the output and performs relevant actions if errors are encountered</li>
</ul>
<p>All the solutions above suffers from being convoluted and not very clean - surely there must exist a simple, clean solution for this problem?</p>",1,0,2020-12-10 09:45:31.447000 UTC,,,1,azure|azure-machine-learning-studio|azure-machine-learning-service,357,2011-06-07 10:27:10.757000 UTC,2022-09-22 07:49:12.447000 UTC,,2042,80,0,80,,,,,,"['azure-machine-learning-studio', 'azure-machine-learning-service']"
Loading S3File in AWS,"<p>I'm trying to download my cifar 10 data that is in S3 to train it in AWS SageMaker.</p>

<p>I'm using this code to load the data: </p>

<pre><code>import s3fs
fs = s3fs.S3FileSystem()

def unpickle(file):
    dict = pickle.load(file, encoding='bytes')
    return dict

with fs.open(f's3://bucket_name/data_batch_1') as f:
    data= unpickle(f)
</code></pre>

<p>I'm getting the error ""EOFError: Ran out of input"" on the unpickle function. I assume the ""file"" is empty, but I tried different ways to get the data from my bucket, and can't seem to get it right. </p>",1,4,2020-01-23 18:24:27.510000 UTC,,,0,python|amazon-web-services|amazon-s3|amazon-sagemaker,167,2019-03-21 04:58:34.443000 UTC,2022-08-19 11:09:30.953000 UTC,Japan,143,37,0,54,,,,,,['amazon-sagemaker']
"How do you resolve an ""Access Denied"" error when invoking `image_uris.retrieve()` in AWS Sagemaker JumpStart?","<p>I am working in a SageMaker environment that is locked down. For example, my user account is prevented from creating S3 buckets. But, I can successfully run vanilla ML training jobs by passing in <code>role=get_execution_role</code> to an instance of the Estimator class when using an out-of-the-box algorithm such as XGBoost.</p>
<p>Now, I'm trying to use an algorithm (LightBGM) that is only available via the JumpStart feature in SageMaker, but I can't get it to work. When I try to retrieve an image URI via <code>image_uris.retrieve()</code>, it returns the following error:<br />
<code>ClientError: An error occurred (AccessDenied) when calling the GetObject operation: Access Denied</code>.</p>
<p>This makes some sense to me if my user permissions are being used when creating an object. But what I want to do is specify another role - like the one returned from get_execution_role - to perform these tasks.</p>
<p>Is that possible? Is there another work-around available? How can I see which role is being used?</p>
<p>Thanks,</p>",0,1,2022-06-23 16:46:17.660000 UTC,,,1,amazon-sagemaker|amazon-machine-learning,47,2011-09-25 03:16:54.520000 UTC,2022-09-22 19:27:16.913000 UTC,,21,0,0,1,,,,,,['amazon-sagemaker']
Not all data Stores (workspaceblobstore) specified in the run configuration exist,"<p>I am submitting a run to an AML workspace programmatically, and it is failing with the error message:</p>

<pre><code>""error"": {
""code"": ""UserError"",
            ""message"": ""Not all data Stores (workspaceblobstore) specified in the run configuration exist."" }

</code></pre>

<p>Inspecting the run configuration object, the <code>sourceDirectoryDataStore</code> is set to null, and the other two data-related properties are empty.  </p>

<pre><code>""dataReferences"": {},
""data"": {},
""sourceDirectoryDataStore"": null 
</code></pre>

<p>The script that I am submitting does not make use of any datastores registered through aml, it is just the simple diabetes regression, using the <code>sklearn</code> diabetes dataset. 
The training script is copied to the <code>snapshots</code> container of the storage account linked with the AML workspace.  </p>

<p>What would be the next steps on troubleshooting this?</p>

<p>SDK Version: 1.0.85. </p>

<p>When retrieving the datastores, through ws.datastores (suggested in the comments), I get another exception about the storage service missing name or key, but there is a storage account that got deployed with the AML workspace. Inspecting the ARM template of the AML workspace the storage account id is in the properties of the AML template, and the usual containers (revisions, snapshots, snapshotzips, azureml-bloblstore-GUID) are created and the *.py files that I am attempting to run are being uploaded. </p>

<pre><code>    print(ws.datastores)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\azureml\core\workspace.py"", line 789, in datastores
    return {datastore.name: datastore for datastore in _DatastoreClient.list(self)}
  File ""C:\ProgramData\Anaconda3\lib\site-packages\azureml\data\datastore_client.py"", line 486, in list
    dss, ct = _DatastoreClient._list(workspace, ct, 100)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\azureml\data\datastore_client.py"", line 688, in _list
    return list(datastores), datastore_dtos.continuation_token
  File ""C:\ProgramData\Anaconda3\lib\site-packages\azureml\data\datastore_client.py"", line 687, in &lt;lambda&gt;
    map(lambda dto: _DatastoreClient._dto_to_datastore(ws, dto), datastore_dtos.value))
  File ""C:\ProgramData\Anaconda3\lib\site-packages\azureml\data\datastore_client.py"", line 760, in _dto_to_datastore
    as_section.sas_token, as_section.account_key, as_section.protocol, as_section.endpoint)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\azureml\data\azure_storage_datastore.py"", line 390, in __init__
    endpoint_suffix=endpoint
  File ""C:\ProgramData\Anaconda3\lib\site-packages\azureml\_vendor\azure_storage\file\fileservice.py"", line 184, in __init__
    raise ValueError(_ERROR_STORAGE_MISSING_INFO)
ValueError: You need to provide an account name and either an account_key or sas_token when creating a storage service.
</code></pre>",2,2,2020-01-27 20:08:34.563000 UTC,,2020-01-28 17:01:55.747000 UTC,2,azure-machine-learning-service,717,2012-08-02 23:01:34.333000 UTC,2022-09-02 23:21:30.913000 UTC,"Seattle, WA, USA",1390,122,2,121,,,,,,['azure-machine-learning-service']
How to deploy model trained with SparkR in sagemaker service?,"<p>I have created a training model with k-means and ALG algorithm in sparkR language.
I wanted to deploy the model throw AWS Sagemaker service.
I have ran some inbuilt examples, which uses conda_python3 language, but how it is possible with SparkR .?</p>",1,0,2018-01-22 04:41:33.847000 UTC,,,0,sparkr|amazon-sagemaker,378,2012-05-23 07:46:54.690000 UTC,2018-07-30 12:44:23.377000 UTC,Pune India,1036,34,1,124,,,,,,['amazon-sagemaker']
change run display name azureml,"<p>same problem as this however using script run config  <a href=""https://stackoverflow.com/questions/69857591/how-to-set-azure-experiment-name-from-the-code-after-2021-08-18-sdk-change"">How to set azure experiment name from the code after 2021-08-18 SDK change?</a>. Tried all the solutions however did not work</p>
<pre><code>src = ScriptRunConfig(
            source_directory=&quot;.&quot;,
            script='train.py',
            arguments=training_params,
            compute_target=compute_name,
            environment = conda_env
            )
        #DataReference for underlying RunConfiguration object
        src.run_config.data_references = {Data_Refrence.data_reference_name: Data_Refrence.to_config()}

try:
    run = exp.submit(config=src)
    run.display_name = &quot;Test&quot;
</code></pre>",1,0,2022-08-03 13:05:55.680000 UTC,,,0,python|azure-machine-learning-service|azureml-python-sdk,72,2020-11-30 17:06:44.663000 UTC,2022-08-31 08:48:49.383000 UTC,,49,9,0,33,,,,,,['azure-machine-learning-service']
Optionally use component functions added in VertexAI python SDK,"<p>I am using vertex ai's python SDK and it's built on top of Kubeflow pipelines. In it, you supposedly can do this:</p>
<pre><code>train_op = (sklearn_classification_train(
        train_data = data_op.outputs['train_out']
    ).
    set_cpu_limit(training_cpu_limit).
    set_memory_limit(training_memory_limit).
    add_node_selector_constraint(training_node_selector).
    set_gpu_limit(training_gpu_limit)
)
</code></pre>
<p>where you can add these functions (<code>set_cpu_limit</code>, <code>set_memory_limit</code>, <code>add_node_selector</code>, and <code>set_gpu_limit</code>) onto your component. I've haven't used this syntax before.</p>
<p>How I can optionally use each 'sub function' only if the variables are specified each function?</p>
<p>For example, if <code>training_gpu_limit</code> isn't set, I don't want to execute <code>set_gpu_limit</code> on the component.</p>",1,0,2021-12-23 15:20:12.233000 UTC,,,0,python-3.x|kubeflow-pipelines|google-cloud-vertex-ai,51,2014-11-26 14:46:22.680000 UTC,2022-09-23 13:33:34.890000 UTC,"Boston, MA",1256,391,2,245,,,,,,['google-cloud-vertex-ai']
AWS Sagemaker Lifecycle Configuration,"<p>I am working with AWS Sagemaker Notebook, now every time I start the notebook I should install packages I am working with like Librosa ( this one takes forever to be installed) so I look for a way to keep my installed packages in the notebook instance there where I found lifecycle configuration for instance and I have try every possiblity I found in net without any success always give long time error, please if some have a real good working solution help me, and thanks.</p>",0,3,2020-05-22 11:28:43.840000 UTC,,,0,pip|jupyter-notebook|data-science|lifecycle|amazon-sagemaker,240,2017-04-02 19:11:15.130000 UTC,2022-09-23 17:01:57.790000 UTC,"Meknes, Morocco",1181,58,40,281,,,,,,['amazon-sagemaker']
Azure Machine Learning Experiment Canceled after 10 hours of running,"<p>I'm working on a deep learning project and I'm using pipelines with an ""Execute python script"" to run my computations on a private training cluster (<strong>STANDARD_NC12</strong>), which is running on the Enterprise Edition.</p>

<p>It is running correctly, but after ten hours the run is canceled, without an error message or any indication of what happened.</p>

<p>Note that the job status is <em>Canceled</em> not <em>Failed</em>. </p>

<p>What could cause this? I didn't cancel the job.</p>",1,0,2020-04-02 14:22:26.653000 UTC,,2020-04-03 00:12:44.163000 UTC,0,azure|machine-learning|azure-pipelines|azure-machine-learning-service,246,2020-04-02 14:01:52.877000 UTC,2021-01-18 18:05:19.273000 UTC,"Casablanca, Maroc",11,0,0,1,,,,,,['azure-machine-learning-service']
Experiments disappear when adding --backend-store-uri,"<p>I have an EC2 instance running a mlflow server using the following command:</p>
<pre><code>mlflow server -h 0.0.0.0 --default-artifact-root s3://xxxx
</code></pre>
<p>After running multiple experiments, I was trying to register the best one. However, when trying to register or accessing the tab &quot;Models&quot;, I get the following error:</p>
<blockquote>
<p>INVALID_PARAMETER_VALUE: Model registry functionality is unavailable; got unsupported URI './mlruns' for model registry data storage. Supported URI schemes are: ['postgresql', 'mysql', 'sqlite', 'mssql']. See <a href=""https://www.mlflow.org/docs/latest/tracking.html#storage"" rel=""nofollow noreferrer"">https://www.mlflow.org/docs/latest/tracking.html#storage</a> for how to run an MLflow server against one of the supported backend storage locations.</p>
</blockquote>
<p><a href=""https://stackoverflow.com/questions/63255631/mlflow-invalid-parameter-value-unsupported-uri-mlruns-for-model-registry-s"">This SO answer</a> suggested adding a <code>backend-store-uri</code>:</p>
<pre><code>mlflow server -h 0.0.0.0 --default-artifact-root --backend-store-uri sqlite:///mlflow.db
</code></pre>
<p>That solved the above issue, however, now all experiments are gone. The Experiments tab is blank. Is there a way to add a <code>backend-store-uri</code> after running multiple experiments while keeping all of them?</p>",0,0,2022-05-07 06:49:14.493000 UTC,,,2,mlflow,208,2014-03-15 16:29:47.340000 UTC,2022-09-23 14:38:41.817000 UTC,"Curitiba, State of Paraná, Brazil",1490,99,8,346,,,,,,['mlflow']
Cannot debug code with Azure ML workspace + VS Code,"<p>I am trying to remotely debug Python code in an Azure ML workspace using VS Code 1.64.2. I have Azure ML extension installed in VS Code.</p>
<p>I can connect to Azure ML workspace and most of the features work ok. I'd like to start remote debugging following a tutorial on youtube. However I cannot do it because when I right-click a python file, there is no <code>Azure ML: Run as Experiment in Azure</code> menu which I can see in the video.</p>
<p>What am I doing wrong?</p>
<p><a href=""https://i.stack.imgur.com/BYfcQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BYfcQ.png"" alt=""enter image description here"" /></a></p>",1,0,2022-03-11 16:04:11.957000 UTC,,2022-03-12 08:07:20.037000 UTC,0,azure|visual-studio-code|azure-machine-learning-studio|azure-machine-learning-service,142,2013-10-03 09:39:21.493000 UTC,2022-09-23 14:17:28.327000 UTC,,2522,1691,15,274,,,,,,"['azure-machine-learning-studio', 'azure-machine-learning-service']"
wandb : move runs from a blocked entity,"<p>I accidentally moved several runs from my own user account to a team entity.
Unfortunatly, this team entity had a restriction on the quantity of experiments tracked, and it now appears as blocked. I have this error message :</p>
<blockquote>
<p>Your organization is over the limit of 250 tracked hours. Please upgrade your plan to keep using W&amp;B.</p>
</blockquote>
<p>I can't find a way to move back these runs to my free account, does anyone know how to do this ?
Thanks</p>",1,0,2022-07-21 14:38:38.473000 UTC,,,0,wandb,19,2020-08-26 07:08:24.360000 UTC,2022-07-25 11:43:58.907000 UTC,"Toulouse, France",11,0,0,2,,,,,,['wandb']
"sagmaker deploy() model gives error exec: ""serve"": executable file not found in $PATH","<p>I have fitted a model succefully in Sagemaker with <code>classifier.fit()</code>, I have got success message saying that <code>Finished saving the model.Finished training the model. Script Status - Finished</code></p>
<p>Now, I tried to deploy the model (make endpoint with the following code.</p>
<pre><code>classifier.deploy(initial_instance_count=1, instance_type='ml.m4.4xlarge')
</code></pre>
<p>It initially gave me error.</p>
<blockquote>
<p>Error hosting endpoint
keras-seq-modelling2021-11-2021-01-07-11-26-56-705: Failed. Reason:
The primary container for production variant AllTraffic did not pass
the ping health check. Please check CloudWatch logs for this
endpoint..</p>
</blockquote>
<p>When I looked in logs (cloud watch log stream), It is giving following error.</p>
<blockquote>
<p>exec: &quot;serve&quot;: executable file not found in $PATH</p>
</blockquote>
<p>I know it is relating to making some file executable with granting right permision in some docker container, but the point is that it should be handled by sagemaker and all we need to do according to tutorial is <code>model.deploy()</code></p>
<p>Am I doing something wrong. Any idea how to resolved this.</p>",0,4,2021-01-07 11:54:15.803000 UTC,,,3,python|amazon-web-services|machine-learning|amazon-sagemaker,237,2014-05-27 17:36:45.023000 UTC,2022-09-24 22:00:51.297000 UTC,"London, Uk",19335,345,77,1053,,,,,,['amazon-sagemaker']
Convert `String Feature` DataFrame into Float in Azure ML Using Python Script,"<p>I am trying to understand how to convert azure ml <code>String Feature</code> data type into float using python script. my data set is contain ""HH:MM"" data time format. It recognized as <code>String Feature</code> like the following img:</p>

<p><a href=""https://i.stack.imgur.com/gy4A7.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gy4A7.jpg"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/aB4P6.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/aB4P6.jpg"" alt=""enter image description here""></a></p>

<p>I want to convert it into float type which will divide the timestamp by  84600 ( 24 hour) so <code>17:30</code> will be converted into <code>0,729166666666667</code>, so I write python script to convert that. This is my script:</p>

<pre><code>import pandas as pd
import numpy as np 

def timeToFloat(x):
    frt = [3600,60]
    data = str(x)
    result = float(sum([a*b for a,b in zip(frt, map(int,data.split(':')))]))/86400
    return result if isNotZero(x) else 0.0

def isNotZero(x):
    return (x is ""0"")

def azureml_main(dataframe1 = None):

    df = pd.DataFrame(dataframe1)
    df[""Departure Time""] = pd.to_numeric(df[""Departure Time""]).apply(timeToFloat)

    print(df[""Departure Time""])

    return df,
</code></pre>

<p>When I run the script it was failed. Then I try to check whether it is <code>str</code> or not, but it returns <code>None</code>.</p>

<p>can we treat <code>String Feature</code> as <code>String</code>? or how should I covert this data correctly?</p>",1,0,2018-06-21 02:36:08.263000 UTC,,,0,python|pandas|csv|azure-machine-learning-studio,299,2015-10-31 00:21:47.867000 UTC,2022-09-23 03:18:20.793000 UTC,"Depok, Depok City, West Java, Indonesia",123,5,0,11,,,,,,['azure-machine-learning-studio']
Pip installations on Colab from local,"<p>I'd like to use wandb on Colab, and I've installed it through pip on the command line. However, the import isn't recognized on Colab, so I have to run <code>!pip install wandb</code> each time.</p>
<p>How can I install <code>wandb</code> locally so that I don't have to install it on the Colab notebook each time?</p>",0,4,2022-02-20 03:36:27.297000 UTC,,,0,python|pip|google-colaboratory|wandb,516,2019-05-19 03:48:33.157000 UTC,2022-09-23 15:43:15.227000 UTC,,421,20,5,33,,,,,,['wandb']
How to build and install dlib for python without GUI support?,"<h3>Background:</h3>
<p>I'm running a ‍<code>jupyter‍</code> notebook on an AWS <code>sagemaker</code> ec2 instance (Which uses Fedora Linux) and one of my requirements is <code>dlib</code>.
However, <code>dlib</code> (by default) uses <code>xorg's x11</code> libs for GUI support, and these are not installed on the <code>sagemaker</code> instance. I do not need the GUI support, and <code>sagemaker</code> does not support yum installs, so I'm trying to build it without them.</p>
<p>I've cloned <code>dlib</code>'s github repo and have attempted to build with <code>python setup.py</code>, where it throws x11 errors. I've read through the website and it says that <a href=""http://dlib.net/compile.html"" rel=""nofollow noreferrer"">you can define the <code>DLIB_NO_GUI_SUPPORT</code> preprocessor directive to compile without GUI support</a>. Sounds great! I read through the setup.py file and see that I can add that by running <code>python setup.py x DLIB_NO_GUI_SUPPORT</code> where <code>x</code> is one of [<code>--no</code>, <code>--set</code>, <code>--compiler-flags</code>, and <code>-G</code>].
I don't know which one, so I try them all:</p>
<ul>
<li><code>python setup.py --no DLIB_NO_GUI_SUPPORT</code></li>
<li><code>python setup.py --set DLIB_NO_GUI_SUPPORT</code></li>
<li><code>python setup.py --compiler-flags DLIB_NO_GUI_SUPPORT</code></li>
<li><code>python setup.py -G DLIB_NO_GUI_SUPPORT</code></li>
</ul>
<p>None of them worked.</p>
<h3>Question:</h3>
<p>What is the correct syntax for setting the <code>DLIB_NO_GUI_SUPPORT</code> preprocessor directive for <code>dlib</code> using <code>setup.py</code>?</p>",1,0,2020-11-03 00:39:17.200000 UTC,,2020-11-03 21:41:27.207000 UTC,4,python|setup.py|amazon-sagemaker|dlib,365,2017-09-06 01:36:29.123000 UTC,2022-06-07 14:10:55.690000 UTC,"KY, United States",41,2,0,3,,,,,,['amazon-sagemaker']
"Error: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!""","<p>I'm running this code on Azure machine learning notebook.</p>
<pre><code>import os
import torch
import gradio as gr
from vilmedic import AutoModel
from vilmedic.blocks.scorers import RadGraph
import glob

model, processor = AutoModel.from_pretrained(&quot;rrg/baseline-mimic&quot;)
device = torch.device(&quot;cuda&quot;) if torch.cuda.is_available() else torch.device(&quot;cpu&quot;)
model = model.to(device)
radgraph = RadGraph(cuda=-1)
</code></pre>
<p>.....</p>
<pre><code>def run(image, beam_size, num_return_sequences, include_words, exclude_words, do_radgraph):
    if image is None:
        return {}, '&lt;b&gt;Please select an image&lt;/b&gt;'  # , &quot;&quot;
    if num_return_sequences &gt; beam_size:
        return {}, '&lt;b&gt;&quot;Beam size&quot;&lt;/b&gt; must be greater or equal than &lt;b&gt;&quot;Number of generated reports&quot;&lt;/b&gt;'  # , &quot;&quot;

    try:
        include_words_ids, include_words = get_token_from_strings(include_words)
        exclude_words_ids, exclude_words = get_token_from_strings(exclude_words)

        if include_words_ids is not None and [3] in include_words_ids:
            return {}, '&lt;b&gt;&quot;' + include_words[
                include_words_ids.index([3])] + '&quot;&lt;/b&gt; is not in the vocabulary&quot;&lt;/b&gt;'  # , &quot;&quot;

        with torch.no_grad():
            batch = processor.inference(image=[
                [image]
            ])
            batch_size = 1
            encoder_output, encoder_attention_mask = model.encode(**batch)
            expanded_idx = torch.arange(batch_size).view(-1, 1).repeat(1, beam_size).view(-1)
            input_ids = torch.ones((len(batch[&quot;images&quot;]), 1), dtype=torch.long)
            if torch.cuda.is_available():
                expanded_idx = expanded_idx.cuda()
                print(&quot;89 line&quot;)
                input_ids = input_ids.cuda()

            # Using huggingface generate method
            hyps = model.dec.generate(
                input_ids=input_ids * model.dec.config.bos_token_id,
                encoder_hidden_states=encoder_output.index_select(0, expanded_idx),
                encoder_attention_mask=encoder_attention_mask.index_select(0, expanded_idx),
                num_return_sequences=num_return_sequences,
                max_length=processor.tokenizer_max_len,
                num_beams=beam_size,
                bad_words_ids=exclude_words_ids,
                force_words_ids=include_words_ids,
            )

            # Decode
            hyps = [processor.tokenizer.decode(h, skip_special_tokens=True, clean_up_tokenization_spaces=False) for h in
                    hyps]

            # RadGraph
            if do_radgraph:
                radgraph_annots = [radgraph(hyps=[h], refs=[h])[-1][0][&quot;entities&quot;] for h in hyps]
                # Find entites : Radgraph
                new_hyp_strs = []
                for hyp_str, radgraph_annot in zip(hyps, radgraph_annots):
                    values = radgraph_annot.values()
                    new_hyp_str = hyp_str.split()
                    for v in values:
                        new_hyp_str[v[&quot;start_ix&quot;]] = highlight_radgraph_entities(v[&quot;tokens&quot;], v[&quot;label&quot;])
                    new_hyp_strs.append(' '.join(new_hyp_str))
            else:
                new_hyp_strs = hyps

            # Find user entites
            if include_words is not None:
                for w in include_words:
                    new_hyp_strs = [h.replace(w, highlight_word(w, &quot;user&quot;)) for h in new_hyp_strs]

            # Formating
            new_hyp_strs = [&quot;&lt;p&gt;&lt;b&gt;Hypothesis {}:&lt;/b&gt; &lt;br/&gt; {} &lt;/p&gt;&quot; \
                            &quot;&quot;.format(i + 1, h) for i, h in enumerate(new_hyp_strs)] + (
                               [&quot;&lt;br/&gt;&lt;br/&gt;&lt;i&gt;Anat: anatomy&lt;br/&gt;&quot;
                                &quot;OBS: observation&lt;br/&gt;&quot;
                                &quot;DA: definitely absent&lt;br/&gt;&quot;
                                &quot;DP: definitely present&lt;/i&gt;&quot;] if do_radgraph else [&quot;&quot;])

            # Params
            out_json = {
                &quot;beam size&quot;: beam_size, &quot;number of generated reports&quot;: num_return_sequences,
                &quot;included words&quot;: include_words, &quot;excluded words&quot;: exclude_words, &quot;show radgraph&quot;: do_radgraph
            }

            return out_json, str(''.join(new_hyp_strs))  # , str(refs[os.path.basename(image)])

    except Exception as e:
        print(e)
        return {}, &quot;&lt;b&gt;An error occured, try again...&quot;

</code></pre>
<p>The full code is here:</p>
<p><a href=""https://huggingface.co/spaces/StanfordAIMI/radiology_report_generation/blob/main/app.py"" rel=""nofollow noreferrer"">https://huggingface.co/spaces/StanfordAIMI/radiology_report_generation/blob/main/app.py</a></p>
<p>It keeps giving me this error when I upload an image and click the submit button:
<strong>Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!</strong>.
I'm using an azure computation with following specifications:
Virtual machine size: <em>Standard_NV6 (6 cores, 56 GB RAM, 380 GB disk)</em>
Processing unit: <em>GPU - 1 x NVIDIA Tesla M60</em></p>",0,1,2022-08-19 08:19:52.447000 UTC,,2022-08-19 10:44:10.610000 UTC,0,pytorch|azure-machine-learning-studio|huggingface,147,2022-08-05 06:57:03.830000 UTC,2022-09-22 19:04:26.410000 UTC,,1,0,0,1,,,,,,['azure-machine-learning-studio']
Install and load Tidymodels package in AML,"<p>I'm trying install and load some R packages in the Execute R Script in Azure Machine Learning for to run models, such as <strong>tidymodels, timetk, modeltime, modeltime.ensemble</strong>.</p>
<pre><code>library(forecast)
library(tidyverse)
library(lubridate)
install.packages(&quot;quantdates&quot;,repos = &quot;https://cloud.r-project.org&quot;)
install.packages(&quot;tidymodels&quot;,repos = &quot;https://cloud.r-project.org&quot;)
library(quantdates)
library(tidymodels) 
library(timetk) 
library(modeltime) 
library(modeltime.resample) 
library(modeltime.ensemble)
</code></pre>
<p>However I get the following error:</p>
<pre><code>Error: package or namespace load failed for ‘tidymodels’ in loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI[[i]]):
namespace ‘rlang’ 0.4.5 is already loaded, but &gt;= 1.0.2 is required

azureml_main(input_dataframe_1), library(tidymodels), tryCatch({
    attr(package, &quot;LibPath&quot;) &lt;- which.lib.loc
    ns &lt;- loadNamespace(package, lib.loc)
    env &lt;- attachNamespace(ns, pos = pos, deps)
}, error = function(e) {
    P &lt;- if (!is.null(cc &lt;- conditionCall(e))) 
        paste(&quot; in&quot;, deparse(cc)[1])
    else &quot;&quot;
    msg &lt;- gettextf(&quot;package or namespace load failed for %s%s:\n %s&quot;, sQuote(package), P, conditionMessage(e))
    if (logical.return) 
        message(paste(&quot;Error:&quot;, msg), domain = NA)
    else stop(msg, call. = FALSE, domain = NA)
}), tryCatchList(expr, classes, parentenv, handlers), tryCatchOne(expr, names, parentenv, handlers[[1]]), value[[3]](cond), stop(msg, call. = FALSE, domain = NA), .handleSimpleError(function (e) 
{
    error_msg &lt;&lt;- paste(toString(e), toString(sys.calls()[-c(1:3)]), sep = &quot;\n&quot;)
    stop(e)
}, &quot;package or namespace load failed for ‘tidymodels’ in loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI[[i]]):\n namespace ‘rlang’ 0.4.5 is already loaded, but &gt;= 1.0.2 is required&quot;, quote(NULL)), h(simpleError(msg, call))
'.
---------- End of error message from R  interpreter  ----------
</code></pre>
<p>I have also tried with devtools package for install a particular version but I keep getting the same error with the <strong>rlang package</strong>. Sometimes, I get the same error with the <strong>cli package</strong>.</p>
<p>In my local machine, the R code runs fine. I have the R version 4.1.3 and the Azure Machine Learning has the R version 3.5.1.</p>
<p>Does anyone know how I can solve this problem?</p>",1,1,2022-05-25 14:41:58.947000 UTC,,,0,r|azure|r-package|azure-machine-learning-service|tidymodels,80,2022-04-22 20:44:26.457000 UTC,2022-09-22 14:37:18.330000 UTC,,1,0,0,1,,,,,,['azure-machine-learning-service']
Is it possible to run Vertex AI Workbench on Spot machines?,"<p>I'm trying to save budget on jupyter notebooks on Google Cloud but couldn't find a way to run Vertex AI Workbench (Notebooks) on spot machines.
What are my alternatives?</p>",1,0,2022-06-28 09:25:32.770000 UTC,1.0,,2,google-cloud-platform|jupyter-notebook|jupyter|google-cloud-vertex-ai,176,2011-11-25 20:39:39.120000 UTC,2022-08-22 07:33:27.010000 UTC,,73,14,0,14,,,,,,['google-cloud-vertex-ai']
Registering and getting an environment in Azure Machine Learning Studio that derives from a self-created Docker image,"<p>I need to register an environment in Azure Machine Learning Studio which derives from a self-created Docker image. I have found this <a href=""https://docs.microsoft.com/en-us/azure/machine-learning/how-to-train-with-custom-image#use-a-private-container-registry-optional"" rel=""nofollow noreferrer"">documentation</a> by Microsoft that describes the process of using a custom Docker image as an environment.</p>
<p>Unfortunately, the documentation does not explain how to register the resulting environment into the workspace in order to get the environment when needed.</p>
<p>Normally, you describe and register an environment like this:</p>
<pre><code>from azureml.core import Environment
from azureml.core.conda_dependencies import CondaDependencies

# Create a Python environment for the experiment
diabetes_env = Environment(&quot;diabetes-experiment-env&quot;)
diabetes_env.python.user_managed_dependencies = False # Let Azure ML manage dependencies
diabetes_env.docker.enabled = True # Use a docker container

# Create a set of package dependencies (conda or pip as required)
diabetes_packages = CondaDependencies.create(conda_packages=['scikit-learn','ipykernel','matplotlib','pandas','pip'],
                                             pip_packages=['azureml-sdk','pyarrow'])

# Add the dependencies to the environment
diabetes_env.python.conda_dependencies = diabetes_packages

# Register the environment
diabetes_env.register(workspace=ws)
</code></pre>
<p>Then, when you need the defined environment, you can get it with:</p>
<pre><code># get the registered environment
registered_env = Environment.get(ws, 'diabetes-experiment-env')
</code></pre>
<p><a href=""https://github.com/MicrosoftLearning/DP100/blob/master/05A%20-%20Working%20with%20Environments.ipynb"" rel=""nofollow noreferrer"">Source</a></p>
<p>As it seems to me, this approach does not work for environments that derive from self-created Docker images.</p>
<p>For example, I define and register an environment like this:</p>
<pre><code>from azureml.core import Environment
from azureml.core import ContainerRegistry

registry = ContainerRegistry()
registry.address = &quot;name_of_container_registry.azurecr.io&quot;
registry.username = &quot;username&quot;
registry.password = &quot;password&quot;
myenv = Environment.from_docker_image('env_name', 'name_of_container_registry.azurecr.io/docker_image_name:latest', container_registry=registry, conda_specification=None, pip_requirements=None)
myenv.register(workspace=ws)
</code></pre>
<p>Then, in the script where I need to have the above-defined environment:</p>
<pre><code>from azureml.core.model import Model
from azureml.core import Environment
from azureml.core.model import InferenceConfig
from azureml.core.webservice import LocalWebservice

model = Model(ws, 'exemplarily_model')

registered_env = Environment.get(ws, 'env_name')

inference_config = InferenceConfig(environment=registered_env, 
                                   source_directory='./source_dir', 
                                   entry_script='./score.py') 

deployment_config = LocalWebservice.deploy_configuration(port=6789)

service = Model.deploy(
    ws,
    &quot;myservice&quot;,
    [model],
    inference_config,
    deployment_config,
    overwrite=True,
)

service.wait_for_deployment(show_output=True)
print(service.get_logs())
</code></pre>
<p>Everytime the inference service deployment fails with the exception &quot;Authentication failed for container registry name_of_container_registry.azurecr.io&quot;. However, when I change <code>environment=registered_env</code> to <code>environment=myenv</code>, the inference service starts successfully without any error.</p>
<p>This brings me to the conclusion that registering and getting an environment in Azure Machine Learning Studio that derives from a self-created Docker image works differently. <br>
<br>
Therefore, I would like to ask for the correct commands for registering and getting an environment in Azure Machine Learning Studio that derives from a self-created Docker image.</p>
<p><strong>Update:</strong> <br>
I found a possibility to get the environments that derives from a self-created Docker image:</p>
<p>Instead of registering the environment with the command <code>myenv.register(workspace=ws)</code>, you can save the environment to a directory with the command <code>myenv.save_to_directory(path=&quot;./env&quot;, overwrite=True)</code>.
Then you can load the environment in a different script with <code>newenv = Environment.load_from_directory(path=&quot;./env&quot;)</code>.
This approach works for me to deploy my inference service.
I found the possibility for getting a saved environment here: <a href=""https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-environments#use-existing-environments"" rel=""nofollow noreferrer"">Link</a></p>",2,0,2022-02-15 18:12:53.413000 UTC,,2022-02-16 07:58:25.760000 UTC,1,azure|environment|azure-machine-learning-studio|docker-image|azure-machine-learning-service,888,2021-09-29 07:06:53.153000 UTC,2022-09-23 08:06:23.973000 UTC,,139,6,0,12,,,,,,"['azure-machine-learning-service', 'azure-machine-learning-studio']"
How to automatically start and stop a compute instance to execute Azure Machine Learning pipelines,"<p>I've been following <a href=""https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-schedule-for-a-published-pipeline.ipynb"" rel=""nofollow noreferrer"">this notebook</a> to schedule the execution of some code every hour. It makes a schedule and creates the pipelines. The problem is that if the computer instance is not running, the pipeline is just queued and waiting for it to run.</p>
<p>Is there a way to automatically run a compute instance when a pipeline is triggered within Azure Machine Learning, as well as stop them when the pipeline is completed?</p>
<p>I need to do it within the AzureML Studio platform because that is the only thing external Data Scientists have access to. I can't use clusters because some of their behaviors cause issues with the code.</p>
<p>I can schedule an instance to become active approximately at the same time as the schedule, but I want to do it in the code, so the scripts can run when they are up, as well as shut down the instance when the pipeline run is over.</p>",1,1,2022-01-27 13:38:58.653000 UTC,1.0,,2,azure|azure-devops|azure-machine-learning-studio|azure-machine-learning-service,820,2022-01-27 13:22:55.790000 UTC,2022-09-22 11:17:49.317000 UTC,"Berlin, Germany",21,0,0,2,,,,,,"['azure-machine-learning-studio', 'azure-machine-learning-service']"
h2o models on Azure ML Containers,"<p>I have a requirement to deploy h2o models on Azure . I have successfully handled sklearn models but for sklearn the dependencies in my view are easier . For h2o the java runtime dependency is my bottle-neck.</p>
<p>Will the container that i create will have java runtime ?&gt; Else what are the suggested strategies ?</p>
<p>Should I go for a VM instead ?</p>
<p>Thanks,</p>",0,4,2020-06-29 10:43:50.617000 UTC,,,1,containers|h2o|azure-machine-learning-service,84,2014-04-08 11:57:06.127000 UTC,2022-09-24 17:34:44.423000 UTC,"Mumbai, India",169,12,0,33,,,,,,['azure-machine-learning-service']
wandb pytorch: top1 accuracy per class,"<p>I have 5 classes in validation set and i want to draw a graph based on top1 results per class in validation loop using wandb . I have tried a single accuracy graph based on the average of 5 classes and it works fine but i want to do a separate way like top1 accuracy for each class. I am unable to achieve, are there any way to achieve it?</p>
<p><strong>Validation Loader</strong></p>
<pre><code> val_loaders = []
    for nuisance in val_nuisances:
        val_loaders.append((nuisance, torch.utils.data.DataLoader(
            datasets.ImageFolder(os.path.join(valdir, nuisance), transforms.Compose([
                transforms.Resize(256),
                transforms.CenterCrop(224),
                transforms.ToTensor(),
                normalize,
            ])),
            batch_size=args.batch_size, shuffle=False,
            num_workers=args.workers, pin_memory=True,
        )))


val_nuisances = ['shape', 'pose', 'texture', 'context', 'weather']
</code></pre>
<p><strong>Validation Loop</strong></p>
<pre><code>def validate(val_loaders, model, criterion, args):
    overall_top1 = 0
    for nuisance, val_loader in val_loaders:
        batch_time = AverageMeter('Time', ':6.3f', Summary.NONE)
        losses = AverageMeter('Loss', ':.4e', Summary.NONE)
        top1 = AverageMeter('Acc@1', ':6.2f', Summary.AVERAGE)
        top5 = AverageMeter('Acc@5', ':6.2f', Summary.AVERAGE)
        progress = ProgressMeter(
            len(val_loader),
            [batch_time, losses, top1, top5],
            prefix=f'Test {nuisance}: ')

        # switch to evaluate mode
        model.eval()

        with torch.no_grad():
            end = time.time()
            for i, (images, target) in enumerate(val_loader):
                if args.gpu is not None:
                    images = images.cuda(args.gpu, non_blocking=True)
                if torch.cuda.is_available():
                    target = target.cuda(args.gpu, non_blocking=True)

                # compute output
                output = model(images)
                loss = criterion(output, target)

                # measure accuracy and record loss
                acc1, acc5 = accuracy(output, target, topk=(1, 5))
                losses.update(loss.item(), images.size(0))
                top1.update(acc1[0], images.size(0))
                top5.update(acc5[0], images.size(0))

                # measure elapsed time
                batch_time.update(time.time() - end)
                end = time.time()

                if i % args.print_freq == 0:
                    progress.display(i)

            progress.display_summary()
        overall_top1 += top1.avg
    overall_top1 /= len(val_loaders)
    return top1.avg
</code></pre>",1,0,2022-06-09 05:42:03.610000 UTC,,,0,pytorch|wandb,71,2015-01-05 16:22:18.783000 UTC,2022-09-23 07:10:40.323000 UTC,"Seoul, South Korea",2201,186,3,556,,,,,,['wandb']
"""No module named PIL"" after ""RUN pip3 install Pillow"" in docker container; neither PIL nor Pillow present in dist-packages directory","<p>I'm following this SageMaker guide and using the 1.12 cpu docker file.
<a href=""https://github.com/aws/sagemaker-tensorflow-serving-container"" rel=""nofollow noreferrer"">https://github.com/aws/sagemaker-tensorflow-serving-container</a></p>

<p>If I use the requirements.txt file to install Pillow, my container works great locally, but when I deploy to SageMaker, 'pip3 install' fails with an error indicating my container doesn't have internet access.</p>

<p>To work around that issue, I'm trying to install Pillow in my container before deploying to SageMaker.</p>

<p>When I include the lines ""RUN pip3 install Pillow"" and ""RUN pip3 show Pillow"" in my docker file, when building, I see output saying ""Successfully installed Pillow-6.2.0"" and the show command indicates the lib was installed at /usr/local/lib/python3.5/dist-packages.  Also running ""RUN ls /usr/local/lib/python3.5/dist-packages"" in the docker files shows ""PIL"" and ""Pillow-6.2.0.dist-info"" in dist-packages, and the PIL directory includes many code files.</p>

<p>However, when I run my container locally, trying to import in python using ""from PIL import Image"" results in error ""No module named PIL"".  I've tried variations like ""import Image"", but PIL doesn't seem to be installed in the context in which the code is running when I start the container.  </p>

<p>Before the line ""from PIL import Image"", I added ""import subprocess"" and 'print(subprocess.check_output(""ls /usr/local/lib/python3.5/dist-packages"".split()))'</p>

<p>This <em>ls</em> output matches what I get when running it in the docker file, except ""PIL"" and ""Pillow-6.2.0.dist-info"" are missing.  Why are those two in /usr/local/lib/python3.5/dist-packages when I run the docker file but not when my container is started locally?</p>

<p>Is there a better way to include Pillow in my container?  The referenced Github page also shows that I can deploy libraries by including the files (in code/lib of model package), but to get files compatible with Ubuntu 16.04 (which the docker container uses; I'm on a Mac), I'd probably copy them from the docker container after running ""RUN pip3 install Pillow"" in my docker file, and it seems odd that I would need to get files from the docker container to deploy to the docker container.</p>

<p>My docker file looks like:</p>

<pre><code>ARG TFS_VERSION

FROM tensorflow/serving:${TFS_VERSION} as tfs
FROM ubuntu:16.04
LABEL com.amazonaws.sagemaker.capabilities.accept-bind-to-port=true

COPY --from=tfs /usr/bin/tensorflow_model_server /usr/bin/tensorflow_model_server

# nginx + njs
RUN \
    apt-get update &amp;&amp; \
    apt-get -y install --no-install-recommends curl &amp;&amp; \
    curl -s http://nginx.org/keys/nginx_signing.key | apt-key add - &amp;&amp; \
    echo 'deb http://nginx.org/packages/ubuntu/ xenial nginx' &gt;&gt; /etc/apt/sources.list &amp;&amp; \
    apt-get update &amp;&amp; \
    apt-get -y install --no-install-recommends nginx nginx-module-njs python3 python3-pip python3-setuptools &amp;&amp; \
    apt-get clean

RUN pip3 install Pillow

# cython, falcon, gunicorn, tensorflow-serving
RUN \
    pip3 install --no-cache-dir cython falcon gunicorn gevent requests grpcio protobuf tensorflow &amp;&amp; \
    pip3 install --no-dependencies --no-cache-dir tensorflow-serving-api 

COPY ./ /

ARG TFS_SHORT_VERSION
ENV SAGEMAKER_TFS_VERSION ""${TFS_SHORT_VERSION}""
ENV PATH ""$PATH:/sagemaker""

RUN pip3 show Pillow
RUN ls /usr/local/lib/python3.5/dist-packages
</code></pre>

<p>I've tried installing Pillow on the same line as cython and other dependencies, but the result is the same...those dependencies are in /usr/local/lib/python3.5/dist-packages both at the time the container is built and when the container is started locally, while ""PIL"" and ""Pillow-6.2.0.dist-info"" are only present when the container is built.</p>",1,2,2019-10-06 19:05:14.627000 UTC,,,1,docker|python-imaging-library|amazon-sagemaker,663,2016-10-24 23:51:45.107000 UTC,2021-08-22 14:39:59.943000 UTC,,11,0,0,0,,,,,,['amazon-sagemaker']
private vnet: Git clone my azure repo from Azure ML compute instance is impossible,"<p>Since a few days, I try to clone my Azure Devops repo in my Azure ML compute instance but I can't do it.
In fact, I am in my private network with a private compute instance.</p>
<p>So obviously I have to create rules in my network security group to allow in/outbound to https 443.
However... It doesn't work. I tried everything, even the AzureDevops service tag with a multitude of ports.</p>
<p>My terminal after git clone https:</p>
<pre><code>Immediate connect fail for xxxx:1ec:21::20: Network is unreachable
Failed to connect to dev.azure.com port 443: Connection timed out
</code></pre>
<p>Does anyone have a solution please?</p>",1,0,2022-01-31 00:35:58.247000 UTC,,,0,azure|azure-devops|azure-virtual-network|azure-machine-learning-studio|azure-nsg,322,2018-11-08 19:30:23.717000 UTC,2022-05-01 14:44:35.850000 UTC,,13,0,0,2,,,,,,['azure-machine-learning-studio']
What are the various Run metrics that can be added in run in AzureML,<p>I have multiple stuff that i want to record while performing ML experiment in AzureML. what are the various objects that can be recorded.</p>,1,0,2020-12-05 12:33:18.433000 UTC,,,0,azure-machine-learning-service|azureml-python-sdk,1531,2017-01-19 06:38:11.853000 UTC,2022-09-20 02:56:14.233000 UTC,"Gurugram, Haryana, India",623,29,1,54,,,,,,['azure-machine-learning-service']
In sagemaker model explainability monitoring How to pass values for a shap_baseline if we have categorical valuesas features?,"<p>using <a href=""https://sagemaker.readthedocs.io/en/stable/api/training/processing.html#sagemaker.clarify.SHAPConfig"" rel=""nofollow noreferrer"">this documentation</a> I passing a single row as to shap_baseline parameter to implement explainability monitoring, a similar implementation of what is done in <a href=""https://github.com/aws/amazon-sagemaker-examples/blob/main/sagemaker_model_monitor/fairness_and_explainability/SageMaker-Model-Monitor-Fairness-and-Explainability.ipynb"" rel=""nofollow noreferrer"">in this GitHub repo implementation</a>. if I am passing a single row as input to shap_baseline parameter, the schedule is failing by concatenating 2 rows. If I ignore the shap_baseline (as it is optional), the schedule takes forever to run.</p>",0,0,2022-09-23 13:04:15.160000 UTC,,2022-09-25 05:09:06.127000 UTC,-1,amazon-web-services|machine-learning|amazon-sagemaker,31,2019-11-12 07:17:01.663000 UTC,2022-09-25 05:05:06.423000 UTC,,17,12,0,9,,,,,,['amazon-sagemaker']
"How to delete Feature Group from SageMaker Feature Store, by name","<p>The way to delete a feature group using the SageMaker Python SDK is as follows:</p>
<pre><code>my_feature_group.delete()
</code></pre>
<p>But this only deletes the feature group you are currently working on. How can one delete feature groups from prior sessions? I tried deleting them out of the S3 bucket directly, but they still appear in the Feature Store UI.</p>
<p>It would be great if feature groups could be deleted through the UI. But if not, is there a way to delete a feature group using it's full name; the one that was created using:</p>
<pre><code>my-feature-group-&quot; + strftime(&quot;%d-%H-%M-%S&quot;, gmtime())
</code></pre>",2,0,2021-07-12 20:24:24.447000 UTC,,,2,amazon-web-services|amazon-sagemaker|feature-store|aws-feature-store,901,2012-08-31 20:08:40.090000 UTC,2022-09-25 04:17:41.297000 UTC,,11650,6318,21,977,,,,,,['amazon-sagemaker']
aws sagemaker error on Create labelling job,"<p>I uploaded a csv file to S3, created a role with S3FullAccess and SageMakerFullAccess. While creating labelling job, I chose Automated data setup and chose the bucket that the csv had uploaded. Then Data type: text and chose the IAM Role. Then clicked &quot;Complete data set up&quot;
then I get below error message</p>
<pre><code>Connection error
There was an issue with your input data setup. Ground Truth could not setup a connection with your dataset in S3. Please check your input data setup and try again, or use the manual data setup option. Network Failure Request id:xxx-xxx-xxxx
</code></pre>
<p>No idea why this error message shows up... Please help!</p>",1,0,2021-10-05 08:58:30.520000 UTC,,,2,amazon-web-services|amazon-s3|amazon-iam|amazon-sagemaker|aws-policies,427,2021-07-28 05:01:28.540000 UTC,2022-04-13 23:05:53.167000 UTC,,87,0,0,5,,,,,,['amazon-sagemaker']
Azureml Training model fail with libgomp.so.1,"<p>I hope you can help me out with this issue as I cannot figure out what is wrong or how to solve it.
I am diving into MLOps to have a better understanding of the workflow and process. I found an open source project to test my knowledge(I am not sure if I can share a GitHub link here of the project.</p>
<p>I started with creating the infra (workspace, storage account, KeyVault and container registry and a cluster).</p>
<p>Once that has been done, I create the following pipeline:</p>
<pre><code>trigger:
  branches:
    include:
      - machine-learning-pipelines
pool:
  vmImage: &quot;ubuntu-latest&quot;

steps:
- task: UsePythonVersion@0
  displayName: 'Use Python 3.7'
  inputs:
    versionSpec: 3.7

- task: Bash@3
  displayName: 'Install Python Requirements'
  inputs:
    targetType: filePath
    filePath: './package_requirement/install_requirements.sh'
    workingDirectory: 'package_requirement'

- bash: |
   pytest training/train_test.py --doctest-modules --junitxml=junit/test-results.xml --cov=data_test --cov-report=xml --cov-report=html
   
  displayName: 'Data Test'

- task: PublishTestResults@2
  displayName: 'Publish Test Results **/test-*.xml'
  inputs:
    testResultsFiles: '**/test-*.xml'
  condition: succeededOrFailed()

- task: AzureCLI@2
  displayName: 'Install Azure ml CLI'
  inputs:
    azureSubscription: '&lt;service-principle&gt;'
    scriptType: bash
    scriptLocation: inlineScript
    inlineScript: 'az extension add -n azure-cli-ml'

- task: AzureCLI@2
  displayName: 'create Azure ML workspace'
  inputs:
    azureSubscription: '&lt;service-principle&gt;'
    scriptType: bash
    scriptLocation: inlineScript
    inlineScript: 'az ml workspace create -g &lt;resource-group&gt; -w &lt;workspace&gt; -l westeurope --exist-ok --yes'

- task: AzureCLI@2
  displayName: 'Azure CLI '
  inputs:
    azureSubscription: '&lt;service-principle&gt;'
    scriptType: bash
    scriptLocation: inlineScript
    inlineScript: 'az ml computetarget create amlcompute -g &lt;resource-group&gt; -w &lt;workspace&gt; -n amlhricluster -s STANDARD_DS2_V2 --min-nodes 0 --max-nodes 2 --idle-seconds-before-scaledown 300'

- task: AzureCLI@2
  displayName: 'Upload Data to Datastore'
  inputs:
    azureSubscription: '&lt;service-principle&gt;'
    scriptType: bash
    scriptLocation: inlineScript
    inlineScript: 'az ml datastore upload -w &lt;workspace&gt; -g &lt;resource-group&gt; -n $(az ml datastore show-default -w &lt;workspace&gt; -g &lt;resource-group&gt; --query name -o tsv) -p data -u insurance --overwrite true'

- bash: 'mkdir metadata &amp;&amp; mkdir models'
  displayName: 'Make Metadata and Models Directory'


- task: AzureCLI@2
  displayName: 'Training Model'
  inputs:
    azureSubscription: '&lt;service-principle&gt;'
    scriptType: bash
    scriptLocation: inlineScript
    inlineScript: 'az ml run submit-script -g &lt;resource-group&gt; -w &lt;workspace&gt; -e insurance_classification --ct amlhricluster -d conda_dependencies.yml -c train_insurance -t ../metadata/run.json train_aml.py'
    workingDirectory: training

- task: AzureCLI@2
  displayName: 'Registering Model'
  inputs:
    azureSubscription: '&lt;service-principle&gt;'
    scriptType: bash
    scriptLocation: inlineScript
    inlineScript: 'az ml model register -g &lt;resource-group&gt; -w &lt;workspace&gt; -n insurance_model -f metadata/run.json --asset-path outputs/models/insurance_model.pkl -d &quot;Classification model for filling a claim prediction&quot; --tag &quot;data&quot;=&quot;insurance&quot; --tag &quot;model&quot;=&quot;classification&quot; --model-framework ScikitLearn -t metadata/model.json'

- task: AzureCli@2
  displayName: 'Downloading Model'
  inputs:
    azureSubscription: '&lt;service-principle&gt;'
    scriptType: bash
    scriptLocation: inlineScript
    inlineScript: 'az ml model download -g &lt;resource-group&gt; -w &lt;workspace&gt; -i $(jq -r .modelId metadata/model.json) -t ./models --overwrite'

- task: CopyFiles@2
  displayName: 'Copy Files to: $(Build.ArtifactStagingDirectory)'
  inputs:
    SourceFolder: '$(Build.SourcesDirectory)'
    Contents: |
     **/metadata/*
     **/models/*
     **/deployment/*
     **/tests/integration/*
     **/package_requirement/*
    TargetFolder: '$(Build.ArtifactStagingDirectory)'

- task: PublishPipelineArtifact@1
  displayName: 'Publish Pipeline Artifact'
  inputs:
    targetPath: '$(Build.ArtifactStagingDirectory)'
    artifact: Landing
</code></pre>
<p>My <code>train_insurance.runconfig</code> looks like this</p>
<pre><code>framework: Python
communicator: None
autoPrepareEnvironment: true
maxRunDurationSeconds:
nodeCount: 1
environment:
  name: project_environment
  python:
    userManagedDependencies: false
    interpreterPath: python
    condaDependenciesFile: conda_dependencies.yml
    baseCondaEnvironment:
  docker:
    enabled: true
    baseImage: mcr.microsoft.com/azureml/o16n-sample-user-base/ubuntu-miniconda
    sharedVolumes: true
    gpuSupport: false
    shmSize: 1g
    arguments: []
history:
  outputCollection: true
  snapshotProject: true
  directoriesToWatch:
  - logs
dataReferences:
  workspaceblobstore:
    dataStoreName: workspaceblobstore
    pathOnDataStore: insurance
    mode: download
    overwrite: true
    pathOnCompute: 
</code></pre>
<p>and my <code>conda_dependencies.yaml</code> is this:</p>
<pre><code># Conda environment specification. The dependencies defined in this file will
# be automatically provisioned for managed runs. These include runs against
# the localdocker, remotedocker, and cluster compute targets.

# Note that this file is NOT used to automatically manage dependencies for the
# local compute target. To provision these dependencies locally, run:
# conda env update --file conda_dependencies.yml

# Details about the Conda environment file format:
# https://conda.io/docs/using/envs.html#create-environment-file-by-hand

# For managing Spark packages and configuration, see spark_dependencies.yml.
# Version of this configuration file's structure and semantics in AzureML.
# This directive is stored in a comment to preserve the Conda file structure.
# [AzureMlVersion] = 2

name: amlproj06_training_env
dependencies:
  # The python interpreter version.
  # Currently Azure ML Workbench only supports 3.5.2 and later.
  - python=3.7.*
  - pip=20.2.4

  - pip:
      - urllib3_1_26_2
      - azureml
      - azure-cli
      - Cython
      - gcc7
      # Base AzureML SDK
      - azureml-sdk

      # Must match AzureML SDK version.
      # https://docs.microsoft.com/en-us/azure/machine-learning/concept-environments
      - azureml-defaults
      - azureml-core
      # Training deps
      - scikit-learn
      - numpy
      - pytest
      - pytest-cov
      # Scoring deps
      - inference-schema[numpy-support]

      # MLOps with R
      - azure-storage-blob

      # LightGBM bosting lib
      - lightgbm

      # lightgbm Caps because we are throwing darts
      - LightGBM

      # Job lib- whatever I don't know what we use it for
      - joblib

      # Install Pandas
      - pandas
</code></pre>
<p>and my <code>install_requirements.sh</code> is this:</p>
<pre><code>sudo apt-get update
sudo apt-get install -y libgomp1
python --version
pip install --upgrade azure-cli
pip install --upgrade azureml-sdk
pip install -r requirements.txt
pip freeze
</code></pre>
<p>But at the task <code>Training Model</code>, everything seems to be going well, until this error shows up, and the task fails:</p>
<pre><code>WARNING: Auto upgrade failed. name 'exit_code' is not defined
2022-07-21T10:12:17.3892253Z Traceback (most recent call last):
2022-07-21T10:12:17.3895010Z   File &quot;/opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages/azure/cli/core/commands/__init__.py&quot;, line 697, in _run_job
2022-07-21T10:12:17.3896066Z     result = cmd_copy(params)
2022-07-21T10:12:17.3897247Z   File &quot;/opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages/azure/cli/core/commands/__init__.py&quot;, line 333, in __call__
2022-07-21T10:12:17.3898550Z     return self.handler(*args, **kwargs)
2022-07-21T10:12:17.3900098Z   File &quot;/opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages/azure/cli/core/commands/command_operation.py&quot;, line 121, in handler
2022-07-21T10:12:17.3901165Z     return op(**command_args)
2022-07-21T10:12:17.3902335Z   File &quot;/opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages/azureml/_cli/cli_command.py&quot;, line 305, in command_wrapper
2022-07-21T10:12:17.3903290Z     retval = function(*args, **kwargs)
2022-07-21T10:12:17.3904464Z   File &quot;/opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages/azureml/_cli/run/run_commands.py&quot;, line 542, in submit_run
2022-07-21T10:12:17.3905481Z     run.wait_for_completion(show_output=True, wait_post_processing=True)
2022-07-21T10:12:17.3906746Z   File &quot;/opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages/azureml/core/run.py&quot;, line 846, in wait_for_completion
2022-07-21T10:12:17.3907678Z     raise_on_error=raise_on_error)
2022-07-21T10:12:17.3908827Z   File &quot;/opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages/azureml/core/run.py&quot;, line 1096, in _stream_run_output
2022-07-21T10:12:17.3909824Z     raise ActivityFailedException(error_details=json.dumps(error, indent=4))
2022-07-21T10:12:17.3910729Z azureml.exceptions._azureml_exception.ActivityFailedException: ActivityFailedException:
2022-07-21T10:12:17.3911536Z    Message: Activity Failed:
2022-07-21T10:12:17.3912179Z {
2022-07-21T10:12:17.3912780Z     &quot;error&quot;: {
2022-07-21T10:12:17.3913417Z         &quot;code&quot;: &quot;UserError&quot;,
2022-07-21T10:12:17.3914262Z         &quot;message&quot;: &quot;User program failed with OSError: libgomp.so.1: cannot open shared object file: No such file or directory&quot;,
2022-07-21T10:12:17.3915114Z         &quot;messageParameters&quot;: {},
2022-07-21T10:12:17.3916068Z         &quot;detailsUri&quot;: &quot;https://aka.ms/azureml-run-troubleshooting&quot;,
2022-07-21T10:12:17.3917645Z         &quot;details&quot;: []
2022-07-21T10:12:17.3918315Z     },
2022-07-21T10:12:17.3919200Z     &quot;time&quot;: &quot;0001-01-01T00:00:00.000Z&quot;
2022-07-21T10:12:17.3919847Z }
2022-07-21T10:12:17.3920390Z    InnerException None
2022-07-21T10:12:17.3920963Z    ErrorResponse 
2022-07-21T10:12:17.3921493Z {
2022-07-21T10:12:17.3922011Z     &quot;error&quot;: {
2022-07-21T10:12:17.3923684Z         &quot;message&quot;: &quot;Activity Failed:\n{\n    \&quot;error\&quot;: {\n        \&quot;code\&quot;: \&quot;UserError\&quot;,\n        \&quot;message\&quot;: \&quot;User program failed with OSError: libgomp.so.1: cannot open shared object file: No such file or directory\&quot;,\n        \&quot;messageParameters\&quot;: {},\n        \&quot;detailsUri\&quot;: \&quot;https://aka.ms/azureml-run-troubleshooting\&quot;,\n        \&quot;details\&quot;: []\n    },\n    \&quot;time\&quot;: \&quot;0001-01-01T00:00:00.000Z\&quot;\n}&quot;
2022-07-21T10:12:17.3925244Z     }
</code></pre>
<p>the WARNING happens during the docker image pull.
Regarding the libgomp I did installed it in my ubuntu agent, but it keeps showing the error.</p>
<p>Please did anyone ever faced this issue and know a workaround?</p>
<p>If you need more info please do not hesitate to ask and I will provide.</p>",1,1,2022-07-21 10:28:11.820000 UTC,,,0,python-3.x|azure-pipelines-build-task|azure-machine-learning-service|mlops|azuremlsdk,85,2019-10-27 17:33:31.880000 UTC,2022-09-22 23:04:00.370000 UTC,,777,88,0,201,,,,,,['azure-machine-learning-service']
"Azure ML CLI - (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED])","<pre><code>OS: windows 10 

azure-cli                         2.39.0
core                              2.39.0
telemetry                          1.0.6 *

Extensions:

azure-cli-ml                      1.41.0

Dependencies:

msal                            1.18.0b1
azure-mgmt-resource             21.1.0b1
</code></pre>
<p>I am using <a href=""https://docs.microsoft.com/en-us/cli/azure/ml(v1)/datastore?view=azure-cli-latest#az-ml(v1)-datastore-upload:%7E:text=Global%20Parameters-,az%20ml%20datastore%20upload,-Upload%20files%20to"" rel=""nofollow noreferrer"">AZ ML datastore upload</a> to upload data to blob storage, but getting</p>
<pre><code>az ml datastore upload -w workspace -n workspaceblobstore  -u datafile -p data
</code></pre>
<p>Error</p>
<pre><code>HTTPSConnectionPool(.... (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')))
</code></pre>
<p>I have already explored following , but none of them solve for CLI commands</p>
<p><a href=""https://stackoverflow.com/questions/52805115/certificate-verify-failed-unable-to-get-local-issuer-certificate"">certificate verify failed: unable to get local issuer certificate</a></p>
<p><a href=""https://stackoverflow.com/questions/51925384/unable-to-get-local-issuer-certificate-when-using-requests-in-python"">Unable to get local issuer certificate when using requests in python</a></p>
<p><a href=""https://stackoverflow.com/questions/71972703/certificate-verify-failed-unable-to-get-local-issuer-certificate-python-request"">certificate verify failed: unable to get local issuer certificate python requests</a></p>",0,2,2022-08-02 12:11:27.280000 UTC,,,0,python|ssl-certificate|azure-cli|azure-machine-learning-service|azure-sdk,170,2021-03-12 10:30:03.547000 UTC,2022-08-25 14:00:52.290000 UTC,,377,78,6,55,,,,,,['azure-machine-learning-service']
Use `sentence-transformers` inside of a Tensorflow-recommendation keras model in SageMaker,"<p>I've been going crazy for a few days over a problem that I thought trivial. My end-goal is to deploy to AWS Sagemaker a Tensorflow model that uses a simple string as input, calculates the embedding using a 'sentence-transformer' pre-trained model and eventually uses TensorFlow Recommenders to suggest the knn among a collection of embedding I already have calculated. I would like to do this entirely from the model, including the preprocessing (tokenization).</p>
<p>I made the predictions works with different approaches in my notebook. I start having troubles when I try to save my model.</p>
<p>The problem seems to be that HF's AutoTokenizer needs a pure <code>List of Strings</code> as input, and I hit a roadblock whenever I try to save my model using , and trying to go around this with tf.py_function using <a href=""https://stackoverflow.com/questions/71411065/use-sentence-transformers-inside-of-a-keras-model"">this approach</a> results in problems with Sagemaker.</p>
<p><strong>My approaches so far:</strong></p>
<p><strong>1. THE 'I THOUGHT IT WAS SO SIMPLE'</strong></p>
<pre><code>   startups_ids: list, startup_vectors
):
   import tensorflow as tf
   import tensorflow_recommenders as tfrs
   import numpy as np
   from random import randint
    
   exported_model = tfrs.layers.factorized_top_k.BruteForce(SentenceTransformer(&quot;all-mpnet-base-v2&quot;).encode)
   exported_model.index(np.array(startup_vectors), np.array(startups_ids))
   
   # TESTS the model
   #for some reason this seems to be needed in order to save the model :/ 
   # https://github.com/tensorflow/recommenders/issues/131

   test = exported_model(['Test Text Query'])

   print(test)
   
   return exported_model


text_to_startup_model(search_db_ids, search_db_embeddings)
#--&gt; WORKS PERFECTLY, AS I GET SOME SUGGESTIONS

tf.saved_model.save(text_to_startup_model(search_db_ids, search_db_embeddings), export_dir=&quot;/home/nicholas/test_model_save/1&quot;)

#TypeError                                 Traceback (most recent call last)
# /home/nicholas/Documents/Dev/Rialto-predict-1/notebooks/t2s_different_approaches.ipynb Cell 5 in &lt;cell line: 22&gt;()
#      19 text_to_startup_model(search_db_ids, search_db_embeddings)
#      20 #--&gt; WORKS PERFECTLY, AS I GET SOME SUGGESTIONS
# ---&gt; 22 tf.saved_model.save(text_to_startup_model(search_db_ids, search_db_embeddings), export_dir=&quot;/home/nicholas/test_model_save/1&quot;)

# File ~/Documents/Dev/Rialto-predict-1/venv/lib/python3.10/site-packages/tensorflow/python/saved_model/save.py:1334, in save(obj, export_dir, signatures, options)
#    1332 # pylint: enable=line-too-long
#    1333 metrics.IncrementWriteApi(_SAVE_V2_LABEL)
# -&gt; 1334 save_and_return_nodes(obj, export_dir, signatures, options)
#    1335 metrics.IncrementWrite(write_version=&quot;2&quot;)
# 
# .........
# 
# 
# File ~/Documents/Dev/Rialto-predict-1/venv/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:677, in Function._defun_with_scope.&lt;locals&gt;.wrapped_fn(*args, **kwds)
#     673 with default_graph._variable_creator_scope(scope, priority=50):  # pylint: disable=protected-access
#     674   # __wrapped__ allows AutoGraph to swap in a converted function. We give
#     675   # the function a weak reference to itself to avoid a reference cycle.
#     676   with OptionalXlaContext(compile_with_xla):
# --&gt; 677     out = weak_wrapped_fn().__wrapped__(*args, **kwds)
#     678   return out

# File ~/Documents/Dev/Rialto-predict-1/venv/lib/python3.10/site-packages/tensorflow/python/framework/func_graph.py:1147, in func_graph_from_py_func.&lt;locals&gt;.autograph_handler(*args, **kwargs)
#    1145 except Exception as e:  # pylint:disable=broad-except
#    1146   if hasattr(e, &quot;ag_error_metadata&quot;):
# -&gt; 1147     raise e.ag_error_metadata.to_exception(e)
#    1148   else:
#    1149     raise

# TypeError: in user code:

#     File &quot;/home/nicholas/Documents/Dev/Rialto-predict-1/venv/lib/python3.10/site-packages/keras/saving/saving_utils.py&quot;, line 138, in _wrapped_model  *
#         outputs = model(*args, **kwargs)
#     File &quot;/home/nicholas/Documents/Dev/Rialto-predict-1/venv/lib/python3.10/site-packages/keras/utils/traceback_utils.py&quot;, line 67, in error_handler  **
#         raise e.with_traceback(filtered_tb) from None

#     TypeError: Exception encountered when calling layer &quot;brute_force_3&quot; (type BruteForce).
    
#     in user code:
    
#         File &quot;/home/nicholas/Documents/Dev/Rialto-predict-1/venv/lib/python3.10/site-packages/tensorflow_recommenders/layers/factorized_top_k.py&quot;, line 567, in call  *
#             queries = self.query_model(queries)
#         File &quot;/home/nicholas/Documents/Dev/Rialto-predict-1/venv/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py&quot;, line 160, in encode  *
#             features = self.tokenize(sentences_batch)
#         File &quot;/home/nicholas/Documents/Dev/Rialto-predict-1/venv/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py&quot;, line 318, in tokenize  *
#             return self._first_module().tokenize(texts)
#         File &quot;/home/nicholas/Documents/Dev/Rialto-predict-1/venv/lib/python3.10/site-packages/sentence_transformers/models/Transformer.py&quot;, line 102, in tokenize  *
#             batch1.append(text_tuple[0])
    
#         TypeError: 'NoneType' object is not subscriptable
    
# ...
    
#     Call arguments received:
#       • queries=['None']
#       • k=None
</code></pre>
<p><strong>2. THE tf.py_function</strong>
As from my understanding the problem with the first approach is that it has no knowledge of the input type/value this second approach, from <a href=""https://stackoverflow.com/questions/71411065/use-sentence-transformers-inside-of-a-keras-model"">Use `sentence-transformers` inside of a keras model</a> was supposedly gonna work, as it uses tf.py_function to accept a List of Strings as first input, without complaining.</p>
<pre><code>def approach_2(startups_ids: list, startup_vectors):
        import tensorflow as tf
        import tensorflow_recommenders as tfrs
        import numpy as np
        from transformers import MPNetTokenizer, TFMPNetModel

        # Here it loads the specific pre-trained model we are using for Rialto
        tokenizer = MPNetTokenizer.from_pretrained(
            &quot;sentence-transformers/all-mpnet-base-v2&quot;
        )
        model = TFMPNetModel.from_pretrained(
            &quot;sentence-transformers/all-mpnet-base-v2&quot;, from_pt=True
        )

        class SBert(tf.keras.layers.Layer):
            def __init__(self, tokenizer, model):
                super(SBert, self).__init__()

                self.tokenizer = tokenizer
                self.model = model

            def tf_encode(self, inputs):
                def encode(inputs):
                    inputs = [x[0].decode(&quot;utf-8&quot;) for x in inputs.numpy()]
                    outputs = self.tokenizer(
                        inputs, padding=True, truncation=True, return_tensors=&quot;tf&quot;
                    )
                    return outputs[&quot;input_ids&quot;], outputs[&quot;attention_mask&quot;]

                return tf.py_function(
                    func=encode, inp=[inputs], Tout=[tf.int32, tf.int32]
                )

            def process(self, i, a):
                def __call(i, a):
                    model_output = self.model(
                        {&quot;input_ids&quot;: i.numpy(), &quot;attention_mask&quot;: a.numpy()}
                    )
                    return model_output[0]

                return tf.py_function(func=__call, inp=[i, a], Tout=[tf.float32])

            def mean_pooling(self, model_output, attention_mask):

                token_embeddings = tf.squeeze(tf.stack(model_output), axis=0)
                input_mask_expanded = tf.cast(
                    tf.broadcast_to(
                        tf.expand_dims(attention_mask, -1), tf.shape(token_embeddings)
                    ),
                    tf.float32,
                )
                a = tf.math.reduce_sum(token_embeddings * input_mask_expanded, axis=1)
                b = tf.clip_by_value(
                    tf.math.reduce_sum(input_mask_expanded, axis=1),
                    1e-9,
                    tf.float32.max,
                )
                embeddings = a / b
                embeddings, _ = tf.linalg.normalize(embeddings, 2, axis=1)

                return embeddings

            def call(self, inputs):
                input_ids, attention_mask = self.tf_encode(inputs)
                model_output = self.process(input_ids, attention_mask)
                embeddings = self.mean_pooling(model_output, attention_mask)
                return embeddings

        #  Uses the keras-ified model in a Keras model
        sbert = SBert(tokenizer, model)
        inputs = tf.keras.layers.Input((1,), dtype=tf.string)
        outputs = sbert(inputs)
        model = tf.keras.Model(inputs, outputs)

        # Implements the model we just build for top KNN retrieval, from the pool of pre-calculated startups embeddings.
        exported_model = tfrs.layers.factorized_top_k.BruteForce(model)
        exported_model.index(np.array(startup_vectors), np.array(startups_ids))

        # TESTS the model
        # for some reason this seems to be needed in order to save the model :/
        # https://github.com/tensorflow/recommenders/issues/131

        print(exported_model(tf.constant([&quot;'Test Text Query'&quot;])))

        return exported_model


model_to_store_1 = approach_2(search_db_ids, search_db_embeddings)

tf.saved_model.save(model_to_store_1, export_dir=&quot;/home/nicholas/test_model_save/2&quot;)

# THIS ONE WORKS LIKE A CHARM, saving the model and everything. Deploy on sagemaker is successful.
 
# FAILS TO WORK ON SAGEMAKER. BELOW THE LOGS WHEN THE MODEL IS CALLED

# ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from model with message &quot;{
#     &quot;error&quot;: &quot;No OpKernel was registered to support Op 'EagerPyFunc' used by {{node StatefulPartitionedCall/brute_force/model/s_bert/EagerPyFunc}} with these attrs: [is_async=false, Tin=[DT_STRING], _output_shapes=[&lt;unknown&gt;, &lt;unknown&gt;], Tout=[DT_INT32, DT_INT32], token=\&quot;pyfunc_4\&quot;]\nRegistered devices: [CPU]\nRegistered kernels:\n  &lt;no registered kernels&gt;\n\n\t [[StatefulPartitionedCall/brute_force/model/s_bert/EagerPyFunc]]\n\t [[StatefulPartitionedCall]]&quot;
# }&quot;. See https://us-west-2.console.aws.amazon.com/cloudwatch/home?region=us-west-2#logEventViewer:group=/aws/sagemaker/Endpoints/rialto-t2s-model-endpoint in account 634470116418 for more information
</code></pre>
<p>As you can see from the log, that the problem seems to be with the Eager mode and py_functions. I tried to google and found absolutely nothing on how to address this issue.</p>
<p><strong>3. THE Classes approach</strong></p>
<p>I've tried implementing something building upon <a href=""https://www.philschmid.de/tensorflow-sentence-transformers"" rel=""nofollow noreferrer"">this article</a>, but I am running into similar issues that with the first approach, as when I go to save the model, the expected input clashed with the requirements of tokenizer.</p>
<p>EDIT 1 - here a coolab showcasing the approach: <a href=""https://colab.research.google.com/drive/1gibFdEoHTs0hzD5yiXzLT_-asmilUoAQ?usp=sharing#scrollTo=TibAssWm3D5e"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1gibFdEoHTs0hzD5yiXzLT_-asmilUoAQ?usp=sharing#scrollTo=TibAssWm3D5e</a></p>
<hr />
<p>All of this journey triggered some questions:</p>
<p><strong>Question 1</strong> Is this even a best practice? Should I serve my model the tokenized sentences as a tensor?</p>
<p><strong>Question 2</strong> How the hell do I make it work? :)</p>",0,3,2022-09-08 14:03:52.027000 UTC,,2022-09-12 07:12:13.047000 UTC,1,tensorflow|amazon-sagemaker|huggingface-transformers|sentence-transformers,53,2020-07-20 13:18:09.420000 UTC,2022-09-23 13:24:48.093000 UTC,,23,0,0,0,,,,,,['amazon-sagemaker']
what is a optimal setting for a sagemaker batch job?,"<p>Based on AWS documentation, <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html"" rel=""nofollow noreferrer""> docs</a>, I've set up a batch inference job. however, once we choose the instance type and instance count, bare minimum , does sagemaker choose optimal plan to process jobs, say if there are more than one files , and if resource are available, can those files in parallel?</p>
<pre><code>from sagemaker.transformer import Transformer

tr = Transformer(model_name='custom_model',instance_count=2, instance_type='ml.m4.xlarge')
</code></pre>",1,0,2022-03-31 00:31:54.690000 UTC,,,1,amazon-sagemaker,175,2020-05-30 00:10:41.983000 UTC,2022-09-24 19:51:20.543000 UTC,,525,69,0,98,,,,,,['amazon-sagemaker']
What causes AWS SageMaker kmeans predict method to output a 413 Request Entity Too Large error?,"<p>I am attempting to create an Unsupervised Machine Learning Model using the k-means algorithm in AWS SageMaker. I am getting a 413 Request Entity Too Large error when calling the kmeans predictor predict method. What causes such an error? I have searched and most responses talk about Elastic Beanstalk and nginx settings which doesn't apply to my case.</p>
<p>The relevant code and error can be seen below and my entire Jupyter Notebook can be found at (<a href=""https://github.com/bgaber/machine-learning-recommendation-engine-with-aws-sagemaker/blob/main/imdb-movie-recommendation.ipynb"" rel=""nofollow noreferrer"">https://github.com/bgaber/machine-learning-recommendation-engine-with-aws-sagemaker/blob/main/imdb-movie-recommendation.ipynb</a>). The complete error can be seen at the bottom of the Jupyter notebook. I have looked in CloudWatch Logs, but it did not provide anymore specific information.</p>
<pre><code>scaler=MinMaxScaler()
df_scaled=pd.DataFrame(scaler.fit_transform(df))
df_scaled.columns=df.columns
df_scaled.index=df.index
train_data = df_scaled.values.astype('float32')
role = get_execution_role()
bucket_name = 'bg-sagemaker-bucket'
num_clusters = 15
kmeans = KMeans(role=role,
                train_instance_count=1,
                train_instance_type='ml.c4.xlarge',
                output_path='s3://'+ bucket_name +'/kmeans-project/',              
                k=num_clusters)

kmeans.fit(kmeans.record_set(train_data))    
kmeans_predictor = kmeans.deploy(initial_instance_count=1, 
                                 instance_type='ml.t2.medium')
result=kmeans_predictor.predict(train_data)

Error:
ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (413) from model with message &quot;
413 Request Entity Too Large
Request Entity Too Large
The data value transmitted exceeds the capacity limit. 
</code></pre>",1,2,2020-11-16 17:23:47.920000 UTC,,,1,amazon-web-services|machine-learning|k-means|amazon-sagemaker,781,2020-08-17 15:43:07.577000 UTC,2022-09-07 23:44:22.900000 UTC,,86,3,0,12,,,,,,['amazon-sagemaker']
"Can't get model inference using mlflow.pytorch.log_model, but could get it with mlflow.pyfunc.log_model","<p>I've used <code>mlflow.pyfunc.log_model</code> and I was able to get model inference with this, but not with<code>mlflow.pytorch.log_model</code>. The error was Verify that the serialized input Dataframe is compatible with the model for inference.</p>
<pre><code>    data = torch.randn(10, 3, 224, 224)  # shape: [bs, channel, size, size]
    model_input = {
                &quot;inputs&quot;: { 
                    &quot;x&quot;: data.tolist() }
                }
    request = json.dumps(model_input)
    headers = {&quot;content-type&quot;: &quot;application/json&quot;}
    response = requests.post(URL, data=request, headers=headers) # to mlflow
    response = response.json() 
    print(response)
</code></pre>
<p>The very same input to the model, but I could get inference on one but not the other? Am I missing something here? I would like to use <code>mlflow.pytorch.log_model</code> so I don't have to do a model wrapper for generalisation with <code>mlflow.pyfunc.log_model</code>.</p>
<p>Can anyone help me with this please.</p>",0,0,2022-09-22 02:12:45.467000 UTC,,2022-09-22 02:15:10.113000 UTC,0,rest|deployment|pytorch|mlflow|serving,14,2017-02-23 06:31:44.143000 UTC,2022-09-24 07:48:55.500000 UTC,,1,0,0,4,,,,,,['mlflow']
Video classification with AWS Lambda websocket and Sagemaker,"<p>I'm trying to do webcam video classification with AWS Sagemaker. I currently have a proof-of-concept pipeline that works, which looks like:</p>
<ol>
<li>Front-end gets frame of video</li>
<li>Sends in websocket to AWS API Gateway</li>
<li>Uses a Lambda proxy to send the image to Sagemaker, which classifies the image and returns a prediction</li>
</ol>
<p>This works for images. I'm trying to move to multiple frames - i.e. video. AWS has a limit of 32kb in each websocket message, so I can't concatenate frames, each of which is ~30kb. So I have to send each frame separately. What is the simplest way to insert some concept of &quot;connected messages&quot; into this pipeline? Do I have to use a database (e.g. Dynamo DB) or is there any concept of preserving information across websocket messages / Lambda function calls?</p>",0,0,2020-08-17 17:59:41.720000 UTC,,,1,amazon-web-services|websocket|aws-lambda|aws-api-gateway|amazon-sagemaker,100,2016-10-14 06:25:09.390000 UTC,2022-07-08 10:18:19.977000 UTC,"Adelaide SA, Australia",21,0,0,1,,,,,,['amazon-sagemaker']
How can I use GPUs on Azure ML with a NVIDIA CUDA custom docker base image?,"<p>In my dockerfile to build the custom docker base image, I specify the following base image:</p>

<pre><code>FROM nvidia/cuda:10.1-cudnn7-devel-ubuntu16.04
</code></pre>

<p>The dockerfile corresponding to the nvidia-cuda base image is found here: <a href=""https://gitlab.com/nvidia/container-images/cuda/blob/master/dist/ubuntu16.04/10.1/devel/cudnn7/Dockerfile"" rel=""nofollow noreferrer"">https://gitlab.com/nvidia/container-images/cuda/blob/master/dist/ubuntu16.04/10.1/devel/cudnn7/Dockerfile</a></p>

<p>Now when I print the AzureML log:</p>

<pre class=""lang-py prettyprint-override""><code>run = Run.get_context()
# setting device on GPU if available, else CPU
run.log(""Using device: "", torch.device('cuda' if torch.cuda.is_available() else 'cpu'))
</code></pre>

<p>I get  </p>

<pre><code>device(type='cpu')
</code></pre>

<p>but I would like to have a GPU and not a CPU. What am I doing wrong?</p>

<p>EDIT: I do not know exactly what you need.
But I can give you the following information:
azureml.core VERSION is 1.0.57.
The compute_target is defined via:</p>

<pre class=""lang-py prettyprint-override""><code>def compute_target(ws, cluster_name):
    try:
        cluster = ComputeTarget(workspace=ws, name=cluster_name)
    except ComputeTargetException:
        compute_config=AmlCompute.provisioning_configuration(vm_size='STANDARD_NC6',min_nodes=0,max_nodes=4)
        cluster = ComputeTarget.create(ws, cluster_name, compute_config)
</code></pre>

<p>The experiment is run via:</p>

<pre class=""lang-py prettyprint-override""><code>    ws = workspace(os.path.join(""azure_cloud"", 'config.json'))
    exp = experiment(ws, name=&lt;name&gt;)
    c_target = compute_target(ws, &lt;name&gt;)
    est = Estimator(source_directory='.',
                   script_params=script_params,
                   compute_target=c_target,
                   entry_script='azure_cloud/azure_training_wrapper.py',
                   custom_docker_image=image_name,
                   image_registry_details=img_reg_details,
                   user_managed = True,
                   environment_variables = {""SYSTEM"": ""azure_cloud""})

    # run the experiment / train the model
    run = exp.submit(config=est)
</code></pre>

<p>The yaml file contains:</p>

<pre><code>dependencies:
  - conda-package-handling=1.3.10
  - python=3.6.2
  - cython=0.29.10
  - scikit-learn==0.21.2
  - anaconda::cloudpickle==1.2.1
  - anaconda::cffi==1.12.3
  - anaconda::mxnet=1.5.0
  - anaconda::psutil==5.6.3
  - anaconda::pycosat==0.6.3
  - anaconda::pip==19.1.1
  - anaconda::six==1.12.0
  - anaconda::mkl==2019.4
  - anaconda::cudatoolkit==10.1.168
  - conda-forge::pycparser==2.19
  - conda-forge::openmpi=3.1.2
  - pytorch::pytorch==1.2.0
  - tensorboard==1.13.1
  - tensorflow==1.13.1
  - tensorflow-estimator==1.13.0
  - pip:
      - pytorch-transformers==1.2.0
      - azure-cli==2.0.72
      - azure-storage-nspkg==3.1.0
      - azureml-sdk==1.0.57
      - pandas==0.24.2
      - tqdm==4.32.1
      - numpy==1.16.4
      - matplotlib==3.1.0
      - requests==2.22.0
      - setuptools==41.0.1
      - ipython==7.8.0
      - boto3==1.9.220
      - botocore==1.12.220
      - cntk==2.7
      - ftfy==5.6
      - gensim==3.8.0
      - horovod==0.16.4
      - keras==2.2.5
      - langdetect==1.0.7
      - langid==1.1.6
      - nltk==3.4.5
      - ptvsd==4.3.2
      - pytest==5.1.2
      - regex==2019.08.19
      - scipy==1.3.1
      - scikit_learn==0.21.3
      - spacy==2.1.8
      - tensorpack==0.9.8
</code></pre>

<p>EDIT 2: I tried <code>use_gpu = True</code> as well as upgrading to <code>azureml-sdk=1.0.65</code> but to no avail. Some people suggest additionally installing cuda-drivers via <code>apt-get install cuda-drivers</code>, but this does not work and I cannot build a docker image with that.
The output of <code>nvcc --version</code> on the docker image yields:</p>

<pre><code>nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2019 NVIDIA Corporation
Built on Sun_Jul_28_19:07:16_PDT_2019
Cuda compilation tools, release 10.1, V10.1.243
</code></pre>

<p>So I think that should be o.k. The docker image itself of course has no GPU, so command <code>nvidia-smi</code> is not found and </p>

<pre class=""lang-sh prettyprint-override""><code>python -i
</code></pre>

<p>and then</p>

<pre class=""lang-py prettyprint-override""><code>import torch
print(torch.cuda.is_available())
</code></pre>

<p>will print False.</p>",1,2,2019-10-01 09:31:41.213000 UTC,,2019-10-03 11:12:30.170000 UTC,0,docker|gpu|azure-machine-learning-service,1389,2018-04-25 15:32:23.370000 UTC,2022-09-21 14:08:46.433000 UTC,"Zürich, Schweiz",460,668,0,52,,,,,,['azure-machine-learning-service']
How to use mlflow.log_params to append params when used alongside mlflow.autolog(),"<p>I am using mlflow.autolog() to log params and metrics for a tensorflow based training and it works well. It captures 26 different parameters automatically.
However if I use mlflow.log_params() to log custom params those 26 are not being logged anymore. I am wondering if there is a way to append params alongside regular params?</p>",0,1,2022-01-19 16:16:30.920000 UTC,,,3,mlflow,149,2015-10-06 10:55:49.560000 UTC,2022-07-08 04:01:26.250000 UTC,,51,6,0,3,,,,,,['mlflow']
How to update azure ml workspace service image id?,"<p>I have workspace service created in machine learning workspace
How can I update ACR repository name and tag in running service ?</p>
<p><a href=""https://docs.azure.cn/zh-cn/cli/ext/azure-cli-ml/ml/service?view=azure-cli-latest#ext_azure_cli_ml_az_ml_service_update"" rel=""nofollow noreferrer"">https://docs.azure.cn/zh-cn/cli/ext/azure-cli-ml/ml/service?view=azure-cli-latest#ext_azure_cli_ml_az_ml_service_update</a></p>
<p>This does not show any arguments related to image id update</p>
<p>az ml service update --name</p>",1,0,2021-07-21 18:27:58.587000 UTC,,,0,machine-learning|azure-machine-learning-studio|azure-machine-learning-service,61,2019-08-31 00:00:56.790000 UTC,2022-09-16 17:26:27.993000 UTC,,417,53,0,233,,,,,,"['azure-machine-learning-studio', 'azure-machine-learning-service']"
tensorflow serving Error: Invalid argument: JSON object: does not have named input,"<p>I am trying to train a model with Amazon Sagemaker and I want serve it using with Tensorflow serving. To achieve that, I am downloading the model to a Tensorflow serving docker and I am trying to serve it from there.</p>
<p>The Sagemaker's training and evaluating stages are completed without errors, but when I load my model to the Tensorflow serving server and try to invoke it I get Tensorflow serving errors that suggest that my model has no defined inputs. It can be seen that the Tensorflow serving server that the model is being served.</p>
<p>For debugging purposes, I tried to serve it with Sagemaker but all I got was a vague error message saying I have an error invoking the endpoint.</p>
<p>I think that the problem is that I am not defining well either the serving_input_fn or invoking it wrong or both. Can anyone help?</p>
<h3>Tensorflow serving server invocation curl:</h3>
<pre><code>curl -d '{&quot;instances&quot;: [{&quot;col3&quot;: 1.0}]}' -X POST http://localhost:8501/v1/models/test_model:predict
</code></pre>
<h3>The error I receive from Tensorflow serving:</h3>
<pre><code>{ &quot;error&quot;: &quot;Failed to process element: 0 key: col3 of \'instances\' list. Error: Invalid argument: JSON object: does not have named input: col3&quot; }%    
</code></pre>
<h3>Sagemaker's training python file:</h3>
<pre><code>import os
import tensorflow as tf
from tensorflow.python.ops import nn


TRAIN_FILENAME = 'test.csv'
TEST_FILENAME = 'train.csv'

NODES_IN_LAYER = 6
LAYERS_NUM = 10
NUM_LINES_TO_SKIP = 1

CSV_COLUMNS = ['col1', 'col2', 'col3', 'col4', 'col5', 'col6', 'col7', 'col8', 'label']
RECORDS_DEFAULTS = [[0], [0], [0.0], [0.0], [0], [0.0], [0.0], [0], [0.0]]

BATCH_SIZE = 32

FEATURE_SPEC = {
    'col3': tf.FixedLenFeature(dtype=tf.float32, shape=[]),
}


def estimator_fn(run_config, params):
    feature_columns = [
        tf.feature_column.numeric_column('col3')]
    return tf.estimator.DNNRegressor(feature_columns=feature_columns,
                                     hidden_units=[NODES_IN_LAYER] * LAYERS_NUM,
                                     activation_fn=nn.tanh,
                                     config=run_config)


def serving_input_fn(params):
    return tf.estimator.export.build_raw_serving_input_receiver_fn(FEATURE_SPEC)


def train_input_fn(training_dir, params):
    &quot;&quot;&quot;Returns input function that would feed the model during training&quot;&quot;&quot;
    return _generate_input_fn(training_dir, TRAIN_FILENAME)


def eval_input_fn(training_dir, params):
    &quot;&quot;&quot;Returns input function that would feed the model during evaluation&quot;&quot;&quot;
    return _generate_input_fn(training_dir, TEST_FILENAME)


def parse_csv(line):
    columns = tf.decode_csv(line, record_defaults=RECORDS_DEFAULTS)
    line_features = dict(zip(CSV_COLUMNS, columns))
    line_label = line_features.pop('label')
    return {'col3': line_features.pop('col3')}, line_label


def _generate_input_fn(training_dir, training_filename):
    filename = os.path.join(training_dir, training_filename)
    dataset = tf.data.TextLineDataset(filename)
    dataset = dataset.skip(NUM_LINES_TO_SKIP).map(parse_csv).batch(BATCH_SIZE)
    return dataset
</code></pre>",4,0,2018-09-06 16:36:13.583000 UTC,,2020-06-20 09:12:55.060000 UTC,2,python-2.7|tensorflow|tensorflow-serving|amazon-sagemaker,5954,2018-09-06 09:47:47.183000 UTC,2019-06-25 12:28:57.643000 UTC,,21,0,0,7,,,,,,['amazon-sagemaker']
TensorFlow Optimization in DSVM,"<p><strong>Problem statement first:</strong> How does one properly setup tensorflow for running on a DSVM using a <a href=""https://docs.microsoft.com/en-us/azure/machine-learning/desktop-workbench/experimentation-service-configuration#running-a-script-on-a-remote-docker"" rel=""nofollow noreferrer"">remote Docker environment</a>? Can this be done in <code>aml_config/*.runconfig</code>?</p>

<p>I receive the following message and I would like to be able to utilize the increased speeds of the extended FMA operations.</p>

<blockquote>
  <p>tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA</p>
</blockquote>

<p><strong>Background:</strong> I utilize a local docker environment managed through Azure ML Workbench for initial testing and code validation so that I'm not running an expensive DSVM constantly. Once I assess that my code is to my liking, I then run it on a remote docker instance on an <a href=""https://azuremarketplace.microsoft.com/en-us/marketplace/apps/microsoft-ads.linux-data-science-vm-ubuntu"" rel=""nofollow noreferrer"">Azure DSVM</a>.</p>

<p>I want a consistent conda environment across my compute environments, so this works out extremely well. However, I cannot figure out how to control the tensorflow build to optimize for the hardware at hand (i.e. my local docker on macOS vs. remote docker on Ubuntu DSVM)</p>",1,0,2018-06-05 16:35:12.097000 UTC,,,0,azure|docker|tensorflow|azure-machine-learning-studio,79,2016-01-12 02:19:06.830000 UTC,2022-09-23 22:50:47.183000 UTC,"Seattle, WA, United States",578,109,1,70,,,,,,['azure-machine-learning-studio']
"Deploy Pretrained Model Test Error: The first dimension of paddings must be the rank of inputs[4,2]","<p>I have a successfully trained and tested a custom instance segmentation model using pixellib Mask_RCNN model.  The model runs inferences fine locally, but when I try to serve predictions using vertex ai / google cloud platform I cannot get the predictions to serve correctly.</p>
<h2>Model Signature:</h2>
<pre><code>  inputs['input_anchors'] tensor_info:
      dtype: DT_FLOAT
      shape: (-1, -1, 4)
      name: serving_default_input_anchors:0
  inputs['input_image'] tensor_info:
      dtype: DT_FLOAT
      shape: (-1, -1, -1, 3)
      name: serving_default_input_image:0
  inputs['input_image_meta'] tensor_info:
      dtype: DT_FLOAT
      shape: (-1, 14)
      name: serving_default_input_image_meta:0
</code></pre>
<h2>JSON Requests - Snippet:</h2>
<p>To test the model independent of any other code I use the test feature in google cloud console for vertex AI.  I enter JSON structured as follows.</p>
<pre class=""lang-json prettyprint-override""><code>{
    &quot;instances&quot;: 
    [
    {
     &quot;input_anchors&quot;: [[[-0.35, ...], ... ]],
     &quot;input_image&quot;: [[[[-123.7, -116.8, -103.9], ... ]]], 
     &quot;input_image_meta&quot;: [[0.0, 240.0, ....]]
    }
    ]
}
</code></pre>
<h2>Shapes of input</h2>
<p>I can confirm that the shapes of the inputs are the correct dimensions for the model signature.</p>
<pre class=""lang-py prettyprint-override""><code>np.array(instance['input_anchors']).shape -&gt; (1,1023,4)
np.array(instance['input_image']).shape -&gt; (1,64,64,3)
np.array(instance['input_image_meta'].shape -&gt; (1,14)
</code></pre>
<h2>Returns the error:</h2>
<p>Testing the model returns the following error.</p>
<pre class=""lang-json prettyprint-override""><code>{
 &quot;error&quot;: {
   &quot;code&quot;: 400,
   &quot;message&quot;: &quot;{\n    \&quot;error\&quot;: \&quot;The first dimension of paddings must be the rank of inputs[4,2] [1,1,64,64,3]\\n\\t [[{{node mask_rcnn/zero_padding2d_1/Pad}}]]\&quot;\n}&quot;,
   &quot;status&quot;: &quot;INVALID_ARGUMENT&quot;
 }
}
</code></pre>
<p>The message indicates that the model is looking for a 5D input <code>[1,1,64,64,3]</code>, but the model signature requires a 4D input <code>(-1,-1,-1,3)</code>.</p>
<p>Using np.expand_dims to make the input_image 5D, it results in the same error message but now asking for a 6D input <code>[1,1,1,64,64,3]</code>.  A 6D input yields an error message asking for a 7D input...</p>
<p>If I reduce the input_image to 3D (which should definitely be incorrect) <code>[64,64,3]</code> I get a different error:</p>
<pre class=""lang-json prettyprint-override""><code>{
 &quot;error&quot;: {
   &quot;code&quot;: 400,
   &quot;message&quot;: &quot;{\n    \&quot;error\&quot;: \&quot;slice index 1 of dimension 0 out of bounds.\\n\\t [[{{node mask_rcnn/roi_align_classifier/strided_slice_8}}]]\&quot;\n}&quot;,
   &quot;status&quot;: &quot;INVALID_ARGUMENT&quot;
 }
}
</code></pre>
<p>Can someone help me understand if I'm structuring my inputs incorrectly for the model or if I'm running into a known bug?  It's strange that the error message always asks for +1 dimension than the dimension that I give it.</p>",0,0,2022-05-30 21:27:07.497000 UTC,1.0,,0,python|tensorflow|google-cloud-platform|google-cloud-vertex-ai,41,2016-03-08 10:05:14.813000 UTC,2022-09-01 10:08:54.273000 UTC,,36,4,0,1,,,,,,['google-cloud-vertex-ai']
Run script locally with remote dataset on AzureML,"<p>I have a script that for development purposes I would like to run and debug locally. However, I do not want to store the data needed for my experiment on my local machine.</p>
<p>I am using the <code>azureml</code> library with the Azure Machine Learning Studio. See my code below</p>
<pre><code># General
import os
import argparse

# Data analysis and wrangling
import pandas as pd

# Machine learning
from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC
from azureml.core import Run

# Get the environment of this run
run = Run.get_context()

if __name__ == &quot;__main__&quot;:
    parser = argparse.ArgumentParser()
    parser.add_argument(
        '--data_path',
        type=str,
        help='Path to the training data',
        # The default path is on my local machine, however I would like to reference a remote datastore on Azure as a parameter to this script
        default=os.path.join(os.getcwd(), 'data')
    )
    args = parser.parse_args()

    # Obtain the data from the datastore
    train_df = pd.read_csv(os.path.join(args.data_path, os.listdir(args.data_path)[0]))

    # Drop unnecessary columns
    train_df = train_df.drop(['Name', 'PassengerId', 'Ticket', 'Cabin'], axis=1)

    # Encode non-numeric features as dummies
    train_df = pd.get_dummies(train_df)

    # Drop NA's
    train_df.dropna(inplace=True)

    # Use gridsearch CV to find the best parameters for the model
    parameters = {'kernel': ('linear', 'rbf'),
                  'C': [1, 10]}

    # Initialize the grid search
    search = GridSearchCV(SVC(), param_grid=parameters, cv=8)

    # Train the model
    search.fit(train_df.drop(&quot;Survived&quot;, axis=1), train_df[&quot;Survived&quot;])

</code></pre>
<p>Now, the script uses a local folder 'data'. However, I would like to give an argument to this script that indicates I would like to use a remote datastore in the Azure Machine Learning Studio. How could I achieve that?</p>",1,0,2021-07-01 10:39:02.170000 UTC,1.0,,1,python|azure-machine-learning-studio|azure-machine-learning-service,245,2020-04-06 10:20:27.240000 UTC,2022-09-23 14:24:39.773000 UTC,,299,44,2,21,,,,,,"['azure-machine-learning-studio', 'azure-machine-learning-service']"
Advice on classification approach,"<p>I need to classify incoming car rentals, but my historic data that I could use for training is in ""grouped"" form and I can't see how I could train a classification model. </p>

<p>My incoming data is a list of car model, quantity and unit price:</p>

<pre><code> Chevrolet Spark, 1, 196.91
 Fiat 500, 1, 196.91
 Toyota Prius Hybrid, 3, 213.73
</code></pre>

<p>This incoming data is currently manually classified and saved grouped by class and total price per group (Chevy and Fiat is Economy, Prius is Hybrid):</p>

<pre><code> Economy, 393.82
 Hybrid, 641.19
</code></pre>

<p>This problem should be solvable by machine learning but I can't figure out how to build a training set for a supervised classifier. Any guidance appreciated.</p>

<p>Thanks</p>",2,0,2016-06-22 11:00:47.613000 UTC,,,-1,machine-learning|azure-machine-learning-studio,93,2016-06-21 12:15:40.493000 UTC,2017-05-11 09:48:26.363000 UTC,,1,0,0,2,,,,,,['azure-machine-learning-studio']
Preprocessing data for Sagemaker Inference Pipeline with Blazingtext,"<p>I'm trying to figure out the best way to preprocess my input data for my inference endpoint for AWS Sagemaker. I'm using the BlazingText algorithm.</p>

<p>I'm not really sure the best way forward and I would be thankful for any pointers.</p>

<p>I currently train my model using a Jupyter notebook in Sagemaker and that works wonderfully, but the problem is that I use NLTK to clean my data (Swedish stopwords and stemming etc):</p>

<pre><code>import nltk
nltk.download('punkt')
nltk.download('stopwords')
</code></pre>

<p>So the question is really, how do I get the same  pre-processing logic to the inference endpoint  ?</p>

<p>I have a couple of thoughts about how to proceed:</p>

<ul>
<li><p>Build a docker container with the python libs &amp; data installed with the sole purpose of pre-processing the data. Then use this container in the inference pipeline. </p></li>
<li><p>Supply the Python libs and Script to an existing container in the same way you can do for external lib an notebook </p></li>
<li><p>Build a custom fastText container with the libs I need and run it outside of Sagemaker.</p></li>
<li><p>Will probably work, but feels like a ""hack"": Build a Lambda function that has the proper Python libs&amp;data installed and calls the Sagemaker Endpoint. I'm worried about cold start delays as the prediction traffic volume will be low. </p></li>
</ul>

<p>I would like to go with the first option, but I'm struggling a bit to understand if there is a docker image that I could build from, and add my dependencies to, or if I need to build something from the ground up. For instance, would the image sagemaker-sparkml-serving:2.2 be a good candidate? </p>

<p>But maybe there is a better way all around? </p>",0,0,2020-04-07 09:18:33.373000 UTC,,,2,python|amazon-web-services|machine-learning|amazon-sagemaker|inference,848,2015-01-05 22:37:28.900000 UTC,2022-09-24 06:04:55.290000 UTC,,56,5,0,4,,,,,,['amazon-sagemaker']
Get weights after training from Sagemaker Estimator,"<p>Tensorflow's Estimator provides a method to get desired variable values after training/testing using <a href=""https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator#get_variable_value"" rel=""nofollow noreferrer"">get_variable_value</a>.
Does there exist similar functionality in Sagemaker's Estimator, so that I am able to obtain weights after my model is trained.</p>",1,0,2018-07-04 08:56:11.770000 UTC,,,1,amazon-web-services|amazon-sagemaker,487,2018-07-04 08:48:17.097000 UTC,2018-07-26 05:49:49.070000 UTC,,11,0,0,1,,,,,,['amazon-sagemaker']
Cannot get correct predictions from ONNX model from Customvision,"<p>I am evaluating customvision.ai for training image classification model and then downloading that model as an onnx file that would be consumed within a .Net Windows forms app.</p>
<p>I created a new project, uploaded few images, tagged them and was able to fetch predictions from the model within Customvision.ai .The model accuracy was acceptable. CustomVision allows you to download a model as an ONNX file which can be deployed within a cross platform application. In my case I plan to deploy and consume the model within a Windows forms application.</p>
<p>When I download the model as onnx, I receive a zip file that contains the .onnx file and few others.</p>
<p>One of the file is Metadata_properties.json, and it has the following contents:</p>
<pre><code>{
    &quot;CustomVision.Metadata.AdditionalModelInfo&quot;: &quot;&quot;,
    &quot;CustomVision.Metadata.Version&quot;: &quot;1.2&quot;,
    &quot;CustomVision.Postprocess.Method&quot;: &quot;ClassificationMultiClass&quot;,
    &quot;CustomVision.Postprocess.Yolo.Biases&quot;: &quot;[]&quot;,
    &quot;CustomVision.Postprocess.Yolo.NmsThreshold&quot;: &quot;0.0&quot;,
    &quot;CustomVision.Preprocess.CropHeight&quot;: &quot;0&quot;,
    &quot;CustomVision.Preprocess.CropMethod&quot;: &quot;FullImageShorterSide&quot;,
    &quot;CustomVision.Preprocess.CropWidth&quot;: &quot;0&quot;,
    &quot;CustomVision.Preprocess.MaxDimension&quot;: &quot;0&quot;,
    &quot;CustomVision.Preprocess.MaxScale&quot;: &quot;0.0&quot;,
    &quot;CustomVision.Preprocess.MinDimension&quot;: &quot;0&quot;,
    &quot;CustomVision.Preprocess.MinScale&quot;: &quot;0.0&quot;,
    &quot;CustomVision.Preprocess.NormalizeMean&quot;: &quot;[0.0, 0.0, 0.0]&quot;,
    &quot;CustomVision.Preprocess.NormalizeStd&quot;: &quot;[1.0, 1.0, 1.0]&quot;,
    &quot;CustomVision.Preprocess.ResizeMethod&quot;: &quot;Stretch&quot;,
    &quot;CustomVision.Preprocess.TargetHeight&quot;: &quot;300&quot;,
    &quot;CustomVision.Preprocess.TargetWidth&quot;: &quot;300&quot;,
    &quot;Image.BitmapPixelFormat&quot;: &quot;Rgb8&quot;,
    &quot;Image.ColorSpaceGamma&quot;: &quot;SRGB&quot;,
    &quot;Image.NominalPixelRange&quot;: &quot;Normalized_0_1&quot;
}
</code></pre>
<p>What I understand from this file is that the eventual Tensor that would be provided to the model for inference would need to be stretch resized to 300x300, Normalized between 0 and 1, Mean set to zero and stdev set to 1. In order to consume this model within my code, here is what I put together from various online sources:</p>
<pre><code>using SixLabors.ImageSharp;
using SixLabors.ImageSharp.PixelFormats;
using SixLabors.ImageSharp.Processing;
using System;
using System.Collections.Generic;
using System.ComponentModel;
using System.Data;
//using System.Drawing;
using System.Linq;
using System.Text;
using System.Threading.Tasks;
using System.Windows.Forms;
using Microsoft.ML.OnnxRuntime.Tensors;
using Microsoft.ML.OnnxRuntime;
using System.IO;

namespace TestONNXRunner
{
   

    public partial class Form1 : Form
    {
        public Form1()
        {
            InitializeComponent();

            RunModel();
        }

        public void RunModel()
        {
            // Read paths
            string modelFilePath = @&quot;C:\ImageMLProjects\MarbleImagesDataset\OnnxModel\onnxdataset\model.onnx&quot;;
            var LabelsDict = GetLabelMap(@&quot;C:\ImageMLProjects\MarbleImagesDataset\OnnxModel\onnxdataset\labels.txt&quot;);


            string imageFilePath = @&quot;&quot;;
            OpenFileDialog openFileDialog1 = new OpenFileDialog
            {
                InitialDirectory = @&quot;C:\&quot;,
                Title = &quot;Browse Image Files&quot;,

                CheckFileExists = true,
                CheckPathExists = true,

                FilterIndex = 2,
                RestoreDirectory = true,

                ReadOnlyChecked = true,
                ShowReadOnly = true
            };

            if (openFileDialog1.ShowDialog() == DialogResult.OK)
            {
                imageFilePath = openFileDialog1.FileName;

                // Read image
                using Image&lt;Rgb24&gt; image = Image.Load&lt;Rgb24&gt;(imageFilePath);

                // Resize image
                image.Mutate(x =&gt;
                {
                    x.Resize(new ResizeOptions
                    {
                        Size = new SixLabors.ImageSharp.Size(300, 300),
                        Mode = ResizeMode.Stretch
                    });
                });

                // Preprocess image
                Tensor&lt;float&gt; input = new DenseTensor&lt;float&gt;(new[] { 1, 3, image.Height, image.Width });
                var mean = new[] { 0f, 0f, 0f };
                var stddev = new[] { 1f, 1f, 1f };
                for (int y = 0; y &lt; image.Height; y++)
                {
                    Span&lt;Rgb24&gt; pixelSpan = image.GetPixelRowSpan(y);
                    for (int x = 0; x &lt; image.Width; x++)
                    {
                        input[0, 0, x, y] = ((pixelSpan[x].R / 255f) - mean[0]) / stddev[0];
                        input[0, 1, x, y] = ((pixelSpan[x].G / 255f) - mean[1]) / stddev[1];
                        input[0, 2, x, y] = ((pixelSpan[x].B / 255f) - mean[2]) / stddev[2];
                    }
                }

                // Setup inputs
                var inputs = new List&lt;NamedOnnxValue&gt;
                {
                    NamedOnnxValue.CreateFromTensor(&quot;data&quot;, input)
                };

                // Run inference
                //int gpuDeviceId = 0; // The GPU device ID to execute on
                //var session = new InferenceSession(&quot;model.onnx&quot;, SessionOptions.MakeSessionOptionWithCudaProvider(gpuDeviceId));
                using var session = new InferenceSession(modelFilePath);
                using IDisposableReadOnlyCollection&lt;DisposableNamedOnnxValue&gt; results = session.Run(inputs);

                // Postprocess to get softmax vector
                IEnumerable&lt;float&gt; output = results.First().AsEnumerable&lt;float&gt;();
                float sum = output.Sum(x =&gt; (float)Math.Exp(x));
                IEnumerable&lt;float&gt; softmax = output.Select(x =&gt; (float)Math.Exp(x) / sum);

                // Extract top 10 predicted classes
                IEnumerable&lt;Prediction&gt; top10 = softmax.Select((x, i) =&gt; new Prediction { Label = LabelsDict[i], Confidence = x })
                                   .OrderByDescending(x =&gt; x.Confidence)
                                   .Take(10);

                // Print results to console
                Console.WriteLine(&quot;Top 10 predictions for ResNet50 v2...&quot;);
                Console.WriteLine(&quot;--------------------------------------------------------------&quot;);
                foreach (var t in top10)
                {
                    Console.WriteLine($&quot;Label: {t.Label}, Confidence: {t.Confidence}&quot;);
                }
            }
        }


        public Dictionary&lt;int, string&gt; GetLabelMap(string LabelMapFile)
        {
            Dictionary&lt;int, string&gt; labelsDict = new Dictionary&lt;int, string&gt;();
            if(File.Exists(LabelMapFile))
            {
                string data = File.ReadAllText(LabelMapFile);

                string[] labels = data.Split('\n');
                int i = 0;
                foreach (var label in labels)
                {
                    labelsDict.Add(i, label);
                    i++;
                }
            }
            return labelsDict;
        }
        internal class Prediction
        {
            public string Label { get; set; }
            public float Confidence { get; set; }
        }


      
    }
}
</code></pre>
<p><strong>Now what is the problem?</strong></p>
<p>I see no errors, Irrespective of what image I use for inference, I just get the same result.</p>
<p><strong>Questions</strong></p>
<ol>
<li>Should I structure the tensor differently? I am not sure if this is something to do with the way the Tensor is structured.</li>
<li>The last updates to Customvision pages on Github was several years ago, Is CustomVision recommended for production usage in 2021? Should I be looking out for something else? The idea is to be able to build/train high quality image classification models with a low/zero code approach and then deploy the model onto on premise computers for use in low latency applications.</li>
</ol>
<p>Any help in this regard would be appreciated</p>",1,0,2021-07-19 08:16:41.310000 UTC,,2021-07-19 11:10:15.740000 UTC,0,azure-machine-learning-service|microsoft-custom-vision|onnxruntime,351,2018-07-06 13:16:27.360000 UTC,2022-09-14 06:40:15.180000 UTC,Dubai - United Arab Emirates,233,48,3,46,,,,,,['azure-machine-learning-service']
Member must satisfy regular expression pattern: ^(https|s3)://([^/]+)/?(.*)$,"<p>I am writing a notebook on amazon SageMaker studio. I follow the instructions given by the
instructor who is an amazon engineer. However, I am getting the following error.</p>
<pre><code>%%sh
pip3 -q install --upgrade pip
pip3 -q install sagemaker awscli boto3 smdebug pandas matplotlib seaborn --upgrade

from IPython.core.display import HTML
HTML('&lt;script&gt;.jupyter.notebook.kernel.restart(&lt;/script&gt;)')
import numpy as np
import pandas as pd
import sagemaker
import boto3, os
from sagemaker.estimator import Estimator
from sagemaker.debugger import rule_configs, Rule, DebuggerHookConfig, CollectionConfig

!wget -N https://archive.ics.uci.edu/ml/machine-learning-databases/00222/bank-additional.zip'
!unzip -o bank-additional.zip'


!head ./bank-additional/bank-additional-full.csv

data = pd.read_csv('bank-additional/bank-additional-full.csv', sep=';')
pd.set_option('display.max_columns', 500)
pd.set_option('display.max_rows', 500)
data[:10]


data['no_previous_contact'] = np.where(data['pdays'], 1, 0)
data.drop(data['pdays'])
data['not_working'] = np.where(np.in1d(data['job'], ['student', 'retired', 'unemployed']), 1, 0)


model_data = pd.get_dummies(data)
model_data[:10]

train_data, validation_data, test_data = np.split(model_data.sample(frac=1, random_state=123), 
                                                 [int(0.7 * len(model_data)), int(0.9*len(model_data))])  

pd.concat([train_data['y_yes'], train_data.drop(['y_no', 'y_yes'], axis=1)], axis=1).to_csv('train.csv', index=False, header=False)
pd.concat([validation_data['y_yes'], validation_data.drop(['y_no', 'y_yes'], axis=1)], axis=1).to_csv('validation.csv', index=False, header=False)

test_data.drop(['y_no', 'y_yes'], axis=1).to_csv('test.csv', index=False, header=False)

bucket = sagemaker.Session().default_bucket()                     
prefix = 'sagemaker/DEMO-xgboost-dm'

boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train/train.csv')).upload_file('train.csv')
boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'validation/validation.csv')).upload_file('validation.csv')
boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'test/test.csv')).upload_file('test.csv')


s3_input_train = sagemaker.TrainingInput(s3_data='s3://{}/{}/train'.format(bucket, prefix), content_type='csv')
s3_input_validation = sagemaker.TrainingInput(s3_data='s3://{}/{}/validation/'.format(bucket, prefix), content_type='csv')
s3_data = {'train': s3_input_train, 'validation': s3_input_validation}

sess = sagemaker.Session()
region = boto3.Session().region_name


container = sagemaker.image_uris.retrieve('xgboost', region, version='latest')
save_interval = '1'

xgb = Estimator(  # The algorithm (XGBoost)
                  # IAM Permissions for SageMaker
                  # save the model here
    container,
    role=sagemaker.get_execution_role(),
    sagemaker_session=sess,
    input_mode='File',
    output_path='s3//{}/{}/output'.format(bucket, prefix),
    instance_count=1,
    instance_type='ml.m4.2xlarge',
    use_spot_instances=True,
    max_run=300,
    max_weight=600,
    debugger_hook_config=DebuggerHookConfig(s3_output_path='s3//{}/{}/output'.format(bucket,
            prefix), collection_configs=[CollectionConfig(name='metrics'
            , parameters={'save_interval': str(save_interval)}),
            CollectionConfig(name='predictions',
            parameters={'save_interval': str(save_interval)})]),
    rules=[Rule.sagemaker(rule_configs.class_imbalance(),
            rule_parameters={'collection_names': 'metrics'})],
    )

xgb.set_hyperparameters(
objective='binary:logistic', 
eval_metric='auc', 
num_round=100, 
early_stopping_rounds=10)

xgb.fit(s3_data)

</code></pre>
<p>ClientError: An error occurred (ValidationException) when calling the CreateTrainingJob operation: 3 validation errors detected: Value 's3//sagemaker-us-west-2-034611191912/sagemaker/DEMO-xgboost-dm/output' at 'profilerConfig.s3OutputPath' failed to satisfy constraint: Member must satisfy regular expression pattern: ^(https|s3)://([^/]+)/?(.<em>)$; Value 's3//sagemaker-us-west-2-034611191912/sagemaker/DEMO-xgboost-dm/output' at 'debugHookConfig.s3OutputPath' failed to satisfy constraint: Member must satisfy regular expression pattern: ^(https|s3)://([^/]+)/?(.</em>)$; Value 's3//sagemaker-us-west-2-034611191912/sagemaker/DEMO-xgboost-dm/output' at 'outputDataConfig.s3OutputPath' failed to satisfy constraint: Member must satisfy regular expression pattern: ^(https|s3)://([^/]+)/?(.*)$</p>",1,1,2021-11-30 02:20:31.027000 UTC,,,0,amazon-web-services|amazon-s3|amazon-sagemaker,640,2018-03-22 19:27:05.663000 UTC,2022-05-18 15:32:39.297000 UTC,,125,5,0,20,,,,,,['amazon-sagemaker']
Install SQL SERVER Machine Learning Services (In-Database) Failed,"<p>I followed the SQL Server 2017 Setup wizard to install the Python Feature but completed with failures:<br>
The database engine services has installed successful but Python and Machine Learning Services failed.<br>
Below is the error detail：  </p>

<pre><code>&gt;Error installing Machine Learning Services (In-Database)  
&gt;An error occurred while creating local user account SQLEXPRESS00.   
&gt;Error code: 0x85700005
</code></pre>

<p>And then I check the Summary log it say:</p>

<pre><code>&gt;Detailed results:
  Feature:                       Python
  Status:                        Failed
  Reason for failure:            An error occurred for a dependency of the feature causing the setup process for the feature to fail.
  Next Step:                     Use the following information to resolve the error, uninstall this feature, and then run the setup process again.
  Component name:                Machine Learning Services (In-Database)
  Component error code:          0x85700005
</code></pre>

<p>Dose anyone have a solution?</p>",1,3,2019-12-04 08:22:44.787000 UTC,,,0,sql-server|azure-machine-learning-service,735,2019-03-12 08:42:43.343000 UTC,2022-04-13 14:32:28.777000 UTC,,51,0,0,37,,,,,,['azure-machine-learning-service']
How do I undeploy a model from an endpoint without knowing its id in Vertex AI?,"<p>I have managed to undeploy a model from an endpoint using <code>UndeployModelRequest</code>:</p>
<pre><code>    model_name = f'projects/{project}/locations/{location}/models/{model_id}'
    model_request = aiplatform_v1.types.GetModelRequest(name=model_name)
    model_info = client_model.get_model(request=model_request)
    deployed_models_info = model_info.deployed_models
    deployed_model_id=model_info.deployed_models[0].deployed_model_id       
    
    undeploy_request = aiplatform_v1.types.UndeployModelRequest
                       (endpoint=end_point, deployed_model_id=deployed_model_id)

    client.undeploy_model(request=undeploy_request)
</code></pre>
<p>but all this depends on knowing <code>model_id</code>. I want to be able to just undeploy a model from an endpoint without knowing the model's id (there will only be one model per endpoint ever). Is that possible or can I get the model id from the endpoint somehow?</p>",0,0,2022-09-22 08:49:45.407000 UTC,,,0,google-cloud-platform|google-cloud-vertex-ai|kfp,14,2012-10-25 08:48:34.717000 UTC,2022-09-23 10:10:32.783000 UTC,,2564,304,8,451,,,,,,['google-cloud-vertex-ai']
what is the value of mlflow nested runs?,"<p>What is the value of nested runs in mlflow? I thought it would be that a child run inherits params of the parent, but I dont see that</p>
<pre><code>with mlflow.start_run(run_name='myrun'):
    mlflow.log_param('kl', '0p0')
    mlflow.log_param('name', 'ios')
    mlflow.log_metric('mu', 1.0)
    with mlflow.start_run(run_name='myrun2', nested=True):
        mlflow.log_param('name', 'weighted')        
        mlflow.log_metric('mu', 2.0)
</code></pre>
<p>if I collect the run info in python</p>
<pre><code>df = mlflow.search_runs()
</code></pre>
<p>then we have</p>
<pre><code>df['params.kl']
</code></pre>
<p>giving</p>
<pre><code>0    None
1     0p0
Name: params.kl, dtype: object
</code></pre>",0,0,2020-09-10 00:13:32.703000 UTC,,,5,mlflow,674,2013-04-14 16:23:58.770000 UTC,2022-09-21 20:00:03.557000 UTC,"San Francisco, CA",1879,12,0,147,,,,,,['mlflow']
Sagemaker Endpoint throttling exception,"<p>I have created an endpoint using Sagemaker, and designed my system so that it is called about 100 times <strong>simultaneously</strong>. This seemed to cause <em>'Model error'</em> and take too much time. Do I need to create an endpoint for each event, and make one call per endpoint, instead?</p>",1,0,2019-10-25 12:01:42.170000 UTC,,2019-10-25 16:26:57.223000 UTC,2,endpoint|amazon-sagemaker,2073,2019-10-25 11:56:38.460000 UTC,2020-02-11 11:15:41.053000 UTC,,21,0,0,2,,,,,,['amazon-sagemaker']
Getting error while infering sagemaker endpoint,"<p>I created training job in sagemaker with my own training and inference code using MXNet framework. I am able to train the model successfully and created endpoint as well. But while inferring the model, I am getting the following error:</p>

<p><strong><em>‘ClientError: An error occurred (413) when calling the InvokeEndpoint operation: HTTP content length exceeded 5246976 bytes.’</em></strong></p>

<p>What I understood from my research is the error is due to the size of the image. The image shape is (480, 512, 3). I trained the model with images of same shape (480, 512, 3).</p>

<p>When I resized the image to (240, 256), the error was gone. But producing another error 'shape inconsistent in convolution' as I the trained the model with images of size (480, 512).</p>

<p>I didn’t understand why I am getting this error while inferring.
Can't we use images of larger size to infer the model?
Any suggestions will be helpful</p>

<p>Thanks, Harathi</p>",1,1,2018-05-08 20:25:31.837000 UTC,,,0,python|mxnet|amazon-sagemaker,1317,2018-03-07 23:07:41.813000 UTC,2022-05-10 19:51:16.653000 UTC,,789,5,0,11,,,,,,['amazon-sagemaker']
Log metrics with configuation in Pytorch Lightning using w&b,"<p>I am using PyTorch Lightning together with w&amp;b and trying associate metrics with a finite set of configurations. In the <a href=""https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html"" rel=""nofollow noreferrer""><code>LightningModule</code></a> class I have defined the <code>test_step</code> as:</p>
<pre class=""lang-py prettyprint-override""><code>def test_step(self, batch, batch_idx):
  x, y_true, config_file =  batch
  y_pred = self.forward(x)
  accuracy = self.accuracy(y_pred, y_true)
  self.log(&quot;test/accuracy&quot;, accuracy)
</code></pre>
<p>Assuming (for simplicity) that the batch size is 1, this will log the accuracy for 1 sample and it will be displayed as a chart in the w&amp;b dashboard.</p>
<p>I would like to associate this accuracy with some configuration of the experimental environment. This configuration might include BDP factor, bandwith delay, queue_size, location, etc. I don't want to plot the configurations I just want to be able to filter or group the accuracy by some configuration value.</p>
<p>The only solution I can come up with is to add these configurations as a querystring:</p>
<pre class=""lang-py prettyprint-override""><code>def test_step(self, batch, batch_idx):
  x, y_true, config_file =  batch
  # read values in config file
  # ...

  y_pred = self.forward(x)
  accuracy = self.accuracy(y_pred, y_true)
  self.log(&quot;test/BDP=2&amp;delay=10ms&amp;queue_size=10&amp;topology=single/accuracy&quot;, accuracy)
</code></pre>
<p>Is there a better solution for this that integrates my desired functionality of being able to group and filter by values like BDP?</p>",1,0,2022-03-01 17:14:23.157000 UTC,,2022-03-01 17:37:20.833000 UTC,2,pytorch|metrics|pytorch-lightning|wandb,315,2021-01-01 15:57:22.837000 UTC,2022-09-24 21:04:49.570000 UTC,,2789,102,1,304,,,,,,['wandb']
Import ML Model from ADLS to Azure ML using Databricks,"<p>I have stored one ml model in my ADLS and I want to register the model to Azure ML using databricks. Tried to use the following codes to register my ml model but keep encountering an error that the path cannot be found. I have mount the storage to my databricks.</p>
<pre><code>import urllib.request
from azureml.core.model import Model

# Register a model 
model = Model.register(model_path = 'dbfs:/mnt/machinelearning/classifier.joblib',
                      model_name = &quot;pretrained-classifier&quot;,
                      description = &quot;Pretrained Classifier&quot;,
                       workspace=ws)


</code></pre>
<p>Thanks in advance!</p>",1,1,2022-01-17 10:23:40.303000 UTC,,,1,azure|databricks|azure-databricks|azure-machine-learning-studio|azure-machine-learning-service,113,2019-04-12 07:33:39.330000 UTC,2022-02-08 09:29:56.247000 UTC,,11,0,0,4,,,,,,"['azure-machine-learning-service', 'azure-machine-learning-studio']"
MlFlow: Can't find runs using api,"<p>I try get list of runs, but get empty list.</p>
<p>There are my runs:</p>
<p><a href=""https://i.stack.imgur.com/v8uF2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/v8uF2.png"" alt=""enter image description here"" /></a></p>
<p>But if I try get it using api:
I expect(<a href=""https://mlflow.org/docs/latest/rest-api.html#get-experiment"" rel=""nofollow noreferrer"">by API</a>) that I also watch &quot;runs&quot;, but watch &quot;experiment&quot; only</p>
<pre><code>http://localhost:5000/api/2.0/mlflow/experiments/get?experiment_id=0
</code></pre>
<p><a href=""https://i.stack.imgur.com/78a3Y.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/78a3Y.png"" alt=""enter image description here"" /></a></p>
<p>I read in doc, that &quot;This field is deprecated. Please use the “Search Runs” API to fetch runs within an experiment.&quot;, Ok, I try “Search Runs”</p>
<p><a href=""https://i.stack.imgur.com/EExIn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EExIn.png"" alt=""enter image description here"" /></a></p>
<p>Nothing again.</p>
<p>But I try get run by id(from ui):</p>
<p><a href=""https://i.stack.imgur.com/4eHB7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4eHB7.png"" alt=""enter image description here"" /></a></p>
<p>I need get list of run ids by experiment id. How can I do it?</p>",0,3,2021-09-07 12:20:53.323000 UTC,,,0,python|api|mlflow,306,2018-05-07 14:07:35.273000 UTC,2022-09-24 19:23:37.137000 UTC,"Moscow, Russia",155,34,0,30,,,,,,['mlflow']
How to tell how much an individual job costs,"<p>Is there a way to tell how much an individual training job cost? </p>

<p>I can see the daily / hourly costs in the billing dashboard, which is a good proxy. I looked at the usage report as well, but didn't see a way to get the UsageValues to add up, and din't see tags coming through into the usage report.</p>",1,2,2019-01-30 17:45:27.240000 UTC,,,0,amazon-sagemaker,52,2012-09-23 23:56:52.630000 UTC,2022-09-22 19:11:36.750000 UTC,,346,23,0,22,,,,,,['amazon-sagemaker']
How to log additional single variable with wandb and huggingface transformers,"<p>I am using Huggingface's Transformers Trainer object and I really like the support it has for wandb.</p>
<p>For my use case, I have subclassed the Trainer and, in addition to the values that are logged by default, I would like to keep track of one additional variable that gets updated at each training step.</p>
<p>What is the easiest way to add tracking for a single variable with wandb?</p>",0,1,2022-03-15 03:55:49.707000 UTC,,,1,python|huggingface-transformers|wandb,169,2017-01-19 23:23:49.953000 UTC,2022-06-09 17:18:42.843000 UTC,,507,61,0,50,,,,,,['wandb']
Azure ML batch execution latency,"<p>After reading this post here: <a href=""https://stackoverflow.com/questions/34990561/azure-machine-learning-request-response-latency/35020997#35020997?newreg=18fbd305056d4e6ba239783ebefb9629"">Azure Machine Learning Request Response latency</a>
and the article mentioned in the comments I was wondering if this behavior is also true when a published webservice is called in batch mode. 
Especially since I have read somewhere (sorry, can't find the link at the moment) that the batch calls are not influenced by the ""concurrent calls"" config...</p>

<p>In our scenario we have a custom R module uploaded to our workspace which includes some libraries that are not available on aML by default. The module takes a dataset, trains a binary tree, creates some plots and encodes them in base64 before returning those as a dataset. Locally that does not take more than 5s. But in the aML webservice it takes approx. 90s and it seems that the runtime in batchmode does not improve when calling the service multiple times.</p>

<p>Additionally it would be nice to know for how long the containers, mentioned in the linked post, will stay warm.</p>",0,5,2016-01-28 09:10:29.010000 UTC,,2017-05-23 11:59:15.230000 UTC,0,r|batch-processing|azure-machine-learning-studio,212,2016-01-28 08:51:11.070000 UTC,2016-03-21 13:18:04.110000 UTC,,1,0,0,4,,,,,,['azure-machine-learning-studio']
"AWS Sagemaker init 1K+ models ""endpoints""?","<p>under the assumptions that the model training itself is very fast, I'm wondering what is the best practice to spin up ~ > 1K models endpoints 
fast as possible.</p>

<p>Thanks for any hint
Christian</p>",1,0,2019-01-21 11:20:26.337000 UTC,,2019-01-21 17:41:58.557000 UTC,0,amazon-web-services|amazon-sagemaker,159,2013-07-16 07:44:08.337000 UTC,2022-09-16 12:33:21.773000 UTC,Europa,181,15,0,41,,,,,,['amazon-sagemaker']
Value changed after Azure ML Studio Web Service,"<p>I am using Azure ML Studio in order to predict some values. I have noticed that one of my value was changed when I receive the result from the Web Service. Indeed, I have the following array <strong>[27,7,2018,11,2,4,1]</strong> which become <strong>[27,7,2018,11,2,4,0]</strong>. It is the first time I notice a such comportment. I did not see other value changed in my csv. It occurs all the time with my actual input. I do not know where to start to find the source of the issue.</p>

<p>I tried to read the response that way :</p>

<pre><code>HttpResponseMessage response = await client.PostAsJsonAsync("""", scoreRequest);
if (response.IsSuccessStatusCode)
{
    string result = await response.Content.ReadAsStringAsync();
}
</code></pre>

<p>And that way :</p>

<pre><code>HttpResponseMessage response = await client.PostAsJsonAsync("""", scoreRequest);

if (response.IsSuccessStatusCode)
{
    var tmp3 = await response.Content.ReadAsStreamAsync();
    var tmp4 = ReadFully(tmp3);
    var tmp5 = System.Text.Encoding.UTF8.GetString(tmp4);
}

public static byte[] ReadFully(Stream input)
    {
        byte[] buffer = new byte[16 * 1024];
        using (MemoryStream ms = new MemoryStream())
        {
            int read;
            while ((read = input.Read(buffer, 0, buffer.Length)) &gt; 0)
            {
                ms.Write(buffer, 0, read);
            }
            return ms.ToArray();
        }
    }
</code></pre>

<p>This is the shape of my model on Azure ML (In the top left, top right and bottom python scripts random forest is applied) :
<a href=""https://i.stack.imgur.com/XfIq9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XfIq9.png"" alt=""enter image description here""></a></p>",1,1,2018-07-18 07:03:10.873000 UTC,,2018-07-19 06:57:56.933000 UTC,0,c#|web-services|azure-machine-learning-studio,24,2016-07-17 08:43:00.410000 UTC,2022-09-23 16:04:38.850000 UTC,,334,108,1,20,,,,,,['azure-machine-learning-studio']
"""Sagemaker Notebook with Interactive Session -- Install packages","<p>We have followed this doc to spin up notebook running with interactive sessions. We want to add a few python packages to the environment to assist with development (i.e. pyright). I have added the pip install at the bottom, stopped the instance, restart instance, run &quot;import pyright&quot;, but I get &quot;ModuleNotFoundError: No module named 'pyright'&quot;</p>
<pre><code>#!/bin/bash
set -ex
sudo -u ec2-user -i &lt;&lt;'EOF'
 
ANACONDA_DIR=/home/ec2-user/anaconda3
 
# Create and Activate Conda Env
echo &quot;Creating glue_pyspark conda enviornment&quot;
conda create --name glue_pyspark python=3.7 ipykernel jupyter nb_conda -y
 
echo &quot;Activating glue_pyspark&quot;
source activate glue_pyspark
 
# Install Glue Sessions to Env
echo &quot;Installing AWS Glue Sessions with pip&quot;
pip install aws-glue-sessions
 
# Clone glue_pyspark to glue_scala. This is required because I had to match kernel naming conventions to their environments and couldn't have two kernels in one conda env. 
echo &quot;Cloning glue_pyspark to glue_scala&quot;
conda create --name glue_scala --clone glue_pyspark
 
# Remove python3 kernel from glue_pyspark
rm -r ${ANACONDA_DIR}/envs/glue_pyspark/share/jupyter/kernels/python3
rm -r ${ANACONDA_DIR}/envs/glue_scala/share/jupyter/kernels/python3
 
# Copy kernels to Jupyter kernel env (Discoverable by conda_nb_kernel)
echo &quot;Copying Glue PySpark Kernel&quot;
cp -r ${ANACONDA_DIR}/envs/glue_pyspark/lib/python3.7/site-packages/aws_glue_interactive_sessions_kernel/glue_pyspark/ ${ANACONDA_DIR}/envs/glue_pyspark/share/jupyter/kernels/glue_pyspark/
 
echo &quot;Copying Glue Spark Kernel&quot;
mkdir ${ANACONDA_DIR}/envs/glue_scala/share/jupyter/kernels
cp -r ${ANACONDA_DIR}/envs/glue_scala/lib/python3.7/site-packages/aws_glue_interactive_sessions_kernel/glue_spark/ ${ANACONDA_DIR}/envs/glue_scala/share/jupyter/kernels/glue_spark/
 
echo &quot;Changing Jupyter kernel manager from EnvironmentKernelSpecManager to CondaKernelSpecManager&quot;
JUPYTER_CONFIG=/home/ec2-user/.jupyter/jupyter_notebook_config.py
 
sed -i '/EnvironmentKernelSpecManager/ s/^/#/' ${JUPYTER_CONFIG}
echo &quot;c.CondaKernelSpecManager.name_format='conda_{environment}'&quot; &gt;&gt; ${JUPYTER_CONFIG}
echo &quot;c.CondaKernelSpecManager.env_filter='anaconda3$|JupyterSystemEnv$|/R$'&quot; &gt;&gt; ${JUPYTER_CONFIG}

# Install python modules to env
pip install &quot;pyright&quot;
EOF
systemctl restart jupyter-server  
</code></pre>
<p>Am I missing something in the script? I assumed just &quot;pip install &quot;pyright&quot;&quot; would've worked.</p>
<p>Update:
I have included the following under the pip install aws-glue-sessions:</p>
<blockquote>
<p>pip install &quot;pyright&quot;
and
pip install pyright</p>
</blockquote>
<p>When I check the CloudWatch logs, I see that the package is being downloaded... I would assume it means it's installed.
[1]: <a href=""https://i.stack.imgur.com/JeKce.png"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/JeKce.png</a></p>",0,2,2022-09-20 23:21:16.617000 UTC,,2022-09-21 03:09:59.607000 UTC,0,amazon-web-services|aws-glue|amazon-sagemaker,27,2015-08-17 20:28:43.250000 UTC,2022-09-24 20:02:38.657000 UTC,,137,3,0,35,,,,,,['amazon-sagemaker']
Sagemaker Distributed Data Parallelism not working as expected ( smdistributed.dataparallel.torch.distributed ),"<p>All,</p>
<p>I was trying the AWS sagemaker data parallelism approach for the distributed training ( using the two lib ) from smdistributed.dataparallel.torch.parallel.distributed import DistributedDataParallel as DDP
import smdistributed.dataparallel.torch.distributed as dist although the data is getting divided to all the GPUs(&quot;ml.p3.16xlarge&quot; , 8 Gpus) however the training time is still not getting reduced either with single instance or the double instance.</p>
<p><strong>Earlier we tried with Sagemaker Inbuilt algo Resnet101 for the same data for 100 epoch training time was around 2080 sec  ( batch size - 64 ) which was the benchmark we wanted to improve with our distributed training</strong></p>
<p>Now when we tried distributed training with the distributed data parallelism approach with the same instance we are training for 20 epoch - time is 1600 sec ( batch size - 64) we are training for 20 epoch - time is 1300 sec ( batch size - 128)
we are training for 20 epoch - time is 1063 sec ( batch size - 258).</p>
<p>Even with different batch sizes training time is not improving much.</p>
<p>Train Data - 6016 Images.
Test Data - 745 Images.</p>",1,0,2021-11-11 07:02:33.697000 UTC,,,0,amazon-web-services|pytorch|amazon-sagemaker|distributed-training,165,2021-11-11 06:46:31.787000 UTC,2022-04-19 13:38:48.017000 UTC,,1,0,0,3,,,,,,['amazon-sagemaker']
Deploying Machine Learning model to AzureML Web Service - how to deal with extreme high dimensionality?,"<p>I want to deploy a Sklearn knn model onto Azure ML Web Service according the following tutorial.</p>

<pre><code>from azureml import services
from sklearn.neighbors import KNeighborsClassifier

knn_model = KNeighborsClassifier(n_neighbors=5, n_jobs=-1, algorithm=""auto"")
knn_model.fit(X_train, Y_train)

@services.publish(workspace_id, authorization_token)
@services.types(input_1=int, input_2=int, input_3=int, input_4=int)
@services.returns(int)
def my_knn_predictor(input_1, input_2, input_3, input_4):
    return knn_model.predict([input_1, input_2, input_3, input_4])
</code></pre>

<p>This works indeed, but in the meantime I have to deploy a model where both the input and output dimensionality is very high (magnitude of thousands) so I do not want to type in the <code>input_1, input_2, input_3, input_4 ...</code> variable sequences by hand. Is there a way to extract these as a list from the <code>X_train</code> and <code>Y_train</code> and feed this list into the <code>my_knn_predictor</code> input/output definition? (Simple copy of the data frame headers is not enough because I have to add <code>=int</code> to the column names and remove the quotation marks.)</p>",0,2,2017-11-10 11:59:05.730000 UTC,,,0,python|scikit-learn|azure-machine-learning-studio,133,2016-06-24 18:19:52.047000 UTC,2022-07-19 04:30:48.127000 UTC,,1078,12,1,90,,,,,,['azure-machine-learning-studio']
Extract feature weights of a linear regression in Azure Machine Learning Studio,"<p>Currently, we can only view the feature weights (or coefficient estimates) of a trained linear regression through 'visualize' option but not possible to save this as a table or dataset.
I am experimenting on a market-mix model to understand the incremental sales lift by each media variable, so I need to save the regression estimates.</p>

<p>Is there any workaround for this other than to use 'Execute R' module.</p>

<p><a href=""https://i.stack.imgur.com/Cukw7.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Cukw7.jpg"" alt=""enter image description here""></a></p>",0,2,2018-07-25 14:16:25.210000 UTC,,2018-08-02 08:48:45.057000 UTC,1,r|azure|azure-machine-learning-studio|ml-studio,337,2017-01-17 15:17:50.837000 UTC,2018-12-12 15:51:51.863000 UTC,,63,0,0,15,,,,,,['azure-machine-learning-studio']
Possible to attach Elastic IP to sagemaker notebook instance?,"<p>I want to connect to a database running in different cloud provider and it is exposed publicly.</p>

<p>I need to connect to that database from sagemaker notebook instance.</p>

<p>But the public ip of the sagemaker notebook instance needs to be whitelisted on the other side.</p>

<p>Is it possible to attach elastic ip to sagemaker notebook instance as I don't see any option to attach eip to sagemaker notebook instance?</p>",2,2,2019-11-20 14:10:12.963000 UTC,1.0,,2,amazon-web-services|jupyter-notebook|amazon-sagemaker,708,2013-05-30 13:54:41.203000 UTC,2022-09-23 09:43:32.973000 UTC,"Bangalore, India",3577,2395,20,626,,,,,,['amazon-sagemaker']
Can you edit the tags of a registered model version after the fact using mlflow api?,<p>I am trying to use the mlflow model registry. My plan is to have a monthly scheduled retraining pipeline. I know from reading the documentation that so long as I set the model name to the same string if I save a model/use create_model_version it will create a new version of the model. I also saw from the documentation that I can set the tags associated with a version using create_model_version as well. I want to set tags for valid_to_date and valid_from_date for each version so that if I want to go back in time and backfill predictions with the correct version <em>as of</em> when the data is from I am using the correct model. My initial thought was every time I create a new model version I set the valid_from_date as the date of that model version creation and the valid_to_date as 1-1-2099. Then when I train a new version edit the tag of the previous version valid_to_date from 1-1-2099 to the date it was superceded by a new version. Is that something that can be done using the mlflow python api?</p>,1,0,2022-06-20 14:34:23.570000 UTC,,,0,python|mlflow,108,2012-04-02 17:46:36.950000 UTC,2022-09-23 19:06:28.533000 UTC,,335,7,0,43,,,,,,['mlflow']
How to Organize Training Data for AWS Sagemaker,"<p>I am training a model and the training data uses images for both the source and the label.</p>
<p>For example, <code>image1.jpg =&gt; label_image.jpg</code></p>
<p>The images and their corresponding &quot;label&quot; are in different directories.</p>
<p>So I have images stored like <code>s3://bucket/v1/imgs</code> and their labels stored like <code>s3://bucket/v1/lbls</code>.</p>
<h2>Question</h2>
<p>How do I go about passing this data into an estimator in sagemaker?</p>
<p>I've seen numerous examples, none of which have the data stored in a similar fashion. I've also tried to find the way that sagemaker expects the data to be organized but haven't had much luck.</p>
<p>Any help would be greatly appreaciated.</p>
<p>Thanks!</p>",1,1,2021-11-10 15:18:13.463000 UTC,,,0,amazon-web-services|amazon-s3|amazon-sagemaker|training-data,36,2016-11-07 20:54:55.660000 UTC,2022-09-02 18:25:33.913000 UTC,,1149,132,10,150,,,,,,['amazon-sagemaker']
Unable to open Jupyter Notebook instance on AWS Sagemaker,"<p>I was working on the Notebook instance yesterday and was able to run some scripts successfully, but this morning I was unable to connect to the notebook instance. I keep on getting the loading icon before 504 error appears. Not sure if this is related to memory since I am currently using t2.medium and I can upgrade if that's the case. My only concern with this is that the data shouldn't be lost.</p>

<p>Any help on how I can check what issue is will be appreciated.</p>",1,0,2020-02-13 05:58:25.327000 UTC,,,0,amazon-web-services|amazon-ec2|jupyter-notebook|amazon-sagemaker,1042,2013-10-05 11:29:52.487000 UTC,2022-09-22 10:42:27.737000 UTC,"Pune, Maharashtra, India",180,347,0,48,,,,,,['amazon-sagemaker']
How can I stop sagemaker pipeline pausing my jobs after a day,"<p>I have a long running job that'll probably take over a day to run. It's just collecting initial data. However, after a day the job is paused automatically, I have to go in everyday to hit the resume button in the pipeline execution. How can I stop this from happening?</p>",1,0,2022-03-08 16:25:04.503000 UTC,,,0,pipeline|amazon-sagemaker,111,2018-03-15 15:48:18.623000 UTC,2022-04-05 09:55:22.773000 UTC,,91,2,0,8,,,,,,['amazon-sagemaker']
Unable to configure SageMaker execution Role with access to S3 bucket in another AWS account,"<p><strong>Requirement:</strong> Create SakeMaker GroundTruth labeling job with input/output location pointing to S3 bucket in another AWS account</p>
<p><strong>High Level Steps Followed:</strong>  Lets say, <em>Account_A:</em> SageMaker GroundTruth labeling job and <em>Account_B</em>: S3 bucket</p>
<ol>
<li>Create role <em>AmazonSageMaker-ExecutionRole</em> in <em>Account_A</em> with 3 policies attached:</li>
</ol>
<ul>
<li>AmazonSageMakerFullAccess</li>
<li>Account_B_S3_AccessPolicy: Policy with necessary S3 permissions to access S3 bucket in Account_B</li>
<li>AssumeRolePolicy: Assume role policy for <em>arn:aws:iam::Account_B:role/Cross-Account-S3-Access-Role</em></li>
</ul>
<ol start=""2"">
<li>Create role <em>Cross-Account-S3-Access-Role</em>  in <em>Account_B</em> with 1 policy and 1 trust relationship attached:</li>
</ol>
<ul>
<li>S3_AccessPolicy: Policy with necessary S3 permissions to access S3 bucket in the this Account_B</li>
<li>TrustRelationship: For principal <em>arn:aws:iam::Account_A:role/AmazonSageMaker-ExecutionRole</em></li>
</ul>
<p><strong>Error:</strong> While trying to create SakeMaker GroundTruth labeling job with IAM role as <em>AmazonSageMaker-ExecutionRole</em>, it throws error <em>AccessDenied: Access Denied - The S3 bucket 'Account_B_S3_bucket_name' you entered in Input dataset location cannot be reached. Either the bucket does not exist, or you do not have permission to access it. If the bucket does not exist, update Input dataset location with a new S3 URI. If the bucket exists, give the IAM entity you are using to create this labeling job permission to read and write to this S3 bucket, and try your request again.</em></p>",2,0,2022-06-21 10:01:16.400000 UTC,,,0,amazon-web-services|amazon-s3|amazon-iam|amazon-sagemaker|amazon-ground-truth,210,2012-01-06 11:41:49.283000 UTC,2022-09-23 10:56:37.970000 UTC,,71,2,0,28,,,,,,['amazon-sagemaker']
Azure Machine Learning Results Interpretation,"<p>I try to do an experiment in Azure Machine Learning whith a ""Decision Forest Regression"" Algorythm to predict Weather. 
<a href=""https://i.stack.imgur.com/3gH9b.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/3gH9b.png"" alt=""enter image description here""></a></p>

<p>I use the Weather Dataset that AML Studio suggested me (It's 400K rows of Wheater in a airport). 
<a href=""https://i.stack.imgur.com/JpNdT.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/JpNdT.png"" alt=""enter image description here""></a></p>

<p>I would like to predict the ""DryBulbCelsus"" column (it's values between 20 and 23), so I select the column in the Train Model. I run it everything goes well. 
But the problem is that I don't understand my score model. I have 2 more colums of results ""Score Label Mean"" and ""Score Label Standard Deviation"" with data that I don't understand. 
<a href=""https://i.stack.imgur.com/4uPZO.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/4uPZO.png"" alt=""enter image description here""></a></p>

<p>If someone work with AML and can explain me how I must interprete the data in result. 
Thank you ! </p>",2,4,2016-09-23 08:36:49.300000 UTC,,2016-09-23 11:49:20.947000 UTC,5,azure|regression|azure-machine-learning-studio,1691,2016-09-16 12:26:08.687000 UTC,2020-10-08 16:40:22.363000 UTC,,185,29,0,15,,,,,,['azure-machine-learning-studio']
Categorical Variable in EditMetadata module of Azure ML,<p>Could anyone please let me know what is the purpose of making some variable as Categorical in EditMetadata module of Machine Learning? Would appreciate if explained with some example. Also is it applicable on both features as well as label?</p>,1,0,2016-05-07 07:05:51.650000 UTC,,,1,machine-learning|azure-machine-learning-studio,892,2014-12-22 04:31:22.013000 UTC,2016-09-01 11:17:01.790000 UTC,,11,0,0,8,,,,,,['azure-machine-learning-studio']
MLFlow Azure Blob Storage Artifact upload times out,"<p>I am trying to upload mlflow artifacts to an azure blob storage instance.
The backend server is able to connect to the blob storage and is working fine.
The client is also working fine.
The only issue on the client-side is a time-out while uploading artifacts to the blob storage.
The following exception is thrown:</p>
<pre><code>/venv/lib/python3.9/site-packages/azure/core/pipeline/transport/_requests_basic.py&quot;, line 361, in send
raise error
azure.core.exceptions.ServiceResponseError: ('Connection aborted.', timeout('The write operation timed out'))
</code></pre>
<p>Usually, I would just increase the timeout, but I don't know how to do that for mlflow. I searched for a possible solution <a href=""https://www.mlflow.org/docs/latest/tracking.html#mlflow-tracking-servers"" rel=""nofollow noreferrer"">here</a> as well as checking their GitHub for possible open and/or closed issues regarding the same problem, but I have yet to find a solution. Is it possible to adjust the timeout for the artifact logging in mlflow?</p>",0,0,2022-02-22 18:22:45.417000 UTC,,,1,mlflow,163,2017-11-06 09:28:56.470000 UTC,2022-09-22 05:02:21.253000 UTC,"Portland, Oregon, USA",524,177,51,95,,,,,,['mlflow']
Using the MsticPY library in Azure ML to Open A Data Provider to Log Analytics,"<p>I am using the msticpy Library in Azure ML to open a connection to a Log Analytics workspace.
I keep unable to get connected. I get &quot;Could not connect to kql query provider for loganalytics...&quot;
I have tired using both the CLI login passing cli=locals() as well as msi=locals().</p>
<p>I have made sure the System Managed Identity ML is setup in Identity, and assigned it the Log reader role for the resource group.</p>
<p>has anyone been able to successfully make a connection to a Log Analytics workspace from Azure ML using the msticpy Library?</p>
<p>thank you</p>",0,1,2021-12-03 15:53:26.737000 UTC,,2021-12-14 19:12:09.887000 UTC,0,python|python-3.x|azure|azure-machine-learning-service,64,2017-11-23 16:24:01.753000 UTC,2022-09-22 14:52:56.360000 UTC,,2221,0,0,24,,,,,,['azure-machine-learning-service']
CUDA out of memory when running Bert with Pytorch (Previously worked),"<p>I am building a BERT binary classification on SageMaker using Pytorch.</p>
<p>Previously when I ran the model, I set the Batch size to 16 and the model were able to run successfully. However, yesterday after I stopped SageMaker and restarted the this morning, I can't run the model with Batch size as 16 any more. I am able to run the model with batch size 8.<br />
However, the model is not producing the same result (of course). I didn't change anything else in between. All other settings are the same. (Except I change the SageMaker volume from 30GB to 200GB.)</p>
<p>Does anyone know what may cause this problem? I really want to reproduce the result with batch size 16.</p>
<p>Any answers will help and thank you in advance!</p>",0,4,2020-09-24 20:45:28.767000 UTC,,2020-09-24 21:06:07.627000 UTC,0,pytorch|amazon-sagemaker|bert-language-model|spacy-transformers,349,2020-09-22 03:10:31.770000 UTC,2021-09-28 00:21:34.097000 UTC,,31,0,0,8,,,,,,['amazon-sagemaker']
SQL Server Launchpad fail to start,"<p>I want to use machine learning services in SQL Server database, and now I install machine learning services(In-database) and python successfully, but I cannot run python in SSMS, when I run:</p>

<pre><code>&gt;```SQL  
EXEC sp_execute_external_script  @language =N'Python',
@script=N'
OutputDataSet = InputDataSet;
',
@input_data_1 =N'SELECT 1 AS hello'
WITH RESULT SETS (([hello] int not null));
GO```
</code></pre>

<p>it displays the error as below:</p>

<pre><code>&gt;SQL Server was unable to communicate with the LaunchPad service. Please verify the configuration of 
the service. 
</code></pre>

<p>So I need to start the SQL Server Launchpad in SQL Server Configuration Manager, but fail with the error messages:</p>

<pre><code>&gt;The request failed or the services did not respond in a timely fashion.Consult the event log or 
other applicable error logs for details.
</code></pre>

<p>I don't know where the error log is, but I have check the Event Viewer it just shown a similar info:</p>

<pre><code>&gt;The service did not respond in a timely fashion.
&gt;A timeout was reached (30000 milliseconds) while waiting for the SQL Server Launchpad service to connect. 
</code></pre>

<p>What I have try:<br>
1. Restart the SQL Server multiple times.<br>
2. Make sure the account ""NT SERVICE\MSSQLLaunchpad$SQLEXPRESS"" that log on Launchpad has the necessary permission:<br>
- Adjust memory quotas for a process (SeIncreaseQuotaPrivilege)<br>
- Bypass traverse checking (SeChangeNotifyPrivilege)<br>
- Log on as a service (SeServiceLogonRight)<br>
- Replace a process-level token (SeAssignPrimaryTokenPrivilege)<br>
- Allow log on locally  </p>

<p>My SQL Server Version is :</p>

<blockquote>
  <p>SQL Server 2017 (RTM-CU17) (KB4515579) - 14.0.3238.1 (X64)</p>
</blockquote>",0,2,2019-12-05 04:10:48.160000 UTC,,2019-12-06 01:30:43.177000 UTC,0,sql-server|azure-machine-learning-service|launchpad,1364,2019-03-12 08:42:43.343000 UTC,2022-04-13 14:32:28.777000 UTC,,51,0,0,37,,,,,,['azure-machine-learning-service']
How to load file in sagemaker custom deploy endpoint script,"<p>I am trying to deploy a sentiment analysis model on sagemaker to an endpoint to predict sentiment in real time of an input text. This model will take a single text String as input and return the sentiment.</p>

<p>To train the xgboost model, I followed this <a href=""https://github.com/NadimKawwa/sagemaker_ml/blob/master/SageMaker_IMDB_highlevel.ipynb"" rel=""nofollow noreferrer""> notebook</a> upto step 23. 
This uploaded model.tar.gz to s3 bucket. I additionally uploaded vocabulary_dict generated by sklearn's CountVectorizer(to create bag of words)to s3 bucket as well. </p>

<p>To deploy this pre-trained model, I can use <a href=""https://sagemaker.readthedocs.io/en/stable/using_sklearn.html#deploying-endpoints-from-model-data"" rel=""nofollow noreferrer"">this method</a> and supply an entry point python file predict.py.</p>

<pre><code>sklearn_model = SKLearnModel(model_data=""s3://bucket/model.tar.gz"", role=""SageMakerRole"", entry_point=""predict.py"")
</code></pre>

<p>Documentation says that I have to provide model.tar.gz only as argument and it will be loaded in model_fn. But if I am writing my own model_fn, how do I load the model then? If I put additional files in the same directory as of model.tar.gz in S3, can I load them as well?</p>

<p>Now to do the classification, I will have to vectorize the input text before calling model.predict(bow_vector) in the method predict_fn. In order to do that, I need word_dict which I prepared during pre-processing training data and wrote to s3. </p>

<p>My question is how do I get the word_dict inside the model_fn? Can I load it from s3? 
Below is code for predict.py.</p>

<pre><code>import os
import re
import pickle
import numpy as np
import pandas as pd
import nltk
nltk.download(""stopwords"")
from nltk.corpus import stopwords
from nltk.stem.porter import *
from bs4 import BeautifulSoup
import sagemaker_containers

from sklearn.feature_extraction.text import CountVectorizer



def model_fn(model_dir):

    #TODO How to load the word_dict.
    #TODO How to load the model.
    return model, word_dict

def predict_fn(input_data, model):
    print('Inferring sentiment of input data.')
    trained_model, word_dict = model
    if word_dict is None:
        raise Exception('Model has not been loaded properly, no word_dict.')

    #Process input_data so that it is ready to be sent to our model.

    input_bow_csv = process_input_text(word_dict, input_data)
    prediction = trained_model.predict(input_bow_csv)
    return prediction


def process_input_text(word_dict, input_data):

    words = text_to_words(input_data);
    vectorizer = CountVectorizer(preprocessor=lambda x: x, tokenizer=lambda x: x, word_dict)
    bow_array = vectorizer.transform([words]).toarray()[0]
    bow_csv = "","".join(str(bit) for bit in bow_array)
    return bow_csv

def text_to_words(text):
    """"""
    Uses the Porter Stemmer to stem words in a review
    """"""
    #instantiate stemmer
    stemmer = PorterStemmer()
    text_nohtml = BeautifulSoup(text, ""html.parser"").get_text() # Remove HTML tags
    text_lower = re.sub(r""[^a-zA-Z0-9]"", "" "", text_nohtml.lower()) # Convert to lower case
    words = text_lower.split() # Split string into words
    words = [w for w in words if w not in stopwords.words(""english"")] # Remove stopwords
    words = [PorterStemmer().stem(w) for w in words] # stem
    return words

def input_fn(input_data, content_type):
    return input_data;

def output_fn(prediction_output, accept):
    return prediction_output;
</code></pre>",0,0,2019-08-18 18:04:31.257000 UTC,,,2,python|amazon-web-services|scikit-learn|aws-sdk|amazon-sagemaker,676,2019-08-18 01:53:16.300000 UTC,2019-09-25 20:27:58.817000 UTC,,21,0,0,3,,,,,,['amazon-sagemaker']
Multiprocessing convert S3 content body to pandas DataFrame AWS Sagemaker,"<p>In a conda_python3 notebook of AWS SageMaker, I have defined the following function that turns the S3 object content into a data frame:</p>

<pre><code>import io
import pandas as pd
def readS3Csv(corpus):
    df = pd.read_csv(io.BytesIO(corpus['Body'].read()))
    print(str(corpus) + ' read')
    return(df)
</code></pre>

<p>I tested it with:</p>

<pre><code>corpus1 = s3.get_object(Bucket='XXXX', Key='ZZZZ')
x = readS3Csv(corpus1)
</code></pre>

<p>And it worked well so far. Then I tried to paralellize the conversion of content to DataFrame for the content in different S3 objects:</p>

<pre><code>corpus1 = s3.get_object(Bucket='XXX', Key='QQQ')
corpus2 = s3.get_object(Bucket='XXX', Key='EEE')
corpus3 = s3.get_object(Bucket='XXX', Key='KKK')
corpus4 = s3.get_object(Bucket='XXX', Key='ZZZ')
</code></pre>

<p>I used the multiprocessing library as:</p>

<pre><code>corpus = [corpus1,corpus2,corpus3,corpus4,corpus5,corpus6]
pool = multiprocessing.Pool(processes = 6)

dfs = pool.map(readS3Cvs, corpus)
</code></pre>

<p>I got this error:</p>

<blockquote>
  <p>TypeError: cannot serialize '_io.BufferedReader' object</p>
</blockquote>

<p>Then I tried:</p>

<pre><code>with multiprocessing.Pool() as p:
    print(p.map(readS3Csv, corpus))
</code></pre>

<p>And I still got the same error.</p>

<p>Then I tried including the s3.get object inside the defined function as:</p>

<pre><code>import io
import pandas as pd
import boto3

def readS3Csv(key):
    s3 = boto3.client(
            's3',
            aws_access_key_id='HHH',
            aws_secret_access_key='ZZZ'
        )
    corpus = s3.get_object(Bucket='XXX', Key=key)
    df = pd.read_csv(io.BytesIO(corpus['Body'].read()))
    print(str(key) + ' read')
    return(df)
</code></pre>

<p>And when I run:</p>

<pre><code>keys = ['ttt','uuu','rrr','iii']
dfs = readS3Csv(keys[0])
</code></pre>

<p>I get the dataframe with no erros. But when I do </p>

<pre><code>keys = ['ttt','uuu','rrr','iii']
dfs = pool.map(readS3Csv,keys)
</code></pre>

<p>But I am still getting an error:</p>

<blockquote>
  <p>Reason: 'error(""'i' format requires -2147483648 &lt;= number &lt;= 2147483647"",)'</p>
</blockquote>",1,0,2019-05-21 20:40:44.977000 UTC,,2019-05-23 16:16:34.753000 UTC,0,python-3.x|pandas|amazon-s3|python-multiprocessing|amazon-sagemaker,542,2016-03-19 19:45:07.413000 UTC,2022-08-25 22:03:06.273000 UTC,"Ecuador, Quito",617,74,0,117,,,,,,['amazon-sagemaker']
Train an object detection model using dataset from labeling job,"<p>I've created a public labeling job so people could help me label objects on 50+ images using 8 different classes.
This job is finished, but I'm still unable to run the training job I've created. </p>

<p>Here's how the job is set up:</p>

<ul>
<li>Algorithm: built-in object detection</li>
<li>Input data configuration:

<ul>
<li>Data source: S3</li>
<li>URI: the manifest url generated by the labeling job On S3</li>
</ul></li>
</ul>

<p>I'm getting this error message: ""Missing image files in train channel"". 
Shouldn't it get the images path from the manifest?</p>

<p>What am I missing? </p>",1,0,2019-12-18 18:19:35.373000 UTC,,,0,amazon-sagemaker,75,2012-02-02 09:18:41.837000 UTC,2022-08-05 19:38:30.850000 UTC,"Belo Horizonte - MG, Brasil",7753,98,30,743,,,,,,['amazon-sagemaker']
Read data in Amazon Sagemaker from an Amazon S3 bucket with different credentials,"<p>I'm a beginner to Amazon Sagemaker. My organization has Amazon Sagemaker and some Amazon S3 buckets which I can access with a given key and secret key (say account A). One of our providers has given us read-access to some data in one of their own Amazon S3 buckets using a different key and secret key (say account B). I have verified that I have read-access to the data using boto3 in a jupyter notebook with the keys they provided.</p>
<p>The application is training a CNN using Tensorflow, the data are images (annotations have been provided separately in the form of json files and are not in the bucket). I am aware Amazon Sagemaker has some functionality with Tensorflow but I have never used them. I would like to understand if I can benefit from them in my use case.</p>
<p>Questions:</p>
<ol>
<li><p>While using Amazon Sagemaker in account A, can I (in general) read data from an Amazon S3 bucket in account B without having to copy it locally?</p>
</li>
<li><p>Can I take advantage of Amazon Sagemaker's integration with Tensorflow in my specific use case or is that only useful for buckets in the same account as Amazon Sagemaker?</p>
</li>
<li><p>From the perspective of training the model: Would it make any difference/would it be advisable to duplicate the data by copying the images to an Amazon S3 bucket in account A? There are over 10k high-resolution pictures.</p>
</li>
</ol>",1,0,2021-09-29 08:44:22.273000 UTC,,2021-09-29 14:56:28.950000 UTC,0,amazon-web-services|tensorflow|amazon-s3|amazon-sagemaker,146,2013-12-11 22:50:01.087000 UTC,2022-02-23 14:27:04.423000 UTC,"London, UK",13,0,0,3,,,,,,['amazon-sagemaker']
Use SageMaker Lifecycle configuration to execute a jupyter notebook on start,"<p>I want to set up some automatic schedule for running my SageMaker Notebook.<br />
Currently I found link like this:<br />
<a href=""https://towardsdatascience.com/automating-aws-sagemaker-notebooks-2dec62bc2c84"" rel=""nofollow noreferrer"">https://towardsdatascience.com/automating-aws-sagemaker-notebooks-2dec62bc2c84</a></p>
<p>I followed the steps to set up the lamda, cloudwatch, and the Lifecycle configuration.<br />
During different experiment, some times the on_start lifecycle configuration can execute the jupyter notebook (In the notebook i just install some package and load the package and save the loading status to S3 bucket). However, it failed due to it can't stop the notebook.</p>
<p>Then I added permission to my IAM role for SageMaker autostop. Now the notebook instance can be turn on and turn off. But I don't see anything uploaded to my S3 any more. I am wondering if the on_start started the auto-stop too early before it finish the steps?</p>
<p>Below is my script for the current lifecycle configuration</p>
<pre><code>set -e

ENVIRONMENT=python3
NOTEBOOK_FILE=&quot;/home/ec2-user/SageMaker/Test Notebook.ipynb&quot;
AUTO_STOP_FILE=&quot;/home/ec2-user/SageMaker/auto-stop.py&quot;

source /home/ec2-user/anaconda3/bin/activate &quot;$ENVIRONMENT&quot;

nohup jupyter nbconvert --ExecutePreprocessor.timeout=-1 --ExecutePreprocessor.kernel_name=python3 --execute &quot;$NOTEBOOK_FILE&quot; &amp;

echo &quot;Finishing running the jupyter notebook&quot;

source /home/ec2-user/anaconda3/bin/deactivate

# PARAMETERS
IDLE_TIME=60  # 1 minute

echo &quot;Fetching the autostop script&quot;
wget -O autostop.py https://raw.githubusercontent.com/mariokostelac/sagemaker-setup/master/scripts/auto-stop-idle/autostop.py

echo &quot;Starting the SageMaker autostop script in cron&quot;
(crontab -l 2&gt;/dev/null; echo &quot;*/1 * * * * /bin/bash -c '/usr/bin/python3 $DIR/autostop.py --time ${IDLE_TIME} | tee -a /home/ec2-user/SageMaker/auto-stop-idle.log'&quot;) | crontab -
</code></pre>
<p>Note that, I do see the echo &quot;Finishing running the jupyter notebook&quot; from the cloudwatch log. But that's usually the first thing i saw from the log and it shows up immediately - faster than I expect how long it should take.</p>
<p>Also, currently the notebook is only running some fake task. The real task may take more than an hour.</p>
<p>Any suggestions help! Thank you for taking the time to read my questions.</p>",2,1,2020-10-20 17:02:30.937000 UTC,1.0,,3,python|amazon-web-services|jupyter-notebook|lifecycle|amazon-sagemaker,3101,2020-09-22 03:10:31.770000 UTC,2021-09-28 00:21:34.097000 UTC,,31,0,0,8,,,,,,['amazon-sagemaker']
MLFlow Pytorch Model,"<p>I have a trained Yolo model and is in model.pt format, I am able to upload the model to create an artifact in mlflow. However, when I look at the yaml file it has a few dependencies listed. I am sure that I am loading in the wrong way.</p>
<p>channels:</p>
<ul>
<li>conda-forge
dependencies:</li>
<li>python=3.6.13</li>
<li>pip</li>
<li>pip:
**- mlflow
<ul>
<li>scikit-learn==0.24.2</li>
<li>cloudpickle==1.6.0**
name: mlflow-env</li>
</ul>
</li>
</ul>
<p>Anybody, please let me know how to use pre-trained model to push it to mlflow to create artifact and then containerize dependency(docker) to push to AWS ECR</p>",2,1,2021-09-16 06:54:00.450000 UTC,,,0,amazon-web-services|data-science|amazon-ecr|mlflow|mlops,92,2021-08-31 13:27:02.787000 UTC,2022-02-25 18:47:26.297000 UTC,,1,0,0,4,,,,,,['mlflow']
Azure ML pipeline runs slowly due to AppInsights,"<p>I have set up an Azure ML pipeline with a single Python script step. Provided that the compute has already been spun up, initially the pipeline took around 2 minutes with an <code>executionlogs.txt</code> like so:</p>
<pre><code>[2020-09-23 22:36:14Z] Experiment: &lt;EXPERIMENT&gt;
[2020-09-23 22:36:14Z] Run Id:     &lt;RUN_ID&gt;
[2020-09-23 22:36:14Z] Run target: &lt;RUN_TARGET&gt;
[2020-09-23 22:36:14Z] Starting run in Execution Service
[2020-09-23 22:36:17Z] RunId:[&lt;RUN_ID&gt;] ParentRunId:[&lt;PARENT_RUN_ID&gt;] ComputeTarget:[AmlCompute]
[2020-09-23 22:36:19Z] Job is running, job runstatus is Queued
[2020-09-23 22:36:30Z] Job is running, job runstatus is Running
[2020-09-23 22:38:19Z] Job is running, job runstatus is Running
[2020-09-23 22:38:31Z] Job is running, job runstatus is Finalizing
[2020-09-23 22:38:40Z] Job finished, job RunId is &lt;RUN_ID&gt;
</code></pre>
<p>Now, with the same spun-up compute and code, I'm finding a run time that is 3 times larger:</p>
<pre><code>[2020-10-14 17:14:23Z] Experiment: &lt;EXPERIMENT&gt;
[2020-10-14 17:14:23Z] Run Id:     &lt;RUN_ID&gt;
[2020-10-14 17:14:23Z] Run target: &lt;RUN_TARGET&gt;
[2020-10-14 17:14:23Z] Starting run in Execution Service
[2020-10-14 17:14:26Z] RunId:[&lt;RUN_ID&gt;] ParentRunId:[&lt;PARENT_RUN_ID&gt;] ComputeTarget:[AmlCompute]
[2020-10-14 17:14:31Z] Job is running, job runstatus is Queued
[2020-10-14 17:14:40Z] Job is running, job runstatus is Running
[2020-10-14 17:16:26Z] Job is running, job runstatus is Running
[2020-10-14 17:18:27Z] Job is running, job runstatus is Running
[2020-10-14 17:19:08Z] Job is running, job runstatus is Finalizing
[2020-10-14 17:20:28Z] Job is running, job runstatus is Finalizing
[2020-10-14 17:21:10Z] Job finished, job RunId is &lt;RUN_ID&gt;
</code></pre>
<p>Even 'Finalize' to 'Finish' takes over 2 minutes now! The run times for <code>70_driver_log.txt</code> are similar for both runs.</p>
<p>I identified a difference in the <code>55_azureml-execution-tvmps_....txt</code> log. My slow runs contain the following line related to application insights:</p>
<pre><code>appinsightlogger.go:42: Time Out after 20 second retries for flushing the logs, doing another retry before exiting
</code></pre>
<p>How can I fix this?</p>",0,3,2020-10-14 17:33:18.580000 UTC,,2020-10-14 21:36:56.277000 UTC,1,azure-application-insights|azure-machine-learning-service,222,2020-05-17 18:00:51.347000 UTC,2022-06-27 19:36:47.687000 UTC,,179,2,0,53,,,,,,['azure-machine-learning-service']
Ingesting missing data field into feature group of SageMaker Feature Store,"<p>I have a record which I want to ingest into a feature group of a sagemaker feature store. The feature name 'z' is fractional in definition schema. I have some missing data from feature 'z'. When I try to ingest it, I get errors:</p>
<p>[{'FeatureName': 'ji', 'ValueAsString': '8829a094'}, {'FeatureName': 'time', 'ValueAsString': '2020-08-27T13:00:00Z'}, {'FeatureName': 't2', 'ValueAsString': '289.26111111111106'}, {'FeatureName': 're', 'ValueAsString': '86'}, {'FeatureName': 'pwat', 'ValueAsString': '0.9609375'}, {'FeatureName': 'li700', 'ValueAsString': '3'}, {'FeatureName': 'c', 'ValueAsString': '0'}, {'FeatureName': 'd', 'ValueAsString': '0'}, {'FeatureName': 'x', 'ValueAsString': '0'}, {'FeatureName': 'y', 'ValueAsString': '0.0'}, {'FeatureName': 'z', 'ValueAsString': 'None'}]</p>
<p>Attempted to parse the feature value for the feature named [z] into a FeatureValue of type Fractional. The provided value must be within the range of a double precision floating point number defined by the IEEE 754 standard. The input format can be in either decimal form or scientific notation.</p>
<p>How do you deal with missing data for ingesting into feature groups?</p>",1,0,2021-11-15 18:59:19.730000 UTC,1.0,,2,amazon-web-services|amazon-sagemaker|aws-feature-store,327,2019-03-12 17:23:29.183000 UTC,2022-01-12 15:12:30.017000 UTC,,49,1,0,3,,,,,,['amazon-sagemaker']
Proper Format of Vertex AI AutoML Action Recognition Data Labels,"<p>I'm trying to build an action recognition model in Vertex AI AutoML. I've studied the documentation thoroughly, but so far my model is not able to make any decent predictions in the wild. I've made three attempts so far, and my most recent attempt had a precision-recall curve that could be described as 'respectable', but the predictions are really awful. I'll try to explain my process below as best as I can.</p>
<h2>The Raw Data</h2>
<p>I recorded the same action in 34 ~3 min videos, with the number of actions in each video varying between 30 and 100. The actions themselves take &lt; 1 second. I recorded the data from 4 cameras at multiple angles, and because I was moving around a lot, there was plenty of variance in each action performed. While each raw video contains only one action, there are a total of six classes of action we hope to identify.</p>
<h2>First Model Attempt</h2>
<p>According to the Vertex AI documentation, it's expecting time segments for the actions The annotation JSONL/CSV documentation says as much, but somewhere else in the documentation it says it's expecting the maximal point at which the action is performed if you wish to label the videos inside the console. Anyways, I created a labeling job and my team and I labeled all the time segments for the actions in the videos. The precision-recall curve alluded to some kind of data leakage, and when we inspected the batch prediction results we discovered that it appeared that the model was training on the 0th frame of the time segment. We were careful not to include any frames that weren't part of the action, but due to the nature of the actions, they all essentially start and end at a 'neutral' spot. At seemingly random intervals in the video, multiple or all actions would be labeled, but ONLY in those spots.</p>
<h2>Second Attempt</h2>
<p>We took the annotation data that we had built with the labeling job and chopped up the original videos into a series of subclips. We had all the labels and the time segments, so we did this with a simple script. We did not remove any of the frames of the video, so the neutral position in the beginning and end frames were still present. The precision-recall curve again looked suspicious, but slightly better. Inference in the wild yielded the same results.</p>
<h2>Third Attempt</h2>
<p>After further reviewing the documentation, Vertex AI appeared to contradict itself in what it expects in the data labels:</p>
<blockquote>
<p>When the action starts appearing that you want to identify, slowly
progress through till you find the center or the most representative
moment of the action using &quot;Next frame&quot; option.</p>
</blockquote>
<p>To avoid spending a ton of time on another labeling task (takes us about three days), we labeled a subset of the original subclip dataset according to this information and trained a model to analyze the precision-recall curve. FINALLY something much more respectable. However, the inferences in the wild were still terrible, suffering from the same.</p>
<p>My question is: <strong>do I need to annotate negative action sequences?</strong> In the object tracking or object detection documentation it says that adding a <code>None_of_the_above</code> label would help the model to identify that which it doesn't need to focus on. And again in the action recognition documentation it points out a limitation in the labeling console:</p>
<blockquote>
<p>Limitation: There's a limitation when using the VAR labeling console,
which means if you want to use the labeling tool to label actions, you
must label all the actions in that video.</p>
</blockquote>
<p>I can write a script to fill in the dead space in the video as a negative action sequence, but I'd like to know what the best practice is before going down that route and spending the money to train yet another terrible model.</p>",0,2,2022-08-08 14:53:09.697000 UTC,,,0,google-cloud-platform|computer-vision|automl|google-cloud-automl|google-cloud-vertex-ai,44,2013-02-10 22:40:48.187000 UTC,2022-09-23 16:38:41.943000 UTC,"Miami, FL, USA",45,5,0,22,,,,,,['google-cloud-vertex-ai']
Labeling texts with Amazon Sagemaker Ground Truth using Amazon Mechanical Turk workforce,"<p>Using Amazon SageMaker Ground Truth, is it possible to specify a spanish language fluency for Mechanical Turk workers in order to label texts written in spanish language? If not, how does SageMaker Ground Truth select Mechanical Turk workers to correctly label a text written in a specific language?</p>",1,0,2020-06-05 18:22:34.980000 UTC,,2020-12-01 13:38:36.590000 UTC,1,amazon-web-services|amazon-sagemaker|labeling|amazon-ground-truth,179,2020-06-05 18:10:26.723000 UTC,2022-09-22 17:47:30.453000 UTC,,11,0,0,3,,,,,,['amazon-sagemaker']
Retrieve Sagemaker instance metrics from cloudwatch via CLI or API,"<p>I've got a training job running on Sagemaker. I would like to retrieve instance metrics like MemoryUtilization etc by CLI or boto3 client.</p>

<p>Obviously I can see them in the console. However, I cannot see them in the CLI/API. For example, when running:</p>

<pre><code>aws cloudwatch list-metrics --namespace ""AWS/SageMaker""
</code></pre>

<p>I can see only metrics regarding endpoint invocation but not any training job related metrics.</p>

<p>Any idea?</p>

<p>Thanks!</p>",0,0,2019-12-10 10:56:05.270000 UTC,,,2,amazon-web-services|aws-cli|amazon-cloudwatch|amazon-sagemaker,200,2016-04-25 11:24:42.737000 UTC,2020-12-15 07:37:42.120000 UTC,Israel,169,32,1,39,,,,,,['amazon-sagemaker']
what is the unversioned models in sagemaker?,"<p>I read the sagemaker model registry <a href=""https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_model_package"" rel=""nofollow noreferrer"">doc</a>. It mentioned versioned models and unversioned models. But the doc lacks a clear description of how these two behave differently for model management/deployment.</p>
<p>Here is the definition in the doc</p>
<blockquote>
<p>There are two types of model packages:</p>
<p>Versioned - a model that is part of a model group in the model registry.
Unversioned - a model package that is not part of a model group.</p>
</blockquote>",1,0,2021-02-18 18:27:46.377000 UTC,,,1,machine-learning|amazon-sagemaker,99,2017-10-10 00:57:43.513000 UTC,2022-09-25 05:33:39.507000 UTC,,635,6,5,43,,,,,,['amazon-sagemaker']
Sagemaker neo compile with general pytorch model converted to ONNX,"<p>At this moment, the general pytorch model is still not supported on Neo compilation.</p>
<p>On the other hand, the general MXNet model can be compiled and the general pytorch model can be converted to ONNX format which can be loaded using MXNet.</p>
<p>Therefore, with this process, I think ONNX converted general pytorch model can be compiled with sagemaker neo.</p>
<p>Is there someone who tried this and check for some dependencies while converting and neo compiling?</p>",0,0,2022-01-11 01:08:02.917000 UTC,,,0,pytorch|amazon-sagemaker|onnx,112,2020-12-08 13:53:20.047000 UTC,2022-02-11 05:32:38.613000 UTC,,1,0,0,1,,,,,,['amazon-sagemaker']
(AWS Lambda) An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (415) from model,"<p>I am working on an AWS Lambda function that takes an input image from S3 and invokes a SageMaker endpoint to classify that image. I have been using the AWS docs to come up with my current lambda function which is as follows:</p>
<pre><code>import os
import io
import urllib.parse
import boto3
import json

runtime = boto3.Session().client(service_name='sagemaker-runtime')
s3 = boto3.client('s3')
ENDPOINT_NAME = os.environ['ENDPOINT_NAME']
def lambda_handler(event, context):
    print(&quot;Received event: &quot; + json.dumps(event))
    bucket = event['Records'][0]['s3']['bucket']['name']
    key = urllib.parse.unquote(event['Records'][0]['s3']['object']['key'], encoding='utf-8')
    obj = s3.get_object(Bucket=bucket, Key=key)
    object_content = obj['Body'].read()
    # print(object_content)
    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME, ContentType='application/x-image', Body=bytearray(object_content))
</code></pre>
<p>So from what I understand, lambda receives the bucket and key for the image in the s3 bucket from the test event, and the image is received in the form of JSON. To get it in the form of bytes, you get the value of 'Body' from the JSON and do a read() on it. But when I try to test this, I end up with this error:</p>
<blockquote>
<p>&quot;errorMessage&quot;: &quot;An error occurred (ModelError) when calling the
InvokeEndpoint operation: Received client error (415) from model with
message &quot;{&quot;error&quot;: &quot;Unsupported Media Type:
application/x-image&quot;}&quot;.</p>
</blockquote>
<p>On searching around, I noticed people using &quot;.read().decode('utf-8')&quot; and mentioned that the body of the payload must be in that format. When I try that out, I get this:</p>
<blockquote>
<p>&quot;errorMessage&quot;: &quot;'utf-8' codec can't decode byte 0xff in position 0:
invalid start byte&quot;
Can anyone suggest how to get past either of these errors? Or if what I'm trying is wrong, can someone please guide me in the right way?</p>
</blockquote>",0,0,2021-10-12 13:08:14.607000 UTC,,,0,json|amazon-web-services|amazon-s3|aws-lambda|amazon-sagemaker,351,2018-12-04 09:26:00.983000 UTC,2021-11-14 16:43:20.487000 UTC,,1,0,0,8,,,,,,['amazon-sagemaker']
"How should Pubsub, acting a log sink, fire a function without sending the log?","<p>I have been using <a href=""https://github.com/GoogleCloudPlatform/mlops-with-vertex-ai/blob/main/08-model-monitoring.ipynb"" rel=""nofollow noreferrer"">this</a> example of creating a Vertex AI monitoring job. It sends an email and have adapted it to send a Pubsub message, with @Jose Gutierrez Paliza's help.</p>
<p>I have got this working, sort of. But what seems to be happening is that Pubsub pushes the log to  a function which errors.</p>
<p>My log sink includes:
<a href=""https://i.stack.imgur.com/00ksv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/00ksv.png"" alt=""enter image description here"" /></a></p>
<p>When I look at logs I see an INFO entry:</p>
<pre><code>   my-fn an_id Event data: {&quot;insertId&quot;:&quot;another_id...
</code></pre>
<p>followed by a separate ERROR entry:</p>
<pre><code>...
ValueError: The pipeline parameter insertId is not found in the pipeline job input definitions.
</code></pre>
<p>So I assume Pubsub is sending the log to the function which gets extraneous crap, including <code>insertId</code>.</p>
<p>I can run the pipeline fine via Jupyter:</p>
<pre><code>from google.cloud import pubsub

publish_client = pubsub.PublisherClient()
topic = f'projects/{PROJECT}/topics/{PUBSUB_TOPIC}'
data = {}
message = json.dumps(data)

_ = publish_client.publish(topic, message.encode())
</code></pre>
<p>So how do I the equivalent via Pubsub?</p>",1,1,2022-08-23 15:27:23.880000 UTC,,,1,google-cloud-platform|google-cloud-functions|google-cloud-pubsub|google-cloud-monitoring|google-cloud-vertex-ai,60,2012-10-25 08:48:34.717000 UTC,2022-09-23 10:10:32.783000 UTC,,2564,304,8,451,,,,,,['google-cloud-vertex-ai']
Pycaret MlFlow authentication,"<p>How can I use <code>log_environment = True</code> in Pycaret <code>setup</code> with</p>
<p><code> import os import mlflow mlflow.set_tracking_uri(&quot;https://dagshub.com/BexTuychiev/pet_pawpularity.mlflow&quot;) os.environ[&quot;MLFLOW_TRACKING_USERNAME&quot;] = &quot;MLFLOW_TRACKING_USERNAME&quot; os.environ[&quot;MLFLOW_TRACKING_PASSWORD&quot;] = &quot;MLFLOW_TRACKING_PASSWORD&quot;</code></p>
<p>Without getting  <code>RestException: INTERNAL_ERROR: Response: {'error': 'not found'} </code></p>",1,0,2022-08-16 02:48:10.423000 UTC,,,0,google-colaboratory|mlflow|pycaret|dagshub,35,2016-12-14 15:47:36.640000 UTC,2022-09-23 15:09:37.720000 UTC,Kansas,675,54,0,1816,,,,,,['mlflow']
Web Service deployment failed in Azure ML,"<p>I am trying to deploy a new webservice in Azure ML, When i click on Deploy web service [new] preview. I get the following error:</p>

<p>Web Service deployment failed. This account does not have sufficient access to the Azure subscription that contains the Workspace. In order to deploy a Web Service to Azure, the same account must be invited to the Workspace and be given access to the Azure subscription that contains the Workspace.</p>

<p><a href=""https://i.stack.imgur.com/V4NMD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/V4NMD.png"" alt=""enter image description here""></a></p>

<p>I am an account owner of this azure subscription and i was able to deploy last week with no issues.</p>",1,0,2017-11-06 18:56:52.463000 UTC,,2017-11-06 19:10:32.963000 UTC,0,web-services|azure|machine-learning|azure-machine-learning-studio,406,2011-05-17 14:46:01.000000 UTC,2021-10-19 22:18:41.833000 UTC,"Redmond, WA, USA",3298,49,19,517,,,,,,['azure-machine-learning-studio']
Platform Error: SageMaker pipe channel timed out,"<p>I'm following coursera <a href=""https://www.coursera.org/learn/aws-machine-learning/lecture/lfa8u/amazon-sagemaker-object-detection-on-images-labeled-with-ground-truth"" rel=""nofollow noreferrer"">AWS sage-maker object detect video</a>. When I used my own image data, the training job encounters ""Platform Error: SageMaker pipe channel timed out."" error. I didn't find any info related to this error online. Does anybody know what went wrong? The full log is <a href=""https://drive.google.com/open?id=1M_3E5gdLvNSR967Vc_u-zg6ixHKaGMN1"" rel=""nofollow noreferrer"">here</a>.</p>",1,0,2020-01-28 10:53:30.563000 UTC,,,1,amazon-sagemaker,141,2015-03-14 08:08:49.033000 UTC,2022-08-30 09:04:06.180000 UTC,,481,0,0,32,,,,,,['amazon-sagemaker']
How to make Vertex AI multi-label classification AutoML not ignore texts with no labels?,"<p>I prepared a training dataset for multi-label classification in JSON Lines format as described in <a href=""https://cloud.google.com/vertex-ai/docs/datasets/prepare-text#json-lines_1"" rel=""nofollow noreferrer"">docs</a>.</p>
<p>My upload file looks like</p>
<pre class=""lang-js prettyprint-override""><code>{
  &quot;textContent&quot;: &quot;This text corresponds to 2 labels&quot;,
  &quot;classificationAnnotations&quot;: [
    {&quot;displayName&quot;: &quot;LABEL_1&quot;},
    {&quot;displayName&quot;: &quot;LABEL_2&quot;}
  ]
}
{
  &quot;textContent&quot;: &quot;This text doesn't correspond to any labels&quot;,
  &quot;classificationAnnotations&quot;: []
}
// ... and other 5,853 lines
</code></pre>
<p>Only 1,037 texts have non-empty list of labels.</p>
<p><a href=""https://i.stack.imgur.com/NpmKf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NpmKf.png"" alt=""Training dataset labels"" /></a></p>
<p>Other texts are considered &quot;Unlabeled&quot;. AutoML ignores unlabeled texts.</p>
<p><a href=""https://i.stack.imgur.com/UXEEj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UXEEj.png"" alt=""AutoML training result"" /></a></p>
<p>As a workaround I added an extra label to every text</p>
<pre class=""lang-js prettyprint-override""><code>{
  &quot;textContent&quot;: &quot;This text corresponds to 2 labels&quot;,
  &quot;classificationAnnotations&quot;: [
    {&quot;displayName&quot;: &quot;LABEL_1&quot;},
    {&quot;displayName&quot;: &quot;LABEL_2&quot;},
    {&quot;displayName&quot;: &quot;EXTRA_LABEL&quot;}
  ]
}
{
  &quot;textContent&quot;: &quot;This text doesn't correspond to any labels&quot;,
  &quot;classificationAnnotations&quot;: [
    {&quot;displayName&quot;: &quot;EXTRA_LABEL&quot;}
  ]
}
// ... and other 5,853 texts
</code></pre>
<p>Is there a way to make AutoML use &quot;Unlabeled&quot; texts as texts with 0 labels?</p>",0,1,2022-06-17 15:55:56.500000 UTC,,2022-07-30 16:41:46.813000 UTC,0,google-cloud-vertex-ai,87,2017-01-13 22:43:47.133000 UTC,2022-09-24 18:14:12.057000 UTC,Russia,323,207,0,39,,,,,,['google-cloud-vertex-ai']
No ready replicas for service - azure aks,"<p>I am following the below link to deploy a model to azure Kubernetes service using SDK.
<a href=""https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-azure-kubernetes-service"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-azure-kubernetes-service</a></p>

<p>When I call deployed service, while it is running, either by service.run() or by request.post(). I get this message ""No ready replicas for service"". Then in a while service goes to transitioning state. Any suggestions?</p>",1,0,2020-06-11 06:57:02.833000 UTC,,2020-06-11 12:31:51.800000 UTC,0,python|python-3.x|azure|azure-aks|azure-machine-learning-service,463,2015-03-13 09:18:37.637000 UTC,2021-01-22 14:22:32.703000 UTC,,111,42,0,40,,,,,,['azure-machine-learning-service']
DVC Push KeyError fileSize,"<p>I've added a large list of CSV files to my dvc repository but when I try to do DVC push it complains with</p>
<pre><code>ERROR: unexpected error - KeyError('fileSize')
</code></pre>
<p><strong>Edit</strong>
So searching around it seem that it might help to include the verbose log with regards to the error.</p>
<pre><code>T11:27:08~/documents/*****/data$ dvc push -v
2022-02-01 11:32:13,186 DEBUG: Adding '/home/jhylands/Documents/*****/.dvc/config.local' to gitignore file.
2022-02-01 11:32:13,199 DEBUG: Adding '/home/jhylands/Documents/*****/.dvc/tmp' to gitignore file.
2022-02-01 11:32:13,200 DEBUG: Adding '/home/jhylands/Documents/*****/.dvc/cache' to gitignore file.
2022-02-01 11:32:14,102 DEBUG: Preparing to transfer data from '/home/jhylands/Documents/*****/.dvc/cache' to '*********'
2022-02-01 11:32:14,102 DEBUG: Preparing to collect status from '********'
2022-02-01 11:32:14,103 DEBUG: Collecting status from '*******'
2022-02-01 11:32:14,439 DEBUG: GDrive remote auth with config '{'client_config_backend': 'settings', 'client_config_file': 'client_secrets.json', 'save_credentials': True, 'oauth_scope': ['https://www.googleapis.com/auth/drive', 'https://www.googleapis.com/auth/drive.appdata'], 'save_credentials_backend': 'file', 'save_credentials_file': '/home/jhylands/Documents/*****/.dvc/tmp/gdrive-user-credentials.json', 'get_refresh_token': True, 'client_config': {'client_id': '*****.apps.googleusercontent.com', 'client_secret': '****************', 'auth_uri': 'https://accounts.google.com/o/oauth2/auth', 'token_uri': 'https://oauth2.googleapis.com/token', 'revoke_uri': 'https://oauth2.googleapis.com/revoke', 'redirect_uri': ''}}'.
2022-02-01 11:32:14,994 DEBUG: Estimated remote size: 256 files
2022-02-01 11:32:14,995 DEBUG: Querying '316' hashes via traverse
2022-02-01 11:32:15,325 ERROR: unexpected error - KeyError('fileSize')
------------------------------------------------------------
Traceback (most recent call last):
  File &quot;/home/jhylands/.local/lib/python3.8/site-packages/pydrive2/files.py&quot;, line 226, in __getitem__
    return dict.__getitem__(self, key)
KeyError: 'fileSize'


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;/home/jhylands/.local/lib/python3.8/site-packages/dvc/main.py&quot;, line 55, in main
    ret = cmd.do_run()
  File &quot;/home/jhylands/.local/lib/python3.8/site-packages/dvc/command/base.py&quot;, line 45, in do_run
    return self.run()
  File &quot;/home/jhylands/.local/lib/python3.8/site-packages/dvc/command/data_sync.py&quot;, line 57, in run
    processed_files_count = self.repo.push(
  File &quot;/home/jhylands/.local/lib/python3.8/site-packages/dvc/repo/__init__.py&quot;, line 49, in wrapper
    return f(repo, *args, **kwargs)
  File &quot;/home/jhylands/.local/lib/python3.8/site-packages/dvc/repo/push.py&quot;, line 56, in push
    pushed += self.cloud.push(
  File &quot;/home/jhylands/.local/lib/python3.8/site-packages/dvc/data_cloud.py&quot;, line 85, in push
    return transfer(
  File &quot;/home/jhylands/.local/lib/python3.8/site-packages/dvc/objects/transfer.py&quot;, line 153, in transfer
    status = compare_status(src, dest, obj_ids, check_deleted=False, **kwargs)
  File &quot;/home/jhylands/.local/lib/python3.8/site-packages/dvc/objects/status.py&quot;, line 158, in compare_status
    dest_exists, dest_missing = status(
  File &quot;/home/jhylands/.local/lib/python3.8/site-packages/dvc/objects/status.py&quot;, line 131, in status
    exists.update(odb.hashes_exist(hashes, name=odb.fs_path, **kwargs))
  File &quot;/home/jhylands/.local/lib/python3.8/site-packages/dvc/objects/db/base.py&quot;, line 499, in hashes_exist
    remote_hashes = set(
  File &quot;/home/jhylands/.local/lib/python3.8/site-packages/dvc/objects/db/base.py&quot;, line 334, in _list_hashes_traverse
    yield from itertools.chain.from_iterable(in_remote)
  File &quot;/usr/lib/python3.8/concurrent/futures/_base.py&quot;, line 611, in result_iterator
    yield fs.pop().result()
  File &quot;/usr/lib/python3.8/concurrent/futures/_base.py&quot;, line 439, in result
    return self.__get_result()
  File &quot;/usr/lib/python3.8/concurrent/futures/_base.py&quot;, line 388, in __get_result
    raise self._exception
  File &quot;/usr/lib/python3.8/concurrent/futures/thread.py&quot;, line 57, in run
    result = self.fn(*self.args, **self.kwargs)
  File &quot;/home/jhylands/.local/lib/python3.8/site-packages/dvc/objects/db/base.py&quot;, line 324, in list_with_update
    return list(
  File &quot;/home/jhylands/.local/lib/python3.8/site-packages/dvc/objects/db/base.py&quot;, line 215, in _list_hashes
    for path in self._list_paths(prefix, progress_callback):
  File &quot;/home/jhylands/.local/lib/python3.8/site-packages/dvc/objects/db/base.py&quot;, line 195, in _list_paths
    for file_info in self.fs.find(fs_path, prefix=prefix):
  File &quot;/home/jhylands/.local/lib/python3.8/site-packages/dvc/fs/fsspec_wrapper.py&quot;, line 107, in find
    yield from self.fs.find(path)
  File &quot;/home/jhylands/.local/lib/python3.8/site-packages/pydrive2/fs/spec.py&quot;, line 323, in find
    &quot;size&quot;: int(item[&quot;fileSize&quot;]),
  File &quot;/home/jhylands/.local/lib/python3.8/site-packages/pydrive2/files.py&quot;, line 229, in __getitem__
    raise KeyError(e)
KeyError: KeyError('fileSize')
</code></pre>",0,5,2022-02-01 11:16:50.580000 UTC,,2022-02-01 11:40:42.690000 UTC,1,dvc,54,2012-04-08 18:08:56.530000 UTC,2022-09-24 13:24:30.700000 UTC,"London, United Kingdom",884,185,3,59,,,,,,['dvc']
Training Data Recommendations for Text classification using Azure Machine learning,"<p>We want to work on a text classification problem using Azure machine learning. The problem that we have is client has ONLY approx 1300 rows of data for training. </p>

<p>We have 2 questions
So considering the few rows for training, do we have any recommendation on the training data? 
Any suggestions on how to handle a problem like this in AML?</p>

<p>Attachment shows the training data distribution across 15 categories.</p>

<p><a href=""https://i.stack.imgur.com/jTaQi.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jTaQi.png"" alt=""Data distribution across 15 categories""></a></p>

<p>Update:
we have added all categories with small data volume into a super category and have made the training data to have only 5 categories. Even then we are having an over all accuracy of 30%. Any Ideas on how we can improve overall accuracy of the AML model?</p>",0,1,2018-02-12 12:17:41.847000 UTC,1.0,2018-02-14 06:12:25.800000 UTC,1,azure-machine-learning-studio,151,2016-11-02 07:30:28.857000 UTC,2020-05-29 17:22:31.103000 UTC,,97,0,0,21,,,,,,['azure-machine-learning-studio']
Unable to run mlflow ui in Jupyter,"<p>I am new to MLflow. I was trying to use it in Jupyter. As part of the quickstart, I ran the following code:</p>

<pre><code>import os
from mlflow import log_metric, log_param, log_artifact

if __name__ == ""__main__"":
    # Log a parameter (key-value pair)
    log_param(""param1"", 5)

    # Log a metric; metrics can be updated throughout the run
    log_metric(""foo"", 1)
    log_metric(""foo"", 2)
    log_metric(""foo"", 3)

    # Log an artifact (output file)
    with open(""output.txt"", ""w"") as f:
        f.write(""Hello world!"")
    log_artifact(""output.txt"")
</code></pre>

<p>which ran without any problems. However when I then typed in mlflow ui, I got the error: invalid syntax. What could I be doing wrong?</p>",2,0,2019-07-22 19:37:33.953000 UTC,,2019-07-22 19:39:53.660000 UTC,2,python|mlflow,5356,2017-01-16 15:48:00.953000 UTC,2022-08-25 04:31:34.580000 UTC,,148,9,0,39,,,,,,['mlflow']
How are confidence scores calculated in AWS SageMaker GroundTruth?,"<p>AWS's SageMaker/GroundTruth Labelling jobs return a <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/sms-data-output.html#sms-output-confidence"" rel=""nofollow noreferrer"">confidence score</a> for each human-annotated label.<br />
However, the score is not a direct function of the responses of the N workers who labeled the task.<br />
For example, on tasks with all three workers assigning different labels the score varies (0.61, 0.55, 0.68). And where 2/3 agree, the score varies also (0.95, 0.91).</p>
<p>&quot;Automated data labelling&quot; is disabled, which indicates that all items are labeled by a human, rather than being fully/partially automatically classified.</p>
<p>How does AWS calculate these confidence scores?</p>",1,0,2020-08-07 11:05:20.883000 UTC,,,1,amazon-web-services|amazon-sagemaker,334,2012-08-19 11:22:13.223000 UTC,2022-09-23 15:06:45.677000 UTC,,2661,3314,8,298,,,,,,['amazon-sagemaker']
Exception while Setup SageMaker Domain,"<p>I have created a user and grant three permissions to it.<br />
IAMFullAccess, AmazonSageMakerFullAccess and AmazonS3FullAccess
Now when i try to setup sagemaker domain, it throw following exceptions</p>
<p>ValidationException
Access denied in getting/accepting the portfolio shared by SageMaker. Please call withservicecatalog:AcceptPortfolioShare &amp; servicecatalog:ListAcceptedPortfolioShares permission.</p>
<p>AccessDeniedException
User: arn:aws:iam::117609614511:user/tac-sagemaker is not authorized to perform: sagemaker:CreateDomain on resource: arn:aws:sagemaker:us-east-1:117609614511:domain/d-bpq1nh2g5t9l because no identity-based policy allows the sagemaker:CreateDomain action</p>
<p>First, I did not understand, though i granted full sagmaker access, why its not picking createDomain policy,</p>
<p>Second, i go to my role and manually added these policies.
Here are policies for my sagemaker role
<a href=""https://i.stack.imgur.com/3d8o0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3d8o0.png"" alt=""enter image description here"" /></a>
<strong>AmazonSageMaker-ExecutionPolicy-20220813T004513</strong></p>
<pre><code>{
    &quot;Version&quot;: &quot;2012-10-17&quot;,
    &quot;Statement&quot;: [
        {
            &quot;Action&quot;: [
                &quot;s3:ListBucket&quot;
            ],
            &quot;Effect&quot;: &quot;Allow&quot;,
            &quot;Resource&quot;: [
                &quot;arn:aws:s3:::s34sagemaker&quot;
            ]
        },
        {
            &quot;Action&quot;: [
                &quot;s3:GetObject&quot;,
                &quot;s3:PutObject&quot;,
                &quot;s3:DeleteObject&quot;
            ],
            &quot;Effect&quot;: &quot;Allow&quot;,
            &quot;Resource&quot;: [
                &quot;arn:aws:s3:::s34sagemaker/*&quot;
            ]
        }
    ]
}
</code></pre>
<p><strong>SagemakerCreateDomain</strong></p>
<pre><code>{
    &quot;Version&quot;: &quot;2012-10-17&quot;,
    &quot;Statement&quot;: [
        {
            &quot;Sid&quot;: &quot;VisualEditor0&quot;,
            &quot;Effect&quot;: &quot;Allow&quot;,
            &quot;Action&quot;: &quot;sagemaker:CreateDomain&quot;,
            &quot;Resource&quot;: &quot;arn:aws:sagemaker:*:117609614511:domain/*&quot;
        }
    ]
}
</code></pre>
<p><strong>sagemakerportfolioservices</strong></p>
<pre><code>{
    &quot;Version&quot;: &quot;2012-10-17&quot;,
    &quot;Statement&quot;: [
        {
            &quot;Sid&quot;: &quot;VisualEditor0&quot;,
            &quot;Effect&quot;: &quot;Allow&quot;,
            &quot;Action&quot;: &quot;servicecatalog:AcceptPortfolioShare&quot;,
            &quot;Resource&quot;: &quot;arn:aws:catalog:*:117609614511:portfolio/*&quot;
        },
        {
            &quot;Sid&quot;: &quot;VisualEditor1&quot;,
            &quot;Effect&quot;: &quot;Allow&quot;,
            &quot;Action&quot;: &quot;servicecatalog:ListAcceptedPortfolioShares&quot;,
            &quot;Resource&quot;: &quot;*&quot;
        }
    ]
}
</code></pre>
<p>But still it giving me same error</p>
<p><strong>Edit</strong><br />
I added two new policy to my user
AmazonSageMakerAdmin-ServiceCatalogProductsServiceRolePolicy<br />
AWSServiceCatalogAdminFullAccess</p>
<p>Now ValidationException is gone, but still having AccessDeniedException</p>
<p><strong>EDIT2:</strong>
I have attached a new custom policy</p>
<pre><code>{
    &quot;Version&quot;: &quot;2012-10-17&quot;,
    &quot;Statement&quot;: [
        {
            &quot;Sid&quot;: &quot;VisualEditor0&quot;,
            &quot;Effect&quot;: &quot;Allow&quot;,
            &quot;Action&quot;: [
                &quot;iam:PassRole&quot;,
                &quot;iam:CreateServiceLinkedRole&quot;,
                &quot;sagemaker:CreateDomain&quot;
            ],
            &quot;Resource&quot;: &quot;*&quot;
        }
    ]
}
</code></pre>
<p>But still the error is same</p>",1,1,2022-08-12 20:37:02.817000 UTC,,2022-08-12 21:15:36.310000 UTC,0,amazon-web-services|amazon-iam|amazon-sagemaker,65,2019-03-08 10:46:52.063000 UTC,2022-09-24 19:28:25.140000 UTC,Pakistan,2170,241,14,359,,,,,,['amazon-sagemaker']
Automatic hyperparameter tuning in Sagemaker aws failed to run,"<p>I'm using SageMaker to train my model. And to have better results i'm trying to run the <strong>automatic hyperparameters tuning</strong>. <strong>The training without using this method is running just fine</strong> and giving the result needed, but once I try to run it using this method it gives me and error that is similar to the following error in training jobs (num_filters and learning_rate change):</p>
<pre><code>algorithmerror: ExecuteUserScriptError: Command &quot;/usr/bin/python3 script_unet.py --batch_size 54 --learning_rate 0.0002596573898074083
--model_dir s3://sagemaker-us-east-2-6713267672/tensorflow-training-2020-07-04-10-02-56-198/model/tensorflow-training-200704-1002-002-b7291d39/model --num_filters 46&quot;
</code></pre>
<p>I have tried many other batch sizes just to be sure that it is not a memory problem and It always gives the same error, so i guess it's not.
I need a h5 model extension to use it externally, that is why i'm using that saving lines to a bucket named models-pfe.</p>
<p>The model script i'm using is the following:</p>
<pre><code>#Dependencies:
import argparse, os
import numpy as np

import tensorflow as tf
from keras import backend as K
from keras.models import Model, load_model
from keras.layers import Input
from keras.layers.core import Dropout, Lambda
from keras.layers.convolutional import Conv2D, Conv2DTranspose
from keras.layers.pooling import MaxPooling2D
from keras.layers.merge import concatenate
from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard
from keras.optimizers import Adam
from keras.utils import multi_gpu_model
import boto3
from botocore.exceptions import NoCredentialsError

print(&quot;All the dependencies imported&quot;)


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    
    parser.add_argument('--epochs', type=int, default=60)
    parser.add_argument('--num_filters', type=int, default=32)
    parser.add_argument('--learning_rate', type=float, default=0.0001)
    parser.add_argument('--batch_size', type=int, default=64)
    parser.add_argument('--model_dir', type=str, default='s3://model-pfe')
    parser.add_argument('--training', type=str, default=os.environ['SM_CHANNEL_TRAINING'])
    parser.add_argument('--testing', type=str, default=os.environ['SM_CHANNEL_TESTING'])
    parser.add_argument('--access_key', type=str)
    parser.add_argument('--secret_key', type=str)



    args, _ = parser.parse_known_args()

    epochs       = args.epochs
    num_filters  = args.num_filters
    lr           = args.learning_rate
    batch_size   = args.batch_size
    model_dir    = args.model_dir
    training_dir = args.training
    testing_dir  = args.testing
    access_key  = args.access_key
    secret_key  = args.secret_key



    X_train = np.load(os.path.join(training_dir, 'training.npz'))['image']
    Y_train = np.load(os.path.join(training_dir, 'training.npz'))['label']
    X_test  = np.load(os.path.join(testing_dir, 'testing.npz'))['image']
    Y_test  = np.load(os.path.join(testing_dir, 'testing.npz'))['label']

    # input image dimensions
    img_rows, img_cols = 512,512

    # Tensorflow needs image channels last, e.g. (batch size, width, height, channels)
    K.set_image_data_format('channels_last')
    print(K.image_data_format())



    print('X_train shape:', X_train.shape)
    print(X_train.shape[0], 'train samples')
    print(X_test.shape[0], 'test samples')

    # Normalize pixel values
    X_train   = X_train.astype('float32')
    X_test    = X_test.astype('float32')
    X_train  /= 255
    X_test   /= 255


    # U-Net model
    inputs = Input((512, 512, 3))
    c1 = Conv2D(num_filters, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (inputs)
    c1 = Dropout(0.1) (c1)
    c1 = Conv2D(num_filters, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c1)
    p1 = MaxPooling2D((2, 2)) (c1)

    c2 = Conv2D(num_filters*2, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (p1)
    c2 = Dropout(0.1) (c2)
    c2 = Conv2D(num_filters*2, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c2)
    p2 = MaxPooling2D((2, 2)) (c2)

    c3 = Conv2D(num_filters*4, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (p2)
    c3 = Dropout(0.2) (c3)
    c3 = Conv2D(num_filters*4, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c3)
    p3 = MaxPooling2D((2, 2)) (c3)

    c4 = Conv2D(num_filters*8, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (p3)
    c4 = Dropout(0.2) (c4)
    c4 = Conv2D(num_filters*8, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c4)
    p4 = MaxPooling2D(pool_size=(2, 2)) (c4)

    c5 = Conv2D(num_filters*16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (p4)
    c5 = Dropout(0.3) (c5)
    c5 = Conv2D(num_filters*16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c5)

    u6 = Conv2DTranspose(num_filters*8, (2, 2), strides=(2, 2), padding='same') (c5)
    u6 = concatenate([u6, c4])
    c6 = Conv2D(num_filters*8, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (u6)
    c6 = Dropout(0.2) (c6)
    c6 = Conv2D(num_filters*8, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c6)

    u7 = Conv2DTranspose(num_filters*4, (2, 2), strides=(2, 2), padding='same') (c6)
    u7 = concatenate([u7, c3])
    c7 = Conv2D(num_filters*4, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (u7)
    c7 = Dropout(0.2) (c7)
    c7 = Conv2D(num_filters*4, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c7)

    u8 = Conv2DTranspose(num_filters*2, (2, 2), strides=(2, 2), padding='same') (c7)
    u8 = concatenate([u8, c2])
    c8 = Conv2D(num_filters*2, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (u8)
    c8 = Dropout(0.1) (c8)
    c8 = Conv2D(num_filters*2, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c8)

    u9 = Conv2DTranspose(num_filters, (2, 2), strides=(2, 2), padding='same') (c8)
    u9 = concatenate([u9, c1], axis=3)
    c9 = Conv2D(num_filters, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (u9)
    c9 = Dropout(0.1) (c9)
    c9 = Conv2D(num_filters, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c9)

    outputs = Conv2D(1, (1, 1), activation='sigmoid') (c9)
    model = Model(inputs=[inputs], outputs=[outputs])
    print(model.summary())

    # Use GPUs (for ml.p2.8xlarge = 8 GPUs)
    model = multi_gpu_model(model, gpus=8)

    model.compile(optimizer=Adam(lr=lr), loss='binary_crossentropy', metrics=['accuracy'])
    
    #Fit model
    results = model.fit(X_train, Y_train,
                        validation_data=(X_test, Y_test),
                        batch_size=batch_size,
                        epochs=epochs,
                        verbose=1,
                        shuffle=True)

    
    # Validation evaluation
    score= model.evaluate(X_test, Y_test)
    print('Validation loss    :', score[0])
    print('Validation accuracy:', score[1])

    s3 = boto3.client('s3', aws_access_key_id=access_key, aws_secret_access_key=secret_key)
    
    def upload_to_aws(local_file, bucket, s3_file):
        try:
            s3.upload_file(local_file, bucket, s3_file)
            print(&quot;Upload Successful&quot;)
            return True
        except FileNotFoundError:
            print(&quot;The file was not found&quot;)
            return False
        except NoCredentialsError:
            print(&quot;Credentials not available&quot;)
            return False
    model.save('model.h5')
    upload_to_aws('model.h5','models-pfe',&quot;model.h5&quot;)
</code></pre>
<p>and to run this script on the automatic hyperparameters tuning i'm using the following script:</p>
<pre><code>import sagemaker
sess = sagemaker.Session()
role = sagemaker.get_execution_role()

#My data location in s3
training_input_path=&quot;s3://sagemaker-us-east-2-6713267672/pfe-unet/training/training.npz&quot;
validation_input_path=&quot;s3://sagemaker-us-east-2-6713267672/pfe-unet/validation/testing.npz&quot;


from sagemaker.tensorflow import TensorFlow

tf_estimator = TensorFlow(entry_point='script_unet.py', 
                          role=role,
                          train_instance_count=1, 
                          train_instance_type='ml.p2.8xlarge',
                          framework_version='1.12', 
                          py_version='py3',
                          script_mode=True,
                          hyperparameters={
                              'epochs': 60,
                              'batch_size': 32, 
                              'access_key'   : '',
                              'secret_key'   : ''}
                         )
from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner

hyperparameter_ranges = {
    'num_filters'  : IntegerParameter(32,64),
    'learning_rate': ContinuousParameter(0.0001, 0.005)}

objective_metric_name = 'loss'
objective_type = 'Minimize'
metric_definitions = [{'Name': 'loss','Regex': 'loss = ([0-9\\.]+)'}]

tuner = HyperparameterTuner(tf_estimator,
                            objective_metric_name,
                            hyperparameter_ranges,
                            metric_definitions,
                            max_jobs=6,
                            max_parallel_jobs=1,
                            objective_type=objective_type,
                            early_stopping_type='Auto')

tuner.fit({'training': training_input_path, 'validation': validation_input_path})
</code></pre>
<p>I have changed my bucket name, secret key and access key for security purposes</p>",0,3,2020-07-06 11:11:13.723000 UTC,1.0,2020-07-06 16:19:26.093000 UTC,0,amazon-web-services|amazon-s3|amazon-ec2|deep-learning|amazon-sagemaker,376,2020-06-08 03:36:14.463000 UTC,2020-12-26 01:39:19.783000 UTC,,1,0,0,1,,,,,,['amazon-sagemaker']
Does az ml model deploy --overwrite or az ml service update temporarily put the service into transitioning/ unhealthy state?,"<p>When we update a webservice endpoint, using <code>az ml model deploy .... --overwrite</code> or <code>az ml service update ...</code>, does it temporarily go out of service?
Since AKS is managed Kubernetes cluster, does Azure ensure zero down-time by managing pods by updating them to ensure it is always up and running?</p>",0,0,2021-05-28 08:01:06.033000 UTC,,,1,azure-aks|azure-machine-learning-service,55,2020-10-03 12:46:02.437000 UTC,2022-09-21 15:27:45.773000 UTC,"Bengaluru, Karnataka, India",887,187,32,130,,,,,,['azure-machine-learning-service']
Is it possible to use ssm Session Manager to connect to a sagemaker notebook?,"<p>SageMaker Notebooks do not have an &quot;Official&quot; way for ssh connections, although it is possible to find instructions for it using ngrok.</p>
<p>Is it possible to use session manager instead? Although the SageMaker Notebook looks like an ec2 the arn does not starts in the same way, it has <code>arn:aws:sagemaker</code> instead of <code>arn:aws:ec2</code>.</p>
<p>I tried this via aws cli but I get <code>An error occurred (TargetNotConnected) when calling the StartSession operation:</code></p>",0,0,2021-07-02 11:39:01.860000 UTC,,,3,amazon-web-services|amazon-sagemaker|aws-session-manager,199,2015-12-09 11:24:53.110000 UTC,2022-09-23 20:19:07.053000 UTC,,683,17,1,27,,,,,,['amazon-sagemaker']
sagemaker:CreateTrainingJob on resource: arn:aws:sagemaker:us-east-1 training-job/object-detection-2019-12-10-07-43-06-930 with an explicit deny,"<p>I am using <strong>amazon sagemake</strong>r to perform object detection task and while running the exucution task i am facing this issue. Am i missing any policies in I am role?</p>

<p>the whole error is </p>

<p>SageMaker is not authorized to perform: sagemaker:CreateTrainingJob on resource: arn:aws:sagemaker:us-east-1:576481626755:training-job/object-detection-2019-12-10-07-43-06-930 with an explicit deny</p>",1,1,2019-12-10 08:11:13.507000 UTC,,,0,amazon-web-services|amazon-sagemaker,431,2017-06-04 17:42:26.653000 UTC,2020-10-28 15:43:23.880000 UTC,"Delhi, India",31,0,0,15,,,,,,['amazon-sagemaker']
Vertex API - Failed to build Pipeline Internal Error,<p>I built a dataset using 30 books in .txt format. Loaded them up started training and pipeline build and it kept failing with an &quot;Internal Error&quot;. Does anyone out there have an idea of what is the root cause of this error? I am using the full Vertex AI AutoML preconfigured models as this is a quick demo. Please Advise..</p>,0,0,2022-08-05 13:51:29.087000 UTC,,,0,google-cloud-vertex-ai,8,2022-08-05 13:45:00.253000 UTC,2022-09-12 19:30:38.870000 UTC,,1,0,0,0,,,,,,['google-cloud-vertex-ai']
Is it possible to use data cleansing in SQL Server by using Azure ML?,"<p>I have a SQL Server which I want to run data cleansing every night. To do this I want to use Azure Machine Learning.</p>
<p>To my question; Is it possible to run data cleansing with Azure Machine Learning directly in my SQL Server?</p>",0,0,2022-09-14 09:12:51.947000 UTC,,,0,sql-server|data-cleaning|azure-machine-learning-service,22,2018-05-07 14:04:53.967000 UTC,2022-09-23 07:57:33.580000 UTC,"Uppsala, Sverige",31,1,0,6,,,,,,['azure-machine-learning-service']
How to time tigger a python script in the Azure ML notebooks,"<p>Hi I am currently working on a small image classification project where the model classifies whether the image contains potholes or not. For this section i have wrote the python script, and this script needs to be triggered at scheduled time. I created a scheduled compute instance but the script doesn't get implemented when the compute instance is running. So i want to know what method should i use to get this sorted.</p>",2,2,2022-05-12 10:36:00.120000 UTC,,,0,python|azure-functions|azure-machine-learning-studio|azure-container-instances|azure-notebooks,82,2019-05-01 13:40:46.720000 UTC,2022-09-11 08:42:22.377000 UTC,"1040/3 Athurugiriya Road, Malabe, Sri Lanka",25,14,0,6,,,,,,['azure-machine-learning-studio']
How to train your own model in AWS Sagemaker?,"<p>I just started with AWS and I want to train my own model with own dataset. I have my model as keras model with tensorflow backend in Python. I read some documentations, they say I need a Docker image to load my model. So, how do I convert keras model into Docker image. I searched through internet but found nothing that explained the process clearly. How to make docker image of keras model, how to load it to sagemaker. And also how to load my data from a h5 file into S3 bucket for training? Can anyone please help me in getting clear explanation? </p>",4,0,2018-12-25 10:26:24.547000 UTC,,,0,python|amazon-web-services|tensorflow|keras|amazon-sagemaker,2132,2018-10-24 10:18:27.147000 UTC,2022-09-25 05:25:54.047000 UTC,"Mumbai, Maharashtra, India",852,42,2,338,,,,,,['amazon-sagemaker']
Jupyter notebook in sagemaker kernel keeps dying when I try to load data from s3,"<p>In sagemaker jupyter notebook I run the following code to load data from an s3 bucket.</p>

<pre><code>import boto3
import pandas as pd
from sagemaker import get_execution_role

role = get_execution_role()
bucket='bucketname'
data_key = 'filename'
data_location = 's3://{}/{}'.format(bucket, data_key)

data=pd.read_csv(data_location)
</code></pre>

<p>Then the kernel dies and I get a pop up saying ""The kernel appears to have died. It will restart automatically.""</p>

<p>Is there an easy way to load the data from s3 in sagemaker?</p>",2,7,2019-10-31 14:35:14.580000 UTC,,,4,python|amazon-s3|jupyter-notebook|amazon-sagemaker,5852,2019-09-17 16:39:44.723000 UTC,2021-08-28 14:01:39.740000 UTC,,137,6,0,5,,,,,,['amazon-sagemaker']
Sagemaker Neo compiled model is giving slighthly different results than the actual keras model,"<p>I created a simple Keras regression model and trained it and then saved it in h5 format.
Then in order to deploy on greengrass, i compiled this model using neo and then deployed it and inferenced using lamda function. Final prediction result is slightly different than the prediction of actual keras model.</p>
<p>Model:</p>
<pre><code>import pandas as pd
from keras.models import Sequential
from keras.layers import *

model = Sequential()
model.add(Dense(50, input_dim=9, activation='relu'))
model.add(Dense(100, activation='relu'))
model.add(Dense(50, activation='relu'))
model.add(Dense(1, activation='linear'))
model.compile(loss='mean_squared_error', optimizer='adam')
</code></pre>
<p>Is it an expected behaviour?</p>",1,0,2021-02-23 09:33:16.723000 UTC,,,2,amazon-web-services|machine-learning|keras|amazon-sagemaker|greengrass,135,2011-12-23 16:18:03.847000 UTC,2022-09-20 14:12:39.590000 UTC,Germany,659,72,9,201,,,,,,['amazon-sagemaker']
How can I use modules in Azure ML studio designer pipeline?,"<p>I am currently using a python script in my Azure pipeline</p>
<pre><code>Import data as Dataframe  --&gt;  Run Python Script  --&gt;  Export Dataframe
</code></pre>
<p>My script is developed locally and <strong>I get import errors when trying to import tensorflow</strong>... No problem, guess I just have to add it to environment dependencies somewhere -- and it is here the documentation fails me. They seem to rely on the SDK without touching the GUI, but I am using the designer.</p>
<p>I have at this point already build some enviroments with the dependencies, but utilizing these environments on the run or script level is not obvious to me.</p>
<p>It seems trivial, so any help as to use modules is greatly appreciated.</p>",1,2,2021-02-17 17:33:18.267000 UTC,,,0,azure|azure-devops|azure-machine-learning-service|azureml-python-sdk,283,2017-01-23 13:54:14.243000 UTC,2022-09-21 11:41:07.837000 UTC,,167,25,0,24,,,,,,['azure-machine-learning-service']
Is there a way to disable the default containers on sagemaker studio?,"<p>In sagemaker studio, there is a set of default images provided (e.g: datascience-2.0 for example). While this may be useful in most cases, there may be situations where we may need to disable it for enterprise related reasons. Is it possible?</p>",1,1,2022-07-12 17:30:04.017000 UTC,,,0,amazon-web-services|amazon-sagemaker,23,2022-02-16 03:15:56.940000 UTC,2022-09-23 15:06:17.550000 UTC,,53,0,0,2,,,,,,['amazon-sagemaker']
How to store a .tar.gz formatted model to AWS SageMaker and use it as a deployed model?,"<p>I have a pre-trained BERT model which was trained on Google Cloud Platform, and the model is stored in a .tar.gz formatted file, I wanted to deploy this model to SageMaker and also be able to trigger the model via API, how can I achieve this?</p>
<p>I found <a href=""https://stackoverflow.com/questions/54916866/with-aws-sagemaker-is-it-possible-to-deploy-a-pre-trained-model-using-the-sagem"">this question</a> is a little bit related to what I'm asking here, but it's for a scikit-learn model, I'm new to this area, can someone give me some guidance regarding this? Many thanks.</p>",1,7,2020-11-21 16:37:01.920000 UTC,,,1,amazon-web-services|machine-learning|deployment|amazon-sagemaker|bert-language-model,649,2018-10-30 17:35:56.270000 UTC,2022-09-22 19:30:36.883000 UTC,United Kingdom,2385,1007,16,585,,,,,,['amazon-sagemaker']
How to create seperate mlflow custom models for training and prediction?,"<p>My requirement is to create separate Mlflow custom models for training and prediction.
I want to create training model and use those training model in prediction model</p>",1,0,2022-04-29 06:41:58.370000 UTC,,,0,mlflow,45,2022-04-29 06:10:27.497000 UTC,2022-09-21 07:40:29.710000 UTC,,1,0,0,1,,,,,,['mlflow']
SQL query from SageMaker with Amazon Athena,"<p>I am trying to query my s3 files (JSON format) from SageMaker with Athena. The s3 location looks like this: s3://name/year=2017/month=01/day=01. Here are stored some  JSON files which I want to query with Athena. I created a connection variable, next in the variable connection.execute(""query"") I run the query to create the table.</p>

<p>query:</p>

<pre><code>CREATE EXTERNAL TABLE testing (
id int

)    PARTITIONED BY (day string) ROW FORMAT  serde 'org.apache.hive.hcatalog.data.JsonSerDe' with serdeproperties ( 'paths'='id' ) LOCATION 's3://name/2018/07/';"""""")
</code></pre>

<p>Next I run the sql query:</p>

<pre><code>df = pd.read_sql(""SELECT * FROM default.testing WHERE day='01' LIMIT 10 "", conn)
</code></pre>

<p>After that I print the DataFrame but I get a 0 value instead. </p>

<p>The JSON looks like this:</p>

<pre><code>{""shoe_number"":43,""country_id"":6.. etc
</code></pre>

<p>What am I missing?</p>",1,0,2018-08-18 12:03:39.817000 UTC,,,0,amazon-web-services|amazon-athena|amazon-sagemaker,881,2017-10-10 10:27:11.830000 UTC,2021-09-24 15:27:28.810000 UTC,,161,14,0,34,,,,,,['amazon-sagemaker']
How to set SAGEMAKER_SUBMIT_DIRECTORY environment variable in sagemaker,"<p>I am trying to deploy a model that i have registered. I registered the model using the following code:</p>
<pre><code>step_register = RegisterModel(
    name=&quot;RegisterCustomModel&quot;,
    estimator=estimator,
    model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,
    content_types=[&quot;text/csv&quot;],
    response_types=[&quot;text/csv&quot;],
    inference_instances=[&quot;ml.t2.medium&quot;, &quot;ml.m5.large&quot;],
    transform_instances=[&quot;ml.m5.large&quot;],
    model_package_group_name=model_package_group_name,
    approval_status=model_approval_status,
    model_metrics=model_metrics,
)
</code></pre>
<p>However, I am getting an error when i deploy this model which I believe is because the environment variable <code>SAGEMAKER_SUBMIT_DIRECTORY</code> is not set.</p>
<p>My question is, can I set the environment variable <code>SAGEMAKER_SUBMIT_DIRECTORY</code> in the <code>RegisterModel</code> function and if I can, how do I do that?</p>",0,2,2021-11-08 14:09:01.730000 UTC,,,0,amazon-sagemaker,444,2018-03-22 16:45:24.657000 UTC,2022-09-23 14:29:34.747000 UTC,Milton Keynes,738,22,7,69,,,,,,['amazon-sagemaker']
How can I feed outputed augmented manifest file as input to blazingtext in a pipeline?,"<p>I'm creating a pipeline with multiple steps</p>
<p>One to preprocess a dataset and the other one takes the preprocessed one as an input to train a BlazingText model for classification</p>
<p>My first <code>ProcessingStep</code> outputs augmented manifest files</p>
<pre><code>step_process = ProcessingStep(
name=&quot;Nab3Process&quot;,
processor=sklearn_processor,
inputs=[
  ProcessingInput(source=raw_input_data, destination=raw_dir),
  ProcessingInput(source=categories_input_data, destination=categories_dir)
],
outputs=[
    ProcessingOutput(output_name=&quot;train&quot;, source=train_dir),
    ProcessingOutput(output_name=&quot;validation&quot;, source=validation_dir),
    ProcessingOutput(output_name=&quot;test&quot;, source=test_dir),
    ProcessingOutput(output_name=&quot;mlb_train&quot;, source=mlb_data_train_dir),
    ProcessingOutput(output_name=&quot;mlb_validation&quot;, source=mlb_data_validation_dir),
    ProcessingOutput(output_name=&quot;mlb_test&quot;, source=mlb_data_test_dir),
    ProcessingOutput(output_name=&quot;le_vectorizer&quot;, source=le_vectorizer_dir),
    ProcessingOutput(output_name=&quot;mlb_vectorizer&quot;, source=mlb_vectorizer_dir)
],
code=preprocessing_dir)
</code></pre>
<p>But I'm having a hard time when I try to feed my <code>train</code> output as a <code>TrainingInput</code> to the model step to use it to train.</p>
<pre><code>step_train = TrainingStep(
name=&quot;Nab3Train&quot;,
estimator=bt_train,
inputs={
    &quot;train&quot;: TrainingInput(
        step_process.properties.ProcessingOutputConfig.Outputs[
            &quot;train&quot;
        ].S3Output.S3Uri,
        distribution=&quot;FullyReplicated&quot;,
        content_type=&quot;application/x-recordio&quot;,
        s3_data_type='AugmentedManifestFile',
        attribute_names=['source', 'label'],
        input_mode='Pipe',
        record_wrapping='RecordIO'
    ),
    &quot;validation&quot;: TrainingInput(
        step_process.properties.ProcessingOutputConfig.Outputs[
            &quot;validation&quot;
        ].S3Output.S3Uri,
        distribution=&quot;FullyReplicated&quot;,
        content_type='application/x-recordio',
        s3_data_type='AugmentedManifestFile',
        attribute_names=['source', 'label'],
        input_mode='Pipe',
        record_wrapping='RecordIO'
    )
})
</code></pre>
<p>And I'm getting the following error</p>
<pre><code>'FailureReason': 'ClientError: Could not download manifest file with S3 URL &quot;s3://sagemaker-us-east-1-xxxxxxxxxx/Nab3Process-xxxxxxxxxx/output/train&quot;. Please ensure that the bucket exists in the selected region (us-east-1), that the manifest file exists at that S3 URL, and that the role &quot;arn:aws:iam::xxxxxxxxxx:role/service-role/AmazonSageMakerServiceCatalogProductsUseRole&quot; has &quot;s3:GetObject&quot; permissions on the manifest file. Error message from S3: The specified key does not exist.'
</code></pre>
<p>What Should I do?</p>
<p>[EDIT]</p>
<p>I made sure the role has the permissions, and the file exists in the required path in the required bucket, I even used the training files generated by a failed pipeline run as a static input to the model training process in a new pipeline run and it did well, the problem here is that the training step needs a file path, but the preprocessing step is outputting a directory path.</p>
<p>the traininbg step works with a path like this one</p>
<p>&quot;s3://sagemaker-us-east-1-xxxxxxxxxx/Nab3Process-xxxxxxxxxx/output/train/train.json&quot;</p>
<p>but not like this one</p>
<p>&quot;s3://sagemaker-us-east-1-xxxxxxxxxx/Nab3Process-xxxxxxxxxx/output/train&quot;</p>",0,4,2022-03-14 13:12:13.503000 UTC,,2022-03-16 02:02:58.340000 UTC,0,amazon-web-services|machine-learning|pipeline|amazon-sagemaker,81,2020-04-03 16:51:57.853000 UTC,2022-07-05 13:13:41.167000 UTC,,1,0,0,3,,,,,,['amazon-sagemaker']
Unable to download artifacts from FTP server using MLFLOW,"<p>I'm not able to load my sklearn model using <code>mlflow.sklearn.load_model</code>. Internally, <code>mlflow</code> uses the function <code>_download_artifact_from_uri</code> from the module <code>mlflow.tracking.artifact_utils</code>.</p>

<p>If I try, to download an entire artifact folder I receive the following error message: <code>PermissionError: [Errno 13] Permission denied: '/0'</code>. </p>

<p>If I try to retrieve a single file from an artifact folder I do not get the error message, and I'm able to create a folder using the <code>os</code> module. </p>

<p>The following is the converted jupyter notebook I've used.</p>

<pre class=""lang-py prettyprint-override""><code>import os

import mlflow
from mlflow.tracking.artifact_utils import _download_artifact_from_uri
</code></pre>

<pre class=""lang-py prettyprint-override""><code>mlflow.set_tracking_uri(""file:mlruns"")
</code></pre>

<pre class=""lang-py prettyprint-override""><code>artifact_uri = 'ftp://user:pass@ftp/0/69a874f1f8a6474cae6bca5b3b5f9ffc/artifacts'
</code></pre>

<pre class=""lang-py prettyprint-override""><code>model_uri = 'ftp://user:pass@ftp/0/25f46678f1d44842910f185672ca852c/artifacts/linear model/model/MLmodel'
</code></pre>

<pre class=""lang-py prettyprint-override""><code>_download_artifact_from_uri(artifact_uri, ""./mlruns"")
</code></pre>

<pre><code>---------------------------------------------------------------------------

PermissionError                           Traceback (most recent call last)

&lt;ipython-input-14-834201128eef&gt; in &lt;module&gt;
----&gt; 1 _download_artifact_from_uri(artifact_uri, ""./mlruns"")


/opt/conda/lib/python3.7/site-packages/mlflow/tracking/artifact_utils.py in _download_artifact_from_uri(artifact_uri, output_path)
     73 
     74     return get_artifact_repository(artifact_uri=root_uri).download_artifacts(
---&gt; 75         artifact_path=artifact_path, dst_path=output_path)


/opt/conda/lib/python3.7/site-packages/mlflow/store/artifact/artifact_repo.py in download_artifacts(self, artifact_path, dst_path)
    135         # Check if the artifacts points to a directory
    136         if self._is_directory(artifact_path):
--&gt; 137             return download_artifact_dir(artifact_path)
    138         else:
    139             return download_file(artifact_path)


/opt/conda/lib/python3.7/site-packages/mlflow/store/artifact/artifact_repo.py in download_artifact_dir(dir_path)
    116                 for file_info in dir_content:
    117                     if file_info.is_dir:
--&gt; 118                         download_artifact_dir(dir_path=file_info.path)
    119                     else:
    120                         download_file(file_info.path)


/opt/conda/lib/python3.7/site-packages/mlflow/store/artifact/artifact_repo.py in download_artifact_dir(dir_path)
    116                 for file_info in dir_content:
    117                     if file_info.is_dir:
--&gt; 118                         download_artifact_dir(dir_path=file_info.path)
    119                     else:
    120                         download_file(file_info.path)


/opt/conda/lib/python3.7/site-packages/mlflow/store/artifact/artifact_repo.py in download_artifact_dir(dir_path)
    118                         download_artifact_dir(dir_path=file_info.path)
    119                     else:
--&gt; 120                         download_file(file_info.path)
    121             return local_dir
    122         if not os.path.exists(dst_path):


/opt/conda/lib/python3.7/site-packages/mlflow/store/artifact/artifact_repo.py in download_file(fullpath)
    101             local_file_path = os.path.join(dst_path, fullpath)
    102             if not os.path.exists(local_dir_path):
--&gt; 103                 os.makedirs(local_dir_path)
    104             self._download_file(remote_file_path=fullpath, local_path=local_file_path)
    105             return local_file_path


/opt/conda/lib/python3.7/os.py in makedirs(name, mode, exist_ok)
    209     if head and tail and not path.exists(head):
    210         try:
--&gt; 211             makedirs(head, exist_ok=exist_ok)
    212         except FileExistsError:
    213             # Defeats race condition when another thread created the path


/opt/conda/lib/python3.7/os.py in makedirs(name, mode, exist_ok)
    209     if head and tail and not path.exists(head):
    210         try:
--&gt; 211             makedirs(head, exist_ok=exist_ok)
    212         except FileExistsError:
    213             # Defeats race condition when another thread created the path


/opt/conda/lib/python3.7/os.py in makedirs(name, mode, exist_ok)
    209     if head and tail and not path.exists(head):
    210         try:
--&gt; 211             makedirs(head, exist_ok=exist_ok)
    212         except FileExistsError:
    213             # Defeats race condition when another thread created the path


/opt/conda/lib/python3.7/os.py in makedirs(name, mode, exist_ok)
    209     if head and tail and not path.exists(head):
    210         try:
--&gt; 211             makedirs(head, exist_ok=exist_ok)
    212         except FileExistsError:
    213             # Defeats race condition when another thread created the path


/opt/conda/lib/python3.7/os.py in makedirs(name, mode, exist_ok)
    219             return
    220     try:
--&gt; 221         mkdir(name, mode)
    222     except OSError:
    223         # Cannot rely on checking for EEXIST, since the operating system


PermissionError: [Errno 13] Permission denied: '/0'
</code></pre>

<pre class=""lang-py prettyprint-override""><code>_download_artifact_from_uri(model_uri, ""./mlruns"")
</code></pre>

<pre><code>'/home/jovyan/notebooks/mlruns/MLmodel'
</code></pre>

<pre class=""lang-py prettyprint-override""><code>os.mkdir(""mlruns/0"")
</code></pre>

<pre class=""lang-py prettyprint-override""><code>
</code></pre>",0,1,2020-01-20 15:52:16.323000 UTC,1.0,2020-07-02 10:20:55.693000 UTC,4,python|scikit-learn|mlflow,1099,2020-01-20 15:44:58.393000 UTC,2022-02-17 05:56:26.410000 UTC,"Copenhagen, Denmark",41,0,0,6,,,,,,['mlflow']
data format to predict with model fitted via Sagemaker's XGBoost built-in algorithm and training container,"<p>Looking at the following code, taken from <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html"" rel=""nofollow noreferrer"">here</a>, I wonder what format dtest is (sorry I could not gleen this from the post):</p>
<pre><code>import pickle as pkl 
import tarfile

t = tarfile.open('model.tar.gz', 'r:gz')
t.extractall()

model = pkl.load(open(model_file_path, 'rb'))

# prediction with test data
pred = model.predict(dtest)
</code></pre>
<p>In my case the training and validation data are in csv format coming from a S3 bucket:</p>
<pre><code>content_type = &quot;csv&quot;
train_input = TrainingInput(&quot;s3://{}/{}/{}/&quot;.format(bucket, prefix, 'train'), content_type=content_type)
</code></pre>
<p>So ideally, I would also like to use the same format for scoring/prediction/inference.</p>
<p>PS:</p>
<p>This little function appears to work fine:</p>
<pre><code>def write_prediction_data(data_file_name, target_name, model_file_name, output_file_name):

    model = pkl.load(open(model_file_name, 'rb'))
    data = pd.read_csv(data_file_name) 
    target = data[target_name]
    data = data.drop([target_name], axis=1)
    xgb_data = xgb.DMatrix(data.values, target.values)

    data = pd.read_csv(data_file_name)
    data['Prediction'] = model.predict(xgb_data)

    data.to_csv(output_file_name, index=False)
</code></pre>
<p>Improvement suggestions always welcome (-:</p>",1,0,2022-04-29 07:25:35.783000 UTC,,2022-04-30 10:05:38.547000 UTC,0,python|xgboost|amazon-sagemaker,65,2010-03-01 10:53:04.443000 UTC,2022-09-24 18:56:19.313000 UTC,Somewhere,15705,2171,91,2150,,,,,,['amazon-sagemaker']
SageMaker Estimator fit job never ends,"<p>I have the following code</p>
<pre><code>estimator = Estimator(                                                     
    image_uri=ecr_image,                                                   
    role=role,                                                             
    instance_count=1,                                                      
    instance_type=instance_type,                                           
    hyperparameters=hyperparameters                                        
)                                                                          

estimator.fit({&quot;training&quot;: &quot;s3://&quot; + sess.default_bucket() + &quot;/&quot; + prefix})
</code></pre>
<p>which seems to run smoothly until it is stuck at:</p>
<pre><code>Finished Training
2020-12-02 15:00:45,352 sagemaker-training-toolkit INFO     Reporting training SUCCESS
</code></pre>
<p>and I see InProgress job in AWS SageMaker console. How can I fix this?</p>
<p>I use <code>763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-inference-eia:1.3.1-cpu-py36-ubuntu16.04</code> Docker image with <code>pip install sagemaker-training</code> added.</p>",0,5,2020-12-02 15:08:44.283000 UTC,,2020-12-02 15:18:43.817000 UTC,1,amazon-sagemaker,252,2013-10-28 16:49:44.190000 UTC,2022-09-24 09:44:05.480000 UTC,,1311,149,0,49,,,,,,['amazon-sagemaker']
Notebook restarts when making predictions,"<p>My regression model is already trained, and saved using joblib, and when I load it in order to get y_predicted eventually the notebook restarts and I don't see any error on CloudWatch.</p>
<p>When I run:</p>
<pre><code>y_predicted = model.predict(x_test)
</code></pre>
<p>Here is what I get on CloudWatch:</p>
<pre><code>[I 17:43:34.145 NotebookApp] KernelRestarter: restarting kernel (1/5), keep random ports

2022-07-03T19:43:34.310+02:00


kernel 97adefc6-bb3a-469c-8c4a-91681e6eb64a restarted
kernel 97adefc6-bb3a-469c-8c4a-91681e6eb64a restarted

2022-07-03T19:43:34.310+02:00   kernel 97adefc6-bb3a-469c-8c4a-91681e6eb64a restarted

2022-07-03T19:43:34.310+02:00   kernel 97adefc6-bb3a-469c-8c4a-91681e6eb64a restarted

2022-07-03T19:43:34.310+02:00   kernel 97adefc6-bb3a-469c-8c4a-91681e6eb64a restarted

2022-07-03T19:43:34.310+02:00   kernel 97adefc6-bb3a-469c-8c4a-91681e6eb64a restarted

2022-07-03T19:43:34.310+02:00   kernel 97adefc6-bb3a-469c-8c4a-91681e6eb64a restarted

2022-07-03T19:43:38.343+02:00   kernel 97adefc6-bb3a-469c-8c4a-91681e6eb64a restarted
</code></pre>
<p>Edit: I even took a smaller part of <code>y_test</code> to see if it works, but just restarts the notebook faster than using the entire y_test, I think it is about having too many unique values on 1 feature (about 7k) and I applied one hot encoding.</p>",0,4,2022-07-03 17:51:17.850000 UTC,,2022-07-04 15:21:45.030000 UTC,0,python|amazon-web-services|regression|amazon-sagemaker|jupyter-lab,34,2020-04-17 01:31:21.620000 UTC,2022-09-24 12:53:17.577000 UTC,"Caracas, Venezuela",113,25,0,25,,,,,,['amazon-sagemaker']
How to autoscale a SKLearn job on sagemaker,"<p>I want to launch a SKLearn job using <code>sagemaker</code>. The way I do this is as follows:</p>
<pre><code>from sagemaker.sklearn.estimator import SKLearn

FRAMEWORK_VERSION = '0.23-1' 
script_path = 'main.py'

sklearn = SKLearn(
    entry_point=os.path.join(script_path),
    framework_version=FRAMEWORK_VERSION,
    instance_type='ml.m5.2xlarge',
    source_dir='src',
    output_path='my/output/path',
)
</code></pre>
<p>I am not sure if the instance_type that I have chosen is enough (in terms of memory etc) for my application though.</p>
<p>Is there a way to &quot;let sagemaker&quot; decide on the instance type ?</p>
<p>Or, is there a way to choose an instance_type and if along the way it is about to run out of memory, the sagemaker to automatically scale up ?</p>",1,1,2021-12-14 10:32:24.663000 UTC,,,0,python|python-3.x|amazon-web-services|amazon-sagemaker,37,2016-02-01 14:54:20.480000 UTC,2022-09-24 18:36:48.790000 UTC,,3527,352,6,440,,,,,,['amazon-sagemaker']
Logging SKLearn Models in the same folder while running multiple models in Pandas UDF,"<p>I am trying to run multiple XGBoost models and save the resulting models in the form of experiments. However, when I call the UDF function on my pyspark dataframe the models are being saved in a multiple folders.</p>
<p>It appears that they are being randomly split in smaller batches and stored. Is there a way to ensure that all models are saved in the same run/ folder such that I can easily load them back later.</p>
<pre><code>def classification_xgb(df):
  #modeling code
  mlflow.sklearn.log_model(xgb, model_name)


dat_m.groupBy(&quot;Product&quot;).applyInPandas(classification_xgb, schema).show(10000,False)
</code></pre>
<p>I have over 100 products for which I need to create models and save in the same run instance.</p>",0,0,2021-07-03 03:43:38.387000 UTC,,2021-07-03 04:19:08.293000 UTC,1,pandas|pyspark|azure-databricks|sklearn-pandas|mlflow,79,2021-07-03 03:24:39.850000 UTC,2022-01-04 15:00:59.047000 UTC,,11,0,0,3,,,,,,['mlflow']
"Unable to import mlflow, getting ModuleNotFoundError: No module named 'mlflow'","<p>Unable to import <code>mlflow</code> in a .py script.</p>
<pre><code>ModuleNotFoundError: No module named 'mlflow'
</code></pre>
<p>The script runs in a <code>python:3.7-stretch Docker</code> container</p>
<p>Use <code>requirements.txt</code> to pip install packages.</p>
<pre><code>(...)
sqlalchemy==1.4.1
psycopg2==2.8.6
mlflow==1.18.0
</code></pre>
<pre><code>RUN pip3 install --default-timeout=5000 --use-deprecated=legacy-resolver -r /root/requirements.txt
</code></pre>
<p>Can see that it is installed.</p>
<pre><code>root@abc:~# pip uninstall mlflow
Found existing installation: mlflow 1.18.0
Uninstalling mlflow-1.18.0:
  Would remove:
    /usr/local/bin/mlflow
    /usr/local/lib/python3.7/site-packages/mlflow-1.18.0.dist-info/*
    /usr/local/lib/python3.7/site-packages/mlflow/*
Proceed (y/n)? n
</code></pre>
<p>Can do an import from python shell.</p>
<pre><code>root@abc:~# python
Python 3.7.10 (default, Feb 16 2021, 19:46:13)
[GCC 6.3.0 20170516] on linux
Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt; import mlflow
&gt;&gt;&gt;
</code></pre>
<p>But no joy when running from .py script.</p>
<p>Other packages installed from <code>requirements.txt</code> can be imported.</p>
<p>Any ideas what is wrong ?</p>",0,0,2021-07-09 15:24:42.463000 UTC,,2022-04-18 13:17:55.787000 UTC,0,python-3.x|docker|machine-learning|python-module|mlflow,814,2016-04-28 07:45:51.257000 UTC,2022-09-23 15:07:24.783000 UTC,,375,241,0,26,,,,,,['mlflow']
mlflow log_model need to capture runId to use in mlflow models serve,"<p>runId generated in log_model call needs to be accessed in mlflow models serve</p>

<p>I am trying to run mlflow bare minimum to deploy custom models</p>

<p>1st step taken : I save the model using log_model
observation: the artifacts are duly saved in mlruns</p>

<p>2nd step taken: i am able to serve using mlflow models serve -m runs:
observation: the server is started at 5000</p>

<p>3rd step taken: i am able to run a curl invocation to predict
observation: prediction returned</p>

<p>Question : How do i get the runId generated in Step1 to be passed to Step2
ie does the log_model</p>

<p>Please advise the recommended workflow for the above use case (whether tracking/mlflow server) need to be used etc..</p>

<pre><code>mlflow.pyfunc.log_model(artifact_path=""artifacts"", python_model=add5_model)
</code></pre>

<p>Question: how to access the runId returned by the above log_model to call in mlflow models serve -m runs</p>",2,0,2019-09-04 06:30:37.513000 UTC,1.0,2019-09-04 08:05:13.430000 UTC,0,mlflow,566,2016-03-02 23:33:22.137000 UTC,2020-06-24 05:15:18.443000 UTC,,9,0,0,4,,,,,,['mlflow']
AWS sagemaker with fbprophet algorithm,"<p>I am trying to generate some realtime predictions using fbProhet,  AWS sagemaker .
Here what I am trying.</p>
<ol>
<li><p>created a dockerfile which will install fbprohet library and copy the myfile.py file and added
ENTRYPOINT [&quot;python&quot;, &quot;./myfile.py&quot;], where myfile.py contains the code to analyze the sample and upload the result as .csv to s3.</p>
</li>
<li><p>created docker image using the above file and pushed to ECR</p>
</li>
<li><p>created a training job in sagemaker by referring the above docker image.</p>
</li>
<li><p>created model from the training job</p>
</li>
<li><p>created endpoint using the model</p>
</li>
</ol>
<p>But it is getting failed as it couldn't find and model artifact in S3.  and to test I have created a .zip file and uploaded to s3 bucket. but still the endpoint is getting failed as there is a ping error &quot;the default variant did not pass the ping health check&quot;.</p>
<p>I am not sure what I am missing.</p>
<p>the dockerfile is:</p>
<pre><code>FROM python:3-slim
SHELL [&quot;/bin/bash&quot;, &quot;-c&quot;]

RUN apt-get update &amp;&amp; apt-get install -y wget &amp;&amp; apt-get install -y curl &amp;&amp; apt-get install -y git &amp;&amp; apt-get clean &amp;&amp; rm -rf /var/lib/apt/lists/*

RUN curl -L https://github.com/pyenv/pyenv-installer/raw/master/bin/pyenv-installer | bash
ENV PYENV_ROOT $HOME/.pyenv
ENV PATH $PYENV_ROOT/shims:$PYENV_ROOT/bin:$PATH
RUN wget --quiet https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O /tmp/install_miniconda.sh &amp;&amp; \
 /bin/bash /tmp/install_miniconda.sh -b -p /opt/conda
ENV PATH=/opt/conda/bin:$PATH
RUN pip install --no-cache-dir --upgrade \
    pip \
    setuptools \
    wheel

RUN conda install -c conda-forge Prophet

COPY myfile.py .
RUN chmod +x myfile.py
COPY requirement.txt .
RUN python -m pip install -r requirement.txt

ENTRYPOINT [&quot;python&quot;, &quot;./myfile.py&quot;
</code></pre>
<p>and myfile is :</p>
<pre><code>import boto3
import logging
import json
import base64
import pandas as pd
import plotly.express as px
import numpy as np
import sqlite3
from sqlite3 import Error
from time import time
from datetime import datetime
from datetime import timedelta

from configparser import ConfigParser
from sqlalchemy.engine.url import URL
from sqlalchemy import create_engine

from botocore.exceptions import ClientError
import configparser
from prophet import Prophet

def test():
    bucket = 'bucket-test'
    file = 'test.xlsx'
    df = pd.read_excel(f&quot;s3://{bucket}/{file}&quot;)
    df.columns.values
    column1=(df.columns.values[1])
    column2=(df.columns.values[10])
    parsedTimestampColumn=(df[column1])
    parsedMetricsColumn=(df[column2])
    data={'ps':parsedTimestampColumn , 'pd':parsedMetricsColumn}
    df1 = pd.DataFrame(data)
    
    m = Prophet(yearly_seasonality=True)
    m.fit(df1)
    future = m.make_future_dataframe(periods=500,freq='H')
    forecast = m.predict(future)
    forecast[['ps', 'w', 'w_l', 'w_u']].tail()
    fig1 = m.plot(forecast)
    fig2 = m.plot_components(forecast)
    
    results=pd.concat([df1[['pd']],forecast[['ps', 'w', 'w_l', 'w_u']]],axis=1)
    results['error']=results['y']-results['w']
    results[&quot;uncertainity&quot;]=results['w_u']-results['w_l']
    results[results['error'].abs()&gt; 1.2*results['uncertainity']]
    results['Anomalies']=results.apply(lambda x:'Yes' if (np.abs(x['error'])&gt;1.2*x['uncertainity'] )else 'No',axis=1)
    result_Dataframe = pd.DataFrame(results)
    print(result_Dataframe.tail())
    return result_Dataframe
     

if __name__==&quot;__main__&quot;:
  test()

</code></pre>
<p>can any one guide me what I am missing here to create a successful model and endpoint ?</p>",0,6,2022-05-19 19:09:49.657000 UTC,2.0,2022-05-19 20:28:46.387000 UTC,1,python|amazon-web-services|docker|amazon-sagemaker,178,2020-07-26 17:36:21.563000 UTC,2022-09-15 14:20:33.707000 UTC,,51,5,0,14,,,,,,['amazon-sagemaker']
Sagemaker invoke endpoint not returning prediction,"<p>I made a model on the titanic problem and deployed it on sagemaker. </p>

<p>I'm using the following code to send an input:</p>

<pre><code>import boto3
import pandas as pd
import io
import numpy as np

sm=boto3.client(""runtime.sagemaker"",region_name='us-east-1')

def np2csv(arr):
    csv = io.BytesIO()
    np.savetxt(csv, arr, delimiter=',', fmt='%g')
    return csv.getvalue().decode().rstrip()

test_X=np.array([2,22,1,0,40,1,0,1])
payload = np2csv(test_X)
sm.invoke_endpoint(
      EndpointName='xgboost-2018-03-07-11-28-07-434',
      Body=payload,
      ContentType='text/x-libsvm',
      Accept='Accept')
</code></pre>

<p>I get the following output:</p>

<pre><code>{'Body': &lt;botocore.response.StreamingBody at 0x7fddc0dc1278&gt;,
 'ContentType': 'text/csv; charset=utf-8',
 'InvokedProductionVariant': 'AllTraffic',
 'ResponseMetadata': {'HTTPHeaders': {'connection': 'keep-alive',
   'content-length': '119',
   'content-type': 'text/csv; charset=utf-8',
   'date': 'Sun, 11 Mar 2018 11:01:52 GMT',
   'x-amzn-invoked-production-variant': 'AllTraffic',
   'x-amzn-requestid': 'e9061dcc-3dab-44df-8d98-cda861288176'},
  'HTTPStatusCode': 200,
  'RequestId': 'e9061dcc-3dab-44df-8d98-cda861288176',
  'RetryAttempts': 0}}
</code></pre>

<p>Am I doing something wrong? How do I get the actual prediction, I'm expecting a 0 or 1 in my predicted class</p>",1,0,2018-03-11 11:15:41.787000 UTC,,2018-03-11 21:52:20.610000 UTC,0,amazon-web-services|amazon-sagemaker,1115,2015-01-25 18:26:40.603000 UTC,2022-09-15 13:02:40.407000 UTC,"Gurgaon, Haryana, India",719,31,1,172,,,,,,['amazon-sagemaker']
SageMaker's FensorFlow Training Job - how to change script archive path?,"<p>Maybe someone has encountered such a problem: when creating a Training Job in SageMaker by the sagemaker.tensorflow.TensorFlow class, I need to set a parameter that is responsible for saving the training script code in S3, for its path to be precise: <em><strong>sagemaker_submit_directory</strong></em> or maybe <em><strong>module_dir</strong></em> - I am not sure. By default it looks like this:</p>
<pre><code>s3: //bucket/job-name/source/sourcedir.tar.gz. 
</code></pre>
<p>I need to put something between bucket name and job-name. How can I do it? I tried to add such parameter to hyperparameters, but then my Training Job was crushing (unfortunately it was impossible to read the whole error message because the stack trace was larger than a limit and I could not see it fully)?</p>
<p>Any idea?</p>",1,1,2020-09-22 16:19:50.033000 UTC,,,0,python|amazon-web-services|tensorflow|amazon-sagemaker,301,2017-08-19 09:44:48.407000 UTC,2020-10-04 18:34:31.527000 UTC,"Wrocław, Polska",39,0,0,8,,,,,,['amazon-sagemaker']
