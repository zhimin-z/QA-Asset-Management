Question_title,Question_body,Question_answer_count,Question_comment_count,Question_creation_date,Question_favorite_count,Question_last_edit_date,Question_score,Question_tags,Question_view_count,Owner_creation_date,Owner_last_access_date,Owner_location,Owner_reputation,Owner_up_votes,Owner_down_votes,Owner_views,Answer_body,Answer_comment_count,Answer_creation_date,Answer_last_edit_date,Answer_score,Question_valid_tags
How to schedule tasks on SageMaker,<p>I have a notebook on SageMaker I would like to run every night. What's the best way to schedule this task. Is there a way to run a bash script and schedule Cron job from SageMaker?</p>,5,0,2018-03-30 22:37:21.927000 UTC,3.0,,14,amazon-web-services|jupyter-notebook|amazon-sagemaker,17339,2014-12-31 04:45:02.893000 UTC,2021-11-29 22:53:47.930000 UTC,,173,15,0,25,"<p>Amazon SageMaker is a set of API that can help various machine learning and data science tasks. These API can be invoked from various sources, such as CLI, <a href=""https://aws.amazon.com/tools/"" rel=""noreferrer"">SDK</a> or specifically from schedule AWS Lambda functions (see here for documentation: <a href=""https://docs.aws.amazon.com/lambda/latest/dg/with-scheduled-events.html"" rel=""noreferrer"">https://docs.aws.amazon.com/lambda/latest/dg/with-scheduled-events.html</a> )</p>

<p>The main parts of Amazon SageMaker are notebook instances, training and tuning jobs, and model hosting for real-time predictions. Each one has different types of schedules that you might want to have. The most popular are:</p>

<ul>
<li><strong>Stopping and Starting Notebook Instances</strong> - Since the notebook instances are used for interactive ML models development, you don't really need them running during the nights or weekends. You can schedule a Lambda function to call the <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/API_StopNotebookInstance.html"" rel=""noreferrer"">stop-notebook-instance</a> API at the end of the working day (8PM, for example), and the <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/API_StartNotebookInstance.html"" rel=""noreferrer"">start-notebook-instance</a> API in the morning. Please note that you can also run crontab on the notebook instances (after opening the local terminal from the Jupyter interface).</li>
<li><strong>Refreshing an ML Model</strong> - Automating the re-training of models, on new data that is flowing into the system all the time, is a common issue that with SageMaker is easier to solve. Calling <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateTrainingJob.html"" rel=""noreferrer"">create-training-job</a> API from a scheduled Lambda function (or even from a <a href=""https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html"" rel=""noreferrer"">CloudWatch Event</a> that is monitoring the performance of the existing models), pointing to the S3 bucket where the old and new data resides, can <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateModel.html"" rel=""noreferrer"">create a refreshed model</a> that you can now deploy into an <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/API_UpdateEndpointWeightsAndCapacities.html"" rel=""noreferrer"">A/B testing environment</a> .</li>
</ul>

<p>----- UPDATE (thanks to @snat2100 comment) -----</p>

<ul>
<li><strong>Creating and Deleting Real-time Endpoints</strong> - If your realtime endpoints are not needed 24/7 (for example, serving internal company users working during workdays and hours), you can also <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateEndpoint.html"" rel=""noreferrer"">create the endpoints</a> in the morning and <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/API_DeleteEndpoint.html"" rel=""noreferrer"">delete them</a> at night. </li>
</ul>",2.0,2018-04-08 18:45:15.337000 UTC,2019-10-04 14:51:41.000000 UTC,19.0,['amazon-sagemaker']
How to update a previous run into MLFlow?,"<p>I would like to update previous runs done with MLFlow, ie. changing/updating a parameter value to accommodate a change in the implementation. Typical uses cases:</p>
<ul>
<li>Log runs using a parameter A, and much later, log parameters A and B. It would be useful to update the value of parameter B of previous runs using its default value.</li>
<li>&quot;Specialize&quot; a parameter. Implement a model using a boolean flag as a parameter. Update the implementation to take a string instead. Now we need to update the values of the parameter for the previous runs so that it stays consistent with the new behavior.</li>
<li>Correct a wrong parameter value loggued in the previous runs.</li>
</ul>
<p>It is not always easy to trash the whole experiment as I need to keep the previous runs for statistical purpose. I would like also not to generate new experiments just for a single new parameter, to keep a single database of runs.</p>
<p>What is the best way to do this?</p>",2,2,2020-10-05 13:04:07.533000 UTC,2.0,2020-12-12 16:01:03.850000 UTC,6,logging|data-science|mlflow,2834,2012-09-10 21:25:47.147000 UTC,2022-09-24 18:14:33.217000 UTC,,1022,1127,19,66,"<p>To add or correct a parameter, metric or artifact of an existing run, pass run_id instead of experiment_id to mlflow.start_run function</p>
<pre><code>with mlflow.start_run(run_id=&quot;your_run_id&quot;) as run:
    mlflow.log_param(&quot;p1&quot;,&quot;your_corrected_value&quot;)
    mlflow.log_metric(&quot;m1&quot;,42.0) # your corrected metrics
    mlflow.log_artifact(&quot;data_sample.html&quot;) # your corrected artifact file
</code></pre>
<p>You can correct, add to, or delete any MLflow run any time after it is complete. Get the run_id either from the UI or by using <a href=""https://www.mlflow.org/docs/latest/python_api/mlflow.html#mlflow.search_runs"" rel=""noreferrer"">mlflow.search_runs</a>.</p>
<p>Source: <a href=""https://towardsdatascience.com/5-tips-for-mlflow-experiment-tracking-c70ae117b03f"" rel=""noreferrer"">https://towardsdatascience.com/5-tips-for-mlflow-experiment-tracking-c70ae117b03f</a></p>",3.0,2020-12-02 14:45:49.240000 UTC,,10.0,['mlflow']
How to use azureml.core.runconfig.DockerConfiguration class in azureml.core.Environment or azureml.core.ScriptRunConfig class,"<p>I use Microsoft Azure Machine Learning (Azure-ml) to run my (python) experiments.</p>
<p>For specifying the VM and python environment I use:</p>
<pre><code>from azureml.core import Environment
from azureml.core import ScriptRunConfig

# Other imports and code...

# Specify VM and Python environment:
vm_env = Environment.from_conda_specification(name='my-test-env', file_path=PATH_TO_YAML_FILE)
vm_env.docker.enabled = True
vm_env.docker.base_image = 'mcr.microsoft.com/azureml/openmpi3.1.2-cuda10.2-cudnn7-ubuntu18.04'

# Finally, use the environment in the ScriptRunConfig:
src = ScriptRunConfig(source_directory=DEPLOY_CONTAINER_FOLDER_PATH,
                      script=SCRIPT_FILE_TO_EXECUTE,
                      arguments=EXECUTE_ARGUMENTS,
                      compute_target=compute_target,
                      environment=vm_env)
</code></pre>
<p>I get the following warning for the line <code>vm_env.docker.enabled = True</code>:</p>
<pre><code>'enabled' is deprecated. Please use the azureml.core.runconfig.DockerConfiguration object with the 'use_docker' param instead.
</code></pre>
<p>The documentation about the <a href=""https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.environment.dockersection?view=azure-ml-py"" rel=""noreferrer""><code>DockerSection Class</code></a> and <a href=""https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.runconfig.dockerconfiguration?view=azure-ml-py"" rel=""noreferrer""><code>DockerConfiguration Class</code></a> is not very clear about applying the <code>DockerConfiguration Class</code>.</p>
<p><strong>I can't figure out how to use the <code>azureml.core.runconfig.DockerConfiguration</code> object. Can someone provide me with an example? Thank you!</strong></p>",2,0,2021-05-04 14:59:03.337000 UTC,2.0,2021-05-10 07:10:19.490000 UTC,8,python|azure-machine-learning-service|azureml-python-sdk,2662,2017-02-15 12:44:09.613000 UTC,2022-09-24 19:22:12.820000 UTC,,755,3010,0,245,"<p>The <code>ScriptRunConfig</code> class now accepts a <code>docker_runtime_config</code> argument, which is where you pass the <code>DockerConfiguration</code> object.</p>
<p>So, the code would look something like this:</p>
<pre><code>from azureml.core import Environment
from azureml.core import ScriptRunConfig
from azureml.core.runconfig import DockerConfiguration

# Other imports and code...

# Specify VM and Python environment:
vm_env = Environment.from_conda_specification(name='my-test-env', file_path=PATH_TO_YAML_FILE)
vm_env.docker.base_image = 'mcr.microsoft.com/azureml/openmpi3.1.2-cuda10.2-cudnn7-ubuntu18.04'

docker_config = DockerConfiguration(use_docker=True)

# Finally, use the environment in the ScriptRunConfig:
src = ScriptRunConfig(source_directory=DEPLOY_CONTAINER_FOLDER_PATH,
                      script=SCRIPT_FILE_TO_EXECUTE,
                      arguments=EXECUTE_ARGUMENTS,
                      compute_target=compute_target,
                      environment=vm_env,
                      docker_runtime_config=docker_config)
</code></pre>",3.0,2021-05-06 14:23:42.350000 UTC,2021-05-10 09:22:15.880000 UTC,12.0,['azure-machine-learning-service']
What's the difference between regular and ml AWS EC2 instances?,"<p>I'm experimenting with <a href=""https://aws.amazon.com/sagemaker/"" rel=""nofollow noreferrer"">AWS Sagemaker</a> using a Free Tier account. According to the <a href=""https://aws.amazon.com/sagemaker/pricing/"" rel=""nofollow noreferrer"">Sagemaker pricing</a>, I can use 50 hours of m4.xlarge and m5.xlarge instances for training in the free tier. (I am safely within the two-month limit.) But when I attempt to train an algorithm with the XGBoost container using m5.xlarge, I get the error shown below the code.</p>
<p>Are the ml-type and non-ml-type instances the same with just a fancy prefix for those that one would use with Sagemaker or are they entirely different? The <a href=""https://aws.amazon.com/ec2/instance-types/"" rel=""nofollow noreferrer"">EC2 page</a> doesn't even list the ml instances.</p>
<pre><code>sess = sagemaker.Session()

xgb = sagemaker.estimator.Estimator(container,
                                    role, 
                                    instance_count=1, 
                                    instance_type='m5.xlarge',
                                    output_path=output_location,
                                    sagemaker_session=sess)
</code></pre>
<blockquote>
<p>ClientError: An error occurred (ValidationException) when calling the
CreateTrainingJob operation: 1 validation error detected: Value
'm5.xlarge' at 'resourceConfig.instanceType' failed to satisfy
constraint: Member must satisfy enum value set: [ml.p2.xlarge,
ml.m5.4xlarge, ml.m4.16xlarge, ml.p4d.24xlarge, ml.c5n.xlarge,
ml.p3.16xlarge, ml.m5.large, ml.p2.16xlarge, ml.c4.2xlarge,
ml.c5.2xlarge, ml.c4.4xlarge, ml.c5.4xlarge, ml.c5n.18xlarge,
ml.g4dn.xlarge, ml.g4dn.12xlarge, ml.c4.8xlarge, ml.g4dn.2xlarge,
ml.c5.9xlarge, ml.g4dn.4xlarge, ml.c5.xlarge, ml.g4dn.16xlarge,
ml.c4.xlarge, ml.g4dn.8xlarge, ml.c5n.2xlarge, ml.c5n.4xlarge,
ml.c5.18xlarge, ml.p3dn.24xlarge, ml.p3.2xlarge, ml.m5.xlarge,
ml.m4.10xlarge, ml.c5n.9xlarge, ml.m5.12xlarge, ml.m4.xlarge,
ml.m5.24xlarge, ml.m4.2xlarge, ml.p2.8xlarge, ml.m5.2xlarge,
ml.p3.8xlarge, ml.m4.4xlarge]</p>
</blockquote>",2,0,2020-12-13 13:13:01.387000 UTC,1.0,2020-12-15 13:34:25.823000 UTC,5,amazon-web-services|amazon-ec2|amazon-sagemaker,7690,2017-04-25 08:35:17.327000 UTC,2022-09-24 18:42:49.843000 UTC,,1152,20,8,113,"<p>The instances with the <code>ml</code> prefix are instance classes specifically for use in Sagemaker.</p>
<p>In addition to being used within the Sagemaker service, the instance will be running an AMI with all the necessary libraries and packages such as Jupyter.</p>",7.0,2020-12-13 13:18:12.453000 UTC,,12.0,['amazon-sagemaker']
How to use a pretrained model from s3 to predict some data?,"<p>I have trained a semantic segmentation model using the sagemaker and the out has been saved to a s3 bucket. I want to load this model from the s3 to predict some images in sagemaker. </p>

<p>I know how to predict if I leave the notebook instance running after the training as its just an easy deploy but doesn't really help if I want to use an older model.</p>

<p>I have looked at these sources and been able to come up with something myself but it doesn't work hence me being here:</p>

<p><a href=""https://course.fast.ai/deployment_amzn_sagemaker.html#deploy-to-sagemaker"" rel=""noreferrer"">https://course.fast.ai/deployment_amzn_sagemaker.html#deploy-to-sagemaker</a>
<a href=""https://aws.amazon.com/getting-started/tutorials/build-train-deploy-machine-learning-model-sagemaker/"" rel=""noreferrer"">https://aws.amazon.com/getting-started/tutorials/build-train-deploy-machine-learning-model-sagemaker/</a></p>

<p><a href=""https://sagemaker.readthedocs.io/en/stable/pipeline.html"" rel=""noreferrer"">https://sagemaker.readthedocs.io/en/stable/pipeline.html</a></p>

<p><a href=""https://github.com/awslabs/amazon-sagemaker-examples/blob/master/advanced_functionality/inference_pipeline_sparkml_xgboost_abalone/inference_pipeline_sparkml_xgboost_abalone.ipynb"" rel=""noreferrer"">https://github.com/awslabs/amazon-sagemaker-examples/blob/master/advanced_functionality/inference_pipeline_sparkml_xgboost_abalone/inference_pipeline_sparkml_xgboost_abalone.ipynb</a></p>

<p>My code is this:</p>

<pre class=""lang-py prettyprint-override""><code>from sagemaker.pipeline import PipelineModel
from sagemaker.model import Model

s3_model_bucket = 'bucket'
s3_model_key_prefix = 'prefix'
data = 's3://{}/{}/{}'.format(s3_model_bucket, s3_model_key_prefix, 'model.tar.gz')
models = ss_model.create_model() # ss_model is my sagemaker.estimator

model = PipelineModel(name=data, role=role, models= [models])
ss_predictor = model.deploy(initial_instance_count=1, instance_type='ml.c4.xlarge')
</code></pre>",2,0,2019-05-22 10:50:48.223000 UTC,1.0,2019-05-22 11:21:34.857000 UTC,6,python|amazon-web-services|amazon-s3|boto3|amazon-sagemaker,7404,2019-05-22 09:45:28.153000 UTC,2022-09-02 09:43:13.340000 UTC,"London, UK",79,6,0,16,"<p>You can actually instantiate a Python SDK <code>model</code> object from existing artifacts, and deploy it to an endpoint. This allows you to deploy a model from trained artifacts, without having to retrain in the notebook. For example, for the semantic segmentation model:</p>

<pre><code>trainedmodel = sagemaker.model.Model(
    model_data='s3://...model path here../model.tar.gz',
    image='685385470294.dkr.ecr.eu-west-1.amazonaws.com/semantic-segmentation:latest',  # example path for the semantic segmentation in eu-west-1
    role=role)  # your role here; could be different name

trainedmodel.deploy(initial_instance_count=1, instance_type='ml.c4.xlarge')
</code></pre>

<p>And similarly, you can instantiate a predictor object on a deployed endpoint from any authenticated client supporting the SDK, with the following command:</p>

<pre><code>predictor = sagemaker.predictor.RealTimePredictor(
    endpoint='endpoint name here',
    content_type='image/jpeg',
    accept='image/png')
</code></pre>

<p>More on those abstractions:</p>

<ul>
<li><code>Model</code>: <a href=""https://sagemaker.readthedocs.io/en/stable/model.html"" rel=""noreferrer"">https://sagemaker.readthedocs.io/en/stable/model.html</a></li>
<li><code>Predictor</code>:
<a href=""https://sagemaker.readthedocs.io/en/stable/predictors.html"" rel=""noreferrer"">https://sagemaker.readthedocs.io/en/stable/predictors.html</a></li>
</ul>",2.0,2019-05-23 14:25:59.713000 UTC,,13.0,['amazon-sagemaker']
"Panda AssertionError columns passed, passed data had 2 columns","<p>I am working on Azure ML implementation on text analytics with NLTK, the following execution is throwing </p>

<pre><code>AssertionError: 1 columns passed, passed data had 2 columns\r\nProcess returned with non-zero exit code 1
</code></pre>

<p>Below is the code </p>

<pre><code># The script MUST include the following function,
# which is the entry point for this module:
# Param&lt;dataframe1&gt;: a pandas.DataFrame
# Param&lt;dataframe2&gt;: a pandas.DataFrame
def azureml_main(dataframe1 = None, dataframe2 = None):
    # import required packages
    import pandas as pd
    import nltk
    import numpy as np
    # tokenize the review text and store the word corpus
    word_dict = {}
    token_list = []
    nltk.download(info_or_id='punkt', download_dir='C:/users/client/nltk_data')
    nltk.download(info_or_id='maxent_treebank_pos_tagger', download_dir='C:/users/client/nltk_data')
    for text in dataframe1[""tweet_text""]:
        tokens = nltk.word_tokenize(text.decode('utf8'))
        tagged = nltk.pos_tag(tokens)


      # convert feature vector to dataframe object
    dataframe_output = pd.DataFrame(tagged, columns=['Output'])
    return [dataframe_output]
</code></pre>

<p>Error is throwing here </p>

<pre><code> dataframe_output = pd.DataFrame(tagged, columns=['Output'])
</code></pre>

<p>I suspect this to be the tagged data type passed to dataframe, can some one let me know the right approach to add this to dataframe.</p>",1,0,2016-08-12 22:23:17.197000 UTC,3.0,,7,python|pandas|dataframe|nltk|azure-machine-learning-studio,48200,2013-06-11 04:20:18.390000 UTC,2022-09-18 05:28:20.357000 UTC,"Toronto, ON, Canada",1748,136,55,339,"<p>Try this:</p>

<pre><code>dataframe_output = pd.DataFrame(tagged, columns=['Output', 'temp'])
</code></pre>",0.0,2016-08-12 22:26:09.603000 UTC,,13.0,['azure-machine-learning-studio']
"Permission ""artifactregistry.repositories.downloadArtifacts"" denied on resource","<p>While the artifact repository was successfully creating, running a docker push to push the image to the google artifact registry fails with a permissions error even after granting all artifact permissions to the accounting I am using on gcloud cli.</p>
<p><strong>Command used to push image:</strong></p>
<pre><code>docker push us-central1-docker.pkg.dev/project-id/repo-name:v2
</code></pre>
<p><strong>Error message:</strong></p>
<pre><code>The push refers to repository [us-central1-docker.pkg.dev/project-id/repo-name]
6f6f4a472f31: Preparing
bc096d7549c4: Preparing
5f70bf18a086: Preparing
20bed28d4def: Preparing
2a3255c6d9fb: Preparing
3f5d38b4936d: Waiting
7be8268e2fb0: Waiting
b889a93a79dd: Waiting
9d4550089a93: Waiting
a7934564e6b9: Waiting
1b7cceb6a07c: Waiting
b274e8788e0c: Waiting
78658088978a: Waiting
denied: Permission &quot;artifactregistry.repositories.downloadArtifacts&quot; denied on resource &quot;projects/project-id/locations/us-central1/repositories/repo-name&quot; (or it may not exist)


</code></pre>",2,3,2022-05-15 20:00:57.243000 UTC,,2022-05-16 02:21:18.670000 UTC,12,python|google-cloud-platform|google-cloud-ml|google-cloud-vertex-ai|docker-push,5722,2015-03-21 06:55:29.353000 UTC,2022-09-02 13:58:42.187000 UTC,Bangkok,194,8,0,12,"<p>I was able to recreate your use case. This happens when you are trying to push an image on a <code>repository</code> in which its specific hostname (associated with it's repository location) is not yet  added to the credential helper configuration for authentication. You may refer to this <a href=""https://cloud.google.com/artifact-registry/docs/docker/authentication"" rel=""noreferrer"">Setting up authentication for Docker </a> as also provided by @DazWilkin in the comments for more details.</p>
<p>In my example, I was trying to push an image on a repository that has a location of <code>us-east1</code> and got the same error since it is not yet added to the credential helper configuration.
<a href=""https://i.stack.imgur.com/NQeIf.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/NQeIf.png"" alt=""enter image description here"" /></a></p>
<p>And after I ran the authentication using below command (specifically for us-east1 since it is the <code>location</code> of my repository), the image was successfully pushed:</p>
<pre><code>gcloud auth configure-docker us-east1-docker.pkg.dev
</code></pre>
<p><a href=""https://i.stack.imgur.com/q2Q9x.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/q2Q9x.png"" alt=""enter image description here"" /></a></p>
<p><em><strong>QUICK TIP</strong></em>: You may  get your authentication command specific for your repository when you open your desired repository in the <a href=""https://console.cloud.google.com/artifacts"" rel=""noreferrer"">console</a>, and then click on the <code>SETUP INSTRUCTIONS</code>.
<a href=""https://i.stack.imgur.com/KBjqa.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/KBjqa.png"" alt=""enter image description here"" /></a></p>",4.0,2022-05-16 06:40:03.067000 UTC,2022-06-05 22:36:23.460000 UTC,23.0,['google-cloud-vertex-ai']
"Pros and Cons of Amazon SageMaker VS. Amazon EMR, for deploying TensorFlow-based deep learning models?","<p>I want to build some <strong>neural network</strong> models for NLP and recommendation applications. The framework I want to use is <strong>TensorFlow</strong>. I plan to train these models and make predictions on Amazon web services. The application will be most likely <strong>distributed computing</strong>.</p>

<p>I am wondering what are the pros and cons of SageMaker and EMR for TensorFlow applications?</p>

<p>They both have TensorFlow integrated. </p>",2,0,2018-09-21 06:09:49.837000 UTC,1.0,2018-09-21 06:47:31.193000 UTC,8,amazon-web-services|tensorflow|amazon-emr|amazon-sagemaker,10776,2012-07-11 00:42:40.210000 UTC,2022-09-19 04:56:15.667000 UTC,,2832,482,1,368,"<p>In general terms, they serve different purposes.</p>

<p><a href=""https://aws.amazon.com/emr/"" rel=""noreferrer""><strong>EMR</strong></a> is when you need to process massive amounts of data and heavily rely on Spark, Hadoop, and MapReduce (EMR = Elastic MapReduce). Essentially, if your data is in large enough volume to make use of the efficiencies of Spark, Hadoop, Hive, HDFS, HBase and Pig stack then go with EMR.</p>

<p><strong>EMR Pros:</strong></p>

<ul>
<li>Generally, low cost compared to EC2 instances</li>
<li>As the name suggests Elastic meaning you can provision what you need when you need it</li>
<li>Hive, Pig, and HBase out of the box</li>
</ul>

<p><strong>EMR Cons:</strong></p>

<ul>
<li>You need a very specific use case to truly benefit from all the offerings in EMR. Most don't take advantage of its entire offering</li>
</ul>

<p><a href=""https://aws.amazon.com/sagemaker/"" rel=""noreferrer""><strong>SageMaker</strong></a> is an attempt to make Machine Learning easier and distributed. The marketplace provides out of the box algos and models for quick use. It's a great service if you conform to the workflows it enforces. Meaning creating training jobs, deploying inference endpoints </p>

<p><strong>SageMaker Pros:</strong></p>

<ul>
<li>Easy to get up and running with Notebooks</li>
<li>Rich marketplace to quickly try existing models</li>
<li>Many different example notebooks for popular algorithms</li>
<li>Predefined kernels that minimize configuration</li>
<li>Easy to deploy models</li>
<li>Allows you to distribute inference compute by deploying endpoints</li>
</ul>

<p><strong>SageMaker Cons:</strong></p>

<ul>
<li>Expensive!</li>
<li>Enforces a certain workflow making it hard to be fully custom</li>
<li>Expensive!</li>
</ul>",1.0,2019-11-13 17:08:24.093000 UTC,,10.0,['amazon-sagemaker']
How do I load python modules which are not available in Sagemaker?,<p>I want to install spacy which is not available as part of the Sagemaker platform. How should can I pip install it?</p>,2,2,2018-04-05 06:09:19.597000 UTC,,,2,amazon-web-services|amazon-sagemaker,2578,2014-09-17 16:42:55.307000 UTC,2022-09-22 07:49:15.917000 UTC,,1124,156,1,153,"<p>When creating you model, you can specify the requirements.txt as an environment variable. </p>

<p>For Eg. </p>

<pre><code>env = {
    'SAGEMAKER_REQUIREMENTS': 'requirements.txt', # path relative to `source_dir` below.
}
sagemaker_model = TensorFlowModel(model_data = 's3://mybucket/modelTarFile,
                                  role = role,
                                  entry_point = 'entry.py',
                                  code_location = 's3://mybucket/runtime-code/',
                                  source_dir = 'src',
                                  env = env,
                                  name = 'model_name',
                                  sagemaker_session = sagemaker_session,
                                 )
</code></pre>

<p>This would ensure that the requirements file is run after the docker container is created, before running any code on it. </p>",0.0,2018-04-05 15:23:55.117000 UTC,2018-04-05 16:22:03.343000 UTC,10.0,['amazon-sagemaker']
SageMaker and TensorFlow 2.0,"<p>What is the best way to run TensorFlow 2.0 with AWS Sagemeker?</p>

<p>As of today (Aug 7th, 2019) AWS does not provide TensorFlow 2.0 <a href=""https://github.com/aws/sagemaker-tensorflow-container"" rel=""noreferrer"">SageMaker containers</a>, so my understanding is that I need to build my own.</p>

<p>What is the best Base image to use? Example Dockerfile?</p>",4,1,2019-08-07 14:00:51.297000 UTC,1.0,2019-08-07 14:06:30.803000 UTC,13,tensorflow|amazon-sagemaker|tensorflow2.0,4136,2015-11-07 01:25:10.543000 UTC,2022-09-24 20:44:03.983000 UTC,"Toronto, Canada",3259,104,7,233,"<p>EDIT: <strong>Amazon SageMaker does now support TF 2.0 and higher.</strong></p>
<ul>
<li>SageMaker + TensorFlow docs: <a href=""https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/using_tf.html"" rel=""nofollow noreferrer"">https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/using_tf.html</a></li>
<li>Supported Tensorflow versions (and Docker URIs): <a href=""https://aws.amazon.com/releasenotes/available-deep-learning-containers-images"" rel=""nofollow noreferrer"">https://aws.amazon.com/releasenotes/available-deep-learning-containers-images</a></li>
</ul>
<hr />
<p><em>Original answer</em></p>
<p>Here is an example Dockerfile that uses <a href=""https://github.com/aws/sagemaker-containers"" rel=""nofollow noreferrer"">the underlying SageMaker Containers library</a> (this is what is used in the official pre-built Docker images):</p>
<pre><code>FROM tensorflow/tensorflow:2.0.0b1

RUN pip install sagemaker-containers

# Copies the training code inside the container
COPY train.py /opt/ml/code/train.py

# Defines train.py as script entrypoint
ENV SAGEMAKER_PROGRAM train.py
</code></pre>
<p>For more information on this approach, see <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/build-container-to-train-script-get-started.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/build-container-to-train-script-get-started.html</a></p>",4.0,2019-09-04 22:22:31.827000 UTC,2020-07-17 17:54:03.227000 UTC,10.0,['amazon-sagemaker']
Download an entire folder from AWS sagemaker to laptop,"<p>I have a folder with predicted masks on AWS Sagemaker. ( It has 4 folders inside it and lot of files inside those folders. ) I want to download the entire folder to my laptop. 
This might sound so simple and easy, but I could not find a way to do it. Appreciate any help.</p>

<p>Thanks</p>",2,0,2019-02-28 17:36:21.970000 UTC,8.0,,17,amazon-web-services|download|amazon-sagemaker,13015,2013-04-21 07:52:05.213000 UTC,2022-09-22 19:54:43.253000 UTC,"California, USA",440,149,1,24,"<p>You can do that by opening a terminal on sagemaker. Navigate to the path where your folder is. Run the command to zip it</p>

<pre><code>zip -r -X archive_name.zip folder_to_compress
</code></pre>

<p>You will find the zipped folder. You can then select it and download it.</p>",1.0,2019-02-28 17:45:26.143000 UTC,,35.0,['amazon-sagemaker']
Is there a way to get log the descriptive stats of a dataset using MLflow?,<p>Is there a way to get log the descriptive stats of a dataset using MLflow? If any could you please share the details?</p>,2,0,2019-04-24 04:52:09.530000 UTC,,,3,python|mlflow,4592,2014-09-22 04:46:57.027000 UTC,2022-09-03 08:12:58.187000 UTC,"Bengaluru, Karnataka, India",569,41,3,123,"<p>Generally speaking you can log arbitrary output from your code using the mlflow_log_artifact() function.  From <a href=""https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.log_artifact"" rel=""noreferrer"">the docs</a>:</p>
<blockquote>
<p><strong>mlflow.log_artifact(local_path, artifact_path=None)</strong>
Log a local file or directory as an artifact of the currently active run.</p>
</blockquote>
<blockquote>
<p><strong>Parameters:</strong><br />
<em>local_path</em> – Path to the file to write.
<em>artifact_path</em> – If provided, the directory in artifact_uri to write to.</p>
</blockquote>
<p>As an example, say you have your statistics in a pandas dataframe, <code>stat_df</code>.</p>
<pre><code>## Write csv from stats dataframe
stat_df.to_csv('dataset_statistics.csv')

## Log CSV to MLflow
mlflow.log_artifact('dataset_statistics.csv')
</code></pre>
<p>This will show up under the artifacts section of this MLflow run in the Tracking UI.  If you explore the docs further you'll see that you can also log an entire directory and the objects therein.  In general, MLflow provides you a lot of flexibility - anything you write to your file system you can track with MLflow.  Of course that doesn't mean you should. :)</p>",2.0,2019-05-08 01:32:42.457000 UTC,2021-01-26 04:34:50.617000 UTC,9.0,['mlflow']
Randomforest in amazon aws sagemaker?,"<p>I am looking to recreate a randomforest model built locally, and deploy it through sagemaker. The model is very basic, but for comparison I would like to use the same in sagemaker. I don't see randomforest among sagemaker's built in algorithms (which seems weird) - is my only option to go the route of <a href=""https://aws.amazon.com/blogs/machine-learning/train-and-host-scikit-learn-models-in-amazon-sagemaker-by-building-a-scikit-docker-container/"" rel=""nofollow noreferrer"">deploying my own custom model</a>? Still learning about containers, and it seems like a lot of work for something that is just a simple randomforestclassifier() call locally. I just want to baseline against the out of the box randomforest model, and show that it works the same when deployed through AWS sagemaker.</p>",2,1,2019-06-24 16:27:13.193000 UTC,2.0,,4,amazon-web-services|docker|containers|random-forest|amazon-sagemaker,4295,2015-01-15 17:43:03.700000 UTC,2022-08-23 22:54:25.603000 UTC,,1387,51,1,153,"<p><em>edit 03/30/2020: adding a link to the the <a href=""https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/scikit_learn_randomforest/Sklearn_on_SageMaker_end2end.ipynb"" rel=""noreferrer"">SageMaker Sklearn random forest demo</a></em></p>

<p><br/></p>

<p>in SageMaker you have 3 options to write scientific code:</p>

<ul>
<li><strong>Built-in algorithms</strong></li>
<li><strong>Open-source pre-written containers</strong> (available
for sklearn, tensorflow, pytorch, mxnet, chainer. Keras can be
written in the tensorflow and mxnet containers)</li>
<li><strong>Bring your own container</strong> (for R for example)</li>
</ul>

<p><strong>At the time of writing this post there is no random forest classifier nor regressor in the built-in library</strong>. There is an algorithm called <a href=""https://aws.amazon.com/blogs/machine-learning/use-the-built-in-amazon-sagemaker-random-cut-forest-algorithm-for-anomaly-detection/"" rel=""noreferrer"">Random Cut Forest</a> in the built-in library but it is an unsupervised algorithm for anomaly detection, a different use-case than the <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"" rel=""noreferrer"">scikit-learn random forest</a> used in a supervised fashion (also <a href=""https://stackoverflow.com/questions/56728230/aws-sagemaker-randomcutforest-rcf-vs-scikit-lean-randomforest-rf?noredirect=1&amp;lq=1"">answered in StackOverflow here</a>). But it is easy to use the open-source pre-written scikit-learn container to implement your own. There is a <a href=""https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/scikit_learn_randomforest/Sklearn_on_SageMaker_end2end.ipynb"" rel=""noreferrer"">demo showing how to use Sklearn's random forest in SageMaker</a>, with training orchestration bother from the high-level SDK and <code>boto3</code>. You can also use this other <a href=""https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/scikit_learn_iris/Scikit-learn%20Estimator%20Example%20With%20Batch%20Transform.ipynb"" rel=""noreferrer"">public sklearn-on-sagemaker demo</a> and change the model. A benefit of the pre-written containers over the ""Bring your own"" option is that the dockerfile is already written, and web serving stack too.</p>

<p>Regarding your surprise that Random Forest is not featured in the built-in algos, the library and its 18 algos already cover a rich set of use-cases. For example for supervised learning over structured data (the usual use-case for the random forest), if you want to stick to the built-ins, depending on your priorities (accuracy, inference latency, training scale, costs...) you can use SageMaker XGBoost (XGBoost has been winning tons of datamining competitions - every winning team in the top10 of the KDDcup 2015 used XGBoost <a href=""https://arxiv.org/pdf/1603.02754.pdf"" rel=""noreferrer"">according to the XGBoost paper</a> - and scales well) and linear learner, which is extremely fast at inference and can be trained at scale, in mini-batch fashion over GPU(s). <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/fact-machines-howitworks.html"" rel=""noreferrer"">Factorization Machines</a> (linear + 2nd degree interaction with weights being column embedding dot-products) and <a href=""https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-supports-knn-classification-and-regression/"" rel=""noreferrer"">SageMaker kNN</a> are other options. Also, things are not frozen in stone, and the list of built-in algorithms is being improved fast.</p>",1.0,2019-06-25 19:38:06.350000 UTC,2020-03-30 16:46:46.547000 UTC,10.0,['amazon-sagemaker']
Azure: plot without labels,"<p>Suppose I have a dataframe (myDataframe) with two column of values (third and fourth). I want to plot them in a bi-dimensional graph. If I do it in R it works, but it returns me a graph without labels when I run the script from Azure Machine Learning. Someone with ideas?</p>

<pre><code>...
plot(myDataframe[,3],myDataframe[,4], 
       main=""my title"",
       xlab= ""x""
       ylab= ""y"",
       col= ""blue"", pch = 19, cex = 0.1, lty = ""solid"", lwd = 2)

# lines(x,y=x, col=""yellow"")

# add LABELS
text(DF_relativo[,A], DF_relativo[,B], 
       labels=DF_relativo$names, cex= 0.7, pos=2)
...
</code></pre>",1,0,2016-01-22 13:48:38.060000 UTC,,,0,r|plot|label|azure-machine-learning-studio,91,2015-07-09 09:05:28.610000 UTC,2022-09-19 17:14:25.487000 UTC,"Colleferro, Italy",809,109,0,361,"<p>using ggplot2() in AzureML is bit different. It can use to have plots with labels. Here's the <a href=""https://gallery.cortanaintelligence.com/Experiment/b1c26728eb6c4e4d80dddceae992d653"" rel=""nofollow"">Cortana Intelligence gallery example</a> for the particular task.  </p>",0.0,2016-06-20 05:57:16.047000 UTC,,-1.0,['azure-machine-learning-studio']
How do I make this IAM role error in aws sagemaker go away?,"<p>I suspect this has to more to do with IAM roles than Sagemaker.</p>

<p>I'm following the example <a href=""https://github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/tensorflow/README.rst"" rel=""noreferrer"">here</a></p>

<p>Specifically, when it makes this call</p>

<pre><code>tf_estimator.fit('s3://bucket/path/to/training/data')
</code></pre>

<p>I get this error</p>

<pre><code>ClientError: An error occurred (AccessDenied) when calling the GetRole operation: User: arn:aws:sts::013772784144:assumed-role/AmazonSageMaker-ExecutionRole-20181022T195630/SageMaker is not authorized to perform: iam:GetRole on resource: role SageMakerRole
</code></pre>

<p>My notebook instance has an IAM role attached to it.
That role has the <code>AmazonSageMakerFullAccess</code> policy. It also has a custom policy that looks like this</p>

<pre><code>{
""Version"": ""2012-10-17"",
""Statement"": [
    {
        ""Effect"": ""Allow"",
        ""Action"": [
            ""s3:GetObject"",
            ""s3:PutObject"",
            ""s3:DeleteObject"",
            ""s3:ListBucket""
        ],
        ""Resource"": [
            ""arn:aws:s3:::*""
        ]
    }
]
</code></pre>

<p>}</p>

<p>My input files and .py script is in an s3 bucket with the phrase <code>sagemaker</code> in it.</p>

<p>What else am I missing?</p>",3,0,2018-11-22 02:27:05.410000 UTC,,,6,amazon-web-services|amazon-iam|amazon-sagemaker,8160,2011-10-21 21:58:08.810000 UTC,2022-09-17 00:51:12.053000 UTC,,4966,744,11,304,"<p>If you're running the example code on a SageMaker notebook instance, you can use the execution_role which has the <code>AmazonSageMakerFullAccess</code> attached.</p>
<pre><code>from sagemaker import get_execution_role
sagemaker_session = sagemaker.Session()
role = get_execution_role()
</code></pre>
<p>And you can pass this role when initializing <code>tf_estimator</code>.
You can check out the example <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-ex-role.html"" rel=""nofollow noreferrer"">here</a> for using <code>execution_role</code> with S3 on notebook instance.</p>",1.0,2018-11-23 22:08:24.777000 UTC,2021-05-06 09:35:48.347000 UTC,8.0,['amazon-sagemaker']
How to register model from the Azure ML Pipeline Script step,"<p>I am running the <code>pipeline.submit()</code> in AzureML, which has a <code>PythonScriptStep</code>.
Inside this step, I download a model from tensorflow-hub, retrain it and save it as a <code>.zip</code>, and finally, I would like to register it in the Azure ML.
But as inside the script I do not have a workspace, <code>Model.register()</code> is not the case.
So I am trying to use <code>Run.register_model()</code> method as below:</p>

<pre><code>os.replace(os.path.join('.', archive_name + '.zip'), 
           os.path.join('.', 'outputs', archive_name + '.zip'))

print(os.listdir('./outputs'))
print('========================')

run_context = Run.get_context()
finetuning_model = run_context.register_model(model_name='finetuning_similarity_model',
                                              model_path=os.path.join(archive_name+'.zip'),
                                              tags={},
                                              description=""Finetuning Similarity model"")
</code></pre>

<p>But then I have got an error:</p>

<blockquote>
  <p>ErrorResponse 
  {
      ""error"": {
          ""message"": ""Could not locate the provided model_path retrained.zip in the set of files uploaded to the run:</p>
</blockquote>

<p>despite I have the retrained <code>.zip</code> in the <code>./outputs</code> dir as we can see from the log:</p>

<pre><code>['retrained.zip']
========================
</code></pre>

<p>I guess that I am doing something wrong?</p>",2,0,2019-11-19 11:56:24.153000 UTC,2.0,2020-01-11 12:03:44.353000 UTC,7,python|azure-machine-learning-service,3429,2019-11-19 11:24:15.727000 UTC,2021-09-24 08:42:39.343000 UTC,,75,2,0,6,"<p>I was able to fix the same issue (<a href=""https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.exceptions.modelpathnotfoundexception?view=azure-ml-py"" rel=""noreferrer""><code>ModelPathNotFoundException</code></a>) by explicitly uploading the model into the run history record before trying to register the model:</p>

<pre><code>run.upload_file(""outputs/my_model.pickle"", ""outputs/my_model.pickle"")
</code></pre>

<p>Which I found surprising because this wasn't mentioned in many of the official examples and according to the <code>upload_file()</code> <a href=""https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.run.run?view=azure-ml-py#upload-file-name--path-or-stream-"" rel=""noreferrer"">documentation</a>:</p>

<blockquote>
  <p>Runs automatically capture file in the specified output directory, which defaults to ""./outputs"" for most run types. Use upload_file only when additional files need to be uploaded or an output directory is not specified.</p>
</blockquote>",2.0,2020-01-11 12:33:03.587000 UTC,2020-01-11 12:38:39.987000 UTC,14.0,['azure-machine-learning-service']
How do I use an environment in an ML Azure Pipeline,"<p><strong>Background</strong></p>

<p>I have created an ML Workspace environment from a conda <code>environment.yml</code> plus some docker config and environment variables. I can access it from within a Python notebook:</p>

<pre><code>env = Environment.get(workspace=ws, name='my-environment', version='1')
</code></pre>

<p>I can use this successfully to run a Python script as an experiment, i.e.</p>

<pre><code>runconfig = ScriptRunConfig(source_directory='script/', script='my-script.py', arguments=script_params)
runconfig.run_config.target = compute_target
runconfig.run_config.environment = env
run = exp.submit(runconfig)
</code></pre>

<p><strong>Problem</strong></p>

<p>I would now like to run this same script as a Pipeline, so that I can trigger multiple runs with different parameters. I have created the Pipeline as follows:</p>

<pre><code>pipeline_step = PythonScriptStep(
    source_directory='script', script_name='my-script.py',
    arguments=['-a', param1, '-b', param2],
    compute_target=compute_target,
    runconfig=runconfig
)
steps = [pipeline_step]
pipeline = Pipeline(workspace=ws, steps=steps)
pipeline.validate()
</code></pre>

<p>When I then try to run the Pipeline:</p>

<pre><code>pipeline_run = Experiment(ws, 'my_pipeline_run').submit(
    pipeline, pipeline_parameters={...}
)
</code></pre>

<p>I get the following error: <code>Response status code does not indicate success: 400 (Conda dependencies were not specified. Please make sure that all conda dependencies were specified i).</code></p>

<p>When I view the pipeline run in the Azure Portal it seems that the environment has not been picked up: none of my conda dependencies are configured, hence the code does not run. What am I doing wrong?</p>",1,0,2020-03-03 11:37:15.267000 UTC,,2020-03-03 23:14:33.840000 UTC,3,python|azure|azure-machine-learning-studio|azure-machine-learning-service,1972,2014-06-25 12:11:55.160000 UTC,2022-09-24 10:12:50.483000 UTC,"London, UK",1534,171,3,56,"<p>You're almost there, but you need to use <code>RunConfiguration</code> instead of <code>ScriptRunConfig</code>. More info <a href=""https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-getting-started.ipynb"" rel=""noreferrer"">here</a></p>

<pre class=""lang-py prettyprint-override""><code>from azureml.core.runconfig import RunConfiguration

env = Environment.get(workspace=ws, name='my-environment', version='1')
# create a new runconfig object
runconfig = RunConfiguration()
runconfig.environment = env

pipeline_step = PythonScriptStep(
    source_directory='script', script_name='my-script.py',
    arguments=['-a', param1, '-b', param2],
    compute_target=compute_target,
    runconfig=runconfig
)

pipeline = Pipeline(workspace=ws, steps=[pipeline_step])

pipeline_run = Experiment(ws, 'my_pipeline_run').submit(pipeline)
</code></pre>",0.0,2020-03-03 17:07:50.993000 UTC,,8.0,"['azure-machine-learning-studio', 'azure-machine-learning-service']"
Pip installation stuck in infinite loop if unresolvable conflicts in dependencies,"<p>Pip installation is stuck in an infinite loop if there are unresolvable conflicts in dependencies. To reproduce, <code>pip==20.3.0</code> and:</p>
<pre><code>pip install pyarrow==2.0.0 azureml-defaults==1.18.0
</code></pre>",1,6,2020-12-02 16:54:05.387000 UTC,,2020-12-23 16:06:48.113000 UTC,13,python|pip|azure-machine-learning-service,2146,2019-09-16 18:20:32.913000 UTC,2021-07-06 00:01:05.640000 UTC,"Redmond, WA, USA",133,1,0,3,"<p>Workarounds:</p>
<p>Local environment:
Downgrade pip to &lt; 20.3</p>
<p>Conda environment created from yaml:
This will be seen only if conda-forge is highest priority channel, anaconda channel doesn't have pip 20.3 (as of now). To mitigate the issue please explicitly specify pip&lt;20.3 (!=20.3 or =20.2.4 pin to other version) as a conda dependency in the conda specification file</p>
<p>AzureML experimentation:
Follow the case above to make sure pinned pip resulted as a conda dependency in the environment object, either from yml file or programmatically</p>",2.0,2020-12-02 17:02:08.563000 UTC,,12.0,['azure-machine-learning-service']
How to pass a request to sagemaker using postman,"<p>I've trained a model on sagemaker and have created the endpoint. I'm trying to invoke the endpoint using postman. But when training the model and even after that, I have not specified any header for the training data. I'm at a loss as to how to create payload while sending a post request to sagemaker</p>",2,0,2018-04-05 15:03:12.383000 UTC,2.0,,8,web-services|amazon-web-services|postman|amazon-sagemaker,6746,2014-09-17 16:42:55.307000 UTC,2022-09-22 07:49:15.917000 UTC,,1124,156,1,153,"<p>Once the endpoint is created, you can invoke it as any other restful service, with credentials and payload. </p>

<p>I am guessing, there could be two places where might be stuck. 
One could be, sending an actual PostMan Request with all the headers and everything. 
Newer version of Postman has AWS Signature as one of the Authorization types. You can use that to invoke the service. There are no other spacial headers required. Note that there is a bug in Postman still open (<a href=""https://github.com/postmanlabs/postman-app-support/issues/1663"" rel=""noreferrer"">issue-1663</a>) that only affects if you are a AWS federated account. Individual accounts should not be affected by this issue. </p>

<p>Or, you could be stuck at the actual payload. When you invoke the SageMaker endpoint, the payload is passed as is to the model. If you want to preprocess the input before feeding it to the model, you'd have to implement an input_fn method and specify that when instantiating the model. </p>

<p>You might also be able to invoke SageMaker endpoint using AWS SDK boto3 as follows </p>

<pre><code>import boto3
runtime= boto3.client('runtime.sagemaker')

payload = getImageData()


result  = runtime.invoke_endpoint(
    EndpointName='my_endpoint_name',
    Body=payload,
    ContentType='image/jpeg'
)
</code></pre>

<p>Hope this helps.</p>",0.0,2018-04-05 16:37:35.433000 UTC,,11.0,['amazon-sagemaker']
How to use SageMaker Estimator for model training and saving,"<p>The documentations of how to use SageMaker estimators are scattered around, sometimes obsolete, incorrect. Is there a one stop location which gives the comprehensive views of how to use SageMaker SDK Estimator to train and save models?</p>",1,0,2021-09-02 04:01:47.170000 UTC,14.0,,27,amazon-web-services|amazon-sagemaker,6655,2014-11-22 09:22:35.470000 UTC,2022-09-24 22:13:03.237000 UTC,,14749,641,62,968,"<h1>Answer</h1>
<p>There is no one such resource from AWS that provides the comprehensive view of how to use SageMaker SDK Estimator to train and save models.</p>
<h2>Alternative Overview Diagram</h2>
<p>I put a diagram and brief explanation to get the overview on how SageMaker Estimator runs a training.</p>
<ol>
<li><p>SageMaker sets up a docker container for a training job where:</p>
<ul>
<li>Environment variables are set as in <a href=""https://github.com/aws/sagemaker-containers#important-environment-variables"" rel=""noreferrer"">SageMaker Docker Container. Environment Variables</a>.</li>
<li>Training data is setup under <code>/opt/ml/input/data</code>.</li>
<li>Training script codes are setup under <code>/opt/ml/code</code>.</li>
<li><code>/opt/ml/model</code> and <code>/opt/ml/output</code> directories are setup to store training outputs.</li>
</ul>
</li>
</ol>
<pre><code>/opt/ml
├── input
│   ├── config
│   │   ├── hyperparameters.json  &lt;--- From Estimator hyperparameter arg
│   │   └── resourceConfig.json
│   └── data
│       └── &lt;channel_name&gt;        &lt;--- From Estimator fit method inputs arg
│           └── &lt;input data&gt;
├── code
│   └── &lt;code files&gt;              &lt;--- From Estimator src_dir arg
├── model
│   └── &lt;model files&gt;             &lt;--- Location to save the trained model artifacts
└── output
    └── failure                   &lt;--- Training job failure logs
</code></pre>
<ol start=""2"">
<li><p>SageMaker Estimator <code>fit(inputs)</code> method executes the training script. Estimator <code>hyperparameters</code> and <code>fit</code> method <code>inputs</code> are provided as its command line arguments.</p>
</li>
<li><p>The training script saves the model artifacts in the <code>/opt/ml/model</code> once the training is completed.</p>
</li>
<li><p>SageMaker archives the artifacts under <code>/opt/ml/model</code> into <code>model.tar.gz</code> and save it to the S3 location specified to <code>output_path</code> Estimator parameter.</p>
</li>
<li><p>You can set Estimator <code>metric_definitions</code> parameter to extract model metrics from the training logs. Then you can monitor the training progress in the SageMaker console metrics.</p>
</li>
</ol>
<p><a href=""https://i.stack.imgur.com/gi8bU.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/gi8bU.png"" alt=""enter image description here"" /></a></p>
<p>I believe AWS needs to stop mass-producing verbose, redundant, wordy, scattered, and obsolete documents. AWS needs to understand <strong>A picture is worth thousand words</strong>.</p>
<p>Have diagrams and piece document parts together in a <strong>context</strong> with a clear objective to achieve.</p>
<hr />
<h1>Problem</h1>
<p>AWS documentations need serious re-design and re-structuring. Just to understand <strong>how to train and save a model</strong> forces us going through dozens of scattered,  fragmented, verbose, redundant documentations, which are often obsolete, incomplete, and sometime incorrect.</p>
<p>It is well-summarized in <a href=""https://nandovillalba.medium.com/why-i-think-gcp-is-better-than-aws-ea78f9975bda"" rel=""noreferrer"">Why I think GCP is better than AWS</a>:</p>
<blockquote>
<p>It’s not that AWS is harder to use than GCP, it’s that <strong>it is needlessly hard</strong>; a disjointed, sprawl of infrastructure primitives with poor cohesion between them.  <br><br>
A challenge is nice, a confusing mess is not, and <strong>the problem with AWS is that a large part of your working hours will be spent untangling their documentation and weeding through features and products to find what you want</strong>, rather than focusing on cool interesting challenges.</p>
</blockquote>
<p>Especially the SageMaker team keeps changing implementations without updating documents. Its roll-out was also inconsistent, e.g. SDK version 2 was rolled out in the SageMaker Studio making the AWS examples in Github incompatible without announcing it. Whereas SageMaker instance still had SDK 1, hence code worked in Instance but not in Studio.</p>
<p>It is mind-boggling that we have to go through these many documents below to understand how to use the SageMaker SDK Estimator for training.</p>
<h2>Documents for Model Training</h2>
<ul>
<li><a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-training.html"" rel=""noreferrer"">Train a Model with Amazon SageMaker</a></li>
</ul>
<p>This document gives 20,000 feet overview of how SageMaker training but does not give any clue what to do.</p>
<ul>
<li><a href=""https://sagemaker-workshop.com/custom/containers.html"" rel=""noreferrer"">Running a container for Amazon SageMaker training</a></li>
</ul>
<p>This document gives an overview of how SageMaker training looks like. However, this is not up-to-date as it is based on <a href=""https://github.com/aws/sagemaker-containers"" rel=""noreferrer"">SageMaker Containers</a> which is obsolete.</p>
<blockquote>
<p>WARNING: This package has been deprecated. Please use the SageMaker Training Toolkit for model training and the SageMaker Inference Toolkit for model serving.</p>
</blockquote>
<ul>
<li><a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-train-model.html"" rel=""noreferrer"">Step 4: Train a Model</a></li>
</ul>
<p>This document layouts the steps for training.</p>
<blockquote>
<p>The Amazon SageMaker Python SDK provides framework estimators and generic estimators to train your model while orchestrating the machine learning (ML) lifecycle accessing the SageMaker features for training and the AWS infrastructures</p>
</blockquote>
<ul>
<li><a href=""https://sagemaker.readthedocs.io/en/stable/overview.html#train-a-model-with-the-sagemaker-python-sdk"" rel=""noreferrer"">Train a Model with the SageMaker Python SDK</a></li>
</ul>
<blockquote>
<p>To train a model by using the SageMaker Python SDK, you:</p>
<ul>
<li>Prepare a training script</li>
<li>Create an estimator</li>
<li>Call the fit method of the estimator</li>
</ul>
</blockquote>
<p>Finally this document gives concrete steps and ideas. However still missing comprehensiv details about Environment Variables, Directory structure in the SageMaker docker container**, S3 for uploading code, placing data, S3 where the trained model is saved, etc.</p>
<ul>
<li><a href=""https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/using_tf.html"" rel=""noreferrer"">Use TensorFlow with the SageMaker Python SDK</a></li>
</ul>
<p>This documents is focused on TensorFlow Estimator implementation steps. Use <a href=""https://github.com/aws/amazon-sagemaker-examples/blob/master/frameworks/tensorflow/get_started_mnist_train.ipynb"" rel=""noreferrer"">Training a Tensorflow Model on MNIST</a> Github example to accompany with to follow the actual implementation.</p>
<h2>Documents for passing parameters and data locations</h2>
<ul>
<li><a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo-running-container.html#your-algorithms-training-algo-running-container-inputdataconfig"" rel=""noreferrer"">How Amazon SageMaker Provides Training Information</a></li>
</ul>
<blockquote>
<p>This section explains how SageMaker makes training information, such as training data, hyperparameters, and other configuration information, available to your Docker container.</p>
</blockquote>
<p>This document finally gives the idea of how parameters and data are passed around but again, not comprehensive.</p>
<ul>
<li><a href=""https://github.com/aws/sagemaker-containers#important-environment-variables"" rel=""noreferrer"">SageMaker Docker Container Environment Variables</a></li>
</ul>
<p>This documentation is marked as <strong>deprecated</strong> but the only document which explains the SageMaker Environment Variables.</p>
<blockquote>
<h3>IMPORTANT ENVIRONMENT VARIABLES</h3>
<ul>
<li>SM_MODEL_DIR</li>
<li>SM_CHANNELS</li>
<li>SM_CHANNEL_{channel_name}</li>
<li>SM_HPS</li>
<li>SM_HP_{hyperparameter_name}</li>
<li>SM_CURRENT_HOST</li>
<li>SM_HOSTS</li>
<li>SM_NUM_GPUS</li>
</ul>
<h3>List of provided environment variables by SageMaker Containers</h3>
<ul>
<li>SM_NUM_CPUS</li>
<li>SM_LOG_LEVEL</li>
<li>SM_NETWORK_INTERFACE_NAME</li>
<li>SM_USER_ARGS</li>
<li>SM_INPUT_DIR</li>
<li>SM_INPUT_CONFIG_DIR</li>
<li>SM_OUTPUT_DATA_DIR</li>
<li>SM_RESOURCE_CONFIG</li>
<li>SM_INPUT_DATA_CONFIG</li>
<li>SM_TRAINING_ENV</li>
</ul>
</blockquote>
<h2>Documents for SageMaker Docker Container Directory Structure</h2>
<ul>
<li><a href=""https://sagemaker-workshop.com/custom/containers.html"" rel=""noreferrer"">Running a container for Amazon SageMaker training</a></li>
</ul>
<pre><code>/opt/ml
├── input
│   ├── config
│   │   ├── hyperparameters.json
│   │   └── resourceConfig.json
│   └── data
│       └── &lt;channel_name&gt;
│           └── &lt;input data&gt;
├── model
│   └── &lt;model files&gt;
└── output
    └── failure
</code></pre>
<p>This document explains the directory structure and purpose of each directory.</p>
<blockquote>
<h3>The input</h3>
<ul>
<li>/opt/ml/input/config contains information to control how your program runs. hyperparameters.json is a JSON-formatted dictionary of hyperparameter names to values. These values will always be strings, so you may need to convert them. resourceConfig.json is a JSON-formatted file that describes the network layout used for distributed training. Since scikit-learn doesn’t support distributed training, we’ll ignore it here.</li>
<li>/opt/ml/input/data/&lt;channel_name&gt;/ (for File mode) contains the input data for that channel. The channels are created based on the call to CreateTrainingJob but it’s generally important that channels match what the algorithm expects. The files for each channel will be copied from S3 to this directory, preserving the tree structure indicated by the S3 key structure.</li>
<li>/opt/ml/input/data/&lt;channel_name&gt;_&lt;epoch_number&gt; (for Pipe mode) is the pipe for a given epoch. Epochs start at zero and go up by one each time you read them. There is no limit to the number of epochs that you can run, but you must close each pipe before reading the next epoch.</li>
</ul>
<h3>The output</h3>
<ul>
<li>/opt/ml/model/ is the directory where you write the model that your algorithm generates. Your model can be in any format that you want. It can be a single file or a whole directory tree. SageMaker will package any files in this directory into a compressed tar archive file. This file will be available at the S3 location returned in the DescribeTrainingJob result.</li>
<li>/opt/ml/output is a directory where the algorithm can write a file failure that describes why the job failed. The contents of this file will be returned in the FailureReason field of the DescribeTrainingJob result. For jobs that succeed, there is no reason to write this file as it will be ignored.</li>
</ul>
</blockquote>
<p>However, this is not up-to-date as it is based on <a href=""https://github.com/aws/sagemaker-containers"" rel=""noreferrer"">SageMaker Containers</a> which is obsolete.</p>
<h2>Documents for Model Saving</h2>
<p>The information on where the trained model is saved and in what format are fundamentally missing. The training script needs to save the model under <code>/opt/ml/model</code> and the format and sub-directory structure depend on the frameworks e,g TensorFlow, Pytorch. This is because SageMaker deployment uses the Framework dependent model-serving, e,g. TensorFlow Serving for TensorFlow framework.</p>
<p>This is not clearly documented and causing confusions. The developer needs to specify which format to use and under which sub-directory to save.</p>
<p>To use TensorFlow Estimator training and deployment:</p>
<ul>
<li><a href=""https://sagemaker-examples.readthedocs.io/en/latest/aws_sagemaker_studio/frameworks/keras_pipe_mode_horovod/keras_pipe_mode_horovod_cifar10.html#Deploy-the-trained-model"" rel=""noreferrer"">Deploy the trained model</a></li>
</ul>
<blockquote>
<p>Because <strong>we’re using TensorFlow Serving for deployment</strong>, our training script <strong>saves the model in TensorFlow’s SavedModel format</strong>.</p>
</blockquote>
<ul>
<li><a href=""https://github.com/aws/amazon-sagemaker-examples/blob/master/frameworks/tensorflow/code/train.py#L159-L166"" rel=""noreferrer"">amazon-sagemaker-examples/frameworks/tensorflow/code/train.py </a></li>
</ul>
<pre><code>    # Save the model
    # A version number is needed for the serving container
    # to load the model
    version = &quot;00000000&quot;
    ckpt_dir = os.path.join(args.model_dir, version)
    if not os.path.exists(ckpt_dir):
        os.makedirs(ckpt_dir)
    model.save(ckpt_dir)
</code></pre>
<p>The code is saving the model in <code>/opt/ml/model/00000000</code> because this is for TensorFlow serving.</p>
<ul>
<li><a href=""https://www.tensorflow.org/guide/saved_model"" rel=""noreferrer"">Using the SavedModel format</a></li>
</ul>
<blockquote>
<p>The save-path follows a convention used by TensorFlow Serving where the last path component (1/ here) is a version number for your model - it allows tools like Tensorflow Serving to reason about the relative freshness.</p>
</blockquote>
<ul>
<li><a href=""https://www.tensorflow.org/tfx/tutorials/serving/rest_simple#save_your_model"" rel=""noreferrer"">Train and serve a TensorFlow model with TensorFlow Serving</a></li>
</ul>
<blockquote>
<p>To load our trained model into TensorFlow Serving we first need to save it in SavedModel format. This will create a protobuf file in a well-defined directory hierarchy, and will include a version number. TensorFlow Serving allows us to select which version of a model, or &quot;servable&quot; we want to use when we make inference requests. Each version will be exported to a different sub-directory under the given path.</p>
</blockquote>
<h2>Documents for API</h2>
<p>Basically the SageMaker SDK Estimator implements the <a href=""https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateTrainingJob.html"" rel=""noreferrer"">CreateTrainingJob</a> API for training part. Hence, better to understand how it is designed and what parameters need to be defined. Otherwise working on Estimators are like walking in the dark.</p>
<hr />
<h1>Example</h1>
<h2>Jupyter Notebook</h2>
<pre><code>import sagemaker
from sagemaker import get_execution_role

sagemaker_session = sagemaker.Session()
role = get_execution_role()
bucket = sagemaker_session.default_bucket()

metric_definitions = [
    {&quot;Name&quot;: &quot;train:loss&quot;, &quot;Regex&quot;: &quot;.*loss: ([0-9\\.]+) - accuracy: [0-9\\.]+.*&quot;},
    {&quot;Name&quot;: &quot;train:accuracy&quot;, &quot;Regex&quot;: &quot;.*loss: [0-9\\.]+ - accuracy: ([0-9\\.]+).*&quot;},
    {
        &quot;Name&quot;: &quot;validation:accuracy&quot;,
        &quot;Regex&quot;: &quot;.*step - loss: [0-9\\.]+ - accuracy: [0-9\\.]+ - val_loss: [0-9\\.]+ - val_accuracy: ([0-9\\.]+).*&quot;,
    },
    {
        &quot;Name&quot;: &quot;validation:loss&quot;,
        &quot;Regex&quot;: &quot;.*step - loss: [0-9\\.]+ - accuracy: [0-9\\.]+ - val_loss: ([0-9\\.]+) - val_accuracy: [0-9\\.]+.*&quot;,
    },
    {
        &quot;Name&quot;: &quot;sec/sample&quot;,
        &quot;Regex&quot;: &quot;.* - \d+s (\d+)[mu]s/sample - loss: [0-9\\.]+ - accuracy: [0-9\\.]+ - val_loss: [0-9\\.]+ - val_accuracy: [0-9\\.]+&quot;,
    },
]

import uuid

checkpoint_s3_prefix = &quot;checkpoints/{}&quot;.format(str(uuid.uuid4()))
checkpoint_s3_uri = &quot;s3://{}/{}/&quot;.format(bucket, checkpoint_s3_prefix)

from sagemaker.tensorflow import TensorFlow

# --------------------------------------------------------------------------------
# 'trainingJobName' msut satisfy regular expression pattern: ^[a-zA-Z0-9](-*[a-zA-Z0-9]){0,62}
# --------------------------------------------------------------------------------
base_job_name = &quot;fashion-mnist&quot;
hyperparameters = {
    &quot;epochs&quot;: 2, 
    &quot;batch-size&quot;: 64
}
estimator = TensorFlow(
    entry_point=&quot;fashion_mnist.py&quot;,
    source_dir=&quot;src&quot;,
    metric_definitions=metric_definitions,
    hyperparameters=hyperparameters,
    role=role,
    input_mode='File',
    framework_version=&quot;2.3.1&quot;,
    py_version=&quot;py37&quot;,
    instance_count=1,
    instance_type=&quot;ml.m5.xlarge&quot;,
    base_job_name=base_job_name,
    checkpoint_s3_uri=checkpoint_s3_uri,
    model_dir=False
)
estimator.fit()
</code></pre>
<h2>fashion_mnist.py</h2>
<pre><code>import os
import argparse
import json
import multiprocessing

import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Flatten, BatchNormalization
from tensorflow.keras.layers import Conv2D, MaxPooling2D
from tensorflow.keras.layers.experimental.preprocessing import Normalization
from tensorflow.keras import backend as K

print(&quot;TensorFlow version: {}&quot;.format(tf.__version__))
print(&quot;Eager execution is: {}&quot;.format(tf.executing_eagerly()))
print(&quot;Keras version: {}&quot;.format(tf.keras.__version__))


image_width = 28
image_height = 28


def load_data():
    fashion_mnist = tf.keras.datasets.fashion_mnist
    (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()

    number_of_classes = len(set(y_train))
    print(&quot;number_of_classes&quot;, number_of_classes)

    x_train = x_train / 255.0
    x_test = x_test / 255.0
    x_full = np.concatenate((x_train, x_test), axis=0)
    print(x_full.shape)

    print(type(x_train))
    print(x_train.shape)
    print(x_train.dtype)
    print(y_train.shape)
    print(y_train.dtype)

    # ## Train
    # * C: Convolution layer
    # * P: Pooling layer
    # * B: Batch normalization layer
    # * F: Fully connected layer
    # * O: Output fully connected softmax layer

    # Reshape data based on channels first / channels last strategy.
    # This is dependent on whether you use TF, Theano or CNTK as backend.
    # Source: https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py
    if K.image_data_format() == 'channels_first':
        x = x_train.reshape(x_train.shape[0], 1, image_width, image_height)
        x_test = x_test.reshape(x_test.shape[0], 1, image_width, image_height)
        input_shape = (1, image_width, image_height)
    else:
        x_train = x_train.reshape(x_train.shape[0], image_width, image_height, 1)
        x_test = x_test.reshape(x_test.shape[0], image_width, image_height, 1)
        input_shape = (image_width, image_height, 1)

    return x_train, y_train, x_test, y_test, input_shape, number_of_classes

# tensorboard --logdir=/full_path_to_your_logs

validation_split = 0.2
verbosity = 1
use_multiprocessing = True
workers = multiprocessing.cpu_count()


def train(model, x, y, args):
    # SavedModel Output
    tensorflow_saved_model_path = os.path.join(args.model_dir, &quot;tensorflow/saved_model/0&quot;)
    os.makedirs(tensorflow_saved_model_path, exist_ok=True)

    # Tensorboard Logs
    tensorboard_logs_path = os.path.join(args.model_dir, &quot;tensorboard/&quot;)
    os.makedirs(tensorboard_logs_path, exist_ok=True)

    tensorboard_callback = tf.keras.callbacks.TensorBoard(
        log_dir=tensorboard_logs_path,
        write_graph=True,
        write_images=True,
        histogram_freq=1,  # How often to log histogram visualizations
        embeddings_freq=1,  # How often to log embedding visualizations
        update_freq=&quot;epoch&quot;,
    )  # How often to write logs (default: once per epoch)

    model.compile(
        optimizer='adam',
        loss=tf.keras.losses.sparse_categorical_crossentropy,
        metrics=['accuracy']
    )
    history = model.fit(
        x,
        y,
        shuffle=True,
        batch_size=args.batch_size,
        epochs=args.epochs,
        validation_split=validation_split,
        use_multiprocessing=use_multiprocessing,
        workers=workers,
        verbose=verbosity,
        callbacks=[
            tensorboard_callback
        ]
    )
    return history


def create_model(input_shape, number_of_classes):
    model = Sequential([
        Conv2D(
            name=&quot;conv01&quot;,
            filters=32,
            kernel_size=(3, 3),
            strides=(1, 1),
            padding=&quot;same&quot;,
            activation='relu',
            input_shape=input_shape
        ),
        MaxPooling2D(
            name=&quot;pool01&quot;,
            pool_size=(2, 2)
        ),
        Flatten(),  # 3D shape to 1D.
        BatchNormalization(
            name=&quot;batch_before_full01&quot;
        ),
        Dense(
            name=&quot;full01&quot;,
            units=300,
            activation=&quot;relu&quot;
        ),  # Fully connected layer
        Dense(
            name=&quot;output_softmax&quot;,
            units=number_of_classes,
            activation=&quot;softmax&quot;
        )
    ])
    return model


def save_model(model, args):
    # Save the model
    # A version number is needed for the serving container
    # to load the model
    version = &quot;00000000&quot;
    model_save_dir = os.path.join(args.model_dir, version)
    if not os.path.exists(model_save_dir):
        os.makedirs(model_save_dir)
    print(f&quot;saving model at {model_save_dir}&quot;)
    model.save(model_save_dir)


def parse_args():
    # --------------------------------------------------------------------------------
    # https://docs.python.org/dev/library/argparse.html#dest
    # --------------------------------------------------------------------------------
    parser = argparse.ArgumentParser()

    # --------------------------------------------------------------------------------
    # hyperparameters Estimator argument are passed as command-line arguments to the script.
    # --------------------------------------------------------------------------------
    parser.add_argument('--epochs', type=int, default=10)
    parser.add_argument('--batch-size', type=int, default=64)

    # /opt/ml/model
    # sagemaker.tensorflow.estimator.TensorFlow override 'model_dir'.
    # See https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/\
    # sagemaker.tensorflow.html#sagemaker.tensorflow.estimator.TensorFlow
    parser.add_argument('--model_dir', type=str, default=os.environ['SM_MODEL_DIR'])

    # /opt/ml/output
    parser.add_argument(&quot;--output_dir&quot;, type=str, default=os.environ[&quot;SM_OUTPUT_DIR&quot;])

    args = parser.parse_args()
    return args


if __name__ == &quot;__main__&quot;:
    args = parse_args()
    print(&quot;---------- key/value args&quot;)
    for key, value in vars(args).items():
        print(f&quot;{key}:{value}&quot;)

    x_train, y_train, x_test, y_test, input_shape, number_of_classes = load_data()
    model = create_model(input_shape, number_of_classes)

    history = train(model=model, x=x_train, y=y_train, args=args)
    print(history)
    
    save_model(model, args)
    results = model.evaluate(x_test, y_test, batch_size=100)
    print(&quot;test loss, test accuracy:&quot;, results)
</code></pre>
<h2>SageMaker Console</h2>
<p><a href=""https://i.stack.imgur.com/ctcLy.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/ctcLy.png"" alt=""enter image description here"" /></a></p>
<h2>Notebook output</h2>
<pre><code>2021-09-03 03:02:04 Starting - Starting the training job...
2021-09-03 03:02:16 Starting - Launching requested ML instancesProfilerReport-1630638122: InProgress
......
2021-09-03 03:03:17 Starting - Preparing the instances for training.........
2021-09-03 03:04:59 Downloading - Downloading input data
2021-09-03 03:04:59 Training - Downloading the training image...
2021-09-03 03:05:23 Training - Training image download completed. Training in progress.2021-09-03 03:05:23.966037: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.
2021-09-03 03:05:23.969704: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.
2021-09-03 03:05:24.118054: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.
2021-09-03 03:05:26,842 sagemaker-training-toolkit INFO     Imported framework sagemaker_tensorflow_container.training
2021-09-03 03:05:26,852 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)
2021-09-03 03:05:27,734 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:
/usr/local/bin/python3.7 -m pip install -r requirements.txt
WARNING: You are using pip version 21.0.1; however, version 21.2.4 is available.
You should consider upgrading via the '/usr/local/bin/python3.7 -m pip install --upgrade pip' command.

2021-09-03 03:05:29,028 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)
2021-09-03 03:05:29,045 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)
2021-09-03 03:05:29,062 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)
2021-09-03 03:05:29,072 sagemaker-training-toolkit INFO     Invoking user script

Training Env:

{
    &quot;additional_framework_parameters&quot;: {},
    &quot;channel_input_dirs&quot;: {},
    &quot;current_host&quot;: &quot;algo-1&quot;,
    &quot;framework_module&quot;: &quot;sagemaker_tensorflow_container.training:main&quot;,
    &quot;hosts&quot;: [
        &quot;algo-1&quot;
    ],
    &quot;hyperparameters&quot;: {
        &quot;batch-size&quot;: 64,
        &quot;epochs&quot;: 2
    },
    &quot;input_config_dir&quot;: &quot;/opt/ml/input/config&quot;,
    &quot;input_data_config&quot;: {},
    &quot;input_dir&quot;: &quot;/opt/ml/input&quot;,
    &quot;is_master&quot;: true,
    &quot;job_name&quot;: &quot;fashion-mnist-2021-09-03-03-02-02-305&quot;,
    &quot;log_level&quot;: 20,
    &quot;master_hostname&quot;: &quot;algo-1&quot;,
    &quot;model_dir&quot;: &quot;/opt/ml/model&quot;,
    &quot;module_dir&quot;: &quot;s3://sagemaker-us-east-1-316725000538/fashion-mnist-2021-09-03-03-02-02-305/source/sourcedir.tar.gz&quot;,
    &quot;module_name&quot;: &quot;fashion_mnist&quot;,
    &quot;network_interface_name&quot;: &quot;eth0&quot;,
    &quot;num_cpus&quot;: 4,
    &quot;num_gpus&quot;: 0,
    &quot;output_data_dir&quot;: &quot;/opt/ml/output/data&quot;,
    &quot;output_dir&quot;: &quot;/opt/ml/output&quot;,
    &quot;output_intermediate_dir&quot;: &quot;/opt/ml/output/intermediate&quot;,
    &quot;resource_config&quot;: {
        &quot;current_host&quot;: &quot;algo-1&quot;,
        &quot;hosts&quot;: [
            &quot;algo-1&quot;
        ],
        &quot;network_interface_name&quot;: &quot;eth0&quot;
    },
    &quot;user_entry_point&quot;: &quot;fashion_mnist.py&quot;
}

Environment variables:

SM_HOSTS=[&quot;algo-1&quot;]
SM_NETWORK_INTERFACE_NAME=eth0
SM_HPS={&quot;batch-size&quot;:64,&quot;epochs&quot;:2}
SM_USER_ENTRY_POINT=fashion_mnist.py
SM_FRAMEWORK_PARAMS={}
SM_RESOURCE_CONFIG={&quot;current_host&quot;:&quot;algo-1&quot;,&quot;hosts&quot;:[&quot;algo-1&quot;],&quot;network_interface_name&quot;:&quot;eth0&quot;}
SM_INPUT_DATA_CONFIG={}
SM_OUTPUT_DATA_DIR=/opt/ml/output/data
SM_CHANNELS=[]
SM_CURRENT_HOST=algo-1
SM_MODULE_NAME=fashion_mnist
SM_LOG_LEVEL=20
SM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main
SM_INPUT_DIR=/opt/ml/input
SM_INPUT_CONFIG_DIR=/opt/ml/input/config
SM_OUTPUT_DIR=/opt/ml/output
SM_NUM_CPUS=4
SM_NUM_GPUS=0
SM_MODEL_DIR=/opt/ml/model
SM_MODULE_DIR=s3://sagemaker-us-east-1-316725000538/fashion-mnist-2021-09-03-03-02-02-305/source/sourcedir.tar.gz
SM_TRAINING_ENV={&quot;additional_framework_parameters&quot;:{},&quot;channel_input_dirs&quot;:{},&quot;current_host&quot;:&quot;algo-1&quot;,&quot;framework_module&quot;:&quot;sagemaker_tensorflow_container.training:main&quot;,&quot;hosts&quot;:[&quot;algo-1&quot;],&quot;hyperparameters&quot;:{&quot;batch-size&quot;:64,&quot;epochs&quot;:2},&quot;input_config_dir&quot;:&quot;/opt/ml/input/config&quot;,&quot;input_data_config&quot;:{},&quot;input_dir&quot;:&quot;/opt/ml/input&quot;,&quot;is_master&quot;:true,&quot;job_name&quot;:&quot;fashion-mnist-2021-09-03-03-02-02-305&quot;,&quot;log_level&quot;:20,&quot;master_hostname&quot;:&quot;algo-1&quot;,&quot;model_dir&quot;:&quot;/opt/ml/model&quot;,&quot;module_dir&quot;:&quot;s3://sagemaker-us-east-1-316725000538/fashion-mnist-2021-09-03-03-02-02-305/source/sourcedir.tar.gz&quot;,&quot;module_name&quot;:&quot;fashion_mnist&quot;,&quot;network_interface_name&quot;:&quot;eth0&quot;,&quot;num_cpus&quot;:4,&quot;num_gpus&quot;:0,&quot;output_data_dir&quot;:&quot;/opt/ml/output/data&quot;,&quot;output_dir&quot;:&quot;/opt/ml/output&quot;,&quot;output_intermediate_dir&quot;:&quot;/opt/ml/output/intermediate&quot;,&quot;resource_config&quot;:{&quot;current_host&quot;:&quot;algo-1&quot;,&quot;hosts&quot;:[&quot;algo-1&quot;],&quot;network_interface_name&quot;:&quot;eth0&quot;},&quot;user_entry_point&quot;:&quot;fashion_mnist.py&quot;}
SM_USER_ARGS=[&quot;--batch-size&quot;,&quot;64&quot;,&quot;--epochs&quot;,&quot;2&quot;]
SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate
SM_HP_BATCH-SIZE=64
SM_HP_EPOCHS=2
PYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/local/lib/python37.zip:/usr/local/lib/python3.7:/usr/local/lib/python3.7/lib-dynload:/usr/local/lib/python3.7/site-packages

Invoking script with the following command:

/usr/local/bin/python3.7 fashion_mnist.py --batch-size 64 --epochs 2


TensorFlow version: 2.3.1
Eager execution is: True
Keras version: 2.4.0
---------- key/value args
epochs:2
batch_size:64
model_dir:/opt/ml/model
output_dir:/opt/ml/output
</code></pre>",4.0,2021-09-02 04:01:47.170000 UTC,2022-07-04 05:43:30.063000 UTC,65.0,['amazon-sagemaker']
Azure Machine Learning Request Response latency,"<p>I have made an Azure Machine Learning Experiment which takes a small dataset (12x3 array) and some parameters and does some calculations using a few Python modules (a linear regression calculation and some more). This all works fine.</p>

<p>I have deployed the experiment and now want to throw data at it from the front-end of my application. The API-call goes in and comes back with correct results, but it takes up to 30 seconds to calculate a simple linear regression. Sometimes it is 20 seconds, sometimes only 1 second. I even got it down to 100 ms one time (which is what I'd like), but 90% of the time the request takes more than 20 seconds to complete, which is unacceptable.</p>

<p>I guess it has something to do with it still being an experiment, or it is still in a development slot, but I can't find the settings to get it to run on a faster machine.</p>

<p>Is there a way to speed up my execution?</p>

<p>Edit: To clarify: The varying timings are obtained with the same test data, simply by sending the same request multiple times. This made me conclude it must have something to do with my request being put in a queue, there is some start-up latency or I'm throttled in some other way.</p>",1,0,2016-01-25 10:40:39.993000 UTC,5.0,2016-01-27 16:15:36.527000 UTC,8,python|azure|azure-machine-learning-studio,1128,2015-10-29 11:07:20.793000 UTC,2020-05-19 23:12:48.357000 UTC,"Antwerp, Belgium",311,18,0,34,"<p>First, I am assuming you are doing your timing test on the published AML endpoint.</p>

<p>When a call is made to the AML the first call must warm up the container. By default a web service has 20 containers. Each container is cold, and a cold container can cause a large(30 sec) delay. In the string returned by the AML endpoint, only count requests that have the <code>isWarm</code> flag set to true. By smashing the service with MANY requests(relative to how many containers you have running) can get all your containers warmed.</p>

<p>If you are sending out dozens of requests a instance, the endpoint might be getting throttled. You can adjust the number of calls your endpoint can accept by going to manage.windowsazure.com/</p>

<ol>
<li>manage.windowsazure.com/</li>
<li>Azure ML Section from left bar</li>
<li>select your workspace</li>
<li>go to web services tab</li>
<li>Select your web service from list</li>
<li>adjust the number of calls with slider</li>
</ol>

<p>By enabling debugging onto your endpoint you can get logs about the execution time for each of your modules to complete. You can use this to determine if a module is not running as you intended which may add to the time.</p>

<p>Overall, there is an overhead when using the Execute python module, but I'd expect this request to complete in under 3 secs. </p>",11.0,2016-01-26 18:20:06.127000 UTC,2016-01-27 16:10:48.927000 UTC,8.0,['azure-machine-learning-studio']
How to run authentication on a mlFlow server?,"<p>As I am logging my entire models and params into mlflow I thought it will be a good idea to have  it protected under a user name and password.</p>

<p>I use the following code to run the mlflow server</p>

<p><code>mlflow server --host 0.0.0.0 --port 11111</code>
works perfect,in mybrowser i type <code>myip:11111</code> and i see everything (which eventually is the problem)</p>

<p>If I understood the documentation and the following <a href=""https://groups.google.com/forum/#!topic/mlflow-users/E9QW4HdS8a8"" rel=""noreferrer"">https://groups.google.com/forum/#!topic/mlflow-users/E9QW4HdS8a8</a> link here correct, I should use nginx to create the authentication.</p>

<p>I installed <code>nginx open sourcre</code>  and <code>apache2-utils</code></p>

<p>created <code>sudo htpasswd -c /etc/apache2/.htpasswd user1</code> user and passwords.</p>

<p>I edited my <code>/etc/nginx/nginx.conf</code> to the following:</p>

<pre><code>server {
        listen 80;
        listen 443 ssl;

        server_name my_ip;
        root NOT_SURE_WHICH_PATH_TO_PUT_HERE, THE VENV?;
        location / {
            proxy_pass                      my_ip:11111/;
            auth_basic                      ""Restricted Content"";
            auth_basic_user_file /home/path to the password file/.htpasswd;
        }
    }
</code></pre>

<p><strong>but no authentication appears.</strong></p>

<p>if I change the conf to listen to  <code>listen 11111</code>
I get an error that the port is already in use ( of course, by the mlflow server....)</p>

<p>my wish is to have a authentication window before anyone can enter by the mlflow with a browser.</p>

<p>would be happy to hear any suggestions.</p>",4,1,2019-11-20 14:16:40.087000 UTC,3.0,,11,nginx|basic-authentication|mlflow,13870,2019-04-03 13:42:48.017000 UTC,2022-09-10 19:35:30.057000 UTC,wondeland,1540,21,3,118,"<p>the problem here is that both <code>mlflow</code> and <code>nginx</code> are trying to run on the <strong>same port</strong>... </p>

<ol>
<li><p>first lets deal with nginx:</p>

<p>1.1 in /etc/nginx/sites-enable make a new file <code>sudo nano mlflow</code> and delete the exist default.</p>

<p>1.2 in mlflow file:</p></li>
</ol>

<pre><code>server {
    listen YOUR_PORT;
    server_name YOUR_IP_OR_DOMAIN;
    auth_basic           “Administrator’s Area”;
    auth_basic_user_file /etc/apache2/.htpasswd; #read the link below how to set username and pwd in nginx

    location / {
        proxy_pass http://localhost:8000;
        include /etc/nginx/proxy_params;
        proxy_redirect off;
    }
}
</code></pre>

<p>1.3.  restart nginx <code>sudo systemctl restart nginx</code></p>

<ol start=""2"">
<li>on your server run mlflow  <code>mlflow server --host localhost --port 8000</code></li>
</ol>

<p>Now if you try access the YOUR_IP_OR_DOMAIN:YOUR_PORT within your browser an auth popup should appear, enter your host and pass and now you in mlflow</p>

<ol start=""3"">
<li><p>now there are 2 options to tell the mlflow server about it:</p>

<p>3.1 set username and pwd as environment variable 
<code>export MLFLOW_TRACKING_USERNAME=user export MLFLOW_TRACKING_PASSWORD=pwd</code></p>

<p>3.2 edit in your <code>/venv/lib/python3.6/site-packages/mlflowpackages/mlflow/tracking/_tracking_service/utils.py</code> the function </p></li>
</ol>

<pre><code>def _get_rest_store(store_uri, **_):
    def get_default_host_creds():
        return rest_utils.MlflowHostCreds(
            host=store_uri,
            username=replace with nginx user
            password=replace with nginx pwd
            token=os.environ.get(_TRACKING_TOKEN_ENV_VAR),
            ignore_tls_verification=os.environ.get(_TRACKING_INSECURE_TLS_ENV_VAR) == 'true',
        )
</code></pre>

<p>in your .py file where you work with mlflow:</p>

<pre><code>import mlflow
remote_server_uri = ""YOUR_IP_OR_DOMAIN:YOUR_PORT"" # set to your server URI
mlflow.set_tracking_uri(remote_server_uri)
mlflow.set_experiment(""/my-experiment"")
with mlflow.start_run():
    mlflow.log_param(""a"", 1)
    mlflow.log_metric(""b"", 2)
</code></pre>

<p>A link to nginx authentication doc <a href=""https://docs.nginx.com/nginx/admin-guide/security-controls/configuring-http-basic-authentication/"" rel=""noreferrer"">https://docs.nginx.com/nginx/admin-guide/security-controls/configuring-http-basic-authentication/</a></p>",3.0,2019-12-13 16:37:32.617000 UTC,,8.0,['mlflow']
How to download the entire scored dataset from Azure machine studio?,"<p>I have an experiment in azure machine learning studio, and I would like to the see entire scored dataset.</p>

<p>Naturally I used the 'visualise' option on the scored dataset but these yields only 100 rows (the test dataset is around 500 rows)</p>

<p>I also tired the 'save as dataset' option, but then file does not open well with excel or text editor (special character encoding)</p>

<p>Basically, I want to see the entire test data with scored labels as table or download as .csv maybe</p>",2,0,2016-04-12 04:42:39.040000 UTC,,,1,azure-machine-learning-studio,5296,2014-07-25 05:27:39.940000 UTC,2022-09-15 08:56:29.147000 UTC,"Linköping, Sweden",1677,82,2,221,"<p>Please try the Convert to CSV module: <a href=""https://msdn.microsoft.com/library/azure/faa6ba63-383c-4086-ba58-7abf26b85814"" rel=""noreferrer"">https://msdn.microsoft.com/library/azure/faa6ba63-383c-4086-ba58-7abf26b85814</a></p>

<p>After you run the experiment, right click on the output of the module to download the CSV file.</p>",1.0,2016-04-12 04:57:38.280000 UTC,,14.0,['azure-machine-learning-studio']
Azure ML Loops through the different tasks,"<p>I've got the following data structure in an Azure DB Table:</p>

<pre><code>Client_ID | Customer_ID | Item | Preference_Score
</code></pre>

<p>The table can contain different datasets from different clients but the data structure is always the same. Then, the table is imported in Azure ML.</p>

<p>What I need is to repeat the same sequence of tasks in Azure ML for all the Client_ID in the above mentioned table.</p>

<p>So that in the end I will train a single model for each client and score the data of each single client individually and append the scored data and store it again in Azure SQL.</p>

<p>Is there any for each task in Azure ML like in SSIS? What's the best way to do this? </p>

<p>Thanks.</p>",1,0,2016-06-14 07:05:08.690000 UTC,,,1,loops|azure|machine-learning|azure-machine-learning-studio,575,2015-03-31 12:18:20.743000 UTC,2022-09-22 23:58:12.317000 UTC,,983,153,2,259,"<p>You can start from Azure Data Factory for automating batch scoring. In your model instead of web service output, you can use DataWriter exporter module to write the output directly into an Azure Table etc. You can check Microsoft MyDriving reference guide (<a href=""http://aka.ms/mydrivingdocs"" rel=""nofollow"">http://aka.ms/mydrivingdocs</a>) at page 107-8 where the machine learning section starts at page 100.</p>",0.0,2016-06-27 05:39:27.403000 UTC,,-1.0,['azure-machine-learning-studio']
How to connect AMLS to ADLS Gen 2?,"<p>I would like to register a dataset from ADLS Gen2 in my Azure Machine Learning workspace (<code>azureml-core==1.12.0</code>). Given that service principal information is not required in the Python SDK <a href=""https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.datastore.datastore?view=azure-ml-py#register-azure-data-lake-gen2-workspace--datastore-name--filesystem--account-name--tenant-id-none--client-id-none--client-secret-none--resource-url-none--authority-url-none--protocol-none--endpoint-none--overwrite-false-"" rel=""noreferrer"">documentation</a> for <code>.register_azure_data_lake_gen2()</code>, I successfully used the following code to register ADLS gen2 as a datastore:</p>
<pre class=""lang-py prettyprint-override""><code>from azureml.core import Datastore

adlsgen2_datastore_name = os.environ['adlsgen2_datastore_name']
account_name=os.environ['account_name'] # ADLS Gen2 account name
file_system=os.environ['filesystem']

adlsgen2_datastore = Datastore.register_azure_data_lake_gen2(
    workspace=ws,
    datastore_name=adlsgen2_datastore_name,
    account_name=account_name, 
    filesystem=file_system
)
</code></pre>
<p>However, when I try to register a dataset, using</p>
<pre class=""lang-py prettyprint-override""><code>from azureml.core import Dataset
adls_ds = Datastore.get(ws, datastore_name=adlsgen2_datastore_name)
data = Dataset.Tabular.from_delimited_files((adls_ds, 'folder/data.csv'))
</code></pre>
<p>I get an error</p>
<blockquote>
<p>Cannot load any data from the specified path. Make sure the path is accessible and contains data.
<code>ScriptExecutionException</code> was caused by <code>StreamAccessException</code>.
StreamAccessException was caused by AuthenticationException.
<code>'AdlsGen2-ReadHeaders'</code> for '[REDACTED]' on storage failed with status code 'Forbidden' (This request is not authorized to perform this operation using this permission.), client request ID &lt;CLIENT_REQUEST_ID&gt;, request ID &lt;REQUEST_ID&gt;. Error message: [REDACTED]
| session_id=&lt;SESSION_ID&gt;</p>
</blockquote>
<p>Do I need the to enable the service principal to get this to work? Using the ML Studio UI, it appears that the service principal is required even to register the datastore.</p>
<p>Another issue I noticed is that AMLS is trying to access the dataset here:
<code>https://adls_gen2_account_name.**dfs**.core.windows.net/container/folder/data.csv</code> whereas the actual URI in ADLS Gen2 is: <code>https://adls_gen2_account_name.**blob**.core.windows.net/container/folder/data.csv</code></p>",1,0,2020-09-14 20:39:51.930000 UTC,1.0,2020-09-15 09:03:51.357000 UTC,7,python|azure-machine-learning-service|azure-data-lake-gen2,3331,2020-05-17 18:00:51.347000 UTC,2022-06-27 19:36:47.687000 UTC,,179,2,0,53,"<p>According to this <a href=""https://docs.microsoft.com/en-us/azure/machine-learning/how-to-access-data#azure-data-lake-storage-generation-2"" rel=""noreferrer"">documentation</a>,you need to enable the service principal.</p>
<p>1.you need to register your application and grant the service principal with <strong>Storage Blob Data Reader access</strong>.</p>
<p><a href=""https://i.stack.imgur.com/FZl8O.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/FZl8O.png"" alt=""enter image description here"" /></a></p>
<p>2.try this code:</p>
<pre><code>adlsgen2_datastore = Datastore.register_azure_data_lake_gen2(workspace=ws,
                                                             datastore_name=adlsgen2_datastore_name,
                                                             account_name=account_name,
                                                             filesystem=file_system,
                                                             tenant_id=tenant_id,
                                                             client_id=client_id,
                                                             client_secret=client_secret
                                                             )

adls_ds = Datastore.get(ws, datastore_name=adlsgen2_datastore_name)
dataset = Dataset.Tabular.from_delimited_files((adls_ds,'sample.csv'))
print(dataset.to_pandas_dataframe())
</code></pre>
<p><strong>Result:</strong></p>
<p><a href=""https://i.stack.imgur.com/50mit.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/50mit.png"" alt=""enter image description here"" /></a></p>",0.0,2020-09-15 07:41:56.360000 UTC,2020-09-15 10:47:14.147000 UTC,9.0,['azure-machine-learning-service']
Load S3 Data into AWS SageMaker Notebook,"<p>I've just started to experiment with AWS SageMaker and would like to load data from an S3 bucket into a pandas dataframe in my SageMaker python jupyter notebook for analysis.</p>

<p>I could use boto to grab the data from S3, but I'm wondering whether there is a more elegant method as part of the SageMaker framework to do this in my python code?</p>

<p>Thanks in advance for any advice.</p>",8,0,2018-01-15 14:07:26.727000 UTC,15.0,,57,python|amazon-web-services|amazon-s3|machine-learning|amazon-sagemaker,78494,2017-02-17 17:02:25.117000 UTC,2021-01-07 13:51:11.447000 UTC,,673,14,0,20,"<p>If you have a look <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-dg.pdf"" rel=""noreferrer"">here</a> it seems you can specify this in the <em>InputDataConfig</em>. Search for ""S3DataSource"" (<a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/API_S3DataSource.html"" rel=""noreferrer"">ref</a>) in the document. The first hit is even in Python, on page 25/26.</p>",0.0,2018-01-15 17:16:02.537000 UTC,,11.0,['amazon-sagemaker']
sagemaker notebook instance Elastic Inference tensorflow model local deployment,"<p>I am trying to replicate <a href=""https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/tensorflow_serving_using_elastic_inference_with_your_own_model/tensorflow_serving_pretrained_model_elastic_inference.ipynb"" rel=""nofollow noreferrer"">https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/tensorflow_serving_using_elastic_inference_with_your_own_model/tensorflow_serving_pretrained_model_elastic_inference.ipynb</a></p>

<p>My elastic inference accelerator is attached to notebook instance. I am using conda_amazonei_tensorflow_p36 kernel. According to documentation I made the changes for local EI:</p>

<pre><code>%%time
import boto3

region = boto3.Session().region_name
saved_model = 's3://sagemaker-sample-data-{}/tensorflow/model/resnet/resnet_50_v2_fp32_NCHW.tar.gz'.format(region)

import sagemaker
from sagemaker.tensorflow.serving import Model

role = sagemaker.get_execution_role()

tensorflow_model = Model(model_data=saved_model,
role=role,
framework_version='1.14')
tf_predictor = tensorflow_model.deploy(initial_instance_count=1,
instance_type='local',
accelerator_type='local_sagemaker_notebook')
</code></pre>

<p>I am getting following log in the notebook:</p>

<pre><code>Attaching to tmp6uqys1el_algo-1-7ynb1_1
algo-1-7ynb1_1 | INFO:main:starting services
algo-1-7ynb1_1 | INFO:main:using default model name: Servo
algo-1-7ynb1_1 | INFO:main:tensorflow serving model config:
algo-1-7ynb1_1 | model_config_list: {
algo-1-7ynb1_1 | config: {
algo-1-7ynb1_1 | name: ""Servo"",
algo-1-7ynb1_1 | base_path: ""/opt/ml/model/export/Servo"",
algo-1-7ynb1_1 | model_platform: ""tensorflow""
algo-1-7ynb1_1 | }
algo-1-7ynb1_1 | }
algo-1-7ynb1_1 |
algo-1-7ynb1_1 |
algo-1-7ynb1_1 | INFO:main:nginx config:
algo-1-7ynb1_1 | load_module modules/ngx_http_js_module.so;
algo-1-7ynb1_1 |
algo-1-7ynb1_1 | worker_processes auto;
algo-1-7ynb1_1 | daemon off;
algo-1-7ynb1_1 | pid /tmp/nginx.pid;
algo-1-7ynb1_1 | error_log /dev/stderr error;
algo-1-7ynb1_1 |
algo-1-7ynb1_1 | worker_rlimit_nofile 4096;
algo-1-7ynb1_1 |
algo-1-7ynb1_1 | events {
algo-1-7ynb1_1 | worker_connections 2048;
algo-1-7ynb1_1 | }
algo-1-7ynb1_1 |
algo-1-7ynb1_1 | http {
algo-1-7ynb1_1 | include /etc/nginx/mime.types;
algo-1-7ynb1_1 | default_type application/json;
algo-1-7ynb1_1 | access_log /dev/stdout combined;
algo-1-7ynb1_1 | js_include tensorflow-serving.js;
algo-1-7ynb1_1 |
algo-1-7ynb1_1 | upstream tfs_upstream {
algo-1-7ynb1_1 | server localhost:8501;
algo-1-7ynb1_1 | }
algo-1-7ynb1_1 |
algo-1-7ynb1_1 | upstream gunicorn_upstream {
algo-1-7ynb1_1 | server unix:/tmp/gunicorn.sock fail_timeout=1;
algo-1-7ynb1_1 | }
algo-1-7ynb1_1 |
algo-1-7ynb1_1 | server {
algo-1-7ynb1_1 | listen 8080 deferred;
algo-1-7ynb1_1 | client_max_body_size 0;
algo-1-7ynb1_1 | client_body_buffer_size 100m;
algo-1-7ynb1_1 | subrequest_output_buffer_size 100m;
algo-1-7ynb1_1 |
algo-1-7ynb1_1 | set $tfs_version 1.14;
algo-1-7ynb1_1 | set $default_tfs_model Servo;
algo-1-7ynb1_1 |
algo-1-7ynb1_1 | location /tfs {
algo-1-7ynb1_1 | rewrite ^/tfs/(.) /$1 break;
algo-1-7ynb1_1 | proxy_redirect off;
algo-1-7ynb1_1 | proxy_pass_request_headers off;
algo-1-7ynb1_1 | proxy_set_header Content-Type 'application/json';
algo-1-7ynb1_1 | proxy_set_header Accept 'application/json';
algo-1-7ynb1_1 | proxy_pass http://tfs_upstream;
algo-1-7ynb1_1 | }
algo-1-7ynb1_1 |
algo-1-7ynb1_1 | location /ping {
algo-1-7ynb1_1 | js_content ping;
algo-1-7ynb1_1 | }
algo-1-7ynb1_1 |
algo-1-7ynb1_1 | location /invocations {
algo-1-7ynb1_1 | js_content invocations;
algo-1-7ynb1_1 | }
algo-1-7ynb1_1 |
algo-1-7ynb1_1 | location ~ ^/models/(.)/invoke {
algo-1-7ynb1_1 | js_content invocations;
algo-1-7ynb1_1 | }
algo-1-7ynb1_1 |
algo-1-7ynb1_1 | location /models {
algo-1-7ynb1_1 | proxy_pass http://gunicorn_upstream/models;
algo-1-7ynb1_1 | }
algo-1-7ynb1_1 |
algo-1-7ynb1_1 | location / {
algo-1-7ynb1_1 | return 404 '{""error"": ""Not Found""}';
algo-1-7ynb1_1 | }
algo-1-7ynb1_1 |
algo-1-7ynb1_1 | keepalive_timeout 3;
algo-1-7ynb1_1 | }
algo-1-7ynb1_1 | }
algo-1-7ynb1_1 |
algo-1-7ynb1_1 |
algo-1-7ynb1_1 | INFO:main:tensorflow version info:
algo-1-7ynb1_1 | TensorFlow ModelServer: 1.14.0-rc0+dev.sha.34d9e85
algo-1-7ynb1_1 | TensorFlow Library: 1.14.0
algo-1-7ynb1_1 | EI Version: EI-1.4
algo-1-7ynb1_1 | INFO:main:tensorflow serving command: tensorflow_model_server --port=9000 --rest_api_port=8501 --model_config_file=/sagemaker/model-config.cfg
algo-1-7ynb1_1 | INFO:main:started tensorflow serving (pid: 8)
algo-1-7ynb1_1 | INFO:main:nginx version info:
algo-1-7ynb1_1 | nginx version: nginx/1.16.1
algo-1-7ynb1_1 | built by gcc 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.11)
algo-1-7ynb1_1 | built with OpenSSL 1.0.2g 1 Mar 2016
algo-1-7ynb1_1 | TLS SNI support enabled
algo-1-7ynb1_1 | configure arguments: --prefix=/etc/nginx --sbin-path=/usr/sbin/nginx --modules-path=/usr/lib/nginx/modules --conf-path=/etc/nginx/nginx.conf --error-log-path=/var/log/nginx/error.log --http-log-path=/var/log/nginx/access.log --pid-path=/var/run/nginx.pid --lock-path=/var/run/nginx.lock --http-client-body-temp-path=/var/cache/nginx/client_temp --http-proxy-temp-path=/var/cache/nginx/proxy_temp --http-fastcgi-temp-path=/var/cache/nginx/fastcgi_temp --http-uwsgi-temp-path=/var/cache/nginx/uwsgi_temp --http-scgi-temp-path=/var/cache/nginx/scgi_temp --user=nginx --group=nginx --with-compat --with-file-aio --with-threads --with-http_addition_module --with-http_auth_request_module --with-http_dav_module --with-http_flv_module --with-http_gunzip_module --with-http_gzip_static_module --with-http_mp4_module --with-http_random_index_module --with-http_realip_module --with-http_secure_link_module --with-http_slice_module --with-http_ssl_module --with-http_stub_status_module --with-http_sub_module --with-http_v2_module --with-mail --with-mail_ssl_module --with-stream --with-stream_realip_module --with-stream_ssl_module --with-stream_ssl_preread_module --with-cc-opt='-g -O2 -fPIE -fstack-protector-strong -Wformat -Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -fPIC' --with-ld-opt='-Wl,-Bsymbolic-functions -fPIE -pie -Wl,-z,relro -Wl,-z,now -Wl,--as-needed -pie'
algo-1-7ynb1_1 | INFO:main:started nginx (pid: 10)
algo-1-7ynb1_1 | 2020-06-17 05:02:08.888114: I tensorflow_serving/model_servers/server_core.cc:462] Adding/updating models.
algo-1-7ynb1_1 | 2020-06-17 05:02:08.888186: I tensorflow_serving/model_servers/server_core.cc:561] (Re-)adding model: Servo
algo-1-7ynb1_1 | 2020-06-17 05:02:08.988623: I tensorflow_serving/core/basic_manager.cc:739] Successfully reserved resources to load servable {name: Servo version: 1527887769}
algo-1-7ynb1_1 | 2020-06-17 05:02:08.988688: I tensorflow_serving/core/loader_harness.cc:66] Approving load for servable version {name: Servo version: 1527887769}
algo-1-7ynb1_1 | 2020-06-17 05:02:08.988728: I tensorflow_serving/core/loader_harness.cc:74] Loading servable version {name: Servo version: 1527887769}
algo-1-7ynb1_1 | 2020-06-17 05:02:08.988762: I external/org_tensorflow/tensorflow/contrib/session_bundle/bundle_shim.cc:363] Attempting to load native SavedModelBundle in bundle-shim from: /opt/ml/model/export/Servo/1527887769
algo-1-7ynb1_1 | 2020-06-17 05:02:08.988783: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:31] Reading SavedModel from: /opt/ml/model/export/Servo/1527887769
algo-1-7ynb1_1 | 2020-06-17 05:02:09.001922: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:54] Reading meta graph with tags { serve }
algo-1-7ynb1_1 | 2020-06-17 05:02:09.082734: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:202] Restoring SavedModel bundle.
algo-1-7ynb1_1 | 2020-06-17 05:02:09.613725: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:151] Running initialization op on SavedModel bundle at path: /opt/ml/model/export/Servo/1527887769
algo-1-7ynb1_1 | Using Amazon Elastic Inference Client Library Version: 1.5.3
algo-1-7ynb1_1 | Number of Elastic Inference Accelerators Available: 1
algo-1-7ynb1_1 | Elastic Inference Accelerator ID: eia-813285f77ceb448c849e2331116f251b
algo-1-7ynb1_1 | Elastic Inference Accelerator Type: eia2.medium
algo-1-7ynb1_1 | Elastic Inference Accelerator Ordinal: 0
algo-1-7ynb1_1 |
!algo-1-7ynb1_1 | 172.18.0.1 - - [17/Jun/2020:05:02:10 +0000] ""GET /ping HTTP/1.1"" 200 3 ""-"" ""-""
algo-1-7ynb1_1 | [Wed Jun 17 05:02:11 2020, 662569us] [Execution Engine] Error getting application context for [TensorFlow][2]
algo-1-7ynb1_1 | [Wed Jun 17 05:02:11 2020, 662722us] [Execution Engine][TensorFlow][2] Failed - Last Error:
algo-1-7ynb1_1 | EI Error Code: [3, 16, 8]
algo-1-7ynb1_1 | EI Error Description: Unable to authenticate with accelerator
algo-1-7ynb1_1 | EI Request ID: TF-D66B9810-D81A-448F-ACE2-703FFFA0F194 -- EI Accelerator ID: eia-813285f77ceb448c849e2331116f251b
algo-1-7ynb1_1 | EI Client Version: 1.5.3
algo-1-7ynb1_1 | 2020-06-17 05:02:11.668412: F external/org_tensorflow/tensorflow/contrib/ei/session/eia_session.cc:1219] Non-OK-status: SwapExStateWithEI(tmp_inputs, tmp_outputs, tmp_freeze) status: Internal: Failed to get the initial operator whitelist from server.
algo-1-7ynb1_1 | WARNING:main:unexpected tensorflow serving exit (status: 6). restarting.
algo-1-7ynb1_1 | INFO:main:tensorflow version info:
algo-1-7ynb1_1 | TensorFlow ModelServer: 1.14.0-rc0+dev.sha.34d9e85
algo-1-7ynb1_1 | TensorFlow Library: 1.14.0
algo-1-7ynb1_1 | EI Version: EI-1.4
algo-1-7ynb1_1 | INFO:main:tensorflow serving command: tensorflow_model_server --port=9000 --rest_api_port=8501 --model_config_file=/sagemaker/model-config.cfg
algo-1-7ynb1_1 | INFO:main:started tensorflow serving (pid: 38)`enter code here`
algo-1-7ynb1_1 | 2020-06-17 05:02:11.759706: I tensorflow_serving/model_servers/server_core.cc:462] Adding/updating models.
algo-1-7ynb1_1 | 2020-06-17 05:02:11.759783: I tensorflow_serving/model_servers/server_core.cc:561] (Re-)adding model: Servo
algo-1-7ynb1_1 | 2020-06-17 05:02:11.860242: I tensorflow_serving/core/basic_manager.cc:739] Successfully reserved resources to load servable {name: Servo version: 1527887769}
algo-1-7ynb1_1 | 2020-06-17 05:02:11.860309: I tensorflow_serving/core/loader_harness.cc:66] Approving load for servable version {name: Servo version: 1527887769}
algo-1-7ynb1_1 | 2020-06-17 05:02:11.860333: I tensorflow_serving/core/loader_harness.cc:74] Loading servable version {name: Servo version: 1527887769}
algo-1-7ynb1_1 | 2020-06-17 05:02:11.860365: I external/org_tensorflow/tensorflow/contrib/session_bundle/bundle_shim.cc:363] Attempting to load native SavedModelBundle in bundle-shim from: /opt/ml/model/export/Servo/1527887769
algo-1-7ynb1_1 | 2020-06-17 05:02:11.860382: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:31] Reading SavedModel from: /opt/ml/model/export/Servo/1527887769
algo-1-7ynb1_1 | 2020-06-17 05:02:11.873381: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:54] Reading meta graph with tags { serve }
algo-1-7ynb1_1 | 2020-06-17 05:02:11.949421: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:202] Restoring SavedModel bundle.
algo-1-7ynb1_1 | 2020-06-17 05:02:12.512935: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:151] Running initialization op on SavedModel bundle at path: /opt/ml/model/export/Servo/1527887769
algo-1-7ynb1_1 | Using Amazon Elastic Inference Client Library Version: 1.5.3
algo-1-7ynb1_1 | Number of Elastic Inference Accelerators Available: 1
algo-1-7ynb1_1 | Elastic Inference Accelerator ID: eia-813285f77ceb448c849e2331116f251b
algo-1-7ynb1_1 | Elastic Inference Accelerator Type: eia2.medium
algo-1-7ynb1_1 | Elastic Inference Accelerator Ordinal: 0
`
</code></pre>

<p>The log never stops in notebook. It keeps throwing in notebook cells. I am not sure whether the model is deployed correctly.</p>

<p>I can see the docker of the model running
<a href=""https://i.stack.imgur.com/YRXKL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YRXKL.png"" alt=""enter image description here""></a></p>

<p>When I try to infer/predict from that model, I get error:</p>

<pre><code>algo-1-iikpj_1 | [Wed Jun 17 05:29:47 2020, 761607us] [Execution Engine] Error getting application context for [TensorFlow][2]

algo-1-iikpj_1 | [Wed Jun 17 05:29:47 2020, 761691us] [Execution Engine][TensorFlow][2] Failed - Last Error:
algo-1-iikpj_1 | EI Error Code: [3, 16, 8]
algo-1-iikpj_1 | EI Error Description: Unable to authenticate with accelerator
algo-1-iikpj_1 | EI Request ID: TF-ADECD8EF-7138-4B5F-9C37-ADFDC8122DF1 -- EI Accelerator ID: eia-813285f77ceb448c849e2331116f251b
algo-1-iikpj_1 | EI Client Version: 1.5.3
algo-1-iikpj_1 | 2020-06-17 05:29:47.768249: F external/org_tensorflow/tensorflow/contrib/ei/session/eia_session.cc:1219] Non-OK-status: SwapExStateWithEI(tmp_inputs, tmp_outputs, tmp_freeze) status: Internal: Failed to get the initial operator whitelist from server.
algo-1-iikpj_1 | WARNING:main:unexpected tensorflow serving exit (status: 6). restarting.
algo-1-iikpj_1 | INFO:main:tensorflow version info:
algo-1-iikpj_1 | TensorFlow ModelServer: 1.14.0-rc0+dev.sha.34d9e85
algo-1-iikpj_1 | TensorFlow Library: 1.14.0
algo-1-iikpj_1 | EI Version: EI-1.4
algo-1-iikpj_1 | INFO:main:tensorflow serving command: tensorflow_model_server --port=9000 --rest_api_port=8501 --model_config_file=/sagemaker/model-config.cfg
algo-1-iikpj_1 | INFO:main:started tensorflow serving (pid: 1052)
algo-1-iikpj_1 | 2020-06-17 05:29:47.854331: I tensorflow_serving/model_servers/server_core.cc:462] Adding/updating models.
algo-1-iikpj_1 | 2020-06-17 05:29:47.854405: I tensorflow_serving/model_servers/server_core.cc:561] (Re-)adding model: Servo
algo-1-iikpj_1 | 2020/06/17 05:29:47 [error] 11#11: *2 connect() failed (111: Connection refused) while connecting to upstream, client: 172.18.0.1, server: , request: ""POST /invocations HTTP/1.1"", subrequest: ""/v1/models/Servo:predict"", upstream: ""http://127.0.0.1:8501/v1/models/Servo:predict"", host: ""localhost:8080""
algo-1-iikpj_1 | 2020/06/17 05:29:47 [error] 11#11: *2 connect() failed (111: Connection refused) while connecting to upstream, client: 172.18.0.1, server: , request: ""POST /invocations HTTP/1.1"", subrequest: ""/v1/models/Servo:predict"", upstream: ""http://127.0.0.1:8501/v1/models/Servo:predict"", host: ""localhost:8080""
algo-1-iikpj_1 | 172.18.0.1 - - [17/Jun/2020:05:29:47 +0000] ""POST /invocations HTTP/1.1"" 502 157 ""-"" ""-""
algo-1-iikpj_1 | 2020-06-17 05:29:47.954825: I tensorflow_serving/core/basic_manager.cc:739] Successfully reserved resources to load servable {name: Servo version: 1527887769}
algo-1-iikpj_1 | 2020-06-17 05:29:47.954887: I tensorflow_serving/core/loader_harness.cc:66] Approving load for servable version {name: Servo version: 1527887769}
algo-1-iikpj_1 | 2020-06-17 05:29:47.955448: I tensorflow_serving/core/loader_harness.cc:74] Loading servable version {name: Servo version: 1527887769}
algo-1-iikpj_1 | 2020-06-17 05:29:47.955494: I external/org_tensorflow/tensorflow/contrib/session_bundle/bundle_shim.cc:363] Attempting to load native SavedModelBundle in bundle-shim from: /opt/ml/model/export/Servo/1527887769
algo-1-iikpj_1 | 2020-06-17 05:29:47.955859: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:31] Reading SavedModel from: /opt/ml/model/export/Servo/1527887769
algo-1-iikpj_1 | 2020-06-17 05:29:47.969511: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:54] Reading meta graph with tags { serve }
JSONDecodeError Traceback (most recent call last)
in ()

~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/sagemaker/tensorflow/serving.py in predict(self, data, initial_args)
116 args[""CustomAttributes""] = self._model_attributes
117
--&gt; 118 return super(Predictor, self).predict(data, args)
119
120

~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/sagemaker/predictor.py in predict(self, data, initial_args, target_model)
109 request_args = self._create_request_args(data, initial_args, target_model)
110 response = self.sagemaker_session.sagemaker_runtime_client.invoke_endpoint(**request_args)
--&gt; 111 return self._handle_response(response)
112
113 def _handle_response(self, response):

~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/sagemaker/predictor.py in _handle_response(self, response)
119 if self.deserializer is not None:
120 # It's the deserializer's responsibility to close the stream
--&gt; 121 return self.deserializer(response_body, response[""ContentType""])
122 data = response_body.read()
123 response_body.close()

~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/sagemaker/predictor.py in call(self, stream, content_type)
578 """"""
579 try:
--&gt; 580 return json.load(codecs.getreader(""utf-8"")(stream))
581 finally:
582 stream.close()

~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/json/init.py in load(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)
297 cls=cls, object_hook=object_hook,
298 parse_float=parse_float, parse_int=parse_int,
--&gt; 299 parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)
300
301

~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/json/init.py in loads(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)
352 parse_int is None and parse_float is None and
353 parse_constant is None and object_pairs_hook is None and not kw):
--&gt; 354 return _default_decoder.decode(s)
355 if cls is None:
356 cls = JSONDecoder

~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/json/decoder.py in decode(self, s, _w)
337
338 """"""
--&gt; 339 obj, end = self.raw_decode(s, idx=_w(s, 0).end())
340 end = _w(s, end).end()
341 if end != len(s):

~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/json/decoder.py in raw_decode(self, s, idx)
355 obj, end = self.scan_once(s, idx)
356 except StopIteration as err:
--&gt; 357 raise JSONDecodeError(""Expecting value"", s, err.value) from None
358 return obj, end

JSONDecodeError: Expecting value: line 1 column 1 (char 0)

algo-1-iikpj_1 | 2020-06-17 05:29:48.047106: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:202] Restoring SavedModel bundle.
algo-1-iikpj_1 | 2020-06-17 05:29:48.564452: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:151] Running initialization op on SavedModel bundle at path: /opt/ml/model/export/Servo/1527887769
algo-1-iikpj_1 | Using Amazon Elastic Inference Client Library Version: 1.5.3
</code></pre>

<p>I tried several ways to solve JSONDecodeError: Expecting value: line 1 column 1 (char 0) using json.loads, json.dumps etc but nothing helps.
I also tried Rest API post to docker deployed model:</p>

<pre><code>curl -v -X POST \ -H 'content-type:application/json' \ -d '{""data"": {""inputs"": [[[[0.13075708159043742, 0.048010725848070535, 0.9012465727287071], [0.1643217202482622, 0.7392467524276859, 0.5618572640643519], [0.7697097217983989, 0.9829998452540657, 0.08567413146192027]]]]} }' \ http://127.0.0.1:8080/v1/models/Servo:predict
but still getting error:
[![enter image description here][1]][1]
</code></pre>

<p>Please help me to resolve the issue. Initially, I was trying to use my tensorflow serving model and getting the same errors. Then I thought of following with the same model which was used in AWS example notebook (resnet_50_v2_fp32_NCHW.tar.gz'). So, the above experiment is using AWS example notebook with model provided by sagemaker-sample-data.</p>

<p>Please help me out. Thanks</p>",1,0,2020-06-17 06:25:29.670000 UTC,,,2,tensorflow|amazon-sagemaker,350,2011-07-15 03:14:53.433000 UTC,2022-09-22 06:31:23.927000 UTC,,2889,79,0,62,"<p>Solved it. The error I was getting is due to roles/permission of elastic inference attached to notebook. Once fixed these permissions by our devops team. It worked as expected.  See <a href=""https://github.com/aws/sagemaker-tensorflow-serving-container/issues/142"" rel=""nofollow noreferrer"">https://github.com/aws/sagemaker-tensorflow-serving-container/issues/142</a></p>",2.0,2020-06-23 01:37:54.340000 UTC,,-1.0,['amazon-sagemaker']
Intel optimized Python on Machine Learning Service Compute,"<p>Is it possible to run a Python script or Estimator step on the Azure Machine Learning Service in a container with the intel optimized Python distribution?
I understand this is available on the <a href=""https://azure.microsoft.com/en-in/blog/intel-and-microsoft-bring-optimizations-to-deep-learning-on-azure/"" rel=""nofollow noreferrer"">Azure Data Science VMs</a> (<a href=""https://www.intel.ai/intel-optimized-data-science-virtual-machine-azure/#gs.yuntlp"" rel=""nofollow noreferrer"">or described here</a>), but I could not find out how to use this as an Azure Machine Learning Service Compute target.</p>

<p>For my current use case I am specifically interested in using an mkl linked numpy package in the aml service container.</p>

<p>Note: Running numpy.show_config() inside the container suggests numpy is linked against openblas and not mkl</p>

<pre><code>blas_mkl_info:
  NOT AVAILABLE
blis_info:
  NOT AVAILABLE
openblas_info:
    libraries = ['openblas', 'openblas']
    library_dirs = ['/usr/local/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None)]
blas_opt_info:
    libraries = ['openblas', 'openblas']
    library_dirs = ['/usr/local/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None)]
lapack_mkl_info:
  NOT AVAILABLE
openblas_lapack_info:
    libraries = ['openblas', 'openblas']
    library_dirs = ['/usr/local/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None)]
lapack_opt_info:
    libraries = ['openblas', 'openblas']
    library_dirs = ['/usr/local/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None)]
</code></pre>",1,0,2020-03-04 11:54:09.310000 UTC,,2020-03-04 20:46:17.670000 UTC,0,azure-machine-learning-service,81,2020-02-20 08:37:04.717000 UTC,2021-04-11 10:00:37.967000 UTC,,23,0,0,3,"<p>The Azure ML base images use <a href=""https://docs.conda.io/en/latest/miniconda.html"" rel=""nofollow noreferrer"">Miniconda</a> Python distribution, which uses MKL.</p>

<p>You can find the details of the base images here:<a href=""https://github.com/Azure/AzureML-Containers"" rel=""nofollow noreferrer"">https://github.com/Azure/AzureML-Containers</a></p>

<p>Also, if you install using Anaconda numpy in following way</p>

<pre><code>conda_dep.add_conda_package(""numpy"")
runconfig.run_config.environment.python.conda_dependencies = conda_dep
</code></pre>

<p>you should see this kind of output from <code>numpy.show_config()</code>. </p>

<blockquote>
  <p>blas_mkl_info:</p>
  
  <p>libraries = ['blas', 'cblas', 'lapack', 'pthread', 'blas', 'cblas',
  'lapack']</p>
  
  <p>library_dirs =
  ['/azureml-envs/azureml_a8ad8e485613e21e6e8adc1bfda86b40/lib']</p>
  
  <p>define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]</p>
  
  <p>include_dirs =
  ['/azureml-envs/azureml_a8ad8e485613e21e6e8adc1bfda86b40/include']</p>
</blockquote>",5.0,2020-03-04 18:14:24.733000 UTC,2020-03-05 19:46:14.647000 UTC,-1.0,['azure-machine-learning-service']
upload data to S3 with sagemaker,"<p>I have a problem with SageMaker when I try to upload Data into S3 bucket . I get this error : </p>

<blockquote>
  <hr>

<pre><code>NameError                                 Traceback (most recent call last)
&lt;ipython-input-26-d21b1cb0fcab&gt; in &lt;module&gt;()
     19 download('http://data.mxnet.io/data/caltech-256/caltech-256-60-train.rec')
     20 
---&gt; 21 upload_to_s3('train', 'caltech-256-60-train.rec')

&lt;ipython-input-26-d21b1cb0fcab&gt; in upload_to_s3(channel, file)
     13     data = open(file, ""rb"")
     14     key = channel + '/' + file
---&gt; 15     s3.Bucket(bucket).put_object(Key=key, Body=data)
     16 
     17 

NameError: name 'bucket' is not defined
</code></pre>
</blockquote>

<p>Here is the script:</p>

<pre class=""lang-python prettyprint-override""><code>import os
import urllib.request
import boto3

def download(url):
    filename = url.split(""/"")[-1]
    if not os.path.exists(filename):
        urllib.request.urlretrieve(url, filename)


def upload_to_s3(channel, file):
    s3 = boto3.resource('s3')
    data = open(file, ""rb"")
    key = channel + '/' + file
    s3.Bucket(bucket).put_object(Key=key, Body=data)


# caltech-256 download('http://data.mxnet.io/data/caltech-256/caltech-256-60-train.rec')

upload_to_s3('train', 'caltech-256-60-train.rec')
</code></pre>",1,0,2018-04-23 09:31:28.687000 UTC,1.0,2018-12-06 22:53:27.193000 UTC,5,amazon-s3|amazon-sagemaker,9575,2013-03-27 10:20:38.763000 UTC,2022-05-09 06:03:41.087000 UTC,,187,10,0,108,"<p>It is exactly as the error say, the variable <code>bucket</code> is not defined. 
you might want to do something like </p>

<pre><code>bucket = &lt;name of already created bucket in s3&gt;
</code></pre>

<p>before you call </p>

<pre><code>s3.Bucket(bucket).put_object(Key=key, Body=data)
</code></pre>",1.0,2018-04-23 16:25:22.233000 UTC,,8.0,['amazon-sagemaker']
Jupyter notebook kernel dies when creating dummy variables with pandas,"<p>I am working on the Walmart Kaggle competition and I'm trying to create a dummy column of of the ""FinelineNumber"" column. For context, <code>df.shape</code> returns <code>(647054, 7)</code>. I am trying to make a dummy column for <code>df['FinelineNumber']</code>, which has 5,196 unique values. The results should be a dataframe of shape <code>(647054, 5196)</code>, which I then plan to <code>concat</code> to the original dataframe. </p>

<p>Nearly every time I run <code>fineline_dummies = pd.get_dummies(df['FinelineNumber'], prefix='fl')</code>, I get the following error message <code>The kernel appears to have died. It will restart automatically.</code> I am running python 2.7 in jupyter notebook on a MacBookPro with 16GB RAM.</p>

<p>Can someone explain why this is happening (and why it happens most of the time but not every time)? Is it a jupyter notebook or pandas bug? Also, I thought it might have to do with not enough RAM but I get the same error on a Microsoft Azure Machine Learning notebook with >100 GB of RAM. On Azure ML, the kernel dies every time - almost immediately.</p>",1,0,2015-12-02 16:24:31.520000 UTC,1.0,2016-02-01 04:22:08.203000 UTC,5,python|pandas|ipython-notebook|azure-machine-learning-studio,5063,2013-07-10 17:00:15.300000 UTC,2022-09-21 17:11:58.293000 UTC,"New York, United States",2037,61,1,193,"<p>It very much could be memory usage - a 647054, 5196 data frame has 3,362,092,584 elements, which would be 24GB just for the pointers to the objects on a 64-bit system.  On AzureML while the VM has a large amount of memory you're actually limited in how much memory you have available (currently 2GB, soon to be 4GB) - and when you hit the limit the kernel typically dies.  So it seems very likely it is a memory usage issue.</p>

<p>You might try doing <a href=""http://pandas.pydata.org/pandas-docs/stable/sparse.html"" rel=""noreferrer"">.to_sparse()</a> on the data frame first before doing any additional manipulations.  That should allow Pandas to keep most of the data frame out of memory.</p>",0.0,2015-12-10 17:25:00.887000 UTC,,8.0,['azure-machine-learning-studio']
How do I implement a PyTorch Dataset for use with AWS SageMaker?,"<p>I have implemented a PyTorch <code>Dataset</code> that works locally (on my own desktop), but when executed on AWS SageMaker, it breaks. My <code>Dataset</code> implementation is as follows.</p>

<pre><code>class ImageDataset(Dataset):
    def __init__(self, path='./images', transform=None):
        self.path = path
        self.files = [join(path, f) for f in listdir(path) if isfile(join(path, f)) and f.endswith('.jpg')]
        self.transform = transform
        if transform is None:
            self.transform = transforms.Compose([
                transforms.Resize((128, 128)),
                transforms.ToTensor(),
                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
            ])

    def __len__(self):
        return len(files)

    def __getitem__(self, idx):
        img_name = self.files[idx]

        # we may infer the label from the filename
        dash_idx = img_name.rfind('-')
        dot_idx = img_name.rfind('.')
        label = int(img_name[dash_idx + 1:dot_idx])

        image = Image.open(img_name)

        if self.transform:
            image = self.transform(image)

        return image, label
</code></pre>

<p>I am following this <a href=""https://github.com/aws/sagemaker-python-sdk/tree/master/src/sagemaker/pytorch"" rel=""noreferrer"">example</a> and this <a href=""https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/pytorch_cnn_cifar10/pytorch_local_mode_cifar10.ipynb"" rel=""noreferrer"">one too</a>, and I run the <code>estimator</code> as follows.</p>

<pre><code>inputs = {
 'train': 'file://images',
 'eval': 'file://images'
}
estimator = PyTorch(entry_point='pytorch-train.py',
                            role=role,
                            framework_version='1.0.0',
                            train_instance_count=1,
                            train_instance_type=instance_type)
estimator.fit(inputs)
</code></pre>

<p>I get the following error.</p>

<blockquote>
  <p>FileNotFoundError: [Errno 2] No such file or directory: './images'</p>
</blockquote>

<p>In the example that I am following, they upload the CFAIR dataset (which is downloaded locally) to S3.</p>

<pre><code>inputs = sagemaker_session.upload_data(path='data', bucket=bucket, key_prefix='data/cifar10')
</code></pre>

<p>If I take a peek at <code>inputs</code>, it is just a string literal <code>s3://sagemaker-us-east-3-184838577132/data/cifar10</code>. The code to create a <code>Dataset</code> and a <code>DataLoader</code> is shown <a href=""https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/pytorch_mnist/mnist.py#L41"" rel=""noreferrer"">here</a>, which does not help unless I track down the source and step through the logic.</p>

<p>I think what needs to happen inside my <code>ImageDataset</code> is to supply the <code>S3</code> path and use the <code>AWS CLI</code> or something to query the files and acquire their content. I do not think the <code>AWS CLI</code> is the right approach as this relies on the console and I will have to execute some sub-process commands and then parse through. </p>

<p>There must be a recipe or something to create a custom <code>Dataset</code> backed by <code>S3</code> files, right?</p>",2,0,2019-01-02 08:02:40.377000 UTC,,,8,python|amazon-s3|pytorch|amazon-sagemaker,4597,2013-03-15 17:49:38.320000 UTC,2022-09-22 03:51:42.617000 UTC,,7643,416,10,515,"<p>I was able to create a PyTorch <code>Dataset</code> backed by S3 data using <code>boto3</code>. Here's the snippet if anyone is interested.</p>

<pre><code>class ImageDataset(Dataset):
    def __init__(self, path='./images', transform=None):
        self.path = path
        self.s3 = boto3.resource('s3')
        self.bucket = self.s3.Bucket(path)
        self.files = [obj.key for obj in self.bucket.objects.all()]
        self.transform = transform
        if transform is None:
            self.transform = transforms.Compose([
                transforms.Resize((128, 128)),
                transforms.ToTensor(),
                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
            ])

    def __len__(self):
        return len(files)

    def __getitem__(self, idx):
        img_name = self.files[idx]

        # we may infer the label from the filename
        dash_idx = img_name.rfind('-')
        dot_idx = img_name.rfind('.')
        label = int(img_name[dash_idx + 1:dot_idx])

        # we need to download the file from S3 to a temporary file locally
        # we need to create the local file name
        obj = self.bucket.Object(img_name)
        tmp = tempfile.NamedTemporaryFile()
        tmp_name = '{}.jpg'.format(tmp.name)

        # now we can actually download from S3 to a local place
        with open(tmp_name, 'wb') as f:
            obj.download_fileobj(f)
            f.flush()
            f.close()
            image = Image.open(tmp_name)

        if self.transform:
            image = self.transform(image)

        return image, label
</code></pre>",2.0,2019-01-08 16:41:37.560000 UTC,,12.0,['amazon-sagemaker']
Update SageMaker Jupyterlab environment,<p>How can I update my SageMaker notebook's jupyter environment to the latest alpha release and then restart the process?</p>,1,0,2019-04-08 19:10:35.030000 UTC,,,4,amazon-sagemaker|jupyter-lab,1720,2013-02-20 05:47:52.693000 UTC,2022-09-23 20:45:28.400000 UTC,NYC,6281,430,17,958,"<p>Hi and thank you for using SageMaker!</p>

<p>To restart Jupyter from within a SageMaker Notebook Instance, you can issue the following command: <code>sudo initctl restart jupyter-server --no-wait</code>.</p>

<p>Best,
Kevin</p>",1.0,2019-04-11 18:28:36.017000 UTC,,8.0,['amazon-sagemaker']
How to pass dependency files to sagemaker SKLearnProcessor and use it in Pipeline?,"<p>I need to import function from different python scripts, which will used inside <code>preprocessing.py</code> file. I was not able to find a way to pass the dependent files to <code>SKLearnProcessor</code> Object, due to which I am getting <code>ModuleNotFoundError</code>.</p>
<p><strong>Code:</strong></p>
<pre><code>from sagemaker.sklearn.processing import SKLearnProcessor
from sagemaker.processing import ProcessingInput, ProcessingOutput

sklearn_processor = SKLearnProcessor(framework_version='0.20.0',
                                     role=role,
                                     instance_type='ml.m5.xlarge',
                                     instance_count=1)


sklearn_processor.run(code='preprocessing.py',
                      inputs=[ProcessingInput(
                        source=input_data,
                        destination='/opt/ml/processing/input')],
                      outputs=[ProcessingOutput(output_name='train_data',
                                                source='/opt/ml/processing/train'),
                               ProcessingOutput(output_name='test_data',
                                                source='/opt/ml/processing/test')],
                      arguments=['--train-test-split-ratio', '0.2']
                     )
</code></pre>
<p>I would like to pass,
<code>dependent_files = ['file1.py', 'file2.py', 'requirements.txt']</code>. So, that <code>preprocessing.py</code> have access to all the dependent modules.</p>
<p>And also need to install libraries from <code>requirements.txt</code> file.</p>
<p>Can you share any work around or a right way to do this?</p>
<p><strong>Update-25-11-2021:</strong></p>
<p><strong>Q1.</strong>(Answered but looking to solve using <code>FrameworkProcessor</code>)</p>
<p><a href=""https://github.com/aws/sagemaker-python-sdk/blob/99f023e76a5db060907a796d4d8fee550f005844/src/sagemaker/processing.py#L1426"" rel=""noreferrer"">Here</a>, the <code>get_run_args</code> function, is handling <code>dependencies</code>, <code>source_dir</code> and <code>code</code> parameters by using <a href=""https://github.com/aws/sagemaker-python-sdk/blob/99f023e76a5db060907a796d4d8fee550f005844/src/sagemaker/processing.py#L1265"" rel=""noreferrer"">FrameworkProcessor</a>. Is there any way that we can set this parameters from <code>ScriptProcessor</code> or <code>SKLearnProcessor</code> or any other <code>Processor</code> to set them?</p>
<p><strong>Q2.</strong></p>
<p>Can you also please show some reference to use our <code>Processor</code> as <code>sagemaker.workflow.steps.ProcessingStep</code> and then use in <code>sagemaker.workflow.pipeline.Pipeline</code>?</p>
<p>For having <code>Pipeline</code>, do we need <code>sagemaker-project</code> as mandatory or can we create <code>Pipeline</code> directly without any <code>Sagemaker-Project</code>?</p>",2,5,2021-09-03 14:59:45.260000 UTC,2.0,2021-11-26 14:18:30.430000 UTC,11,python|amazon-web-services|scikit-learn|amazon-sagemaker,2139,2017-07-23 15:35:48.410000 UTC,2022-09-24 13:11:02.210000 UTC,India,4419,434,324,962,"<p>There are a couple of options for you to accomplish that.</p>
<p>One that is really simple is adding all additional files to a folder, example:</p>
<pre><code>.
├── my_package
│   ├── file1.py
│   ├── file2.py
│   └── requirements.txt
└── preprocessing.py
</code></pre>
<p>Then send this entire folder as another input under the same <code>/opt/ml/processing/input/code/</code>, example:</p>
<pre class=""lang-py prettyprint-override""><code>from sagemaker.sklearn.processing import SKLearnProcessor
from sagemaker.processing import ProcessingInput, ProcessingOutput

sklearn_processor = SKLearnProcessor(
    framework_version=&quot;0.20.0&quot;,
    role=role,
    instance_type=&quot;ml.m5.xlarge&quot;,
    instance_count=1,
)

sklearn_processor.run(
    code=&quot;preprocessing.py&quot;,  # &lt;- this gets uploaded as /opt/ml/processing/input/code/preprocessing.py
    inputs=[
        ProcessingInput(source=input_data, destination='/opt/ml/processing/input'),
        # Send my_package as /opt/ml/processing/input/code/my_package/
        ProcessingInput(source='my_package/', destination=&quot;/opt/ml/processing/input/code/my_package/&quot;)
    ],
    outputs=[
        ProcessingOutput(output_name=&quot;train_data&quot;, source=&quot;/opt/ml/processing/train&quot;),
        ProcessingOutput(output_name=&quot;test_data&quot;, source=&quot;/opt/ml/processing/test&quot;),
    ],
    arguments=[&quot;--train-test-split-ratio&quot;, &quot;0.2&quot;],
)
</code></pre>
<p>What happens is that <code>sagemaker-python-sdk</code> is going to put your argument <code>code=&quot;preprocessing.py&quot;</code> under <code>/opt/ml/processing/input/code/</code> and you will have <code>my_package/</code> under the same directory.</p>
<p><strong>Edit:</strong></p>
<p>For the <code>requirements.txt</code>, you can add to your <code>preprocessing.py</code>:</p>
<pre class=""lang-py prettyprint-override""><code>import sys
import subprocess

subprocess.check_call([
    sys.executable, &quot;-m&quot;, &quot;pip&quot;, &quot;install&quot;, &quot;-r&quot;,
    &quot;/opt/ml/processing/input/code/my_package/requirements.txt&quot;,
])
</code></pre>",5.0,2021-11-24 19:39:22.627000 UTC,2021-11-24 19:47:08.437000 UTC,17.0,['amazon-sagemaker']
How to use different remotes for different folders?,"<p>I want my data and models stored in separate Google Cloud buckets. The idea is that I want to be able to share the data with others without sharing the models.</p>

<p>One idea I can think of is using separate git submodules for data and models. But that feels cumbersome and imposes some additional requirements from the end user (e.g. having to do <code>git submodule update</code>).</p>

<p>So can I do this without using git submodules?</p>",2,0,2019-11-20 11:10:29.420000 UTC,1.0,,12,dvc,1984,2011-07-22 10:25:49.880000 UTC,2022-09-21 15:11:42.327000 UTC,Tel Aviv,3784,472,0,342,"<p>You can first add the different <a href=""https://dvc.org/doc/command-reference/remote"" rel=""nofollow noreferrer"">DVC remotes</a> you want to establish (let's say you call them <code>data</code> and <code>models</code>, each one pointing to a different <a href=""https://cloud.google.com/storage/docs/json_api/v1/buckets"" rel=""nofollow noreferrer"">GC bucket</a>). <strong>But don't set any remote as the project's default</strong>; This way, <a href=""https://dvc.org/doc/command-reference/push"" rel=""nofollow noreferrer""><code>dvc push</code></a> won't work without the <code>-r</code> (or <code>--remote</code>) option.</p>
<p>You would then need to push each directory or file individually to the appropriate remote, like <code>dvc push data/ -r data</code> and <code>dvc push model.dat -r models</code>.</p>
<p>Note that a feature request to configure this exists on the DVC repo too. See <a href=""https://github.com/iterative/dvc/issues/2095"" rel=""nofollow noreferrer"">Specify file types that can be pushed to remote</a>.</p>",2.0,2019-11-20 16:31:15.363000 UTC,2022-01-18 17:46:31.693000 UTC,13.0,['dvc']
How to load trained model in amazon sagemaker?,"<p>I am following <a href=""https://github.com/mtm12/SageMakerDemo"" rel=""noreferrer"">this example</a> on how to train a machine learning model in Amazon-sagemaker.</p>
<pre><code>data_location = 's3://{}/kmeans_highlevel_example/data'.format(bucket)
output_location = 's3://{}/kmeans_highlevel_example/output'.format(bucket)

print('training data will be uploaded to: {}'.format(data_location))
print('training artifacts will be uploaded to: {}'.format(output_location))

kmeans = KMeans(role=role,
                train_instance_count=2,
                train_instance_type='ml.c4.8xlarge',
                output_path=output_location,
                k=10,
                epochs=100,
                data_location=data_location)
</code></pre>
<p>So after calling the fit function the model should be saved in the S3 bucket?? How can you load this model next time?</p>",1,0,2020-07-19 12:36:37.333000 UTC,2.0,,6,amazon-web-services|amazon-s3|amazon-sagemaker,4776,2011-07-16 13:02:36.880000 UTC,2022-09-24 20:19:39.590000 UTC,Slovenia,14913,307,1,1093,"<p>This can be done by using the sagemaker library combined with the <a href=""https://sagemaker.readthedocs.io/en/stable/api/inference/model.html"" rel=""noreferrer"">Inference Model</a>.</p>
<pre><code>model = sagemaker.model.Model(
    image=image
    model_data='s3://bucket/model.tar.gz',
    role=role_arn)
</code></pre>
<p>The options you're passing in are:</p>
<ul>
<li><code>image</code> - This is the ECR image you're using for inference (which should be for the algorithm you're trying to use). Paths are available <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html"" rel=""noreferrer"">here</a>.</li>
<li><code>model_data</code> - This is the path of where your model is stored (in a <code>tar.gz</code> compressed archive).</li>
<li><code>role</code> - This is the arn of a role that is capable of both pulling the image from ECR and getting the s3 archive.</li>
</ul>
<p>Once you've successfully done this you will need to setup an endpoint, this can be done by performing the following in your notebook through the <a href=""https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.model.Model.deploy"" rel=""noreferrer"">deploy function</a>.</p>
<pre><code>model.deploy(
   initial_instance_count=1,
   instance_type='ml.p2.xlarge'
)
</code></pre>",2.0,2020-07-19 12:49:45.810000 UTC,,8.0,['amazon-sagemaker']
Difference between git-lfs and dvc,<p>What is the difference between these two? We used git-lfs in my previous job and we are starting to use dvc alongside git in my current one. They both place some kind of index instead of file and can be downloaded on demand. Has dvc some improvements over the former one?</p>,2,0,2019-10-24 12:19:46.097000 UTC,5.0,,27,git|git-lfs|dvc,6255,2013-07-12 12:04:03.250000 UTC,2022-09-24 13:14:35.130000 UTC,,382,185,1,11,"<p>DVC is a better replacement for <code>git-lfs</code>. </p>

<p>Unlike git-lfs, DVC doesn't require installing a dedicated server; It can be used on-premises (NAS, SSH, for example) or with any major cloud provider (S3, Google Cloud, Azure).</p>

<p>For more information: <a href=""https://dvc.org/doc/use-cases/data-and-model-files-versioning"" rel=""noreferrer"">https://dvc.org/doc/use-cases/data-and-model-files-versioning</a></p>",5.0,2019-10-24 13:54:37.763000 UTC,,10.0,['dvc']
AWS SageMaker pd.read_pickle() doesn't work but read_csv() does?,"<p>I've recently been trying to train some models on an AWS SageMaker jupyter notebook instance.</p>

<p>Everything is worked very well until I tried to load in some custom dataset (REDD) through files.</p>

<p>I have the dataframes stored in Pickle (.pkl) files on an S3 bucket. I couldn't manage to read them into sagemaker so I decided to convert them to csv's as this seemed to work but I ran into a problem. This data has an index of type datetime64 and when using <code>.to_csv()</code> this index gets converted to pure text and it loses it's data structure (and I need to keep this specific index for correct plotting.)</p>

<p>So I decided to try the Pickle files again but I can't get it to work and have no idea why.</p>

<p>The following code for csv's works but I can't use it due to the index problem:</p>

<pre><code>bucket = 'sagemaker-peno'
houses_dfs = {}
data_key = 'compressed_data/'
data_location = 's3://{}/{}'.format(bucket, data_key)
for file in range(6):
    houses_dfs[file+1] = pd.read_csv(data_location+'house_'+str(file+1)+'.csv', index_col='Unnamed: 0')
</code></pre>

<p>But this code does NOT work even though it uses almost the exact same syntax:</p>

<pre><code>bucket = 'sagemaker-peno'
houses_dfs = {}
data_key = 'compressed_data/'
data_location = 's3://{}/{}'.format(bucket, data_key)
for file in range(6):
    houses_dfs[file+1] =  pd.read_pickle(data_location+'house_'+str(file+1)+'.pkl')
</code></pre>

<p>Yes it's 100% the correct path, because the csv and pkl files are stored in the same directory (compressed_data).</p>

<p>It throws me this error while using the Pickle method:</p>

<pre><code>FileNotFoundError: [Errno 2] No such file or directory: 's3://sagemaker-peno/compressed_data/house_1.pkl'
</code></pre>

<p>I hope to find someone who has dealt with this before and can solve the <code>read_pickle()</code> issue or as an alternative fix my datetime64 type issue with csv's.</p>

<p>Thanks in advance!</p>",1,0,2018-11-13 12:20:48.037000 UTC,,,1,pandas|csv|amazon-s3|pickle|amazon-sagemaker,788,2015-08-09 12:51:41.797000 UTC,2022-07-07 20:29:24.310000 UTC,"Leuven, België",87,8,0,31,<p>read_pickle() likes the full path more than a relative path from where it was run. This fixed my issue.</p>,0.0,2018-11-26 20:32:53.167000 UTC,,-1.0,['amazon-sagemaker']
"How Do You ""Permanently"" Delete An Experiment In Mlflow?","<p>Permanent deletion of an experiment isn't documented anywhere. I'm using Mlflow w/ backend postgres db</p>

<p>Here's what I've run: </p>

<pre><code>client = MlflowClient(tracking_uri=server)
client.delete_experiment(1)
</code></pre>

<p>This deletes the the experiment, but when I run a new experiment with the same name as the one I just deleted, it will return this error:</p>

<pre><code>mlflow.exceptions.MlflowException: Cannot set a deleted experiment 'cross-sell' as the active experiment. You can restore the experiment, or permanently delete the  experiment to create a new one.
</code></pre>

<p>I cannot find anywhere in the documentation that shows how to permanently delete everything.</p>",5,0,2020-02-06 06:26:41.043000 UTC,3.0,,20,python|mlflow,13984,2015-09-26 00:03:29.767000 UTC,2022-09-23 01:58:54.270000 UTC,"Vancouver, BC, Canada",2332,133,3,560,"<p>Unfortunately it seems there is no way to do this via the UI or CLI at the moment :-/</p>

<p>The way to do it depends on the type of backend file store that you are using.</p>

<p><strong>Filestore</strong>:</p>

<p>If you are using the filesystem as a storage mechanism (the default) then it is easy. The 'deleted' experiments are moved to a <code>.trash</code> folder. You just need to clear that out:</p>

<pre class=""lang-sh prettyprint-override""><code>rm -rf mlruns/.trash/*
</code></pre>

<p>As of the current version of the <a href=""https://www.mlflow.org/docs/latest/cli.html#mlflow-experiments-delete"" rel=""noreferrer"">documentation</a> (1.7.2), they remark:</p>

<blockquote>
  <p>It is recommended to use a cron job or an alternate workflow mechanism to clear <code>.trash</code> folder.</p>
</blockquote>

<p><strong>SQL Database:</strong></p>

<p>This is more tricky, as there are dependencies that need to be deleted. I am using MySQL, and these commands work for me:</p>

<pre class=""lang-sql prettyprint-override""><code>USE mlflow_db;  # the name of your database
DELETE FROM experiment_tags WHERE experiment_id=ANY(
    SELECT experiment_id FROM experiments where lifecycle_stage=""deleted""
);
DELETE FROM latest_metrics WHERE run_uuid=ANY(
    SELECT run_uuid FROM runs WHERE experiment_id=ANY(
        SELECT experiment_id FROM experiments where lifecycle_stage=""deleted""
    )
);
DELETE FROM metrics WHERE run_uuid=ANY(
    SELECT run_uuid FROM runs WHERE experiment_id=ANY(
        SELECT experiment_id FROM experiments where lifecycle_stage=""deleted""
    )
);
DELETE FROM tags WHERE run_uuid=ANY(
    SELECT run_uuid FROM runs WHERE experiment_id=ANY(
        SELECT experiment_id FROM experiments where lifecycle_stage=""deleted""
    )
);
DELETE FROM runs WHERE experiment_id=ANY(
    SELECT experiment_id FROM experiments where lifecycle_stage=""deleted""
);
DELETE FROM experiments where lifecycle_stage=""deleted"";
</code></pre>",1.0,2020-03-26 14:05:13.453000 UTC,,22.0,['mlflow']
How to use Serializer and Deserializer in Sagemaker 2,"<p>I spin up a Sagemaker notebook using the <code>conda_python3</code> kernel, and follow the <a href=""https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/random_cut_forest/random_cut_forest.ipynb"" rel=""noreferrer"">example</a> Notebook for <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/randomcutforest.html"" rel=""noreferrer"">Random Cut Forest</a>.</p>
<p>As of this writing, the <a href=""https://sagemaker.readthedocs.io/en/stable/index.html"" rel=""noreferrer"">Sagemaker SDK</a> that comes with <code>conda_python3</code> is version 1.72.0, but I want to use new features, so I update my notebook to use the latest</p>
<pre class=""lang-py prettyprint-override""><code>%%bash
pip install -U sagemaker
</code></pre>
<p>And I see it updates.</p>
<pre class=""lang-py prettyprint-override""><code>print(sagemaker.__version__)

# 2.4.1
</code></pre>
<p>A change from version 1.x to 2.x was the <a href=""https://sagemaker.readthedocs.io/en/stable/v2.html#serializer-and-deserializer-classes"" rel=""noreferrer"">serializer/deserializer classes</a></p>
<p>Previously (in version 1.72.0) I'd update my predictor to use the proper serializer/deserializer, and could run inference on my model</p>
<pre class=""lang-py prettyprint-override""><code>from sagemaker.predictor import csv_serializer, json_deserializer


rcf_inference = rcf.deploy(
    initial_instance_count=1,
    instance_type='ml.m4.xlarge',
)

rcf_inference.content_type = 'text/csv'
rcf_inference.serializer = csv_serializer
rcf_inference.accept = 'application/json'
rcf_inference.deserializer = json_deserializer

results = rcf_inference.predict(some_numpy_array)
</code></pre>
<p>(Note this all comes from the <a href=""https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/random_cut_forest/random_cut_forest.ipynb"" rel=""noreferrer"">example</a></p>
<p>I try and replicate this using sagemaker 2.4.1 like so</p>
<pre class=""lang-py prettyprint-override""><code>from sagemaker.deserializers import JSONDeserializer
from sagemaker.serializers import CSVSerializer

rcf_inference = rcf.deploy(
    initial_instance_count=1,
    instance_type='ml.m5.xlarge',
    serializer=CSVSerializer,
    deserializer=JSONDeserializer
)

results = rcf_inference.predict(some_numpy_array)
</code></pre>
<p>And I receive an error of</p>
<pre><code>TypeError: serialize() missing 1 required positional argument: 'data'
</code></pre>
<p>I know I'm using the serliaizer/deserializer incorrectly, but can't find good documentation on how this should be used</p>",2,0,2020-08-24 20:26:02.780000 UTC,4.0,,8,python-3.x|amazon-web-services|amazon-sagemaker,5910,2012-09-28 14:12:28.500000 UTC,2022-09-23 20:25:27.910000 UTC,,5182,1902,7,315,"<p>in order to use the new serializers/deserializers, you will need to init them, for example:</p>
<pre><code>from sagemaker.deserializers import JSONDeserializer
from sagemaker.serializers import CSVSerializer

rcf_inference = rcf.deploy(
    initial_instance_count=1,
    instance_type='ml.m5.xlarge',
    serializer=CSVSerializer(),
    deserializer=JSONDeserializer()
)
</code></pre>",2.0,2020-08-27 04:48:39.773000 UTC,2020-08-28 11:59:53.680000 UTC,16.0,['amazon-sagemaker']
Use mlflow to serve a custom python model for scoring,"<p>I am using Python code generated from an ml software with mlflow to read a dataframe, perform some table operations and output a dataframe. I am able to run the code successfully and save the new dataframe as an artifact. However I am unable to log the model using log_model because it is not a lr or classifier model where we train and fit. I want to log a model for this so that it can be served with new data and deployed with a rest API</p>
<pre><code>df = pd.read_csv(r&quot;/home/xxxx.csv&quot;)


with mlflow.start_run():
    def getPrediction(row):
        
        perform_some_python_operaions 

        return [Status_prediction, Status_0_probability, Status_1_probability]
    columnValues = []
    for column in columns:
        columnValues.append([])

    for index, row in df.iterrows():
        results = getPrediction(row)
        for n in range(len(results)):
            columnValues[n].append(results[n])

    for n in range(len(columns)):
        df[columns[n]] = columnValues[n]

    df.to_csv('dataset_statistics.csv')
    mlflow.log_artifact('dataset_statistics.csv')
   
</code></pre>",1,0,2021-01-25 15:00:24.463000 UTC,3.0,,4,python|deployment|mlflow|mlops,3026,2019-11-14 13:58:10.560000 UTC,2022-09-23 08:37:32.563000 UTC,,115,16,0,25,"<p>MLflow supports <a href=""https://mlflow.org/docs/latest/models.html#custom-python-models"" rel=""nofollow noreferrer"">custom models</a> of mlflow.pyfunc flavor.  You can create a custom  class  inherited from the <code>mlflow.pyfunc.PythonModel</code>, that needs to provide function <code>predict</code> for performing predictions, and optional <code>load_context</code> to load the necessary artifacts, like this (adopted from the docs):</p>
<pre class=""lang-py prettyprint-override""><code>class MyModel(mlflow.pyfunc.PythonModel):

    def load_context(self, context):
        # load your artifacts

    def predict(self, context, model_input):
        return my_predict(model_input.values)
</code></pre>
<p>You can log to MLflow whatever artifacts you need for your models, define Conda environment if necessary, etc.<br />
Then you can use <code>save_model</code> with your class to save your implementation, that could be loaded with <code>load_model</code> and do the <code>predict</code> using your model:</p>
<pre class=""lang-py prettyprint-override""><code>mlflow.pyfunc.save_model(
        path=mlflow_pyfunc_model_path, 
        python_model=MyModel(), 
        artifacts=artifacts)

# Load the model in `python_function` format
loaded_model = mlflow.pyfunc.load_model(mlflow_pyfunc_model_path)
</code></pre>",3.0,2021-01-25 16:41:54.947000 UTC,2021-10-14 05:05:40.523000 UTC,9.0,['mlflow']
AWS SageMaker on GPU,"<p>I am trying to train a neural network (Tensorflow) on AWS. I have some AWS credits. From my understanding AWS SageMaker is the one best for the job. I managed to load the Jupyter Lab console on SageMaker and tried to find a GPU kernel since, I know it is the best for training neural networks. However, I could not find such kernel. </p>

<p>Would anyone be able to help in this regard.</p>

<p>Thanks &amp; Best Regards</p>

<p>Michael</p>",2,1,2020-03-26 13:22:02.137000 UTC,2.0,,11,amazon-web-services|tensorflow|amazon-sagemaker,13664,2017-01-15 13:07:28.903000 UTC,2022-09-23 03:24:49.200000 UTC,Melbourne Australia,605,54,0,133,"<p>You train models on GPU in the SageMaker ecosystem via 2 different components:</p>

<ol>
<li><p>You can instantiate a GPU-powered <strong><a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/nbi.html"" rel=""noreferrer"">SageMaker Notebook Instance</a></strong>, for example <code>p2.xlarge</code> (NVIDIA K80) or <code>p3.2xlarge</code> (NVIDIA V100). This is convenient for interactive development - you have the GPU right under your notebook and can run code on the GPU interactively and monitor the GPU via <code>nvidia-smi</code> in a terminal tab - a great development experience. However when you develop directly from a GPU-powered machine, there are times when you may not use the GPU. For example when you write code or browse some documentation. All that time you pay for a GPU that sits idle. In that regard, it may not be the most cost-effective option for your use-case. </p></li>
<li><p>Another option is to use a <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-training.html"" rel=""noreferrer""><strong>SageMaker Training Job</strong></a> running on a GPU instance. This is a preferred option for training, because training metadata (data and model path, hyperparameters, cluster specification, etc) is persisted in the SageMaker metadata store, logs and metrics stored in Cloudwatch and the instance automatically shuts down itself at the end of training. Developing on a small CPU instance and launching training tasks using SageMaker Training API will help you make the most of your budget, while helping you retain metadata and artifacts of all your experiments. You can see <a href=""https://aws.amazon.com/fr/blogs/machine-learning/using-tensorflow-eager-execution-with-amazon-sagemaker-script-mode/"" rel=""noreferrer"">here a well documented TensorFlow example</a></p></li>
</ol>",0.0,2020-03-26 22:32:27.503000 UTC,,20.0,['amazon-sagemaker']
MLflow: INVALID_PARAMETER_VALUE: Unsupported URI './mlruns' for model registry store,"<p>I got this error when I was trying to have a model registered in the model registry. Could someone help me?</p>
<pre><code>RestException: INVALID_PARAMETER_VALUE: Unsupported URI './mlruns' for model registry store. 
Supported schemes are: ['postgresql', 'mysql', 'sqlite', 'mssql']. 
See https://www.mlflow.org/docs/latest/tracking.html#storage for how to setup a compatible server.
</code></pre>",1,0,2020-08-04 21:54:55.187000 UTC,2.0,2020-08-30 19:18:05.667000 UTC,10,python|mlflow,12594,2014-12-26 18:42:31.727000 UTC,2022-02-07 21:07:16.150000 UTC,,125,8,0,16,"<p>Mlflow required DB as datastore for Model Registry
So you have to run tracking server with DB as backend-store and log model to this tracking server.
The easiest way to use DB is to use SQLite.</p>
<pre><code>mlflow server \
    --backend-store-uri sqlite:///mlflow.db \
    --default-artifact-root ./artifacts \
    --host 0.0.0.0
</code></pre>
<p>And set MLFLOW_TRACKING_URI environment variable to <em>http://localhost:5000</em> or</p>
<pre><code>mlflow.set_tracking_uri(&quot;http://localhost:5000&quot;)
</code></pre>
<p>After got to http://localhost:5000 and you can register a logged model from UI or from the code.</p>",5.0,2020-08-05 11:19:29.480000 UTC,,27.0,['mlflow']
AWS Sagemaker - Install External Library and Make it Persist,<p>I have a sagemaker instance up and running and I have a few libraries that I frequently use with it but each time I restart the instance they get wiped and I have to reinstall them. Is it possible to install my libraries to one of the anaconda environments and have the change remain?</p>,2,0,2018-06-30 17:34:14.783000 UTC,1.0,,13,amazon-web-services|amazon-sagemaker,10578,2017-03-23 00:32:36.263000 UTC,2021-03-22 21:38:40.833000 UTC,,401,17,0,13,"<p>The supported way to do this for Sagemaker notebook instances is with <strong>Lifecycle Configurations</strong>.</p>

<p>You can create an <strong>onStart</strong> lifecycle hook that can install the required packages into the respective Conda environments each time your notebook instance starts.</p>

<p>Please see the following blog post for more details</p>

<p><a href=""https://aws.amazon.com/blogs/machine-learning/customize-your-amazon-sagemaker-notebook-instances-with-lifecycle-configurations-and-the-option-to-disable-internet-access/"" rel=""noreferrer"">https://aws.amazon.com/blogs/machine-learning/customize-your-amazon-sagemaker-notebook-instances-with-lifecycle-configurations-and-the-option-to-disable-internet-access/</a></p>",1.0,2018-07-02 18:19:26.567000 UTC,,12.0,['amazon-sagemaker']
Uploading a Dataframe to AWS S3 Bucket from SageMaker,"<p>I am new to AWS environment and trying to solve how the data flow works. After successfully uploading CSV files from S3 to SageMaker notebook instance, I am stuck on doing the reverse. </p>

<p>I have a dataframe and want to upload that to S3 Bucket as CSV or JSON. The code that I have is below:</p>

<pre><code>bucket='bucketname'
data_key = 'test.csv'
data_location = 's3://{}/{}'.format(bucket, data_key)
df.to_csv(data_location)
</code></pre>

<p>I assumed since I successfully used <code>pd.read_csv()</code> while loading, using <code>df.to_csv()</code> would also work but it didn't. Probably it is generating error because this way I cannot pick the privacy options while uploading a file manually to S3. Is there a way to upload the data to S3 from SageMaker?</p>",2,2,2019-06-28 00:36:23.347000 UTC,2.0,2019-06-28 02:22:19.710000 UTC,9,python|pandas|amazon-web-services|amazon-s3|amazon-sagemaker,16471,2018-11-11 21:34:52.477000 UTC,2022-09-24 16:12:16.107000 UTC,"Santa Clara, CA, USA",731,65,0,51,"<p>One way to solve this would be to save the CSV to the local storage on the SageMaker notebook instance, and then use the S3 API's via <code>boto3</code> to upload the file as an s3 object. 
<a href=""https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Client.upload_file"" rel=""noreferrer"">S3 docs for <code>upload_file()</code> available here.</a></p>

<p>Note, you'll need to ensure that your SageMaker hosted notebook instance has proper <code>ReadWrite</code> permissions in its IAM role, otherwise you'll receive a permissions error.</p>

<pre><code># code you already have, saving the file locally to whatever directory you wish
file_name = ""mydata.csv"" 
df.to_csv(file_name)
</code></pre>

<pre><code># instantiate S3 client and upload to s3
import boto3

s3 = boto3.resource('s3')
s3.meta.client.upload_file(file_name, 'YOUR_S3_BUCKET_NAME', 'DESIRED_S3_OBJECT_NAME')
</code></pre>

<p>Alternatively, <code>upload_fileobj()</code> may help for parallelizing as a multi-part upload. </p>",1.0,2019-06-28 01:20:44.023000 UTC,2019-06-28 01:45:21.643000 UTC,10.0,['amazon-sagemaker']
get the run id for an mlflow experiment with the name?,"<p>I currently created an experiment in mlflow and created multiple runs in the experiment.</p>
<pre><code>from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
import mlflow

experiment_name=&quot;experiment-1&quot;
mlflow.set_experiment(experiment_name)

no_of_trees=[100,200,300]
depths=[2,3,4]
for trees in no_of_trees:
    for depth in depths:
        with mlflow.start_run() as run:
            model=RandomForestRegressor(n_estimators=trees, criterion='mse',max_depth=depth)
            model.fit(x_train, y_train)
            predictions=model.predict(x_cv)
            mlflow.log_metric('rmse',mean_squared_error(y_cv, predictions))
</code></pre>
<p>after creating the runs, I wanted to get the best run_id for this experiment. for now, I can get the best run by looking at the UI of mlflow but how can we do right the program?</p>",1,0,2020-12-16 02:45:27.487000 UTC,,,6,python|mlflow,6374,2016-11-15 06:12:07.737000 UTC,2022-08-15 17:25:10.400000 UTC,"R G U K T , basar, Andhra Pradesh, India",2470,265,22,251,"<p>we can get the experiment id from the experiment name and we can use python API to get the best runs.</p>
<pre><code>experiment_name = &quot;experiment-1&quot;
current_experiment=dict(mlflow.get_experiment_by_name(experiment_name))
experiment_id=current_experiment['experiment_id']
</code></pre>
<p>By using the experiment id, we can get all the runs and we can sort them based on metrics like below. In the below code, rmse is my metric name (so it may be different for you based on metric name)</p>
<pre><code>df = mlflow.search_runs([experiment_id], order_by=[&quot;metrics.rmse DESC&quot;])
best_run_id = df.loc[0,'run_id']
</code></pre>",0.0,2020-12-16 02:45:27.487000 UTC,,15.0,['mlflow']
Isolation Forest vs Robust Random Cut Forest in outlier detection,"<p>I am examining different methods in outlier detection. I came across sklearn's implementation of Isolation Forest and Amazon sagemaker's implementation of RRCF (Robust Random Cut Forest). Both are ensemble methods based on decision trees, aiming to isolate every single point. The more isolation steps there are, the more likely the point is to be an inlier, and the opposite is true.</p>
<p>However, even after looking at the original papers of the algorithms, I am failing to understand exactly the difference between both algorithms. In what way do they work differently? Is one of them more efficient than the other?</p>
<p>EDIT: I am adding the links to the research papers for more information, as well as some tutorials discussing the topics.</p>
<p>Isolation Forest:</p>
<p><a href=""https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf?q=isolation-forest"" rel=""noreferrer"">Paper</a> <a href=""https://towardsdatascience.com/outlier-detection-with-isolation-forest-3d190448d45e"" rel=""noreferrer"">Tutorial</a></p>
<p>Robust Random Cut Forest:</p>
<p><a href=""http://proceedings.mlr.press/v48/guha16.pdf"" rel=""noreferrer"">Paper</a> <a href=""https://freecontent.manning.com/the-randomcutforest-algorithm/"" rel=""noreferrer"">Tutorial</a></p>",2,0,2020-07-27 12:59:41.210000 UTC,3.0,2020-07-28 11:54:56.953000 UTC,5,python|scikit-learn|amazon-sagemaker|outliers|anomaly-detection,6062,2020-03-30 17:07:50.093000 UTC,2022-08-10 09:27:57.740000 UTC,"Strasbourg, France",123,21,0,19,"<p>In part of my answers I'll assume you refer to Sklearn's Isolation Forest. I believe those are the 4 main differences:</p>
<ol>
<li><p><strong>Code availability:</strong> Isolation Forest has a popular open-source implementation in Scikit-Learn (<a href=""https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html"" rel=""noreferrer""><code>sklearn.ensemble.IsolationForest</code></a>), while both AWS implementation of Robust Random Cut Forest (RRCF) are closed-source, in <a href=""https://docs.aws.amazon.com/kinesisanalytics/latest/sqlref/sqlrf-random-cut-forest.html"" rel=""noreferrer"">Amazon Kinesis</a> and <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/randomcutforest.html"" rel=""noreferrer"">Amazon SageMaker</a>. There is an interesting third party RRCF open-source implementation on GitHub though: <a href=""https://github.com/kLabUM/rrcf"" rel=""noreferrer"">https://github.com/kLabUM/rrcf</a> ; but unsure how popular it is yet</p>
</li>
<li><p><strong>Training design:</strong> RRCF can work on streams, as highlighted in the paper and as exposed in the streaming analytics service Kinesis Data Analytics. On the other hand, the absence of <code>partial_fit</code> method hints me that Sklearn's Isolation Forest is a batch-only algorithm that cannot readily work on data streams</p>
</li>
<li><p><strong>Scalability:</strong> SageMaker RRCF is more scalable. Sklearn's Isolation Forest is single-machine code, which can nonetheless be parallelized over CPUs with the <code>n_jobs</code> parameter. On the other hand, SageMaker RRCF can be used over <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html"" rel=""noreferrer"">one machine or multiple machines</a>. Also, it supports SageMaker Pipe mode (streaming data via unix pipes) which makes it able to learn on much bigger data than what fits on disk</p>
</li>
<li><p><strong>the way features are sampled</strong> at each recursive isolation: RRCF gives more weight to dimension with higher variance (according to <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/rcf_how-it-works.html"" rel=""noreferrer"">SageMaker doc</a>), while I think isolation forest samples at random, which is one reason why RRCF is expected to perform better in high-dimensional space (picture from the RRCF paper)
<a href=""https://i.stack.imgur.com/3FXmE.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/3FXmE.png"" alt=""enter image description here"" /></a></p>
</li>
</ol>",4.0,2020-07-28 12:23:30.300000 UTC,,11.0,['amazon-sagemaker']
AWS Sagemaker AttributeError: can't set attribute error,"<p>I am new to python programming. Following the AWS learning path:</p>
<p><a href=""https://aws.amazon.com/getting-started/hands-on/build-train-deploy-machine-learning-model-sagemaker/?trk=el_a134p000003yWILAA2&amp;trkCampaign=DS_SageMaker_Tutorial&amp;sc_channel=el&amp;sc_campaign=Data_Scientist_Hands-on_Tutorial&amp;sc_outcome=Product_Marketing&amp;sc_geo=mult"" rel=""nofollow noreferrer"">https://aws.amazon.com/getting-started/hands-on/build-train-deploy-machine-learning-model-sagemaker/?trk=el_a134p000003yWILAA2&amp;trkCampaign=DS_SageMaker_Tutorial&amp;sc_channel=el&amp;sc_campaign=Data_Scientist_Hands-on_Tutorial&amp;sc_outcome=Product_Marketing&amp;sc_geo=mult</a></p>
<p>I am getting an error when excuting the following block (in conda_python3):</p>
<pre><code>test_data_array = test_data.drop(['y_no', 'y_yes'], axis=1).values #load the data into an array
xgb_predictor.content_type = 'text/csv' # set the data type for an inference
xgb_predictor.serializer = csv_serializer # set the serializer type
predictions = xgb_predictor.predict(test_data_array).decode('utf-8') # predict!
predictions_array = np.fromstring(predictions[1:], sep=',') # and turn the prediction into an 
array
print(predictions_array.shape)
</code></pre>
<blockquote>
<p>AttributeError                            Traceback (most recent call last)
 in 
1 test_data_array = test_data.drop(['y_no', 'y_yes'], axis=1).values #load the data into an array
----&gt; 2 xgb_predictor.content_type = 'text/csv' # set the data type for an inference
3 xgb_predictor.serializer = csv_serializer # set the serializer type
4 predictions = xgb_predictor.predict(test_data_array).decode('utf-8') # predict!
5 predictions_array = np.fromstring(predictions[1:], sep=',') # and turn the prediction into an array</p>
</blockquote>
<blockquote>
<p>AttributeError: can't set attribute</p>
</blockquote>
<p>I have looked at several prior questions but couldn't find much information related to this error when it comes to creating data types.</p>
<p>Thanks in advance for any help.</p>",2,0,2020-12-27 11:11:37.070000 UTC,,,3,python|amazon-web-services|amazon-s3|amazon-sagemaker,2479,2013-07-20 01:45:54.810000 UTC,2022-09-25 05:27:32.800000 UTC,,279,49,0,93,"<p>If you just remove it then the prediction will work. Therefore, recommend removing this code line.
xgb_predictor.content_type = 'text/csv'</p>",1.0,2020-12-31 07:17:48.573000 UTC,,8.0,['amazon-sagemaker']
Pyathena is super slow compared to querying from Athena,"<p>I run a query from AWS <strong>Athena console</strong> and takes 10s.
The same query run from <strong>Sagemaker</strong> using <strong>PyAthena</strong> takes 155s.
Is PyAthena slowing it down or is the data transfer from Athena to sagemaker so time consuming?</p>
<p>What could I do to speed this up?</p>",1,0,2020-10-02 11:28:26.830000 UTC,,2020-10-02 12:20:59.400000 UTC,3,python|amazon-web-services|amazon-athena|amazon-sagemaker|pyathena,3188,2017-02-05 16:38:14.643000 UTC,2022-09-21 10:19:38.630000 UTC,"München, Germany",668,38,3,85,"<p>Just figure out a way of boosting the queries:</p>
<p>Before I was trying:</p>
<pre><code>import pandas as pd
from pyathena import connect

conn = connect(s3_staging_dir=STAGIN_DIR,
             region_name=REGION)
pd.read_sql(QUERY, conn)
# takes 160s
</code></pre>
<p>Figured out that using a <em>PandasCursor</em> instead of a <em>connection</em> is way faster</p>
<pre><code>import pandas as pd
pyathena import connect
from pyathena.pandas.cursor import PandasCursor

cursor = connect(s3_staging_dir=STAGIN_DIR,
                 region_name=REGION,
                 cursor_class=PandasCursor).cursor()
df = cursor.execute(QUERY).as_pandas()
# takes 12s
</code></pre>
<p>Ref: <a href=""https://github.com/laughingman7743/PyAthena/issues/46"" rel=""noreferrer"">https://github.com/laughingman7743/PyAthena/issues/46</a></p>",3.0,2020-10-02 11:48:43.903000 UTC,2021-08-19 13:38:44.770000 UTC,13.0,['amazon-sagemaker']
Is it possible to set/change mlflow run name after run initial creation?,"<p>I could not find a way yet of setting the runs name after the first start_run for that run (we can pass a name there). </p>

<p>I Know we can use tags but that is not the same thing. I would like to add a run relevant name, but very often we know the name only after run evaluation or while we're running the run interactively in notebook for example.</p>",3,0,2019-07-25 10:09:51.490000 UTC,1.0,2019-07-31 00:38:47.097000 UTC,11,mlflow,7689,2014-02-22 09:53:28.773000 UTC,2021-04-14 22:23:03.790000 UTC,Portugal,133,0,0,6,"<p>It is possible to edit run names from the MLflow UI. First, click into the run whose name you'd like to edit.</p>

<p>Then, edit the run name by clicking the dropdown next the run name (i.e. the downward-pointing caret in this image):</p>

<p><a href=""https://i.stack.imgur.com/sl6Qs.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/sl6Qs.png"" alt=""Rename run dropdown""></a></p>

<p>There's currently no stable public API for setting run names - however, you can programmatically set/edit run names by setting the tag with key <code>mlflow.runName</code>, which is what the UI (currently) does under the hood.</p>",1.0,2019-07-29 06:02:35.503000 UTC,,9.0,['mlflow']
How to pass arguments to scoring file when deploying a Model in AzureML,"<p>I am deploying a trained model to an ACI endpoint on Azure Machine Learning, using the Python SDK.
I have created my score.py file, but I would like that file to be called with an argument being passed (just like with a training file) that I can interpret using <code>argparse</code>.
However, I don't seem to find how I can pass arguments
This is the code I have to create the InferenceConfig environment and which obviously does not work.  Should I fall back on using the extra Docker file steps or so?</p>

<pre class=""lang-py prettyprint-override""><code>from azureml.core.conda_dependencies import CondaDependencies
from azureml.core.environment import Environment
from azureml.core.model import InferenceConfig

env = Environment('my_hosted_environment')
env.python.conda_dependencies = CondaDependencies.create(
    conda_packages=['scikit-learn'],
    pip_packages=['azureml-defaults'])
scoring_script = 'score.py --model_name ' + model_name
inference_config = InferenceConfig(entry_script=scoring_script, environment=env)
</code></pre>

<p>Adding the score.py for reference on how I'd love to use the arguments in that script:</p>

<pre class=""lang-py prettyprint-override""><code>#removed imports
import argparse

def init():
    global model

    parser = argparse.ArgumentParser(description=""Load sklearn model"")
    parser.add_argument('--model_name', dest=""model_name"", required=True)
    args, _ = parser.parse_known_args()

    model_path = Model.get_model_path(model_name=args.model_name)
    model = joblib.load(model_path)

def run(raw_data):
    try:
        data = json.loads(raw_data)['data']
        data = np.array(data)
        result = model.predict(data)
        return result.tolist()

    except Exception as e:
        result = str(e)
        return result
</code></pre>

<p>Interested to hear your thoughts</p>",3,2,2020-03-11 13:27:40.433000 UTC,,2020-03-12 09:38:40.357000 UTC,4,python|azure-machine-learning-service,1681,2013-02-12 07:50:30.743000 UTC,2022-09-21 18:28:12.907000 UTC,Belgium,2947,297,16,355,"<p>How to deploy using environments can be found here <a href=""https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2FAzure%2FMachineLearningNotebooks%2Fblob%2Fmaster%2Fhow-to-use-azureml%2Fdeployment%2Fdeploy-to-cloud%2Fmodel-register-and-deploy.ipynb&amp;data=02%7C01%7CRamprasad.Mula%40microsoft.com%7Ce06d310b0447416ab46b08d7bc836a81%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637185146436156499&amp;sdata=uQo332dpjuiNqWFCguvs3Kgg7UUMN8MBEzLxTPyH4MM%3D&amp;reserved=0"" rel=""nofollow noreferrer"">model-register-and-deploy.ipynb</a> .  InferenceConfig class accepts  source_directory and entry_script <a href=""https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.model.inferenceconfig?view=azure-ml-py#parameters"" rel=""nofollow noreferrer"">parameters</a>, where source_directory  is a path to the folder that contains all files(score.py and any other additional files) to create the image. </p>

<p>This <a href=""https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/deployment/deploy-multi-model/multi-model-register-and-deploy.ipynb"" rel=""nofollow noreferrer"">multi-model-register-and-deploy.ipynb</a> has code snippets on how to create InferenceConfig with source_directory and entry_script.</p>

<pre><code>from azureml.core.webservice import Webservice
from azureml.core.model import InferenceConfig
from azureml.core.environment import Environment

myenv = Environment.from_conda_specification(name=""myenv"", file_path=""myenv.yml"")
inference_config = InferenceConfig(entry_script=""score.py"", environment=myenv)

service = Model.deploy(workspace=ws,
                       name='sklearn-mnist-svc',
                       models=[model], 
                       inference_config=inference_config,
                       deployment_config=aciconfig)

service.wait_for_deployment(show_output=True)

print(service.scoring_uri)
</code></pre>",4.0,2020-03-12 09:36:25.480000 UTC,2020-03-12 11:19:48.083000 UTC,-2.0,['azure-machine-learning-service']
"Automatically ""stop"" Sagemaker notebook instance after inactivity?","<p>I have a Sagemaker Jupyter notebook instance that I keep leaving online overnight by mistake, unnecessarily costing money... </p>

<p>Is there any way to automatically stop the Sagemaker notebook instance when there is no activity for say, 1 hour? Or would I have to make a custom script?</p>",4,0,2018-12-04 09:18:11.383000 UTC,2.0,,19,amazon-web-services|aws-lambda|amazon-cloudwatch|amazon-sagemaker,12683,2011-03-10 10:26:00.990000 UTC,2022-09-24 23:25:14.187000 UTC,,2563,121,0,167,"<p>You can use <a href=""https://aws.amazon.com/blogs/machine-learning/customize-your-amazon-sagemaker-notebook-instances-with-lifecycle-configurations-and-the-option-to-disable-internet-access/"" rel=""noreferrer"">Lifecycle configurations</a> to set up an automatic job that will stop your instance after inactivity.</p>

<p>There's <a href=""https://github.com/aws-samples/amazon-sagemaker-notebook-instance-lifecycle-config-samples"" rel=""noreferrer"">a GitHub repository</a> which has samples that you can use. In the repository, there's a <a href=""https://github.com/aws-samples/amazon-sagemaker-notebook-instance-lifecycle-config-samples/blob/master/scripts/auto-stop-idle/on-start.sh"" rel=""noreferrer"">auto-stop-idle</a> script which will shutdown your instance once it's idle for more than 1 hour.</p>

<p>What you need to do is</p>

<ol>
<li>to create a Lifecycle configuration using the script and</li>
<li>associate the configuration with the instance. You can do this when you edit or create a Notebook instance.</li>
</ol>

<p>If you think 1 hour is too long you can tweak the script. <a href=""https://github.com/aws-samples/amazon-sagemaker-notebook-instance-lifecycle-config-samples/blob/master/scripts/auto-stop-idle/on-start.sh#L17"" rel=""noreferrer"">This line</a> has the value.</p>",1.0,2019-10-15 09:46:13.200000 UTC,,26.0,['amazon-sagemaker']
Difference in usecases for AWS Sagemaker vs Databricks?,"<p>I was looking at Databricks because it integrates with AWS services like Kinesis, but it looks to me like SageMaker is a direct competitor to Databricks? We are heavily using AWS, is there any reason to add DataBricks into the stack or odes SageMaker fill the same role?</p>",2,0,2019-03-13 00:23:26.600000 UTC,3.0,,9,apache-spark|pyspark|databricks|amazon-sagemaker,11894,2015-01-15 17:43:03.700000 UTC,2022-08-23 22:54:25.603000 UTC,,1387,51,1,153,"<p>SageMaker is a great tool for deployment, it simplifies a lot of processes configuring containers, you only need to write 2-3 lines to deploy the model as an endpoint and use it.  SageMaker also provides the dev platform (Jupyter Notebook) which supports Python and Scala (sparkmagic kernal) developing, and i managed installing external scala kernel in jupyter notebook. Overall, SageMaker provides end-to-end ML services. Databricks has unbeatable Notebook environment for Spark development. </p>

<p>Conclusion</p>

<ol>
<li><p>Databricks is a better platform for Big data(scala, pyspark) Developing.(unbeatable notebook environment)</p></li>
<li><p>SageMaker is better for Deployment. and if you are not working on big data, SageMaker is a perfect choice working with (Jupyter notebook + Sklearn + Mature containers + Super easy deployment). </p></li>
<li><p>SageMaker provides ""real time inference"", very easy to build and deploy, very impressive. you can check the official SageMaker Github.
<a href=""https://github.com/awslabs/amazon-sagemaker-examples/tree/master/sagemaker-python-sdk/scikit_learn_inference_pipeline"" rel=""noreferrer"">https://github.com/awslabs/amazon-sagemaker-examples/tree/master/sagemaker-python-sdk/scikit_learn_inference_pipeline</a></p></li>
</ol>",4.0,2019-03-20 21:40:34.270000 UTC,,14.0,['amazon-sagemaker']
How best to convert from azure blob csv format to pandas dataframe while running notebook in azure ml,"<p>I have a number of large csv (tab delimited) data stored as azure blobs, and I want to create a pandas dataframe from these. I can do this locally as follows:</p>

<pre><code>from azure.storage.blob import BlobService
import pandas as pd
import os.path

STORAGEACCOUNTNAME= 'account_name'
STORAGEACCOUNTKEY= ""key""
LOCALFILENAME= 'path/to.csv'        
CONTAINERNAME= 'container_name'
BLOBNAME= 'bloby_data/000000_0'

blob_service = BlobService(account_name=STORAGEACCOUNTNAME, account_key=STORAGEACCOUNTKEY)

# Only get a local copy if haven't already got it
if not os.path.isfile(LOCALFILENAME):
    blob_service.get_blob_to_path(CONTAINERNAME,BLOBNAME,LOCALFILENAME)

df_customer = pd.read_csv(LOCALFILENAME, sep='\t')
</code></pre>

<p>However, when running the notebook on azure ML notebooks, I can't 'save a local copy' and then read from csv, and so I'd like to do the conversion directly (something like pd.read_azure_blob(blob_csv) or just pd.read_csv(blob_csv) would be ideal).</p>

<p>I can get to the desired end result (pandas dataframe for blob csv data), if I first create an azure ML workspace, and then read the datasets into that, and finally using <a href=""https://github.com/Azure/Azure-MachineLearning-ClientLibrary-Python"" rel=""noreferrer"">https://github.com/Azure/Azure-MachineLearning-ClientLibrary-Python</a> to access the dataset as a pandas dataframe, but I'd prefer to just read straight from the blob storage location.</p>",5,0,2015-10-12 23:31:58.393000 UTC,3.0,2015-10-14 09:20:26.277000 UTC,12,python|azure|pandas|azure-blob-storage|azure-machine-learning-studio,22789,2012-08-30 20:36:52.420000 UTC,2018-06-14 21:10:57.573000 UTC,,395,15,0,26,"<p>I think you want to use <code>get_blob_to_bytes</code>, <code>or get_blob_to_text</code>; these should output a string which you can use to create a dataframe as</p>

<pre><code>from io import StringIO
blobstring = blob_service.get_blob_to_text(CONTAINERNAME,BLOBNAME)
df = pd.read_csv(StringIO(blobstring))
</code></pre>",2.0,2015-10-12 23:38:59.743000 UTC,2018-12-18 06:59:18.717000 UTC,17.0,['azure-machine-learning-studio']
How can I access the Workspace object from a training script in AzureML?,"<p>I want to access the Workspace object in my <code>train.py</code> script, when running in an Estimator.  </p>

<p>I currently can access the Run object, using the following code:</p>

<pre class=""lang-py prettyprint-override""><code>run = Run.get_context()
</code></pre>

<p>But I cannot seem to get my hands on the Workspace object in my training script.  I would use this mostly to get access to the Datastores and Datasets (as I would hope to keep all data set references inside the training script, instead of passing them as input datasets)</p>

<p>Any idea if/how this is possible ?</p>",1,0,2020-06-03 11:28:19.477000 UTC,1.0,,4,python|azure|azure-machine-learning-service,1148,2013-02-12 07:50:30.743000 UTC,2022-09-21 18:28:12.907000 UTC,Belgium,2947,297,16,355,"<p>Sure, try this:</p>

<pre><code>from azureml.core.run import Run
run = Run.get_context()
ws = run.experiment.workspace
</code></pre>",0.0,2020-06-03 15:25:55.080000 UTC,,12.0,['azure-machine-learning-service']
Use Azure Machine learning to detect symbol within an image,"<p>4 years ago I posted <a href=""https://stackoverflow.com/q/6999920/411094"">this question</a> and got a few answers that were unfortunately outside my skill level.  I just attended a build tour conference where they spoke about machine learning and this got me thinking of the possibility of using ML as a solution to my problem.  i found <a href=""https://gallery.azureml.net/MachineLearningAPI/02ce55bbc0ab4fea9422fe019995c02f"" rel=""noreferrer"">this</a> on the azure site but i dont think it will help me because its scope is pretty narrow.</p>

<p>Here is what i am trying to achieve:</p>

<p>i have a source image:</p>

<p><img src=""https://i.stack.imgur.com/6y76s.jpg"" alt=""source image""></p>

<p>and i want to which one of the following symbols (if any) are contained in the image above:</p>

<p><img src=""https://i.stack.imgur.com/SuHkU.jpg"" alt=""symbols""></p>

<p>the compare needs to support minor distortion, scaling, color differences, rotation, and brightness differences.</p>

<p>the number of symbols to match will ultimately at least be greater than 100.</p>

<p>is ML a good tool to solve this problem?  if so, any starting tips?</p>",1,0,2015-06-16 05:58:53.360000 UTC,3.0,2017-05-23 12:10:45.110000 UTC,14,opencv|azure|image-processing|machine-learning|azure-machine-learning-studio,6324,2010-08-04 17:41:06.900000 UTC,2022-09-23 19:56:34.233000 UTC,"Los Angeles, CA",1141,32,0,109,"<p>As far as I know, Project Oxford (MS Azure CV API) wouldn't be suitable for your task. Their APIs are very focused to Face related tasks (detection, verification, etc), OCR and Image description. And apparently you can't extend their models or train new ones from the existing ones.</p>

<p>However, even though I don't know an out of the box solution for your object detection problem; there are easy enough approaches that you could try and that would give you some start point results.</p>

<p>For instance, here is a naive method you could use:</p>

<p><strong>1) Create your dataset:</strong>
    This is probably the more tedious step and paradoxically a crucial one. I will assume you have a good amount of images to work with. What would you need to do is to pick a fixed window size and extract positive and negative examples.
<img src=""https://i.stack.imgur.com/H4uC5.png"" alt=""enter image description here""></p>

<p>If some of the images in your dataset are in different sizes you would need to rescale them to a common size. You don't need to get too crazy about the size, probably 30x30 images would be more than enough. To make things easier I would turn the images to gray scale too. </p>

<p><strong>2) Pick a classification algorithm and train it:</strong>
    There is an awful amount of classification algorithms out there. But if you are new to machine learning I will pick the one I would understand the most. Keeping that in mind, I would check out logistic regression which give decent results, it's easy enough for starters and have a lot of libraries and tutorials. For instance, <a href=""http://blog.yhathq.com/posts/logistic-regression-and-python.html"" rel=""noreferrer"">this one</a> or <a href=""https://msdn.microsoft.com/en-us/magazine/dn948113.aspx"" rel=""noreferrer"">this one</a>. At first I would say to focus in a binary classification problem (like if there is an UD logo in the picture or not) and when you master that one you can jump to the multi-class case. There are resources for that <a href=""http://www.codeproject.com/Articles/821347/MultiClass-Logistic-Classifier-in-Python"" rel=""noreferrer"">too</a> or you can always have several models one per logo and run this recipe for each one separately. </p>

<p>To train your model, you just need to read the images generated in the step 1 and turn them into a vector and label them accordingly. That would be the  dataset that will feed your model. If you are using images in gray scale, then each position in the vector would correspond to a pixel value in the range 0-255. Depending on the algorithm you might need to rescale those values to the range [0-1] (this is because some algorithms perform better with values in that range). Notice that rescaling the range in this case is fairly easy (new_value = value/255).</p>

<p>You also need to split your dataset, reserving some examples for training, a subset for validation and another one for testing. Again, there are different ways to do this, but I'm keeping this answer as naive as possible.</p>

<p><strong>3) Perform the detection:</strong>
    So now let's start the fun part. Given any image you want to run your model and produce coordinates in the picture where there is a logo. There are different ways to do this and I will describe one that probably <strong>is not the best nor the more efficient</strong>, but it's easier to develop in my opinion.</p>

<p>You are going to scan the picture, extracting the pixels in a ""window"", rescaling those pixels to the size you selected in step 1 and then feed them to your model. </p>

<p><img src=""https://i.stack.imgur.com/VGk3f.png"" alt=""Extracting windows to feed the model""></p>

<p>If the model give you a positive answer then you mark that window in the original image. Since the logo might appear in different scales you need to repeat this process with different window sizes. You also would need to tweak the amount of space between windows.</p>

<p><strong>4) Rinse and repeat:</strong>
    At the first iteration it's very likely that you will get a lot of false positives. Then you need to take those as negative examples and retrain your model. This would be an iterative process and hopefully on each iteration you will have less and less false positives and fewer false negatives.</p>

<p>Once you are reasonable happy with your solution, you might want to improve it. You might want to try other classification algorithms like <a href=""https://en.wikipedia.org/wiki/Support_vector_machine"" rel=""noreferrer"">SVM</a> or <a href=""https://en.wikipedia.org/wiki/Deep_learning"" rel=""noreferrer"">Deep Learning Artificial Neural Networks</a>, or to try better object detection frameworks like <a href=""https://en.wikipedia.org/wiki/Viola%E2%80%93Jones_object_detection_framework"" rel=""noreferrer"">Viola-Jones</a>. Also, you will probably need to use <a href=""https://en.wikipedia.org/wiki/Cross-validation_%28statistics%29"" rel=""noreferrer"">crossvalidation</a> to compare all your solutions (you can actually use crossvalidation from the beginning). By this moment I bet you would be confident enough that you would like to use OpenCV or another ready to use framework in which case you will have a fair understanding of what is going on under the hood. </p>

<p>Also you could just disregard all this answer and go for an OpenCV object detection tutorial like this <a href=""http://note.sonots.com/SciSoftware/haartraining.html"" rel=""noreferrer"">one</a>. Or take another answer from another question like this <a href=""https://stackoverflow.com/questions/10168686/algorithm-improvement-for-coca-cola-can-shape-recognition?rq=1"">one</a>. Good luck!</p>",3.0,2015-06-16 15:38:50.290000 UTC,2017-05-23 12:18:30.023000 UTC,22.0,['azure-machine-learning-studio']
By how much can i approx. reduce disk volume by using dvc?,"<p>I want to classify ~1m+ documents and have a Version Control System for in- and Output of the corresponding model. </p>

<p>The data changes over time:</p>

<ul>
<li>sample size increases over time</li>
<li>new Features might appear</li>
<li>anonymization procedure might Change over time</li>
</ul>

<p>So basically ""everything"" might change: amount of observations, Features and the values.
We are interested in making the ml model Building reproducible without using 10/100+ GB 
of disk volume, because we save all updated versions of Input data. Currently the volume size of the data is ~700mb.</p>

<p>The most promising tool i found is: <a href=""https://github.com/iterative/dvc"" rel=""noreferrer"">https://github.com/iterative/dvc</a>. Currently the data
is stored in a database in loaded in R/Python from there.</p>

<p><strong>Question:</strong></p>

<p>How much disk volume can be (very approx.) saved by using dvc? </p>

<p>If one can roughly estimate that. I tried to find out if only the ""diffs"" of the data are saved. I didnt find much info by reading through: <a href=""https://github.com/iterative/dvc#how-dvc-works"" rel=""noreferrer"">https://github.com/iterative/dvc#how-dvc-works</a> or other documentation. </p>

<p><strong>I am aware that this is a very vague question. And it will highly depend on the dataset. However, i would still be interested in getting a very approximate idea.</strong></p>",1,0,2020-02-23 18:31:41.247000 UTC,3.0,2020-02-23 19:28:48.287000 UTC,7,python|sql|r|git|dvc,689,2017-08-30 12:46:30.907000 UTC,2022-03-11 18:10:58.673000 UTC,,1365,145,3,193,"<p>Let me try to summarize how does DVC store data and I hope you'll be able to figure our from this how much space will be saved/consumed in your specific scenario.</p>

<p><strong>DVC is storing and deduplicating data on the individual <em>file level</em>.</strong> So, what does it usually mean from a practical perspective.</p>

<p>I will use <code>dvc add</code> as an example, but the same logic applies to all commands that save data files or directories into DVC cache - <code>dvc add</code>, <code>dvc run</code>, etc.</p>

<h2>Scenario 1: Modifying file</h2>

<p>Let's imagine I have a single 1GB XML file. I start tracking it with DVC:</p>

<pre class=""lang-sh prettyprint-override""><code>$ dvc add data.xml
</code></pre>

<p>On the modern file system (or if <code>hardlinks</code>, <code>symlinks</code> are enabled, see <a href=""https://dvc.org/doc/user-guide/large-dataset-optimization"" rel=""noreferrer"">this</a> for more details) after this command we still consume 1GB (even though file is moved into DVC cache and is still present in the workspace).</p>

<p>Now, let's change it a bit and save it again:</p>

<pre class=""lang-sh prettyprint-override""><code>$ echo ""&lt;test/&gt;"" &gt;&gt; data.xml
$ dvc add data.xml
</code></pre>

<p>In this case we will have 2GB consumed. <strong>DVC does not do diff between two versions of the same file</strong>, neither it splits files into chunks or blocks to understand that only small portion of data has changed.</p>

<blockquote>
  <p>To be precise, it calculates <code>md5</code> of each file and save it in the content addressable key-value storage. <code>md5</code> of the files serves as a key (path of the file in cache) and value is the file itself:</p>
  
  <pre class=""lang-sh prettyprint-override""><code>(.env) [ivan@ivan ~/Projects/test]$ md5 data.xml
0c12dce03223117e423606e92650192c

(.env) [ivan@ivan ~/Projects/test]$ tree .dvc/cache
.dvc/cache
└── 0c
   └── 12dce03223117e423606e92650192c

1 directory, 1 file

(.env) [ivan@ivan ~/Projects/test]$ ls -lh data.xml
data.xml ----&gt; .dvc/cache/0c/12dce03223117e423606e92650192c (some type of link)
</code></pre>
</blockquote>

<h2>Scenario 2: Modifying directory</h2>

<p>Let's now imagine we have a single large 1GB directory <code>images</code> with a lot of files:</p>

<pre class=""lang-sh prettyprint-override""><code>$ du -hs images
1GB

$ ls -l images | wc -l
1001

$ dvc add images
</code></pre>

<p>At this point we still consume 1GB. Nothing has changed. But if we modify the directory by adding more files (or removing some of them):</p>

<pre><code>$ cp /tmp/new-image.png images

$ ls -l images | wc -l
1002

$ dvc add images
</code></pre>

<p>In this case, after saving the new version we <strong>still close to 1GB</strong> consumption. <strong>DVC calculates diff on the directory level.</strong> It won't be saving all the files that were existing before in the directory.</p>

<p>The same logic applies to all commands that save data files or directories into DVC cache - <code>dvc add</code>, <code>dvc run</code>, etc.</p>

<p>Please, let me know if it's clear or we need to add more details, clarifications.</p>",4.0,2020-02-23 19:57:47.857000 UTC,,12.0,['dvc']
