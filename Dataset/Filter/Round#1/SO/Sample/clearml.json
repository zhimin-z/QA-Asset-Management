[
	{
		"Question_id": 71432331,
		"Question_title": "ALB host based routing without domain name",
		"Question_body": "<p>I'm trying to configure host based routing in AWS ALB for ClearML server using <a href=\"https://allegro.ai/clearml/docs/docs/deploying_clearml/clearml_server_config.html#configuration-procedures\" rel=\"nofollow noreferrer\">this tutorial</a>.\nHowever, I don't have a domain name. So can I only use alb's dns for this routing?</p>\n<p>For example, I will have the address as app.<em><strong>.ap-north-east-1.elb.amazonaws.com, api.</strong></em>.ap-north-east-1.elb.amazonaws.com.</p>",
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_creation_date": "2022-03-11 00:25:08.150000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": null,
		"Question_score": 0,
		"Question_tags": "aws-application-load-balancer|clearml",
		"Question_view_count": 190,
		"Owner_creation_date": "2018-10-01 12:52:13.233000 UTC",
		"Owner_last_access_date": "2022-08-13 09:22:52.563000 UTC",
		"Owner_location": "\u014csaka-shi, \u5927\u962a\u5e9c \u65e5\u672c",
		"Owner_reputation": 51,
		"Owner_up_votes": 0,
		"Owner_down_votes": 0,
		"Owner_views": 6,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "clearml",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 56768436,
		"Question_title": "How to Backup/Restore TRAINS-server when moving from AMI to local machine",
		"Question_body": "<p>I recently started using TRAINS, with the server in AWS AMI. We are currently using v0.9.0.</p>\n\n<p>I would like to move the TRAINS-server to run on our on-premises kubernetes cluster. However, I don't want to lose the data on the current server in AWS (experiments, models, logins, etc...).\nIs there a way to backup the current server and restore it to the local server?</p>\n\n<p>Thanks!</p>",
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_creation_date": "2019-06-26 08:27:33.917000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": "2021-01-05 15:40:59.227000 UTC",
		"Question_score": 1,
		"Question_tags": "kubernetes|deep-learning|trains|clearml",
		"Question_view_count": 2703,
		"Owner_creation_date": "2016-11-08 19:13:17.167000 UTC",
		"Owner_last_access_date": "2019-06-26 11:21:41.617000 UTC",
		"Owner_location": null,
		"Owner_reputation": 31,
		"Owner_up_votes": 1,
		"Owner_down_votes": 0,
		"Owner_views": 6,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "clearml",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 73456627,
		"Question_title": "ClearML Remote execution does not use requirements.txt",
		"Question_body": "<p>I want to execute a function on a worker with clearml. I use the code below to do this. But worker will always create a new env, although I set it to use my python binary via environmet settings. I also tried to pass a requirements.txt but this will be ignored or misinterpreted.\nThe main issue is that I need opencv_contrib, but clearml always installs opencv afterwards and the contrib package will get overwritten, so the contrib methods are not available and the remote execution fails.</p>\n<p>How can I force to use my existing python (conda) environment?\nHow can I force to install the packages I define in my requirements.txt?</p>\n<pre><code>    def my_func(path:str):\n        # do calculation\n        return \n    paths = ['/media/hdd/my_folder_1/']\n    for i, path in enumerate(paths):\n        task.create_function_task(my_func, func_name=my_func-{i}',\n                              task_name=f'my_func - {i}', path=path)\n</code></pre>\n<p>Starting script for my clearml worker</p>\n<pre><code>#!/bin/bash\n\nexport CLEARML_HOST_IP=127.0.0.1\nexport CLEARML_AGENT_SKIP_PIP_VENV_INSTALL=/home/user/miniconda3/envs/my_env/bin/python\nclearml-agent daemon --detached --queue default\n</code></pre>\n<p>I'm using latest clearml on ubuntu 20.04.</p>",
		"Question_answer_count": 0,
		"Question_comment_count": 0,
		"Question_creation_date": "2022-08-23 09:54:08.260000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": null,
		"Question_score": 0,
		"Question_tags": "clearml",
		"Question_view_count": 14,
		"Owner_creation_date": "2020-11-03 08:23:14.420000 UTC",
		"Owner_last_access_date": "2022-09-20 10:40:35.373000 UTC",
		"Owner_location": null,
		"Owner_reputation": 89,
		"Owner_up_votes": 2,
		"Owner_down_votes": 0,
		"Owner_views": 4,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "clearml",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 73279794,
		"Question_title": "[Catboost][ClearML] Error: if loss-function is Logloss, then class weights should be given for 0 and 1 classes",
		"Question_body": "<p>Having recently started using ClearML to manage the MLOps, I am facing the following problem:\nWhen running a script that trains a CatBoost in a binary classification problem using different class weights from my computer, it works perfectly, logs the results and no issues at all.\nOnce I try to run that remotely using the ClearML agent, it results in the following error:</p>\n<pre><code>&lt;!-- language: lang-none --&gt;\nTraceback (most recent call last):\n  File &quot;/root/.clearml/venvs-builds/3.9/lib/python3.9/site-packages/clearml/binding/frameworks/catboost_bind.py&quot;, line 102, in _fit\n    return original_fn(obj, *args, **kwargs)\n  File &quot;/root/.clearml/venvs-builds/3.9/lib/python3.9/site-packages/catboost/core.py&quot;, line 5007, in fit\n    self._fit(X, y, cat_features, text_features, embedding_features, None, sample_weight, None, None, None, None, baseline, use_best_model,\n  File &quot;/root/.clearml/venvs-builds/3.9/lib/python3.9/site-packages/catboost/core.py&quot;, line 2262, in _fit\n    train_params = self._prepare_train_params(\n  File &quot;/root/.clearml/venvs-builds/3.9/lib/python3.9/site-packages/catboost/core.py&quot;, line 2194, in _prepare_train_params\n    _check_train_params(params)\n  File &quot;_catboost.pyx&quot;, line 6032, in _catboost._check_train_params\n  File &quot;_catboost.pyx&quot;, line 6051, in _catboost._check_train_params\n**_catboost.CatBoostError: catboost/private/libs/options/catboost_options.cpp:607: if loss-function is Logloss, then class weights should be given for 0 and 1 classes\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):**\n  File &quot;/root/.clearml/venvs-builds/3.9/task_repository/RecSys.git/src/cli/model_training_remote.py&quot;, line 313, in &lt;module&gt;\n    rfs.run(\n  File &quot;/root/.clearml/venvs-builds/3.9/task_repository/RecSys.git/src/cli/model_training_remote.py&quot;, line 232, in run\n    model.fit(\n  File &quot;/root/.clearml/venvs-builds/3.9/lib/python3.9/site-packages/clearml/binding/frameworks/__init__.py&quot;, line 36, in _inner_patch\n    raise ex\n  File &quot;/root/.clearml/venvs-builds/3.9/lib/python3.9/site-packages/clearml/binding/frameworks/__init__.py&quot;, line 34, in _inner_patch\n    ret = patched_fn(original_fn, *args, **kwargs)\n  File &quot;/root/.clearml/venvs-builds/3.9/lib/python3.9/site-packages/clearml/binding/frameworks/catboost_bind.py&quot;, line 110, in _fit\n    return original_fn(obj, *args, **kwargs)\n  File &quot;/root/.clearml/venvs-builds/3.9/lib/python3.9/site-packages/catboost/core.py&quot;, line 5007, in fit\n    self._fit(X, y, cat_features, text_features, embedding_features, None, sample_weight, None, None, None, None, baseline, use_best_model,\n  File &quot;/root/.clearml/venvs-builds/3.9/lib/python3.9/site-packages/catboost/core.py&quot;, line 2262, in _fit\n    train_params = self._prepare_train_params(\n  File &quot;/root/.clearml/venvs-builds/3.9/lib/python3.9/site-packages/catboost/core.py&quot;, line 2194, in _prepare_train_params\n    _check_train_params(params)\n  File &quot;_catboost.pyx&quot;, line 6032, in _catboost._check_train_params\n  File &quot;_catboost.pyx&quot;, line 6051, in _catboost._check_train_params\n**_catboost.CatBoostError: catboost/private/libs/options/catboost_options.cpp:607: if loss-function is Logloss, then class weights should be given for 0 and 1 classes**\n\n</code></pre>\n<p>I do have the dictionary being connected:</p>\n<pre><code>    model_params = {\n        &quot;loss_function&quot;: &quot;Logloss&quot;,\n        &quot;eval_metric&quot;: &quot;AUC&quot;,\n        &quot;class_weights&quot;: {0: 1, 1: 60},\n        &quot;learning_rate&quot;: 0.1\n    }\n</code></pre>\n<p>registered in the ClearML task as</p>\n<pre><code>task.connect(model_params, 'model_params')\n</code></pre>\n<p>and used as parameters for the model in the following call:</p>\n<pre><code>model = CatBoostClassifier(**model_params)\n</code></pre>\n<p>When running it from the container in ClearML interactive mode, it also works fine.</p>",
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_creation_date": "2022-08-08 14:47:32.813000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": null,
		"Question_score": 1,
		"Question_tags": "python|catboost|clearml",
		"Question_view_count": 54,
		"Owner_creation_date": "2022-08-08 14:30:03.173000 UTC",
		"Owner_last_access_date": "2022-09-23 21:14:51.430000 UTC",
		"Owner_location": null,
		"Owner_reputation": 13,
		"Owner_up_votes": 0,
		"Owner_down_votes": 0,
		"Owner_views": 0,
		"Answer_body": "<p>Disclaimer: I'm a team members of ClearML</p>\n<p>I think I understand the problem, basically I think the issue is:</p>\n<pre><code>task.connect(model_params, 'model_params')\n</code></pre>\n<p>Since this is a nested dict:</p>\n<pre><code>    model_params = {\n        &quot;loss_function&quot;: &quot;Logloss&quot;,\n        &quot;eval_metric&quot;: &quot;AUC&quot;,\n        &quot;class_weights&quot;: {0: 1, 1: 60},\n        &quot;learning_rate&quot;: 0.1\n    }\n</code></pre>\n<p>The class_weights is stored as a <code>String</code> key, but <code>catboost</code> expects <code>int</code> key, hence failing.\nOne option would be to remove the <code>task.connect(model_params, 'model_params')</code></p>\n<p>Another solution (until we fix it) would be to do:</p>\n<pre class=\"lang-py prettyprint-override\"><code>task.connect(model_params, 'model_params')\nmodel_params[&quot;class_weights&quot;] = {\n0: model_params[&quot;class_weights&quot;].get(&quot;0&quot;, model_params[&quot;class_weights&quot;].get(0))\n1: model_params[&quot;class_weights&quot;].get(&quot;1&quot;, model_params[&quot;class_weights&quot;].get(1))\n}\n</code></pre>",
		"Answer_comment_count": 0.0,
		"Answer_creation_date": "2022-08-08 20:52:18.293000 UTC",
		"Answer_last_edit_date": null,
		"Answer_score": 0.0,
		"Question_valid_tags": "clearml",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 73108016,
		"Question_title": "Where do augmentations in ClearML run?",
		"Question_body": "<p>In ClearML Dataviews, it is possible to add <a href=\"https://clear.ml/docs/latest/docs/hyperdatasets/dataviews/#data-augmentation\" rel=\"nofollow noreferrer\">augmentations</a>.</p>\n<p>Where do these augmentations run?</p>\n<p>Options</p>\n<ol>\n<li>Original data gets downloaded to local, then runs (on which device? How is multiprocessing handled?)</li>\n<li>Only augmented data gets downloaded to local cache, augmentations run remotely (who pays for compute? How fast? Should pipelines be changed accordingly?)</li>\n</ol>\n<p>I couldn't find this in the docs.</p>",
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_creation_date": "2022-07-25 10:52:01.690000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": null,
		"Question_score": 0,
		"Question_tags": "deep-learning|data-augmentation|mlops|clearml",
		"Question_view_count": 22,
		"Owner_creation_date": "2011-08-25 22:58:29.233000 UTC",
		"Owner_last_access_date": "2022-09-24 23:30:23.147000 UTC",
		"Owner_location": "Technion, Israel",
		"Owner_reputation": 18777,
		"Owner_up_votes": 2376,
		"Owner_down_votes": 137,
		"Owner_views": 2000,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "clearml",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 73570817,
		"Question_title": "ClearML - dynamically updating Plotly plots?",
		"Question_body": "<p>I have a question related to ClearML plot logging. We are currently using:</p>\n<pre><code>self.task_logger.report_table(&quot;TableSpaceName&quot;, &quot;Some Info&quot;, iteration=0, table_plot=df)\n</code></pre>\n<p>To report tables. They appear under &quot;PLOTS&quot; section. Similarly, we are reporting plotly graphs:</p>\n<pre><code>self.task_logger.report_plotly(\n        title=&quot;PlotTitle&quot;, iteration=0, series='SeriesName', figure=fig\n    )\n</code></pre>\n<p>Both work fine. The issue is, each new <code>report_plotly</code> call, instead of replacing the image in the section, creates a new one, and leaves the previous one present too. This cloggs the PLOTS section (tables and figures). The question is, how does one report a plot, so that it's reported in-place (Such as e.g., scalars, where sample plot gets updated in time)?</p>",
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_creation_date": "2022-09-01 14:11:36.990000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": "2022-09-01 14:18:53.803000 UTC",
		"Question_score": 0,
		"Question_tags": "python|plot|clearml",
		"Question_view_count": 23,
		"Owner_creation_date": "2014-04-11 11:22:10.877000 UTC",
		"Owner_last_access_date": "2022-09-24 18:35:51.867000 UTC",
		"Owner_location": null,
		"Owner_reputation": 1968,
		"Owner_up_votes": 43,
		"Owner_down_votes": 6,
		"Owner_views": 190,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "clearml",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 64305945,
		"Question_title": "pip install trains fails",
		"Question_body": "<p>upon running <code>pip install trains</code> in my virtual env</p>\n<p>I am getting</p>\n<pre><code>    ERROR: Command errored out with exit status 1:\n     command: /home/epdadmin/noam/code/venv_linux/bin/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'/tmp/pip-install-owzh8lnl/retrying/setup.py'&quot;'&quot;'; __file__='&quot;'&quot;'/tmp/pip-install-owzh8lnl/retrying/setup.py'&quot;'&quot;';f=getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(__file__);code=f.read().replace('&quot;'&quot;'\\r\\n'&quot;'&quot;', '&quot;'&quot;'\\n'&quot;'&quot;');f.close();exec(compile(code, __file__, '&quot;'&quot;'exec'&quot;'&quot;'))' install --record /tmp/pip-record-lxz5t8pu/install-record.txt --single-version-externally-managed --compile --install-headers /home/epdadmin/noam/code/venv_linux/include/site/python3.8/retrying\n         cwd: /tmp/pip-install-owzh8lnl/retrying/\n    Complete output (10 lines):\n    running install\n    running build\n    running build_py\n    creating build\n    creating build/lib\n    copying retrying.py -&gt; build/lib\n    running install_lib\n    copying build/lib/retrying.py -&gt; /home/epdadmin/noam/code/venv_linux/lib/python3.8/site-packages\n    byte-compiling /home/epdadmin/noam/code/venv_linux/lib/python3.8/site-packages/retrying.py to retrying.cpython-38.pyc\n    error: [Errno 13] Permission denied: '/home/epdadmin/noam/code/venv_linux/lib/python3.8/site-packages/__pycache__/retrying.cpython-38.pyc.139678407381360'\n    ----------------------------------------\nERROR: Command errored out with exit status 1: /home/epdadmin/noam/code/venv_linux/bin/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'/tmp/pip-install-owzh8lnl/retrying/setup.py'&quot;'&quot;'; __file__='&quot;'&quot;'/tmp/pip-install-owzh8lnl/retrying/setup.py'&quot;'&quot;';f=getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(__file__);code=f.read().replace('&quot;'&quot;'\\r\\n'&quot;'&quot;', '&quot;'&quot;'\\n'&quot;'&quot;');f.close();exec(compile(code, __file__, '&quot;'&quot;'exec'&quot;'&quot;'))' install --record /tmp/pip-record-lxz5t8pu/install-record.txt --single-version-externally-managed --compile --install-headers /home/epdadmin/noam/code/venv_linux/include/site/python3.8/retrying Check the logs for full command output.\n</code></pre>\n<p>I know that <a href=\"https://stackoverflow.com/questions/15028648/is-it-acceptable-and-safe-to-run-pip-install-under-sudo\">I am not supposed to run under sudo when using a venv</a>, so I don't really understand the problem</p>\n<p>running for example <code>pip install pandas</code> does work.</p>\n<p>Python 3.8</p>\n<p>How to install trains?</p>\n<hr />\n<p>EDIT:</p>\n<p>running <code>pip install trains --user</code> or <code>pip install --user trains</code> gives</p>\n<pre><code>ERROR: Can not perform a '--user' install. User site-packages are not visible in this virtualenv.\n</code></pre>",
		"Question_answer_count": 3,
		"Question_comment_count": 3,
		"Question_creation_date": "2020-10-11 15:44:31.093000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": "2020-12-30 18:45:56.110000 UTC",
		"Question_score": 1,
		"Question_tags": "python|pip|trains|clearml",
		"Question_view_count": 1031,
		"Owner_creation_date": "2011-08-25 22:58:29.233000 UTC",
		"Owner_last_access_date": "2022-09-24 23:30:23.147000 UTC",
		"Owner_location": "Technion, Israel",
		"Owner_reputation": 18777,
		"Owner_up_votes": 2376,
		"Owner_down_votes": 137,
		"Owner_views": 2000,
		"Answer_body": "<p>The problem was a permissions problem for the venv.\nAnother problem was trains required some packages that were not yet available with wheels on Python3.8, so I had to downgrade Python to 3.7</p>\n<p>That venv was created using Pycharm, and for some reason it was created with low permissions.</p>\n<p>There was probably a way to elevate its permissions, but instead I just deleted it and created another one using command line by</p>\n<pre><code>python -m virtualenv --python=/usr/bin/python3.7 venv\n</code></pre>\n<p>And now <code>pip install trains</code> worked.</p>\n<p>Very annoying.</p>",
		"Answer_comment_count": 0.0,
		"Answer_creation_date": "2020-10-15 13:18:50.970000 UTC",
		"Answer_last_edit_date": "2020-11-02 09:09:43.097000 UTC",
		"Answer_score": 1.0,
		"Question_valid_tags": "clearml",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 72381916,
		"Question_title": "Remotely execute ClearML task using local-only repo",
		"Question_body": "<p>I want to execute ClearML task remotely. According to docs there are 2 options: 1) execute single python file; 2) ClearML would identify that script is part of repo, that repo will be cloned and installed into docker and executed on the worker.</p>\n<p>In this second scenario it is assumed that repo has remote url and it is accessible by worker. What if it isn't the case? Is it possible to somehow pack the local repo and send it for remote execution.</p>\n<p>I think it is somewhat extending scenario 1, where not a single file is passed for execution but whole directory with file in it.</p>\n<p>PS: i understand reproducibility concerns that arise, but repo is really not accessible from worker :(</p>\n<p>Thanks in advance.</p>",
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_creation_date": "2022-05-25 17:24:40.970000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": null,
		"Question_score": 0,
		"Question_tags": "clearml",
		"Question_view_count": 25,
		"Owner_creation_date": "2022-05-25 17:13:50.777000 UTC",
		"Owner_last_access_date": "2022-09-02 10:09:50.107000 UTC",
		"Owner_location": null,
		"Owner_reputation": 3,
		"Owner_up_votes": 0,
		"Owner_down_votes": 0,
		"Owner_views": 0,
		"Answer_body": "<p>Disclaimer: I'm a team members of ClearML</p>\n<blockquote>\n<p>In this second scenario it is assumed that repo has remote url and it is accessible by worker. What if it isn't the case? Is it possible to somehow pack the local repo and send it for remote execution.</p>\n</blockquote>\n<p>well, no :( if your code is a single script, then yes ClearML would store the entire script, then the worker will reproduce it on the remote machine. But if your code base is composed of more than a single file, then why not use git? it is free hosted by GitHub, Bitbucket, GitLab etc.</p>\n<p>In theory this is doable and if you feel the need, I urge you to PR this feature. Basically you would store the entire folder as an artifact (ClearML will auto zip it for you), then the agent needs to unzip the artifact and run it. The main issue would be that cloning the Task will not clone the artifact...</p>",
		"Answer_comment_count": 0.0,
		"Answer_creation_date": "2022-08-08 21:03:38.003000 UTC",
		"Answer_last_edit_date": null,
		"Answer_score": 0.0,
		"Question_valid_tags": "clearml",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 70423979,
		"Question_title": "Export metrics of ClearML to Prometheus and show them in Grafana",
		"Question_body": "<p>Are there any metrics I can get from the API server? or any docker image I can point to the backend and get some metrics?\nMost important is the see how many tasks running in real-time (like we can see on the worker's page) and also check how much time each task is running (also can be found on the worker's page)</p>\n<p>If it does not exist, do they have an API for getting all this information?</p>",
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_creation_date": "2021-12-20 15:16:30.177000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": null,
		"Question_score": 0,
		"Question_tags": "prometheus|grafana|metrics|clearml",
		"Question_view_count": 70,
		"Owner_creation_date": "2019-09-21 19:04:11.057000 UTC",
		"Owner_last_access_date": "2022-09-06 09:31:53.617000 UTC",
		"Owner_location": null,
		"Owner_reputation": 3,
		"Owner_up_votes": 0,
		"Owner_down_votes": 0,
		"Owner_views": 3,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "clearml",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 73442798,
		"Question_title": "Does ClearML have accounting of information security events",
		"Question_body": "<p>ClearML is one of the most famous MLOps tools existing. It has logging of machine learning processes, however I couldn't find any information regarding its system of accounting of <strong>information security events</strong>.</p>\n<p>My question is: does ClearML have such system? Does it register/log events of client-server interaction? If ClearML does, then what format is used?</p>",
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_creation_date": "2022-08-22 09:19:05.233000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": null,
		"Question_score": 1,
		"Question_tags": "logging|clearml",
		"Question_view_count": 37,
		"Owner_creation_date": "2022-08-22 09:04:07.617000 UTC",
		"Owner_last_access_date": "2022-09-22 13:02:34.370000 UTC",
		"Owner_location": null,
		"Owner_reputation": 11,
		"Owner_up_votes": 0,
		"Owner_down_votes": 0,
		"Owner_views": 1,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "clearml",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 63586251,
		"Question_title": "Will Trains automagically log Tensorboard HParams?",
		"Question_body": "<p>I know that it's possible to send hyper-params as a dictionary to Trains.</p>\n<p>But can it also automagically log hyper-params that are logged using the TF2 HParams module?</p>\n<p>Edit: This is done in the <a href=\"https://www.tensorflow.org/tensorboard/hyperparameter_tuning_with_hparams\" rel=\"nofollow noreferrer\">HParams tutorial</a> using <code>hp.hparams(hparams)</code>.</p>\n<p><img src=\"https://www.tensorflow.org/tensorboard/images/hparams_parallel_coordinates.png?raw=1\" alt=\"Tensorboard HParams\" /></p>",
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_creation_date": "2020-08-25 19:59:10.273000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": "2020-12-31 15:11:37.023000 UTC",
		"Question_score": 1,
		"Question_tags": "trains|clearml",
		"Question_view_count": 142,
		"Owner_creation_date": "2011-07-22 10:25:49.880000 UTC",
		"Owner_last_access_date": "2022-09-21 15:11:42.327000 UTC",
		"Owner_location": "Tel Aviv",
		"Owner_reputation": 3784,
		"Owner_up_votes": 472,
		"Owner_down_votes": 0,
		"Owner_views": 342,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "clearml",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 67496760,
		"Question_title": "Mounting an S3 bucket in docker in a clearml agent",
		"Question_body": "<p>What is the best practice for mounting an S3 container inside a docker image that will be using as a ClearML agent?  I can think of 3 solutions, but have been unable to get any to work currently:</p>\n<ol>\n<li>Use <a href=\"https://allegro.ai/clearml/docs/docs/use_cases/clearml_agent_use_case_examples.html?highlight=docker\" rel=\"nofollow noreferrer\">prefabbed configuration in ClearML</a>, specifically CLEARML_AGENT_K8S_HOST_MOUNT.  For this to work, the S3 bucket would be mounted separately on the host using <a href=\"https://rclone.org/\" rel=\"nofollow noreferrer\">rclone</a> and then remapped into docker. This appears to only apply to Kubernetes and not Docker - and therefore would not work.</li>\n<li>Mount using s3fuse as specified <a href=\"https://stackoverflow.com/questions/35189251/docker-mount-s3-container\">here</a>.  The issue is will it work with the S3 bucket secret stored in ClearML browser sessions?  This would also appear to be complicated and require custom docker images, not to mention running the docker image as --privileged or similar.</li>\n<li>Pass arguments to docker using &quot;docker_args and docker_bash_setup_script arguments to Task.create()&quot; as specified in the <a href=\"https://allegro.ai/clearml/docs/docs/release_notes/ver_1_0.html\" rel=\"nofollow noreferrer\">1.0 release notes</a>.  This would be similar to (1), but the arguments would be for <a href=\"https://docs.docker.com/storage/bind-mounts/\" rel=\"nofollow noreferrer\">bind-mounting the volume</a>.  I do not see much documentation or examples on how this new feature may be used for this end.</li>\n</ol>",
		"Question_answer_count": 2,
		"Question_comment_count": 0,
		"Question_creation_date": "2021-05-12 02:58:51.577000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": "2021-05-14 19:57:30.560000 UTC",
		"Question_score": 1,
		"Question_tags": "docker|amazon-s3|wsl-2|rclone|clearml",
		"Question_view_count": 770,
		"Owner_creation_date": "2013-03-06 14:43:00.910000 UTC",
		"Owner_last_access_date": "2022-09-16 18:05:09.150000 UTC",
		"Owner_location": "Akron, OH, USA",
		"Owner_reputation": 4013,
		"Owner_up_votes": 94,
		"Owner_down_votes": 0,
		"Owner_views": 72,
		"Answer_body": "<p>I was able to get another option entirely to work, namely, mount a drive on in WSL and then pass it to Docker.  Let's get to it:</p>\n<p>Why not host in Windows itself, why rclone in WSL?</p>\n<ul>\n<li>Docker running on WSL <a href=\"https://github.com/billziss-gh/winfsp/issues/61\" rel=\"nofollow noreferrer\">cannot access drives mounted through winfsp</a> (what rclone uses)</li>\n</ul>\n<p>Steps to mount the drive in ClearML in Windows:</p>\n<ul>\n<li>You can install rclone in WSL and the mount will be accessible to docker\n<ul>\n<li>create the folder <code>/data/my-mount</code> (this needs to be in <code>/data</code> - I don't know why and I can't find out with a Google search, but I found out about it <a href=\"https://forum.rclone.org/t/fusermount-permission-denied-in-docker-rclone/13914/5\" rel=\"nofollow noreferrer\">here</a>)</li>\n<li>You can put the configuration file in windows (use the <code>--config</code> option).</li>\n<li>Note: ClearML will not support spaces in mounted paths, even though docker will.  Therefore your path has to be <code>/data/my-mount</code> rather than <code>/data/my mount</code>.  There is a <a href=\"https://github.com/allegroai/clearml/issues/358\" rel=\"nofollow noreferrer\">bug that I opened about this</a>.</li>\n</ul>\n</li>\n<li>You can test mounting by calling docker and mounting the file.\n<ul>\n<li>Example: <code>docker run -it -v \\\\wsl$\\Ubuntu\\data:/data my-docker-image:latest ls /data/my-mount</code></li>\n<li>Note: You will have to mount /data rather than /data/my-mount, otherwise you may get this error: <code>docker: Error response from daemon: error while creating mount source path</code></li>\n</ul>\n</li>\n<li>Now, you can setup the clearml.conf file in <code>C:\\Users\\Myself\\clearml.conf</code> such that:</li>\n</ul>\n<pre><code>default_docker: {\n   # default docker image to use when running in docker mode\n   image: &quot;my-docker-image:latest&quot;\n\n   # optional arguments to pass to docker image\n   arguments: [&quot;-v&quot;,&quot;\\\\wsl$\\Ubuntu\\data:/data&quot;, ]\n}\n</code></pre>\n<ul>\n<li>Note that you can also run clearml-agent out of WSL and then would only need to specify <code>[&quot;-v&quot;,&quot;/data:/data&quot;, ]</code>.</li>\n<li>Run clearml agent in cmd: <code>clearml-agent daemon --docker</code></li>\n</ul>",
		"Answer_comment_count": 0.0,
		"Answer_creation_date": "2021-05-14 16:30:41.940000 UTC",
		"Answer_last_edit_date": "2021-05-14 17:07:32.113000 UTC",
		"Answer_score": 0.0,
		"Question_valid_tags": "clearml",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 71644156,
		"Question_title": "make clearml agent do not install envs for every task",
		"Question_body": "<p>I want make my clearml agent do not install python envs for every task, it take too long.</p>\n<p>I tried setup config like: package_manager.system_size_packages=true, but it doesn't work.</p>\n<p>clearml agent won't install python envs anymore</p>",
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_creation_date": "2022-03-28 08:01:10.133000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": null,
		"Question_score": -1,
		"Question_tags": "clearml",
		"Question_view_count": 121,
		"Owner_creation_date": "2022-03-28 07:56:51.643000 UTC",
		"Owner_last_access_date": "2022-03-29 11:40:36.717000 UTC",
		"Owner_location": null,
		"Owner_reputation": 1,
		"Owner_up_votes": 0,
		"Owner_down_votes": 0,
		"Owner_views": 0,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "clearml",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 65509754,
		"Question_title": "Can ClearML (formerly Trains) work a local server?",
		"Question_body": "<p>I am trying to start my way with <a href=\"https://github.com/allegroai/clearml\" rel=\"nofollow noreferrer\">ClearML</a> (formerly known as Trains).</p>\n<p>I see on the <a href=\"https://allegro.ai/clearml/docs/rst/getting_started/index.html\" rel=\"nofollow noreferrer\">documentation</a> that I need to have server running, either on the ClearML platform itself, or on a remote machine using AWS etc.</p>\n<p>I would really like to bypass this restriction and run experiments on my local machine, not connecting to any remote destination.</p>\n<p>According to <a href=\"https://allegro.ai/clearml/docs/rst/deploying_clearml/index.html\" rel=\"nofollow noreferrer\">this</a> I can install the <code>trains-server</code> on any remote machine, so in theory I should also be able to install it on my local machine, but it still requires me to have Kubernetes or Docker, but I am not using any of them.</p>\n<p>Anyone had any luck using ClearML (or Trains, I think it's still quite the same API and all) on a local server?</p>\n<ul>\n<li>My OS is Ubuntu 18.04.</li>\n</ul>",
		"Question_answer_count": 1,
		"Question_comment_count": 2,
		"Question_creation_date": "2020-12-30 15:54:39.217000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": "2020-12-31 15:03:29.850000 UTC",
		"Question_score": 5,
		"Question_tags": "trains|clearml",
		"Question_view_count": 740,
		"Owner_creation_date": "2013-10-29 21:50:14.230000 UTC",
		"Owner_last_access_date": "2022-09-23 12:45:41.383000 UTC",
		"Owner_location": null,
		"Owner_reputation": 2801,
		"Owner_up_votes": 371,
		"Owner_down_votes": 0,
		"Owner_views": 131,
		"Answer_body": "<p>Disclaimer: I'm a member of the ClearML team (formerly Trains)</p>\n<blockquote>\n<p>I would really like to bypass this restriction and run experiments on my local machine, not connecting to any remote destination.</p>\n</blockquote>\n<p>A few options:</p>\n<ol>\n<li>The Clearml Free trier offers free hosting for your experiments, these experiment are only accessible to you, unless you specifically want to share them among your colleagues. This is probably the easiest way to <a href=\"https://app.community.clear.ml\" rel=\"nofollow noreferrer\">get started</a>.</li>\n<li>Install the ClearML-Server basically all you need is docker installed and you should be fine. There are full instructions <a href=\"https://clear.ml/docs/latest/docs/deploying_clearml/clearml_server_linux_mac/\" rel=\"nofollow noreferrer\">here</a> , this is the summary:</li>\n</ol>\n<pre class=\"lang-bash prettyprint-override\"><code>echo &quot;vm.max_map_count=262144&quot; &gt; /tmp/99-trains.conf\nsudo mv /tmp/99-trains.conf /etc/sysctl.d/99-trains.conf\nsudo sysctl -w vm.max_map_count=262144\nsudo service docker restart\n\nsudo curl -L &quot;https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m)&quot; -o /usr/local/bin/docker-compose\nsudo chmod +x /usr/local/bin/docker-compose\n\nsudo mkdir -p /opt/trains/data/elastic_7\nsudo mkdir -p /opt/trains/data/mongo/db\nsudo mkdir -p /opt/trains/data/mongo/configdb\nsudo mkdir -p /opt/trains/data/redis\nsudo mkdir -p /opt/trains/logs\nsudo mkdir -p /opt/trains/config\nsudo mkdir -p /opt/trains/data/fileserver\n\nsudo curl https://raw.githubusercontent.com/allegroai/trains-server/master/docker-compose.yml -o /opt/trains/docker-compose.yml\ndocker-compose -f /opt/trains/docker-compose.yml up -d\n</code></pre>\n<ol start=\"3\">\n<li>ClearML also supports full offline mode (i.e. no outside connection is made). Once your experiment completes, you can manually import the run to your server (either self hosted or free tier server)</li>\n</ol>\n<pre class=\"lang-py prettyprint-override\"><code>from clearml import Task\nTask.set_offline(True)\ntask = Task.init(project_name='examples', task_name='offline mode experiment')\n</code></pre>\n<p>When the process ends you will get a link to a zip file containing the output of the entire offline session:</p>\n<pre><code>ClearML Task: Offline session stored in /home/user/.clearml/cache/offline/offline-2d061bb57d9e408a9420c4fe81e26ad0.zip\n</code></pre>\n<p>Later you can import the session with:</p>\n<pre class=\"lang-py prettyprint-override\"><code>from clearml import Task\nTask.import_offline_session('/home/user/.clearml/cache/offline/offline-2d061bb57d9e408a9420c4fe81e26ad0.zip')\n</code></pre>",
		"Answer_comment_count": 0.0,
		"Answer_creation_date": "2020-12-30 16:18:59.523000 UTC",
		"Answer_last_edit_date": "2022-08-24 07:33:21.413000 UTC",
		"Answer_score": 6.0,
		"Question_valid_tags": "clearml",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	}
]