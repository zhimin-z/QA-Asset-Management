[
	{
		"Question_id": 45039564,
		"Question_title": "How can I get all new or changed files?",
		"Question_body": "<p>I want to make a script that uses all new/changed files in some folder.<br>\nIntellij can tell changed (blue) and new (green) files from the unchanged ones. So, they must be foundable.  </p>\n\n<p>How can I distinct them in a Gradle task?</p>",
		"Question_answer_count": 0,
		"Question_comment_count": 6,
		"Question_creation_date": "2017-07-11 16:05:39.853000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": null,
		"Question_score": 0,
		"Question_tags": "intellij-idea|gradle|groovy|dvcs",
		"Question_view_count": 36,
		"Owner_creation_date": "2011-04-19 13:19:28.397000 UTC",
		"Owner_last_access_date": "2022-09-24 15:30:35.240000 UTC",
		"Owner_location": "Prague, Czech Republic",
		"Owner_reputation": 23445,
		"Owner_up_votes": 710,
		"Owner_down_votes": 52,
		"Owner_views": 3395,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "azure",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 70688649,
		"Question_title": "Where can I check the running cost of a Databricks SQL Endpoint?",
		"Question_body": "<p>Where can I check the running cost of a Databricks SQL Endpoint?</p>\n<p>I checked the Cost Analysis and there is an item called SQL database, is that it?</p>",
		"Question_answer_count": 0,
		"Question_comment_count": 0,
		"Question_creation_date": "2022-01-12 21:29:22.603000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": "2022-01-13 07:20:13.950000 UTC",
		"Question_score": 1,
		"Question_tags": "azure|databricks|azure-databricks|cost-management",
		"Question_view_count": 196,
		"Owner_creation_date": "2009-08-25 02:55:14.770000 UTC",
		"Owner_last_access_date": "2022-08-09 03:34:36.373000 UTC",
		"Owner_location": "Autonomous City of Buenos Aires, Argentina",
		"Owner_reputation": 5044,
		"Owner_up_votes": 171,
		"Owner_down_votes": 8,
		"Owner_views": 584,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "azure",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 60295830,
		"Question_title": "Jvm settings for Azure databricks",
		"Question_body": "<p>As part performance one suggestion in spark documentation is to make pointers 4 bytes instead of 8 as shown in the figure.<a href=\"https://i.stack.imgur.com/FwluO.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/FwluO.png\" alt=\"enter image description here\"></a></p>\n\n<p>I'm working on Azure databricks. Now where do I add this config?</p>\n\n<p>I tried adding in advanced options of a cluster under spark config, the following parameter:</p>\n\n<pre><code>jvm -XX:+UseCompressedOops \n</code></pre>\n\n<p>I'm I adding this config in right location? If not where should I add?</p>\n\n<p>Edit:\nDocument link\n<a href=\"https://spark.apache.org/docs/latest/tuning.html\" rel=\"nofollow noreferrer\">https://spark.apache.org/docs/latest/tuning.html</a></p>",
		"Question_answer_count": 1,
		"Question_comment_count": 3,
		"Question_creation_date": "2020-02-19 08:23:01.873000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": "2020-02-19 08:39:18.413000 UTC",
		"Question_score": 0,
		"Question_tags": "apache-spark|databricks|azure-databricks",
		"Question_view_count": 357,
		"Owner_creation_date": "2017-01-24 13:43:12.027000 UTC",
		"Owner_last_access_date": "2022-09-09 18:54:55.767000 UTC",
		"Owner_location": null,
		"Owner_reputation": 105,
		"Owner_up_votes": 7,
		"Owner_down_votes": 0,
		"Owner_views": 24,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "azure",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 71396863,
		"Question_title": "reading csv to dataframe with dynamic custom schema with pyspark",
		"Question_body": "<p>I'm working with databricks in a notebook. I want to read csv files with custom schema.\nI'd like to be able to loop over all the csv files in a folder and read them with their respective schema.</p>\n<p>So I have a schema for each csv file:</p>\n<pre><code>csv_1 = StructType([\n    StructField('foo', StringType(), False),\n    StructField('bar', StringType(), True),\n])\n\ncsv_2 = StructType([\n    StructField('foo', StringType(), False),\n    StructField('bar', StringType(), True),\n    StructField('baz', StringType(), True),\n])\n\ncsv_3 = StructType([\n    StructField('bar', StringType(), True)\n])\n</code></pre>\n<p>Then I have this loop:</p>\n<pre><code>for file in os.listdir(path):\n    filename = os.path.splitext(file)[0]\n    dataframes[filename] = spark.read.csv(path+file, header=True, schema=???)\n</code></pre>\n<p>I guess I probably need to use some mapping somewhere but I'm not sure how.</p>",
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_creation_date": "2022-03-08 14:44:24.327000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": null,
		"Question_score": 0,
		"Question_tags": "pyspark|databricks",
		"Question_view_count": 470,
		"Owner_creation_date": "2016-04-29 22:03:14.667000 UTC",
		"Owner_last_access_date": "2022-09-24 22:39:16.297000 UTC",
		"Owner_location": "Paris",
		"Owner_reputation": 2482,
		"Owner_up_votes": 452,
		"Owner_down_votes": 4,
		"Owner_views": 833,
		"Answer_body": "<pre class=\"lang-py prettyprint-override\"><code>filename_to_related_mapping = {\n  'name1': csv_1,\n  'name2': csv_2,\n ...\n}.get(filename)\n</code></pre>\n<pre><code>for file in os.listdir(path):\n    filename = os.path.splitext(file)[0]\n    dataframes[filename] = spark.read.csv(path+file, header=True, schema=filename_to_related_mapping[filename])\n</code></pre>\n<p>Anyway its just a CSV, another way is not pass the schema and it will be infered dynamically.</p>",
		"Answer_comment_count": 1.0,
		"Answer_creation_date": "2022-03-08 15:14:29.493000 UTC",
		"Answer_last_edit_date": null,
		"Answer_score": 1.0,
		"Question_valid_tags": "azure",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 35682879,
		"Question_title": "What is the name of the driver to connect to Azure SQL Database from pyodbc in Azure ML?",
		"Question_body": "<p>I'm trying to create a '<strong>Reader</strong>' alternative to read data from Azure SQL Database using the 'Execute python script' module in <strong>Azure ML</strong>.\nwhile doing so, I'm trying to connect to Azure Sql using pyodbc library.\nhere's my code:</p>\n\n<pre><code>def azureml_main(dataframe1 = None, dataframe2 = None):\n    import pyodbc   \n    import pandas as pd\n\n    conn = pyodbc.connect('DRIVER={SQL Server}; SERVER=server.database.windows.net; DATABASE=db_name; UID=user; PWD=Password')\n    SQLCommand = ('''select * from table1 ''')\n    data_frame = pd.read_sql(SQLCommand, conn)\n    return data_frame,\n</code></pre>\n\n<p>also tried to use a different driver name: {SQL Server Native Client 11.0}</p>\n\n<p>Here is the error i'm getting:</p>\n\n<pre><code>Error: ('IM002', '[IM002] [Microsoft][ODBC Driver Manager] Data source name not found and no default driver specified (0) (SQLDriverConnect)')\n</code></pre>\n\n<p>Does anybody know which driver should I use?</p>\n\n<p>just to make sure, I tried  \"{SQL Server}\", \"{SQL Server Native Client 11.0}\" and \"{SQL Server Native Client 10.0}\" and got the same error</p>\n\n<p>I also tried a different format: </p>\n\n<pre><code>conn = pyodbc.connect('DRIVER={SQL Server}; SERVER=server.database.windows.net; DATABASE=db_name; user=user@server; password=Password')\n</code></pre>\n\n<p>and </p>\n\n<pre><code>conn = pyodbc.connect('DRIVER={SQL Server Native Client 11.0}; SERVER=server.database.windows.net; DATABASE=db_name; user=user@server; password=Password')\n</code></pre>",
		"Question_answer_count": 5,
		"Question_comment_count": 1,
		"Question_creation_date": "2016-02-28 13:04:21.510000 UTC",
		"Question_favorite_count": 2.0,
		"Question_last_edit_date": "2016-03-01 06:14:10.913000 UTC",
		"Question_score": 6,
		"Question_tags": "python|pyodbc|azure-sql-database|azure-machine-learning-studio|cortana-intelligence",
		"Question_view_count": 2168,
		"Owner_creation_date": "2011-10-31 11:53:18.253000 UTC",
		"Owner_last_access_date": "2022-06-28 13:56:00.827000 UTC",
		"Owner_location": null,
		"Owner_reputation": 778,
		"Owner_up_votes": 176,
		"Owner_down_votes": 6,
		"Owner_views": 89,
		"Answer_body": "<p>I got an answer from azure support:</p>\n\n<blockquote>\n  <p>Currently it is not possible to access sql azure dbs from within  an\n  \u201cexecute python script\u201d module. As you suspected this is due to\n  missing odbc drivers in the execution environment.   Suggested\n  workarounds are to  a) use reader module   or   b) export to blobs\n  and use the Azure Python SDK for accessing those blobs\n  <a href=\"http://blogs.msdn.com/b/bigdatasupport/archive/2015/10/02/using-azure-sdk-for-python.aspx\" rel=\"nofollow\">http://blogs.msdn.com/b/bigdatasupport/archive/2015/10/02/using-azure-sdk-for-python.aspx</a></p>\n</blockquote>\n\n<p>So currently it it <strong>impossible</strong> to connect to SQL server from \u201cexecute python script\u201d module in Azure-ML. If you like to change it, please vote <a href=\"https://feedback.azure.com/forums/257792-machine-learning/suggestions/12589266-enable-odbc-connection-from-excute-python-script\" rel=\"nofollow\">here</a> </p>",
		"Answer_comment_count": 0.0,
		"Answer_creation_date": "2016-03-03 07:37:05.533000 UTC",
		"Answer_last_edit_date": "2016-03-03 11:01:52.110000 UTC",
		"Answer_score": 2.0,
		"Question_valid_tags": "azure",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 72399408,
		"Question_title": "How to get list of compute instance size under Azure Machine Learning and Azure Databricks?",
		"Question_body": "<p>Goal here is to query a list of frequently used compute instance size under Azure Machine Learning and Azure Databricks using Azure Resource Graph Explorer from Azure Portal using Kusto query. From the <a href=\"https://docs.microsoft.com/en-us/azure/governance/resource-graph/reference/supported-tables-resources#resources\" rel=\"nofollow noreferrer\">documentation</a> here, there is a list of resources can be queried but there isn't any compute under <code>microsoft.machinelearningservices/</code>(not classic studio) and <code>Microsoft.Databricks/workspaces</code>.</p>\n<p>Below is what was tried, to get VM instance size but not showing what we have under Azure Machine Learning/Azure Databricks.</p>\n<pre><code>Resources\n| project name, location, type, vmSize=tostring(properties.hardwareProfile.vmSize)\n| where type =~ 'Microsoft.Compute/virtualMachines'\n| order by name desc\n</code></pre>",
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_creation_date": "2022-05-27 00:55:49.060000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": null,
		"Question_score": 0,
		"Question_tags": "azure|size|azure-databricks|azure-machine-learning-service|azure-resource-graph",
		"Question_view_count": 153,
		"Owner_creation_date": "2019-09-11 07:07:53.007000 UTC",
		"Owner_last_access_date": "2022-09-15 16:01:06.153000 UTC",
		"Owner_location": "Kuala Lumpur, Federal Territory of Kuala Lumpur, Malaysia",
		"Owner_reputation": 383,
		"Owner_up_votes": 70,
		"Owner_down_votes": 1,
		"Owner_views": 35,
		"Answer_body": "<blockquote>\n<p>Unfortunately, Azure Resource Graph Explorer doesn't provide any query\nto get any compute related information from both, Azure Machine\nLearning and Databricks.</p>\n</blockquote>\n<p>Though Azure Resource Graph Explorer supports join functionality, allowing for more advanced exploration of your Azure environment by enabling you to correlate between resources and their properties. But these services only applicable on few Azure resources like VM, storage account, Cosmos DB, SQL databases, Network Security Groups, public IP addresses, etc.</p>\n<p><strong>Hence, there is no such Kusto query available in Azure Resource Graph Explorer which can list compute instance size of Machine Learning service and Databricks.</strong></p>\n<p><strong>Workarounds</strong></p>\n<p>Machine Learning Service</p>\n<p>For machine learning service you can manage the compute instance directly from ML service by using Python SDK. Refer <a href=\"https://docs.microsoft.com/en-us/azure/machine-learning/how-to-create-manage-compute-instance?tabs=python#manage\" rel=\"nofollow noreferrer\">Python SDK azureml v1</a> to know more.</p>\n<p>Azure Databricks</p>\n<p>Cluster is the computational resource in Databricks. You can <strong>filter the cluster list</strong> from Databricks UI and manage the same. Features like cluster configuration, cluster cloning, access control, etc. are available which you can used based on your requirement. For more details, please check <a href=\"https://docs.microsoft.com/en-us/azure/databricks/clusters/clusters-manage#filter-cluster-list\" rel=\"nofollow noreferrer\">here</a>.</p>",
		"Answer_comment_count": 0.0,
		"Answer_creation_date": "2022-05-27 04:42:21.543000 UTC",
		"Answer_last_edit_date": null,
		"Answer_score": 1.0,
		"Question_valid_tags": "azure",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 69656700,
		"Question_title": "How do I efficiently migrate MongoDB to azure CosmosDB with the help of azure Databricks?",
		"Question_body": "<p>While searching for a service to migrate our on-premise MongoDB to Azure CosmosDB with Mongo API, We came across the service called, Azure Data Bricks. We have total of 186GB of data. which we need to migrate to CosmosDB with less downtime as possible. How can we improve the data transfer rate for that. If someone can give some insights to this spark based PaaS provided by Azure, It will be very much helpful.\nThank you</p>",
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_creation_date": "2021-10-21 06:10:03.787000 UTC",
		"Question_favorite_count": 1.0,
		"Question_last_edit_date": "2021-10-21 06:21:43.943000 UTC",
		"Question_score": 1,
		"Question_tags": "mongodb|azure|azure-cosmosdb|database-migration|azure-databricks",
		"Question_view_count": 91,
		"Owner_creation_date": "2021-10-21 06:07:31.407000 UTC",
		"Owner_last_access_date": "2022-04-22 13:41:02.627000 UTC",
		"Owner_location": null,
		"Owner_reputation": 21,
		"Owner_up_votes": 3,
		"Owner_down_votes": 0,
		"Owner_views": 10,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "azure",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 73484457,
		"Question_title": "how to create schema of a delta table in databricks by using column names from text file",
		"Question_body": "<p>I have a text file which stores all the column names\n<strong>for ex:</strong> text file contains following data</p>\n<pre><code>table1=['column1','2_column2','3_column3']\ntable2=['column4','5_column5','6_column6']\n</code></pre>\n<p>I need to fetch all the values and create a schema for tables mentioned in the text file. and also some of the the columns starts with number as well as mentioned above.\n<strong>output needed:</strong></p>\n<pre><code>table1 = StructType([\n        StructField(&quot;column1&quot;, StringType(), True),\n        StructField(&quot;2_column2&quot;, StringType(), True),\n        StructField(&quot;3_column3&quot;, StringType(), True)\n    ]\n\ntable2 = StructType([\n        StructField(&quot;column4&quot;, StringType(), True),\n        StructField(&quot;5_column5&quot;, StringType(), True),\n        StructField(&quot;6_column6&quot;, StringType(), True)\n    ]\n</code></pre>\n<p>all the columns will be <strong>string type</strong>.</p>\n<p>how to achieve this using python/pyspark?</p>",
		"Question_answer_count": 1,
		"Question_comment_count": 1,
		"Question_creation_date": "2022-08-25 08:42:27.633000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": null,
		"Question_score": 0,
		"Question_tags": "python|pyspark|databricks",
		"Question_view_count": 52,
		"Owner_creation_date": "2021-09-23 10:28:35.667000 UTC",
		"Owner_last_access_date": "2022-09-22 06:00:35.183000 UTC",
		"Owner_location": null,
		"Owner_reputation": 31,
		"Owner_up_votes": 0,
		"Owner_down_votes": 0,
		"Owner_views": 26,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "azure",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 73447504,
		"Question_title": "Nested Python package structure and using it to create Databricks wheel task",
		"Question_body": "<p>Problem understanding python package structure and how to use it to trigger python wheel task in Databricks .</p>\n<p>So, it could either be something fundamental related to python packages/modules that I misunderstand or something specific to databricks. I have tried multiple options but none work.</p>\n<p>So,jumping in,<br />\nI would like to call <code>triggerjob function in createtables.py </code> using</p>\n<p><code>package_name: dbxdemo and entry_point jobs.createexternaltables.createtables.triggerjob</code></p>\n<p>I have also tried using</p>\n<p><code>package_name: dbxdemo.jobs.createexternaltables.createtables and entry_point: triggerjob</code></p>\n<p>my package structure is</p>\n<pre><code>dbxdemo\n |--jobs\n     |--createexttables\n            |---__init__.py\n            |---createtables.py\n     |--sample\n          |--__init__.py\n          |--entrypoint.py\n     |--__init__.py\n     |--common.py\n |--__init__.py\n</code></pre>\n<p>Then I updated my <strong>init</strong>.py files in the various subfolders as follows</p>\n<pre><code># dbxdemo/__init.py\nfrom . import jobs\n__all__=['jobs']\n__version__ = &quot;0.0.1&quot;\n\n# dbxdemo/jobs/__init__.py \nfrom . import createexternaltables\nfrom . import sample\n__all__=['createexternaltables', 'sample']\n\n# dbxdemo/jobs/createexternaltables/__init__.py\n\nfrom .createtables import *\n\n</code></pre>\n<p>The createtables.py file has this sample code</p>\n<pre><code>import logging\n#import dbxdemo.common\n\nfrom dbxdemo.common import Job\n\n\nclass CreateExternalTable(Job):\n\n    def launch(self):\n        try:\n            #do something\n        except Exception as e:\n            #do logging\n\n\ndef triggerjob(): #created this outside the class to see if that helps, but no (ideally would want this to be part of the class_\n    job = CreateExternalTable()\n    job.launch()\n\n</code></pre>\n<p>When I try to create a databricks python wheel task and provide the package name as</p>\n<pre><code>dbxdemo\n</code></pre>\n<p>and entry_point as</p>\n<pre><code>jobs.createexternaltables.createtables.triggerjob\n</code></pre>\n<p>I keep getting an error that</p>\n<pre><code>module 'dbxdemo' has no attribute 'jobs'\n</code></pre>\n<p>I have also gone through other S.O posts and tried various combinations.</p>\n<p>I have also tried putting the package name as <code>dbxdemo.jobs.createexternaltables.createtables</code> and entry_point as <code>triggerjob</code> but even that does not work</p>\n<p>In addition , I have also tried changing setup.py (look at the comment)</p>\n<pre><code>from setuptools import find_packages, setup\nfrom dbxdemo import __version__\n\nsetup(\n    name=&quot;dbxdemo.jobs.createexternaltables.createtables&quot;, #earlier also tried with dbxdemo\n    packages=find_packages(exclude=[&quot;tests&quot;, &quot;tests.*&quot;]),\n    setup_requires=[&quot;wheel&quot;],\n    version=__version__,\n    description=&quot;&quot;,\n    author=&quot;&quot;\n)\n\n</code></pre>\n<p>P.S: If the problem is databricks specific then this is the dbx documentation I have been following <a href=\"https://dbx.readthedocs.io/en/latest/README.html\" rel=\"nofollow noreferrer\">here</a></p>\n<p>I have a feeling this is probably databricks related as I can install the library manually and call this successfully</p>\n<pre><code>import dbxdemo\n\ndbxdemo.jobs.createexternaltables.createtables.triggerjob()\n</code></pre>",
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_creation_date": "2022-08-22 15:23:14.887000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": null,
		"Question_score": 0,
		"Question_tags": "python|databricks|python-packaging|databricks-dbx",
		"Question_view_count": 67,
		"Owner_creation_date": "2015-04-10 08:31:54.763000 UTC",
		"Owner_last_access_date": "2022-09-24 09:37:37.383000 UTC",
		"Owner_location": null,
		"Owner_reputation": 596,
		"Owner_up_votes": 53,
		"Owner_down_votes": 1,
		"Owner_views": 80,
		"Answer_body": "<p>Found the answer. Few things</p>\n<ol>\n<li>The import in the various <strong>init</strong>.py files didn't matter. So, I removed them.</li>\n<li>dbx execute needs a mandatory &quot;parameters&quot; element in deployment.yaml (this helped me get to the actual error of not being able to find the entrypoint.)</li>\n<li>The setup.py file modified to add the entrypoint (I looked at various answers like <a href=\"https://stackoverflow.com/questions/71345849/python-wheel-job-module-not-found\">this one</a> and also <a href=\"https://community.databricks.com/s/question/0D53f00001jxOsgCAE/databricks-job-package-name-and-entrypoint-parameters-for-the-python-wheel-file\" rel=\"nofollow noreferrer\">this one</a>, but none actually explained the reason why. Finally <a href=\"https://stackoverflow.com/questions/774824/explain-python-entry-points\">this SO accepted answer</a> explained entrypoints very lucidly)</li>\n</ol>\n<pre><code>from setuptools import find_packages, setup\nfrom dbxdemo import __version__\n\nsetup(\n    name=&quot;dbxdemo&quot;,\n    packages=find_packages(exclude=[&quot;tests&quot;, &quot;tests.*&quot;]),\n    setup_requires=[&quot;wheel&quot;],\n    version=__version__,\n    description=&quot;&quot;,\n    author=&quot;&quot;,\n    entry_points={\n    'console_scripts': [\n        'triggerjob = dbxdemo.jobs.createexternaltables.createtables:triggerjob',\n    ],\n}\n\n)\n</code></pre>\n<p>Once this was done, the package_name can be set to <code>dbxdemo</code> and the entry_point as <code>triggerjob</code> when creating a Databricks python wheel task.</p>\n<p>P.S: for anyone interested in doing through dbx, your deployment.yaml should be</p>\n<pre><code>- name: &quot;dbxdemowhl&quot;\n        &lt;&lt;:\n          - *basic-static-cluster\n        python_wheel_task:\n          package_name: &quot;dbxdemo&quot;\n          entry_point: triggerjob\n          parameters: []  # This must be passed even if empty as dbx execute would error out\n</code></pre>",
		"Answer_comment_count": 0.0,
		"Answer_creation_date": "2022-08-22 21:09:38.967000 UTC",
		"Answer_last_edit_date": null,
		"Answer_score": 0.0,
		"Question_valid_tags": "azure",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 68301665,
		"Question_title": "Ways to support dynamic class loading in spark executor",
		"Question_body": "<p>I have a requirement to load some custom jars on executors depending upon the tasks they will be performing and these jars will not be available on classpath (We have our valid reasons for this).</p>\n<p><strong>Current solution:</strong>\nWe are able to solve this to some extent by providing wrapper around spark functions like map/flatMap , We are achieving this by intercepting Function class &quot;call&quot; method and creating classloader with our new jars. Jars that we need to load are shipped via SparkFiles.</p>\n<p><strong>Challenge</strong>: We are stuck where we need to support all other spark API methods that doesn't support or have any functional interface for example reading and writing methods.</p>\n<p><strong>Idea</strong>: One thought was to use AOP and intercept <code>org.apache.spark.scheduler.ResultTask.runTask(..))</code> on the executor side but couldn't get this working.</p>\n<p>Any help would be appreciated.</p>",
		"Question_answer_count": 0,
		"Question_comment_count": 0,
		"Question_creation_date": "2021-07-08 12:29:44.417000 UTC",
		"Question_favorite_count": 1.0,
		"Question_last_edit_date": null,
		"Question_score": 3,
		"Question_tags": "java|apache-spark|databricks",
		"Question_view_count": 163,
		"Owner_creation_date": "2011-02-28 12:40:07.080000 UTC",
		"Owner_last_access_date": "2022-09-24 17:24:35.947000 UTC",
		"Owner_location": "India",
		"Owner_reputation": 12737,
		"Owner_up_votes": 292,
		"Owner_down_votes": 30,
		"Owner_views": 1221,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "azure",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 73783771,
		"Question_title": "Read delta table in spark with NOT NULL constraint",
		"Question_body": "<p>I am reading a delta table like this</p>\n<pre class=\"lang-py prettyprint-override\"><code>df = (spark\n  .read\n  .format(&quot;delta&quot;)\n  .load(&quot;/path/to/foo/table&quot;)\n  .select(&quot;Foo_ID&quot;, &quot;Bar&quot;)\n)\n</code></pre>\n<p>now when I look at the DataFrame schema, spark has somehow incorrectly inferred nullability:</p>\n<pre><code>&gt;&gt;&gt; df.schema\n...\nStructType([StructField('Foo_ID', LongType(), True), StructField('Bar', TimestampType(), True)])\n</code></pre>\n<p>I know the values should not be null and I want the schema to reflect that by imposing a not null constraint on the columns. In case I try to load a table that actually does contain a null I want to see a big fat error message, rather than silently switching. Is there a clean way to do this? I came up with the following workaround, but it doesn't feel very nice:</p>\n<pre><code>my_schema = StructType([StructField('Foo_ID', LongType(), False), StructField('Bar', TimestampType(), False)])\ndf2 = spark.createDataFrame(df.rdd, schema=my_schema)\n</code></pre>\n<p>Is there a better way?</p>",
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_creation_date": "2022-09-20 08:34:04.470000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": null,
		"Question_score": 1,
		"Question_tags": "dataframe|apache-spark|schema|databricks|delta-lake",
		"Question_view_count": 21,
		"Owner_creation_date": "2022-04-04 09:08:22.570000 UTC",
		"Owner_last_access_date": "2022-09-23 06:36:53.707000 UTC",
		"Owner_location": null,
		"Owner_reputation": 77,
		"Owner_up_votes": 8,
		"Owner_down_votes": 0,
		"Owner_views": 1,
		"Answer_body": "<p>If the table was created with the constraint, then Spark will infer nullability:</p>\n<pre><code>CREATE OR REPLACE TABLE delta.`/tmp/delta/table` (\n  foo_id LONG NOT NULL,\n  bar TIMESTAMP\n) USING DELTA;\n\nval df = spark.read.load(&quot;/tmp/delta/table&quot;)\ndf.printSchema\n\nroot\n |-- foo_id: long (nullable = false)\n |-- bar: timestamp (nullable = true)\n</code></pre>\n<p>Apparently it wasn't. But you can always change it:</p>\n<pre><code>CREATE OR REPLACE TABLE delta.`/tmp/delta/table` (\n  foo_id LONG,\n  bar TIMESTAMP\n) USING DELTA;\n\nALTER TABLE delta.`/tmp/delta/table` CHANGE COLUMN foo_id foo_id LONG NOT NULL;\nALTER TABLE delta.`/tmp/delta/table` CHANGE COLUMN bar bar TIMESTAMP NOT NULL;\n\nval df = spark.read.load(&quot;/tmp/delta/table&quot;)\ndf.printSchema\n\nroot\n |-- foo_id: long (nullable = false)\n |-- bar: timestamp (nullable = false)\n</code></pre>\n<p>Or, if you don't want or cannot change it, then what you came up with is not a workaround, it is a solution. You just explicitly specify a schema you want to read data with.</p>",
		"Answer_comment_count": 1.0,
		"Answer_creation_date": "2022-09-20 10:19:10.370000 UTC",
		"Answer_last_edit_date": null,
		"Answer_score": 1.0,
		"Question_valid_tags": "azure",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 26845,
		"Question_title": "Do you use distributed version control?",
		"Question_body": "<p>I'd like to hear from people who are using distributed version control (aka distributed revision control, decentralized version control) and how they are finding it. What are you using, Mercurial, Darcs, Git, Bazaar? Are you still using it? If you've used client/server rcs in the past, are you finding it better, worse or just different? What could you tell me that would get me to jump on the bandwagon? Or jump off for that matter, I'd be interested to hear from people with negative experiences as well. </p>\n\n<p>I'm currently looking at replacing our current source control system (Subversion) which is the impetus for this question.</p>\n\n<p>I'd be especially interested in anyone who's used it with co-workers in other countries, where your machines may not be on at the same time, and your connection is very slow.</p>\n\n<p>If you're not sure what distributed version control is, here are a couple articles:</p>\n\n<p><a href=\"http://betterexplained.com/articles/intro-to-distributed-version-control-illustrated/\" rel=\"nofollow noreferrer\">Intro to Distributed Version Control</a></p>\n\n<p><a href=\"http://en.wikipedia.org/wiki/Distributed_revision_control\" rel=\"nofollow noreferrer\">Wikipedia Entry</a></p>",
		"Question_answer_count": 18,
		"Question_comment_count": 0,
		"Question_creation_date": "2008-08-25 20:46:10.427000 UTC",
		"Question_favorite_count": 10.0,
		"Question_last_edit_date": "2009-04-10 16:30:58.597000 UTC",
		"Question_score": 37,
		"Question_tags": "version-control|dvcs|revision",
		"Question_view_count": 2921,
		"Owner_creation_date": "2008-08-25 03:16:36.083000 UTC",
		"Owner_last_access_date": "2022-09-23 22:18:22.227000 UTC",
		"Owner_location": "Snohomish, WA",
		"Owner_reputation": 9159,
		"Owner_up_votes": 148,
		"Owner_down_votes": 10,
		"Owner_views": 348,
		"Answer_body": "<p>I've been using Mercurial both at work and in my own personal projects, and I am really happy with it.  The advantages I see are:</p>\n\n<ol>\n<li><strong>Local version control.</strong> Sometimes I'm working on something, and I want to keep a version history on it, but I'm not ready to push it to the central repositories.  With distributed VCS, I can just commit to my local repo until it's ready, without branching.  That way, if other people make changes that I need, I can still get them and integrate them into my code.  When I'm ready, I push it out to the servers.</li>\n<li><strong>Fewer merge conflicts.</strong> They still happen, but they seem to be less frequent, and are less of a risk, because all the code is checked in to my local repo, so even if I botch the merge, I can always back up and do it again.</li>\n<li><strong>Separate repos as branches.</strong> If I have a couple development vectors running at the same time, I can just make several clones of my repo and develop each feature independently.  That way, if something gets scrapped or slipped, I don't have to pull pieces out.  When they're ready to go, I just merge them together.</li>\n<li><strong>Speed.</strong> Mercurial is much faster to work with, mostly because most of your common operations are local.</li>\n</ol>\n\n<p>Of course, like any new system, there was some pain during the transition.  You have to think about version control differently than you did when you were using SVN, but overall I think it's very much worth it.</p>",
		"Answer_comment_count": 3.0,
		"Answer_creation_date": "2008-08-25 22:16:04.377000 UTC",
		"Answer_last_edit_date": "2009-04-10 16:05:34.553000 UTC",
		"Answer_score": 30.0,
		"Question_valid_tags": "azure",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 56752408,
		"Question_title": "Spark 2.4 CSV Load Issue with option \"nullvalue\"",
		"Question_body": "<p>We were using Spark 2.3 before, now we're on 2.4:</p>\n\n<pre><code>Spark version 2.4.0\nUsing Scala version 2.11.12 (OpenJDK 64-Bit Server VM, Java 1.8.0_212)\n</code></pre>\n\n<p>We had a piece of code running in production that converted csv files to parquet format.\nOne of the options we had set csv load is option(\"nullValue\", null). There's something wrong with how it works in spark 2.4.</p>\n\n<p>Here's an example to show the issue.</p>\n\n<ol>\n<li>let's create the following /tmp/test.csv file:</li>\n</ol>\n\n<pre><code>C0,C1,C2,C3,C4,C5\n1,\"1234\",0.00,\"\",\"D\",0.00\n2,\"\",0.00,\"\",\"D\",0.00\n\n</code></pre>\n\n<ol start=\"2\">\n<li>Now if we load it in spark-shell</li>\n</ol>\n\n<pre><code>scala&gt; val data1 = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"treatEmptyValuesAsNulls\",\"true\").option(\"nullValue\", null).csv(\"file:///tmp/test.csv\")\n\nwe get an empty row:\nscala&gt; data1.show\n+----+----+----+----+----+----+\n| C0| C1| C2| C3| C4| C5|\n+----+----+----+----+----+----+\n| 1|1234| 0.0| | D| 0.0|\n|null|null|null|null|null|null|\n+----+----+----+----+----+----+\n\n</code></pre>\n\n<ol start=\"3\">\n<li>If we additionally change the csv a little (replaced empty string with \"1\" in the last row)</li>\n</ol>\n\n<pre><code>C0,C1,C2,C3,C4,C5\n1,\"1234\",0.00,\"\",\"D\",0.00\n2,\"\",0.00,\"1\",\"D\",0.00\n</code></pre>\n\n<p>the result is even worse:</p>\n\n<pre><code>scala&gt; val data2 = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"treatEmptyValuesAsNulls\",\"true\").option(\"nullValue\", null).csv(\"file:///tmp/test.csv\")\n\nscala&gt; data2.show\n+----+----+----+----+----+----+\n| C0| C1| C2| C3| C4| C5|\n+----+----+----+----+----+----+\n|null|null|null|null|null|null|\n|null|null|null|null|null|null|\n+----+----+----+----+----+----+\n\n</code></pre>\n\n<p>Is this a bug in new version of spark 2.4.0 ? Any body faced similar issue ? </p>",
		"Question_answer_count": 1,
		"Question_comment_count": 1,
		"Question_creation_date": "2019-06-25 10:52:43.213000 UTC",
		"Question_favorite_count": 1.0,
		"Question_last_edit_date": "2019-06-25 12:45:43.740000 UTC",
		"Question_score": 2,
		"Question_tags": "scala|apache-spark|databricks|spark-csv",
		"Question_view_count": 9409,
		"Owner_creation_date": "2016-08-26 09:52:06.147000 UTC",
		"Owner_last_access_date": "2022-08-12 08:54:58.170000 UTC",
		"Owner_location": "Netherlands",
		"Owner_reputation": 333,
		"Owner_up_votes": 6,
		"Owner_down_votes": 0,
		"Owner_views": 22,
		"Answer_body": "<p>spark option <strong><em>emptyValue</em></strong> solved issue</p>\n\n<pre><code>val data2 = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"treatEmptyValuesAsNulls\",\"true\").option(\"nullValue\", null)***.option(\"emptyValue\", null)***.csv(\"file:///tmp/test.csv\")\n</code></pre>",
		"Answer_comment_count": 0.0,
		"Answer_creation_date": "2019-08-15 14:57:55.183000 UTC",
		"Answer_last_edit_date": null,
		"Answer_score": 3.0,
		"Question_valid_tags": "azure",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 55279329,
		"Question_title": "How to add dynamic connection string in Azure Data Factory",
		"Question_body": "<p>Can someone help me on achieving dynamic connections to databases using azure data factory?</p>",
		"Question_answer_count": 1,
		"Question_comment_count": 1,
		"Question_creation_date": "2019-03-21 11:21:54.907000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": null,
		"Question_score": -1,
		"Question_tags": "azure|azure-sql-database|azure-cosmosdb|azure-data-factory|azure-databricks",
		"Question_view_count": 1673,
		"Owner_creation_date": "2018-08-01 14:58:33.147000 UTC",
		"Owner_last_access_date": "2019-09-20 09:43:02.913000 UTC",
		"Owner_location": null,
		"Owner_reputation": 111,
		"Owner_up_votes": 0,
		"Owner_down_votes": 0,
		"Owner_views": 11,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "azure",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 66703165,
		"Question_title": "check if folder exists in blob storage and add to a list",
		"Question_body": "<p>I am creating a function in Databricks to check if folder exists in Azure Blob Storage:</p>\n<pre><code>import java.io.File\n\ndef checkFolder(paths: List[String]): Unit = {\n  \n  for (f &lt;- paths) {\n    try\n      {   \n        var pathCheck = dbutils.fs.ls(f) \n       }\n  catch {\n    case ex: Exception =&gt; {\n      println(&quot;Folder does not exists: &quot; + f)\n      }\n    }\n  }\n}\n</code></pre>\n<p>I want to add the folders that exists in  list so that I can read the data in those paths! I also want to add the path that do not exist in a different list.</p>\n<p>How would I do that?</p>",
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_creation_date": "2021-03-19 05:53:52.103000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": null,
		"Question_score": 1,
		"Question_tags": "scala|apache-spark|databricks|azure-blob-storage",
		"Question_view_count": 587,
		"Owner_creation_date": "2016-06-06 21:16:28.527000 UTC",
		"Owner_last_access_date": "2022-08-30 04:34:58.997000 UTC",
		"Owner_location": null,
		"Owner_reputation": 153,
		"Owner_up_votes": 18,
		"Owner_down_votes": 0,
		"Owner_views": 42,
		"Answer_body": "<p>Try something like this:</p>\n<pre class=\"lang-scala prettyprint-override\"><code>val (exist, doesnt) = paths.map {\n  f =&gt; try {\n    dbutils.fs.ls(f)\n    (f, true)\n  } catch {\n    case _: Exception =&gt; \n      (f, false)\n  }\n}.partition(_._2)\n</code></pre>\n<p>This will give you two sequences of type <code>(String, Boolean)</code> for existing &amp; not existing files, then you can extract file paths from it using:</p>\n<pre class=\"lang-scala prettyprint-override\"><code>exist.map(_._1)\ndoesnt.map(_._1)\n</code></pre>",
		"Answer_comment_count": 3.0,
		"Answer_creation_date": "2021-03-19 07:28:52.727000 UTC",
		"Answer_last_edit_date": null,
		"Answer_score": 2.0,
		"Question_valid_tags": "azure",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 67153253,
		"Question_title": "Display <IPython.core.display.HTML object> in Spark",
		"Question_body": "<p>I am trying to display Spacy Dependency Tree in Pyspark (Databricks). But I am getting this error.\nCould someone help me with the display of html object please.\nI am attaching screenshot of the Error message. Thanks in advance.\nHere is my code:</p>\n<pre><code>import spacy\nfrom spacy import displacy\n\ndoc = nlp(&quot;Reliance is looking at buying U.K. based analytics startup for $7 billion&quot;)\ndisplacy.render(doc, style=&quot;dep&quot; , jupyter=True)\n</code></pre>\n<p>Error message:\n<a href=\"https://i.stack.imgur.com/OI6Zh.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/OI6Zh.png\" alt=\"enter image description here\" /></a></p>",
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_creation_date": "2021-04-18 20:49:17.943000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": null,
		"Question_score": 3,
		"Question_tags": "apache-spark|pyspark|ipython|databricks",
		"Question_view_count": 2133,
		"Owner_creation_date": "2016-03-08 07:52:38.363000 UTC",
		"Owner_last_access_date": "2022-09-23 10:20:54.710000 UTC",
		"Owner_location": null,
		"Owner_reputation": 107,
		"Owner_up_votes": 98,
		"Owner_down_votes": 0,
		"Owner_views": 12,
		"Answer_body": "<p>This won't work in the Databricks notebooks, because it's not an Jupyter.  But you can use <a href=\"https://docs.databricks.com/notebooks/visualizations/html-d3-and-svg.html\" rel=\"nofollow noreferrer\">displayHTML function</a> together with rendering as <a href=\"https://spacy.io/usage/visualizers#html\" rel=\"nofollow noreferrer\">HTML</a>:</p>\n<pre class=\"lang-py prettyprint-override\"><code>displayHTML(displacy.render(doc, style=&quot;dep&quot;, page=True))\n</code></pre>\n<p>Or even display SVG data directly:</p>\n<pre><code>displayHTML(displacy.render(doc, style=&quot;dep&quot;))\n</code></pre>\n<p>here is result:</p>\n<p><a href=\"https://i.stack.imgur.com/gMmEr.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/gMmEr.png\" alt=\"enter image description here\" /></a></p>",
		"Answer_comment_count": 1.0,
		"Answer_creation_date": "2021-04-19 06:03:35.617000 UTC",
		"Answer_last_edit_date": null,
		"Answer_score": 3.0,
		"Question_valid_tags": "azure",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 63182088,
		"Question_title": "Optimizing SQL table creation from a Pyspark dataframe in databricks",
		"Question_body": "<p><strong>Spark DF:</strong> <code>jrny_df1.createOrReplaceTempView(&quot;journeymap_drvs1&quot;)</code>\napprox: 10MM records</p>\n<p><strong>Creating a sql table of this view takes a long time:</strong></p>\n<pre><code>create table temp.ms_journey_drvsv1 as select * from journeymap_drvs1;\n</code></pre>\n<p>Is there any process that I can follow to optimize the speed of the table creation. We Spark 2.4, 88 cores, 671 GB memory</p>",
		"Question_answer_count": 1,
		"Question_comment_count": 1,
		"Question_creation_date": "2020-07-30 22:08:49.337000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": null,
		"Question_score": 0,
		"Question_tags": "pyspark|apache-spark-sql|databricks",
		"Question_view_count": 109,
		"Owner_creation_date": "2019-07-19 14:51:13.863000 UTC",
		"Owner_last_access_date": "2022-03-23 20:23:18.597000 UTC",
		"Owner_location": null,
		"Owner_reputation": 109,
		"Owner_up_votes": 26,
		"Owner_down_votes": 0,
		"Owner_views": 21,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "azure",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 61760132,
		"Question_title": "Fastest Way to Calculate Minimum Haversine Distance between Lat Long & an array of Lat longs in PySpark?",
		"Question_body": "<p><strong>Context</strong>: I am looking for a way to efficiently calculate, in PySpark, the distance between a pair of lat longs and an array of lat longs and then taking a minimum of those distances. </p>\n\n<h1>How this would work:</h1>\n\n<ul>\n<li>Step One: I have a <em>Spark</em> Dataframe containing restaurant ids with latitude &amp; longitude as columns</li>\n</ul>\n\n<pre><code># Something like this\n\n&gt;&gt;&gt; restaurants_df\nrestaurant id | lat   | long \n123           | 32.34 | 54.62\n</code></pre>\n\n<ul>\n<li>Step Two: I have a <em>Pandas</em> data frame consisting of gas stations   </li>\n</ul>\n\n<pre><code>&gt;&gt;&gt; gas_stations_df\ngas_station id | lat   | long \n456            | 76.22 | 64.24\n789            | 24.65 | 35.55\n</code></pre>\n\n<ul>\n<li>Step Three:\nI now want to calculate the haversine distance between each restaurant and ALL the gas station locations and then get the minimum distance! So let's say:\n\n<ul>\n<li>Haversine Distance b/w restaurant id <strong>123</strong> and gas station <strong>456</strong> = 5m</li>\n<li>Haversine Distance b/w restaurant id <strong>123</strong> and gas station <strong>789</strong> = 12m</li>\n</ul></li>\n</ul>\n\n<p>Then I want to return 5m as the value since its the lowest distance. I would like to do this for ALL restaurant ids. Some sudo code to better understand this issue:</p>\n\n<pre><code># Sudo code to understand desired logic\nfor each_restaurant in a list of restaurants:\n    calculate the distance between the restaurant and ALL the gas stations\n    return minimum distance\n</code></pre>\n\n<h1>PROGRESS SO FAR</h1>\n\n<p>So far I have employed the use of Vectorized Pandas UDFs and normal UDFs as follows</p>\n\n<pre><code>def haversine_distance(lat, long):\n    \"\"\"Get haversine distances from a single (lat, long) pair to an array\n    of (lat, long) pairs.\n    \"\"\"\n    # Convert the lat long to radians\n    lat = lat.apply(lambda x: radians(x))\n    long = long.apply(lambda x: radians(x))\n\n    unit = 'm'\n    single_loc = pd.DataFrame( [lat,  long] ).T\n    single_loc.columns = ['Latitude', 'Longitude']\n\n    other_locs = gas_stations_df[['Latitude', 'Longitude']].values  # this is a pandas dataframe\n\n    dist_l = []\n    for index,row in single_loc.iterrows():\n        .... do haversine distance calculations\n        d = haversine distance\n\n\n        dist_l.append(np.min(d) )\n\n    return pd.Series(dist_l)\n</code></pre>\n\n<p>Then I apply the Pandas UDF as follows:</p>\n\n<pre><code>restaurant_df = restaurant_df.withColumn('distance_to_nearest_gas_station', lit(haversine_distance('latitude', 'longitude')))\n</code></pre>\n\n<p>Although this approach works, it's still rather slow to scale and I was wondering if there is a simpler way to do this? </p>\n\n<p>Thank you very much for reading!</p>",
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_creation_date": "2020-05-12 19:15:54.883000 UTC",
		"Question_favorite_count": 1.0,
		"Question_last_edit_date": null,
		"Question_score": 0,
		"Question_tags": "python|pyspark|apache-spark-sql|user-defined-functions|databricks",
		"Question_view_count": 520,
		"Owner_creation_date": "2017-12-15 05:10:47.373000 UTC",
		"Owner_last_access_date": "2022-09-24 19:13:13.300000 UTC",
		"Owner_location": null,
		"Owner_reputation": 486,
		"Owner_up_votes": 900,
		"Owner_down_votes": 2,
		"Owner_views": 47,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "azure",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 57566527,
		"Question_title": "Databricks: Job having high shuffle write and executing very long",
		"Question_body": "<p>I am having trouble in running a databricks notebook ( scala) , And I see the job is having high write shuffle size. and it already run over an hour. Let's have a look on the following screen\n<a href=\"https://i.stack.imgur.com/HCMw2.jpg\" rel=\"nofollow noreferrer\">enter image description here</a></p>\n\n<p>Any idea on checking how why ?</p>\n\n<p>shuffle write: 35.5GB/ 1796240509 \nwhat's the meaning of 35.5GB and 1796240509 ??</p>",
		"Question_answer_count": 0,
		"Question_comment_count": 2,
		"Question_creation_date": "2019-08-20 03:58:14.417000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": null,
		"Question_score": 0,
		"Question_tags": "databricks|fpgrowth",
		"Question_view_count": 75,
		"Owner_creation_date": "2019-05-03 07:43:53.913000 UTC",
		"Owner_last_access_date": "2022-09-23 02:21:26.333000 UTC",
		"Owner_location": null,
		"Owner_reputation": 479,
		"Owner_up_votes": 0,
		"Owner_down_votes": 0,
		"Owner_views": 108,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "azure",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 69261285,
		"Question_title": "Scala Pass window partition dataset to UDF",
		"Question_body": "<p>I have a dataframe like below,</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>Id1</th>\n<th>Id2</th>\n<th>Id3</th>\n<th>TaskId</th>\n<th>TaskName</th>\n<th>index</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>11</td>\n<td>bc123-234</td>\n<td>dfr3ws-45d</td>\n<td>randomName1</td>\n<td>1</td>\n</tr>\n<tr>\n<td>1</td>\n<td>11</td>\n<td>bc123-234</td>\n<td>er98d3-lkj</td>\n<td>randomName2</td>\n<td>2</td>\n</tr>\n<tr>\n<td>1</td>\n<td>11</td>\n<td>bc123-234</td>\n<td>hu77d9-mnb</td>\n<td>randomName3</td>\n<td>3</td>\n</tr>\n<tr>\n<td>1</td>\n<td>11</td>\n<td>bc123-234</td>\n<td>xc33d5-rew</td>\n<td>deployhere4</td>\n<td>4</td>\n</tr>\n<tr>\n<td>1</td>\n<td>11</td>\n<td>xre43-876</td>\n<td>dfr3ws-45d</td>\n<td>randomName1</td>\n<td>1</td>\n</tr>\n<tr>\n<td>1</td>\n<td>11</td>\n<td>xre43-876</td>\n<td>er98d3-lkj</td>\n<td>deployhere2</td>\n<td>2</td>\n</tr>\n<tr>\n<td>1</td>\n<td>11</td>\n<td>xre43-876</td>\n<td>hu77d9-mnb</td>\n<td>randomName3</td>\n<td>3</td>\n</tr>\n<tr>\n<td>1</td>\n<td>11</td>\n<td>xre43-876</td>\n<td>xc33d5-rew</td>\n<td>randomName4</td>\n<td>4</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>I partitioned the data using Id3 and Id2 and added the row_number.</p>\n<p>I need to perform the below condition:</p>\n<p>TaskId &quot;hu77d9-mnb&quot; should come before the task name which contains deploy in it. As the table suggests above the name will be random I need to read each name in the partition and see which name contains deploy in it.</p>\n<p>if deploy taskName index is greater than taskID index then I mark the value as 1 otherwise 0.</p>\n<p>I need to get final table like this:</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>Id1</th>\n<th>Id2</th>\n<th>Id3</th>\n<th>TaskId</th>\n<th>TaskName</th>\n<th>index</th>\n<th>result</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>11</td>\n<td>bc123-234</td>\n<td>dfr3ws-45d</td>\n<td>randomName1</td>\n<td>1</td>\n<td>1</td>\n</tr>\n<tr>\n<td>1</td>\n<td>11</td>\n<td>bc123-234</td>\n<td>er98d3-lkj</td>\n<td>randomName2</td>\n<td>2</td>\n<td>1</td>\n</tr>\n<tr>\n<td>1</td>\n<td>11</td>\n<td>bc123-234</td>\n<td>hu77d9-mnb</td>\n<td>randomName3</td>\n<td>3</td>\n<td>1</td>\n</tr>\n<tr>\n<td>1</td>\n<td>11</td>\n<td>bc123-234</td>\n<td>xc33d5-rew</td>\n<td>deployhere4</td>\n<td>4</td>\n<td>1</td>\n</tr>\n<tr>\n<td>1</td>\n<td>11</td>\n<td>xre43-876</td>\n<td>dfr3ws-45d</td>\n<td>randomName1</td>\n<td>1</td>\n<td>0</td>\n</tr>\n<tr>\n<td>1</td>\n<td>11</td>\n<td>xre43-876</td>\n<td>er98d3-lkj</td>\n<td>deployhere2</td>\n<td>2</td>\n<td>0</td>\n</tr>\n<tr>\n<td>1</td>\n<td>11</td>\n<td>xre43-876</td>\n<td>hu77d9-mnb</td>\n<td>randomName3</td>\n<td>3</td>\n<td>0</td>\n</tr>\n<tr>\n<td>1</td>\n<td>11</td>\n<td>xre43-876</td>\n<td>xc33d5-rew</td>\n<td>randomName4</td>\n<td>4</td>\n<td>0</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>I am stuck at this place how can I pass the partition data to UDF (or other functions like UDAF) and perform this task. Any suggestion will be helpful. Thank you for your time.</p>",
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_creation_date": "2021-09-20 21:58:28.427000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": "2021-09-21 08:45:39.517000 UTC",
		"Question_score": 0,
		"Question_tags": "scala|apache-spark|user-defined-functions|azure-databricks",
		"Question_view_count": 99,
		"Owner_creation_date": "2015-05-16 10:43:57.653000 UTC",
		"Owner_last_access_date": "2022-09-22 09:47:47.203000 UTC",
		"Owner_location": null,
		"Owner_reputation": 17,
		"Owner_up_votes": 1,
		"Owner_down_votes": 0,
		"Owner_views": 25,
		"Answer_body": "<p>Index of &quot;deploy&quot; row and index of specific row (&quot;hu77d9-mnb&quot;) can be assigned to each row with Window &quot;first&quot; function, and then just compared:</p>\n<pre><code>val df = Seq(\n  (1, 11, &quot;bc123-234&quot;, &quot;dfr3ws-45d&quot;, &quot;randomName1&quot;, 1),\n  (1, 11, &quot;bc123-234&quot;, &quot;er98d3-lkj&quot;, &quot;randomName2&quot;, 2),\n  (1, 11, &quot;bc123-234&quot;, &quot;hu77d9-mnb&quot;, &quot;randomName3&quot;, 3),\n  (1, 11, &quot;bc123-234&quot;, &quot;xc33d5-rew&quot;, &quot;deployhere4&quot;, 4),\n  (1, 11, &quot;xre43-876&quot;, &quot;dfr3ws-45d&quot;, &quot;randomName1&quot;, 1),\n  (1, 11, &quot;xre43-876&quot;, &quot;er98d3-lkj&quot;, &quot;deployhere2&quot;, 2),\n  (1, 11, &quot;xre43-876&quot;, &quot;hu77d9-mnb&quot;, &quot;randomName3&quot;, 3),\n  (1, 11, &quot;xre43-876&quot;, &quot;xc33d5-rew&quot;, &quot;randomName4&quot;, 4)\n).toDF(&quot;Id1&quot;, &quot;Id2&quot;, &quot;Id3&quot;, &quot;TaskID&quot;, &quot;TaskName&quot;, &quot;index&quot;)\n\nval specificTaskId = &quot;hu77d9-mnb&quot;\nval idsWindow = Window.partitionBy(&quot;Id1&quot;, &quot;Id2&quot;, &quot;Id3&quot;)\n\ndf.withColumn(&quot;deployIndex&quot;,\n  first(\n    when(instr($&quot;TaskName&quot;, &quot;deploy&quot;) &gt; 0, $&quot;index&quot;).otherwise(null),\n    true)\n    .over(idsWindow))\n\n  .withColumn(&quot;specificTaskIdIndex&quot;,\n    first(\n      when($&quot;TaskID&quot; === lit(specificTaskId), $&quot;index&quot;).otherwise(null),\n      true)\n      .over(idsWindow))\n\n  .withColumn(&quot;result&quot;,\n    when($&quot;specificTaskIdIndex&quot; &gt; $&quot;deployIndex&quot;, 0).otherwise(1)\n  )\n</code></pre>\n<p>Output (&quot;deployIndex&quot; and &quot;specificTaskIdIndex&quot; columns have to be dropped):</p>\n<pre><code>+---+---+---------+----------+-----------+-----+-----------+-------------------+------+\n|Id1|Id2|Id3      |TaskID    |TaskName   |index|deployIndex|specificTaskIdIndex|result|\n+---+---+---------+----------+-----------+-----+-----------+-------------------+------+\n|1  |11 |bc123-234|dfr3ws-45d|randomName1|1    |4          |3                  |1     |\n|1  |11 |bc123-234|er98d3-lkj|randomName2|2    |4          |3                  |1     |\n|1  |11 |bc123-234|hu77d9-mnb|randomName3|3    |4          |3                  |1     |\n|1  |11 |bc123-234|xc33d5-rew|deployhere4|4    |4          |3                  |1     |\n|1  |11 |xre43-876|dfr3ws-45d|randomName1|1    |2          |3                  |0     |\n|1  |11 |xre43-876|er98d3-lkj|deployhere2|2    |2          |3                  |0     |\n|1  |11 |xre43-876|hu77d9-mnb|randomName3|3    |2          |3                  |0     |\n|1  |11 |xre43-876|xc33d5-rew|randomName4|4    |2          |3                  |0     |\n+---+---+---------+----------+-----------+-----+-----------+-------------------+------+\n</code></pre>",
		"Answer_comment_count": 6.0,
		"Answer_creation_date": "2021-09-21 08:55:12.450000 UTC",
		"Answer_last_edit_date": null,
		"Answer_score": 0.0,
		"Question_valid_tags": "azure",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 72188479,
		"Question_title": "Not able to create a new cluster in Azure Databricks",
		"Question_body": "<p>I have free trial with some credits remaining , I want to create a new cluster inside azure databricks and write some code in scala notebooks , but it seems everytime i try to create a new clsuter it says terminated. Can someone help what needs to be done to create a new cluster</p>\n<p><a href=\"https://i.stack.imgur.com/8HusC.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/8HusC.png\" alt=\"enter image description here\" /></a></p>",
		"Question_answer_count": 2,
		"Question_comment_count": 0,
		"Question_creation_date": "2022-05-10 14:45:59.330000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": null,
		"Question_score": 0,
		"Question_tags": "azure-databricks",
		"Question_view_count": 218,
		"Owner_creation_date": "2014-01-27 14:21:11.030000 UTC",
		"Owner_last_access_date": "2022-09-16 07:01:42.200000 UTC",
		"Owner_location": null,
		"Owner_reputation": 3445,
		"Owner_up_votes": 234,
		"Owner_down_votes": 0,
		"Owner_views": 625,
		"Answer_body": "<p>Using Databricks with Azure free trial subscription, we cannot use a cluster that utilizes more than 4 cores. It can be understood that you are using a Standard cluster which consumes 8 cores (4 worker and 4 driver cores).</p>\n<p>So, try creating a \u2018Single Node Cluster\u2019 which only consumes 4 cores (driver cores) which does not exceed the limit. You can refer to the following document to understand more about single node cluster.</p>\n<p><a href=\"https://docs.microsoft.com/en-us/azure/databricks/clusters/single-node\" rel=\"nofollow noreferrer\">https://docs.microsoft.com/en-us/azure/databricks/clusters/single-node</a></p>\n<p>If you need to use Standard cluster, upgrade your subscription to pay-as-you-go or use the 14-day free trial of Premium DBUs in Databricks. The following link refers to a problem like the one you are facing.</p>\n<p><a href=\"https://docs.microsoft.com/en-us/answers/questions/35165/databricks-cluster-does-not-work-with-free-trial-s.html\" rel=\"nofollow noreferrer\">https://docs.microsoft.com/en-us/answers/questions/35165/databricks-cluster-does-not-work-with-free-trial-s.html</a></p>",
		"Answer_comment_count": 0.0,
		"Answer_creation_date": "2022-05-12 04:16:57.750000 UTC",
		"Answer_last_edit_date": null,
		"Answer_score": 1.0,
		"Question_valid_tags": "azure",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 11789875,
		"Question_title": "How can I have two \"streams of development\" (branches?) track each other while remaining different in particular ways?",
		"Question_body": "<p>BRIEF:</p>\n\n<p>I want to have two (or more) \"streams of development\" / environments track each other, sending changes between each other in both directions, without converging totally - while preserving certain key, essential, differences?</p>\n\n<p>DETAIL, ONE PARTICULAR EXAMPLE:</p>\n\n<p>Here is one particular example:</p>\n\n<p>I have been version controlling my home directory, glew-home, for, oh, 28 years.  RCS, SCCS, many RCS wrappers, CVS, SVN, a brief period of experimentation with early DVCS like Monotone and Darcs, bzr, git, and now Mercurial. At the moment I am mainly happy using Mercurial, although I'll jump back to git or bzr as needed.</p>\n\n<p>Now, my home directory is similar, but not identical, on many systems.  The biggest differences are between Cygwin and the various Linuxes at work. I try to make them as smilar as possible, but the differences arise, and often need to persist.</p>\n\n<p>Here is a trivial example of the differences:  on Cygwin, on my personally owned laptop, ~/LOG is a symlink to ~/LOG.dir/LOG.cygwin.glew-home, while at work ~/LOG is a symlink to something like ~/work/LOG.dir/LOG.work.</p>\n\n<p>Reason: anything proprietary needs to stay at work.  ~/work is a separate repository, ~/work/.hg, and is NOT pushed/pulled or otherwise synchronized with my personal computer(s).</p>\n\n<p>Problem:  I want to keep these symlinks (and several other files) different.  But I want to synchronize all other files.  I make changes to my environment in both places.  If I make a change to my ~/.emacs at work I want to send it home, and vice versa.</p>\n\n<p>Q: how can I most conveniently do this?</p>\n\n<p>In the bad old days I would use a common repository, say a common CVS repo.  CVS doesn't handle symlinks, but say that I had a script that generated symlinks from a template stored in CVS.  I would arrange for the symlink template for ~/LOG to have different branches for my cygwin-laptop and for work. I would create workspaces with most files pointing to the same branch of their corresponding RCS/CVS repos, while the files that differed between cygwin-linux and work would have their corresponding branches checked out into their corresponding workspaces.  </p>\n\n<p>This worked, although it was a bit of a pain to maintain.</p>\n\n<p>I have not figured out a good way to do this with a modern DVCS, like Mercurial (or Got, or Bzr).</p>\n\n<p>These modern DVCS tools do whole-repo branching, not per-file branching.  They do not understand the notion of two branches that are identical for most files, but which differ in only certain files.</p>\n\n<p>When I try to track two branches, I always end up with the essential differences being propagated.</p>\n\n<p>Some have suggested Makefiles.  Not attractive.</p>\n\n<p>I have considered making the essential changes base revs, and constantly rebasing.  But I don't like rebasing that much.</p>\n\n<p>Better ideas appreciated. </p>\n\n<p>Quilt?</p>",
		"Question_answer_count": 1,
		"Question_comment_count": 1,
		"Question_creation_date": "2012-08-03 05:39:23.143000 UTC",
		"Question_favorite_count": 1.0,
		"Question_last_edit_date": null,
		"Question_score": 6,
		"Question_tags": "git|mercurial|dvcs|bazaar",
		"Question_view_count": 631,
		"Owner_creation_date": "2011-11-17 05:59:52.473000 UTC",
		"Owner_last_access_date": "2022-09-25 02:33:28.453000 UTC",
		"Owner_location": "Portland, OR",
		"Owner_reputation": 6928,
		"Owner_up_votes": 505,
		"Owner_down_votes": 40,
		"Owner_views": 2335,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "azure",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 66147164,
		"Question_title": "Installing c libraries needed for R spatial packages on databricks clusters",
		"Question_body": "<p>Spatial packages in R often depend on C libraries for their numerical computation. This presents a problem when installing R packages that depend on these libraries if the R engine is unable to install these libraries using default permissions. It appears that databricks clusters present such an obstacle for R. I guess there are two ways around this, 1) to create a docker container with the relevant scripts to install the packages or 2) to install them by way of an init script. I figured the latter approach would be easier but I'm having some problems. The clusters fail to start up bc my init script fails to execute. See below -I've also tried with <code>sudo</code></p>\n<pre><code>set -euxo pipefail\n\napt install libgeos-dev\napt install libudunits2-dev\napt install libgdal-dev\n</code></pre>\n<p>Relatedly, should these only be installed on the driver node? I dont see a reason why they need to be on worker nodes. The above code installs it on workers and drivers I think. To install on just the driver I suppose it would be:</p>\n<pre><code>if [[ $DB_IS_DRIVER = &quot;TRUE&quot; ]]; then\napt install libgeos-dev\napt install libudunits2-dev\napt install libgdal-dev\n</code></pre>",
		"Question_answer_count": 1,
		"Question_comment_count": 4,
		"Question_creation_date": "2021-02-11 00:33:31.003000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": null,
		"Question_score": 2,
		"Question_tags": "r|databricks|gdal|geos|aws-databricks",
		"Question_view_count": 253,
		"Owner_creation_date": "2015-08-28 19:07:24.940000 UTC",
		"Owner_last_access_date": "2022-09-22 04:16:56.780000 UTC",
		"Owner_location": "USA",
		"Owner_reputation": 4713,
		"Owner_up_votes": 1238,
		"Owner_down_votes": 497,
		"Owner_views": 909,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "azure",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 71464033,
		"Question_title": "Azure Databricks - Resolve : User does not have permission SELECT on any file error stopping from executing 'save'",
		"Question_body": "<p>We have two different Azure cloud resource groups, RG1 and RG2, where RG1 hosts the ADB_source of the data source, and RG2 hosts the ADB_sink &amp; ADLS_sink(gen2) of the data sink.</p>\n<p><strong>Use Case:</strong>\nWe have a few delta tables in ADB_source (ACL enabled) where a list of users has Read access.\nIn the ADB_source workspace, we need to read the delta tables and write them into ADLS_sink as parquet for further processing at the sink.</p>\n<p><strong>What's Available:</strong>\nWe have a high concurrency cluster created in ADB_Source workspace, which -</p>\n<ul>\n<li>Allows only Python &amp; SQL (dbutils.fs also restricted).</li>\n<li>Credential Passthrough is disabled.</li>\n<li>Has ACLs Enabled in spark config.</li>\n<li>Has mount point created to a container in ADLS_sink.</li>\n<li>Has no Admin Access to the cluster.</li>\n</ul>\n<p><strong>Errors Observed:</strong>\nWe could read the delta tables as expected and run action commands as long as they are in the ADB_source workspace. However, when we write that data into the ADLS_sink with <code>.save()</code>, we get the below error.</p>\n<pre><code>Py4JJavaError: An error occurred while calling o410.save. : java.lang.SecurityException: User does not have permission SELECT on any file. User does not have permission MODIFY on any file. \n</code></pre>\n<p>I would appreciate it if anyone could explain this and recommend additional security checks/accesses needed to implement the use case successfully.</p>",
		"Question_answer_count": 0,
		"Question_comment_count": 0,
		"Question_creation_date": "2022-03-14 07:03:59.150000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": "2022-03-15 12:24:45.553000 UTC",
		"Question_score": 2,
		"Question_tags": "azure-databricks|access-control|azure-data-lake-gen2",
		"Question_view_count": 1096,
		"Owner_creation_date": "2019-06-04 17:07:54.390000 UTC",
		"Owner_last_access_date": "2022-09-23 10:48:34.737000 UTC",
		"Owner_location": "India",
		"Owner_reputation": 420,
		"Owner_up_votes": 47,
		"Owner_down_votes": 10,
		"Owner_views": 61,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "azure",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 2599101,
		"Question_title": "Cheap cloning/local branching in Mercurial",
		"Question_body": "<p>Just started working with Mercurial a few days ago and there's something I don't understand.</p>\n\n<p>I have an experimental thing I want to do, so the normal thing to do would be to clone my repository, work on the clone and if eventually I want to keep those changes, I'll push them to my main repository. </p>\n\n<p>Problem is cloning my repository takes alot of time (we have alot of code) and just compiling the cloned copy would take up to an hour.<br>\nSo I need to somehow work on a different repository but still in my original working copy.  </p>\n\n<p>Enter <a href=\"http://mercurial.selenic.com/wiki/LocalBranches\" rel=\"nofollow noreferrer\"><strong>local</strong> branches</a>. </p>\n\n<p>Problem is just creating a local branch takes forever, and working with them isn't all that fun either. Because when moving between local branches doesn't \"revert\" to the target branch state, I have to issue a <code>hg purge</code> (to remove files that were added in the moved from branch) and then <code>hg update -c</code> (to revert modified files in the moved from branch). (note: I did try PK11 fork of local branch extension, it a simple local branch creation crashes with an exception)</p>\n\n<p>At the end of the day, this is just too complex. What are my options?</p>",
		"Question_answer_count": 4,
		"Question_comment_count": 0,
		"Question_creation_date": "2010-04-08 10:32:42.080000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": null,
		"Question_score": 2,
		"Question_tags": "mercurial|dvcs",
		"Question_view_count": 642,
		"Owner_creation_date": "2009-11-13 09:41:30.617000 UTC",
		"Owner_last_access_date": "2012-09-18 09:36:32.200000 UTC",
		"Owner_location": null,
		"Owner_reputation": 1931,
		"Owner_up_votes": 46,
		"Owner_down_votes": 0,
		"Owner_views": 64,
		"Answer_body": "<p>There are several ways of working with local branches <em>besides cloning</em>:</p>\n\n<ul>\n<li>Bookmarks</li>\n<li>Named branches</li>\n<li>Anonymous branches</li>\n</ul>\n\n<p>You may be interested in reading a very insightful <a href=\"http://stevelosh.com/blog/2009/08/a-guide-to-branching-in-mercurial/\" rel=\"nofollow noreferrer\">guide to branching in Mercurial</a>. I guess the <strong>bookmarks</strong> extension is the most appropriate way of branching in the context you've described.</p>",
		"Answer_comment_count": 5.0,
		"Answer_creation_date": "2010-04-08 11:09:26.077000 UTC",
		"Answer_last_edit_date": null,
		"Answer_score": 5.0,
		"Question_valid_tags": "azure",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 59181788,
		"Question_title": "Gremlin Python: How to use coalesce to get vertex if exists, else insert",
		"Question_body": "<p>Is it possible to use the Gremlin <code>coalesce</code> step to select a vertex by id (or properties) if such a vertex exists, otherwise insert the vertex? I've attempted to do so with the following, but I get a <code>'list' object has no attribute 'coalesce'</code> error, which I can see is caused by <code>.fold().next()</code> returning a python list object:</p>\n\n<pre><code>    my_vertex = g.V(1).fold().next().coalesce(__.unfold(), g.addV(\n         'my_label'\n        ).property(\n            T.id,\n            1\n        ).property(\n            'test', 'foo'\n        ).property(\n            'name', 'my_vertex_name'\n        ).property(\n            'created_timestamp', 1575480467\n        ).next()\n    )\n</code></pre>\n\n<p>Is there any performance benefit to doing it this way, or should I simply break this into an if/else on the hasNext() of the initial vertex query?</p>",
		"Question_answer_count": 2,
		"Question_comment_count": 0,
		"Question_creation_date": "2019-12-04 17:33:08.497000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": "2019-12-04 18:25:20.090000 UTC",
		"Question_score": 0,
		"Question_tags": "gremlin|amazon-neptune|gremlinpython",
		"Question_view_count": 543,
		"Owner_creation_date": "2013-06-11 22:07:42.193000 UTC",
		"Owner_last_access_date": "2022-04-11 16:57:17.057000 UTC",
		"Owner_location": "Michigan",
		"Owner_reputation": 3276,
		"Owner_up_votes": 1125,
		"Owner_down_votes": 49,
		"Owner_views": 227,
		"Answer_body": "<p>I was able to accomplish the \"get if exists, else insert\" by moving the next() terminal step to the end of the traversal as shown below:</p>\n\n<pre><code>my_vertex = g.V(1).fold().next().coalesce(__.unfold(), g.addV(\n         'my_label'\n        ).property(\n            T.id,\n            1\n        ).property(\n            'test', 'foo'\n        ).property(\n            'name', 'my_vertex_name'\n        ).property(\n            'created_timestamp', 1575480467\n        )\n    ).next() //notice that .next was simply moved to the close of the .coalesce step\n</code></pre>",
		"Answer_comment_count": 0.0,
		"Answer_creation_date": "2019-12-12 21:35:52.897000 UTC",
		"Answer_last_edit_date": "2020-02-11 18:39:19.710000 UTC",
		"Answer_score": -1.0,
		"Question_valid_tags": "azure",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 68899682,
		"Question_title": "Apache Spark error, Cloned Python environment not found",
		"Question_body": "<p>I am attempting to upgrade huggingface to a later version of what we currently have, 2.11. when i install any newer version of the transformer via pip install transformers=={any version}in azure databricks notebook, I receive the following error during execution. I am pretty new to this, but any feedback on ways to troubleshoot would be greatly appreciated. Thank you.</p>\n<pre><code>org.apache.spark.SparkException: Cloned Python environment not found at /local_disk0/.ephemeral_nfs/envs/pythonEnv-89bc8046-d7ae-4968-b280-fc233a9bf3e4\nat org.apache.spark.api.python.PythonWorkerFactory.waitForPythonEnvironment(PythonWorkerFactory.scala:190)\nat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:313)\nat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:222)\nat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:119)\nat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:192)\nat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:184)\nat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:70)\nat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:129)\nat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:844)\nat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:844)\nat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)\nat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:356)\nat org.apache.spark.rdd.RDD.iterator(RDD.scala:320)\nat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)\nat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:356)\nat org.apache.spark.rdd.RDD.iterator(RDD.scala:320)\nat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)\nat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:356)\nat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:369)\nat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1376)\nat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1303)\nat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1367)\nat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1187)\nat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:367)\nat org.apache.spark.rdd.RDD.iterator(RDD.scala:318)\nat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)\nat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:356)\nat org.apache.spark.rdd.RDD.iterator(RDD.scala:320)\nat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)\nat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:356)\nat org.apache.spark.rdd.RDD.iterator(RDD.scala:320)\nat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\nat org.apache.spark.scheduler.Task.doRunTask(Task.scala:144)\nat org.apache.spark.scheduler.Task.run(Task.scala:117)\nat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$9(Executor.scala:642)\nat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1581)\nat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:645)\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\nat java.lang.Thread.run(Thread.java:748)\n</code></pre>",
		"Question_answer_count": 1,
		"Question_comment_count": 3,
		"Question_creation_date": "2021-08-23 22:24:08.343000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": null,
		"Question_score": 0,
		"Question_tags": "azure|apache-spark|databricks|azure-databricks",
		"Question_view_count": 288,
		"Owner_creation_date": "2019-01-22 04:19:30.583000 UTC",
		"Owner_last_access_date": "2022-09-03 23:37:39.767000 UTC",
		"Owner_location": null,
		"Owner_reputation": 15,
		"Owner_up_votes": 0,
		"Owner_down_votes": 0,
		"Owner_views": 5,
		"Answer_body": "<p>Please follow below <strong>steps to install huggingface library on azure databricks</strong></p>\n<p><strong>Step1:</strong> First use install transformers using this command - <code>pip install transformers</code></p>\n<p><a href=\"https://i.stack.imgur.com/wuaRZ.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/wuaRZ.png\" alt=\"enter image description here\" /></a></p>\n<p><strong>Step2:</strong>  To test installation try this - <code>from transformers import pipeline</code></p>\n<p><a href=\"https://i.stack.imgur.com/xyTEj.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/xyTEj.png\" alt=\"enter image description here\" /></a></p>\n<p><strong>Step3:</strong> Next install pytorch with this command - <code>pip install transformers[torch]</code></p>\n<p><a href=\"https://i.stack.imgur.com/qcpev.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/qcpev.png\" alt=\"enter image description here\" /></a></p>\n<p><strong>Step4:</strong> To install Tensorflow use - <code>pip install transformers[tf-cpu]</code></p>\n<p><a href=\"https://i.stack.imgur.com/A54Tw.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/A54Tw.png\" alt=\"enter image description here\" /></a></p>\n<p><strong>Step5:</strong> To test installation I used <code>print(pipeline('sentiment-analysis')('we love you'))</code></p>\n<p><a href=\"https://i.stack.imgur.com/QIgn7.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/QIgn7.png\" alt=\"enter image description here\" /></a></p>\n<p>This way I successfully installed huggingface library.</p>\n<p><strong>Refer</strong> - <a href=\"https://huggingface.co/transformers/installation.html\" rel=\"nofollow noreferrer\">Installation</a></p>",
		"Answer_comment_count": 3.0,
		"Answer_creation_date": "2021-08-26 03:37:43.057000 UTC",
		"Answer_last_edit_date": null,
		"Answer_score": 1.0,
		"Question_valid_tags": "azure",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 69113428,
		"Question_title": "Model deployment in Azure ML",
		"Question_body": "<p>I am a beginner in the Azure. I am using this tutorial <a href=\"https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-and-where?tabs=python\" rel=\"nofollow noreferrer\">https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-and-where?tabs=python</a> of setting a dummy script for a local web service but many errors are coming up. It is strange because I am using an h5 file (model involving Keras and tensor flow) in place of onxx file. I used the code</p>\n<pre><code>from azureml.core import Environment\nfrom azureml.core.model import InferenceConfig\n\nenv = Environment(name=&quot;myenv&quot;)\nconda_dep = CondaDependencies()\nconda_dep.add_conda_package(&quot;tensorflow&quot;)\nconda_dep.add_conda_package(&quot;pip&quot;)\nconda_dep.add_pip_package(&quot;azureml-core&quot;)\nconda_dep.add_pip_package(&quot;azureml-contrib-services&quot;)\nenv.python.conda_dependencies=conda_dep\ninference_config = InferenceConfig(\n    environment=env,\n    source_directory=&quot;./source_dir&quot;,\n    entry_script=&quot;./echo_score.py&quot;,\n)\n \n</code></pre>\n<p>There is a Error</p>\n<pre><code>ERROR: Could not find a version that satisfies the requirement pickle (from -r /azureml-environment-setup/condaenv.4f206b3i.requirements.txt (line 5)) (from versions: none)\n</code></pre>\n<p>I have tried many times and getting error. Can anyone help.\nMy azureml core is 1.34.0 and python version 3.8.11.I am also not sure if it is a pickle protocol error. Pickle version in my system is 4.</p>",
		"Question_answer_count": 0,
		"Question_comment_count": 2,
		"Question_creation_date": "2021-09-09 06:46:13.543000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": "2021-09-16 12:53:10.997000 UTC",
		"Question_score": 0,
		"Question_tags": "python|azure-web-app-service|web-deployment|azure-machine-learning-service",
		"Question_view_count": 185,
		"Owner_creation_date": "2021-05-28 05:28:29.977000 UTC",
		"Owner_last_access_date": "2021-09-30 06:52:30.927000 UTC",
		"Owner_location": "Singapore",
		"Owner_reputation": 21,
		"Owner_up_votes": 0,
		"Owner_down_votes": 0,
		"Owner_views": 1,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "azure",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 62490163,
		"Question_title": "Unable to copy dataframe in pyspark to csv file in Databricks",
		"Question_body": "<p>I am working in Pyspark environment in Databricks and have a pyspark data frame which I will call as df.</p>\n<p>I need to push this spark dataframe into csv file, I am unable to do so. Though there is no error popping up but the dataframe doesn\u2019t get copied into the csv. Below is the generic code</p>\n<p><div class=\"snippet\" data-lang=\"js\" data-hide=\"false\" data-console=\"true\" data-babel=\"false\">\n<div class=\"snippet-code\">\n<pre class=\"snippet-code-html lang-html prettyprint-override\"><code>path = \u201c \u201c #CSV File Location\nheader = \u201cThis is the header of the file\"\nWith open(path,\u201da\u201d) as f:\n    f.write(header+\u201d\\n\u201d)\n    df.write.csv(path=path,format=\u201ccsv\u201d,mode=\u201cappend\u201d)\n    f.close</code></pre>\n</div>\n</div>\n</p>\n<p>Only the header gets reflected in the file and not the dataframe</p>",
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_creation_date": "2020-06-20 18:40:40.633000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": null,
		"Question_score": 0,
		"Question_tags": "pyspark|databricks",
		"Question_view_count": 73,
		"Owner_creation_date": "2019-02-23 12:47:14.513000 UTC",
		"Owner_last_access_date": "2020-08-14 13:44:11.550000 UTC",
		"Owner_location": null,
		"Owner_reputation": 1,
		"Owner_up_votes": 0,
		"Owner_down_votes": 0,
		"Owner_views": 2,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "azure",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 73544548,
		"Question_title": "Extracting the start and end time of Task Id running inside a Job (Job orchestration) from Databricks Jobs API 2.1 using Python",
		"Question_body": "<p>I want to fetch the list of all running jobs by using Jobs API 2.1 &quot;<strong>2.1/jobs/runs/list</strong>&quot; and the detail of task running inside it. I have gone through this link &quot;https://docs.databricks.com/dev-tools/api/latest/jobs.html#operation/JobsRunsList&quot; which states that we can view the Task details in the JSON but I am not sure how to make <strong>expand_task = True</strong>. Basically, I am only able to extract the start and end time of the particular job/run ID  but I am looking for start and end time of the task ID\u2019s under that particular job.</p>\n<p>I have tried this code:</p>\n<pre><code>API_URL = '{instance}'   \nTOKEN = '{Token}'\nAPI_VERSION = '/api/2.1'\nAPI_CMD = '/jobs/runs/list'\nREQ_HEADERS = {&quot;Authorization&quot;: &quot;Bearer &quot; + TOKEN}\nresponse = requests.get(url = API_URL + API_VERSION + API_CMD,\n                         params = {'limit' : '1000', 'offset' : '0'},\n                         headers = REQ_HEADERS)\nresponse.status_code\n</code></pre>\n<p>Let me know, How I can view the Tasks ID inside a running Jobs or enable the expand_tasks.</p>",
		"Question_answer_count": 0,
		"Question_comment_count": 0,
		"Question_creation_date": "2022-08-30 14:58:24.307000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": null,
		"Question_score": 0,
		"Question_tags": "json|python-3.x|databricks|databricks-connect|databricks-community-edition",
		"Question_view_count": 17,
		"Owner_creation_date": "2019-06-21 06:02:50.393000 UTC",
		"Owner_last_access_date": "2022-09-22 17:35:34.563000 UTC",
		"Owner_location": null,
		"Owner_reputation": 69,
		"Owner_up_votes": 3,
		"Owner_down_votes": 0,
		"Owner_views": 16,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "azure",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 73187262,
		"Question_title": "Gremlin Tinkerpop get vertex with partial ID",
		"Question_body": "<p>Is there a way to get a vertex with only a partial ID? For example, let's say I have two unique values that make up part of the ID, such as: <code>employeeName/{employeeName}/dob/{employeeDoB}</code>.</p>\n<p>If I only have ONE of the values, such as <code>employeeName</code>, can I still get the vertex by just knowing that part?</p>\n<p>I was thinking I could use the Tinkerpop <code>Text</code> enum and do something like:</p>\n<pre><code>g.V(&quot;employeeName/&quot; + id, Text.startingWith);\n</code></pre>\n<p>But I get a serialization error with that:</p>\n<pre><code>- it could not be sent to the server - Reason: org.apache.tinkerpop.gremlin.driver.ser.SerializationException: java.io.IOException: Serializer for type org.apache.tinkerpop.gremlin.process.traversal.Text$3 not found\n</code></pre>\n<p>Using Tinkerpop 3.5.1 with Amazon Neptune.</p>",
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_creation_date": "2022-07-31 22:12:46.940000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": "2022-08-01 12:00:57.000000 UTC",
		"Question_score": 1,
		"Question_tags": "java|gremlin|tinkerpop|amazon-neptune|tinkerpop3",
		"Question_view_count": 33,
		"Owner_creation_date": "2017-01-29 14:23:19.997000 UTC",
		"Owner_last_access_date": "2022-09-23 21:52:26.280000 UTC",
		"Owner_location": "Littleton, CO, United States",
		"Owner_reputation": 85,
		"Owner_up_votes": 10,
		"Owner_down_votes": 0,
		"Owner_views": 22,
		"Answer_body": "<p>You need to use <code>TextP</code> and not <code>Text</code>, and you also need to add a filtering step. So it becomes:</p>\n<pre class=\"lang-java prettyprint-override\"><code>g.V().hasId(TextP.startingWith(&quot;employeeName/&quot; + name));\n</code></pre>\n<p>This assumes you are sending the request from Java. In a large graph this may not be efficient, and you may want to consider looking at the OpenSearch/ElasticSearch Full Text Search (FTS) integration.</p>",
		"Answer_comment_count": 0.0,
		"Answer_creation_date": "2022-07-31 23:27:33.820000 UTC",
		"Answer_last_edit_date": "2022-08-01 11:27:17.627000 UTC",
		"Answer_score": 3.0,
		"Question_valid_tags": "azure",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 56926564,
		"Question_title": "Pyspark/Databricks - org.apache.spark.SparkException: Job aborted due to stage failure:",
		"Question_body": "<p>I'm trying to use Linear Regression on a simple dataframe with <strong>one feature</strong> and <strong>one label</strong> using Python pyspark in Databricks. However, I'm running into some issues with stage failure. I've reviewed many similar problems, but most of them are in Scala or are out of the scope of what I'm doing here.</p>\n\n<p><strong>Versions:</strong></p>\n\n<p>Notebook: Databricks \n5.3 (includes Apache Spark 2.4.0, Scala 2.11)\nPython version: 2</p>\n\n<p><strong>Here's what I've done:</strong></p>\n\n<ol>\n<li>Original dataframe looks like this:</li>\n</ol>\n\n<pre><code>    df_red = df_extra.select('cca3', 'class', 'device_id').groupby('cca3').pivot('class').count()\n\n    display(df_red)\n</code></pre>\n\n<p><a href=\"https://i.stack.imgur.com/IfhOq.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/IfhOq.png\" alt=\"enter image description here\"></a></p>\n\n<p>I want the 'mac' column as my label and the 'other' column as my single feature.</p>\n\n<p>2.Drop column 'cca3' and create label/feature</p>\n\n<pre><code>features = ['other']\nlr_data = df_red.drop('cca3').select(col('mac').alias('label'), *features)\ndisplay(lr_data)\n</code></pre>\n\n<p><a href=\"https://i.stack.imgur.com/I4C60.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/I4C60.png\" alt=\"enter image description here\"></a></p>\n\n<ol start=\"3\">\n<li>Create Vector Assembler and remove null values in dataframe</li>\n</ol>\n\n<pre><code>assembler = VectorAssembler(inputCols = features, outputCol = \"features\")\noutput = assembler.transform(lr_data)\nnew_lr_data = output.select(\"label\", \"features\").where(col('label').isNotNull())\nnew_lr_data.show()\n</code></pre>\n\n<p><a href=\"https://i.stack.imgur.com/oC7eI.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/oC7eI.png\" alt=\"enter image description here\"></a></p>\n\n<ol start=\"4\">\n<li>Linear Regression Model Fit:</li>\n</ol>\n\n<pre><code># Fit the model\nlr = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\nlrModel = lr.fit(new_lr_data)\n\n# Print the coefficients and intercept for linear regression\nprint(\"Coefficients: %s\" % str(lrModel.coefficients))\nprint(\"Intercept: %s\" % str(lrModel.intercept))\n\n# Summarize the model over the training set and print out some metrics\ntrainingSummary = lrModel.summary\n#print(\"numIterations: %d\" % trainingSummary.totalIterations)\n#print(\"objectiveHistory: %s\" % str(trainingSummary.objectiveHistory))\n#trainingSummary.residuals.show()\n#print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n#print(\"r2: %f\" % trainingSummary.r2)\n</code></pre>\n\n<p>At this point I get the error below:</p>\n\n<blockquote>\n  <p>org.apache.spark.SparkException: Job aborted due to stage failure:\n  Task 73 in stage 979.0 failed 1 times, most recent failure: Lost task\n  73.0 in stage 979.0 (TID 32624, localhost, executor driver): org.apache.spark.SparkException: Failed to execute user defined\n  function($anonfun$4:\n  (struct&lt;other_double_VectorAssembler_a2059b1f0691:double&gt;) =&gt;\n  struct&lt;type:tinyint,size:int,indices:array&lt;int&gt;,values:array&lt;double&gt;&gt;)</p>\n</blockquote>\n\n<p>What causes the above error to occur within Databricks? Could this be because I only used one feature as opposed to many features (usually the case)?</p>\n\n<p>Any help is much appreciated!</p>",
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_creation_date": "2019-07-07 22:24:44.000000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": null,
		"Question_score": 1,
		"Question_tags": "python|pyspark|databricks",
		"Question_view_count": 1168,
		"Owner_creation_date": "2013-07-04 21:11:30.817000 UTC",
		"Owner_last_access_date": "2019-09-27 01:24:14.310000 UTC",
		"Owner_location": null,
		"Owner_reputation": 387,
		"Owner_up_votes": 6,
		"Owner_down_votes": 0,
		"Owner_views": 47,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "azure",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 62977067,
		"Question_title": "Error while creating data frame from Rest Api in Pyspark",
		"Question_body": "<p>I have a below pyspark code. I am reading a json data from Rest API and trying to load using pyspark.\nBut i couldnt read the data in DataFrame in spark.Can some one help me in this.</p>\n<pre><code>import urllib\nfrom pyspark.sql.types import StructType,StructField,StringType\n\nschema = StructType([StructField('dropoff_latitude',StringType(),True),\\\n                     StructField('dropoff_longitude',StringType(),True),\n                     StructField('extra',StringType(),True),\\\n                     StructField('fare_amount',StringType(),True),\\\n                     StructField('improvement_surcharge',StringType(),True),\\\n                     StructField('lpep_dropoff_datetime',StringType(),True),\\\n                     StructField('mta_tax',StringType(),True),\\\n                     StructField('passenger_count',StringType(),True),\\\n                     StructField('payment_type',StringType(),True),\\\n                     StructField('pickup_latitude',StringType(),True),\\\n                     StructField('ratecodeid',StringType(),True),\\\n                     StructField('tip_amount',StringType(),True),\\\n                     StructField('tolls_amount',StringType(),True),\\\n                     StructField('total_amount',StringType(),True),\\\n                     StructField('trip_distance',StringType(),True),\\\n                     StructField('trip_type',StringType(),True),\\\n                     StructField('vendorid',StringType(),True)\n                    ])\nurl = 'https://data.cityofnewyork.us/resource/pqfs-mqru.json'\ndata =  urllib.request.urlopen(url).read().decode('utf-8')\n\nrdd = sc.parallelize(data)\ndf = spark.createDataFrame(rdd,schema)\ndf.show()```\n\n**The Error message is TypeError: StructType can not accept object '[' in type &lt;class 'str'&gt;**\n** I have been able to do using dataset in scala but i am not able to understand why its not possible using python **\n</code></pre>\n<p>import spark.implicits._</p>\n<p>// Load the data from the New York City Taxi data REST API for 2016 Green Taxi Trip Data\nval url=&quot;https://data.cityofnewyork.us/resource/pqfs-mqru.json&quot;\nval result = scala.io.Source.fromURL(url).mkString</p>\n<p>// Create a dataframe from the JSON data\nval taxiDF = spark.read.json(Seq(result).toDS)</p>\n<p>// Display the dataframe containing trip data\ntaxiDF.show()</p>\n<pre><code></code></pre>",
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_creation_date": "2020-07-19 06:38:10.750000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": "2020-07-19 15:14:37.573000 UTC",
		"Question_score": -1,
		"Question_tags": "apache-spark|pyspark|azure-databricks",
		"Question_view_count": 166,
		"Owner_creation_date": "2019-02-13 05:50:19.390000 UTC",
		"Owner_last_access_date": "2022-08-22 09:59:38.133000 UTC",
		"Owner_location": "Bangalore, Karnataka, India",
		"Owner_reputation": 69,
		"Owner_up_votes": 1,
		"Owner_down_votes": 0,
		"Owner_views": 18,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "azure",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 68278752,
		"Question_title": "Mount Azure Storage Container to Databricks Workspace / Notebook results in AttributeError",
		"Question_body": "<p>I'm trying to mount an Azure Blob Storage Container to a Databricks workbook using a Key Vault-backed secret scope.</p>\n<p><strong>Setup:</strong></p>\n<ol>\n<li>Created a Key Vault</li>\n<li>Created a Secret in Key Vault</li>\n<li>Created a Databricks Secret Scope</li>\n</ol>\n<ul>\n<li>This is known-good.\n<ul>\n<li>Running <code>dbutils.secrets.get(scope = dbrick_secret_scope, key = dbrick_secret_name)</code> results in no errors</li>\n<li>Viewing the secret in Databricks results in <code>[REDACTED]</code></li>\n</ul>\n</li>\n</ul>\n<p><strong>Cell in Databricks:</strong></p>\n<pre><code>%python\n\ndbrick_secret_scope = &quot;dbricks_kv_dev&quot;\ndbrick_secret_name = &quot;scrt-account-key&quot;\n\nstorage_account_key = dbutils.secrets.get(scope = dbrick_secret_scope, key = dbrick_secret_name)\nstorage_container = 'abc-test'\nstorage_account = 'stgdev'\n\ndbutils.fs.mount(\n    source = f'abfss://{storage_container}@{storage_account}.dfs.core.windows.net/',\n    mount_point = f'/mnt/{storage_account}',\n    extra_configs = {f'fs.azure.accountkey.{storage_account}.dfs.core.windows.net:{storage_account_key}'}\n)\n</code></pre>\n<p><strong>Results:</strong></p>\n<ul>\n<li>Error: <code>AttributeError: 'set' object has no attribute 'keys'</code> with the <code>mount_point</code> line of <code>dbutils.fs.mount()</code> highlighted in red.</li>\n<li>Full error:</li>\n</ul>\n<pre><code>AttributeError                            Traceback (most recent call last)\n&lt;command-3166320686381550&gt; in &lt;module&gt;\n      9     source = f'abfss://{storage_container}@{storage_account}.dfs.core.windows.net/',\n     10     mount_point = f'/mnt/{storage_account}',\n---&gt; 11     extra_configs = {f'fs.azure.accountkey.{storage_account}.dfs.core.windows.net:{storage_account_key}'}\n     12 )\n\n/local_disk0/tmp/1625601199293-0/dbutils.py in f_with_exception_handling(*args, **kwargs)\n    298             def f_with_exception_handling(*args, **kwargs):\n    299                 try:\n--&gt; 300                     return f(*args, **kwargs)\n    301                 except Py4JJavaError as e:\n    302                     class ExecutionError(Exception):\n\n/local_disk0/tmp/1625601199293-0/dbutils.py in mount(self, source, mount_point, encryption_type, owner, extra_configs)\n    389                 self.check_types([(owner, string_types)])\n    390             java_extra_configs = \\\n--&gt; 391                 MapConverter().convert(extra_configs, self.sc._jvm._gateway_client)\n    392             return self.print_return(self.dbcore.mount(source, mount_point,\n    393                                                        encryption_type, owner,\n\n/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_collections.py in convert(self, object, gateway_client)\n    520         HashMap = JavaClass(&quot;java.util.HashMap&quot;, gateway_client)\n    521         java_map = HashMap()\n--&gt; 522         for key in object.keys():\n    523             java_map[key] = object[key]\n    524         return java_map\n\nAttributeError: 'set' object has no attribute 'keys'\n</code></pre>\n<p>Appears to be related to the <code>extra_configs</code> parameters, but I'm not exactly sure what. Can anyone see what I'm missing?</p>",
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_creation_date": "2021-07-07 00:36:04.900000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": "2021-07-08 16:08:33.980000 UTC",
		"Question_score": 1,
		"Question_tags": "python|databricks|azure-keyvault|azure-databricks|azure-secrets",
		"Question_view_count": 1091,
		"Owner_creation_date": "2018-09-27 21:08:18.920000 UTC",
		"Owner_last_access_date": "2022-09-23 21:48:13.310000 UTC",
		"Owner_location": null,
		"Owner_reputation": 1109,
		"Owner_up_votes": 194,
		"Owner_down_votes": 0,
		"Owner_views": 290,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "azure",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 52976560,
		"Question_title": "spark xml: need struct type but got string;",
		"Question_body": "<p>I have a folder which consists from XML files and we assume that these files has the same structure. But some of these files are incomplete due to errors in the application which generates these files:</p>\n\n<p><em>Expected structure:</em></p>\n\n<pre><code>root\n |-- R: struct (nullable = true)\n |    |-- LTI: struct (nullable = true)\n |    |    |-- C: long (nullable = true)\n |    |    |-- V: long (nullable = true)\n |    |-- MFV: string (nullable = true)\n</code></pre>\n\n<p><em>Needs to be ignored structure:</em></p>\n\n<pre><code>root\n |-- R: struct (nullable = true)\n |    |-- LTI: long (nullable = true)\n |    |-- MFV: string (nullable = true)\n</code></pre>\n\n<p>That is how I parse</p>\n\n<pre><code>df = spark.read \\\n  .format(\"com.databricks.spark.xml\") \\\n  .options(rowTag=\"RList\") \\\n  .load(\"/mnt/data/uploads/*/*/*/RList.xml\")\n\ndf.select(\n  \"R.LTI.C\", \n  \"R.LTI.V\", \n  \"R.MFV\", \n).show()\n</code></pre>\n\n<p>That is what I catch when it tries to load incomplete XML</p>\n\n<pre><code>Can't extract value from R#204.LTI: need struct type but got string;\"\n</code></pre>\n\n<p><strong>My question is how to ignore such kind of files?</strong></p>",
		"Question_answer_count": 0,
		"Question_comment_count": 5,
		"Question_creation_date": "2018-10-24 19:26:22.170000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": "2018-10-25 05:23:11.843000 UTC",
		"Question_score": 1,
		"Question_tags": "apache-spark|pyspark|apache-spark-sql|databricks",
		"Question_view_count": 2489,
		"Owner_creation_date": "2018-09-21 20:59:59.030000 UTC",
		"Owner_last_access_date": "2022-06-02 12:31:57.327000 UTC",
		"Owner_location": null,
		"Owner_reputation": 93,
		"Owner_up_votes": 4,
		"Owner_down_votes": 0,
		"Owner_views": 31,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "azure",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 4671332,
		"Question_title": "Mercurial how to rollback to a tag?",
		"Question_body": "<p>I am a little confused about how to rollback to a tag in Mercurial. (which I am very new to)</p>\n<p>Say I have a tag called &quot;Version-1.0&quot; which was several revisions ago. lets say we are at r400 now</p>\n<p>Now if my managers were to tell me they don't like the direction things have been going and basically want to ditch everything since that tag and go back to Version-1.0.</p>\n<p>Well I can checkout that tag with:</p>\n<blockquote>\n<p>hg update -r Version-1.0</p>\n</blockquote>\n<p>Ok so now I am back to the version 1.0 tag, and if I were to never need to make a change this would be fine. However, as soon as I make a change and commit, I now have 2 heads (my new changes to Version-1.0 and r400 the stuff the managers want to ditch).</p>\n<p>So now I need to merge with r400. I don't want to. (I don't really want to wipe all those changes off the earth, I would like them to remain in my history so I can go back to them later if management changes their mind again) but I currently don't want any of them.</p>\n<p>What do I do?</p>\n<hr />\n<h3>update</h3>\n<p>An answer stated:</p>\n<blockquote>\n<p>You could clone the entire repository up until the tag, then use that clone as your &quot;new&quot; central repository.</p>\n</blockquote>\n<p>If my central repository is hosted by bitbucket, I am not sure how to do this. If the current one is at URL <code>https://jisaacks@bitbucket.org/jisaacks/hgml</code> and I want to clone it up to the tag to a new repo named <code>hgml2</code> (that doesn't exist yet) I tried this command locally on my machine:</p>\n<pre><code>hg clone -r Version-1.0 https://jisaacks@bitbucket.org/jisaacks/hgml https://jisaacks@bitbucket.org/jisaacks/hgml2\n</code></pre>\n<p>I get this error:</p>\n<blockquote>\n<p>abort: cannot create a new http repository</p>\n</blockquote>",
		"Question_answer_count": 3,
		"Question_comment_count": 1,
		"Question_creation_date": "2011-01-12 16:39:39.263000 UTC",
		"Question_favorite_count": 2.0,
		"Question_last_edit_date": "2020-06-20 09:12:55.060000 UTC",
		"Question_score": 5,
		"Question_tags": "mercurial|dvcs",
		"Question_view_count": 2158,
		"Owner_creation_date": "2008-12-13 19:05:05.103000 UTC",
		"Owner_last_access_date": "2022-09-23 15:02:31.450000 UTC",
		"Owner_location": "Atlanta, GA",
		"Owner_reputation": 54256,
		"Owner_up_votes": 832,
		"Owner_down_votes": 12,
		"Owner_views": 3386,
		"Answer_body": "<p>Log on onto bitbucket and click the \"Fork\" arrow icon in the upper right corner. Now you can open the \"advanced settings\" to fork from a tag.</p>\n\n<p>You could then rename the first repository to something like \"myproject-abandoned-foo-changes\" and reuse the original name for the forked repository, where you will continue development.</p>\n\n<p><strong>edit</strong>: you could also do a <a href=\"http://mercurial.selenic.com/wiki/PruningDeadBranches#No-Op_Merges\" rel=\"nofollow\">no-op merge</a>. The linked wiki page explains that this might be bad because your history will be contaminated with the unwanted changes, but I think in your case this might be exactly what you want as it would preserve your changes while avoiding the \"switching everything to a new repo\" issue you complain about.</p>",
		"Answer_comment_count": 1.0,
		"Answer_creation_date": "2011-01-12 17:27:11.637000 UTC",
		"Answer_last_edit_date": "2011-01-12 17:43:30.883000 UTC",
		"Answer_score": 3.0,
		"Question_valid_tags": "azure",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 70510331,
		"Question_title": "How do i configure azure databricks with log4j2",
		"Question_body": "<p>log4j:WARN Failed to set property [rollingPolicy] to value &quot;org.apache.log4j.rolling.TimeBasedRollingPolicy&quot;.\nlog4j:WARN Please set a rolling policy for the DatabricksRollingFileAppender named 'publicFile'\nlog4j:WARN Failed to set property [rollingPolicy] to value &quot;org.apache.log4j.rolling.TimeBasedRollingPolicy&quot;.\nlog4j:WARN Please set a rolling policy for the DatabricksRollingFileAppender named 'privateFile'\nlog4j:WARN Failed to set property [rollingPolicy] to value &quot;org.apache.log4j.rolling.TimeBasedRollingPolicy&quot;.\nlog4j:WARN Please set a rolling policy for the DatabricksRollingFileAppender named 'product'\nlog4j:WARN Failed to set property [rollingPolicy] to value &quot;org.apache.log4j.rolling.TimeBasedRollingPolicy&quot;.\nlog4j:WARN Please set a rolling policy for the DatabricksRollingFileAppender named 'metrics'\nlog4j:WARN Failed to set property [rollingPolicy] to value &quot;org.apache.log4j.rolling.TimeBasedRollingPolicy&quot;.\nlog4j:WARN Please set a rolling policy for the DatabricksRollingFileAppender named 'usage</p>",
		"Question_answer_count": 2,
		"Question_comment_count": 2,
		"Question_creation_date": "2021-12-28 17:13:33.913000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": null,
		"Question_score": 0,
		"Question_tags": "databricks|log4j2|azure-databricks",
		"Question_view_count": 165,
		"Owner_creation_date": "2021-03-27 13:37:27.440000 UTC",
		"Owner_last_access_date": "2022-07-22 03:22:43.917000 UTC",
		"Owner_location": "Toronto",
		"Owner_reputation": 1,
		"Owner_up_votes": 0,
		"Owner_down_votes": 0,
		"Owner_views": 1,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "azure",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 64575719,
		"Question_title": "How to proper use sql/hive variables in the new databricks connect",
		"Question_body": "<p>I'm testing the new databricks connect and I often use sql variables in my python scripts on databricks, however I'm not able to use those variables through dbconnect. The example below works fine in databricks but not in dbconnect:</p>\n<pre><code>from pyspark.sql import SparkSession\nfrom pyspark.sql import SQLContext\nimport pandas as pd\n\nspark = SparkSession.builder.getOrCreate()\nsqlContext = SQLContext(spark)\n\ndf = spark.createDataFrame(pd.DataFrame({'a':[2,5,8], 'b':[3,5,5]}))\ndf.createOrReplaceTempView('test_view')\n\nsqlContext.sql(&quot;set a_value = 2&quot;)\nsqlContext.sql(&quot;select * from test_view where a = ${a_value}&quot;)\n</code></pre>\n<p>In dbconnect I received the follow:</p>\n<pre><code>---------------------------------------------------------------------------\nParseException                            Traceback (most recent call last)\n&lt;ipython-input-50-404f4c5b017c&gt; in &lt;module&gt;\n     10 \n     11 sqlContext.sql(&quot;set a_value = 2&quot;)\n---&gt; 12 sqlContext.sql(&quot;select * from test_view where a = ${a_value}&quot;)\n\nc:\\users\\pc\\miniconda3\\lib\\site-packages\\pyspark\\sql\\context.py in sql(self, sqlQuery)\n    369         [Row(f1=1, f2=u'row1'), Row(f1=2, f2=u'row2'), Row(f1=3, f2=u'row3')]\n    370         &quot;&quot;&quot;\n--&gt; 371         return self.sparkSession.sql(sqlQuery)\n    372 \n    373     @since(1.0)\n\nc:\\users\\pc\\miniconda3\\lib\\site-packages\\pyspark\\sql\\session.py in sql(self, sqlQuery)\n    702         [Row(f1=1, f2=u'row1'), Row(f1=2, f2=u'row2'), Row(f1=3, f2=u'row3')]\n    703         &quot;&quot;&quot;\n--&gt; 704         return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)\n    705 \n    706     @since(2.0)\n\nc:\\users\\pc\\miniconda3\\lib\\site-packages\\py4j\\java_gateway.py in __call__(self, *args)\n   1303         answer = self.gateway_client.send_command(command)\n   1304         return_value = get_return_value(\n-&gt; 1305             answer, self.gateway_client, self.target_id, self.name)\n   1306 \n   1307         for temp_arg in temp_args:\n\nc:\\users\\pc\\miniconda3\\lib\\site-packages\\pyspark\\sql\\utils.py in deco(*a, **kw)\n    132                 # Hide where the exception came from that shows a non-Pythonic\n    133                 # JVM exception message.\n--&gt; 134                 raise_from(converted)\n    135             else:\n    136                 raise\n\nc:\\users\\pc\\miniconda3\\lib\\site-packages\\pyspark\\sql\\utils.py in raise_from(e)\n\nParseException: \nmismatched input '&lt;EOF&gt;' expecting {'(', 'COLLECT', 'CONVERT', 'DELTA', 'HISTORY', 'MATCHED', 'MERGE', 'OPTIMIZE', 'SAMPLE', 'TIMESTAMP', 'UPDATE', 'VERSION', 'ZORDER', 'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLONE', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COPY', 'COPY_OPTIONS', 'COST', 'CREATE', 'CREDENTIALS', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DATA', 'DATABASE', DATABASES, 'DAY', 'DBPROPERTIES', 'DEEP', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DROP', 'ELSE', 'ENCRYPTION', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FILES', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMAT_OPTIONS', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PATTERN', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SCHEMA', 'SECOND', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHALLOW', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', '+', '-', '*', 'DIV', '~', STRING, BIGINT_LITERAL, SMALLINT_LITERAL, TINYINT_LITERAL, INTEGER_VALUE, EXPONENT_VALUE, DECIMAL_VALUE, DOUBLE_LITERAL, BIGDECIMAL_LITERAL, IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 34)\n\n== SQL ==\nselect * from test_view where a = \n----------------------------------^^^\n</code></pre>\n<p>So, has anyone managed to make these variables work?</p>\n<p>Thanks</p>",
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_creation_date": "2020-10-28 15:09:00.150000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": null,
		"Question_score": 1,
		"Question_tags": "pyspark|apache-spark-sql|databricks|azure-databricks|databricks-connect",
		"Question_view_count": 859,
		"Owner_creation_date": "2017-08-15 19:30:07.280000 UTC",
		"Owner_last_access_date": "2022-06-23 20:22:00.100000 UTC",
		"Owner_location": "Brazil",
		"Owner_reputation": 11,
		"Owner_up_votes": 0,
		"Owner_down_votes": 0,
		"Owner_views": 8,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "azure",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 72578552,
		"Question_title": "Azure Databricks Clusters: Can you adjust the number of executors/node?",
		"Question_body": "<p><a href=\"https://docs.microsoft.com/en-us/azure/databricks/clusters/configure\" rel=\"nofollow noreferrer\">https://docs.microsoft.com/en-us/azure/databricks/clusters/configure</a></p>\n<p>&quot;Azure Databricks runs one executor per worker node; therefore the terms executor and worker are used interchangeably in the context of the Azure Databricks architecture.&quot;</p>\n<p>Q1 Does that mean no. of executors per node on azure databricks is fixed to 1?</p>\n<p>Q2 Several documentations state 5 cores per executor as a thumb-rule for spark clusters for HDFS throughput? This doesn't seem to be true for Azure databricks.</p>",
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_creation_date": "2022-06-10 18:32:03.207000 UTC",
		"Question_favorite_count": 1.0,
		"Question_last_edit_date": "2022-06-11 07:53:32.443000 UTC",
		"Question_score": 1,
		"Question_tags": "apache-spark|cluster-computing|databricks|azure-databricks",
		"Question_view_count": 256,
		"Owner_creation_date": "2022-06-10 18:23:25.460000 UTC",
		"Owner_last_access_date": "2022-09-23 08:55:32.597000 UTC",
		"Owner_location": null,
		"Owner_reputation": 23,
		"Owner_up_votes": 0,
		"Owner_down_votes": 0,
		"Owner_views": 1,
		"Answer_body": "<p>The Hadoop rules of thumbs aren't applicable for Databricks because in contrast to Hadoop, Databricks doesn't collocate the data with compute, and instead executors are accessing data in cloud storage accounts that have other throughput characteristics compared to the on-prem solutions.</p>\n<p>So take as a granted that each node (except driver node) in the cluster is a single executor with number of cores equal to the number of cores on a single machine.</p>",
		"Answer_comment_count": 0.0,
		"Answer_creation_date": "2022-06-11 07:56:20.980000 UTC",
		"Answer_last_edit_date": null,
		"Answer_score": 2.0,
		"Question_valid_tags": "azure",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 65166372,
		"Question_title": "Acessing Tensorboard on AzureML during training",
		"Question_body": "<p>How to use view Tensorboard during an AzureML training run on in the Cloud.</p>\n<p>Followed this tutorial:\n<a href=\"https://docs.microsoft.com/en-us/azure/machine-learning/how-to-monitor-tensorboard#launch-tensorboard\" rel=\"nofollow noreferrer\">https://docs.microsoft.com/en-us/azure/machine-learning/how-to-monitor-tensorboard#launch-tensorboard</a></p>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.tensorboard import Tensorboard\n\ntb = Tensorboard([run])\n\n# If successful, start() returns a string with the URI of the instance.\ntb.start()\n\n# After your job completes, be sure to stop() the streaming otherwise it will continue to run. \ntb.stop()\n</code></pre>\n<p>The logs contains the url <code>http://localhost:6006/</code> printed by the <code>tb.start()</code> statement.\nThis is somehow silly as it runs in the cloud.</p>\n<p>How to get the full url to the node in the cluster where Tensorboard was launched?</p>",
		"Question_answer_count": 0,
		"Question_comment_count": 0,
		"Question_creation_date": "2020-12-06 09:06:52.760000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": null,
		"Question_score": 1,
		"Question_tags": "tensorboard|azure-machine-learning-service",
		"Question_view_count": 122,
		"Owner_creation_date": "2016-09-22 08:48:50.870000 UTC",
		"Owner_last_access_date": "2022-02-18 09:44:31.300000 UTC",
		"Owner_location": "Germany",
		"Owner_reputation": 975,
		"Owner_up_votes": 52,
		"Owner_down_votes": 0,
		"Owner_views": 122,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "azure",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 56367279,
		"Question_title": "Databricks not updating in SQL query",
		"Question_body": "<p>I am trying to replace special characters from a table column using SQL a SQL query. However, I get the following error. Can anyone tell me what I did wrong or how I should approach this?</p>\n\n<p><strong>SQL QUERY</strong></p>\n\n<pre><code>UPDATE wine SET description = REPLACE(description, '%', '')\n</code></pre>\n\n<p><strong>ERROR</strong></p>\n\n<pre><code>error in sql statement: analysisexception: update destination only supports delta sources.\n</code></pre>",
		"Question_answer_count": 1,
		"Question_comment_count": 1,
		"Question_creation_date": "2019-05-29 19:26:13.220000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": null,
		"Question_score": 2,
		"Question_tags": "python|sql|databricks",
		"Question_view_count": 6730,
		"Owner_creation_date": "2017-02-06 15:01:21.063000 UTC",
		"Owner_last_access_date": "2022-09-23 13:49:49.540000 UTC",
		"Owner_location": "Toronto",
		"Owner_reputation": 1053,
		"Owner_up_votes": 197,
		"Owner_down_votes": 4,
		"Owner_views": 109,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "azure",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 73571376,
		"Question_title": "Method POST from table to API Python Databricks",
		"Question_body": "<p>I have a table in Databricks and I need to write in API this table with the method POST. I would like it.\nHas anyone ever done this kind of API writing with a table?\nWhich parameters API do I need to write in API through a table SQL?\nFor example: I need POST table tab_customer to API.</p>",
		"Question_answer_count": 0,
		"Question_comment_count": 1,
		"Question_creation_date": "2022-09-01 14:49:26.043000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": null,
		"Question_score": 0,
		"Question_tags": "python|sql|api|databricks",
		"Question_view_count": 11,
		"Owner_creation_date": "2020-01-20 14:19:55.853000 UTC",
		"Owner_last_access_date": "2022-09-22 17:49:44.657000 UTC",
		"Owner_location": null,
		"Owner_reputation": 1,
		"Owner_up_votes": 0,
		"Owner_down_votes": 0,
		"Owner_views": 1,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "azure",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 67583631,
		"Question_title": "How to ingest Delta Lake MetaData into Amundsen Data Discovery Engine?",
		"Question_body": "<p>I have setup the Amundsen and the UI Works fine. I am trying to run the sample delta lake loader given in the examples in their repository.</p>\n<pre><code>&quot;&quot;&quot;\nThis is a example script for extracting Delta Lake Metadata Results\n&quot;&quot;&quot;\n\nfrom pyhocon import ConfigFactory\nfrom pyspark.sql import SparkSession\n\nfrom databuilder.extractor.delta_lake_metadata_extractor import DeltaLakeMetadataExtractor\nfrom databuilder.job.job import DefaultJob\nfrom databuilder.loader.file_system_neo4j_csv_loader import FsNeo4jCSVLoader\nfrom databuilder.models.table_metadata import DESCRIPTION_NODE_LABEL\nfrom databuilder.publisher import neo4j_csv_publisher\nfrom databuilder.publisher.neo4j_csv_publisher import Neo4jCsvPublisher\nfrom databuilder.task.task import DefaultTask\n\n# NEO4J cluster endpoints\nNEO4J_ENDPOINT = 'bolt://localhost:7687/'\n\nneo4j_endpoint = NEO4J_ENDPOINT\n\nneo4j_user = 'neo4j'\nneo4j_password = 'test'\ncluster_key = 'my_delta_environment'\ndatabase = 'delta'\n# Or set to empty to do all\nschema_list = ['schema1', 'schema2']\n# Or set to empty list if you don't have any schemas that you don't want to process\nexclude_list = ['bad_schema']\n\n\ndef create_delta_lake_job_config():\n    tmp_folder = '/var/tmp/amundsen/table_metadata'\n    node_files_folder = f'{tmp_folder}/nodes/'\n    relationship_files_folder = f'{tmp_folder}/relationships/'\n\n    job_config = ConfigFactory.from_dict({\n        f'extractor.delta_lake_table_metadata.{DeltaLakeMetadataExtractor.CLUSTER_KEY}': cluster_key,\n        f'extractor.delta_lake_table_metadata.{DeltaLakeMetadataExtractor.DATABASE_KEY}': database,\n        f'extractor.delta_lake_table_metadata.{DeltaLakeMetadataExtractor.SCHEMA_LIST_KEY}': schema_list,\n        f'extractor.delta_lake_table_metadata.{DeltaLakeMetadataExtractor.EXCLUDE_LIST_SCHEMAS_KEY}': exclude_list,\n        f'loader.filesystem_csv_neo4j.{FsNeo4jCSVLoader.NODE_DIR_PATH}': node_files_folder,\n        f'loader.filesystem_csv_neo4j.{FsNeo4jCSVLoader.RELATION_DIR_PATH}': relationship_files_folder,\n        f'publisher.neo4j.{neo4j_csv_publisher.NODE_FILES_DIR}': node_files_folder,\n        f'publisher.neo4j.{neo4j_csv_publisher.RELATION_FILES_DIR}': relationship_files_folder,\n        f'publisher.neo4j.{neo4j_csv_publisher.NEO4J_END_POINT_KEY}': neo4j_endpoint,\n        f'publisher.neo4j.{neo4j_csv_publisher.NEO4J_USER}': neo4j_user,\n        f'publisher.neo4j.{neo4j_csv_publisher.NEO4J_PASSWORD}': neo4j_password,\n        f'publisher.neo4j.{neo4j_csv_publisher.NEO4J_CREATE_ONLY_NODES}': [DESCRIPTION_NODE_LABEL],\n        f'publisher.neo4j.job_publish_tag': 'some_unique_tag'  # TO-DO unique tag must be added\n    })\n    return job_config\n\n\nif __name__ == &quot;__main__&quot;:\n    # This assumes you are running on a spark cluster (for example databricks cluster)\n    # that is configured with a hive metastore that\n    # has pointers to all of your delta tables\n    # Because of this, this code CANNOT run as a normal python operator on airflow.\n    spark = SparkSession.builder.appName(&quot;Amundsen Delta Lake Metadata Extraction&quot;).getOrCreate()\n    job_config = create_delta_lake_job_config()\n    dExtractor = DeltaLakeMetadataExtractor()\n    dExtractor.set_spark(spark)\n    job = DefaultJob(conf=job_config,\n                     task=DefaultTask(extractor=dExtractor, loader=FsNeo4jCSVLoader()),\n                     publisher=Neo4jCsvPublisher())\n    job.launch()\n</code></pre>\n<p>So the above code is the example code given in the Amundsen Repository.</p>\n<p>What should be given at <em>cluster_key</em> and <em>database</em> to connect with my delta lake?</p>\n<p>When I run the code I get no errors but no data is ingested into amundsen.</p>",
		"Question_answer_count": 0,
		"Question_comment_count": 0,
		"Question_creation_date": "2021-05-18 09:35:58.447000 UTC",
		"Question_favorite_count": 4.0,
		"Question_last_edit_date": "2021-10-28 15:48:56.240000 UTC",
		"Question_score": 4,
		"Question_tags": "python|apache-spark|databricks|delta-lake",
		"Question_view_count": 467,
		"Owner_creation_date": "2019-09-06 05:45:01.947000 UTC",
		"Owner_last_access_date": "2021-11-28 12:08:31.720000 UTC",
		"Owner_location": "Coimbatore, Tamil Nadu, India",
		"Owner_reputation": 41,
		"Owner_up_votes": 0,
		"Owner_down_votes": 0,
		"Owner_views": 11,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "azure",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 70304381,
		"Question_title": "Gremlin: sorting nodes by weights provided by the client",
		"Question_body": "<p>Hello dear gremlin jedi,</p>\n<p>I have a bunch of nodes with different labels in my graph:</p>\n<pre><code>  g.addV(&quot;book&quot;).property(:genre, &quot;horror&quot;).as(&quot;b1&quot;)\n   .addV(&quot;book&quot;).property(:genre, &quot;biography&quot;).as(&quot;b2&quot;)\n   .addV(&quot;book&quot;).property(:genre, &quot;sci-fi&quot;).as(&quot;b3&quot;)\n   .addV(&quot;movie&quot;).property(:genre, &quot;biography&quot;).as(&quot;m1&quot;)\n   .addV(&quot;movie&quot;).property(:genre, &quot;sci-fi&quot;).as(&quot;m1&quot;)\n   .addV(&quot;movie&quot;).property(:genre, &quot;horror&quot;).as(&quot;m3&quot;).iterate\n</code></pre>\n<p>And I'd like to sort them by the genre property providing weights for each value from the client, something like:</p>\n<pre><code>WEIGHTS = {\n  &quot;horror&quot;: 3,\n  &quot;biography&quot;: 2,\n  &quot;sci-fi&quot;: 1\n}\n</code></pre>\n<p>So the result in this case should be something like\n<code>[b3, m1, b2, m1, b1, m3]</code></p>\n<p>In case if a vertex has a genre not mentioned in the weights it should not cause an error, such vertices may be simply ignored or put in the end of results.</p>\n<p>The weights might change from query to query, so I can't store them in the graph.</p>\n<p>My idea is to use the <code>withSideEffect</code> configuration step to pass the weights to the server and then somehow use it to sort the vertices:</p>\n<pre><code>g.withSideEffect(&quot;weights&quot;, WEIGHTS)\n .V().order().by(__.select(&quot;weights&quot;).select(__.values(:genre)))\n .toList()\n</code></pre>\n<p>But apparently it's a wrong way because it results with an error:</p>\n<p><code>This traversal parent does not support the replacement of local traversals: org.apache.tinkerpop.gremlin.process.traversal.step.map.TraversalSelectStep</code></p>\n<p>What would be the right way to achieve my goal?</p>\n<p>Thank you!</p>\n<p>PS: if it's important, I use my own gremlin language variant written in Ruby, but it's semantics more or less follows semantics of the official python client.</p>",
		"Question_answer_count": 2,
		"Question_comment_count": 0,
		"Question_creation_date": "2021-12-10 12:17:38.803000 UTC",
		"Question_favorite_count": 1.0,
		"Question_last_edit_date": "2021-12-10 13:13:49.283000 UTC",
		"Question_score": 2,
		"Question_tags": "gremlin|amazon-neptune|gremlin-server",
		"Question_view_count": 92,
		"Owner_creation_date": "2012-07-10 03:29:26.730000 UTC",
		"Owner_last_access_date": "2022-09-23 14:35:04.440000 UTC",
		"Owner_location": "Berlin",
		"Owner_reputation": 133,
		"Owner_up_votes": 3,
		"Owner_down_votes": 0,
		"Owner_views": 9,
		"Answer_body": "<p>You were actually quite close with your original attempt in the question, just missing one more <code>select</code></p>\n<pre class=\"lang-java prettyprint-override\"><code>gremlin&gt; g.withSideEffect(&quot;weights&quot;, [&quot;horror&quot;:3,&quot;sci-fi&quot;:1,&quot;biography&quot;:2]).\n......1&gt;   V().as('a').\n......2&gt;   order().\n......3&gt;     by(select('weights').select(select('a').values('genre'))).\n......4&gt;   elementMap() \n\n==&gt;[id:42780,label:book,genre:sci-fi]\n==&gt;[id:42784,label:movie,genre:sci-fi]\n==&gt;[id:42778,label:book,genre:biography]\n==&gt;[id:42782,label:movie,genre:biography]\n==&gt;[id:42776,label:book,genre:horror]\n==&gt;[id:42786,label:movie,genre:horror]  \n</code></pre>\n<p>By the way I would be very interested to hear more about your Ruby client, especially if it is available as open source.</p>",
		"Answer_comment_count": 1.0,
		"Answer_creation_date": "2021-12-10 23:48:33.443000 UTC",
		"Answer_last_edit_date": null,
		"Answer_score": 1.0,
		"Question_valid_tags": "azure",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 62820211,
		"Question_title": "How do I resolve \"KeyError: 'brand'\" when running an experiment using Azure Automated Machine Learning?",
		"Question_body": "<p>I am using the Azure Automated Machine Learning SDK to train a machine learning model on my dataset. However, after the experiment, all my training iterations fail with a <code>KeyError: 'brand'</code> error even if the model training itself succeeded.</p>\n<p>How can I resolve this?</p>",
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_creation_date": "2020-07-09 17:14:26.923000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": null,
		"Question_score": 2,
		"Question_tags": "azure|azure-machine-learning-service|automl",
		"Question_view_count": 977,
		"Owner_creation_date": "2012-07-05 04:08:24.023000 UTC",
		"Owner_last_access_date": "2021-09-01 12:33:59.740000 UTC",
		"Owner_location": null,
		"Owner_reputation": 716,
		"Owner_up_votes": 2,
		"Owner_down_votes": 0,
		"Owner_views": 50,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "azure",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 44627664,
		"Question_title": "Access files in blob storage in R scripts in Azure Machine Learning?",
		"Question_body": "<p>What is the easy way to access (read and write) files in blob storage in R scripts in Azure Machine Learning?</p>\n\n<p>I can access files in blob storage in python scripts using azure modules, but there seems no easy way to access by R scripts.</p>\n\n<p>I tried to import Azure SMR as a zip file in the R script, but the importing all dependencies is very tough work,</p>\n\n<p><a href=\"https://github.com/Microsoft/AzureSMR\" rel=\"nofollow noreferrer\">https://github.com/Microsoft/AzureSMR</a></p>\n\n<p>Any suggestion and help is appreciated.</p>",
		"Question_answer_count": 2,
		"Question_comment_count": 0,
		"Question_creation_date": "2017-06-19 10:06:52.267000 UTC",
		"Question_favorite_count": 1.0,
		"Question_last_edit_date": null,
		"Question_score": 3,
		"Question_tags": "r|azure|azure-blob-storage|azure-machine-learning-studio",
		"Question_view_count": 3803,
		"Owner_creation_date": "2017-06-16 06:45:45.243000 UTC",
		"Owner_last_access_date": "2022-09-23 12:05:18.207000 UTC",
		"Owner_location": null,
		"Owner_reputation": 49,
		"Owner_up_votes": 7,
		"Owner_down_votes": 0,
		"Owner_views": 13,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "azure",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 60112022,
		"Question_title": "Is it possible to use Writestream directly to an API via spark",
		"Question_body": "<p>I build a code on Databricks to read a delta table in realtime (readstream) and then i need post this stream data to an API.<br>\nIn all paper that I read, writestream is used only to create files (.csv, .avro, .parquet, etc) or sent to an Event Hub. Is possible to use writestream to post to an API!?</p>\n\n<p>My code:</p>\n\n<pre><code>from pyspark.sql.functions import unix_timestamp, round, col\nimport json\nimport pandas as pd\nfrom pyspark.sql.functions import lit\nimport requests\n\n#tried with foreach_batch but it doens't work\ndef foreach_batch_function(df,epochId):\n    r2 = requests.post('https://demo.api.com/index.php/api/v5/smsrequest/', data=str(df), verify=False)\n    r2.json()\n    pass\n\nrs = spark.readStream.format(\"delta\").option('path','/mnt/gen2/raw/mytable').load()\ndf = rs.select(round('id_cliente_fat').alias('id_cliente_fat'),'fone_fat','nome_fat',unix_timestamp('dt_nasc_fat','YYYY-MM-DD').cast('timestamp').cast('date').alias('birth_date'),'email_fat')\n\ndf2 = df.selectExpr('id_cliente_fat as identifier_code','fone_fat as phone_number','nome_fat as name','birth_date','email_fat as email')\n\ndata = {'authentication':{'username':'user','password':'pass'}}\nr = requests.post('https://demo.api.com/index.php/api/v5/login/', data=json.dumps(data), verify=False).json()\n\ndf3 = df2.withColumn(\"steps\", lit(\"[1,2,4,7]\")).withColumn(\"place_id\", lit(164)).withColumn(\"token\", lit(r[\"authentication\"][\"token\"]))\n\ndf4 = df3.select(to_json(struct(struct(\"token\").alias(\"authentication\"), struct(\"identifier_code\", \"phone_number\", \"name\", \"birth_date\", \"email\",\"steps\",\"place_id\").alias(\"smsrequest\").alias(\"smsrequest\"))).alias(\"\"))\n\ndf4.writeStream.foreachBatch(foreach_batch_function).start() \n\n</code></pre>",
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_creation_date": "2020-02-07 10:54:20.080000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": "2020-02-07 23:06:51.830000 UTC",
		"Question_score": -1,
		"Question_tags": "apache-spark|pyspark|spark-streaming|azure-databricks",
		"Question_view_count": 200,
		"Owner_creation_date": "2020-02-07 10:44:56.323000 UTC",
		"Owner_last_access_date": "2021-05-04 12:32:27.187000 UTC",
		"Owner_location": null,
		"Owner_reputation": 1,
		"Owner_up_votes": 0,
		"Owner_down_votes": 0,
		"Owner_views": 0,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "azure",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 15056327,
		"Question_title": "How do I synchronise two remote Git repositories?",
		"Question_body": "<p>I have two repository urls, and I want to synchronise them such that they both contain the same thing. In Mercurial, what I'm trying to do would be:</p>\n\n<pre><code>hg pull {repo1}\nhg pull {repo2}\nhg push -f {repo1}\nhg push -f {repo2}\n</code></pre>\n\n<p>This will result in two heads in both repos (I know it's not common to have two heads, but I'm doing this for synchornisation and it needs to be non-interactive. The heads will be merged manually from one of the repos and then the sync run again).</p>\n\n<p>I'd like to do the same thing in Git. Eg., with no user interaction, get all of the changes into both repos, with multiple branches/heads/whatever to be merged later.\nI'm trying to do this using urls in the commands, rather than adding remotes(?), as there could be a number of repos involved, and having aliases for them all will just make my script more complicated.</p>\n\n<p>I'm currently cloning the repo using <code>git clone --bar {repo1}</code> however I'm struggling to \"update\" it. I've tried <code>get fetch {repo1}</code> but that doesn't seem to pull my changes down; <code>git log</code> still doesn't show the changeset that has been added in repo1.</p>\n\n<p>I also tried using <code>--mirror</code> in my <code>push</code> and <code>clone</code>, but that seemed to remote changesets from repo2 that didn't exist locally, whereas I need to keep changes from both repos :/</p>\n\n<p>What's the best way to do this?</p>\n\n<p><strong>Edit:</strong> To make it a little clearer what I'm trying to do...</p>\n\n<p>I have two repositories (eg. BitBucket and GitHub) and want people to be able to push to either (ultimately, one will be Git, one will be Mercurial, but let's assume they're both Git for now to simplify things). I need to be able to run a script that will \"sync\" the two repos in a way that they both contain both sets of changes, and may require merging manually later.</p>\n\n<p>Eventually, this means I can just interact with one of the repos (eg. the Mercurial one), and my script will periodically pull in Git changes which I can merge in, and then they'll be pushed back.</p>\n\n<p>In Mercurial this is <em>trivial</em>! I just pull from both repos, and push with <code>-f/--force</code> to allow pushing multiple heads. Then anybody can clone one of the repos, merge the heads, and push back. I want to know how to do the closest similar thing in Git. It must be 100% non-interactive, and must keep both repos in a state that the process can be repeated infinitely (that means no rewriting history/changing changesets etc).</p>",
		"Question_answer_count": 6,
		"Question_comment_count": 0,
		"Question_creation_date": "2013-02-24 20:33:38.440000 UTC",
		"Question_favorite_count": 11.0,
		"Question_last_edit_date": "2013-02-24 21:08:23.193000 UTC",
		"Question_score": 27,
		"Question_tags": "git|version-control|github|dvcs",
		"Question_view_count": 58999,
		"Owner_creation_date": "2008-10-04 15:14:46.937000 UTC",
		"Owner_last_access_date": "2022-09-23 10:30:28.980000 UTC",
		"Owner_location": "England, United Kingdom",
		"Owner_reputation": 37590,
		"Owner_up_votes": 550,
		"Owner_down_votes": 88,
		"Owner_views": 2584,
		"Answer_body": "<p>Git branches do not have \"heads\" in the Mercurial sense.  There is only one thing called <code>HEAD</code>, and it's effectively a symlink to the commit you currently have checked out.  In the case of hosted repositories like GitHub, there <em>is no</em> commit checked out\u2014there's just the repository history itself.  (Called a \"bare\" repo.)</p>\n\n<p>The reason for this difference is that Git branch names are completely arbitrary; they don't have to match between copies of a repository, and you can create and destroy them on a whim.[1]  Git branches are like Python variable names, which can be shuffled around and stuck to any value as you like; Mercurial branches are like C variables, which refer to fixed preallocated memory locations you then fill with data.</p>\n\n<p>So when you pull in Mercurial, you have two histories for the same branch, because the branch name is a fixed meaningful thing in both repositories.  The leaf of each history is a \"head\", and you'd normally merge them to create a single head.</p>\n\n<p>But in Git, fetching a remote branch doesn't actually affect your branch at all.  If you fetch the <code>master</code> branch from <code>origin</code>, it just goes into a branch called <code>origin/master</code>.[2]  <code>git pull origin master</code> is just thin sugar for two steps: fetching the remote branch into <code>origin/master</code>, and then merging that other branch into your current branch.  But they don't have to have the same name; your branch could be called <code>development</code> or <code>trunk</code> or whatever else.  You can pull or merge any other branch into it, and you can push it to any other branch.  Git doesn't care.</p>\n\n<p>Which brings me back to your problem: you can't push a \"second\" branch head to a remote Git repository, because the concept doesn't exist.  You <em>could</em> push to branches with mangled names (<code>bitbucket_master</code>?), but as far as I'm aware, you can't update a remote's remotes remotely.</p>\n\n<p>I don't think your plan makes a lot of sense, though, since with unmerged branches exposed to both repositories, you'd either have to merge them both, or you'd merge one and then mirror it on top of the other...  in which case you left the second repository in a useless state for no reason.</p>\n\n<p>Is there a reason you can't just do this:</p>\n\n<ol>\n<li><p>Pick a repository to be canonical\u2014I assume BitBucket.  Clone it.  It becomes <code>origin</code>.</p></li>\n<li><p>Add the other repository as a remote called, say, <code>github</code>.</p></li>\n<li><p>Have a simple script periodically fetch both remotes and attempt to merge the <code>github</code> branch(es) into the <code>origin</code> branches.  If the merge fails, abort and send you an email or whatever.  If the merge is trivial, push the result to both remotes.</p></li>\n</ol>\n\n<p>Of course, if you just do all your work on feature branches, this all becomes much less of a problem.  :)</p>\n\n<hr>\n\n<p>[1] It gets even better: you can merge together branches from different repositories that have <em>no history whatsoever</em> in common.  I've done this to consolidate projects that were started separatedly; they used different directory structures, so it works fine.  GitHub uses a similar trick for its Pages feature: the history of your Pages is stored in a branch called <code>gh-pages</code> that lives in the same repository but has absolutely no history in common with the rest of your project.</p>\n\n<p>[2] This is a white lie.  The branch is still called <code>master</code>, but it belongs to the remote called <code>origin</code>, and the slash is syntax for referring to it.  The distinction can matter because Git has no qualms about slashes in branch names, so you could have a local branch named <code>origin/master</code>, and that would shadow the remote branch.</p>",
		"Answer_comment_count": 9.0,
		"Answer_creation_date": "2013-02-24 22:33:15.687000 UTC",
		"Answer_last_edit_date": "2013-02-25 02:21:02.693000 UTC",
		"Answer_score": 30.0,
		"Question_valid_tags": "azure",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 72855973,
		"Question_title": "Data model tool to connect to Databricks or Data lake",
		"Question_body": "<p>From data modeling documentation (Dimensional/ ER Diagram) is there any tool available which can connect to databricks/ data lake and read the table structure directly and also updates the structure of table whenever there is a addition or deletions of columns in a table?</p>\n<p>And in a process, it should not remove the relationship made between tables whenever there is an update to a columns and/ or tables (addition/ deletion). And version control on same will be helpful using GIT etc.</p>\n<p>Reason being I understand the PK and FK details are not maintained in datalake/ databricks tables entities. Request to please propose if any modeling tools are present for this use case.</p>",
		"Question_answer_count": 0,
		"Question_comment_count": 0,
		"Question_creation_date": "2022-07-04 11:28:32.950000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": "2022-07-04 19:24:18.620000 UTC",
		"Question_score": 0,
		"Question_tags": "azure|databricks|data-modeling|entity-relationship|azure-data-lake",
		"Question_view_count": 23,
		"Owner_creation_date": "2018-04-02 12:36:12.777000 UTC",
		"Owner_last_access_date": "2022-08-11 12:26:48.380000 UTC",
		"Owner_location": "Bengaluru, Karnataka, India",
		"Owner_reputation": 15,
		"Owner_up_votes": 0,
		"Owner_down_votes": 0,
		"Owner_views": 5,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "azure",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 73170090,
		"Question_title": "Apache Spark Federated Views / Queries",
		"Question_body": "<p>Does Spark (Specifically Spark 3.X) support the idea of federated queries via persisted views or tables?  We do a lot of the &quot;JDBC to other sources&quot; stuff in our ETL, so I am familiar with how to make a JDBC connection and even create a temporary view of a JDBC connection, if needed.  I am more interested in persisting that table or view within the metastore, so I can query it as a live connection, for as long as I want.  Something similar to how Presto or Denodo does it.</p>\n<p>We do most of our stuff in Azure Databricks also, if anyone knows if they offer something that vanilla Spark would not.</p>\n<p>Anyone know if that is possible? I haven't had much luck finding anything yet.</p>\n<p>Thanks!</p>",
		"Question_answer_count": 0,
		"Question_comment_count": 0,
		"Question_creation_date": "2022-07-29 19:17:02.097000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": "2022-07-29 19:42:40.903000 UTC",
		"Question_score": 0,
		"Question_tags": "azure|apache-spark|databricks",
		"Question_view_count": 50,
		"Owner_creation_date": "2018-01-09 21:33:20.187000 UTC",
		"Owner_last_access_date": "2022-09-24 21:48:39.410000 UTC",
		"Owner_location": null,
		"Owner_reputation": 11,
		"Owner_up_votes": 0,
		"Owner_down_votes": 0,
		"Owner_views": 1,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "azure",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	}
]