[
	{
		"Question_id": 63861801,
		"Question_title": "Running R on Sagemaker timeout",
		"Question_body": "<p>I'm running R script on Sagemaker using Batch Transform Job and feature bring your own docker. Process works fine for smaller datasets, but when I try to do run bigger ones, job fails after 40 minutes with following error: &quot;model server did not respond to /invocations request within 600 seconds&quot;. Cloudwatch logs shows CPU is 100% utilized and memory below 10%. It seems like it can not respond to ping. Is there any way we can override this 600 seconds to some higher value? Or is there any way to limit CPU utilization of running container?</p>",
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_creation_date": "2020-09-12 15:15:00.537000 UTC",
		"Question_favorite_count": 1.0,
		"Question_last_edit_date": "2020-09-25 11:18:14.503000 UTC",
		"Question_score": 1,
		"Question_tags": "r|amazon-web-services|docker|amazon-sagemaker|inference",
		"Question_view_count": 391,
		"Owner_creation_date": "2016-09-14 07:13:43.713000 UTC",
		"Owner_last_access_date": "2022-07-27 15:19:19.907000 UTC",
		"Owner_location": "Belgrade",
		"Owner_reputation": 353,
		"Owner_up_votes": 23,
		"Owner_down_votes": 1,
		"Owner_views": 66,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "sagemaker",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 73624056,
		"Question_title": "AWS EventBridge triggers SageMaker pipeline successfully but says failure",
		"Question_body": "<p>I have a SageMaker pipeline set up to transform audio data and run a model training script.  We are using custom containers for both parts if that matters.  The pipeline runs end to end when I test it in SageMaker with no issues.</p>\n<p>I set up an EventBridge rule to trigger the pipeline overnight at regular intervals.  This triggers the pipeline successfully, but in my invocations dashboard it appears that the jobs fail.</p>\n<p>I'm using the default event bus.  The execution role has all Read, Write, and List on my pipeline's ARN.</p>\n<p>While the pipelines appear to run successfully, it would be nice if I could rely on the dashboards in the EventBridge console.  Is there anywhere else I should be looking to debug?</p>\n<p><a href=\"https://i.stack.imgur.com/uP4Lt.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/uP4Lt.png\" alt=\"Failed Invocations\" /></a></p>\n<p><a href=\"https://i.stack.imgur.com/SsKDI.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/SsKDI.png\" alt=\"SageMaker Pipelines are Successful\" /></a></p>",
		"Question_answer_count": 0,
		"Question_comment_count": 2,
		"Question_creation_date": "2022-09-06 14:53:50.593000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": null,
		"Question_score": 0,
		"Question_tags": "amazon-web-services|amazon-sagemaker|aws-event-bridge",
		"Question_view_count": 20,
		"Owner_creation_date": "2013-08-16 20:23:45.670000 UTC",
		"Owner_last_access_date": "2022-09-23 21:13:36.407000 UTC",
		"Owner_location": null,
		"Owner_reputation": 1105,
		"Owner_up_votes": 658,
		"Owner_down_votes": 2,
		"Owner_views": 222,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "sagemaker",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 61809661,
		"Question_title": "How to edit manifest when specifying multiple sources for one task with AWS Sagemaker ground truth",
		"Question_body": "<p>I would like to create a task to have one worker perform labeling of multiple sound sources with AWS Sagemaker ground truth.\nI created a manifest file as follows, but I cannot specify multiple sound sources with source-ref.\nHow to create a manifest file?</p>\n\n<p>dataset.manifest</p>\n\n<pre><code> {\"source-ref\":[\"s3://sagemaker-sample/audio_01.wav\", \"s3://sagemaker-sample/audio_02.wav\"]}\n</code></pre>\n\n<p>Error</p>\n\n<blockquote>\n  <p>ClientError: Manifest: s3://sagemaker-sample/dataset.manifest has\n  invalid format at line number 0. Make sure that source or source-ref\n  field contains a string value</p>\n</blockquote>",
		"Question_answer_count": 1,
		"Question_comment_count": 2,
		"Question_creation_date": "2020-05-15 00:25:25.090000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": null,
		"Question_score": 0,
		"Question_tags": "json|amazon-web-services|amazon-sagemaker|crowdsourcing",
		"Question_view_count": 470,
		"Owner_creation_date": "2020-05-15 00:20:44.930000 UTC",
		"Owner_last_access_date": "2022-07-21 08:55:09.810000 UTC",
		"Owner_location": null,
		"Owner_reputation": 3,
		"Owner_up_votes": 0,
		"Owner_down_votes": 0,
		"Owner_views": 3,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "sagemaker",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 56263995,
		"Question_title": "Does the library multprocesing works on sagemaker conda-python3?",
		"Question_body": "<p>Does the library multprocesing works well on sagemaker conda-python3?</p>",
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_creation_date": "2019-05-22 19:49:24.157000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": null,
		"Question_score": 0,
		"Question_tags": "python-3.x|python-multiprocessing|amazon-sagemaker",
		"Question_view_count": 723,
		"Owner_creation_date": "2016-03-19 19:45:07.413000 UTC",
		"Owner_last_access_date": "2022-08-25 22:03:06.273000 UTC",
		"Owner_location": "Ecuador, Quito",
		"Owner_reputation": 617,
		"Owner_up_votes": 74,
		"Owner_down_votes": 0,
		"Owner_views": 117,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "sagemaker",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 73739892,
		"Question_title": "ClientError: An error occurred (ValidationException) when calling the CreateHyperParameterTuningJob operation:The objective metric :[mse]",
		"Question_body": "<p>Getting error: <strong>ClientError: An error occurred (ValidationException) when calling the CreateHyperParameterTuningJob operation: The objective metric for the hyperparameter tuning job, [mse], isn\u2019t valid for the [720646828776.dkr.ecr.ap-south-1.amazonaws.com/sagemaker-xgboost:0.90-2-cpu-py3] algorithm. Choose a valid objective metric.</strong></p>\n<pre><code>import datetime\nimport time\nimport tarfile    \nimport boto3\nimport pandas as pd\nimport numpy as np\nfrom sagemaker import get_execution_role\nimport sagemaker\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import fetch_california_housing\nfrom sagemaker.tuner import (\n    IntegerParameter,\n    CategoricalParameter,\n    ContinuousParameter,\n    HyperparameterTuner,\n)\n\ns3 = boto3.client(&quot;s3&quot;)\nsm_boto3 = boto3.client(&quot;sagemaker&quot;)\n\nsagemaker_session = sagemaker.Session()\n\nregion = sess.boto_session.region_name\n\nrole = get_execution_role()\n#Set the required configurations\nmodel_name = &quot;abc_model&quot;\nenv = &quot;dev&quot;\n#S3 Bucket\nbucket = &quot;abcpoc&quot;\nprint(&quot;Using bucket &quot; + bucket)\n\n\nfrom sagemaker.debugger import Rule, rule_configs\nfrom sagemaker.session import TrainingInput\n\ns3_input_train = TrainingInput(\n    s3_data=f&quot;s3://{default_bucket}/train/&quot;,content_type=&quot;csv&quot;)\ns3_input_validation = TrainingInput(\n    s3_data=f&quot;s3://{default_bucket}/validation/&quot;,content_type=&quot;csv&quot;)\nprefix = 'output'\n\ncontainer=sagemaker.image_uris.retrieve(&quot;xgboost&quot;, region, &quot;1.2-1&quot;)\nprint(container)\nxgb = sagemaker.estimator.Estimator(\n    image_uri=container,\n    role=role,\n    base_job_name=&quot;xgboost-random-search&quot;,\n    instance_count=1,\n    instance_type=&quot;ml.m4.xlarge&quot;,\n    output_path=&quot;s3://{}/{}/output&quot;.format(bucket, prefix),\n    sagemaker_session= sagemaker.Session(),\n    rules=[Rule.sagemaker(rule_configs.create_xgboost_report())]\n)\n\n\nxgb.set_hyperparameters(\n    max_depth = 5,\n    eta = 0.2,\n    gamma = 4,\n    min_child_weight = 6,\n    subsample = 0.7,\n    objective = &quot;reg:squarederror&quot;,\n    num_round = 1000\n)\nhyperparameter_ranges = {\n    &quot;eta&quot;: ContinuousParameter(0, 1),\n    &quot;min_child_weight&quot;: ContinuousParameter(1, 10),\n    &quot;alpha&quot;: ContinuousParameter(0, 2),\n    &quot;max_depth&quot;: IntegerParameter(1, 10),\n}\n\nobjective_metric_name = &quot;mse&quot;\nmetric_definitions = [{&quot;Name&quot;: &quot;mse&quot;, &quot;Regex&quot;: &quot;mse: ([0-9\\\\.]+)&quot;}]\n\ntuner = HyperparameterTuner(estimator, \n                    objective_metric_name, \n                    hyperparameter_ranges, \n                    metric_definitions=None, \n                    strategy='Bayesian', \n                    objective_type='Maximize', \n                    max_jobs=1, \n                    max_parallel_jobs=1, \n                    tags=None, \n                    base_tuning_job_name=None)\n\n#Tune\ntuner.fit({\n    &quot;train&quot;:s3_input_train,\n    &quot;validation&quot;:s3_input_validation\n    },include_cls_metadata=False)\n\n#Explore the best model generated\ntuning_job_result = boto3.client(&quot;sagemaker&quot;).describe_hyper_parameter_tuning_job(\n    HyperParameterTuningJobName=tuner.latest_tuning_job.job_name\n)\n\njob_count = tuning_job_result[&quot;TrainingJobStatusCounters&quot;][&quot;Completed&quot;]\nprint(&quot;%d training jobs have completed&quot; %job_count)\n#10 training jobs have completed\n\n#Get the best training job\n\nfrom pprint import pprint\nif tuning_job_result.get(&quot;BestTrainingJob&quot;,None):\n    print(&quot;Best Model found so far:&quot;)\n    pprint(tuning_job_result[&quot;BestTrainingJob&quot;])\nelse:\n    print(&quot;No training jobs have reported results yet.&quot;)\n</code></pre>",
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_creation_date": "2022-09-16 04:34:17.860000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": null,
		"Question_score": 0,
		"Question_tags": "amazon-web-services|amazon-s3|amazon-sagemaker",
		"Question_view_count": 20,
		"Owner_creation_date": "2014-09-26 13:42:23.827000 UTC",
		"Owner_last_access_date": "2022-09-23 08:45:56.160000 UTC",
		"Owner_location": null,
		"Owner_reputation": 21,
		"Owner_up_votes": 0,
		"Owner_down_votes": 0,
		"Owner_views": 14,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "sagemaker",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 65664382,
		"Question_title": "SageMaker cannot create first user",
		"Question_body": "<p>I need to create first user to getting start, but Amazon won't let me.</p>\n<blockquote>\n<p>ResourceLimitExceeded The account-level service limit 'Maximum number\nof user profiles per domain' is 0 UserProfiles, with current\nutilization of 0 UserProfiles and a request delta of 1 UserProfiles.\nPlease contact AWS support to request an increase for this limit.</p>\n</blockquote>\n<p>I cannot get pass the quickstart guide.</p>\n<p><a href=\"https://i.stack.imgur.com/Twa8Q.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/Twa8Q.png\" alt=\"enter image description here\" /></a></p>\n<p>How should I fix ? What did I miss ?</p>",
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_creation_date": "2021-01-11 09:39:48.797000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": null,
		"Question_score": 1,
		"Question_tags": "amazon-sagemaker",
		"Question_view_count": 378,
		"Owner_creation_date": "2015-08-21 06:48:55.517000 UTC",
		"Owner_last_access_date": "2022-09-25 03:29:15.820000 UTC",
		"Owner_location": "Ho Chi Minh City, Vietnam",
		"Owner_reputation": 4748,
		"Owner_up_votes": 4859,
		"Owner_down_votes": 15,
		"Owner_views": 596,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "sagemaker",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 73514544,
		"Question_title": "NVMe SSD disk on AWS Sagemaker Notebook Instances",
		"Question_body": "<p>Sagemake Studio added support for instances with attached high-performance SSDs. Among others, the M5d instance features an attached SSD. (See, for instance here <a href=\"https://aws.amazon.com/about-aws/whats-new/2021/08/amazon-sagemaker-supports-m5d-r5-p3dn-g4dn-instances-sagemaker-notebook-instances/\" rel=\"nofollow noreferrer\">AWS Blog: Amazon SageMaker now supports M5d, R5, P3dn, and G4dn instances for SageMaker Notebook Instances</a>)</p>\n<p>My question is, how do I make use of the SSD?</p>\n<p>The block device is not mounted when I start a Notebook with a Kernel on an M5d instance.\nBelow is a listing of the mount command form inside the notebook. I can see it when executing <code>lsblk</code>. (output is also below). However, there is no way I can format or mount the disk as the environment is restricted by Linux capabilities.</p>\n<pre><code>!mount\n\noverlay on / type overlay (rw,relatime,lowerdir=/var/lib/dataroot/data-root/200010.1001/overlay2/l/JGWGMF4ZZ6PHEWK6JM4VICT7E4:/var/lib/dataroot/data-root/200010.1001/overlay2/l/66JWDG6GVXACNLSFJJBEOVZUO4:/var/lib/dataroot/data-root/200010.1001/overlay2/l/3U4YS24OOOKWXXZ7GZ3N4YOJZU:/var/lib/dataroot/data-root/200010.1001/overlay2/l/XOM2F2QYQYJMEUDYRFWJF637G2:/var/lib/dataroot/data-root/200010.1001/overlay2/l/4SSLKXEKJRW2M3MDUY6GRWQZOR:/var/lib/dataroot/data-root/200010.1001/overlay2/l/MDSB4ZKIAUXBZ3U5S4YM4DDOLQ:/var/lib/dataroot/data-root/200010.1001/overlay2/l/NE42TLPPST4P26YJZXL7KQYD5V:/var/lib/dataroot/data-root/200010.1001/overlay2/l/ZQA5ST7XAT673MNTWDL4YWBHLL:/var/lib/dataroot/data-root/200010.1001/overlay2/l/UZYHAWYHUINESI25LGXT5V2RHY:/var/lib/dataroot/data-root/200010.1001/overlay2/l/QECAGRX3FEW5FAJXK5DACNEFKA:/var/lib/dataroot/data-root/200010.1001/overlay2/l/IBZBCWE3M7DNAUXEOAHQUCM5HG:/var/lib/dataroot/data-root/200010.1001/overlay2/l/VQMIS7KW6ZB5VOQNKQE3J2RF4J:/var/lib/dataroot/data-root/200010.1001/overlay2/l/UOWDHH53G2E442U6NNUMBITSRJ,upperdir=/var/lib/dataroot/data-root/200010.1001/overlay2/9b5b78d144189e2437ee3a1716d2215d6a101155c2b35f9e8c0c3b1004b627fd/diff,workdir=/var/lib/dataroot/data-root/200010.1001/overlay2/9b5b78d144189e2437ee3a1716d2215d6a101155c2b35f9e8c0c3b1004b627fd/work)\nproc on /proc type proc (rw,nosuid,nodev,noexec,relatime)\ntmpfs on /dev type tmpfs (rw,nosuid,size=65536k,mode=755,uid=200010,gid=1001)\ndevpts on /dev/pts type devpts (rw,nosuid,noexec,relatime,gid=10000004,mode=620,ptmxmode=666)\nsysfs on /sys type sysfs (ro,nosuid,nodev,noexec,relatime)\ntmpfs on /sys/fs/cgroup type tmpfs (rw,nosuid,nodev,noexec,relatime,mode=755,uid=200010,gid=1001)\ncgroup on /sys/fs/cgroup/systemd type cgroup (rw,nosuid,nodev,noexec,relatime,xattr,release_agent=/usr/lib/systemd/systemd-cgroups-agent,name=systemd)\ncgroup on /sys/fs/cgroup/blkio type cgroup (rw,nosuid,nodev,noexec,relatime,blkio)\ncgroup on /sys/fs/cgroup/net_cls,net_prio type cgroup (rw,nosuid,nodev,noexec,relatime,net_cls,net_prio)\ncgroup on /sys/fs/cgroup/cpuset type cgroup (rw,nosuid,nodev,noexec,relatime,cpuset)\ncgroup on /sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,memory)\ncgroup on /sys/fs/cgroup/perf_event type cgroup (rw,nosuid,nodev,noexec,relatime,perf_event)\ncgroup on /sys/fs/cgroup/cpu,cpuacct type cgroup (rw,nosuid,nodev,noexec,relatime,cpu,cpuacct)\ncgroup on /sys/fs/cgroup/freezer type cgroup (rw,nosuid,nodev,noexec,relatime,freezer)\ncgroup on /sys/fs/cgroup/hugetlb type cgroup (rw,nosuid,nodev,noexec,relatime,hugetlb)\ncgroup on /sys/fs/cgroup/pids type cgroup (rw,nosuid,nodev,noexec,relatime,pids)\ncgroup on /sys/fs/cgroup/devices type cgroup (rw,nosuid,nodev,noexec,relatime,devices)\nmqueue on /dev/mqueue type mqueue (rw,nosuid,nodev,noexec,relatime)\nshm on /dev/shm type tmpfs (rw,nosuid,nodev,noexec,relatime,size=65536k,uid=200010,gid=1001)\n127.0.0.1:/200010 on /root type nfs4 (rw,relatime,vers=4.1,rsize=1048576,wsize=1048576,namlen=255,hard,noresvport,proto=tcp,port=20486,timeo=600,retrans=2,sec=sys,clientaddr=127.0.0.1,local_lock=none,addr=127.0.0.1)\n/dev/nvme0n1p1 on /opt/.sagemakerinternal type xfs (rw,noatime,attr2,inode64,usrquota,prjquota)\n/dev/nvme0n1p1 on /etc/resolv.conf type xfs (rw,noatime,attr2,inode64,usrquota,prjquota)\n/dev/nvme0n1p1 on /etc/hostname type xfs (rw,noatime,attr2,inode64,usrquota,prjquota)\n/dev/nvme0n1p1 on /etc/hosts type xfs (rw,noatime,attr2,inode64,usrquota,prjquota)\n/dev/nvme0n1p1 on /var/log/studio type xfs (rw,noatime,attr2,inode64,usrquota,prjquota)\n/dev/nvme0n1p1 on /var/log/apps type xfs (rw,noatime,attr2,inode64,usrquota,prjquota)\n/dev/nvme0n1p1 on /opt/ml/metadata/resource-metadata.json type xfs (ro,noatime,attr2,inode64,usrquota,prjquota)\ndevtmpfs on /dev/null type devtmpfs (rw,nosuid,size=16042360k,nr_inodes=4010590,mode=755)\ndevtmpfs on /dev/random type devtmpfs (rw,nosuid,size=16042360k,nr_inodes=4010590,mode=755)\ndevtmpfs on /dev/full type devtmpfs (rw,nosuid,size=16042360k,nr_inodes=4010590,mode=755)\ndevtmpfs on /dev/tty type devtmpfs (rw,nosuid,size=16042360k,nr_inodes=4010590,mode=755)\ndevtmpfs on /dev/zero type devtmpfs (rw,nosuid,size=16042360k,nr_inodes=4010590,mode=755)\ndevtmpfs on /dev/urandom type devtmpfs (rw,nosuid,size=16042360k,nr_inodes=4010590,mode=755)\nproc on /proc/bus type proc (ro,nosuid,nodev,noexec,relatime)\nproc on /proc/fs type proc (ro,nosuid,nodev,noexec,relatime)\nproc on /proc/irq type proc (ro,nosuid,nodev,noexec,relatime)\nproc on /proc/sys type proc (ro,nosuid,nodev,noexec,relatime)\nproc on /proc/sysrq-trigger type proc (ro,nosuid,nodev,noexec,relatime)\ntmpfs on /proc/acpi type tmpfs (ro,relatime,uid=200010,gid=1001)\ndevtmpfs on /proc/kcore type devtmpfs (rw,nosuid,size=16042360k,nr_inodes=4010590,mode=755)\ndevtmpfs on /proc/keys type devtmpfs (rw,nosuid,size=16042360k,nr_inodes=4010590,mode=755)\ndevtmpfs on /proc/latency_stats type devtmpfs (rw,nosuid,size=16042360k,nr_inodes=4010590,mode=755)\ndevtmpfs on /proc/timer_list type devtmpfs (rw,nosuid,size=16042360k,nr_inodes=4010590,mode=755)\ndevtmpfs on /proc/sched_debug type devtmpfs (rw,nosuid,size=16042360k,nr_inodes=4010590,mode=755)\ntmpfs on /sys/firmware type tmpfs (ro,relatime,uid=200010,gid=1001)\n</code></pre>\n<pre><code>!lsblk\n\nNAME          MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT\nnvme1n1       259:0    0 279.4G  0 disk \nnvme0n1       259:1    0   124G  0 disk \n\u251c\u2500nvme0n1p1   259:2    0   124G  0 part /opt/ml/metadata/resource-metadata.json\n\u2514\u2500nvme0n1p128 259:3    0     1M  0 part \n</code></pre>",
		"Question_answer_count": 0,
		"Question_comment_count": 1,
		"Question_creation_date": "2022-08-27 21:28:17.233000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": "2022-08-27 22:56:19.970000 UTC",
		"Question_score": 0,
		"Question_tags": "amazon-sagemaker",
		"Question_view_count": 43,
		"Owner_creation_date": "2016-11-18 12:14:15.223000 UTC",
		"Owner_last_access_date": "2022-09-16 21:21:44.413000 UTC",
		"Owner_location": null,
		"Owner_reputation": 101,
		"Owner_up_votes": 24,
		"Owner_down_votes": 0,
		"Owner_views": 13,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "sagemaker",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 70825456,
		"Question_title": "Unable to create a notebook instance on Amazon SageMaker",
		"Question_body": "<p>I'm trying to create a notebook instance, but I'm getting the following error:</p>\n<pre><code>AccessDeniedException\nUser: arn:was:...::......:user/{MY_USERNAME} is not authorized to perform: sagemaker:ListNotebookInstanceLifecycleConfigs because no identity-based policy allows the sagemaker:ListNotebookInstanceLifecycleConfigs action\n</code></pre>\n<p>How to resolve it? Is this a permission issue?</p>",
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_creation_date": "2022-01-23 18:47:03.423000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": "2022-01-23 19:19:26.597000 UTC",
		"Question_score": -1,
		"Question_tags": "amazon-web-services|amazon-sagemaker",
		"Question_view_count": 316,
		"Owner_creation_date": "2021-10-10 10:51:29.833000 UTC",
		"Owner_last_access_date": "2022-07-22 21:13:47.017000 UTC",
		"Owner_location": null,
		"Owner_reputation": 47,
		"Owner_up_votes": 2,
		"Owner_down_votes": 0,
		"Owner_views": 3,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "sagemaker",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 62093200,
		"Question_title": "Using Youtube Player API inside Mechanical Turk crowd-form doesn't fire OnStateChange",
		"Question_body": "<p>I am trying to get a youtube iframe inside  to put on amazon mechanical Turk. </p>\n\n<p>However, if the youtube player <code>&lt;div&gt;</code> is placed inside  it doesn't fire the event \"onStateChange\". </p>\n\n<p>Here is a minimal code to reproduce. In this case, OnStateChange doesn't fire when video is paused:</p>\n\n<pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n  &lt;body&gt;\n  &lt;script src=\"https://assets.crowd.aws/crowd-html-elements.js\"&gt;&lt;/script&gt;\n    &lt;crowd-form&gt;\n    &lt;div id=\"player\"&gt;&lt;/div&gt;  &lt;!-- This doesn't print YOLO on StateChange--&gt;\n    &lt;/crowd-form&gt;\n\n    &lt;script&gt;\n      // 2. This code loads the IFrame Player API code asynchronously.\n      var tag = document.createElement('script');\n\n      tag.src = \"https://www.youtube.com/iframe_api\";\n      var firstScriptTag = document.getElementsByTagName('script')[0];\n      firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);\n\n      // 3. This function creates an &lt;iframe&gt; (and YouTube player)\n      //    after the API code downloads.\n      var player;\n      function onYouTubeIframeAPIReady() {\n    player = new YT.Player('player', {\n      height: '390',\n      width: '640',\n      videoId: 'M7lc1UVf-VE',\n      events: {\n        'onStateChange': onPlayerStateChange\n      }\n    });\n      }\n\n      // 5. The API calls this function when the player's state changes.\n      //    The function indicates that when playing a video (state=1),\n      //    the player should play for six seconds and then stop.\n      var done = false;\n      function onPlayerStateChange(event) {\n    console.log('Yolo')\n      }\n\n    &lt;/script&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre>\n\n<p>In this case OnStateChange does fire when video is paused, but I need it inside  to put on mechanical Turk.</p>\n\n<pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n  &lt;body&gt;\n  &lt;script src=\"https://assets.crowd.aws/crowd-html-elements.js\"&gt;&lt;/script&gt;\n    &lt;div id=\"player\"&gt;&lt;/div&gt; &lt;!-- This does print YOLO on StateChange--&gt;\n    &lt;crowd-form&gt;\n    &lt;/crowd-form&gt;\n\n    &lt;script&gt;\n      // 2. This code loads the IFrame Player API code asynchronously.\n      var tag = document.createElement('script');\n\n      tag.src = \"https://www.youtube.com/iframe_api\";\n      var firstScriptTag = document.getElementsByTagName('script')[0];\n      firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);\n\n      // 3. This function creates an &lt;iframe&gt; (and YouTube player)\n      //    after the API code downloads.\n      var player;\n      function onYouTubeIframeAPIReady() {\n    player = new YT.Player('player', {\n      height: '390',\n      width: '640',\n      videoId: 'M7lc1UVf-VE',\n      events: {\n        'onStateChange': onPlayerStateChange\n      }\n    });\n      }\n\n      // 5. The API calls this function when the player's state changes.\n      //    The function indicates that when playing a video (state=1),\n      //    the player should play for six seconds and then stop.\n      var done = false;\n      function onPlayerStateChange(event) {\n    console.log('Yolo')\n      }\n\n    &lt;/script&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre>\n\n<p>I was wondering why this is happening and how this could be resolved.</p>\n\n<p>Thanks.</p>",
		"Question_answer_count": 0,
		"Question_comment_count": 1,
		"Question_creation_date": "2020-05-29 19:17:10.760000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": null,
		"Question_score": 2,
		"Question_tags": "javascript|amazon-sagemaker|youtube-iframe-api|mechanicalturk",
		"Question_view_count": 91,
		"Owner_creation_date": "2020-05-29 17:41:11.733000 UTC",
		"Owner_last_access_date": "2021-09-23 12:53:09.890000 UTC",
		"Owner_location": null,
		"Owner_reputation": 21,
		"Owner_up_votes": 1,
		"Owner_down_votes": 0,
		"Owner_views": 3,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "sagemaker",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 70290180,
		"Question_title": "How to install python-opencv in amazon-sagemaker?",
		"Question_body": "<p>I installed the OpenCV package by <code>pip install opencv-python</code>.</p>\n<p>When I import cv2 in my code.</p>\n<pre class=\"lang-py prettyprint-override\"><code>import cv2\n</code></pre>\n<p>I got the following error:</p>\n<pre><code>\n---------------------------------------------------------------------------\nImportError                               Traceback (most recent call last)\n/tmp/ipykernel_324/571303353.py in &lt;module&gt;\n----&gt; 1 import cv2\n\n~/.conda/envs/default/lib/python3.9/site-packages/cv2/__init__.py in &lt;module&gt;\n      6 import sys\n      7 \n----&gt; 8 from .cv2 import *\n      9 from .cv2 import _registerMatType\n     10 from . import mat_wrapper\n\nImportError: libgthread-2.0.so.0: cannot open shared object file: No such file or directory\n</code></pre>\n<p><a href=\"https://askubuntu.com/a/1072878\">Here</a> are some solutions for this error but seemly need root permission.</p>\n<pre><code>apt-get update -y\napt-get install libglib2.0-0\n</code></pre>\n<p>When I run these commands in the terminal I got the following errors.</p>\n<pre><code>(studiolab) studio-lab-user@default:~/sagemaker-studiolab-notebooks/vit/ViT-pytorch$ apt-get update -y\nReading package lists... Done\nE: List directory /var/lib/apt/lists/partial is missing. - Acquire (13: Permission denied)\n</code></pre>",
		"Question_answer_count": 3,
		"Question_comment_count": 0,
		"Question_creation_date": "2021-12-09 12:48:04.583000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": "2022-07-08 17:13:54.780000 UTC",
		"Question_score": 3,
		"Question_tags": "python|opencv|amazon-sagemaker",
		"Question_view_count": 964,
		"Owner_creation_date": "2017-01-09 11:46:05.637000 UTC",
		"Owner_last_access_date": "2022-08-28 04:09:22.313000 UTC",
		"Owner_location": null,
		"Owner_reputation": 389,
		"Owner_up_votes": 28,
		"Owner_down_votes": 0,
		"Owner_views": 48,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "sagemaker",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 54249334,
		"Question_title": "ImportError libopenblasp-r0 cannot open shared object file No such file or directory -SageMaker",
		"Question_body": "<p>I am trying train my model code using Docker Container - AWS SageMaker using following code.</p>\n\n<pre>\n    'https://github.com/awslabs/amazon-sagemaker- \nexamples/blob/master/advanced_functionality/scikit_bring_your_own/scikit_bring_your_own.ipynb'\n</pre>\n\n<p>But I get below error when I Try to train my model using </p>\n\n<pre><code> tree.fit(data_location)\n</code></pre>\n\n<p>Error:</p>\n\n<pre><code>  Traceback (most recent call last):\n    File \"/opt/program/train\", line 17, in &lt;module&gt;\n     from sklearn import tree\n    File \"/usr/local/lib/python2.7/dist-packages/sklearn/__init__.py\", line \n      64, in &lt;module&gt;\n     from .base import clone\n    File \"/usr/local/lib/python2.7/dist-packages/sklearn/base.py\", line \n       13, in &lt;module&gt;\n      from .utils.fixes import signature\n    File \"/usr/local/lib/python2.7/dist- \n       packages/sklearn/utils/__init__.py\", line 16, in &lt;module&gt;\n      from .fixes import _Sequence as Sequence\n    File \"/usr/local/lib/python2.7/dist-packages/sklearn/utils/fixes.py\", \n       line 85, in &lt;module&gt;\n      from scipy.special import boxcox  # noqa\n    File \"/usr/local/lib/python2.7/dist- \n       packages/scipy/special/__init__.py\", line 641, in &lt;module&gt;\n       from ._ufuncs import *\n    ImportError: libopenblasp-r0-8dca6697.3.0.dev.so: cannot open shared \n       object file: No such file or directory\n</code></pre>\n\n<p>error message 2</p>\n\n<pre><code>   Error for Training job decision-trees-sample-2019-01-18-07-44-37-282: Failed Reason: AlgorithmError: Exit Code: 1\n</code></pre>\n\n<p>I wend to the directory and did not find 'sklearn' directory.</p>\n\n<pre><code>  sh-4.2$ pwd\n    /usr/local/lib/python2.7/dist-packages\n  sh-4.2$ ls -l\n    total 3244\n  -rwxr-xr-x 1 root root 3318568 Sep 18 03:23 cv2.so\n</code></pre>\n\n<p>My current jupyter notebook points to root environment and it has sklearn package available , not sure how make it available in above location where I see error, not sure if this is what will resolve the issue  or something else needs to be done.</p>\n\n<p>I am new to Amazon SageMaker.</p>\n\n<p>Expected result: I am expecting the training job to complete without error</p>",
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_creation_date": "2019-01-18 07:19:02.457000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": "2019-01-18 07:49:29.223000 UTC",
		"Question_score": 1,
		"Question_tags": "python|python-2.7|docker|jupyter-notebook|amazon-sagemaker",
		"Question_view_count": 1661,
		"Owner_creation_date": "2014-12-31 03:46:45.110000 UTC",
		"Owner_last_access_date": "2022-09-13 06:55:27.303000 UTC",
		"Owner_location": "Pune, Maharashtra, India",
		"Owner_reputation": 1050,
		"Owner_up_votes": 749,
		"Owner_down_votes": 10,
		"Owner_views": 168,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "sagemaker",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 53640440,
		"Question_title": "How to use boto3 cloudwatch for SageMaker submitted training jobs?",
		"Question_body": "<p>I have submitted a few training jobs from AWS SageMaker. I want to use boto3 cloudwatch api to fetch the cloudwatch data to be displayed within jupyter notebook instead of using CloudWatch UI. </p>",
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_creation_date": "2018-12-05 20:43:57.003000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": null,
		"Question_score": 0,
		"Question_tags": "amazon-web-services|boto3|amazon-cloudwatch|amazon-sagemaker",
		"Question_view_count": 588,
		"Owner_creation_date": "2017-02-28 23:26:56.977000 UTC",
		"Owner_last_access_date": "2019-03-28 02:25:59.547000 UTC",
		"Owner_location": null,
		"Owner_reputation": 3,
		"Owner_up_votes": 0,
		"Owner_down_votes": 0,
		"Owner_views": 8,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "sagemaker",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 72759570,
		"Question_title": "Kernel restarts when training a sklearn regression model in Sagemaker",
		"Question_body": "<p>I have been trying to train a regression model, with big data on AWS Sagemaker.</p>\n<p>The instance I used on my last try was ml.m5.12xlarge and I was confident it will work this time, but no. I still get the error.</p>\n<p>After some minutes in the training I get this error on Cloudwatch:</p>\n<pre><code>[E 07:00:35.308 NotebookApp] KernelRestarter: restart callback &lt;bound method ZMQChannelsHandler.on_kernel_restarted of ZMQChannelsHandler(f92aff37-be6b-48df-a5f5-522bcc6dd072)&gt; failed\nTraceback (most recent call last):\n  File &quot;/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.7/site-packages/jupyter_client/restarter.py&quot;, line 86, in _fire_callbacks\n    callback()\n  File &quot;/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.7/site-packages/notebook/services/kernels/handlers.py&quot;, line 473, in on_kernel_restarted\n    self._send_status_message('restarting')\n  File &quot;/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.7/site-packages/notebook/services/kernels/handlers.py&quot;, line 469, in _send_status_message\n    self.write_message(json.dumps(msg, default=date_default))\n  File &quot;/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.7/site-packages/tornado/websocket.py&quot;, line 337, in write_message\n    raise WebSocketClosedError()\ntornado.websocket.WebSocketClosedError\n</code></pre>\n<p>Does anyone might know what the error could be?</p>",
		"Question_answer_count": 0,
		"Question_comment_count": 2,
		"Question_creation_date": "2022-06-26 07:10:18.473000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": null,
		"Question_score": 0,
		"Question_tags": "scikit-learn|regression|amazon-sagemaker|dask-ml",
		"Question_view_count": 47,
		"Owner_creation_date": "2020-04-17 01:31:21.620000 UTC",
		"Owner_last_access_date": "2022-09-24 12:53:17.577000 UTC",
		"Owner_location": "Caracas, Venezuela",
		"Owner_reputation": 113,
		"Owner_up_votes": 25,
		"Owner_down_votes": 0,
		"Owner_views": 25,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "sagemaker",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 67123040,
		"Question_title": "How to tell programmatically that an AWS Step Function execution has been completed?",
		"Question_body": "<p>I am triggering a Step Function execution via a Python cell in a SageMaker Notebook, like this:</p>\n<pre><code>state_machine_arn = 'arn:aws:states:us-west-1:1234567891:stateMachine:alexanderMyPackageStateMachineE3411O13-A1vQWERTP9q9'\nsfn = boto3.client('stepfunctions')\n..\nsfn.start_execution(**kwargs)  # Non Blocking Call\nrun_arn = response['executionArn']\nprint(f&quot;Started run {run_name}. ARN is {run_arn}.&quot;)\n</code></pre>\n<p>and then in order to check that the execution (which might take hours to complete depending on the input) has been completed, before I start doing some custom post-analysis on the results, I manually execute a cell with:</p>\n<pre><code>response = sfn.list_executions(\n    stateMachineArn=state_machine_arn,\n    maxResults=1\n)\nprint(response)\n</code></pre>\n<p>where I can see from the output the status of the execution, e.g. <code>'status': 'RUNNING'</code>.</p>\n<p>How can I automate this, i.e. trigger the Step Function and continue the execution on my post-analysis custom logic only after the execution has finished? Is there for example a blocking call to start the execution, or a callback method I could use?</p>\n<p>I can think of putting a sleep method, so that the Python Notebook cell would periodically call <code>list_executions()</code> and check the status, and only when the execution is completed, continue to rest of the code. I can statistically determine the sleep period, but I was wondering if there is a simpler/more accurate way.</p>\n<hr />\n<p>PS: Related: <a href=\"https://stackoverflow.com/questions/46878423/how-to-avoid-simultaneous-execution-in-aws-step-function\">How to avoid simultaneous execution in aws step function</a>, however I would like to avoid creating any new AWS resource, just for this, I would like to do everything from within the Notebook.</p>\n<p>PPS: I cannot make any change to <code>MyPackage</code> and the Step Function definition.</p>",
		"Question_answer_count": 1,
		"Question_comment_count": 2,
		"Question_creation_date": "2021-04-16 09:53:03.027000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": "2021-04-16 14:57:49.787000 UTC",
		"Question_score": 4,
		"Question_tags": "python|amazon-web-services|asynchronous|amazon-sagemaker|aws-step-functions",
		"Question_view_count": 1216,
		"Owner_creation_date": "2013-05-22 21:25:42.213000 UTC",
		"Owner_last_access_date": "2022-09-21 16:01:33.950000 UTC",
		"Owner_location": "London, UK",
		"Owner_reputation": 70285,
		"Owner_up_votes": 7595,
		"Owner_down_votes": 12100,
		"Owner_views": 13121,
		"Answer_body": "<p>Based on the comments.</p>\n<p>If no new resources are to be created (no CloudWatch Event rules, lambda functions) nor any changes to existing Step Function are allowed, then <strong>pooling iteratively</strong> <code>list_executions</code> would be the best solution.</p>\n<p>AWS CLI and boto3 have implemented similar solutions (not for Step Functions), but for some other services. They are called <code>waiters</code> (e.g. <a href=\"https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/ec2.html#waiters\" rel=\"nofollow noreferrer\">ec2 waiters</a>). So basically you would have to create your own <strong>waiter for Step Function</strong>, as AWS does not provide one for that. AWS uses <strong>15 seconds</strong> sleep time from what I recall for its waiters.</p>",
		"Answer_comment_count": 0.0,
		"Answer_creation_date": "2021-04-16 10:18:25.233000 UTC",
		"Answer_last_edit_date": null,
		"Answer_score": 2.0,
		"Question_valid_tags": "sagemaker",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 65545350,
		"Question_title": "How to create hyperparameter tuning job with custom docker container on Amazon Sagemaker?",
		"Question_body": "<p>I created a custom docker container to run Catboost on Amazon Sagemaker, followed this demo (<a href=\"https://github.com/aws-samples/sagemaker-byo-catboost-container-demo/blob/master/Catboost_container_for_SageMaker.ipynb\" rel=\"nofollow noreferrer\">https://github.com/aws-samples/sagemaker-byo-catboost-container-demo/blob/master/Catboost_container_for_SageMaker.ipynb</a>). I now want to do hyperparameter tuning with this custom container, but this is not a built-in or pre-built Sagemaker container, so I am not sure if I could or how to create hyperparameter tuning job on Sagemaker with a custom container. I didn't find any official documentation or official examples about using custom docker container to do HYT.</p>\n<p>So my question is: how to create hyperparameter tuning with a custom container on Amazon Sagemaker?</p>",
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_creation_date": "2021-01-02 23:51:32.080000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": null,
		"Question_score": 4,
		"Question_tags": "amazon-web-services|docker|amazon-sagemaker",
		"Question_view_count": 358,
		"Owner_creation_date": "2021-01-02 23:03:09.943000 UTC",
		"Owner_last_access_date": "2021-03-09 20:23:20.670000 UTC",
		"Owner_location": "Kansas City, MO, USA",
		"Owner_reputation": 41,
		"Owner_up_votes": 0,
		"Owner_down_votes": 0,
		"Owner_views": 3,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "sagemaker",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 72687114,
		"Question_title": "Cannot import librosa on SageMaker Jupyter notebook instance \"OSError: sndfile library not found\"",
		"Question_body": "<p>I am trying to import librosa on a SageMaker notebook instance but it's telling me that the sndfile library is not found. I have tried conda install -c conda-forge libsndfile but it is not working. I have been stuck on this for almost 3 hours now. Would appreciate some help. Thank you.</p>\n<p>UPDATE (NOW WORKING):\nThis is what ended up working.. we had to compile the libsndfile from scrach using the following commands</p>\n<pre><code>%%bash\nwget 'https://github.com/libsndfile/libsndfile/releases/download/1.0.31/libsndfile-1.0.31.tar.bz2'\ntar -xf libsndfile-1.0.31.tar.bz2\ncd libsndfile-1.0.31/\n./configure\nmake\nsudo make install\n</code></pre>",
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_creation_date": "2022-06-20 12:37:51.537000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": "2022-06-20 13:38:32.530000 UTC",
		"Question_score": 0,
		"Question_tags": "python|python-3.x|amazon-web-services|amazon-sagemaker|librosa",
		"Question_view_count": 180,
		"Owner_creation_date": "2022-06-20 12:29:28.083000 UTC",
		"Owner_last_access_date": "2022-06-29 08:59:30.167000 UTC",
		"Owner_location": null,
		"Owner_reputation": 1,
		"Owner_up_votes": 0,
		"Owner_down_votes": 0,
		"Owner_views": 1,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "sagemaker",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 65482767,
		"Question_title": "How to create a hyperparameter tuning step in SageMaker pipeline?",
		"Question_body": "<p>I am trying to use the latest SageMaker Python SDK (v2.23.0) to implement a SageMaker pipeline that includes a hyperparameter tuning job. However I didn't see anything in module sagemaker.workflow.steps or sagemaker.workflow.step_collections that I can use. There is a TrainingStep class but it's not for HPO.</p>\n<p>Is this not supported at this time?</p>",
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_creation_date": "2020-12-28 19:09:53.660000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": null,
		"Question_score": 0,
		"Question_tags": "amazon-sagemaker|hyperparameters|mlops",
		"Question_view_count": 286,
		"Owner_creation_date": "2014-01-15 20:09:46.960000 UTC",
		"Owner_last_access_date": "2021-09-22 15:40:52.590000 UTC",
		"Owner_location": null,
		"Owner_reputation": 1,
		"Owner_up_votes": 0,
		"Owner_down_votes": 0,
		"Owner_views": 1,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "sagemaker",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 50669991,
		"Question_title": "AWS SageMaker is not authorized to perform: ecr:CreateRepository on resource: *",
		"Question_body": "<p>I am creating my own Docker image so that I can use my own models in AWS SageMaker. I sucessfully created a Docker image using command line inside the Jupyter Notebook in SageMaker ml.t2.medium instance using a customized Dockerfile:</p>\n\n<pre><code>REPOSITORY            TAG                 IMAGE ID            CREATED             SIZE\nsklearn               latest              01234212345        6 minutes ago       1.23GB\n</code></pre>\n\n<p>But when I run in Jupyter:</p>\n\n<pre><code>! aws ecr create-repository --repository-name sklearn\n</code></pre>\n\n<p>I get the following error:</p>\n\n<pre><code>An error occurred (AccessDeniedException) when calling the CreateRepository operation: User: arn:aws:sts::1234567:assumed-role/AmazonSageMaker-ExecutionRole-12345/SageMaker is not authorized to perform: ecr:CreateRepository on resource: *\n</code></pre>\n\n<p>I already set up SageMaker, EC2, EC2ContainerService permissions and the following policy for EC2Container but I still get the same error.</p>\n\n<pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"sagemaker:*\",\n        \"ec2:*\"\n      ],\n      \"Resource\": \"*\"\n    }\n  ]\n}\n</code></pre>\n\n<p>Any idea on how I can solve this issue?</p>\n\n<p>Thanks in advance.</p>",
		"Question_answer_count": 1,
		"Question_comment_count": 3,
		"Question_creation_date": "2018-06-03 19:06:16.997000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": "2018-06-03 19:46:45.113000 UTC",
		"Question_score": 1,
		"Question_tags": "amazon-web-services|docker|scikit-learn|dockerfile|amazon-sagemaker",
		"Question_view_count": 4509,
		"Owner_creation_date": "2016-09-29 20:35:09.097000 UTC",
		"Owner_last_access_date": "2022-09-25 02:50:38.173000 UTC",
		"Owner_location": "Brazil",
		"Owner_reputation": 4242,
		"Owner_up_votes": 735,
		"Owner_down_votes": 15,
		"Owner_views": 421,
		"Answer_body": "<p>I solved the problem. We must set a permission at SageMaker Execution Role as following:</p>\n\n<pre><code>{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n    {\n        \"Effect\": \"Allow\",\n        \"Action\": [\n            \"ecr:*\"            ],\n        \"Resource\": \"*\"\n    }\n]}\n</code></pre>",
		"Answer_comment_count": 2.0,
		"Answer_creation_date": "2018-06-04 15:31:01.857000 UTC",
		"Answer_last_edit_date": null,
		"Answer_score": 5.0,
		"Question_valid_tags": "sagemaker",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 58535527,
		"Question_title": "Amazon Sagemaker ResourceLimitExceeded Error for XGBoost (Free Tier)",
		"Question_body": "<p>I am trying to create an XGBoost model in free tier AWS Sagemaker. I am getting an error of:</p>\n\n<p><em>\"ResourceLimitExceeded: An error occurred (ResourceLimitExceeded) when calling the CreateEndpoint operation: The account-level service limit 'ml.m5.xlarge for endpoint usage' is 0 Instances, with current utilization of 0 Instances and a request delta of 1 Instances.\"</em>.</p>\n\n<p>What is the right train_instance_type I should use?</p>\n\n<p>Here is my code:</p>\n\n<pre><code># import libraries\nimport boto3, re, sys, math, json, os, sagemaker, urllib.request\nfrom sagemaker import get_execution_role\nimport numpy as np                                \nimport pandas as pd                               \nimport matplotlib.pyplot as plt                   \nfrom IPython.display import Image                 \nfrom IPython.display import display               \nfrom time import gmtime, strftime                 \nfrom sagemaker.predictor import csv_serializer   \n\n# Define IAM role\nrole = get_execution_role()\nprefix = 'sagemaker/DEMO-xgboost-dm'\ncontainers = {'us-west-2': '433757028032.dkr.ecr.us-west-2.amazonaws.com/xgboost:latest',\n              'us-east-1': '811284229777.dkr.ecr.us-east-1.amazonaws.com/xgboost:latest',\n              'us-east-2': '825641698319.dkr.ecr.us-east-2.amazonaws.com/xgboost:latest',\n              'eu-west-1': '685385470294.dkr.ecr.eu-west-1.amazonaws.com/xgboost:latest'} # each region has its XGBoost container\nmy_region = boto3.session.Session().region_name # set the region of the instance\n\n# Create an instance of the XGBoost model (an estimator), and define the model\u2019s hyperparameters.\n# Note: train_instance_type='ml.m5.large' has 0 free credits! Use one of https://aws.amazon.com/sagemaker/pricing/ \nsess = sagemaker.Session()\nxgb = sagemaker.estimator.Estimator(containers[my_region],role, train_instance_count=1, train_instance_type='ml.m5.xlarge',output_path='s3://{}/{}/output'.format('my_s3_bucket', prefix),sagemaker_session=sess)\nxgb.set_hyperparameters(max_depth=1,eta=0.2,gamma=4,min_child_weight=6,subsample=0.8,silent=0,objective='binary:logistic',num_round=100)\n# Train the model using gradient optimization on a ml.m4.xlarge instance\n# After a few minutes, you should start to see the training logs being generated.\nxgb.fit({'train': s3_input_train})\n</code></pre>\n\n<p>At this step this is what I see:</p>\n\n<pre><code>2019-10-22 06:32:51 Starting - Starting the training job...\n2019-10-22 06:33:00 Starting - Launching requested ML instances......\n2019-10-22 06:33:54 Starting - Preparing the instances for training...\n2019-10-22 06:34:41 Downloading - Downloading input data...\n2019-10-22 06:35:22 Training - Training image download completed. Training in progress..Arguments: train\n[2019-10-22:06:35:22:INFO] Running standalone xgboost training.\n[2019-10-22:06:35:22:INFO] Path /opt/ml/input/data/validation does not exist!\n[2019-10-22:06:35:22:INFO] File size need to be processed in the node: 3.38mb. Available memory size in the node: 8089.9mb\n[2019-10-22:06:35:22:INFO] Determined delimiter of CSV input is ','\n[06:35:22] S3DistributionType set as FullyReplicated\n[06:35:22] 28831x59 matrix with 1701029 entries loaded from /opt/ml/input/data/train?format=csv&amp;label_column=0&amp;delimiter=,\n[06:35:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[0]#011train-error:0.102182\n[06:35:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[1]#011train-error:0.102182\n[06:35:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[2]#011train-error:0.102182\n[06:35:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[3]#011train-error:0.102182\n[06:35:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[4]#011train-error:0.102182\n[06:35:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[5]#011train-error:0.102182\n[06:35:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[6]#011train-error:0.102182\n[06:35:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[7]#011train-error:0.10839\n[06:35:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[8]#011train-error:0.102737\n[06:35:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[9]#011train-error:0.107697\n</code></pre>\n\n<p>And then when I deploy this:</p>\n\n<pre><code># Deploy the model on a server and create an endpoint that you can access\nxgb_predictor = xgb.deploy(initial_instance_count=1,instance_type='ml.m5.xlarge')\n---------------------------------------------------------------------------\nResourceLimitExceeded                     Traceback (most recent call last)\n&lt;ipython-input-38-6d149f3edc98&gt; in &lt;module&gt;()\n      1 # Deploy the model on a server and create an endpoint that you can access\n----&gt; 2 xgb_predictor = xgb.deploy(initial_instance_count=1,instance_type='ml.m5.xlarge')\n\n~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/estimator.py in deploy(self, initial_instance_count, instance_type, accelerator_type, endpoint_name, use_compiled_model, update_endpoint, wait, model_name, kms_key, **kwargs)\n    559             tags=self.tags,\n    560             wait=wait,\n--&gt; 561             kms_key=kms_key,\n    562         )\n    563 \n\n~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/model.py in deploy(self, initial_instance_count, instance_type, accelerator_type, endpoint_name, update_endpoint, tags, kms_key, wait)\n    464         else:\n    465             self.sagemaker_session.endpoint_from_production_variants(\n--&gt; 466                 self.endpoint_name, [production_variant], tags, kms_key, wait\n    467             )\n    468 \n\n~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/session.py in endpoint_from_production_variants(self, name, production_variants, tags, kms_key, wait)\n   1361 \n   1362             self.sagemaker_client.create_endpoint_config(**config_options)\n-&gt; 1363         return self.create_endpoint(endpoint_name=name, config_name=name, tags=tags, wait=wait)\n   1364 \n   1365     def expand_role(self, role):\n\n~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/session.py in create_endpoint(self, endpoint_name, config_name, tags, wait)\n    975 \n    976         self.sagemaker_client.create_endpoint(\n--&gt; 977             EndpointName=endpoint_name, EndpointConfigName=config_name, Tags=tags\n    978         )\n    979         if wait:\n\n~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/client.py in _api_call(self, *args, **kwargs)\n    355                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n    356             # The \"self\" in this scope is referring to the BaseClient.\n--&gt; 357             return self._make_api_call(operation_name, kwargs)\n    358 \n    359         _api_call.__name__ = str(py_operation_name)\n\n~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/client.py in _make_api_call(self, operation_name, api_params)\n    659             error_code = parsed_response.get(\"Error\", {}).get(\"Code\")\n    660             error_class = self.exceptions.from_code(error_code)\n--&gt; 661             raise error_class(parsed_response, operation_name)\n    662         else:\n    663             return parsed_response\n\nResourceLimitExceeded: An error occurred (ResourceLimitExceeded) when calling the CreateEndpoint operation: The account-level service limit 'ml.m5.xlarge for endpoint usage' is 0 Instances, with current utilization of 0 Instances and a request delta of 1 Instances. Please contact AWS support to request an increase for this limit.\n</code></pre>\n\n<p><strong>Edit:</strong> Trying <strong>ml.m4.xlarge</strong> instance:</p>\n\n<p>When I use ml.m4.xlarge, I get the same message of \"ResourceLimitExceeded: An error occurred (ResourceLimitExceeded) when calling the CreateEndpoint operation: The account-level service limit 'ml.m4.xlarge for endpoint usage' is 0 Instances, with current utilization of 0 Instances and a request delta of 1 Instances. Please contact AWS support to request an increase for this limit.\"</p>",
		"Question_answer_count": 3,
		"Question_comment_count": 0,
		"Question_creation_date": "2019-10-24 06:40:58.980000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": "2019-10-25 11:45:47.573000 UTC",
		"Question_score": 3,
		"Question_tags": "python|amazon-web-services|boto3|amazon-sagemaker",
		"Question_view_count": 5276,
		"Owner_creation_date": "2016-06-27 19:09:37.583000 UTC",
		"Owner_last_access_date": "2022-06-15 16:52:21.103000 UTC",
		"Owner_location": "Dallas, TX, United States",
		"Owner_reputation": 4281,
		"Owner_up_votes": 374,
		"Owner_down_votes": 160,
		"Owner_views": 681,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "sagemaker",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 68682085,
		"Question_title": "Assume Sagemaker Notebook instance role from Docker container with default network mode",
		"Question_body": "<p>I have an interesting use case and a problem.</p>\n<p>We are leveraging <strong>Sagemaker Notebooks</strong> as a development environment for our data science teams. These notebooks are essentially EC2 instances with a (relatively) nice IDE (not as good as Cloud9, though).</p>\n<p>In addition, we are running some docker containers on these instances. However, we are forced to use <code>--network=host</code> mode, otherwise, the role assigned to the Notebook Instance is not assumed inside the docker container.</p>\n<p>On the host (here <code>1234567890</code> is our account number, and <code>DataScientist</code> is the role attached to the Sagemaker Notebook instance):</p>\n<pre><code>$ aws sts get-caller-identity\n{\n    &quot;UserId&quot;: &quot;AROAU2P5VGYMMVxxxxxJ:SageMaker&quot;,\n    &quot;Account&quot;: &quot;1234567890&quot;,\n    &quot;Arn&quot;: &quot;arn:aws:sts::1234567890:assumed-role/DataScientist/SageMaker&quot;\n}\n</code></pre>\n<p>Running the same command inside a Docker container with <code>--network=host</code> produces the same result:</p>\n<pre><code>$ docker run --network host amazon/aws-cli sts get-caller-identity\n{\n    &quot;UserId&quot;: &quot;AROAU2P5VGYMMVxxxxxJ:SageMaker&quot;,\n    &quot;Account&quot;: &quot;1234567890&quot;,\n    &quot;Arn&quot;: &quot;arn:aws:sts::1234567890:assumed-role/DataScientist/SageMaker&quot;\n}\n</code></pre>\n<p>However, it doesn't work with Docker <code>--network=bridge</code>:</p>\n<pre><code>$ docker run amazon/aws-cli sts get-caller-identity\n{\n    &quot;UserId&quot;: &quot;AROAIMGPPFPT5T6N7BYX6:i-0b2a9080d5ed1cb98&quot;,\n    &quot;Account&quot;: &quot;366152344081&quot;,\n    &quot;Arn&quot;: &quot;arn:aws:sts::366152344081:assumed-role/BaseNotebookInstanceEc2InstanceRole/i-0b2a9080d5ed1cb98&quot;\n}\n</code></pre>\n<p>As you can see, it's a completely different role being assumed. Notice the account number 366152344081 and the role ARN - it's sth internal to AWS.</p>\n<p>We would like to keep the default networking option for Docker (bridge) and at the same time be able to assume the correct role (the one attached to SageMaker Notebook instance e.g. <code>DataScientist</code> in our case) attached to the host system (Sagemaker Notebook). Are there any hacks (e.g. iptable rules, etc.) to achieve that?</p>",
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_creation_date": "2021-08-06 13:06:25.970000 UTC",
		"Question_favorite_count": 1.0,
		"Question_last_edit_date": "2021-12-10 12:39:06.700000 UTC",
		"Question_score": 0,
		"Question_tags": "amazon-web-services|docker|amazon-iam|amazon-sagemaker",
		"Question_view_count": 195,
		"Owner_creation_date": "2013-11-15 15:40:39.387000 UTC",
		"Owner_last_access_date": "2022-09-24 12:00:52.630000 UTC",
		"Owner_location": "Ljubljana, Slovenia",
		"Owner_reputation": 2470,
		"Owner_up_votes": 910,
		"Owner_down_votes": 11,
		"Owner_views": 285,
		"Answer_body": "<p>If we look at the networks that were created on a clean Sagemaker Notebook instance, we can notice a user-defined bridge network named <code>sagemaker-local</code>:</p>\n<pre class=\"lang-sh prettyprint-override\"><code>$ docker network ls\nNETWORK ID          NAME                DRIVER              SCOPE\nf1d5a59a8c9e        bridge              bridge              local\n6142e6764495        host                host                local\n194adfb00f0a        none                null                local\n99de6c086aa8        sagemaker-local     bridge              local\n</code></pre>\n<p>If we then attach to this custom bridge, we will be able to assume the correct role (the one attached to the Sagemaker Notebook instance itself):</p>\n<pre class=\"lang-sh prettyprint-override\"><code>$ docker run --network sagemaker-local amazon/aws-cli sts get-caller-identity\n{\n    &quot;UserId&quot;: &quot;AROAU2P5VGYMMVxxxxxJ:SageMaker&quot;,\n    &quot;Account&quot;: &quot;1234567890&quot;,\n    &quot;Arn&quot;: &quot;arn:aws:sts::1234567890:assumed-role/DataScientist/SageMaker&quot;\n}\n</code></pre>\n<hr />\n<p><strong>UPDATE</strong></p>\n<p>As of this writing (10 Dec 2021) you don't need to attach to <code>sagemaker-local</code> bridge network anymore, the default <code>bridge</code> will work as well (note <code>--network bridge</code> is implicit in this call):</p>\n<pre class=\"lang-sh prettyprint-override\"><code>$ docker run amazon/aws-cli sts get-caller-identity\n{\n    &quot;UserId&quot;: &quot;AROAU2P5VGYMMVxxxxxJ:SageMaker&quot;,\n    &quot;Account&quot;: &quot;1234567890&quot;,\n    &quot;Arn&quot;: &quot;arn:aws:sts::1234567890:assumed-role/DataScientist/SageMaker&quot;\n}\n</code></pre>\n<p>Make sure you restart your SageMaker Notebook instance.</p>\n<p>Also, <a href=\"https://github.com/aws/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/mxnet_gluon_mnist/setup.sh\" rel=\"nofollow noreferrer\">here</a> I found some manual patching (iptables etc.), but with the update it's already patched.</p>\n<p>Thanks to AWS who fixed this :)</p>",
		"Answer_comment_count": 0.0,
		"Answer_creation_date": "2021-12-08 08:24:37.413000 UTC",
		"Answer_last_edit_date": "2021-12-10 10:14:48.447000 UTC",
		"Answer_score": 0.0,
		"Question_valid_tags": "sagemaker",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 71611419,
		"Question_title": "Mixing shell variables and python variables in IPython '!command'",
		"Question_body": "<p>Trying to figure out whether this behaviour on IPython (v7.12.0, on Amazon SageMaker) is a bug or I'm missing some proper way / documented constraint...</p>\n<p>Say I have some Python variables like:</p>\n<pre class=\"lang-py prettyprint-override\"><code>NODE_VER = &quot;v16.14.2&quot;\nNODE_DISTRO = &quot;linux-x64&quot;\n</code></pre>\n<p>These commands both work fine in a notebook:</p>\n<pre class=\"lang-sh prettyprint-override\"><code>!echo $PATH\n# Shows **contents of system path**\n!echo /usr/local/lib/nodejs/node-{NODE_VER}-{NODE_DISTRO}/bin:\n# Shows /usr/local/lib/nodejs/node-v16.14.2-linux-x64/bin\n</code></pre>\n<p>...But this does not:</p>\n<pre class=\"lang-sh prettyprint-override\"><code>!echo /usr/local/lib/nodejs/node-{NODE_VER}-{NODE_DISTRO}/bin:$PATH\n# Shows:\n# /usr/local/lib/nodejs/node-{NODE_VER}-{NODE_DISTRO}/bin:**contents of system path**\n</code></pre>\n<p>I've tried a couple of combinations of e.g. using <code>$NODE_VER</code> syntax instead (which produces <code>node--/</code> instead of <code>node-{NODE_VER}-{NODE_DISTRO}/</code>, but seems like any combination using both shell variables (PATH) and Python variables (NODE_VER/NODE_DISTRO) fails.</p>\n<p>Can anybody help me understand why and how to work around it?</p>\n<p>My end goal, as you might have guessed already, is to actually add this folder to the PATH rather than just echoing it - something like:</p>\n<pre><code>!export PATH=/usr/local/lib/nodejs/node-{NODE_VER}-{NODE_DISTRO}/bin:$PATH\n</code></pre>",
		"Question_answer_count": 1,
		"Question_comment_count": 1,
		"Question_creation_date": "2022-03-25 02:12:04.543000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": null,
		"Question_score": 1,
		"Question_tags": "jupyter-notebook|ipython|jupyter-lab|amazon-sagemaker",
		"Question_view_count": 160,
		"Owner_creation_date": "2020-04-19 07:33:10.603000 UTC",
		"Owner_last_access_date": "2022-09-20 07:39:49.537000 UTC",
		"Owner_location": null,
		"Owner_reputation": 473,
		"Owner_up_votes": 26,
		"Owner_down_votes": 0,
		"Owner_views": 37,
		"Answer_body": "<p><a href=\"https://stackoverflow.com/questions/69194172/how-to-reference-both-a-python-and-environment-variable-in-jupyter-bash-magic\">How to reference both a python and environment variable in jupyter bash magic?</a></p>\n<p>Try</p>\n<pre><code>!echo /usr/local/lib/nodejs/node-{NODE_VER}-{NODE_DISTRO}/bin:$$PATH\n</code></pre>\n<p><code>$$PATH</code> forces it to use the system variable rather than try to find a Python/local one.</p>\n<p>Various examples:</p>\n<pre><code>In [130]: foo = 'foo*.txt'\nIn [131]: HOME = 'myvar'\nIn [132]: !echo $foo\nfoo1.txt foobar0.txt foobar2.txt foobar3.txt foo.txt\nIn [133]: !echo $foo $HOME\nfoo1.txt foobar0.txt foobar2.txt foobar3.txt foo.txt myvar\nIn [134]: !echo $foo $$HOME\nfoo1.txt foobar0.txt foobar2.txt foobar3.txt foo.txt /home/paul\nIn [135]: !echo $foo $PWD\n/home/paul/mypy\nIn [136]: !echo $foo $$PWD\nfoo1.txt foobar0.txt foobar2.txt foobar3.txt foo.txt /home/paul/mypy\nIn [137]: !echo {foo} $PWD\n{foo} /home/paul/mypy\nIn [138]: !echo {foo} $$PWD\nfoo1.txt foobar0.txt foobar2.txt foobar3.txt foo.txt /home/paul/mypy\n</code></pre>\n<p>Any variable not locally defined forces the behavior you see:</p>\n<pre><code>In [139]: !echo $abc\n\nIn [140]: !echo {foo} $abc\n{foo}\n</code></pre>\n<p>It may put the substitution in a <code>try/except</code> block, and &quot;give up&quot; if there's any <code>NameError</code>.</p>\n<p>This substitution can occur in most of the magics, not just <code>!</code>.</p>",
		"Answer_comment_count": 1.0,
		"Answer_creation_date": "2022-03-25 04:55:37.557000 UTC",
		"Answer_last_edit_date": "2022-03-25 07:08:28.500000 UTC",
		"Answer_score": 3.0,
		"Question_valid_tags": "sagemaker",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 64118186,
		"Question_title": "Getting an anomaly score for every datapoint in SageMaker?",
		"Question_body": "<p>I'm very new to SageMaker, and I've run into a bit of confusion as to how to achieve the output I am looking for. I am currently attempting to use the built-in RCF algorithm to perform anomaly detection on a list of stock volumes, like this:</p>\n<pre><code>apple_stock_volumes = [123412, 465125, 237564, 238172]\n</code></pre>\n<p>I have created a training job, model, and endpoint, and I'm trying now to invoke the endpoint using boto3. My current code looks like this:</p>\n<pre><code>apple_stock_volumes = [123412, 465125, 237564, 238172]\ndef inference():\n    client = boto3.client('sagemaker-runtime')\n    \n    body = &quot; &quot;.join(apple_stock_volumes)\n    response = client.invoke_endpoint(\n        EndpointName='apple-volume-endpoint',\n        Body=body,\n        ContentType='text/csv'\n    )\n    inference = json.loads(response['Body'].read())\n    print(inference)\n\ninference()\n</code></pre>\n<p>What I wanted was to get an anomaly score for every datapoint, and then to alert if the anomaly score was a few standard deviations above the mean. However, what I'm actually receiving is just a single anomaly score. The following is my output:</p>\n<pre><code>{'scores': [{'score': 0.7164874384}]}\n</code></pre>\n<p>Can anyone explain to me what's going on here? Is this an average anomaly score? Why can't I seem to get SageMaker to output a list of anomaly scores corresponding to my data? Thanks in advance!</p>\n<p>Edit: I have already trained the model on a csv of historical volume data for the last year, and I have created an endpoint to hit.</p>\n<p>Edit 2: I've accepted @maafk's answer, although the actual answer to my question was provided in one of his comments. The piece I was missing was that each data point must be on a new line in your csv input to the endpoint. Once I substituted <code>body = &quot; &quot;.join(apple_stock_volumes)</code> for <code>body = &quot;\\n&quot;.join(apple_stock_volumes)</code>, everything worked as expected.</p>",
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_creation_date": "2020-09-29 10:56:43.873000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": "2020-09-29 15:44:02.040000 UTC",
		"Question_score": 0,
		"Question_tags": "amazon-web-services|random-forest|amazon-sagemaker",
		"Question_view_count": 54,
		"Owner_creation_date": "2020-05-13 03:31:33.533000 UTC",
		"Owner_last_access_date": "2022-07-26 19:57:01.630000 UTC",
		"Owner_location": null,
		"Owner_reputation": 51,
		"Owner_up_votes": 0,
		"Owner_down_votes": 0,
		"Owner_views": 3,
		"Answer_body": "<p>In your case, you'll want to get the standard deviation from getting the scores from historical stock volumes, and figuring out what your anomaly score is by calculating <code>3 * standard deviation</code></p>\n<p>Update your code to do inference on <em>multiple</em> records at once</p>\n<pre class=\"lang-py prettyprint-override\"><code>apple_stock_volumes = [123412, 465125, 237564, 238172]\ndef inference():\n    client = boto3.client('sagemaker-runtime')\n    \n    body = &quot;\\n&quot;.join(apple_stock_volumes). # New line for each record\n    response = client.invoke_endpoint(\n        EndpointName='apple-volume-endpoint',\n        Body=body,\n        ContentType='text/csv'\n    )\n    inference = json.loads(response['Body'].read())\n    print(inference)\n\ninference()\n</code></pre>\n<p>This will return a list of scores</p>\n<p>Assuming <code>apple_stock_volumes_df</code> has your volumes and the scores (after running inference on each record):</p>\n<pre class=\"lang-py prettyprint-override\"><code>score_mean = apple_stock_volumes_df['score'].mean()\nscore_std = apple_stock_volumes_df['score'].std()\nscore_cutoff = score_mean + 3*score_std\n</code></pre>\n<p>There is a great example <a href=\"https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/random_cut_forest/random_cut_forest.ipynb\" rel=\"nofollow noreferrer\">here</a> showing this</p>",
		"Answer_comment_count": 6.0,
		"Answer_creation_date": "2020-09-29 11:10:32.670000 UTC",
		"Answer_last_edit_date": "2020-09-29 16:58:49.940000 UTC",
		"Answer_score": 0.0,
		"Question_valid_tags": "sagemaker",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 60156370,
		"Question_title": "how SageMaker to access s3 bucket data",
		"Question_body": "<p>I was using the code <code>pd.read_json('s3://example2020/kaggle.json')</code> to access S3 bucket data, but it threw the error of <code>FileNotFoundError: example2020/kaggle.json</code>. </p>\n\n<p>The methods I tried:</p>\n\n<p><strong>[Region]</strong>\nThe s3 bucket is in Ohio region while the SageMaker notebook instance is in Singapore. Not sure if this matters. I tried to recreate a s3 bucket in Singapore region but I still cannot access it and got the same file not found error. </p>\n\n<p><strong>[IAM Role]</strong>\nI checked the permission of IAM-SageMaker Execution role\n<a href=\"https://i.stack.imgur.com/st4AR.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/st4AR.png\" alt=\"enter image description here\"></a></p>",
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_creation_date": "2020-02-10 18:31:36.750000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": null,
		"Question_score": 1,
		"Question_tags": "amazon-s3|amazon-sagemaker",
		"Question_view_count": 1386,
		"Owner_creation_date": "2018-02-13 14:44:57.007000 UTC",
		"Owner_last_access_date": "2022-09-23 01:39:29.630000 UTC",
		"Owner_location": "Australia",
		"Owner_reputation": 896,
		"Owner_up_votes": 1273,
		"Owner_down_votes": 3,
		"Owner_views": 177,
		"Answer_body": "<p>The problem is still IAM permission. </p>\n\n<p>I created a new notebook instance and a new IAM role. You would be asked how to access s3 bucket. I chose <code>all s3 bucket</code>. Then the problem solved. \n<a href=\"https://i.stack.imgur.com/B0qOO.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/B0qOO.png\" alt=\"enter image description here\"></a></p>\n\n<p><br>\n<br>\n<strong>[Solution]</strong>\nIn Resource tab, check whether bucket name is general.\n <a href=\"https://i.stack.imgur.com/LL6Fw.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/LL6Fw.png\" alt=\"enter image description here\"></a></p>\n\n<p>If you changed old IAM and it is not working, you can create a new IAM role. And attach this role to the notebook.</p>",
		"Answer_comment_count": 0.0,
		"Answer_creation_date": "2020-02-10 18:31:36.750000 UTC",
		"Answer_last_edit_date": null,
		"Answer_score": 2.0,
		"Question_valid_tags": "sagemaker",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 50133418,
		"Question_title": "How to manage huge data in machine learning with AWS AMI",
		"Question_body": "<p>I am relatively new to Tensorflow and machine learning. I have a dataset running into million rows with >2000 columns each. I am thinking of using Tensorflow on AWS AMI (C5 Instance). I am not sure about where/ how to store this huge data. </p>\n\n<p>1- Should I be storing this as csv in S3 bucket or should I load this in some bigdata system and then use Apache spark for streaming it out? Can you please guide me here. </p>\n\n<p>2- Also, if I have to clean this data, what would be the right approach? Maybe use AWS Sagemaker and use python/ pandas (via notebook) to clean the data? Is that the right approach? </p>",
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_creation_date": "2018-05-02 11:19:19.470000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": null,
		"Question_score": -1,
		"Question_tags": "amazon-web-services|apache-spark|machine-learning|amazon-sagemaker",
		"Question_view_count": 147,
		"Owner_creation_date": "2014-04-24 04:06:03.670000 UTC",
		"Owner_last_access_date": "2022-05-24 07:27:49.117000 UTC",
		"Owner_location": null,
		"Owner_reputation": 421,
		"Owner_up_votes": 14,
		"Owner_down_votes": 1,
		"Owner_views": 77,
		"Answer_body": "<p>I can give some advices but every project is different and use what works best for you.</p>\n\n<p>Is it a one timer data exploration or something you need to crunch on a frequent schedule?  In order to do it frequently, spark might be the right tool. Spark is awesome at transforming/featurizing/cleaning/preprocessing your data into something more usable for tensorflow (usually into sparse format). The important thing here is to keep your gpus busy and to achieve that, you need to preprocess as much as you can before using tf. S3 is a good storage if you do not have small files.</p>\n\n<p>For tensorflow to be happy, most of the time you need to densify your feature vectors. By that, you take a minibatch of records and transform sparse feature vectors into dense vectors. Only then you can send it to tf. This is because gpus are pretty bad at working with sparse data and some operations like convolutions do not even support sparse inputs. (all that can change anytime since it is an active field of research)</p>",
		"Answer_comment_count": 2.0,
		"Answer_creation_date": "2018-05-02 11:54:13.980000 UTC",
		"Answer_last_edit_date": null,
		"Answer_score": 0.0,
		"Question_valid_tags": "sagemaker",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 61380051,
		"Question_title": "Sagemaker usage of EC2 instances",
		"Question_body": "<p>Is there a way to view/monitor AWS Sagemaker's usage of EC2 instances?\nI am running a Sagemaker endpoint and tried to find its instances (ml.p3.2xlarge in this case) in the EC2 UI, but couldn't find them. </p>",
		"Question_answer_count": 1,
		"Question_comment_count": 3,
		"Question_creation_date": "2020-04-23 05:37:43.500000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": null,
		"Question_score": 5,
		"Question_tags": "amazon-ec2|amazon-sagemaker",
		"Question_view_count": 615,
		"Owner_creation_date": "2016-03-21 08:49:39.920000 UTC",
		"Owner_last_access_date": "2022-07-17 11:53:14.840000 UTC",
		"Owner_location": null,
		"Owner_reputation": 383,
		"Owner_up_votes": 10,
		"Owner_down_votes": 0,
		"Owner_views": 19,
		"Answer_body": "<p>ml EC2 instances do not appear in the EC2 console. You can find their metrics in Cloudwatch though, and create dashboards to monitor what you need:</p>\n\n<p><a href=\"https://i.stack.imgur.com/BgUkm.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/BgUkm.png\" alt=\"enter image description here\"></a>\n<a href=\"https://i.stack.imgur.com/wD4w0.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/wD4w0.png\" alt=\"enter image description here\"></a></p>",
		"Answer_comment_count": 2.0,
		"Answer_creation_date": "2020-05-04 07:02:18.650000 UTC",
		"Answer_last_edit_date": null,
		"Answer_score": 3.0,
		"Question_valid_tags": "sagemaker",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 70909916,
		"Question_title": "No GPU detected on AWS SageMaker pytorch-1.8-gpu-py36 instance",
		"Question_body": "<p>I've got a pytorch-1.8-gpu-py36 instance running on AWS SageMaker Studio.</p>\n<p>If I'm in a notebook in and I enter:</p>\n<pre><code>!nvidia-smi -L\n</code></pre>\n<p>I get:</p>\n<pre><code>GPU 0: Tesla T4 (UUID: GPU-786d298a-2648-3506-6c3a-f541fa46d777)\n</code></pre>\n<p>But if I open a terminal and enter:</p>\n<pre><code>nvidia-smi -L\n</code></pre>\n<p>I get command not found, and if I try to run a .py script that requires a GPU I get this error from PyTorch:</p>\n<pre><code>pytorch_lightning.utilities.exceptions.MisconfigurationException: \nYou requested GPUs: [0]\nBut your machine only has: []\n</code></pre>\n<p>Do the terminal windows and notebooks run off of separate instances even if they're in the same folder? Is there a way to get the terminal to be part of the same instance as the notebook?</p>\n<p>I can't simply run the command line from the notebook as I require a Conda environment that can't be activated from the notebook interface.</p>",
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_creation_date": "2022-01-29 21:32:28.780000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": "2022-01-30 05:19:40.470000 UTC",
		"Question_score": 0,
		"Question_tags": "python|amazon-web-services|jupyter-notebook|amazon-sagemaker",
		"Question_view_count": 278,
		"Owner_creation_date": "2013-05-23 11:55:21.733000 UTC",
		"Owner_last_access_date": "2022-08-30 19:13:25.677000 UTC",
		"Owner_location": null,
		"Owner_reputation": 931,
		"Owner_up_votes": 7,
		"Owner_down_votes": 0,
		"Owner_views": 48,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "sagemaker",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 71073934,
		"Question_title": "Is there a need to create virtual environments inside SageMaker (given SageMaker notebook instances are already separate conda environments)?",
		"Question_body": "<p>It is common practice to create virtual environments on any Python project, to avoid global installation and package collision errors, and of course keep track of Python versions across projects.</p>\n<p>Amazon SageMaker already comes with conda environments. In fact, SageMaker notebook instances are already separate conda environments. So, this begs the question (I think), does it make sense to create a virtual environment inside an AWS SageMaker notebook when working on a Python project inside SageMaker?</p>\n<p>What if I want to use a specific version of Python...well SageMaker allows us to simply change the kernal. So is that good enough in terms of the advantages that a separate virtual environment brings?</p>\n<p>What about package organization? Knowing exactly the packages you need to run your code in case someone else needs to run it on their machines is an obvious advantage of using virtual environments. Does SageMaker help with this if I don't setup a virtual environment?</p>\n<p>So, do SageMaker users have to run things like:</p>\n<pre><code>conda create --name myenv\n</code></pre>\n<p>or</p>\n<pre><code>python3 -m venv env\n</code></pre>",
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_creation_date": "2022-02-11 00:24:04.737000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": null,
		"Question_score": 0,
		"Question_tags": "python|amazon-web-services|virtualenv|conda|amazon-sagemaker",
		"Question_view_count": 786,
		"Owner_creation_date": "2012-08-31 20:08:40.090000 UTC",
		"Owner_last_access_date": "2022-09-25 04:17:41.297000 UTC",
		"Owner_location": null,
		"Owner_reputation": 11650,
		"Owner_up_votes": 6318,
		"Owner_down_votes": 21,
		"Owner_views": 977,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "sagemaker",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 67210677,
		"Question_title": "If I close my JupyterLab from notebook instance, would my code be gone?",
		"Question_body": "<p>I'm new to AWS and I'm trying out AWS Sagemaker. I'm currently doing my project which involves quite a long time to finish and I don't think I can finish it in a day. I'm worried if I close my JupyterLab of my notebook instance in SageMaker, my code will be gone. How do I save my code and cell run progress when using Sagemaker?</p>",
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_creation_date": "2021-04-22 09:36:23.027000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": null,
		"Question_score": 2,
		"Question_tags": "python|amazon-web-services|tensorflow|amazon-sagemaker",
		"Question_view_count": 305,
		"Owner_creation_date": "2020-11-21 06:04:32.327000 UTC",
		"Owner_last_access_date": "2021-06-03 07:19:55.190000 UTC",
		"Owner_location": "Jakarta Selatan, South Jakarta City, Jakarta, Indonesia",
		"Owner_reputation": 97,
		"Owner_up_votes": 18,
		"Owner_down_votes": 0,
		"Owner_views": 34,
		"Answer_body": "<p>If you are training directly in the notebook the answer is yes.\nHowever the best practice is not to train directly with the notebook.\nUse instead the notebook (you can choose a very cheap instance for the notebook) to launch your training job (in the instance type you desire) adapting you code to be the entrypoint of the estimator. In that way, you can close the notebook after launching the training job and monitor the training job using cloudwatch. You can also define some regex to capture metrics from the stout and cloudwatch will automatically plot for you, which is very useful!\nAs a quick example.. in my notebook I have this cell:</p>\n<pre><code>import sagemaker from sagemaker.tensorflow import TensorFlow from sagemaker import get_execution_role\n\nbucket = 'mybucket'\n\ntrain_data = 's3://{}/{}'.format(bucket,'train')\n\nvalidation_data = 's3://{}/{}'.format(bucket,'test')\n\ns3_output_location = 's3://{}'.format(bucket)\n\nhyperparameters = {'epochs': 70, 'batch-size' : 32, 'learning-rate' :\n0.01}\n\nmetrics = [{'Name': 'Loss', 'Regex': 'loss: ([0-9\\.]+)'},\n           {'Name': 'Accuracy', 'Regex': 'acc: ([0-9\\.]+)'},\n           {'Name': 'Epoch', 'Regex': 'Epoch ([0-9\\.]+)'},\n           {'Name': 'Validation_Acc', 'Regex': 'val_acc: ([0-9\\.]+)'},\n           {'Name': 'Validation_Loss', 'Regex': 'val_loss: ([0-9\\.]+)'}]\n\ntf_estimator = TensorFlow(entry_point='training.py', \n                          role=get_execution_role(),\n                          train_instance_count=1, \n                          train_instance_type='ml.p2.xlarge',\n                          train_max_run=172800,\n                          output_path=s3_output_location,\n                          framework_version='1.12',\n                          py_version='py3',\n                          metric_definitions = metrics,\n                          hyperparameters = hyperparameters)\n\ninputs = {'train': train_data, 'test': validation_data}\n\nmyJobName = 'myname'\n\ntf_estimator.fit(inputs=inputs, job_name=myJobName)\n</code></pre>\n<p>My training script training.py is something like this:</p>\n<pre><code>if __name__ =='__main__':\n\n    parser = argparse.ArgumentParser()\n\n    # input data and model directories\n    parser.add_argument('--gpu-count', type=int, default=os.environ['SM_NUM_GPUS'])\n    parser.add_argument('--model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\n    parser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))\n    parser.add_argument('--learning-rate', type=float, default=0.0001)\n    parser.add_argument('--batch-size', type=int, default=32)\n    parser.add_argument('--epochs', type=int, default=1)\n....\n</code></pre>",
		"Answer_comment_count": 0.0,
		"Answer_creation_date": "2021-04-22 09:58:14.700000 UTC",
		"Answer_last_edit_date": null,
		"Answer_score": 2.0,
		"Question_valid_tags": "sagemaker",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 69600689,
		"Question_title": "How to set SageMaker xgboost's eval_metric to f1?",
		"Question_body": "<p>I tried SageMaker's AutoPilot to solve a binary classification problem and I found it is using f1 as the evaluation metric. But when I tried to write some code without tuning like this:</p>\n<pre class=\"lang-py prettyprint-override\"><code>xgb.set_hyperparameters(max_depth=5,\n                        eta=0.2,\n                        gamma=4,\n                        min_child_weight=6,\n                        subsample=0.8,\n                        objective='binary:logistic',\n                        eval_metric='f1',\n                        num_round=100)\n</code></pre>\n<p>This generates the following error:</p>\n<blockquote>\n<p>[2021-10-17:00:02:19:ERROR] Customer Error: Metric 'f1' is not\nsupported. Parameter 'eval_metric' should be one of these\noptions:'rmse', 'mae', 'logloss', 'error', 'merror', 'mlogloss',\n'auc', 'ndcg', 'map', 'poisson-nloglik', 'gamma-nloglik',\n'gamma-deviance', 'tweedie-nloglik'.</p>\n</blockquote>\n<p>Since the autopilot was able to compute F1, I feel like it is supported in the hyperparameter setting in some fashion? Am I misunderstanding?</p>\n<p>Any help is going to be appreciated.</p>",
		"Question_answer_count": 1,
		"Question_comment_count": 3,
		"Question_creation_date": "2021-10-17 01:22:38.807000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": "2021-10-17 09:08:57.223000 UTC",
		"Question_score": 0,
		"Question_tags": "xgboost|amazon-sagemaker|xgbclassifier",
		"Question_view_count": 388,
		"Owner_creation_date": "2011-05-13 06:51:53.437000 UTC",
		"Owner_last_access_date": "2022-09-25 04:56:44.010000 UTC",
		"Owner_location": null,
		"Owner_reputation": 10317,
		"Owner_up_votes": 268,
		"Owner_down_votes": 1,
		"Owner_views": 595,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "sagemaker",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 60801292,
		"Question_title": "View Neptune Graph Schema using Jupyter notebook",
		"Question_body": "<p>Is there a way to view the schema of a graph in a Neptune cluster using Jupyter Notebook? </p>\n\n<p>Like you would do a \"select * from tablename limit 10\" in an RDS using SQL, similarly is there a way to get a sense of the graph data through Jupyter Notebook?</p>",
		"Question_answer_count": 2,
		"Question_comment_count": 1,
		"Question_creation_date": "2020-03-22 15:36:34.693000 UTC",
		"Question_favorite_count": 2.0,
		"Question_last_edit_date": "2021-11-23 23:00:19.890000 UTC",
		"Question_score": 2,
		"Question_tags": "gremlin|amazon-sagemaker|amazon-neptune|gremlinpython|graph-notebook",
		"Question_view_count": 831,
		"Owner_creation_date": "2020-03-22 15:31:06.787000 UTC",
		"Owner_last_access_date": "2022-09-17 07:01:39.723000 UTC",
		"Owner_location": null,
		"Owner_reputation": 99,
		"Owner_up_votes": 5,
		"Owner_down_votes": 0,
		"Owner_views": 12,
		"Answer_body": "<p>It depends on how large your graph is as to how well this will perform but you can get a sense of the type of nodes and edges you have using something like the example below. From the tags you used I assume you are using Gremlin:</p>\n\n<pre><code>g.V().groupCount().by(label)\ng.E().groupCount().by(label)\n</code></pre>\n\n<p>If you have a very large graph try putting something like <code>limit(100000)</code> before the <code>groupCount</code> step.</p>\n\n<p>If you are using a programming language like Python (with gremlin python installed) then you will need to add a <code>next()</code> terminal step to the queries as in:</p>\n\n<pre><code>g.V().groupCount().by(label).next()\ng.E().groupCount().by(label).next()\n</code></pre>\n\n<p>Having found the labels and distribution of the labels you could use one of them to explore some properties. Let's imagine there is a label called \"person\".</p>\n\n<pre><code>g.V().hasLabel('person').limit(10).valueMap().toList()\n</code></pre>\n\n<p>Remember with Gremlin property graphs vertices with the same label may not necessarily have all the same properties so it's good to look at more than one vertex to get a sense for that as well.</p>",
		"Answer_comment_count": 0.0,
		"Answer_creation_date": "2020-03-22 18:40:12.783000 UTC",
		"Answer_last_edit_date": null,
		"Answer_score": 4.0,
		"Question_valid_tags": "sagemaker",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 57485064,
		"Question_title": "How to run an Object Detection prediction on a MobileNet SSD model hosted on SageMaker",
		"Question_body": "<p>I trained a MobileNet v1 SSD model using AWS SageMaker and it trained perfectly.  I have an export/Servo/xxx/saved_graph (a tflite model derived from the checkpoints worked perfectly on an IoT device.)    The model deployed on SageMaker as an endpoint without a problem.   My problem is, I don't know how to feed images into the model endpoint.  (Which presumably is the same as asking how do I feed data into a served model?)   </p>\n\n<p>When I query the model (below), I see the input signature is 'serialized_example'.   So, I'm assuming I need a serialized example.   That's where I run out of talent.  </p>\n\n<p>I need help understanding how to convert jpg file into a serialized example.  Or how can I use a tfrecord.  </p>\n\n<pre><code>saved_model_cli show --dir {MODEL_PATH} --tag_set serve --signature_def tensorflow/serving/predict\n</code></pre>\n\n<p>yields (I omitted the outputs):</p>\n\n<pre><code>  inputs['serialized_example'] tensor_info:\n      dtype: DT_STRING\n      shape: ()\n      name: tf_example:0\n</code></pre>\n\n<p>I have a Dataset but don't know how to get the right class -</p>\n\n<pre><code>raw_dataset = tf.data.TFRecordDataset(tfrecord_file_list)\nexample_decoder = TfExampleDecoder()\ndecoded_dataset = raw_dataset.map(example_decoder.decode)\n</code></pre>\n\n<p>if I enumerate across a decoded dataset, how can I feed the data into my model?</p>\n\n<pre><code>for n, e in enumerate(decoded_dataset.take(4)):\n   what do I do with e - which looks like:\n\n&lt;DatasetV1Adapter shapes: {image: (?, ?, 3), source_id: (), key: (), filename: (), groundtruth_boxes: (?, 4), groundtruth_area: (?,), groundtruth_is_crowd: (?,), groundtruth_difficult: (?,), groundtruth_group_of: (?,), groundtruth_weights: (?,), groundtruth_classes: (?,), groundtruth_image_classes: (?,), original_image_spatial_shape: (2,)}, types: {image: tf.uint8, source_id: tf.string, key: tf.string, filename: tf.string, groundtruth_boxes: tf.float32, groundtruth_area: tf.float32, groundtruth_is_crowd: tf.bool, groundtruth_difficult: tf.int64, groundtruth_group_of: tf.int64, groundtruth_weights: tf.float32, groundtruth_classes: tf.int64, groundtruth_image_classes: tf.int64, original_image_spatial_shape: tf.int32}&gt;\n</code></pre>",
		"Question_answer_count": 0,
		"Question_comment_count": 1,
		"Question_creation_date": "2019-08-13 20:43:14.413000 UTC",
		"Question_favorite_count": 1.0,
		"Question_last_edit_date": null,
		"Question_score": 1,
		"Question_tags": "tensorflow|object-detection|amazon-sagemaker|mobilenet",
		"Question_view_count": 429,
		"Owner_creation_date": "2016-01-11 20:03:25.717000 UTC",
		"Owner_last_access_date": "2022-06-10 16:43:54.400000 UTC",
		"Owner_location": null,
		"Owner_reputation": 360,
		"Owner_up_votes": 4,
		"Owner_down_votes": 0,
		"Owner_views": 24,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "sagemaker",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 50758127,
		"Question_title": "Why is the code not able to find the file specified in the AWS S3 path, when I can find it manually?",
		"Question_body": "<p>I have a bucket called <code>my_bucket</code> and a folder in it called <code>Images</code>. I am trying to read the files (images) inside the <code>Image</code> folder.</p>\n\n<pre><code>file = pd.read_csv(some_csv_file)\nX = file.values[:,0]\n\nrole = get_execution_role()\nbucket='my_bucket'\ndata_key = 'Images'\ndata_dir = 's3://{}/{}'.format(bucket, data_key)\ns = '/'\n\nfor img_name in X:\n    seq = (data_dir, img_name)\n    img_path = s.join(seq)\n    img = imread(img_path)\n</code></pre>\n\n<p>But it gives the following error:</p>\n\n<pre><code>---------------------------------------------------------------------------\nFileNotFoundError                         Traceback (most recent call last)\n&lt;ipython-input-20-a273242ed30e&gt; in &lt;module&gt;()\n     43     img_path = s.join(seq)\n     44     print(img_path)\n---&gt; 45     img = imread(img_path)\n     46     img = imresize(img, (32, 32))\n     47     img = img.astype('float32') # this will help us in later stage\n\n~/anaconda3/envs/python3/lib/python3.6/site-packages/numpy/lib/utils.py in newfunc(*args, **kwds)\n     99             \"\"\"`arrayrange` is deprecated, use `arange` instead!\"\"\"\n    100             warnings.warn(depdoc, DeprecationWarning, stacklevel=2)\n--&gt; 101             return func(*args, **kwds)\n    102 \n    103         newfunc = _set_function_name(newfunc, old_name)\n\n~/anaconda3/envs/python3/lib/python3.6/site-packages/scipy/misc/pilutil.py in imread(name, flatten, mode)\n    162     \"\"\"\n    163 \n--&gt; 164     im = Image.open(name)\n    165     return fromimage(im, flatten=flatten, mode=mode)\n    166 \n\n~/anaconda3/envs/python3/lib/python3.6/site-packages/PIL/Image.py in open(fp, mode)\n   2541 \n   2542     if filename:\n-&gt; 2543         fp = builtins.open(filename, \"rb\")\n   2544         exclusive_fp = True\n   2545 \n\nFileNotFoundError: [Errno 2] No such file or directory: 's3://my_bucket/Images/377.jpg'\n</code></pre>\n\n<p><code>377.jpg</code> is the first row in <code>X</code>. I checked manually in the S3 storage; this file is present there. So, why am I getting this error, and how to fix it? The only reason I can think of is, maybe the process of specifying the S3 path is wrong - but in the S3 documentation, the process to specify storage is given as <code>'s3://{}/{}'.format(bucket, data_key)</code>. Moreover, in the last line of the error message, the filename is <code>s3://my_bucket/Images/377.jpg</code>, which is the path I navigate manually to locate the file in the bucket.</p>",
		"Question_answer_count": 2,
		"Question_comment_count": 3,
		"Question_creation_date": "2018-06-08 10:01:28.657000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": null,
		"Question_score": 1,
		"Question_tags": "python|amazon-web-services|amazon-s3|amazon-sagemaker",
		"Question_view_count": 3376,
		"Owner_creation_date": "2015-09-06 08:28:25.783000 UTC",
		"Owner_last_access_date": "2022-09-21 06:01:13.377000 UTC",
		"Owner_location": null,
		"Owner_reputation": 3240,
		"Owner_up_votes": 982,
		"Owner_down_votes": 4,
		"Owner_views": 509,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "sagemaker",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 54432761,
		"Question_title": "Amazon SageMaker hyperparameter tuning error for built-in algorithm using the Python SDK",
		"Question_body": "<p>When using the Python SDK to start a SageMaker hyperparameter tuning job using one of the built-in algorithms (in this case, the Image Classifier) with the following code:</p>\n\n<pre><code># [...] Some lines elided for brevity\n\nfrom sagemaker.tuner import HyperparameterTuner, IntegerParameter, CategoricalParameter, ContinuousParameter\nhyperparameter_ranges = {'optimizer': CategoricalParameter(['sgd', 'adam']),\n                         'learning_rate': ContinuousParameter(0.0001, 0.2),\n                         'mini_batch_size': IntegerParameter(2, 30),}\n\nobjective_metric_name = 'validation:accuracy'\n\ntuner = HyperparameterTuner(image_classifier,\n                            objective_metric_name,\n                            hyperparameter_ranges,\n\n                            max_jobs=50,\n                            max_parallel_jobs=3)\n\ntuner.fit(inputs=data_channels, logs=True)\n</code></pre>\n\n<p>The job fails and I get this error when checking on the job status in the SageMaker web console:</p>\n\n<pre><code>ClientError: Additional hyperparameters are not allowed (u'sagemaker_estimator_module', u'sagemaker_estimator_class_name' were unexpected) (caused by ValidationError) \n\nCaused by: Additional properties are not allowed (u'sagemaker_estimator_module', u'sagemaker_estimator_class_name' were unexpected) \n\nFailed validating u'additionalProperties' in schema: {u'$schema': u'http://json-schema.org/schema#', u'additionalProperties': False, u'definitions': {u'boolean_0_1': {u'oneOf': [{u'enum': [u'0', u'1'], u'type': u'string'}, {u'enum': [0, 1], u'type': u'number'}]}, u'boolean_true_false_0_1': {u'oneOf': [{u'enum': [u'true', u'false',\n</code></pre>\n\n<p>I'm not explicitly passing the <code>sagemaker_estimator_module</code> or <code>sagemaker_estimator_class_name</code> properties anywhere, so I'm not sure why it's returning this error. </p>\n\n<p>What's the right way to start this tuning job?</p>",
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_creation_date": "2019-01-30 02:58:11.420000 UTC",
		"Question_favorite_count": 1.0,
		"Question_last_edit_date": null,
		"Question_score": 3,
		"Question_tags": "python|amazon-web-services|amazon-sagemaker|hyperparameters",
		"Question_view_count": 595,
		"Owner_creation_date": "2008-10-23 03:43:42.317000 UTC",
		"Owner_last_access_date": "2022-08-22 05:29:27.157000 UTC",
		"Owner_location": "Singapore",
		"Owner_reputation": 7707,
		"Owner_up_votes": 162,
		"Owner_down_votes": 6,
		"Owner_views": 583,
		"Answer_body": "<p>I found the answer via <a href=\"https://translate.google.com/translate?hl=en&amp;sl=ja&amp;u=https://dev.classmethod.jp/machine-learning/sagemaker-tuning-stack/&amp;prev=search\" rel=\"nofollow noreferrer\">this post translated from Japanese</a>.</p>\n\n<p>When starting hyperparameter tuning jobs using the built-in algorithms in the Python SDK, <strong>you need to explicitly pass <code>include_cls_metadata=False</code></strong> as a keyword argument to <code>tuner.fit()</code> like this:</p>\n\n<p><code>tuner.fit(inputs=data_channels, logs=True, include_cls_metadata=False)</code></p>",
		"Answer_comment_count": 0.0,
		"Answer_creation_date": "2019-01-30 02:58:11.420000 UTC",
		"Answer_last_edit_date": null,
		"Answer_score": 2.0,
		"Question_valid_tags": "sagemaker",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 70462753,
		"Question_title": "AWS GroundTruth text labeling - hide columns in the data, and checking quality of answers",
		"Question_body": "<p>I am new to SageMaker. I have a large csv dataset which I would like labelled:</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>sentence_id</th>\n<th>sentence</th>\n<th>pre_agreed_label</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>148392</td>\n<td>A sentence</td>\n<td>0</td>\n</tr>\n<tr>\n<td>383294</td>\n<td>Another sentence</td>\n<td>1</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>For each sentence, I would like a) a yes/no binary classification in response to a question, and b) on a scale of 1-3, how obvious the classification was. I need the sentence id to map to other parts of the dataset, and will use the pre-agreed labels to assess accuracy.</p>\n<p>I have identified SageMaker GroundTruth labelling jobs as a possible way to do this. Is this the best way? In trying to set it up I have run into a few problems.</p>\n<p>The first problem is I can't find a way to display only the sentence column to the labellers, hiding the sentence_id and pre_agreed_labels.</p>\n<p>The second is that there is either single labelling or multi labelling, but I would like a way to have two sets of single-selection labels:</p>\n<p>Select one for binary classification:</p>\n<ol>\n<li>Yes</li>\n<li>No</li>\n</ol>\n<p>Select one for difficulty of classification:</p>\n<ol>\n<li>Easy</li>\n<li>Medium</li>\n<li>Hard</li>\n</ol>\n<p>It seems as though this can be done using custom HTML, but I don't know how to do this - the template it gives you doesn't even render</p>\n<p>Finally, having not used mechanical turk before, are there ways of ensuring people take the work seriously and don't just select random answers? I can see there's an option to have x number of people answer the same question, but is there also a way to put in an obvious question to which we already have a 'pre_agreed_label' every nth question, and kick people off the task if they get it wrong? There also appears to be a maximum of $1.20 per task which seems odd.</p>",
		"Question_answer_count": 0,
		"Question_comment_count": 0,
		"Question_creation_date": "2021-12-23 13:25:09.000000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": null,
		"Question_score": 4,
		"Question_tags": "amazon-web-services|amazon-sagemaker|training-data|multilabel-classification|amazon-ground-truth",
		"Question_view_count": 63,
		"Owner_creation_date": "2021-01-20 21:45:22.663000 UTC",
		"Owner_last_access_date": "2022-05-05 10:26:59.963000 UTC",
		"Owner_location": null,
		"Owner_reputation": 51,
		"Owner_up_votes": 13,
		"Owner_down_votes": 0,
		"Owner_views": 25,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "sagemaker",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 57362300,
		"Question_title": "Is it possible to use the multiclass classifier of aws to recognize the given place of the text?",
		"Question_body": "<p>I'm using AWS SageMaker, and i want to create something that, with a given text, it recognize the place of that description. Is it possible?</p>",
		"Question_answer_count": 2,
		"Question_comment_count": 0,
		"Question_creation_date": "2019-08-05 16:06:23.273000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": null,
		"Question_score": 1,
		"Question_tags": "amazon-web-services|amazon-s3|classification|amazon-sagemaker|multiclass-classification",
		"Question_view_count": 103,
		"Owner_creation_date": "2018-04-16 15:34:34.820000 UTC",
		"Owner_last_access_date": "2022-09-22 15:26:41.440000 UTC",
		"Owner_location": null,
		"Owner_reputation": 125,
		"Owner_up_votes": 8,
		"Owner_down_votes": 0,
		"Owner_views": 28,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "sagemaker",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 53695161,
		"Question_title": "How do I resolve a SQL ParseError in PySpark?",
		"Question_body": "<p>I'm a new-bee to PySpark and AWS Sagemaker using Jupyter Notebook. I do know how to write SQL statements to answer my questions. This code piece is supposed to:\n1. Extract available methods of death dispositions in my dataset (CDC Death Data -in CSV) by year\n2. Count the frequency for each disposition by year</p>\n\n<p>I was able to run the SQL statement on the same dataset in a MySQL database. But once I added the query to my PySpark code, I got a <code>ParseError</code> Please, see the error below. </p>\n\n<p>How would I go about resolving this error? and if I want to create a graph/plot with the output, how would I go about it?\nTHANKS</p>\n\n<pre><code>df.registerTempTable(\"data\")\nmethods = sqlContext.sql(\"\\\n    SELECT current_data_year AS Year, \\\n        CASE method_of_disposition \\\n            WHEN 'C' THEN 'Cremation' \\\n            WHEN 'B' THEN 'Burial' \\\n            WHEN 'D' THEN 'D' \\\n            WHEN 'E' THEN 'E' \\\n            WHEN 'O' THEN 'O' \\\n            WHEN 'R' THEN 'R' \\\n            WHEN 'U' THEN 'Unknown' \\\n            END AS 'Method of Disposition', \\\n        COUNT(method_of_disposition) AS Count \\\n        FROM data \\\n        GROUP BY current_data_year, method_of_disposition\\\n    \").show()\n</code></pre>\n\n<p>MY NEW OUTPUT</p>\n\n<pre><code>+----+-------------------+-------+\n|Year|MethodofDisposition|  Count|\n+----+-------------------+-------+\n|   0|               null|     10|\n|2005|              Other|   2199|\n|2005|           Donation|   4795|\n|2005|                  E|  21247|\n|2005|     RemovedFromUSA|  31954|\n|2005|          Cremation| 350018|\n|2005|             Burial| 553202|\n|2005|            Unknown|1489091|\n|2006|              Other|   2252|\n|2006|           Donation|   6883|\n|2006|                  E|  23412|\n|2006|     RemovedFromUSA|  40870|\n|2006|          Cremation| 423282|\n|2006|             Burial| 667169|\n|2006|            Unknown|1266857|\n|2007|              Other|   3119|\n|2007|           Donation|   8719|\n|2007|                  E|  26139|\n|2007|     RemovedFromUSA|  41411|\n|2007|          Cremation| 472220|\n|2007|             Burial| 725666|\n|2007|            Unknown|1151069|\n|2008|              Other|   5511|\n|2008|           Donation|  10981|\n|2008|                  E|  31913|\n|2008|     RemovedFromUSA|  44713|\n|2008|          Cremation| 579827|\n|2008|             Burial| 866384|\n|2008|            Unknown| 937482|\n|2009|              Other|   3688|\n|2009|           Donation|  12011|\n|2009|                  E|  30344|\n|2009|     RemovedFromUSA|  45451|\n|2009|          Cremation| 599202|\n|2009|             Burial| 802305|\n|2009|            Unknown| 948218|\n|2010|              Other|   3782|\n|2010|           Donation|  15208|\n|2010|                  E|  32807|\n|2010|     RemovedFromUSA|  47899|\n|2010|          Cremation| 706224|\n|2010|            Unknown| 760192|\n|2010|             Burial| 906430|\n|2011|              Other|   5169|\n|2011|           Donation|  17450|\n|2011|                  E|  33847|\n|2011|     RemovedFromUSA|  47199|\n|2011|            Unknown| 685325|\n|2011|          Cremation| 780480|\n|2011|             Burial| 950372|\n|2012|              Other|   6649|\n|2012|           Donation|  20790|\n|2012|                  E|  35110|\n|2012|     RemovedFromUSA|  52896|\n|2012|            Unknown| 440569|\n|2012|          Cremation| 898222|\n|2012|             Burial|1093628|\n|2013|              Other|   6962|\n|2013|           Donation|  21653|\n|2013|                  E|  36949|\n|2013|     RemovedFromUSA|  53678|\n|2013|            Unknown| 395080|\n|2013|          Cremation| 973768|\n|2013|             Burial|1113362|\n|2014|              Other|   7871|\n|2014|           Donation|  24004|\n|2014|                  E|  39321|\n|2014|     RemovedFromUSA|  59884|\n|2014|            Unknown| 242963|\n|2014|          Cremation|1094292|\n|2014|             Burial|1162836|\n|2015|              Other|  11729|\n|2015|           Donation|  27870|\n|2015|                  E|  40880|\n|2015|     RemovedFromUSA|  71744|\n|2015|            Unknown|  74050|\n|2015|          Cremation|1244297|\n|2015|             Burial|1247628|\n+----+-------------------+-------+\n</code></pre>\n\n<p>ERROR MESSAGE</p>\n\n<pre><code>---------------------------------------------------------------------------\nPy4JJavaError                             Traceback (most recent call last)\n~/anaconda3/envs/python3/lib/python3.6/site-packages/pyspark/sql/utils.py in deco(*a, **kw)\n     62         try:\n---&gt; 63             return f(*a, **kw)\n     64         except py4j.protocol.Py4JJavaError as e:\n\n~/anaconda3/envs/python3/lib/python3.6/site-packages/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)\n    318                     \"An error occurred while calling {0}{1}{2}.\\n\".\n--&gt; 319                     format(target_id, \".\", name), value)\n    320             else:\n\nPy4JJavaError: An error occurred while calling o19.sql.\n: org.apache.spark.sql.catalyst.parser.ParseException: \nextraneous input ''Method of Disposition'' expecting {&lt;EOF&gt;, ',', 'FROM', 'WHERE', 'GROUP', 'ORDER', 'HAVING', 'LIMIT', 'LATERAL', 'WINDOW', 'UNION', 'EXCEPT', 'MINUS', 'INTERSECT', 'SORT', 'CLUSTER', 'DISTRIBUTE'}(line 1, pos 213)\n\n== SQL ==\nSELECT current_data_year AS Year, CASE method_of_disposition WHEN 'C' THEN 'Cremation' WHEN 'B' THEN 'Burial' WHEN 'D' THEN 'D' WHEN 'E' THEN 'E' WHEN 'O' THEN 'O' WHEN 'R' THEN 'R' WHEN 'U' THEN 'Unknown' END AS 'Method of Disposition', COUNT(method_of_disposition) AS Count FROM data GROUP BY current_data_year, method_of_disposition\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------^^^\n\n    at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:217)\n    at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:114)\n    at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:48)\n    at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:68)\n    at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:632)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:498)\n    at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n    at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n    at py4j.Gateway.invoke(Gateway.java:280)\n    at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n    at py4j.commands.CallCommand.execute(CallCommand.java:79)\n    at py4j.GatewayConnection.run(GatewayConnection.java:214)\n    at java.lang.Thread.run(Thread.java:745)\n\n\nDuring handling of the above exception, another exception occurred:\n\nParseException                            Traceback (most recent call last)\n&lt;ipython-input-7-f99c8a5b941c&gt; in &lt;module&gt;()\n      1 #Grouping and counting Cremation vs Burial by Year\n      2 df.registerTempTable(\"data\")\n----&gt; 3 sqlContext.sql(\"SELECT current_data_year AS Year, CASE method_of_disposition WHEN 'C' THEN 'Cremation' WHEN 'B' THEN 'Burial' WHEN 'D' THEN 'D' WHEN 'E' THEN 'E' WHEN 'O' THEN 'O' WHEN 'R' THEN 'R' WHEN 'U' THEN 'Unknown' END AS 'Method of Disposition', COUNT(method_of_disposition) AS Count FROM data GROUP BY current_data_year, method_of_disposition\").show()\n\n~/anaconda3/envs/python3/lib/python3.6/site-packages/pyspark/sql/context.py in sql(self, sqlQuery)\n    382         [Row(f1=1, f2=u'row1'), Row(f1=2, f2=u'row2'), Row(f1=3, f2=u'row3')]\n    383         \"\"\"\n--&gt; 384         return self.sparkSession.sql(sqlQuery)\n    385 \n    386     @since(1.0)\n\n~/anaconda3/envs/python3/lib/python3.6/site-packages/pyspark/sql/session.py in sql(self, sqlQuery)\n    601         [Row(f1=1, f2=u'row1'), Row(f1=2, f2=u'row2'), Row(f1=3, f2=u'row3')]\n    602         \"\"\"\n--&gt; 603         return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)\n    604 \n    605     @since(2.0)\n\n~/anaconda3/envs/python3/lib/python3.6/site-packages/py4j/java_gateway.py in __call__(self, *args)\n   1131         answer = self.gateway_client.send_command(command)\n   1132         return_value = get_return_value(\n-&gt; 1133             answer, self.gateway_client, self.target_id, self.name)\n   1134 \n   1135         for temp_arg in temp_args:\n\n~/anaconda3/envs/python3/lib/python3.6/site-packages/pyspark/sql/utils.py in deco(*a, **kw)\n     71                 raise AnalysisException(s.split(': ', 1)[1], stackTrace)\n     72             if s.startswith('org.apache.spark.sql.catalyst.parser.ParseException: '):\n---&gt; 73                 raise ParseException(s.split(': ', 1)[1], stackTrace)\n     74             if s.startswith('org.apache.spark.sql.streaming.StreamingQueryException: '):\n     75                 raise StreamingQueryException(s.split(': ', 1)[1], stackTrace)\n\nParseException: \"\\nextraneous input ''Method of Disposition'' expecting {&lt;EOF&gt;, ',', 'FROM', 'WHERE', 'GROUP', 'ORDER', 'HAVING', 'LIMIT', 'LATERAL', 'WINDOW', 'UNION', 'EXCEPT', 'MINUS', 'INTERSECT', 'SORT', 'CLUSTER', 'DISTRIBUTE'}(line 1, pos 213)\\n\\n== SQL ==\\nSELECT current_data_year AS Year, CASE method_of_disposition WHEN 'C' THEN 'Cremation' WHEN 'B' THEN 'Burial' WHEN 'D' THEN 'D' WHEN 'E' THEN 'E' WHEN 'O' THEN 'O' WHEN 'R' THEN 'R' WHEN 'U' THEN 'Unknown' END AS 'Method of Disposition', COUNT(method_of_disposition) AS Count FROM data GROUP BY current_data_year, method_of_disposition\\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------^^^\\n\"\n</code></pre>\n\n<p>Sample MYSQL OUTPUT\n<a href=\"https://i.stack.imgur.com/Jqq1I.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/Jqq1I.png\" alt=\"enter image description here\"></a></p>",
		"Question_answer_count": 0,
		"Question_comment_count": 5,
		"Question_creation_date": "2018-12-09 18:03:31.580000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": "2018-12-09 23:31:13.427000 UTC",
		"Question_score": 1,
		"Question_tags": "python|apache-spark|jupyter-notebook|pyspark-sql|amazon-sagemaker",
		"Question_view_count": 1911,
		"Owner_creation_date": "2017-06-12 15:24:21.713000 UTC",
		"Owner_last_access_date": "2019-07-24 07:17:14.217000 UTC",
		"Owner_location": "Washing DC",
		"Owner_reputation": 67,
		"Owner_up_votes": 7,
		"Owner_down_votes": 0,
		"Owner_views": 9,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "sagemaker",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 54476091,
		"Question_title": "AWS Sagemaker - ClientError: An error occurred (ValidationException) when calling the CreateTransformJob operation",
		"Question_body": "<p>I am trying to run a batch transform job on AWS sagemaker. However, I keep getting the following error when I create a transformer and run the transform method:</p>\n\n<pre><code>ClientError: An error occurred (ValidationException) when calling the CreateTransformJob operation: Could not find model \"arn:aws:sagemaker:eu-west-1:775938635291:model/tensorflow-yolov3-2019-01-31-15-41-17-966\".\n</code></pre>\n\n<p>This is the part of my code where I run the transform method:</p>\n\n<pre><code># Initialize the transformer object\ntransformer = sagemaker.transformer.Transformer(\n    base_transform_job_name='Batch-Transform',\n    model_name=\"tensorflow-xxxxx-xxxx-xx-xx-xx-xx-xx-xx\",\n    instance_count=1,\n    instance_type='ml.c4.xlarge',\n    output_path=output_location,\n    accept='application/json', \n    sagemaker_session=sage.Session(),\n    max_payload = 100,\n    max_concurrent_transforms = 5\n    )\n\n# To start a transform job:\ntransformer.transform(input_location, content_type='application/json')\n# Then wait until transform job is completed\ntransformer.wait()\n</code></pre>\n\n<p>Anybody that can explain why I could get the error above? </p>",
		"Question_answer_count": 1,
		"Question_comment_count": 1,
		"Question_creation_date": "2019-02-01 09:02:13.420000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": null,
		"Question_score": 2,
		"Question_tags": "amazon-web-services|machine-learning|amazon-sagemaker",
		"Question_view_count": 3185,
		"Owner_creation_date": "2014-03-12 14:46:04.327000 UTC",
		"Owner_last_access_date": "2022-09-23 14:06:51.510000 UTC",
		"Owner_location": null,
		"Owner_reputation": 867,
		"Owner_up_votes": 292,
		"Owner_down_votes": 10,
		"Owner_views": 81,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "sagemaker",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 68598991,
		"Question_title": "Keras Predictions and Sagemaker Predictions are different",
		"Question_body": "<p>I am Trying to deploy a tensorflow keras model using amazon sagemaker. Process finishes successfully, yet i get different prediction results when predicted directly using keras and when calling sagemaker endpoint to make predictions.</p>\n<p>I used <a href=\"https://aws.amazon.com/blogs/machine-learning/deploy-trained-keras-or-tensorflow-models-using-amazon-sagemaker/\" rel=\"nofollow noreferrer\">these</a> steps in deploying the model to sagemaker.</p>\n<p>Check the following example.</p>\n<pre><code>data = np.random.randn(1, 150, 150, 3)\n\n# predict using amazon sagemaker\nsagemaker_predict = uncompiled_predictor.predict(data)\nprint(sagemaker_predict)\n\n#predict same using keras\nval = model.predict(data)\nprint(val)\n\n\n&gt;&gt;{'predictions': [[0.491645753]]}\n[[0.]]\n</code></pre>\n<p>Is this something supposed to happen? For my knowledge it should be the same. For some reason data gets corrupted or sagemaker weights get reinitialized. Any ideas?</p>",
		"Question_answer_count": 2,
		"Question_comment_count": 0,
		"Question_creation_date": "2021-07-31 04:04:43.240000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": null,
		"Question_score": 1,
		"Question_tags": "python|tensorflow|machine-learning|keras|amazon-sagemaker",
		"Question_view_count": 173,
		"Owner_creation_date": "2014-10-06 09:48:30.500000 UTC",
		"Owner_last_access_date": "2022-09-25 02:34:09.790000 UTC",
		"Owner_location": "Australia",
		"Owner_reputation": 530,
		"Owner_up_votes": 23,
		"Owner_down_votes": 2,
		"Owner_views": 97,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "sagemaker",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 64703268,
		"Question_title": "How to use AWS SageMaker and S3 for Object Detection?",
		"Question_body": "<p>I am looking to run a pre-trained object detection model onto a folder of ~400k images which is about 1.5GB. When I've tried running locally, it was estimated to take ~8 days to complete (with keras yolov3). Thus, I am looking to use AWS SageMaker and S3.</p>\n<p>When I have uploaded the zip folder of my images in the SageMaker jupyter notebook and tried to unzip by using bash command, an error pops ups saying that I have insufficient space. The volume assigned to my notebook is 5GB EBS, I do have other heavy datasets in my jupyter notebook space which could be causing this issue.</p>\n<p>To tackle that, I am looking for a way where I can upload my data to S3 and run SageMaker to read the images hosted and run an object detection model over. However, it does not look like there's a method to unzip folders on S3 without using an additional service (read that AWS Lambda may help) as these services are paid by my school.</p>\n<p>I could possibly re-run my code to extract my images from URL. In this case, how can I save these images to S3 directly in this case? Also, does anyone know if I am able to run yolov3 on SageMaker or if there is a better model I can look to use. Appreciate any advice that may help.</p>",
		"Question_answer_count": 1,
		"Question_comment_count": 5,
		"Question_creation_date": "2020-11-05 18:28:16.053000 UTC",
		"Question_favorite_count": 1.0,
		"Question_last_edit_date": null,
		"Question_score": 0,
		"Question_tags": "amazon-web-services|amazon-s3|object-detection|amazon-sagemaker",
		"Question_view_count": 256,
		"Owner_creation_date": "2017-10-13 07:37:10.153000 UTC",
		"Owner_last_access_date": "2022-09-20 08:00:20.060000 UTC",
		"Owner_location": "Singapore",
		"Owner_reputation": 180,
		"Owner_up_votes": 268,
		"Owner_down_votes": 0,
		"Owner_views": 29,
		"Answer_body": "<p>yes u are right u can upload thousands of images using aws cli using $aws s3 cp  ; or $aws s3 sync</p>",
		"Answer_comment_count": 0.0,
		"Answer_creation_date": "2020-11-06 02:23:45.800000 UTC",
		"Answer_last_edit_date": null,
		"Answer_score": 0.0,
		"Question_valid_tags": "sagemaker",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 68072684,
		"Question_title": "Call Sagemaker endpoint invoke_endpoint in lambda function, question for request body formats",
		"Question_body": "<p>I have a deployed Sagemaker endpoint. When testing the endpoint using <code>Predictor.predict</code>, the endpoint works fine. I can pass down whichever Json format, it is able to process it correctly. However, I've been struggling calling endpoint from Lambda by using <code>client.invoke_endpoint</code></p>\n<p>I am trying to modify my request body to follow this format in this <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/cdf-inference.html\" rel=\"nofollow noreferrer\">AWS documentation</a>.</p>\n<pre><code>let request = {\n  // Instances might contain multiple rows that predictions are sought for.\n  &quot;instances&quot;: [\n    {\n      // Request and algorithm specific inference parameters.\n      &quot;configuration&quot;: {},\n      // Data in the specific format required by the algorithm.\n      &quot;data&quot;: {\n         &quot;&lt;field name&gt;&quot;: dataElement\n       }\n    }\n  ]\n}\n</code></pre>\n<p>I am not sure what should the configuration be, so this is what my request body looks like.</p>\n<pre><code>{\n  &quot;instances&quot;: [\n    {\n      &quot;data&quot;: {\n        &quot;ID&quot;: &quot;some ID&quot;,\n        &quot;ACCOUNT&quot;: null,\n        &quot;LEAD&quot;: some ID\n        &quot;FORMNAME&quot;: &quot;some Form&quot;,\n        &quot;UTMMEDIUM&quot;: &quot;some Medium&quot;,\n        &quot;UTMSOURCE&quot;: &quot;some Source&quot;\n      }\n    },\n    {\n      &quot;data&quot;: {\n        &quot;ID&quot;: &quot;some ID&quot;\n        &quot;ACCOUNT&quot;: &quot;some ID&quot;\n        &quot;LEAD&quot;: null,\n        &quot;FORMNAME&quot;: &quot;some Form&quot;\n        &quot;UTMMEDIUM&quot;: null,\n        &quot;UTMSOURCE&quot;: null\n      }\n    }\n  ]\n}\n</code></pre>\n<p>This is my Lambda function</p>\n<pre><code>import os\nimport io\nimport boto3\nimport json\n\n# grab environment variables\nENDPOINT_NAME = 'xxxxx'\n\nclient = boto3.client('sagemaker-runtime')\ndef lambda_handler(event, context):\n    print(&quot;Received event: &quot; + json.dumps(event, indent=2))\n    data = json.loads(json.dumps(event))\n    payload = str(data[&quot;instances&quot;])\n    response = client.invoke_endpoint(EndpointName = ENDPOINT_NAME,\n                                      Body = payload,\n                                      ContentType = 'application/json',\n                                      Accept = 'application/json')\n    print(response)\n    return 'nothing'\n</code></pre>\n<p>I am able to invoke the Endpoint using the code above, but the endpoint kept having trouble processing the input. The <code>input_fn</code> in the endpoint looks like this</p>\n<pre><code>def input_fn(input_data, content_type):\n\n    if content_type == 'text/csv':\n        # Read the raw input data as CSV. \n        df = pd.read_csv(StringIO(input_data))\n\n        return df\n    \n    elif content_type == 'application/json':\n        print('input_fn (elif): input_data')\n        print(input_data)\n        print(type(input_data))\n        print('input_fn (elif): input_data eval')\n        print(eval(input_data))\n        print('input_fn (elif): input_data eval type')\n        print(type(eval(input_data)))\n        df = pd.read_json(eval(input_data))\n        print('input_fn (elif): df.columns')\n        print(df.columns)\n        return df\n    \n    else:\n        raise ValueError(&quot;{} not supported by script!&quot;.format(content_type))\n</code></pre>\n<p>The error I got is<code>ValueError: Invalid file path or buffer object type: &lt;class 'list'&gt;</code></p>\n<p>The type of <code>input_data</code> is a string, and the type of <code>eval(input_data)</code> is a list.</p>\n<p>I appreciate any insight! I've tried so many different things, including removing <code>eval</code> from my <code>input_fn</code>, or change <code>pd.json_read</code> to <code>json.loads(json.dumps())</code> with <code>pd.DataFrame.from_dict</code>. I've gotten different errors like <code>json.decoder.JSONDecodeError: Expecting value: Line Column 42</code> (column 42 is where the location of null), and <code>unhashable type: 'dict'</code></p>\n<p>I am really confused and not sure what to to next. Thank  you!</p>",
		"Question_answer_count": 2,
		"Question_comment_count": 0,
		"Question_creation_date": "2021-06-21 18:01:35.403000 UTC",
		"Question_favorite_count": 1.0,
		"Question_last_edit_date": "2021-06-21 18:14:30.717000 UTC",
		"Question_score": 0,
		"Question_tags": "python|amazon-web-services|aws-lambda|amazon-sagemaker",
		"Question_view_count": 1811,
		"Owner_creation_date": "2020-09-29 01:04:33.190000 UTC",
		"Owner_last_access_date": "2021-06-28 19:18:06.517000 UTC",
		"Owner_location": "Boston, MA, USA",
		"Owner_reputation": 1,
		"Owner_up_votes": 0,
		"Owner_down_votes": 0,
		"Owner_views": 4,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "sagemaker",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 72653823,
		"Question_title": "Inconsistent keras model.summary() output shapes on AWS SageMaker and EC2",
		"Question_body": "<p>I have the following model in a jupyter notebook:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import layers\n\n\n\nphysical_devices = tf.config.list_physical_devices('GPU')\ntf.config.experimental.set_memory_growth(physical_devices[0], True)\n\nSIZE = (549, 549)\nSHUFFLE = False \nBATCH = 32\nEPOCHS = 20\n\ntrain_datagen =  DataGenerator(train_files, batch_size=BATCH, dim=SIZE, n_channels=1, shuffle=SHUFFLE)\ntest_datagen =  DataGenerator(test_files, batch_size=BATCH, dim=SIZE, n_channels=1, shuffle=SHUFFLE)\n\n\ninp = layers.Input(shape=(*SIZE, 1))\n\nx = layers.Conv2D(filters=549, kernel_size=(5,5), padding=&quot;same&quot;, activation=&quot;relu&quot;)(inp)\nx = layers.BatchNormalization()(x)\n\n\nx = layers.Conv2D(filters=549, kernel_size=(3, 3), padding=&quot;same&quot;, activation=&quot;relu&quot;)(x)\nx = layers.BatchNormalization()(x)\n\n\nx = layers.Conv2D(filters=549, kernel_size=(1, 1), padding=&quot;same&quot;, activation=&quot;relu&quot;)(x)\nx = layers.BatchNormalization()(x)\n\nx = layers.Conv2D(filters=549, kernel_size=(3, 3), padding=&quot;same&quot;, activation=&quot;sigmoid&quot;)(x)\n\nmodel = Model(inp, x)\n\nmodel.compile(loss=tf.keras.losses.binary_crossentropy, optimizer=Adam())\n\nmodel.summary()\n</code></pre>\n<p>Sagemaker and EC2 are running tensorflow 2.7.1. The EC2 instance is p3.2xlarge with Deep Learning AMI GPU TensorFlow 2.7.0 (Amazon Linux 2) 20220607. The SageMaker notebook is using ml.p3.2xlarge and I am using the conda_tensorflow2_p38 kernel. The notebook is in an FSx Lustre file system that is mounted to both SageMaker and EC2 so it is definitely the same code running on both machines.</p>\n<p>nvidia-smi output on SageMaker:</p>\n<pre><code>+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla V100-SXM2...  On   | 00000000:00:1E.0 Off |                    0 |\n| N/A   37C    P0    24W / 300W |      0MiB / 16384MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n</code></pre>\n<p>nvidia-smi output on EC2:</p>\n<pre><code>+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla V100-SXM2...  On   | 00000000:00:1E.0 Off |                    0 |\n| N/A   42C    P0    51W / 300W |   2460MiB / 16384MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|    0   N/A  N/A     11802      C   /bin/python3.8                    537MiB |\n|    0   N/A  N/A     26391      C   python3.8                        1921MiB |\n+-----------------------------------------------------------------------------+\n</code></pre>\n<p>The model.summary() output on SageMaker is (this is what I want it to be):</p>\n<pre class=\"lang-py prettyprint-override\"><code>Model: &quot;model&quot;\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, 549, 549, 1)]     0         \n                                                                 \n conv2d (Conv2D)             (None, 549, 549, 1)       7535574   \n                                                                 \n batch_normalization (BatchN  (None, 549, 549, 1)      4         \n ormalization)                                                   \n                                                                 \n conv2d_1 (Conv2D)           (None, 549, 549, 1)       2713158   \n                                                                 \n batch_normalization_1 (Batc  (None, 549, 549, 1)      4         \n hNormalization)                                                 \n                                                                 \n conv2d_2 (Conv2D)           (None, 549, 549, 1)       301950    \n                                                                 \n batch_normalization_2 (Batc  (None, 549, 549, 1)      4         \n hNormalization)                                                 \n                                                                 \n conv2d_3 (Conv2D)           (None, 549, 549, 1)       2713158   \n                                                                 \n=================================================================\nTotal params: 13,263,852\nTrainable params: 13,263,846\nNon-trainable params: 6\n\n</code></pre>\n<p>The model.summary() output on EC2 is (notice the shape change):</p>\n<pre class=\"lang-py prettyprint-override\"><code>\nModel: &quot;model&quot;\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, 549, 549, 1)]     0         \n                                                                 \n conv2d (Conv2D)             (None, 549, 549, 549)     14274     \n                                                                 \n batch_normalization (BatchN  (None, 549, 549, 549)    2196      \n ormalization)                                                   \n                                                                 \n conv2d_1 (Conv2D)           (None, 549, 549, 549)     2713158   \n                                                                 \n batch_normalization_1 (Batc  (None, 549, 549, 549)    2196      \n hNormalization)                                                 \n                                                                 \n conv2d_2 (Conv2D)           (None, 549, 549, 549)     301950    \n                                                                 \n batch_normalization_2 (Batc  (None, 549, 549, 549)    2196      \n hNormalization)                                                 \n                                                                 \n conv2d_3 (Conv2D)           (None, 549, 549, 549)     2713158   \n                                                                 \n=================================================================\nTotal params: 5,749,128\nTrainable params: 5,745,834\nNon-trainable params: 3,294\n_________________________________________________________________\n</code></pre>\n<p>One other thing that is interesting, if I change my model on the EC2 instance to:</p>\n<pre class=\"lang-py prettyprint-override\"><code>inp = layers.Input(shape=(*SIZE, 1))\n\nx = layers.Conv2D(filters=1, kernel_size=(5,5), padding=&quot;same&quot;, activation=&quot;relu&quot;)(inp)\nx = layers.BatchNormalization()(x)\n\n\nx = layers.Conv2D(filters=1, kernel_size=(3, 3), padding=&quot;same&quot;, activation=&quot;relu&quot;)(x)\nx = layers.BatchNormalization()(x)\n\n\nx = layers.Conv2D(filters=1, kernel_size=(1, 1), padding=&quot;same&quot;, activation=&quot;relu&quot;)(x)\nx = layers.BatchNormalization()(x)\n\nx = layers.Conv2D(filters=1, kernel_size=(3, 3), padding=&quot;same&quot;, activation=&quot;sigmoid&quot;)(x)\n\nmodel = Model(inp, x)\n\nmodel.compile(loss=tf.keras.losses.binary_crossentropy, optimizer=Adam())\n</code></pre>\n<p>My model.summary() output becomes:</p>\n<pre class=\"lang-py prettyprint-override\"><code>Model: &quot;model_2&quot;\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_3 (InputLayer)        [(None, 549, 549, 1)]     0         \n                                                                 \n conv2d_8 (Conv2D)           (None, 549, 549, 1)       26        \n                                                                 \n batch_normalization_6 (Batc  (None, 549, 549, 1)      4         \n hNormalization)                                                 \n                                                                 \n conv2d_9 (Conv2D)           (None, 549, 549, 1)       10        \n                                                                 \n batch_normalization_7 (Batc  (None, 549, 549, 1)      4         \n hNormalization)                                                 \n                                                                 \n conv2d_10 (Conv2D)          (None, 549, 549, 1)       2         \n                                                                 \n batch_normalization_8 (Batc  (None, 549, 549, 1)      4         \n hNormalization)                                                 \n                                                                 \n conv2d_11 (Conv2D)          (None, 549, 549, 1)       10        \n                                                                 \n=================================================================\nTotal params: 60\nTrainable params: 54\nNon-trainable params: 6\n_________________________________________________________________\n</code></pre>\n<p>In the last model the shape is correct but the trainable parameters is very low.</p>\n<p>Any ideas as to why the output shape is different and why this is happening with the filters?</p>",
		"Question_answer_count": 0,
		"Question_comment_count": 0,
		"Question_creation_date": "2022-06-17 02:45:17.067000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": "2022-06-17 04:19:15.147000 UTC",
		"Question_score": 0,
		"Question_tags": "python|tensorflow|keras|amazon-ec2|amazon-sagemaker",
		"Question_view_count": 39,
		"Owner_creation_date": "2020-10-02 00:29:37.080000 UTC",
		"Owner_last_access_date": "2022-09-02 11:23:54.887000 UTC",
		"Owner_location": null,
		"Owner_reputation": 131,
		"Owner_up_votes": 21,
		"Owner_down_votes": 0,
		"Owner_views": 19,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "sagemaker",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 64033258,
		"Question_title": "How to explicitly set sagemaker autopilot's validation set?",
		"Question_body": "<p>The example notebook: <a href=\"https://github.com/awslabs/amazon-sagemaker-examples/blob/master/autopilot/autopilot_customer_churn.ipynb\" rel=\"nofollow noreferrer\">https://github.com/awslabs/amazon-sagemaker-examples/blob/master/autopilot/autopilot_customer_churn.ipynb</a> states that in the Analyzing Data step:</p>\n<p><code>The dataset is analyzed and Autopilot comes up with a list of ML pipelines that should be tried out on the dataset. The dataset is also split into train and validation sets.</code></p>\n<p>Presumably, autopilot uses this validation set to select the best performing model candidates to return to the user. However, I have not found a way to manually set this validation set used by sagemaker autopilot.</p>\n<p>For example, google automl, allows users to add TRAIN, VALIDATE,TEST keywords to a data_split column to manually set which data points are in which set.</p>\n<p>Is something like this currently possible which sagemaker autopilot?</p>",
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_creation_date": "2020-09-23 17:26:01.153000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": null,
		"Question_score": 1,
		"Question_tags": "amazon-web-services|amazon-sagemaker|automl",
		"Question_view_count": 159,
		"Owner_creation_date": "2014-10-09 02:10:54.710000 UTC",
		"Owner_last_access_date": "2022-09-10 14:48:43.730000 UTC",
		"Owner_location": null,
		"Owner_reputation": 13,
		"Owner_up_votes": 0,
		"Owner_down_votes": 0,
		"Owner_views": 3,
		"Answer_body": "<p>I'm afraid you can't do this at the moment. The validation set is indeed built by Autopilot itself.</p>",
		"Answer_comment_count": 0.0,
		"Answer_creation_date": "2020-09-24 07:04:57.000000 UTC",
		"Answer_last_edit_date": null,
		"Answer_score": 0.0,
		"Question_valid_tags": "sagemaker",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 66084649,
		"Question_title": "Sagemaker Model is trying to find an old docker image",
		"Question_body": "<p>I'm stuck in a situation where Sagemaker is looking for a docker image in ECS registry which I had to remove and I can't figure out how to make it forget about that.</p>\n<p>I had to rebuild a docker with torchserve for Sagemaker. I removed the old one (let's call it <code>torchserve-old-name</code>, and uploaded a new <code>torchserve:v2</code>. For some weird reason SM is still looking for the old one.</p>\n<pre><code>from sagemaker.model import Model\nfrom sagemaker.predictor import Predictor\n\nimage = &quot;134244564256.dkr.ecr.us-west-2.amazonaws.com/torchserve:v2&quot;\nmodel_data = &quot;s3://my-models/torchserve/my-model.tar.gz&quot;\nsm_model_name = 'my-model'\n\nprint(f&quot;docker image: {image}&quot;)\n# docker image: 134244564256.dkr.ecr.us-west-2.amazonaws.com/torchserve:v2\n\ntorchserve_model = Model(model_data = model_data, \n                         image_uri = image,\n                         role  = role,\n                         predictor_cls=Predictor,\n                         name  = sm_model_name)\nendpoint_name = 'torchserve-endpoint-' + time.strftime(&quot;%Y-%m-%d-%H-%M-%S&quot;, time.gmtime())\n\npredictor = torchserve_model.deploy(instance_type='ml.m4.xlarge',\n                                    initial_instance_count=1,\n                                    endpoint_name = endpoint_name)\n</code></pre>\n<p>Results in:</p>\n<pre><code>---------------------------------------------------------------------------\nUnexpectedStatusException                 Traceback (most recent call last)\n&lt;ipython-input-12-67fff0e32d2d&gt; in &lt;module&gt;\n      3 predictor = torchserve_model.deploy(instance_type='ml.m4.xlarge',\n      4                                     initial_instance_count=1,\n----&gt; 5                                     endpoint_name = endpoint_name)\n\n~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/model.py in deploy(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, tags, kms_key, wait, data_capture_config, **kwargs)\n    762             kms_key=kms_key,\n    763             wait=wait,\n--&gt; 764             data_capture_config_dict=data_capture_config_dict,\n    765         )\n    766 \n\n~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/session.py in endpoint_from_production_variants(self, name, production_variants, tags, kms_key, wait, data_capture_config_dict)\n   3454 \n   3455             self.sagemaker_client.create_endpoint_config(**config_options)\n-&gt; 3456         return self.create_endpoint(endpoint_name=name, config_name=name, tags=tags, wait=wait)\n   3457 \n   3458     def expand_role(self, role):\n\n~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/session.py in create_endpoint(self, endpoint_name, config_name, tags, wait)\n   2957         )\n   2958         if wait:\n-&gt; 2959             self.wait_for_endpoint(endpoint_name)\n   2960         return endpoint_name\n   2961 \n\n~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/session.py in wait_for_endpoint(self, endpoint, poll)\n   3243                 ),\n   3244                 allowed_statuses=[&quot;InService&quot;],\n-&gt; 3245                 actual_status=status,\n   3246             )\n   3247         return desc\n UnexpectedStatusException: Error hosting endpoint torchserve-endpoint-2021-02-06-21-39-31: Failed. Reason: \n The repository 'torchserve-old-name' does not exist in the registry with id '134244564256'..\n</code></pre>\n<p>I am 100% sure that the old image 'torchserve-old-name' has not been mentioned in this code. I've restarted and re-ran notebook.</p>\n<p>Where is it cached, and how can I clear that cache? Sagemaker function documentation doesn't seem to mention it.</p>",
		"Question_answer_count": 0,
		"Question_comment_count": 1,
		"Question_creation_date": "2021-02-07 04:47:16.417000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": "2021-02-14 12:04:31.143000 UTC",
		"Question_score": 3,
		"Question_tags": "docker|amazon-sagemaker",
		"Question_view_count": 88,
		"Owner_creation_date": "2012-10-03 10:06:31.847000 UTC",
		"Owner_last_access_date": "2022-09-25 03:37:17.877000 UTC",
		"Owner_location": "Zurich, Switzerland",
		"Owner_reputation": 12050,
		"Owner_up_votes": 2598,
		"Owner_down_votes": 4,
		"Owner_views": 488,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "sagemaker",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 50871651,
		"Question_title": "Python Notebook Invoke Endpoint Sagemaker from Local",
		"Question_body": "<p>I am trying to invoke an Amazon Sagemaker Endpoint from a local python notebook. This is the code I am using.  </p>\n\n<pre><code>import boto3\n\naws_access_key_id = '...............'\naws_secret_access_key = '................'\ntkn = '..........'\nregion_name = '............'\n\namz = boto3.client('sagemaker-runtime',\n                   aws_access_key_id=aws_access_key_id,\n                   aws_secret_access_key=aws_secret_access_key,\n                   aws_session_token=tkn,\n                   region_name=region_name)\n\n\nresponse = amz.invoke_endpoint(\n    EndpointName='mymodel',\n    Body=b'bytes'\n)               \n</code></pre>\n\n<p>However, this doesn't work. Do I have to specify something else in <em>Body</em> ?   </p>",
		"Question_answer_count": 2,
		"Question_comment_count": 0,
		"Question_creation_date": "2018-06-15 08:19:27.617000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": null,
		"Question_score": 1,
		"Question_tags": "python|amazon-sagemaker",
		"Question_view_count": 3972,
		"Owner_creation_date": "2015-05-26 22:53:10.120000 UTC",
		"Owner_last_access_date": "2019-06-03 07:51:39.510000 UTC",
		"Owner_location": null,
		"Owner_reputation": 455,
		"Owner_up_votes": 7,
		"Owner_down_votes": 0,
		"Owner_views": 58,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "sagemaker",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 62155269,
		"Question_title": "A function/API to list out the supported hyperparameters of SageMaker built-in algorithm",
		"Question_body": "<p>I am looking for a function/API that can list out all the hyperparameters supported by the built-in algorithms on SageMaker. I fully understand that the best place to look at this list (manually though) is this link:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html\" rel=\"nofollow noreferrer\">https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html</a></p>\n\n<p>But I have a requirement in which the Python code should have the capability to fetch all available hyperparameters for an algorithm.</p>\n\n<p>Is there a way to do this?</p>\n\n<p>An added bonus to this would be to also get the supported data-types, Scaling type, Value / Range supported for the respective hyperparameters.</p>",
		"Question_answer_count": 0,
		"Question_comment_count": 1,
		"Question_creation_date": "2020-06-02 15:16:45.047000 UTC",
		"Question_favorite_count": 1.0,
		"Question_last_edit_date": null,
		"Question_score": 1,
		"Question_tags": "python|amazon-web-services|amazon-sagemaker|hyperparameters",
		"Question_view_count": 36,
		"Owner_creation_date": "2019-05-17 19:09:24.560000 UTC",
		"Owner_last_access_date": "2022-09-24 23:45:42.990000 UTC",
		"Owner_location": null,
		"Owner_reputation": 2025,
		"Owner_up_votes": 86,
		"Owner_down_votes": 6,
		"Owner_views": 94,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "sagemaker",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 70492796,
		"Question_title": "What framework does Sagemaker use to train a model?",
		"Question_body": "<p>Models trained on Sagemaker produced <code>model.tar.gz</code> files as output.<br />\nI want to use these files for prediction on my local machine.</p>\n<p>I'm not sure which framework is being used by Sgaemaker to train these models.</p>\n<p>I guess I'd have to use the same framework to load the model artifacts for prediction.</p>",
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_creation_date": "2021-12-27 07:38:08.077000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": null,
		"Question_score": 0,
		"Question_tags": "amazon-web-services|amazon-sagemaker",
		"Question_view_count": 27,
		"Owner_creation_date": "2019-03-04 12:06:56.487000 UTC",
		"Owner_last_access_date": "2022-09-23 07:48:34.613000 UTC",
		"Owner_location": null,
		"Owner_reputation": 735,
		"Owner_up_votes": 167,
		"Owner_down_votes": 15,
		"Owner_views": 106,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "sagemaker",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 71308112,
		"Question_title": "How to return a float with 'text/csv' as \"Content-Type\" from SageMaker endpoint that uses custom inference code?",
		"Question_body": "<p>I am trying to return output(or predictions) from a SageMaker endpoint with 'text/csv' or 'text/csv; charset=utf-8' as &quot;Content-Type&quot;. I have tried multiple ways, but the sagemaker always returns with 'text/html; charset=utf-8' as the &quot;Content-Type&quot;, and I would like SageMaker to return 'text/csv' or 'text/csv; charset=utf-8'.</p>\n<p>Here's the <a href=\"https://sagemaker.readthedocs.io/en/stable/frameworks/sklearn/using_sklearn.html#process-output\" rel=\"nofollow noreferrer\"><code>output_fn</code></a> from my inference-code:</p>\n<pre><code>** my other code **\ndef output_fn(prediction, content_type='text/csv'):\n    ** my other code **\n    return output_float\n</code></pre>\n<p>above function returns number with float data-type and I got error(in cloudwatch logs) that  this function should only be returning string, tuple, dict or Respoonse instance.</p>\n<p>So, here are all the different ways I have tried to have SageMaker return my number with 'text/csv' but only receives 'text/html; charset=utf-8'</p>\n<ol>\n<li><code>return json.dumps(output_float)</code>. this sent 'text/html; charset=utf-8'.</li>\n<li><code>return f&quot;{output_float}&quot;</code>. this sent 'text/html; charset=utf-8'.</li>\n<li><code>return f&quot;{output_float},&quot;</code>. this sent 'text/html; charset=utf-8'.</li>\n<li><code>return f&quot;{output_float}\\n&quot;</code>. this sent 'text/html; charset=utf-8'.</li>\n<li><code>return f&quot;{output_float},\\n&quot;</code>. this sent 'text/html; charset=utf-8'.</li>\n<li>by using <a href=\"https://sagemaker.readthedocs.io/en/stable/api/inference/serializers.html\" rel=\"nofollow noreferrer\"><code>sagemaker.serializers.CSVSerializer</code></a> like this:\n<pre><code>from sagemaker.serializers import CSVSerializer\ncsv_serialiser = CSVSerializer(content_type='text/csv')\n\ndef output_fn(prediction, content_type='text/csv'):\n    ** my other code **\n    return csv_serialiser.serialize(output_float)\n</code></pre>\nI got <code>'NoneType' object has no attribute 'startswith'</code> error with this.</li>\n<li>as a tuple: <code>return (output_float,)</code>. I haven't noted down what this did, but it sure didn't return the number with 'text/csv' as &quot;Content-Type&quot;.</li>\n<li>made changes in my model object to return a float on calling <code>.predict_proba</code> on my model-object and deployed it from sagemaker studio without using any custom inference code and deployed this from SageMaker studio. but got this error after sending a request to the endpoint: <code>'NoneType' object has no attribute 'startswith'</code>, but at my side, when I pass proper inputs to the unpicked model and call .predict_proba, i get float as expected.</li>\n<li>by returning <a href=\"https://tedboy.github.io/flask/generated/generated/flask.Response.html#flask.Response\" rel=\"nofollow noreferrer\"><code>flask.Response</code></a> like this:\n<pre><code>from flask import Response\n\ndef output_fn(prediction, content_type='text/csv'):\n    ** my other code **\n    return Response(response=output_float, status=200, headers={'Content-Type':'text/csv; charset=utf-8'})\n</code></pre>\nbut, I got some IndexError with this(I didn't notedown the traceback.)</li>\n</ol>\n<p>Some other info:</p>\n<ol>\n<li>Model I am using is completely outside of SageMaker, not from a training job or anything like that.</li>\n<li>All of the aforementioned endpoints have been deployed entirely from aws-cli with relevant .json files, except the one in point-8 above.</li>\n</ol>\n<p>How do I return number with &quot;text/csv&quot; as content-type from sagemaker? I need my output in &quot;text/csv&quot; content-type specifically for Model-Quality-Monitor. How do I do this?</p>",
		"Question_answer_count": 1,
		"Question_comment_count": 2,
		"Question_creation_date": "2022-03-01 11:54:47.137000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": "2022-03-07 11:21:09.630000 UTC",
		"Question_score": 0,
		"Question_tags": "amazon-web-services|amazon-sagemaker",
		"Question_view_count": 461,
		"Owner_creation_date": "2019-06-07 12:24:06.180000 UTC",
		"Owner_last_access_date": "2022-09-24 17:19:11.323000 UTC",
		"Owner_location": "Bengaluru, Karnataka, India",
		"Owner_reputation": 2046,
		"Owner_up_votes": 2858,
		"Owner_down_votes": 5,
		"Owner_views": 369,
		"Answer_body": "<p>From the <a href=\"https://github.com/aws/amazon-sagemaker-examples/blob/main/advanced_functionality/scikit_bring_your_own/container/decision_trees/predictor.py#L88\" rel=\"nofollow noreferrer\">scikit_bring_your_own</a> example, I suggest testing by setting the Response as follows:</p>\n<pre><code>return flask.Response(response= output_float, status=200, mimetype=&quot;text/csv&quot;)\n</code></pre>",
		"Answer_comment_count": 1.0,
		"Answer_creation_date": "2022-03-07 20:45:03.080000 UTC",
		"Answer_last_edit_date": null,
		"Answer_score": 1.0,
		"Question_valid_tags": "sagemaker",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 69533933,
		"Question_title": "How to pass environment variables while deploying sagemaker endpoint?",
		"Question_body": "<p>I'm trying to send a parameter as an environment variable for my deployed model. I ran a hyperparameter tuning run and I want to pass the string for one of the model parameters into the deployed endpoint.</p>\n<p>I'm loading my trained PyTorch model with:</p>\n<pre><code>inference_model = PyTorchModel(\nentry_point=&quot;inference.py&quot;,\nsource_dir=&quot;serve&quot;,\nrole=role,\nmodel_data=model_data_s3_path,\nenv={'MODEL_ARCHITECTURE': best_architecture_name}, \nframework_version=&quot;1.8.1&quot;,\npy_version=&quot;py36&quot;,\n)\n</code></pre>\n<p>I'm using &quot;env&quot; to try and grab it during inference, but it doesn't seem to work when I use <code>os.environ['MODEL_ARCHITECTURE']</code>. I get an error that there is no such environment variable? What am I missing? Should I even be passing the string this way?</p>\n<p>I'm also using <code>os.environ['MODEL_ARCHITECTURE']</code> in the <code>model.py</code> script, which is called in <code>inference.py</code>.</p>\n<p>How can I pass the string along? I need to do it this way (it needs to be automated, I can't change the model name by hand) since I'll be handing it off to some people to run it straight through and they can't manually change the architecture name.</p>\n<p>EDIT: I tried SM_ARCHITECTURE_NAME since SageMaker often adds &quot;SM_&quot; to the environment variables / hyperparameters during training, but that didn't work either.</p>",
		"Question_answer_count": 1,
		"Question_comment_count": 1,
		"Question_creation_date": "2021-10-12 00:56:33.163000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": "2021-10-12 20:58:31.723000 UTC",
		"Question_score": 3,
		"Question_tags": "amazon-sagemaker",
		"Question_view_count": 586,
		"Owner_creation_date": "2016-07-19 00:48:21.237000 UTC",
		"Owner_last_access_date": "2022-09-13 07:31:55.037000 UTC",
		"Owner_location": null,
		"Owner_reputation": 819,
		"Owner_up_votes": 42,
		"Owner_down_votes": 0,
		"Owner_views": 87,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "sagemaker",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 54216797,
		"Question_title": "SageMaker Javascript SDK Endpoint Invocation Error: \"CustomerError: Unable to parse payload to numeric values\"",
		"Question_body": "<p>I was invoking the SageMaker endpoint from my angular front-end when I came across this error on AWS CloudWatch regarding my model making inferences from the data (In the form of a comma-separated string with target values at the first index) I was sending : <strong>Unable to parse numeric values</strong>. The string I'm using to invoke the endpoint was : \"1533071820,0.05619,0.05619,0.05611,0.05611,0.006076\\n\"</p>\n\n<p>String request = \"1533071820,0.05619,0.05619,0.05611,0.05611,0.006076\\n\"</p>\n\n<p>ByteBufferbuf = ByteBuffer.wrap(request.getBytes());\ninvokeEndpointRequest.setBody(buf);\n<code>Use the SageMaker API</code>\nAmazonSageMakerRuntime amazonSageMaker = AmazonSageMakerRuntimeClientBuilder.defaultClient();\n<code>Invoke the model endpoint on SageMaker</code>\nInvokeEndpointResult invokeEndpointResult = amazonSageMaker.invokeEndpoint(invokeEndpointRequest);</p>\n\n<p>The result I was expecting from the endpoint is a JSON object with the 'score' attribute in the format : {\"predictions\": [{\"score\": xxxxxxx}]}</p>\n\n<p>I am getting a 'ModelError: Unable to evaluate payload' from the IDE logs and 'Unable to parse numeric values on CloudWatch'</p>",
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_creation_date": "2019-01-16 12:10:27.280000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": null,
		"Question_score": 0,
		"Question_tags": "javascript|node.js|aws-sdk|amazon-sagemaker",
		"Question_view_count": 305,
		"Owner_creation_date": "2015-08-14 13:47:37.473000 UTC",
		"Owner_last_access_date": "2020-01-22 10:30:18.243000 UTC",
		"Owner_location": "Johannesburg North, Randburg, Gauteng, South Africa",
		"Owner_reputation": 29,
		"Owner_up_votes": 14,
		"Owner_down_votes": 0,
		"Owner_views": 6,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "sagemaker",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 70582506,
		"Question_title": "Read Excel file in Amazon Sage maker using R Notebook",
		"Question_body": "<p>I am having S3 bucket named &quot;Temp-Bucket&quot;. Inside that I am having folder named &quot;folder&quot;.\nI want to read file named file1.xlsx. This file is present inside the S3 bucket(Temp-Bucket) under the folder (folder). How to read that file ?</p>",
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_creation_date": "2022-01-04 17:05:20.903000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": null,
		"Question_score": 0,
		"Question_tags": "r|file|amazon-sagemaker",
		"Question_view_count": 121,
		"Owner_creation_date": "2020-04-24 10:34:21.987000 UTC",
		"Owner_last_access_date": "2022-08-13 06:30:57.293000 UTC",
		"Owner_location": null,
		"Owner_reputation": 95,
		"Owner_up_votes": 39,
		"Owner_down_votes": 0,
		"Owner_views": 30,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "sagemaker",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	}
]