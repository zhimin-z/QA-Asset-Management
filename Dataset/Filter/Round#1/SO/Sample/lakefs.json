[
	{
		"Question_id": 70337958,
		"Question_title": "Do I need garbage collector when I delete object from branch by API?",
		"Question_body": "<p>Do I need a garbage collector in LakeFS when I delete an object from a branch by API?\nUsing appropriate method of course.\nDo I understand right that the garbage collector is used only for objects that are deleted by a commit. And this objects are soft deleted (by the commit). And if I use the delete API method than the object is hard deleted and I don\u2019t need to invoke the garbage collector?</p>",
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_creation_date": "2021-12-13 16:37:42.713000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": null,
		"Question_score": 4,
		"Question_tags": "lakefs",
		"Question_view_count": 67,
		"Owner_creation_date": "2020-03-03 05:43:31.067000 UTC",
		"Owner_last_access_date": "2022-09-24 15:23:44.207000 UTC",
		"Owner_location": null,
		"Owner_reputation": 73,
		"Owner_up_votes": 2,
		"Owner_down_votes": 0,
		"Owner_views": 14,
		"Answer_body": "<p>lakeFS manages versions of your data.  So deletions only affect successive versions.  The object itself remains, and can be accessed by accessing an older version.</p>\n<p>Garbage collection removes the underlying files.  Once the file is gone, its key is still <em>visible</em> in older versions, but if you try to access the file itself you will receive HTTP status code <code>410 Gone</code>.</p>\n<p>For full information, please see the <a href=\"https://docs.lakefs.io/reference/garbage-collection.html\" rel=\"nofollow noreferrer\">Garbage collection</a> docs.</p>",
		"Answer_comment_count": 0.0,
		"Answer_creation_date": "2021-12-14 07:58:15.777000 UTC",
		"Answer_last_edit_date": null,
		"Answer_score": 4.0,
		"Question_valid_tags": "lakefs",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 70141714,
		"Question_title": "How to hard delete objects older than n-days in LakeFS?",
		"Question_body": "<p>How to find and hard delete objects older than n-days in LakeFS? Later it'll be a scheduled job.</p>",
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_creation_date": "2021-11-28 08:23:22.253000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": "2021-11-28 11:02:06.610000 UTC",
		"Question_score": 1,
		"Question_tags": "object-storage|lakefs",
		"Question_view_count": 114,
		"Owner_creation_date": "2020-03-03 05:43:31.067000 UTC",
		"Owner_last_access_date": "2022-09-24 15:23:44.207000 UTC",
		"Owner_location": null,
		"Owner_reputation": 73,
		"Owner_up_votes": 2,
		"Owner_down_votes": 0,
		"Owner_views": 14,
		"Answer_body": "<p>To do that you should use the <a href=\"https://docs.lakefs.io/reference/garbage-collection.html\" rel=\"nofollow noreferrer\">Garbage Collection</a> (GC) feature in lakeFS.</p>\n<p><strong>Note:</strong> This feature cleans objects from the storage only after they are deleted from your branches in lakeFS.</p>\n<p>You will need to:</p>\n<ol>\n<li><p>Define GC rules to set your desired retention period.</p>\n<p>From the lakeFS UI, go to the repository you would like to hard delete objects from -&gt; Settings -&gt; Retention, and define the GC rule for each branch under the repository. For example -</p>\n<pre><code>{\n    &quot;default_retention_days&quot;: 21,\n    &quot;branches&quot;: [\n        {&quot;branch_id&quot;: &quot;main&quot;, &quot;retention_days&quot;: 28},\n        {&quot;branch_id&quot;: &quot;dev&quot;, &quot;retention_days&quot;: 7}\n    ]\n}\n</code></pre>\n</li>\n<li><p>Run the GC Spark job that does the actual cleanup, with -</p>\n<pre><code>spark-submit --class io.treeverse.clients.GarbageCollector \\\n  -c spark.hadoop.lakefs.api.url=https://lakefs.example.com:8000/api/v1  \\\n  -c spark.hadoop.lakefs.api.access_key=&lt;LAKEFS_ACCESS_KEY&gt; \\\n  -c spark.hadoop.lakefs.api.secret_key=&lt;LAKEFS_SECRET_KEY&gt; \\\n  -c spark.hadoop.fs.s3a.access.key=&lt;S3_ACCESS_KEY&gt; \\\n  -c spark.hadoop.fs.s3a.secret.key=&lt;S3_SECRET_KEY&gt; \\\n  --packages io.lakefs:lakefs-spark-client-301_2.12:0.5.0 \\\n  example-repo us-east-1\n</code></pre>\n</li>\n</ol>",
		"Answer_comment_count": 3.0,
		"Answer_creation_date": "2021-11-28 09:37:29.080000 UTC",
		"Answer_last_edit_date": "2021-11-28 10:34:21.197000 UTC",
		"Answer_score": 3.0,
		"Question_valid_tags": "lakefs",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	}
]