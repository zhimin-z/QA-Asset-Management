[
	{
		"Question_id": 65937623,
		"Question_title": "Unable to serve an mlflow model locally",
		"Question_body": "<p>I have created an mlflow model with custom pyfunc. It shows the results when I send input to the loaded model in Jupyter notebook.\nHowever if I am trying to serve it to a local port</p>\n<pre><code>!mlflow models serve -m Home/miniconda3/envs/mlruns/0/baa40963927a49258c845421e3175c06/artifacts/model -p 8001\n</code></pre>\n<p>I am getting this error</p>\n<pre><code> Traceback (most recent call last):\n  File &quot;/home/subhojyoti/miniconda3/envs/python3-env/bin/mlflow&quot;, line 10, in &lt;module&gt;\n    sys.exit(cli())\n  File &quot;/home/subhojyoti/miniconda3/envs/python3-env/lib/python3.6/site-packages/click/core.py&quot;, line 829, in __call__\n    return self.main(*args, **kwargs)\n  File &quot;/home/subhojyoti/miniconda3/envs/python3-env/lib/python3.6/site-packages/click/core.py&quot;, line 782, in main\n    rv = self.invoke(ctx)\n  File &quot;/home/subhojyoti/miniconda3/envs/python3-env/lib/python3.6/site-packages/click/core.py&quot;, line 1259, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File &quot;/home/subhojyoti/miniconda3/envs/python3-env/lib/python3.6/site-packages/click/core.py&quot;, line 1259, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File &quot;/home/subhojyoti/miniconda3/envs/python3-env/lib/python3.6/site-packages/click/core.py&quot;, line 1066, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File &quot;/home/subhojyoti/miniconda3/envs/python3-env/lib/python3.6/site-packages/click/core.py&quot;, line 610, in invoke\n    return callback(*args, **kwargs)\n  File &quot;/home/subhojyoti/miniconda3/envs/python3-env/lib/python3.6/site-packages/mlflow/models/cli.py&quot;, line 56, in serve\n    install_mlflow=install_mlflow).serve(model_uri=model_uri, port=port,\n  File &quot;/home/subhojyoti/miniconda3/envs/python3-env/lib/python3.6/site-packages/mlflow/models/cli.py&quot;, line 163, in _get_flavor_backend\n    append_to_uri_path(underlying_model_uri, &quot;MLmodel&quot;), output_path=tmp.path())\n  File &quot;/home/subhojyoti/miniconda3/envs/python3-env/lib/python3.6/site-packages/mlflow/tracking/artifact_utils.py&quot;, line 76, in _download_artifact_from_uri\n    artifact_path=artifact_path, dst_path=output_path)\n  File &quot;/home/subhojyoti/miniconda3/envs/python3-env/lib/python3.6/site-packages/mlflow/store/artifact/local_artifact_repo.py&quot;, line 67, in download_artifacts\n    return super(LocalArtifactRepository, self).download_artifacts(artifact_path, dst_path)\n  File &quot;/home/subhojyoti/miniconda3/envs/python3-env/lib/python3.6/site-packages/mlflow/store/artifact/artifact_repo.py&quot;, line 140, in download_artifacts\n    return download_file(artifact_path)\n  File &quot;/home/subhojyoti/miniconda3/envs/python3-env/lib/python3.6/site-packages/mlflow/store/artifact/artifact_repo.py&quot;, line 105, in download_file\n    self._download_file(remote_file_path=fullpath, local_path=local_file_path)\n  File &quot;/home/subhojyoti/miniconda3/envs/python3-env/lib/python3.6/site-packages/mlflow/store/artifact/local_artifact_repo.py&quot;, line 95, in _download_file\n    shutil.copyfile(remote_file_path, local_path)\n  File &quot;/home/subhojyoti/miniconda3/envs/python3-env/lib/python3.6/shutil.py&quot;, line 120, in copyfile\n    with open(src, 'rb') as fsrc:\nFileNotFoundError: [Errno 2] No such file or directory: 'Home/miniconda3/envs/mlruns/0/baa40963927a49258c845421e3175c06/artifacts/model/MLmodel'\n</code></pre>",
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_creation_date": "2021-01-28 13:01:27.777000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": null,
		"Question_score": 0,
		"Question_tags": "deployment|localhost|mlflow",
		"Question_view_count": 1277,
		"Owner_creation_date": "2019-11-14 13:58:10.560000 UTC",
		"Owner_last_access_date": "2022-09-23 08:37:32.563000 UTC",
		"Owner_location": null,
		"Owner_reputation": 115,
		"Owner_up_votes": 16,
		"Owner_down_votes": 0,
		"Owner_views": 25,
		"Answer_body": "<p>From your error traceback, the model artifact can't be located. In your code, you are executing the 'mlflow' command from within a Jupyter Notebook. I would suggest trying the following:</p>\n<ol>\n<li>Check if your models artifacts are on the path you are using Home/miniconda3/envs/mlruns/0/baa40963927a49258c845421e3175c06/artifacts/model</li>\n<li>Try opening a terminal, then <code>cd /Home/miniconda3/envs</code> and  execute <code>mlflow models serve -m ./mlruns/0/baa40963927a49258c845421e3175c06/artifacts/model -p 8001</code></li>\n<li>MLFlow offers different solutions to serve a model, you can try to register your model and refer to it as &quot;models:/{model_name}/{stage}&quot; as mentioned in the Model Registry <a href=\"https://mlflow.org/docs/latest/model-registry.html#serving-an-mlflow-model-from-model-registry\" rel=\"nofollow noreferrer\">docs</a></li>\n</ol>",
		"Answer_comment_count": 2.0,
		"Answer_creation_date": "2021-01-28 13:30:03.100000 UTC",
		"Answer_last_edit_date": null,
		"Answer_score": 2.0,
		"Question_valid_tags": "mlflow",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 59363969,
		"Question_title": "mlflow: problems with pip installation",
		"Question_body": "<p>I read through many threads regarding installation issues using pip. However, I could find a solution to help me fix my problem.\nI installed mlflow with :</p>\n\n<pre><code>    pip3 install mlflow\n</code></pre>\n\n<p>so mlflow is installed in /usr/local/bin/mlflow</p>\n\n<p>Since it is not in /Users/xxxx/opt/anaconda3/lib/python3.7/site-packages, I get \"ModuleNotFoundError: No module named 'mlflow' error when I try to run code that imports mlflow module. How should I fix this?</p>",
		"Question_answer_count": 1,
		"Question_comment_count": 1,
		"Question_creation_date": "2019-12-16 20:37:11.793000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": null,
		"Question_score": 1,
		"Question_tags": "pip|python-import|python-3.7|importerror|mlflow",
		"Question_view_count": 9843,
		"Owner_creation_date": "2018-04-26 22:59:32.553000 UTC",
		"Owner_last_access_date": "2021-12-14 18:54:28.437000 UTC",
		"Owner_location": "San Francisco, CA, USA",
		"Owner_reputation": 87,
		"Owner_up_votes": 1,
		"Owner_down_votes": 0,
		"Owner_views": 20,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "mlflow",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 73696427,
		"Question_title": "How does MLFlow track the data used in experiments?",
		"Question_body": "<p>I am just starting learning about MLFlow, so apologies if I don't use the correct terminology.</p>\n<p>I have done some coding and experiments with MLFlow, in which I named an experiment, and track some metrics, plots and even models.</p>\n<p>Later in the MLFlow UI I can see a list of experiments with their tracked elements and artifacts.</p>\n<p>My question is how does this work with datasets?</p>\n<p>For example if I use a particular data set to train , or to do inference with a model and some metrics are recorded, how can I track that a particular dataset was used to obtain a particular metric?</p>\n<p>I am imaging that the <em>entire</em> dataset is not stored, is it? Because that would use a lot of disk?</p>\n<p>Any pointers about this theme will be greatly appreciated</p>",
		"Question_answer_count": 0,
		"Question_comment_count": 0,
		"Question_creation_date": "2022-09-13 00:20:12.000000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": null,
		"Question_score": 0,
		"Question_tags": "mlflow",
		"Question_view_count": 20,
		"Owner_creation_date": "2015-01-14 01:17:49.333000 UTC",
		"Owner_last_access_date": "2022-09-24 09:09:14.427000 UTC",
		"Owner_location": null,
		"Owner_reputation": 5585,
		"Owner_up_votes": 792,
		"Owner_down_votes": 53,
		"Owner_views": 1350,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "mlflow",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 73791082,
		"Question_title": "how to registered the log_model in MLflow?",
		"Question_body": "<p>I have tried to load the deep learning model on mlflow, it's perfectly loaded, but the model is not stored in the model registry, can any one guide me how to register the model and its dataset for inference?<br />\nThanks</p>\n<p>from mlflow import MlflowClient\nexperiment_name = &quot;nlp_model&quot;</p>\n<pre><code>try:\n    exp_id = mlflow.create_experiment(name=experiment_name) # set the experiment id \nexcept Exception as e:\n    exp_id = mlflow.get_experiment_by_name(experiment_name).experiment_id\n\nwith mlflow.start_run(experiment_id=exp_id):\n    run_id = mlflow.active_run().info.run_id\n    print(run_id)\n    mlflow.sklearn.autolog(log_models=True)\n    print(mlflow.tracking.get_tracking_uri())\n    model = Demucs(**args.demucs, sample_rate=args.sample_rate)# fitting the model\n    &quot;&quot;&quot;\n    loaded model on mlflow\n    &quot;&quot;&quot;\n    mlflow.sklearn.log_model(model, &quot;nlp_model&quot;)\n    &quot;&quot;&quot;\n    saved model on mlflow model registory\n    &quot;&quot;&quot;\n    client = MlflowClient()\n    model = client.create_model_version(\n        name=&quot;denoiser_nlp_model&quot;,\n        source=f&quot;./mlruns/{exp_id}/{run_id}/artifacts/nlp_model&quot;,\n        run_id=mlflow.active_run().info.run_id\n    )\n</code></pre>\n<p>#Here is the error msg\n[1]: <a href=\"https://i.stack.imgur.com/LNAsc.png\" rel=\"nofollow noreferrer\">https://i.stack.imgur.com/LNAsc.png</a></p>",
		"Question_answer_count": 0,
		"Question_comment_count": 0,
		"Question_creation_date": "2022-09-20 18:20:39.243000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": null,
		"Question_score": 0,
		"Question_tags": "deep-learning|nlp|pytorch|mlflow",
		"Question_view_count": 12,
		"Owner_creation_date": "2020-04-02 15:17:33.983000 UTC",
		"Owner_last_access_date": "2022-09-23 11:59:36.240000 UTC",
		"Owner_location": "Lahore, Pakistan",
		"Owner_reputation": 1,
		"Owner_up_votes": 0,
		"Owner_down_votes": 0,
		"Owner_views": 0,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "mlflow",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 57676613,
		"Question_title": "Saving artifacts on remote mlflow server",
		"Question_body": "<p>I am trying to store <code>MLflow</code> artifacts on a remote server running <code>MLflow</code>. The server I am accessing from and server running <code>MLflow</code> are both VMs on google cloud. I can see the matrices in the <code>MLflow</code> server but not the artifacts.</p>\n\n<p>I tried the flollowing methods but nonoe of them is working:</p>\n\n<ul>\n<li><code>mlflow server     --backend-store-uri /mnt/persistent-disk     --default-artifact-root /tmp/ --host=0.0.0.0</code></li>\n<li>mlflow server     --backend-store-uri /mnt/persistent-disk     --default-artifact-root /path/to/folder/with/mlrun --host=0.0.0.0</li>\n</ul>\n\n<p>I also gave <code>rwx</code> permissions to the path but still getting the same error :</p>\n\n<pre><code>PermissionError: [Errno 13] Permission denied: '/home/user/folder'\n</code></pre>",
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_creation_date": "2019-08-27 14:13:07.633000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": "2019-08-27 14:30:43.157000 UTC",
		"Question_score": 3,
		"Question_tags": "python|mlflow",
		"Question_view_count": 2002,
		"Owner_creation_date": "2014-05-29 12:37:30.427000 UTC",
		"Owner_last_access_date": "2020-10-15 08:35:10.407000 UTC",
		"Owner_location": "Kuala Lumpur Federal Territory of Kuala Lumpur Malaysia",
		"Owner_reputation": 1990,
		"Owner_up_votes": 49,
		"Owner_down_votes": 1,
		"Owner_views": 268,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "mlflow",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 59881297,
		"Question_title": "How to serve custom MLflow model with Docker?",
		"Question_body": "<p>We have a project following essentially this\n<a href=\"https://github.com/mlflow/mlflow/tree/master/examples/docker\" rel=\"nofollow noreferrer\">docker example</a> with the only difference that we created a custom model similar to <a href=\"https://www.mlflow.org/docs/latest/models.html#custom-python-models\" rel=\"nofollow noreferrer\">this</a> whose code lies in a directory called <code>forecast</code>. We succeeded in running the model with <code>mlflow run</code>. The problem arises when we try to serve the model. After doing </p>\n\n<pre><code>mlflow models build-docker -m \"runs:/my-run-id/my-model\" -n \"my-image-name\"\n</code></pre>\n\n<p>we fail running the container with</p>\n\n<pre><code>docker run -p 5001:8080 \"my-image-name\"\n</code></pre>\n\n<p>with the following error:</p>\n\n<pre><code>ModuleNotFoundError: No module named 'forecast'\n</code></pre>\n\n<p>It seems that the docker image is not aware of the source code defining our custom model class.\nWith Conda environnement the problem does not arise thanks to the <code>code_path</code> argument in <code>mlflow.pyfunc.log_model</code>.</p>\n\n<p>Our Dockerfile is very basic, with just <code>FROM continuumio/miniconda3:4.7.12, RUN pip install {model_dependencies}</code>.</p>\n\n<p>How to let the docker image know about the source code for deserialising the model and run it?</p>",
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_creation_date": "2020-01-23 14:52:13.580000 UTC",
		"Question_favorite_count": 1.0,
		"Question_last_edit_date": null,
		"Question_score": 3,
		"Question_tags": "docker|mlflow",
		"Question_view_count": 2105,
		"Owner_creation_date": "2015-04-16 17:17:00.943000 UTC",
		"Owner_last_access_date": "2022-09-10 21:14:50.857000 UTC",
		"Owner_location": "Paris, France",
		"Owner_reputation": 41,
		"Owner_up_votes": 5,
		"Owner_down_votes": 0,
		"Owner_views": 20,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "mlflow",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 58917918,
		"Question_title": "How to make predictions using a model that requires an input shape with more than two dimensions using MLflow?",
		"Question_body": "<p>I'm trying to implement a tensorflow (keras) based model into mlflow while learning how it works and if it suite our needs. I'm trying to implement the Fashion MNIST example from tensorflow website <a href=\"https://www.tensorflow.org/tutorials/keras/classification?hl=it\" rel=\"nofollow noreferrer\">Here the link</a></p>\n\n<p>I was able to train and to log the model successfully into mlflow using this code:</p>\n\n<pre><code>import mlflow\nimport mlflow.tensorflow\nimport mlflow.keras\n\n# TensorFlow and tf.keras\nimport tensorflow as tf\nfrom tensorflow import keras\n\n# Helper libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nprint(tf.__version__)\n\nfashion_mnist = keras.datasets.fashion_mnist\n\n(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n\nclass_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n           'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n\ntrain_images = train_images / 255.0\n\ntest_images = test_images / 255.0\n\nmodel = keras.Sequential([\n    keras.layers.Flatten(input_shape=(28, 28)),\n    keras.layers.Dense(128, activation='relu'),\n    keras.layers.Dense(10, activation='softmax')\n])\n\nmodel.compile(optimizer='adam',\n          loss='sparse_categorical_crossentropy',\n          metrics=['accuracy'])\n\nif __name__ == \"__main__\":\n\n    model.fit(train_images, train_labels, epochs=10)\n    test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n    print('\\nTest accuracy:', test_acc)\n\n    mlflow.log_metric(\"validation accuracy\", float(test_acc))\n    mlflow.log_metric(\"validation loss\", float(test_loss))\n    mlflow.keras.log_model(model, \n                        \"model\", \n                        registered_model_name = \"Fashion MNIST\")\n</code></pre>\n\n<p>Then I'm now serving it with the models serve subcommand</p>\n\n<pre><code>$ mlflow models serve -m [model_path_here] -p 1234\n</code></pre>\n\n<p>The problem is that I'm not able to make predictions:</p>\n\n<pre><code>fashion_mnist = keras.datasets.fashion_mnist\n(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\ntrain_images = train_images / 255.0\ntest_images = test_images / 255.0\nlabels = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n           'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n\nurl = \"http://127.0.0.1:1234/invocations\"\n\nto_predict = test_images[0]\n\ndata = {\n    \"data\": [to_predict.tolist()]\n}\nheaders = {'Content-type': 'application/json', 'Accept': 'text/plain'}\nr = requests.post(url, data=json.dumps(data), headers=headers)\nres = r.json()\n</code></pre>\n\n<p>I'm getting this error:</p>\n\n<pre><code>{'error_code': 'BAD_REQUEST', 'message': 'Encountered an unexpected error while evaluating the model. Verify that the serialized input Dataframe is compatible with the model for inference.', 'stack_trace': 'Traceback (most recent call last):\\n  File \"/home/ferama/.local/lib/python3.6/site-packages/mlflow/pyfunc/scoring_server/__init__.py\", line 196, in transformation\\n    raw_predictions = model.predict(data)\\n  File \"/home/ferama/.local/lib/python3.6/site-packages/mlflow/keras.py\", line 298, in predict\\n    predicted = pd.DataFrame(self.keras_model.predict(dataframe))\\n  File \"/home/ferama/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\", line 909, in predict\\n    use_multiprocessing=use_multiprocessing)\\n  File \"/home/ferama/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_arrays.py\", line 715, in predict\\n    x, check_steps=True, steps_name=\\'steps\\', steps=steps)\\n  File \"/home/ferama/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\", line 2472, in _standardize_user_data\\n    exception_prefix=\\'input\\')\\n  File \"/home/ferama/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_utils.py\", line 564, in standardize_input_data\\n    \\'with shape \\' + str(data_shape))\\nValueError: Error when checking input: expected flatten_input to have 3 dimensions, but got array with shape (1, 28)\\n'}\n</code></pre>\n\n<p>That code above worked fine with a one dimension model</p>\n\n<p>The error seems to me related to the fact that a pandas DataFrame is a two dimensional data structure and the model instead requires a three dimensional input.</p>\n\n<p>The latest words from the error \"...but got array with shape (1, 28)\". The input shape should be (1, 28, 28) instead</p>\n\n<p>There is a way to use this kind of models with mlflow? There is a way to serialize and send numpy arrays directly as input instead of pandas dataframes?</p>",
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_creation_date": "2019-11-18 15:28:15.563000 UTC",
		"Question_favorite_count": 1.0,
		"Question_last_edit_date": null,
		"Question_score": 3,
		"Question_tags": "python|tensorflow|keras|mlflow",
		"Question_view_count": 1221,
		"Owner_creation_date": "2010-12-30 14:55:10.407000 UTC",
		"Owner_last_access_date": "2022-01-08 07:38:22.447000 UTC",
		"Owner_location": null,
		"Owner_reputation": 503,
		"Owner_up_votes": 9,
		"Owner_down_votes": 0,
		"Owner_views": 35,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "mlflow",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 58554979,
		"Question_title": "Cannot Start mlflow ui on google cloud platform virtual machine instance",
		"Question_body": "<p>after running mlflow ui on command line\nand  clicking <a href=\"http://127.0.0.1:5000/\" rel=\"nofollow noreferrer\">http://127.0.0.1:5000/</a>\ni get site cannot be reached\n127.0.0.1 refused to connect.</p>\n<p>I have already updated firewall rules on VPC network in GCP and on my local machine and activated the ports</p>\n<blockquote>\n<p>This site can\u2019t be reached127.0.0.1 refused to connect.</p>\n<p>Try:</p>\n<ul>\n<li>Checking the connection</li>\n<li>Checking the proxy and the firewall</li>\n</ul>\n<p>ERR_CONNECTION_REFUSED</p>\n</blockquote>",
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_creation_date": "2019-10-25 08:35:26.673000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": "2020-06-20 09:12:55.060000 UTC",
		"Question_score": 0,
		"Question_tags": "google-cloud-platform|mlflow",
		"Question_view_count": 360,
		"Owner_creation_date": "2018-01-21 16:42:32.317000 UTC",
		"Owner_last_access_date": "2021-03-20 09:36:52.253000 UTC",
		"Owner_location": "Haryana, India",
		"Owner_reputation": 1,
		"Owner_up_votes": 0,
		"Owner_down_votes": 0,
		"Owner_views": 3,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "mlflow",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 69882574,
		"Question_title": "How to download an artifact from MLFlow using REST?",
		"Question_body": "<p>I see the Python API:\n<code>download_artifacts(run_id: str, path: str, dst_path: Optional[str] = None) \u2192 str</code> (<a href=\"https://www.mlflow.org/docs/latest/python_api/mlflow.tracking.html#mlflow.tracking.MlflowClient.download_artifacts\" rel=\"nofollow noreferrer\">here</a>), but I can't find the equivalent in REST.</p>",
		"Question_answer_count": 0,
		"Question_comment_count": 0,
		"Question_creation_date": "2021-11-08 11:13:52.513000 UTC",
		"Question_favorite_count": 1.0,
		"Question_last_edit_date": "2021-11-09 10:44:51.217000 UTC",
		"Question_score": 3,
		"Question_tags": "rest|databricks|mlflow",
		"Question_view_count": 243,
		"Owner_creation_date": "2009-06-14 12:54:00.077000 UTC",
		"Owner_last_access_date": "2022-09-23 21:20:51.750000 UTC",
		"Owner_location": "New York, NY",
		"Owner_reputation": 13408,
		"Owner_up_votes": 306,
		"Owner_down_votes": 12,
		"Owner_views": 687,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "mlflow",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 57693162,
		"Question_title": "Specify database backend store creation in specific schema",
		"Question_body": "<p>When creating an mlflow tracking server and specifying that a SQL Server database is to be used as a backend store, mlflow creates a bunch of table within the dbo schema. Does anyone know if it is possible to specify a different schema in which to create these tables?</p>",
		"Question_answer_count": 3,
		"Question_comment_count": 0,
		"Question_creation_date": "2019-08-28 13:06:36.983000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": null,
		"Question_score": 1,
		"Question_tags": "python|sqlalchemy|mlflow",
		"Question_view_count": 823,
		"Owner_creation_date": "2019-08-28 12:52:40.530000 UTC",
		"Owner_last_access_date": "2021-08-29 20:31:05.473000 UTC",
		"Owner_location": null,
		"Owner_reputation": 11,
		"Owner_up_votes": 0,
		"Owner_down_votes": 0,
		"Owner_views": 0,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "mlflow",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 70516388,
		"Question_title": "Defining routes in MLflow serving",
		"Question_body": "<p>When we serve mlflow model we define different ports for each serving and to access these models we use IP:port/invocations</p>\n<p>ex:</p>\n<p>app 1 : IP:2020/invocations</p>\n<p>app 2 : IP:2021/invocations</p>\n<p>But I want to serve 2 mlflow models at same port with different routes.</p>\n<p>ex:</p>\n<p>app 1 : IP:2020/app1</p>\n<p>app 2 : IP:2020/app2</p>\n<p>How can I achieve it using MLflow.</p>",
		"Question_answer_count": 0,
		"Question_comment_count": 0,
		"Question_creation_date": "2021-12-29 07:32:38.347000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": "2021-12-30 05:36:53.347000 UTC",
		"Question_score": 0,
		"Question_tags": "python|routes|mlflow|serving",
		"Question_view_count": 56,
		"Owner_creation_date": "2017-09-04 04:11:52.457000 UTC",
		"Owner_last_access_date": "2022-09-21 08:05:37.620000 UTC",
		"Owner_location": null,
		"Owner_reputation": 334,
		"Owner_up_votes": 105,
		"Owner_down_votes": 8,
		"Owner_views": 24,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "mlflow",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 65922084,
		"Question_title": "AttributeError: module 'time' has no attribute 'clock' in MLFlow UI",
		"Question_body": "<p>I have successfully installed <strong>MLFlow</strong> using <code>pip install mlflow</code> but while running <code>mlflow ui</code> command in the console it gives the following <strong>error</strong></p>\n<pre><code>    time_func = time.clock\nAttributeError: module 'time' has no attribute 'clock'\n</code></pre>\n<p>I am aware of the fact that <code>time.clock</code> is deprecated for <code>Python v3.8</code> and above. How can I fix this, as I don't want to downgrade python version.</p>",
		"Question_answer_count": 2,
		"Question_comment_count": 3,
		"Question_creation_date": "2021-01-27 15:19:26.830000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": null,
		"Question_score": 1,
		"Question_tags": "python|python-datetime|python-dateutil|mlflow",
		"Question_view_count": 589,
		"Owner_creation_date": "2020-12-23 08:27:44.260000 UTC",
		"Owner_last_access_date": "2022-09-24 16:36:21.650000 UTC",
		"Owner_location": null,
		"Owner_reputation": 66,
		"Owner_up_votes": 33,
		"Owner_down_votes": 0,
		"Owner_views": 21,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "mlflow",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 72994988,
		"Question_title": "How to mlflow-autolog a sklearn ConfusionMatrixDisplay?",
		"Question_body": "<p>I'm trying to log the plot of a <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html#sklearn.metrics.ConfusionMatrixDisplay.from_estimator\" rel=\"nofollow noreferrer\">confusion matrix generated with scikit-learn</a> for a <em>test</em> set using <a href=\"https://www.mlflow.org/docs/latest/python_api/mlflow.sklearn.html\" rel=\"nofollow noreferrer\">mlflow's support for scikit-learn</a>.</p>\n<p>For this, I tried something that resemble the code below (I'm using mlflow hosted on Databricks, and <code>sklearn==1.0.1</code>)</p>\n<pre class=\"lang-py prettyprint-override\"><code>import sklearn.datasets\nimport pandas as pd\nimport numpy as np\nimport mlflow\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\n\nmlflow.set_tracking_uri(&quot;databricks&quot;)\nmlflow.set_experiment(&quot;/Users/name.surname/plotcm&quot;)\n\ndata = sklearn.datasets.fetch_20newsgroups(categories=['alt.atheism', 'sci.space'])\n\ndf = pd.DataFrame(data = np.c_[data['data'], data['target']])\\\n       .rename({0:'text', 1:'class'}, axis = 'columns')\n\ntrain, test = train_test_split(df)\n\nmy_pipeline = Pipeline([\n    ('vectorizer', TfidfVectorizer()),\n    ('classifier', SGDClassifier(loss='modified_huber')),\n])\n\nmlflow.sklearn.autolog()\n\nfrom sklearn.metrics import ConfusionMatrixDisplay # should I import this after the call to `.autolog()`?\n\nmy_pipeline.fit(train['text'].values, train['class'].values)\n\ncm = ConfusionMatrixDisplay.from_predictions(\n      y_true=test[&quot;class&quot;], y_pred=my_pipeline.predict(test[&quot;text&quot;])\n  )\n</code></pre>\n<p>while the confusion matrix for the training set is saved in my mlflow run, no png file is created in the mlflow frontend for the <code>test</code> set.</p>\n<p>If I try to add</p>\n<pre class=\"lang-py prettyprint-override\"><code>cm.figure_.savefig('test_confusion_matrix.png')\nmlflow.log_artifact('test_confusion_matrix.png')\n</code></pre>\n<p>that does the job, but requires explicitly logging the artifact.</p>\n<p>Is there an idiomatic/proper way to autolog the confusion matrix computed using a test set after <code>my_pipeline.fit()</code>?</p>",
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_creation_date": "2022-07-15 13:44:41.357000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": "2022-07-17 18:51:20.967000 UTC",
		"Question_score": 0,
		"Question_tags": "python|scikit-learn|confusion-matrix|mlflow",
		"Question_view_count": 157,
		"Owner_creation_date": "2014-11-11 16:17:30.717000 UTC",
		"Owner_last_access_date": "2022-09-24 20:31:18.173000 UTC",
		"Owner_location": "Verona, VR, Italy",
		"Owner_reputation": 4811,
		"Owner_up_votes": 376,
		"Owner_down_votes": 73,
		"Owner_views": 713,
		"Answer_body": "<p>The proper way to do this is to use <code>mlflow.log_figure</code> as a fluent API announced in <code>MLflow 1.13.0</code>. You can read the documentation <a href=\"https://www.mlflow.org/docs/latest/python_api/mlflow.html#mlflow.log_figure\" rel=\"nofollow noreferrer\">here</a>. This code will do the job.</p>\n<pre class=\"lang-py prettyprint-override\"><code>mlflow.log_figure(cm.figure_, 'test_confusion_matrix.png')\n</code></pre>\n<p>This function implicitly store the image, and then calls <code>log_artifact</code> against that path, something like you did.</p>",
		"Answer_comment_count": 0.0,
		"Answer_creation_date": "2022-07-20 08:15:34.100000 UTC",
		"Answer_last_edit_date": null,
		"Answer_score": 1.0,
		"Question_valid_tags": "mlflow",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 62286596,
		"Question_title": "How to log custom Pytorch model with Mlflow?",
		"Question_body": "<p>I have been using Presumm <a href=\"https://github.com/nlpyang/PreSumm\" rel=\"nofollow noreferrer\">https://github.com/nlpyang/PreSumm</a> for text summarization.</p>\n\n<p>However, in <code>src/train_abstractive.py</code>, the model learner <code>trainer</code> is not a <code>torch.nn.Module</code>. However, the input <code>AbsSummarizer</code> is an extension of the <code>torch.nn.Module</code> class.</p>\n\n<p>I want to use <code>mlflow.pytorch.log_model</code> to save the model as a native pytorch model. But <code>trainer</code> is not a <code>torch.nn.Module</code>. How do I go about this?</p>",
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_creation_date": "2020-06-09 15:50:00.773000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": null,
		"Question_score": 1,
		"Question_tags": "pytorch|mlflow",
		"Question_view_count": 495,
		"Owner_creation_date": "2017-05-10 23:32:23.587000 UTC",
		"Owner_last_access_date": "2022-09-13 21:34:49.320000 UTC",
		"Owner_location": null,
		"Owner_reputation": 1319,
		"Owner_up_votes": 592,
		"Owner_down_votes": 5,
		"Owner_views": 377,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "mlflow",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 73562615,
		"Question_title": "MlflowException: API request (Caused by ResponseError('too many 503 error responses'))",
		"Question_body": "<p>I am using mlflow to register my model. I try to use 'Scenario 4' when artifacts load to S3 bucket from local.</p>\n<ol>\n<li><p>Add credentials of S3 bucket to .aws/credentials</p>\n</li>\n<li><p>Set endpoint and mlflow URI:</p>\n<p>os.environ[&quot;MLFLOW_S3_ENDPOINT_URL&quot;]='https://storage.yandexcloud.net'\nos.environ[&quot;MLFLOW_TRACKING_URI&quot;]='http://:8000'</p>\n</li>\n<li><p>Log model to S3 via mlflow:</p>\n<p>import mlflow\nimport mlflow.sklearn\nmlflow.set_experiment(&quot;my&quot;)\n...\nmlflow.sklearn.log_model(model, artifact_path=&quot;models_mlflow&quot;)</p>\n</li>\n</ol>\n<p>But get error:</p>\n<pre><code>MlflowException: API request to http://&lt;IP&gt;:8000/api/2.0/mlflow-artifacts/artifacts/6/95972bcc493c4a8cbd8432fea4cc8bac/artifacts/models_mlflow/model.pkl failed with exception HTTPConnectionPool(host='62.84.121.234', port=8000): Max retries exceeded with url: /api/2.0/mlflow-artifacts/artifacts/6/95972bcc493c4a8cbd8432fea4cc8bac/artifacts/models_mlflow/model.pkl (Caused by ResponseError('too many 503 error responses'))\n</code></pre>",
		"Question_answer_count": 0,
		"Question_comment_count": 0,
		"Question_creation_date": "2022-08-31 22:41:17.113000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": null,
		"Question_score": 0,
		"Question_tags": "python|amazon-s3|mlflow|mlops|yandexcloud",
		"Question_view_count": 38,
		"Owner_creation_date": "2014-04-07 09:58:41.170000 UTC",
		"Owner_last_access_date": "2022-09-23 11:17:10.007000 UTC",
		"Owner_location": "Moscow, Russia",
		"Owner_reputation": 75,
		"Owner_up_votes": 105,
		"Owner_down_votes": 0,
		"Owner_views": 26,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "mlflow",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 72236258,
		"Question_title": "MLflow: Unable to store artifacts to S3",
		"Question_body": "<p>I'm running my mlflow tracking server in a docker container on a remote server and trying to log mlflow runs from local computer with the eventual goal that anyone on my team can send their run data to the same tracking server.  I've set the tracking URI to be <code>http://&lt;ip of remote server &gt;:&lt;port on docker container&gt;</code>.  I'm not explicitly setting any of the AWS credentials on the local machine because I would like to just be able to train locally and log to the remote server (run data to RDS and artifacts to S3).  I have no problem logging my runs to an RDS database but I keep getting the following error when it get to the point of trying to log artifacts: <code>botocore.exceptions.NoCredentialsError: Unable to locate credentials</code>.  Do I have to have the credentials available outside of the tracking server for this to work (ie: on my local machine where the mlflow runs are taking place)?  I know that all of my credentials are available in the docker container that is hosting the tracking server. I've be able to upload files to my S3 bucket using the aws cli inside of the container that hosts my tracking server so I know that it as access.  I'm confused by the fact that I can log to RDS but not S3. I'm not sure what I'm doing wrong at this point.  TIA.</p>",
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_creation_date": "2022-05-13 23:32:23.673000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": null,
		"Question_score": 0,
		"Question_tags": "docker|amazon-s3|boto3|mlflow",
		"Question_view_count": 223,
		"Owner_creation_date": "2021-12-16 00:24:08.310000 UTC",
		"Owner_last_access_date": "2022-09-23 00:00:12.383000 UTC",
		"Owner_location": null,
		"Owner_reputation": 51,
		"Owner_up_votes": 0,
		"Owner_down_votes": 0,
		"Owner_views": 3,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "mlflow",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 71738738,
		"Question_title": "How to set custom path for databricks mlflow artifacts on s3",
		"Question_body": "<p>I've created an empty experiments from databricks experiments console and given the path for my artifacts on s3 i.e. s3:///. When i run the scripts, the artifacts are stored at</p>\n<pre><code>s3://&lt;bucket&gt;//&lt;32 char id&gt;/artifacts/model-Elasticnet/model.pkl\n</code></pre>\n<p>I want to replace //&lt;32 char id&gt;/artifacts/ with /datetime/artifacts/ so something like</p>\n<pre><code>s3://&lt;bucket&gt;/&lt;datetime&gt;/artifacts/model-Elasticnet/model.pkl\n</code></pre>\n<p>Is there any way i could achieve that?</p>\n<p><a href=\"https://i.stack.imgur.com/wUDcE.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/wUDcE.png\" alt=\"enter image description here\" /></a></p>\n<p>Note: experiment_id is from databricks experiment console</p>",
		"Question_answer_count": 0,
		"Question_comment_count": 1,
		"Question_creation_date": "2022-04-04 14:12:33.880000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": "2022-04-07 07:21:35.153000 UTC",
		"Question_score": 2,
		"Question_tags": "databricks|mlflow|aws-databricks|mlops",
		"Question_view_count": 140,
		"Owner_creation_date": "2018-05-12 15:57:03.120000 UTC",
		"Owner_last_access_date": "2022-09-24 08:37:01.693000 UTC",
		"Owner_location": "Berlin, Germany",
		"Owner_reputation": 962,
		"Owner_up_votes": 106,
		"Owner_down_votes": 9,
		"Owner_views": 128,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "mlflow",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 52372118,
		"Question_title": "Store scaler with mlflow keras-model",
		"Question_body": "<p>We are looking into using mlflow to handle our keras models, and we would also like to use mlflow to deploy the models in e.g azure. But the models require some simple preprocessing of the data, in our case the use of a minmax scaler. For the deployed models to answer correctly they must apply the scaler on the input (and inverse on the output). I have not found any way to include the scaling in the persisted/deployed models. Have I overlooked something, or is it not possible?</p>",
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_creation_date": "2018-09-17 16:26:20.813000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": null,
		"Question_score": 4,
		"Question_tags": "python|keras|mlflow",
		"Question_view_count": 486,
		"Owner_creation_date": "2013-03-06 13:18:07.873000 UTC",
		"Owner_last_access_date": "2022-09-24 20:35:16.157000 UTC",
		"Owner_location": null,
		"Owner_reputation": 199,
		"Owner_up_votes": 13,
		"Owner_down_votes": 0,
		"Owner_views": 95,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "mlflow",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 59006453,
		"Question_title": "MLflow: active run ID does not match environment run ID",
		"Question_body": "<p>OS: Ubuntu 18</p>\n\n<p>Python: Python 3.6</p>\n\n<p>MLflow: 1.4</p>\n\n<p>I'm trying to get MLflow Projects to run. Here is my project:</p>\n\n<ul>\n<li><p>MLflow</p>\n\n<ul>\n<li><p>conda.yaml</p></li>\n<li><p>main.py</p></li>\n<li><p>prep_data.py</p></li>\n<li><p>learn.py</p></li>\n<li><p>List item</p></li>\n</ul></li>\n</ul>\n\n<p>The project is heavily based up on this repo: <a href=\"https://github.com/mlflow/mlflow/tree/master/examples/multistep_workflow\" rel=\"nofollow noreferrer\">https://github.com/mlflow/mlflow/tree/master/examples/multistep_workflow</a>\nI'm trying to run both the prep_data and learn scripts using MLflow Projects and the main.py script as an entry point.\nFor execution I use the following command: <code>mlflow run . -P experiment_name=testproject</code></p>\n\n<p>But I get the following Error:</p>\n\n<pre><code>Traceback (most recent call last):\n  File \"prep_data.py\", line 126, in &lt;module&gt;\n    prep_data()\n  File \"/home/ubuntu/venv/lib/python3.6/site-packages/click/core.py\", line 764, in __call__\n   return self.main(*args, **kwargs)\n  File \"/home/ubuntu/venv/lib/python3.6/site-packages/click/core.py\", line 717, in main\n    rv = self.invoke(ctx)\n  File \"/home/ubuntu/venv/lib/python3.6/site-packages/click/core.py\", line 956, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"/home/ubuntu/venv/lib/python3.6/site-packages/click/core.py\", line 555, in invoke\n    return callback(*args, **kwargs)\n  File \"prep_data.py\", line 65, in prep_data\n    with mlflow.start_run() as active_run:\n  File \"/home/ubuntu/venv/lib/python3.6/site-packages/mlflow/tracking/fluent.py\", line 129, in start_run\n    \"arguments\".format(existing_run_id))\nmlflow.exceptions.MlflowException: Cannot start run with ID 405b83bbb61046afa83b8dcd71b4db14 because active run ID does not match environment run ID. Make sure --experiment-name or --experiment-id matches experiment set with set_experiment(), or just use command-line arguments\nTraceback (most recent call last):\n  File \"main.py\", line 75, in &lt;module&gt;\n    workflow()\n  File \"/home/ubuntu/venv/lib/python3.6/site-packages/click/core.py\", line 764, in __call__\n    return self.main(*args, **kwargs)\n  File \"/home/ubuntu/venv/lib/python3.6/site-packages/click/core.py\", line 717, in main\n    rv = self.invoke(ctx)\n  File \"/home/ubuntu/venv/lib/python3.6/site-packages/click/core.py\", line 956, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"/home/ubuntu/venv/lib/python3.6/site-packages/click/core.py\", line 555, in invoke\n    return callback(*args, **kwargs)\n  File \"main.py\", line 61, in workflow\n    }, experiment_name)\n  File \"main.py\", line 40, in _get_or_run\n    submitted_run = mlflow.run('.', entry_point=entry_point, parameters=params)\n  File \"/home/ubuntu/venv/lib/python3.6/site-packages/mlflow/projects/__init__.py\", line 287, in run\n    _wait_for(submitted_run_obj)\n  File \"/home/ubuntu/venv/lib/python3.6/site-packages/mlflow/projects/__init__.py\", line 304, in _wait_for\n    raise ExecutionException(\"Run (ID '%s') failed\" % run_id)\nmlflow.exceptions.ExecutionException: Run (ID '405b83bbb61046afa83b8dcd71b4db14') failed\n2019/11/22 18:51:59 ERROR mlflow.cli: === Run (ID '62c229b2d9194b569a7b2bfc14338800') failed ===\n</code></pre>\n\n<p>I'm not sure if I understand the error correctly but it seems like it's saying I am using multiple experiments. However I'm fairly certain I am only using 1 (testproject).\nBrowsing SO and Github issues suggested I'd should set the environment variable <code>MLFLOW_TRACKING_URI</code> but it wasn't stated on how to set that. Thus I tried two different ways:\n1) exporting it before running the MLflow project: $ export <code>MLFLOW_TRACKING_URI='http://127.0.0.1:5099'</code>\n2) setting it at the beginning of my main.py script using python: <code>os.environ['MLFLOW_TRACKING_URI'] = 'http://127.0.0.1:5099'</code>\nNeither had any effect.\nHere you can see my project:</p>\n\n<p>main.py</p>\n\n<pre><code>import os\nimport click\nimport mlflow\nfrom mlflow.entities import RunStatus\ndef _already_ran(entry_point, params, experiment_name):\n    # experiment = mlflow.get_experiment_by_name('{}_{}'.format(experiment_name, entry_point))\n    experiment = mlflow.get_experiment_by_name(experiment_name)\n    if experiment == None:\n        return None\n    experiment_id = experiment.experiment_id\n    client = mlflow.tracking.MlflowClient()\n    all_run_infos = reversed(client.list_run_infos(experiment_id))\n    match_failed = False\n    for run_info in all_run_infos\n        full_run = client.get_run(run_info.run_id)\n        for p_key, p_val in params:\n            run_value = full_run.data.params.get(p_key)\n            if run_value != p_val:\n                match_failed = True\n                break\n        if match_failed:\n            continue\n        if run_info.to_proto().status != RunStatus.FINISHED:\n            continue\n        return client.get_run(run_info.run_id)\n    return None\n\n\ndef _get_or_run(entry_point, params, experiment_name, use_cache=True):\n    existing_run = _already_ran(entry_point, params, experiment_name)\n    if use_cache and existing_run:\n        return existing_run\n    submitted_run = mlflow.run('.', entry_point=entry_point, parameters=params)\n    return mlflow.tracking.MlflowClient().get_run(submitted_run.run_id)\n\n@click.command()\n@click.option(\"--experiment-name\")\n@click.option('--prep-data-time-avg', default='placeholder')\n@click.option('--prep-data-sensor-id', default='placeholder')\n@click.option('--learn-epochs', default=100, type=int)\n@click.option('--learn-neurons', default=5, type=int)\n@click.option('--learn-layers', default=2, type=int)\ndef workflow(experiment_name, prep_data_time_avg, prep_data_sensor_id, learn_epochs, learn_neurons, learn_layers):\n    # mlflow.set_tracking_uri('http://127.0.0.1:5099')\n\n    # mlflow.set_experiment(experiment_name)\n    # with mlflow.start_run() as active_run:\n\n    data_run = _get_or_run('prep_data', {\n        'time_avg': prep_data_time_avg,\n        'sensor_id':prep_data_sensor_id,\n        'experiment_name': experiment_name\n    }, experiment_name)\n\n    learn_run = _get_or_run('learn', {\n        'epochs': learn_epochs,\n        'neurons': learn_neurons,\n        'layers': learn_layers,\n        'prep_data_run_id': data_run.run_id,\n        'experiment_name': experiment_name,\n    }, experiment_name)\nif __name__ == '__main__':\n    # os.environ['MLFLOW_TRACKING_URI'] = 'http://127.0.0.1:5099'\n    workflow()\n\n\n</code></pre>\n\n<p>prep_data.py</p>\n\n<pre><code>@click.command()\n@click.option(\"--experiment-name\")\n@click.option('--time-avg', default='placeholder')\n@click.option('--sensor-id', default='placeholder')\ndef prep_data(experiment_name, time_avg, sensor_id):\n    mlflow.set_experiment(experiment_name)\n    with mlflow.start_run() as active_run:\n      # logic code of prep_data\n\nif __name__ == '__main__':\n    prep_data()\n\n</code></pre>\n\n<p>I'm happy about any ideas on how to fix this issue.</p>\n\n<p>Thank you very much!</p>\n\n<p>Cheers,\nRaphael</p>",
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_creation_date": "2019-11-23 10:00:29.297000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": null,
		"Question_score": 4,
		"Question_tags": "python-3.x|mlflow",
		"Question_view_count": 2256,
		"Owner_creation_date": "2016-09-10 16:12:07.980000 UTC",
		"Owner_last_access_date": "2022-06-30 12:40:05.403000 UTC",
		"Owner_location": null,
		"Owner_reputation": 4045,
		"Owner_up_votes": 7,
		"Owner_down_votes": 4,
		"Owner_views": 73,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "mlflow",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 56113569,
		"Question_title": "MLFlow Projects throw JSONDecode error when run",
		"Question_body": "<p>I'm trying to get MLFlow Projects to run using the MLFlow CLI and its following the tutorial leads to an error.  For any project I try to run from the CLI, I get the following error</p>\n\n<pre><code>Traceback (most recent call last):\n  File \"/home/rbc/.local/bin/mlflow\", line 11, in &lt;module&gt;\n    sys.exit(cli())\n  File \"/home/rbc/.local/lib/python3.6/site-packages/click/core.py\", line 764, in __call__\n    return self.main(*args, **kwargs)\n  File \"/home/rbc/.local/lib/python3.6/site-packages/click/core.py\", line 717, in main\n    rv = self.invoke(ctx)\n  File \"/home/rbc/.local/lib/python3.6/site-packages/click/core.py\", line 1137, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"/home/rbc/.local/lib/python3.6/site-packages/click/core.py\", line 956, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"/home/rbc/.local/lib/python3.6/site-packages/click/core.py\", line 555, in invoke\n    return callback(*args, **kwargs)\n  File \"/home/rbc/.local/lib/python3.6/site-packages/mlflow/cli.py\", line 139, in run\n    run_id=run_id,\n  File \"/home/rbc/.local/lib/python3.6/site-packages/mlflow/projects/__init__.py\", line 230, in run\n    storage_dir=storage_dir, block=block, run_id=run_id)\n  File \"/home/rbc/.local/lib/python3.6/site-packages/mlflow/projects/__init__.py\", line 88, in _run\n    active_run = _create_run(uri, experiment_id, work_dir, entry_point)\n  File \"/home/rbc/.local/lib/python3.6/site-packages/mlflow/projects/__init__.py\", line 579, in _create_run\n    active_run = tracking.MlflowClient().create_run(experiment_id=experiment_id, tags=tags)\n  File \"/home/rbc/.local/lib/python3.6/site-packages/mlflow/tracking/client.py\", line 101, in create_run\n    source_version=source_version\n  File \"/home/rbc/.local/lib/python3.6/site-packages/mlflow/store/rest_store.py\", line 156, in create_run\n    response_proto = self._call_endpoint(CreateRun, req_body)\n  File \"/home/rbc/.local/lib/python3.6/site-packages/mlflow/store/rest_store.py\", line 66, in _call_endpoint\n    js_dict = json.loads(response.text)\n  File \"/usr/lib/python3.6/json/__init__.py\", line 354, in loads\n    return _default_decoder.decode(s)\n  File \"/usr/lib/python3.6/json/decoder.py\", line 339, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"/usr/lib/python3.6/json/decoder.py\", line 357, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n</code></pre>\n\n<p>Here's an example of the type of command I'm using to start the run, which comes directly from the tutorial </p>\n\n<pre><code>mlflow run https://github.com/mlflow/mlflow#examples/sklearn_elasticnet_wine -m databricks -c cluster-spec.json --experiment-id 72647065958042 -P alpha=2.0 -P l1_ratio=0.5\n</code></pre>\n\n<p>I've traced the error to something involving MLFLow returning empty when it tries to start a run but I can successfully run MLFlow experiments using the Databricks environment I'm connecting to so I'm not sure where the problem is, I'm running MLFlow 0.9.1 on Ubuntu 18.04</p>",
		"Question_answer_count": 1,
		"Question_comment_count": 2,
		"Question_creation_date": "2019-05-13 13:37:47.253000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": null,
		"Question_score": 3,
		"Question_tags": "mlflow",
		"Question_view_count": 386,
		"Owner_creation_date": "2017-10-31 17:37:18.900000 UTC",
		"Owner_last_access_date": "2019-10-24 20:16:31.737000 UTC",
		"Owner_location": "Orlando, FL, United States",
		"Owner_reputation": 31,
		"Owner_up_votes": 0,
		"Owner_down_votes": 0,
		"Owner_views": 3,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "mlflow",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	}
]