[
	{
		"Question_id": 54324146,
		"Question_title": "Azure DataLake with DVC",
		"Question_body": "<p>We are thinking to use DVC for versioning input data for DataScience project.\nmy data resides in Azure DataLake Gen1.</p>\n\n<p>how do i configure DVC to push data to Azure DataLake using Service Principal?\ni want DVC to store cache and data into Azure DataLake instead on local disk.</p>",
		"Question_answer_count": 0,
		"Question_comment_count": 1,
		"Question_creation_date": "2019-01-23 09:38:51.560000 UTC",
		"Question_favorite_count": 1.0,
		"Question_last_edit_date": null,
		"Question_score": 5,
		"Question_tags": "azure-data-lake|service-principal|dvc",
		"Question_view_count": 473,
		"Owner_creation_date": "2009-11-10 14:07:58.920000 UTC",
		"Owner_last_access_date": "2022-09-09 06:39:04.550000 UTC",
		"Owner_location": "Pune, India",
		"Owner_reputation": 6219,
		"Owner_up_votes": 227,
		"Owner_down_votes": 7,
		"Owner_views": 587,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "dvc",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 68004538,
		"Question_title": "Is dvc.yaml supposed to be written or generated by dvc run command?",
		"Question_body": "<p>Trying to understand <a href=\"https://dvc.org/doc/start\" rel=\"nofollow noreferrer\">dvc</a>, most tutorials mention generation of dvc.yaml by running <code>dvc run</code> command.</p>\n<p>But at the same time, dvc.yaml which defines the DAG is also <a href=\"https://dvc.org/doc/user-guide/project-structure/pipelines-files\" rel=\"nofollow noreferrer\">well documented</a>. Also the fact that it is a yaml format and human readable/writable would point to the fact that it is meant to be a DSL for specifying your data pipeline.</p>\n<p>Can somebody clarify which is the better practice?\nWriting the dvc.yaml or let it be generated by <code>dvc run</code> command?\nOr is it left to user's choice and there is no technical difference?</p>",
		"Question_answer_count": 2,
		"Question_comment_count": 0,
		"Question_creation_date": "2021-06-16 14:19:55.940000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": "2021-06-16 19:58:05.370000 UTC",
		"Question_score": 5,
		"Question_tags": "directed-acyclic-graphs|data-pipeline|dvc",
		"Question_view_count": 1101,
		"Owner_creation_date": "2009-05-27 17:42:14.993000 UTC",
		"Owner_last_access_date": "2022-09-23 06:17:58.327000 UTC",
		"Owner_location": "Gothenburg, Sweden",
		"Owner_reputation": 1547,
		"Owner_up_votes": 28,
		"Owner_down_votes": 9,
		"Owner_views": 212,
		"Answer_body": "<p>I'd recommend manual editing as the main route! (I believe that's officially recommended since <a href=\"https://dvc.org/blog/dvc-2-0-release\" rel=\"nofollow noreferrer\">DVC 2.0</a>)</p>\n<p><code>dvc stage add</code> can still be very helpful for programmatic generation of pipelines files, but it doesn't support all the features of <code>dvc.yaml</code>, for example setting <code>vars</code> values or defining <a href=\"https://dvc.org/doc/user-guide/project-structure/pipelines-files#foreach-stages\" rel=\"nofollow noreferrer\"><code>foreach</code> stages</a>.</p>",
		"Answer_comment_count": 7.0,
		"Answer_creation_date": "2021-06-16 16:00:05.940000 UTC",
		"Answer_last_edit_date": "2021-06-16 20:39:37.407000 UTC",
		"Answer_score": 4.0,
		"Question_valid_tags": "dvc",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 67393339,
		"Question_title": "dvc push, change the names of files on the remote storage",
		"Question_body": "<p>I'm working on a project with DVC (Data Version Control), when I push files in my remote storage, the name of the files are changed. How I can conserve the names?</p>",
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_creation_date": "2021-05-04 23:20:53.717000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": "2021-05-04 23:30:22.370000 UTC",
		"Question_score": 2,
		"Question_tags": "dvc",
		"Question_view_count": 447,
		"Owner_creation_date": "2013-09-20 14:02:00.450000 UTC",
		"Owner_last_access_date": "2022-09-21 22:02:08.603000 UTC",
		"Owner_location": null,
		"Owner_reputation": 43,
		"Owner_up_votes": 13,
		"Owner_down_votes": 0,
		"Owner_views": 13,
		"Answer_body": "<p>Short answer: there is no way to do that.</p>\n<p>Long answer:\nDvc remote is a content-based storage, so names are not preserved. Dvc creates metafiles (*.dvc files) in your workspace that contain names and those files are usually tracked by git, so you need to use git remote and dvc remote together to have both filenames and their contents. Here is a more detailed explanation about the format of local and remote storage <a href=\"https://dvc.org/doc/user-guide/project-structure/internal-files#structure-of-the-cache-directory\" rel=\"noreferrer\">https://dvc.org/doc/user-guide/project-structure/internal-files#structure-of-the-cache-directory</a> . Also, checkout <a href=\"https://dvc.org/doc/use-cases/sharing-data-and-model-files\" rel=\"noreferrer\">https://dvc.org/doc/use-cases/sharing-data-and-model-files</a></p>",
		"Answer_comment_count": 0.0,
		"Answer_creation_date": "2021-05-04 23:40:51.067000 UTC",
		"Answer_last_edit_date": null,
		"Answer_score": 5.0,
		"Question_valid_tags": "dvc",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 67744934,
		"Question_title": "Is it possible to check that the version of a file tracked by a DVC metadata file exists in remote storage without pulling the file?",
		"Question_body": "<p>My team has a set up wherein we track datasets and models in DVC, and have a GitLab repository for tracking our code and DVC metadata files. We have a job in our dev GitLab pipeline (run on each push to a merge request) that has the goal of checking to be sure that the developer remembered to run <code>dvc push</code> to keep DVC remote storage up-to-date. Right now, the way we do this is by running <code>dvc pull</code> on the GitLab runner, which will fail with errors telling you which files (new files or latest versions of existing files) were not found.</p>\n<p>The downside to this approach is that we are loading the entirety of our data stored in DVC onto a GitLab runner, and we've run into out-of-memory issues, not to mention lengthy run time to download all that data. Since the path and md5 hash of the objects are stored in the DVC metadata files, I would think that's all the information that DVC would need to be able to answer the question &quot;is the remote storage system up-to-date&quot;.</p>\n<p>It seems like <code>dvc status</code> is similar to what I'm asking for, but compares the cache or workspace and remote storage. In other words, it requires the files to actually be present on whatever filesystem is making the call.</p>\n<p>Is there some way to achieve the goal I laid out above (&quot;inform the developer that they need to run <code>dvc push</code>&quot;) without pulling everything from DVC?</p>",
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_creation_date": "2021-05-28 20:10:29.793000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": "2021-05-29 03:04:51.983000 UTC",
		"Question_score": 5,
		"Question_tags": "git|gitlab|continuous-integration|dvc",
		"Question_view_count": 488,
		"Owner_creation_date": "2021-04-12 19:17:42.697000 UTC",
		"Owner_last_access_date": "2022-02-22 18:42:26.683000 UTC",
		"Owner_location": null,
		"Owner_reputation": 75,
		"Owner_up_votes": 5,
		"Owner_down_votes": 0,
		"Owner_views": 2,
		"Answer_body": "<blockquote>\n<p>It seems like dvc status is similar to what I'm asking for</p>\n</blockquote>\n<p><code>dvc status --cloud</code> will give you a list of &quot;new&quot; files if they that haven't been pushed to the (default) remote. It won't error out though, so your CI script should fail depending on the stdout message.</p>\n<p>More info: <a href=\"https://dvc.org/doc/command-reference/status#options\" rel=\"nofollow noreferrer\">https://dvc.org/doc/command-reference/status#options</a></p>\n<p>I'd also ask everyone to run <code>dvc install</code>, which will setup some Git hooks, including automatic <code>dvc push</code> with <code>git push</code>.</p>\n<p>See <a href=\"https://dvc.org/doc/command-reference/install\" rel=\"nofollow noreferrer\">https://dvc.org/doc/command-reference/install</a></p>",
		"Answer_comment_count": 1.0,
		"Answer_creation_date": "2021-05-29 03:09:19.210000 UTC",
		"Answer_last_edit_date": "2021-05-31 23:24:13.297000 UTC",
		"Answer_score": 3.0,
		"Question_valid_tags": "dvc",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 73627516,
		"Question_title": "facing issues in gitlab-runner for ci-cml and face issue to use AWS-s3 bucket for dvc",
		"Question_body": "<p>I am working on Mlops. so train.py is my script in which i am using dvc for data versioning and train the model by using s3 bucket. this all thing i push on gitlab and I have to create ci-cml pipeline, but not able to use s3 bucket to use dvc data and facing error when try run .gitlab-runner so find error in gitlab-runner.</p>",
		"Question_answer_count": 0,
		"Question_comment_count": 1,
		"Question_creation_date": "2022-09-06 20:35:30.497000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": null,
		"Question_score": 0,
		"Question_tags": "amazon-s3|gitlab-ci-runner|mlops|dvc|cml",
		"Question_view_count": 23,
		"Owner_creation_date": "2019-12-12 05:07:25.140000 UTC",
		"Owner_last_access_date": "2022-09-24 21:10:52.373000 UTC",
		"Owner_location": "Mumbai, Maharashtra, India",
		"Owner_reputation": 21,
		"Owner_up_votes": 0,
		"Owner_down_votes": 0,
		"Owner_views": 3,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "dvc",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 60861593,
		"Question_title": "How do I specify encryption type when using s3remote for DVC",
		"Question_body": "<p>I have just started to explore DVC. I am trying with s3 as my DVC remote. I am getting </p>\n\n<p>But when I run the <code>dvc push</code> command, I get the generic error saying </p>\n\n<pre><code>An error occurred (AccessDenied) when calling the PutObject operation: Access Denied\n</code></pre>\n\n<p>which I know for a fact that I get that error when I don't specify the encryption.</p>\n\n<p>It is similar to running <code>aws s3 cp</code> with <code>--sse</code> flag or specifying <code>ServerSideEncryption</code> when using boto3 library. How can I specify the encryption type when using DVC. Coz underneath DVC uses boto3 so there must be an easy way to do this.</p>",
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_creation_date": "2020-03-26 05:45:08.167000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": null,
		"Question_score": 1,
		"Question_tags": "dvc",
		"Question_view_count": 293,
		"Owner_creation_date": "2017-04-11 13:31:59.307000 UTC",
		"Owner_last_access_date": "2022-03-29 20:52:48.753000 UTC",
		"Owner_location": null,
		"Owner_reputation": 1756,
		"Owner_up_votes": 82,
		"Owner_down_votes": 5,
		"Owner_views": 199,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "dvc",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 61816429,
		"Question_title": "DVC dependencies for derived data without imports",
		"Question_body": "<p>I am new to DVC, and so far I like what I see. But possibly my question is fairly easy to answer.</p>\n\n<p><strong>My question:</strong> how do we correctly track the dependencies to files in an original hugedatarepo  (lets assume that this can also change) in a derivedData project, but WITHOUT the huge files being imported generally when the derived data is checked out? I don't think I can use <code>dvc import</code> to achieve this.</p>\n\n<p><strong>Details:</strong> We have a repository with a large amount of quite big data files (scans) and use this data to design and train various algorithms. Often we want to use only specific files and even only small chunks from within the files for training, annotation and so on. That is, we derive data for specific tasks, that we want to put in new repositories.</p>\n\n<p>Currently my Idea is to <code>dvc get</code> the relevant data, put it in a untracked temporary folder and then again manage the derived data with dvc. But still to put in the dependency to the original data.</p>\n\n<pre><code>hugeFileRepo\n +metaData.csv\n +dataFolder\n +-- hugeFile_1\n ...\n +-- hugeFile_n\n</code></pre>\n\n<p>in the derivedData repository I do</p>\n\n<pre><code> dvc import hugeFileRepo.git metaData.csv\n dvc run -f derivedData.dvc \\\n    -d metaData.csv \\\n    -d deriveData.py \\\n    -o derivedDataFolder \\\n    python deriveData.py \n</code></pre>\n\n<p>My deriveData.py does something along the line (pseudocode)</p>\n\n<pre><code>metaData = read(metaData.csv)\n\n#Hack because I don't know how to it right:\ngitRevision = getGitRevision(metaData.csv.dvc)          \n...\nfor metaDataForFile, file in metaData:\n   if(iWantFile(metaDataForFile) ):\n      #download specific file\n      !dvc get --rev {gitRevision} -o tempFolder/{file} hugeFileRepo.git {file}\n\n      #do processing of huge file and store result in derivedDataFolder\n      processAndWrite(tempFolder/file)\n</code></pre>\n\n<p>So I use the metaData file as a proxy for the actual data. The hugeFileRepo data will not change frequently and the metaData file will be kept up to date. And I am absolutely fine with having a dependency to the data in general and not to the actual files I used. So I believe this solution would work for me, but I am sure there is a better way.</p>",
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_creation_date": "2020-05-15 09:52:02.927000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": null,
		"Question_score": 2,
		"Question_tags": "dvc",
		"Question_view_count": 179,
		"Owner_creation_date": "2016-01-25 12:22:13.500000 UTC",
		"Owner_last_access_date": "2022-09-13 09:05:05.923000 UTC",
		"Owner_location": null,
		"Owner_reputation": 171,
		"Owner_up_votes": 2,
		"Owner_down_votes": 0,
		"Owner_views": 1,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "dvc",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 73126208,
		"Question_title": "Python: Ssl Certificate verify failed",
		"Question_body": "<p>I have installed <code>dvc</code> on my <code>ubuntu-18.04-LTS</code> system and while trying to download the <code>data</code> files from github using dvc, it fails with below error.</p>\n<pre><code>$ dvc get https://github.com/iterative/dataset-registry get-started/data.xml -o data/data.xml -v\n\n2022-07-22 12:55:22,260 DEBUG: Creating external repo https://github.com/iterative/dataset-registry@None\n2022-07-22 12:55:22,260 DEBUG: erepo: git clone 'https://github.com/iterative/dataset-registry' to a temporary dir\n2022-07-22 12:55:23,683 DEBUG: Removing '/dvc/dvc_test/data/.UEeAzwmJCY3q85YQuCeahx'\n2022-07-22 12:55:23,684 ERROR: failed to get 'get-started/data.xml' from 'https://github.com/iterative/dataset-registry' - Failed to clone repo 'https://github.com/iterative/dataset-registry' to '/tmp/tmpvmrmu9qsdvc-clone'\n------------------------------------------------------------\nTraceback (most recent call last):\n  File &quot;urllib3/connectionpool.py&quot;, line 703, in urlopen\n  File &quot;urllib3/connectionpool.py&quot;, line 386, in _make_request\n  File &quot;urllib3/connectionpool.py&quot;, line 1042, in _validate_conn\n  File &quot;urllib3/connection.py&quot;, line 414, in connect\n  File &quot;urllib3/util/ssl_.py&quot;, line 449, in ssl_wrap_socket\n  File &quot;urllib3/util/ssl_.py&quot;, line 493, in _ssl_wrap_socket_impl\n  File &quot;ssl.py&quot;, line 500, in wrap_socket\n  File &quot;ssl.py&quot;, line 1040, in _create\n  File &quot;ssl.py&quot;, line 1309, in do_handshake\nssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1108)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;dvc/scm.py&quot;, line 145, in clone\n  File &quot;scmrepo/git/__init__.py&quot;, line 143, in clone\n  File &quot;scmrepo/git/backend/dulwich/__init__.py&quot;, line 199, in clone\nscmrepo.exceptions.CloneError: Failed to clone repo 'https://github.com/iterative/dataset-registry' to '/tmp/tmpvmrmu9qsdvc-clone'\n</code></pre>\n<p>Already our corporate proxy certificate has been installed and traffic to <code>github.com</code> allowed I'm able to clone above repository separately on CLI. But with <code>dvc</code>the above errors are occurring, Even the below couldn't solve the issue.</p>\n<pre><code>$ python -c &quot;import ssl; print(ssl.get_default_verify_paths())&quot;\n\nDefaultVerifyPaths(cafile=None, capath='/usr/lib/ssl/certs', openssl_cafile_env='SSL_CERT_FILE', openssl_cafile='/usr/lib/ssl/cert.pem', openssl_capath_env='SSL_CERT_DIR', openssl_capath='/usr/lib/ssl/certs')\n</code></pre>\n<pre><code>export SSL_CERT_DIR=/etc/ssl/certs/\nexport REQUESTS_CA_BUNDLE=/usr/local/lib/python2.7/dist-packages/certifi/cacert.pem\npip install --upgrade certifi\nexport PYTHONHTTPSVERIFY=0\n\nsudo apt install ca-certificates\nsudo update-ca-certificates --fresh\n</code></pre>\n<pre><code>$ python --version\nPython 2.7.17\n\n$ dvc doctor\nDVC version: 2.13.0 (deb)\n---------------------------------\nPlatform: Python 3.8.3 on Linux-5.4.0-92-generic-x86_64-with-glibc2.14\nSupports:\n        azure (adlfs = 2022.7.0, knack = 0.9.0, azure-identity = 1.10.0),\n        gdrive (pydrive2 = 1.10.1),\n        gs (gcsfs = 2022.5.0),\n        hdfs (fsspec = 2022.5.0, pyarrow = 8.0.0),\n        webhdfs (fsspec = 2022.5.0),\n        http (aiohttp = 3.8.1, aiohttp-retry = 2.5.1),\n        https (aiohttp = 3.8.1, aiohttp-retry = 2.5.1),\n        s3 (s3fs = 2022.5.0, boto3 = 1.21.21),\n        ssh (sshfs = 2022.6.0),\n        oss (ossfs = 2021.8.0),\n        webdav (webdav4 = 0.9.7),\n        webdavs (webdav4 = 0.9.7)\n</code></pre>\n<p>Tp bypass the ssl validation in git we have <code>git config http.sslVerify &quot;false&quot;</code> Similarly do we have option in dvc?</p>\n<p>Further what should i update to resolve this issue?</p>",
		"Question_answer_count": 1,
		"Question_comment_count": 2,
		"Question_creation_date": "2022-07-26 15:36:20.953000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": "2022-07-27 05:01:17.490000 UTC",
		"Question_score": 1,
		"Question_tags": "python|ssl|pip|ssl-certificate|dvc",
		"Question_view_count": 157,
		"Owner_creation_date": "2015-05-28 11:02:07.473000 UTC",
		"Owner_last_access_date": "2022-09-25 03:23:10.223000 UTC",
		"Owner_location": null,
		"Owner_reputation": 1609,
		"Owner_up_votes": 68,
		"Owner_down_votes": 0,
		"Owner_views": 447,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "dvc",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 72473641,
		"Question_title": "How do launch experiments in DVC?",
		"Question_body": "<p>I want to launch some experiments in DVC. But when I set values of experiment parameters, DVC deletes file 'params.yaml', and experiment doesn't set in queue.</p>\n<p>Simplified code for example:\nPython file 'test.py':</p>\n<pre><code>import numpy as np\nimport json\nimport yaml\n\nparams = yaml.safe_load(open('params.yaml'))[&quot;test&quot;]\n\nprecision = np.random.random()\nrecall = params['value']\naccuracy = np.random.random()\n \n\nrows = {'precision': precision,\n        'recall': recall,\n        'accuracy': accuracy}\n\n\nwith open(params['metrics_path'], 'w') as outfile:\n    json.dump(rows, outfile)\n\nfpr = 10*np.random.random((1,10)).tolist()\ntpr = 10*np.random.random((1,10)).tolist()\n\nwith open('plot.json', 'w') as outfile2:\n    json.dump(\n      {\n        &quot;roc&quot;: [ {&quot;fpr&quot;: f, &quot;tpr&quot;: t} for f, t in zip(fpr, tpr) ]\n      }, \n      outfile2\n      )\n</code></pre>\n<p>params.yaml:</p>\n<pre><code>test:\n  metrics_path: &quot;scores.json&quot;\n  value: 1\n</code></pre>\n<p>dvc.yaml:</p>\n<pre><code>stages:\n  test:\n    cmd: python test.py\n    deps:\n    - test.py\n    params:\n    - test.metrics_path\n    - test.value\n    metrics:\n    - scores.json:\n        cache: false\n    plots:\n    - plot.json:\n        cache: false\n        x: fpr\n        y: tpr\n</code></pre>\n<p>It is strange behavior. Is it possible to fix it?</p>",
		"Question_answer_count": 1,
		"Question_comment_count": 8,
		"Question_creation_date": "2022-06-02 09:10:42.840000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": null,
		"Question_score": 2,
		"Question_tags": "python|dvc",
		"Question_view_count": 167,
		"Owner_creation_date": "2021-06-12 15:13:14.510000 UTC",
		"Owner_last_access_date": "2022-09-23 11:35:31.300000 UTC",
		"Owner_location": null,
		"Owner_reputation": 95,
		"Owner_up_votes": 3,
		"Owner_down_votes": 0,
		"Owner_views": 8,
		"Answer_body": "<p>I solved my problem. It is necessary, that all files (executable scripts, 'dvc.yaml', 'params.yaml') be tracked by git. In this case <code>dvc exp run</code> command works correctly.</p>",
		"Answer_comment_count": 0.0,
		"Answer_creation_date": "2022-06-03 08:28:54.587000 UTC",
		"Answer_last_edit_date": null,
		"Answer_score": 0.0,
		"Question_valid_tags": "dvc",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 70693420,
		"Question_title": "DVC(Data Version Control) keeps stuck at \"dvc add xxx\" with \"Collecting stages from the workspace\" in the terminal?",
		"Question_body": "<p>I used : <code>dvc[webhdfs]==2.9.3</code>, installed by <code>pip install dvc[webhdfs]</code></p>\n<p>Then the repo is already cloned by git.</p>\n<p>I have also typed : <code>dvc remote add -d storage webhdfs://xxx/dvc</code> and <code>git add .dvc/config</code></p>\n<p>But the command <code>dvc add ./assets/xxx/*</code> was still stuck...</p>\n<p>The command line window keeps showing : <code>Collecting stages from the workspace</code></p>",
		"Question_answer_count": 0,
		"Question_comment_count": 2,
		"Question_creation_date": "2022-01-13 08:24:36.767000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": "2022-01-13 10:11:54.663000 UTC",
		"Question_score": 1,
		"Question_tags": "python|deployment|continuous-integration|dvc",
		"Question_view_count": 115,
		"Owner_creation_date": "2019-03-24 03:57:13.283000 UTC",
		"Owner_last_access_date": "2022-07-29 12:48:42.787000 UTC",
		"Owner_location": "Beijing",
		"Owner_reputation": 11,
		"Owner_up_votes": 0,
		"Owner_down_votes": 0,
		"Owner_views": 2,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "dvc",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 56285351,
		"Question_title": "Updating tracked dir in DVC",
		"Question_body": "<p>According to <a href=\"https://dvc.org/doc/user-guide/update-tracked-file\" rel=\"nofollow noreferrer\">this tutorial</a> when I update file I should remove file from under DVC control first (i.e. execute <code>dvc unprotect &lt;myfile&gt;.dvc</code> or <code>dvc remove &lt;myfile&gt;.dvc</code>) and then add it again via <code>dvc add &lt;mifile&gt;</code>. However It's not clear if I should apply the same workflow for the directories.</p>\n\n<p>I have the directory under DVC control with the following structure:</p>\n\n<pre><code>data/\n    1.jpg\n    2.jpg\n</code></pre>\n\n<p>Should I run <code>dvc unprotect data</code> every time the directory content is updated?</p>\n\n<p>More specifically I'm interested if I should run <code>dvc unprotect data</code> in the following use cases:</p>\n\n<ul>\n<li><strong>New file is added.</strong> For example if I put <code>3.jpg</code> image in the data dir</li>\n<li><strong>File is deleted.</strong> For example if I delete <code>2.jpg</code> image in the <code>data</code> dir</li>\n<li><strong>File is updated.</strong> For example if I edit <code>1.jpg</code> image via graphic editor.</li>\n<li>A combination of the previous use cases (i.e. some files are updated, other deleted and new files are added)</li>\n</ul>",
		"Question_answer_count": 2,
		"Question_comment_count": 0,
		"Question_creation_date": "2019-05-24 03:10:22.490000 UTC",
		"Question_favorite_count": 1.0,
		"Question_last_edit_date": "2019-05-24 14:39:32.617000 UTC",
		"Question_score": 4,
		"Question_tags": "dvc",
		"Question_view_count": 995,
		"Owner_creation_date": "2018-03-28 16:31:38.710000 UTC",
		"Owner_last_access_date": "2022-09-23 10:08:33.687000 UTC",
		"Owner_location": "Russia",
		"Owner_reputation": 784,
		"Owner_up_votes": 32,
		"Owner_down_votes": 0,
		"Owner_views": 77,
		"Answer_body": "<p>Only when file is updated - i.e. edit <code>1.jpg</code> with your editor <strong>AND</strong> only if hadrlink or symlink cache type is enabled.</p>\n\n<p>Please, check this <a href=\"https://dvc.org/doc/user-guide/update-tracked-file\" rel=\"nofollow noreferrer\">link</a>:</p>\n\n<blockquote>\n  <p>updating tracked files has to be carried out with caution to avoid data corruption when the DVC config option cache.type is set to hardlink or/and symlink</p>\n</blockquote>\n\n<p>I would strongly recommend reading this document: <a href=\"https://dvc.org/doc/user-guide/cache-file-linking\" rel=\"nofollow noreferrer\">Performance Optimization for Large Files</a> it explains benefits of using hardlinks/symlinks.</p>",
		"Answer_comment_count": 2.0,
		"Answer_creation_date": "2019-05-24 05:04:26.680000 UTC",
		"Answer_last_edit_date": null,
		"Answer_score": 2.0,
		"Question_valid_tags": "dvc",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 72645665,
		"Question_title": "How to track a folder again when used \"git rm -rf --cached folder_name\" : Error: The following paths are ignored by one of your .gitignore files",
		"Question_body": "<p>I wanted to un-track my git files so I put <code>.dvc</code> inside my <code>.gitignore</code> file, and run</p>\n<pre><code>git rm -rf --cached .dvc\n</code></pre>\n<p>and then committed.</p>\n<p>I realised my mistake soon and then wanted to add the files again . I tried deleting the <code>gitignore</code> file, commit, make a new <code>.gitignore</code> and then try adding but all is futile. <code>git add .dvc</code> does not track my files and using <code>git add .dvc/*</code> gives me error:</p>\n<pre><code>The following paths are ignored by one of your .gitignore files:\n.dvc/cache\n.dvc/tmp\nUse -f if you really want to add them.\n</code></pre>\n<p>Running the command <code>git check-ignore -v .dvc/*</code> gives me:</p>\n<pre><code>.dvc/.gitignore:3:/cache    .dvc/cache\n.dvc/.gitignore:2:/tmp  .dvc/tmp\n\n</code></pre>\n<p>What can be done now?</p>",
		"Question_answer_count": 1,
		"Question_comment_count": 7,
		"Question_creation_date": "2022-06-16 12:13:05.170000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": "2022-06-16 12:23:53.167000 UTC",
		"Question_score": 1,
		"Question_tags": "git|gitignore|dvc",
		"Question_view_count": 41,
		"Owner_creation_date": "2019-07-01 15:44:17.583000 UTC",
		"Owner_last_access_date": "2022-09-24 15:58:25.223000 UTC",
		"Owner_location": "Noida, Uttar Pradesh, India",
		"Owner_reputation": 2716,
		"Owner_up_votes": 403,
		"Owner_down_votes": 26,
		"Owner_views": 668,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "dvc",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 73580703,
		"Question_title": "DVC | Permission denied ERROR: failed to reproduce stage: failed to run: .py, exited with 126",
		"Question_body": "<p>Goal: run <code>.py</code> files via. <code>dvc.yaml</code>.</p>\n<p>There are stages before it, in <code>dvc.yaml</code>, that don't produce the error.</p>\n<p><code>dvc exp run</code>:</p>\n<pre><code>(venv) me@ubuntu-pcs:~/PycharmProjects/project$ dvc exp run\nStage 'inference' didn't change, skipping\nRunning stage 'load_data':\n&gt; load_data.py\n/bin/bash: line 1: load_data.py: Permission denied\nERROR: failed to reproduce 'load_data': failed to run: load_data.py, exited with 126\n</code></pre>\n<p><code>dvc repro</code>:</p>\n<pre><code>(venv) me@ubuntu-pcs:~/PycharmProjects/project$ dvc repro\nStage 'predict' didn't change, skipping                                                                                                                                                                                                                        \nStage 'evaluate' didn't change, skipping\nStage 'inference' didn't change, skipping\nRunning stage 'load_data':\n&gt; load_data.py\n/bin/bash: line 1: load_data.py: Permission denied\nERROR: failed to reproduce 'load_data': failed to run: pdl1_lung_model/load_data.py, exited with 126\n</code></pre>\n<hr />\n<p><code>dvc doctor</code>:</p>\n<pre><code>DVC version: 2.10.2 (pip)\n---------------------------------\nPlatform: Python 3.9.12 on Linux-5.15.0-46-generic-x86_64-with-glibc2.35\nSupports:\n        webhdfs (fsspec = 2022.5.0),\n        http (aiohttp = 3.8.1, aiohttp-retry = 2.5.2),\n        https (aiohttp = 3.8.1, aiohttp-retry = 2.5.2),\n        s3 (s3fs = 2022.5.0, boto3 = 1.21.21)\nCache types: hardlink, symlink\nCache directory: ext4 on /dev/nvme0n1p5\nCaches: local\nRemotes: s3\nWorkspace directory: ext4 on /dev/nvme0n1p5\nRepo: dvc, git\n</code></pre>\n<p><code>dvc exp run -v</code>:</p>\n<p><a href=\"https://gist.github.com/danielbellsa/3f2fe05c1535d494a8677e54cddf684a\" rel=\"nofollow noreferrer\">output.txt</a></p>\n<p><code>dvc exp run -vv</code>:</p>\n<p><a href=\"https://gist.github.com/danielbellsa/a124cf28b3f0252556deb90b042b7cec\" rel=\"nofollow noreferrer\">output2.txt</a></p>",
		"Question_answer_count": 1,
		"Question_comment_count": 1,
		"Question_creation_date": "2022-09-02 09:45:17.277000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": "2022-09-05 09:06:49.863000 UTC",
		"Question_score": 1,
		"Question_tags": "python-3.x|permission-denied|dvc",
		"Question_view_count": 77,
		"Owner_creation_date": "2021-09-07 12:58:02.980000 UTC",
		"Owner_last_access_date": "2022-09-23 15:24:35.073000 UTC",
		"Owner_location": null,
		"Owner_reputation": 234,
		"Owner_up_votes": 708,
		"Owner_down_votes": 14,
		"Owner_views": 155,
		"Answer_body": "<h3>Solution 1</h3>\n<p><code>.py</code> files weren't running as scripts.</p>\n<p>They need to be; if you want to run one <code>.py</code> file per <code>stage</code> in <code>dvc.yaml</code>.</p>\n<p>To do so, you want to append <strong>Boiler-plate code</strong>, at the bottom of each <code>.py</code> file.</p>\n<pre><code>if __name__ == &quot;__main__&quot;:\n    # invoke primary function() in .py file, w/ params\n</code></pre>\n<h3>Solution 2</h3>\n<pre><code>chmod 777 ....py\n</code></pre>\n<h3>Soution 3</h3>\n<p>I forgot the <code>python</code> in <code>cmd:</code></p>\n<pre><code>  load_data:\n    cmd: python pdl1_lung_model/load_data.py\n</code></pre>",
		"Answer_comment_count": 0.0,
		"Answer_creation_date": "2022-09-02 10:34:04.133000 UTC",
		"Answer_last_edit_date": "2022-09-05 09:05:53.113000 UTC",
		"Answer_score": 1.0,
		"Question_valid_tags": "dvc",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 71338160,
		"Question_title": "DVC Experiment management workflow",
		"Question_body": "<p>I'm struggling with the DVC experiment management. Suppose the following scenario:</p>\n<p>I have <code>params.yaml</code> file:</p>\n<pre><code>recommendations:\n  k: 66\n  q: 5\n</code></pre>\n<p>I run the experiment with <code>dvc exp run -n exp_66</code>, and then I do <code>dvc exp push origin exp_66</code>. After this, I modify <code>params.yaml</code> file:</p>\n<pre><code>recommendations:\n  k: 99\n  q: 5\n</code></pre>\n<p>and then run another experiment <code>dvc exp run -n exp_99</code>, after which I commit with <code>dvc exp push origin exp_99</code>.</p>\n<p>Now, when I pull the corresponding branch with Git, I try to pull <code>exp_66</code> from dvc by running <code>dvc exp pull origin exp_66</code>. This does the pull (no error messages), but the content of the <code>params.yaml</code> file is with <code>k: 99</code> (and I would expect <code>k: 66</code>). What am I doing wrong? Does <code>git push</code> have to be executed after <code>dvc push</code>? Apart from that, I also found <code>dvc exp apply exp_66</code>, but I'm not sure what it does (it is suggested that after <code>apply</code> one should execute <code>git add .</code>, then <code>git commit</code>?</p>\n<p>I would really appreciate if you could write down the workflow with committing different experiments, pushing, pulling, applying, etc.</p>",
		"Question_answer_count": 1,
		"Question_comment_count": 1,
		"Question_creation_date": "2022-03-03 13:38:53.127000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": null,
		"Question_score": 1,
		"Question_tags": "git|dvcs|dvc",
		"Question_view_count": 152,
		"Owner_creation_date": "2022-01-28 12:42:49.633000 UTC",
		"Owner_last_access_date": "2022-09-21 13:14:46.480000 UTC",
		"Owner_location": null,
		"Owner_reputation": 67,
		"Owner_up_votes": 3,
		"Owner_down_votes": 0,
		"Owner_views": 3,
		"Answer_body": "<p>You did everything alright. In the end, after pulling, you can see that when using <code>dvc exp show</code> your experiments will be there. To restore the experiment available from your experiment list into your workspace, you simply need to run <code>dvc exp apply exp_66</code>. DVC will make sure that the changes corresponding to this experiment will be checked out.</p>\n<p>Your workflow seems correct so far. One addition: once you make sure one of the experiments is what you want to &quot;keep&quot; in git history, you can use <code>dvc exp branch {exp_id} {branch_name}</code> to create a separate branch for this experiment. Then you can use <code>git</code> commands to save the changes.</p>",
		"Answer_comment_count": 5.0,
		"Answer_creation_date": "2022-03-03 15:05:59.073000 UTC",
		"Answer_last_edit_date": null,
		"Answer_score": 3.0,
		"Question_valid_tags": "dvc",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 66477468,
		"Question_title": "Data Version Control with Google Drive Remote: \"googleapiclient.errors.UnknownApiNameOrVersion: name: drive version: v2\"",
		"Question_body": "<p>I'm trying to setup DVC with Google Drive storage as shown <a href=\"https://dvc.org/doc/user-guide/setup-google-drive-remote#url-format\" rel=\"nofollow noreferrer\">here</a>. So far, I've been unsuccessful in pushing data to the remote. I tried both with and without the Google App setup.</p>\n<p>After running a <code>dvc push -v</code>, the following exception is shown:</p>\n<pre><code>  File &quot;(...)/anaconda3/lib/python3.8/site-packages/googleapiclient/discovery.py&quot;, line 387, in _retrieve_discovery_doc\n    raise UnknownApiNameOrVersion(&quot;name: %s  version: %s&quot; % (serviceName, version))\ngoogleapiclient.errors.UnknownApiNameOrVersion: name: drive  version: v2\n</code></pre>\n<p>DVC was installed via <code>pip install dvc[gdrive]</code>. The <code>pip freeze</code> of the concerning packages is:</p>\n<pre><code>oauth2client==4.1.3\ngoogle-api-python-client==2.0.1\ndvc==2.0.1\n</code></pre>\n<p>Any help is thoroughly appreciated.</p>",
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_creation_date": "2021-03-04 14:51:05.907000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": "2021-03-04 16:12:18.737000 UTC",
		"Question_score": 4,
		"Question_tags": "google-client|dvc",
		"Question_view_count": 979,
		"Owner_creation_date": "2020-06-24 13:54:59.210000 UTC",
		"Owner_last_access_date": "2022-09-25 01:13:01.880000 UTC",
		"Owner_location": null,
		"Owner_reputation": 704,
		"Owner_up_votes": 53,
		"Owner_down_votes": 6,
		"Owner_views": 33,
		"Answer_body": "<p>Can you try to install <code>google-api-python-client==1.12.8</code> and test in that way?</p>\n<p>Edit:</p>\n<p>It appears to be that, this was a bug in the 2.0.0-2.0.1 of google-api-client and resolved in 2.0.2. So this should also work <code>google-api-python-client&gt;=2.0.2</code></p>",
		"Answer_comment_count": 1.0,
		"Answer_creation_date": "2021-03-04 15:58:09.453000 UTC",
		"Answer_last_edit_date": "2021-03-05 06:56:47.287000 UTC",
		"Answer_score": 4.0,
		"Question_valid_tags": "dvc",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 68743071,
		"Question_title": "dvc.api.read() raises an \"UnicodeDecodeError\"",
		"Question_body": "<p>I am trying to acess a DICOM file [image saved in the Digital Imaging and Communications in Medicine (DICOM) format]:</p>\n<pre><code>import dvc.api\n\npath = 'dir/image.dcm'\nremote = 'remote_name'\nrepo = 'git_repo'\nmode = 'r'\n\ndata = dvc.api.read(path = path, remote = remote, repo = repo, mode = mode)\n</code></pre>\n<p>When I run the previous code, and after the &quot;downloading progress bar&quot; is complete, I get the following error:</p>\n<pre><code>Traceback (most recent call last): File &quot;draft.py&quot;, line 7, in &lt;module&gt; mode ='r') File &quot;C:\\Users\\lbrandao\\anaconda3\\envs\\my_env\\lib\\site-packages\\dvc\\api.py&quot;, line 91, in read return fd.read() File &quot;C:\\Users\\lbrandao\\anaconda3\\envs\\my_env\\lib\\encodings\\cp1252.py&quot;, line 23, in decode return codecs.charmap_decode(input,self.errors,decoding_table)[0] UnicodeDecodeError: 'charmap' codec can't decode byte 0x81 in position 764: character maps to &lt;undefined&gt;\n</code></pre>\n<p>I tried to overcome this issue by using the encoding argument:</p>\n<pre><code>data = dvc.api.read(path = path, remote = remote, repo = repo, mode = mode, encoding='ANSI')\n</code></pre>\n<p>Since, when I open a DICOM file using for example Notepad++, this is the encoding specified. However, it raises the error:</p>\n<pre><code>Exception ignored in: &lt;bound method Pool.__del__ of &lt;dvc.fs.pool.Pool object at 0x0000021D1347A160&gt;&gt; Traceback (most recent call last): File &quot;C:\\Users\\lbrandao\\anaconda3\\envs\\my_env\\lib\\site-packages\\dvc\\fs\\pool.py&quot;, line 42, in __del__ File &quot;C:\\Users\\lbrandao\\anaconda3\\envs\\my_env\\lib\\site-packages\\dvc\\fs\\pool.py&quot;, line 46, in close File &quot;C:\\Users\\lbrandao\\anaconda3\\envs\\my_env\\lib\\site-packages\\dvc\\fs\\ssh\\connection.py&quot;, line 71, in close File &quot;C:\\Users\\lbrandao\\anaconda3\\envs\\my_env\\lib\\site-packages\\paramiko\\sftp_client.py&quot;, line 194, in close File &quot;C:\\Users\\lbrandao\\anaconda3\\envs\\my_env\\lib\\site-packages\\paramiko\\sftp_client.py&quot;, line 185, in _log File &quot;C:\\Users\\lbrandao\\anaconda3\\envs\\my_env\\lib\\site-packages\\paramiko\\sftp.py&quot;, line 158, in _log File &quot;C:\\Users\\lbrandao\\anaconda3\\envs\\my_env\\lib\\logging\\__init__.py&quot;, line 1372, in log File &quot;C:\\Users\\lbrandao\\anaconda3\\envs\\my_env\\lib\\logging\\__init__.py&quot;, line 1441, in _log File &quot;C:\\Users\\lbrandao\\anaconda3\\envs\\my_env\\lib\\logging\\__init__.py&quot;, line 1411, in makeRecord TypeError: 'NoneType' object is not callable\n</code></pre>\n<p>I also tried <code>encoding = 'utf-8'</code>, but the &quot;UnicodeDecodeError&quot; continues to appear:</p>\n<pre><code>Traceback (most recent call last): File &quot;draft.py&quot;, line 7, in &lt;module&gt; mode ='r', encoding='utf-8') File &quot;C:\\Users\\lbrandao\\anaconda3\\envs\\ccab_env_dev\\lib\\site-packages\\dvc\\api.py&quot;, line 91, in read return fd.read() File &quot;C:\\Users\\lbrandao\\anaconda3\\envs\\ccab_env_dev\\lib\\codecs.py&quot;, line 321, in decode (result, consumed) = self._buffer_decode(data, self.errors, final) UnicodeDecodeError: 'utf-8' codec can't decode byte 0xd0 in position 140: invalid continuation byte\n</code></pre>\n<p>Can anyone please help? Thanks.</p>",
		"Question_answer_count": 1,
		"Question_comment_count": 4,
		"Question_creation_date": "2021-08-11 13:32:25.447000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": "2021-08-11 15:09:10.723000 UTC",
		"Question_score": 1,
		"Question_tags": "python|encoding|dvc",
		"Question_view_count": 186,
		"Owner_creation_date": "2019-09-30 10:15:09.623000 UTC",
		"Owner_last_access_date": "2022-09-19 14:01:39.763000 UTC",
		"Owner_location": null,
		"Owner_reputation": 95,
		"Owner_up_votes": 13,
		"Owner_down_votes": 0,
		"Owner_views": 12,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "dvc",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 64653042,
		"Question_title": "Control tracked version of external dependency",
		"Question_body": "<p>I am trying to set up a DVC repository for machine learning data with different tagged versions of the dataset. I do this with something like:</p>\n<pre class=\"lang-sh prettyprint-override\"><code>$ cd /raid/ml_data  # folder on a data drive\n$ git init\n$ dvc init\n$ [add data]\n$ [commit to dvc, git]\n$ git tag -a 1.0.0\n$ [add or change data]\n$ [commit to dvc, git]\n$ git tag -a 1.1.0\n</code></pre>\n<p>I have multiple projects that each need to reference some version of this dataset. The problem is I can't figure out how to set up those projects to reference a specific version. I'm able to track the <code>HEAD</code> of the repo with something like:</p>\n<pre class=\"lang-sh prettyprint-override\"><code>$ cd ~/my_proj  # different drive than the remote\n$ mkdir data\n$ git init\n$ dvc init\n$ dvc remote add -d local /raid/ml_data  # add the remote on my data drive\n$ dvc cache dir /raid/ml_data/.dvc/cache  # tell DVC to use the remote cache\n$ dvc checkout\n$ dvc run --external -d /raid/ml_data -o data/ cp -r /raid/ml_data data\n</code></pre>\n<p>This gets me the latest version of the dataset, symlinked into my <code>data</code> folder, but what if I want some projects to use the <code>1.0.0</code> version and some to use the <code>1.1.0</code> version, or another version? Or for that matter, if I update the dataset to <code>2.0.0</code> but don't want my existing projects to necessarily track <code>HEAD</code> and instead keep the version with which they were set up?</p>\n<p>It's important to me to not create a ton of local copies of my dataset as the <code>/home</code> drive is much smaller than the <code>/raid</code> drive and some of these datasets are huge.</p>",
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_creation_date": "2020-11-02 20:42:34.297000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": "2020-11-02 23:50:23.337000 UTC",
		"Question_score": 2,
		"Question_tags": "version-control|dvc",
		"Question_view_count": 139,
		"Owner_creation_date": "2013-06-07 18:26:33.700000 UTC",
		"Owner_last_access_date": "2022-09-14 18:00:36.650000 UTC",
		"Owner_location": "Colorado Springs, CO",
		"Owner_reputation": 11685,
		"Owner_up_votes": 2855,
		"Owner_down_votes": 47,
		"Owner_views": 1329,
		"Answer_body": "<p>I think you are looking for the <a href=\"https://dvc.org/doc/start/data-access\" rel=\"nofollow noreferrer\">data access</a> set of commands.</p>\n<p>In your particular case, <code>dvc import</code> makes sense:</p>\n<pre><code>$ dvc import /raid/ml_data data\n</code></pre>\n<p>if you want to get the most recent version (HEAD). Then you will be able to update it with the <code>dvc update</code> command (if 2.0.0 is released, for example).</p>\n<pre><code>$ dvc import /raid/ml_data data --rev 1.0.0\n</code></pre>\n<p>if you'd like to &quot;fix&quot; it to the specific version.</p>\n<h3>Avoiding copies</h3>\n<p>Make sure also, that <code>symlinks</code> are set for the second project, as described in the <a href=\"https://dvc.org/doc/user-guide/large-dataset-optimization\" rel=\"nofollow noreferrer\">Large Dataset Optimization</a>:</p>\n<pre><code>$ dvc config cache.type reflink,hardlink,symlink,copy\n</code></pre>\n<p>(there are config modifiers <code>--global</code>, <code>--local</code>, <code>--system</code> to set this setting for everyone at once, or just for one project, etc)</p>\n<p>Check the details instruction <a href=\"https://dvc.org/doc/user-guide/large-dataset-optimization#configuring-dvc-cache-file-link-type\" rel=\"nofollow noreferrer\">here</a>.</p>\n<hr />\n<p>Overall, it's a great setup, and looks like you got pretty much everything right. Please, don't hesitate to follow up and/or create other questions here- we'll help you with this.</p>",
		"Answer_comment_count": 2.0,
		"Answer_creation_date": "2020-11-02 21:12:41.433000 UTC",
		"Answer_last_edit_date": "2020-11-03 00:16:03.343000 UTC",
		"Answer_score": 1.0,
		"Question_valid_tags": "dvc",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 67421254,
		"Question_title": "How to add a file to a dvc-tracked folder without pulling the whole folder's content?",
		"Question_body": "<p>Let's say I am working inside a git/dvc repo. There is a folder <code>data</code> containing 100k small files. I track it with DVC as a single element, as recommended by the doc:</p>\n<pre class=\"lang-sh prettyprint-override\"><code>dvc add data\n</code></pre>\n<p>and because in my experience, DVC is kinda slow when tracking that many files one by one.</p>\n<p>I clone the repo on another workspace, and now I have the <code>data.dvc</code> file locally but none of the actual files inside yet. I want to add a file named <code>newfile.txt</code> to the <code>data</code> folder and track it with DVC. Is there a way to do this <em>without pulling the whole content of <code>data</code> locally</em> ?</p>\n<p>What I have tried for now:</p>\n<ol>\n<li><p>Adding the <code>data</code> folder again:</p>\n<pre><code>mkdir data\nmv path/to/newfile.txt data/newfile.txt\ndvc add data\n</code></pre>\n<p>The <code>data.dvc</code> file is built again from the local state of <code>data</code> which only contains <code>newfile.txt</code> so this doesn't work.</p>\n</li>\n<li><p>Adding the file as a single element in <code>data</code> folder:</p>\n<pre><code> dvc add data/newfile.txt\n</code></pre>\n<p>I get :</p>\n<pre><code> Cannot add 'data/newfile.txt', because it is overlapping with other DVC tracked output: 'data'. \n To include 'data/newfile.txt' in 'data', run 'dvc commit data.dvc'\n</code></pre>\n</li>\n<li><p>Using dvc commit as suggested</p>\n<pre><code> mkdir data\n mv path/to/newfile.txt data/newfile.txt\n dvc commit data.dvc\n</code></pre>\n<p>Similarly as 1., the <code>data.dvc</code> is rebuilt again from local state of <code>data</code>.</p>\n</li>\n</ol>",
		"Question_answer_count": 1,
		"Question_comment_count": 1,
		"Question_creation_date": "2021-05-06 15:25:19.220000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": "2021-05-06 15:28:32.033000 UTC",
		"Question_score": 3,
		"Question_tags": "dvc",
		"Question_view_count": 1781,
		"Owner_creation_date": "2021-05-06 14:35:20.937000 UTC",
		"Owner_last_access_date": "2021-07-20 09:38:49.723000 UTC",
		"Owner_location": null,
		"Owner_reputation": 31,
		"Owner_up_votes": 0,
		"Owner_down_votes": 0,
		"Owner_views": 1,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "dvc",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 60365473,
		"Question_title": "By how much can i approx. reduce disk volume by using dvc?",
		"Question_body": "<p>I want to classify ~1m+ documents and have a Version Control System for in- and Output of the corresponding model. </p>\n\n<p>The data changes over time:</p>\n\n<ul>\n<li>sample size increases over time</li>\n<li>new Features might appear</li>\n<li>anonymization procedure might Change over time</li>\n</ul>\n\n<p>So basically \"everything\" might change: amount of observations, Features and the values.\nWe are interested in making the ml model Building reproducible without using 10/100+ GB \nof disk volume, because we save all updated versions of Input data. Currently the volume size of the data is ~700mb.</p>\n\n<p>The most promising tool i found is: <a href=\"https://github.com/iterative/dvc\" rel=\"noreferrer\">https://github.com/iterative/dvc</a>. Currently the data\nis stored in a database in loaded in R/Python from there.</p>\n\n<p><strong>Question:</strong></p>\n\n<p>How much disk volume can be (very approx.) saved by using dvc? </p>\n\n<p>If one can roughly estimate that. I tried to find out if only the \"diffs\" of the data are saved. I didnt find much info by reading through: <a href=\"https://github.com/iterative/dvc#how-dvc-works\" rel=\"noreferrer\">https://github.com/iterative/dvc#how-dvc-works</a> or other documentation. </p>\n\n<p><strong>I am aware that this is a very vague question. And it will highly depend on the dataset. However, i would still be interested in getting a very approximate idea.</strong></p>",
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_creation_date": "2020-02-23 18:31:41.247000 UTC",
		"Question_favorite_count": 3.0,
		"Question_last_edit_date": "2020-02-23 19:28:48.287000 UTC",
		"Question_score": 7,
		"Question_tags": "python|sql|r|git|dvc",
		"Question_view_count": 689,
		"Owner_creation_date": "2017-08-30 12:46:30.907000 UTC",
		"Owner_last_access_date": "2022-03-11 18:10:58.673000 UTC",
		"Owner_location": null,
		"Owner_reputation": 1365,
		"Owner_up_votes": 145,
		"Owner_down_votes": 3,
		"Owner_views": 193,
		"Answer_body": "<p>Let me try to summarize how does DVC store data and I hope you'll be able to figure our from this how much space will be saved/consumed in your specific scenario.</p>\n\n<p><strong>DVC is storing and deduplicating data on the individual <em>file level</em>.</strong> So, what does it usually mean from a practical perspective.</p>\n\n<p>I will use <code>dvc add</code> as an example, but the same logic applies to all commands that save data files or directories into DVC cache - <code>dvc add</code>, <code>dvc run</code>, etc.</p>\n\n<h2>Scenario 1: Modifying file</h2>\n\n<p>Let's imagine I have a single 1GB XML file. I start tracking it with DVC:</p>\n\n<pre class=\"lang-sh prettyprint-override\"><code>$ dvc add data.xml\n</code></pre>\n\n<p>On the modern file system (or if <code>hardlinks</code>, <code>symlinks</code> are enabled, see <a href=\"https://dvc.org/doc/user-guide/large-dataset-optimization\" rel=\"noreferrer\">this</a> for more details) after this command we still consume 1GB (even though file is moved into DVC cache and is still present in the workspace).</p>\n\n<p>Now, let's change it a bit and save it again:</p>\n\n<pre class=\"lang-sh prettyprint-override\"><code>$ echo \"&lt;test/&gt;\" &gt;&gt; data.xml\n$ dvc add data.xml\n</code></pre>\n\n<p>In this case we will have 2GB consumed. <strong>DVC does not do diff between two versions of the same file</strong>, neither it splits files into chunks or blocks to understand that only small portion of data has changed.</p>\n\n<blockquote>\n  <p>To be precise, it calculates <code>md5</code> of each file and save it in the content addressable key-value storage. <code>md5</code> of the files serves as a key (path of the file in cache) and value is the file itself:</p>\n  \n  <pre class=\"lang-sh prettyprint-override\"><code>(.env) [ivan@ivan ~/Projects/test]$ md5 data.xml\n0c12dce03223117e423606e92650192c\n\n(.env) [ivan@ivan ~/Projects/test]$ tree .dvc/cache\n.dvc/cache\n\u2514\u2500\u2500 0c\n   \u2514\u2500\u2500 12dce03223117e423606e92650192c\n\n1 directory, 1 file\n\n(.env) [ivan@ivan ~/Projects/test]$ ls -lh data.xml\ndata.xml ----&gt; .dvc/cache/0c/12dce03223117e423606e92650192c (some type of link)\n</code></pre>\n</blockquote>\n\n<h2>Scenario 2: Modifying directory</h2>\n\n<p>Let's now imagine we have a single large 1GB directory <code>images</code> with a lot of files:</p>\n\n<pre class=\"lang-sh prettyprint-override\"><code>$ du -hs images\n1GB\n\n$ ls -l images | wc -l\n1001\n\n$ dvc add images\n</code></pre>\n\n<p>At this point we still consume 1GB. Nothing has changed. But if we modify the directory by adding more files (or removing some of them):</p>\n\n<pre><code>$ cp /tmp/new-image.png images\n\n$ ls -l images | wc -l\n1002\n\n$ dvc add images\n</code></pre>\n\n<p>In this case, after saving the new version we <strong>still close to 1GB</strong> consumption. <strong>DVC calculates diff on the directory level.</strong> It won't be saving all the files that were existing before in the directory.</p>\n\n<p>The same logic applies to all commands that save data files or directories into DVC cache - <code>dvc add</code>, <code>dvc run</code>, etc.</p>\n\n<p>Please, let me know if it's clear or we need to add more details, clarifications.</p>",
		"Answer_comment_count": 4.0,
		"Answer_creation_date": "2020-02-23 19:57:47.857000 UTC",
		"Answer_last_edit_date": null,
		"Answer_score": 12.0,
		"Question_valid_tags": "dvc",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	},
	{
		"Question_id": 68517516,
		"Question_title": "Forbidden: An error occurred (403) when calling the HeadObject operation:",
		"Question_body": "<p>my ~/.aws/credentials looks like</p>\n<pre><code>[default]\naws_access_key_id = XYZ\naws_secret_access_key = ABC\n\n[testing]\nsource_profile = default\nrole_arn = arn:aws:iam::54:role/ad\n</code></pre>\n<p>I add my remote like</p>\n<pre><code>dvc remote add --local -v myremote s3://bib-ds-models-testing/data/dvc-test\n</code></pre>\n<p>I have made my .dvc/config.local to look like</p>\n<pre><code>[\u2018remote \u201cmyremote\u201d\u2019]\nurl = s3://bib-ds-models-testing/data/dvc-test\naccess_key_id = XYZ\nsecret_access_key = ABC/h2hOsRcCIFqwYWV7eZaUq3gNmS\nprofile=\u2018testing\u2019\ncredentialpath = /Users/nyt21/.aws/credentials\n</code></pre>\n<p>but still after running <code>dvc push -r myremote</code> I get</p>\n<blockquote>\n<p>ERROR: unexpected error - Forbidden: An error occurred (403) when calling the HeadObject operation: Forbidden</p>\n</blockquote>\n<p>** Update\nhere is the output of <code>dvc push -v</code></p>\n<pre><code>2021-07-25 22:40:38,887 DEBUG: Check for update is enabled.\n2021-07-25 22:40:39,022 DEBUG: Preparing to upload data to 's3://bib-ds-models-testing/data/dvc-test'\n2021-07-25 22:40:39,022 DEBUG: Preparing to collect status from s3://bib-ds-models-testing/data/dvc-test\n2021-07-25 22:40:39,022 DEBUG: Collecting information from local cache...\n2021-07-25 22:40:39,022 DEBUG: Collecting information from remote cache...                                                                                                                     \n2021-07-25 22:40:39,022 DEBUG: Matched '0' indexed hashes\n2021-07-25 22:40:39,022 DEBUG: Querying 1 hashes via object_exists\n2021-07-25 22:40:39,644 ERROR: unexpected error - Forbidden: An error occurred (403) when calling the HeadObject operation: Forbidden                                                          \n------------------------------------------------------------\nTraceback (most recent call last):\n  File &quot;/Users/nyt21/opt/miniconda3/envs/dvc/lib/python3.8/site-packages/s3fs/core.py&quot;, line 246, in _call_s3\n    out = await method(**additional_kwargs)\n  File &quot;/Users/nyt21/opt/miniconda3/envs/dvc/lib/python3.8/site-packages/aiobotocore/client.py&quot;, line 154, in _make_api_call\n    raise error_class(parsed_response, operation_name)\nbotocore.exceptions.ClientError: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File &quot;/Users/nyt21/opt/miniconda3/envs/dvc/lib/python3.8/site-packages/s3fs/core.py&quot;, line 1057, in _info\n    out = await self._simple_info(path)\n  File &quot;/Users/nyt21/opt/miniconda3/envs/dvc/lib/python3.8/site-packages/s3fs/core.py&quot;, line 970, in _simple_info\n    out = await self._call_s3(\n  File &quot;/Users/nyt21/opt/miniconda3/envs/dvc/lib/python3.8/site-packages/s3fs/core.py&quot;, line 265, in _call_s3\n    raise translate_boto_error(err)\nPermissionError: Access Denied\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;/Users/nyt21/opt/miniconda3/envs/dvc/lib/python3.8/site-packages/s3fs/core.py&quot;, line 246, in _call_s3\n    out = await method(**additional_kwargs)\n  File &quot;/Users/nyt21/opt/miniconda3/envs/dvc/lib/python3.8/site-packages/aiobotocore/client.py&quot;, line 154, in _make_api_call\n    raise error_class(parsed_response, operation_name)\nbotocore.exceptions.ClientError: An error occurred (403) when calling the HeadObject operation: Forbidden\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File &quot;/Users/nyt21/opt/miniconda3/envs/dvc/lib/python3.8/site-packages/dvc/main.py&quot;, line 55, in main\n    ret = cmd.do_run()\n  File &quot;/Users/nyt21/opt/miniconda3/envs/dvc/lib/python3.8/site-packages/dvc/command/base.py&quot;, line 50, in do_run\n    return self.run()\n  File &quot;/Users/nyt21/opt/miniconda3/envs/dvc/lib/python3.8/site-packages/dvc/command/data_sync.py&quot;, line 57, in run\n    processed_files_count = self.repo.push(\n  File &quot;/Users/nyt21/opt/miniconda3/envs/dvc/lib/python3.8/site-packages/dvc/repo/__init__.py&quot;, line 51, in wrapper\n    return f(repo, *args, **kwargs)\n  File &quot;/Users/nyt21/opt/miniconda3/envs/dvc/lib/python3.8/site-packages/dvc/repo/push.py&quot;, line 44, in push\n    pushed += self.cloud.push(objs, jobs, remote=remote)\n  File &quot;/Users/nyt21/opt/miniconda3/envs/dvc/lib/python3.8/site-packages/dvc/data_cloud.py&quot;, line 79, in push\n    return remote_obj.push(\n  File &quot;/Users/nyt21/opt/miniconda3/envs/dvc/lib/python3.8/site-packages/dvc/remote/base.py&quot;, line 57, in wrapper\n    return f(obj, *args, **kwargs)\n  File &quot;/Users/nyt21/opt/miniconda3/envs/dvc/lib/python3.8/site-packages/dvc/remote/base.py&quot;, line 494, in push\n    ret = self._process(\n  File &quot;/Users/nyt21/opt/miniconda3/envs/dvc/lib/python3.8/site-packages/dvc/remote/base.py&quot;, line 351, in _process\n    dir_status, file_status, dir_contents = self._status(\n  File &quot;/Users/nyt21/opt/miniconda3/envs/dvc/lib/python3.8/site-packages/dvc/remote/base.py&quot;, line 195, in _status\n    self.hashes_exist(\n  File &quot;/Users/nyt21/opt/miniconda3/envs/dvc/lib/python3.8/site-packages/dvc/remote/base.py&quot;, line 145, in hashes_exist\n    return indexed_hashes + self.odb.hashes_exist(list(hashes), **kwargs)\n  File &quot;/Users/nyt21/opt/miniconda3/envs/dvc/lib/python3.8/site-packages/dvc/objects/db/base.py&quot;, line 438, in hashes_exist\n    remote_hashes = self.list_hashes_exists(hashes, jobs, name)\n  File &quot;/Users/nyt21/opt/miniconda3/envs/dvc/lib/python3.8/site-packages/dvc/objects/db/base.py&quot;, line 389, in list_hashes_exists\n    ret = list(itertools.compress(hashes, in_remote))\n  File &quot;/Users/nyt21/opt/miniconda3/envs/dvc/lib/python3.8/concurrent/futures/_base.py&quot;, line 619, in result_iterator\n    yield fs.pop().result()\n  File &quot;/Users/nyt21/opt/miniconda3/envs/dvc/lib/python3.8/concurrent/futures/_base.py&quot;, line 444, in result\n    return self.__get_result()\n  File &quot;/Users/nyt21/opt/miniconda3/envs/dvc/lib/python3.8/concurrent/futures/_base.py&quot;, line 389, in __get_result\n    raise self._exception\n  File &quot;/Users/nyt21/opt/miniconda3/envs/dvc/lib/python3.8/concurrent/futures/thread.py&quot;, line 57, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File &quot;/Users/nyt21/opt/miniconda3/envs/dvc/lib/python3.8/site-packages/dvc/objects/db/base.py&quot;, line 380, in exists_with_progress\n    ret = self.fs.exists(path_info)\n  File &quot;/Users/nyt21/opt/miniconda3/envs/dvc/lib/python3.8/site-packages/dvc/fs/fsspec_wrapper.py&quot;, line 92, in exists\n    return self.fs.exists(self._with_bucket(path_info))\n  File &quot;/Users/nyt21/opt/miniconda3/envs/dvc/lib/python3.8/site-packages/fsspec/asyn.py&quot;, line 87, in wrapper\n    return sync(self.loop, func, *args, **kwargs)\n  File &quot;/Users/nyt21/opt/miniconda3/envs/dvc/lib/python3.8/site-packages/fsspec/asyn.py&quot;, line 68, in sync\n    raise result[0]\n  File &quot;/Users/nyt21/opt/miniconda3/envs/dvc/lib/python3.8/site-packages/fsspec/asyn.py&quot;, line 24, in _runner\n    result[0] = await coro\n  File &quot;/Users/nyt21/opt/miniconda3/envs/dvc/lib/python3.8/site-packages/s3fs/core.py&quot;, line 802, in _exists\n    await self._info(path, bucket, key, version_id=version_id)\n  File &quot;/Users/nyt21/opt/miniconda3/envs/dvc/lib/python3.8/site-packages/s3fs/core.py&quot;, line 1061, in _info\n    out = await self._version_aware_info(path, version_id)\n  File &quot;/Users/nyt21/opt/miniconda3/envs/dvc/lib/python3.8/site-packages/s3fs/core.py&quot;, line 1004, in _version_aware_info\n    out = await self._call_s3(\n  File &quot;/Users/nyt21/opt/miniconda3/envs/dvc/lib/python3.8/site-packages/s3fs/core.py&quot;, line 265, in _call_s3\n    raise translate_boto_error(err)\nPermissionError: Forbidden\n------------------------------------------------------------\n2021-07-25 22:40:39,712 DEBUG: Version info for developers:\nDVC version: 2.5.4 (pip)\n---------------------------------\nPlatform: Python 3.8.10 on macOS-10.16-x86_64-i386-64bit\nSupports:\n        http (requests = 2.26.0),\n        https (requests = 2.26.0),\n        s3 (s3fs = 2021.6.1, boto3 = 1.18.6)\nCache types: reflink, hardlink, symlink\nCache directory: apfs on /dev/disk3s1s1\nCaches: local\nRemotes: s3\nWorkspace directory: apfs on /dev/disk3s1s1\nRepo: dvc, git\n\nHaving any troubles? Hit us up at https://dvc.org/support, we are always happy to help!\n2021-07-25 22:40:39,713 DEBUG: Analytics is enabled.\n2021-07-25 22:40:39,765 DEBUG: Trying to spawn '['daemon', '-q', 'analytics', '/var/folders/4x/xhm22wt16gl6m9nvkl9gllkc0000gn/T/tmpo86jdns5']'\n2021-07-25 22:40:39,769 DEBUG: Spawned '['daemon', '-q', 'analytics', '/var/folders/4x/xhm22wt16gl6m9nvkl9gllkc0000gn/T/tmpo86jdns5']'\n</code></pre>\n<p>I can upload through python</p>\n<pre><code>import boto3\nimport os\nimport pickle\n\nbucket_name = 'bib-ds-models-testing'\nos.environ[&quot;AWS_PROFILE&quot;] = &quot;testing&quot;\nsession = boto3.Session()\ns3_client = boto3.client('s3')\n\ns3_client.upload_file('/Users/nyt21/Devel/DVC/test/data/iris.csv',\n    'bib-ds-models-testing',\n    'data/dvc-test/my_iris.csv')\n</code></pre>\n<p>I don't use aws CLI but the following also gives an access deny !</p>\n<pre><code>aws s3 ls s3://bib-ds-models-testing/data/dvc-test\n</code></pre>\n<blockquote>\n<p>An error occurred (AccessDenied) when calling the ListObjectsV2\noperation: Access Denied</p>\n</blockquote>\n<p>but it works if I add --profile=testing</p>\n<pre><code>aws s3 ls s3://bib-ds-models-testing/data/dvc-test --profile=testing\n                       \n</code></pre>\n<blockquote>\n<p>PRE dvc-test/</p>\n</blockquote>\n<p>just you know environment variable <code>AWS_PROFILE</code> is already set to 'testing'</p>\n<p><strong>UPDATE</strong></p>\n<p>I have tried both <code>AWS_PROFILE='testing'</code> and <code>AWS_PROFILE=testing</code>, neither of them worked.</p>\n<p><a href=\"https://i.stack.imgur.com/DZQlz.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/DZQlz.png\" alt=\"enter image description here\" /></a></p>",
		"Question_answer_count": 0,
		"Question_comment_count": 7,
		"Question_creation_date": "2021-07-25 10:04:37.173000 UTC",
		"Question_favorite_count": null,
		"Question_last_edit_date": "2021-07-27 10:54:57.973000 UTC",
		"Question_score": 1,
		"Question_tags": "dvc",
		"Question_view_count": 1526,
		"Owner_creation_date": "2011-02-10 16:54:52.887000 UTC",
		"Owner_last_access_date": "2022-09-21 18:56:24.013000 UTC",
		"Owner_location": "Copenhagen, Denmark",
		"Owner_reputation": 4988,
		"Owner_up_votes": 350,
		"Owner_down_votes": 22,
		"Owner_views": 416,
		"Answer_body": null,
		"Answer_comment_count": null,
		"Answer_creation_date": null,
		"Answer_last_edit_date": null,
		"Answer_score": null,
		"Question_valid_tags": "dvc",
		"id": null,
		"taxonomy": null,
		"annotator": null,
		"annotation_id": null,
		"created_at": null,
		"updated_at": null,
		"lead_time": null
	}
]