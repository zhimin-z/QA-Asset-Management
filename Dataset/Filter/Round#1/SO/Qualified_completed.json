[
    {
        "Question_id":62841756,
        "Question_title":"Can't use HDFS path to set_tracking_uri in mlflow within python",
        "Question_body":"<p>I'm new to mlflow so I may misunderstand how things are supposed to work on a fundamental level.<\/p>\n<p>However when I try to do the following:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>TRACKING_URI = os.path.join(\n    &quot;hdfs:\/\/namenode\/user\/userid\/&quot;,\n    &quot;mlflow&quot;,\n    &quot;anomaly_detection&quot;,\n)\n        \nmlflow.set_tracking_uri(TRACKING_URI)\nclient = mlflow.tracking.MlflowClient(TRACKING_URI)\n<\/code><\/pre>\n<p>I get the following error:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>UnsupportedModelRegistryStoreURIException:  Model registry functionality is unavailable; got unsupported URI 'hdfs:\/\/nameservice1\/user\/rxb427\/mlflow\/anomaly_detection' for model registry data storage. Supported URI schemes are: ['', 'file', 'databricks', 'http', 'https', 'postgresql', 'mysql', 'sqlite', 'mssql']. See https:\/\/www.mlflow.org\/docs\/latest\/tracking.html#storage for how to run an MLflow server against one of the supported backend storage locations.\n<\/code><\/pre>\n<p>Within the above link provided by the error it states that hdfs is supported. Bug or am I missing something?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2020-07-10 20:21:30.347000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-07-10 21:00:46.223000 UTC",
        "Question_score":0,
        "Question_tags":"python|mlflow",
        "Question_view_count":621,
        "Owner_creation_date":"2014-06-23 16:02:53.873000 UTC",
        "Owner_last_access_date":"2022-09-23 14:18:04.920000 UTC",
        "Owner_location":"Atlanta, GA",
        "Owner_reputation":458,
        "Owner_up_votes":76,
        "Owner_down_votes":5,
        "Owner_views":119,
        "Answer_body":"<p>Ok. So it looks like while the ARTIFACTS STORE does support hdfs, you have to use either file or a sql like for the BACKEND STORE.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2020-07-13 17:24:30.470000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_valid_tags":[
            "mlflow"
        ]
    },
    {
        "Question_id":73085293,
        "Question_title":"Library is not installed on PATH - How can I install on path?",
        "Question_body":"<p>I am running this notebook in my managed notebooks environment on Google Cloud and I'm getting the following error when trying to install the packages: &quot;WARNING: The script google-oauthlib-tool is installed in '\/home\/jupyter\/.local\/bin' which is not on PATH.\nConsider adding this directory to PATH.&quot;<\/p>\n<p>Here is the python code that I'm trying to run for reference. <a href=\"https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/main\/notebooks\/official\/model_monitoring\/model_monitoring.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/main\/notebooks\/official\/model_monitoring\/model_monitoring.ipynb<\/a><\/p>\n<p>Any suggestions on how I can update the package installation so it is on path and resolve the error? I'm currently working on GCP user-managed notebooks on a Mac.<\/p>\n<p>Thanks so much for any tips!<\/p>\n<ul>\n<li>RE<\/li>\n<\/ul>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-07-22 19:34:07.040000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"python|google-cloud-platform|installation|package|google-cloud-vertex-ai",
        "Question_view_count":52,
        "Owner_creation_date":"2021-05-21 18:13:40.567000 UTC",
        "Owner_last_access_date":"2022-08-09 15:09:16.750000 UTC",
        "Owner_location":null,
        "Owner_reputation":29,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":16,
        "Answer_body":"<p>Open up your shell config file (likely .zshrc because the default shell on Mac is now zsh and that's the name of the zsh config file) located at your home directory in a text editor (TextEdit, etc) and add the path to the  executable.\nLike this:\nOpen the file:\n<code>open -e ~\/.zshrc<\/code>\nEdit the file:\nAdd this line at the top (may vary, check the documentation):\n<code>export PATH=&quot;\/home\/jupyter\/.local\/bin&quot;<\/code>\nThat may not work, try this:\n<code>export PATH=&quot;$PATH:\/home\/jupyter\/.local\/bin&quot;<\/code>\nYour best bet is to read the package documentation.<\/p>\n<p>After saving the config file, run <code>source ~\/.zshrc<\/code> and replace .zshrc with the config file name if it's different OR open a new terminal tab.<\/p>\n<p>What this does is tells the shell that the command exists and where to find it.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-07-23 17:21:01.407000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "google-cloud-vertex-ai"
        ]
    },
    {
        "Question_id":45582412,
        "Question_title":"How to build a Convolution Neural Net in Azure Machine Learning?",
        "Question_body":"<p>Someone should add \"net#\" as a tag. I'm trying to improve my neural network in Azure Machine Learning Studio by turning it into a convolution neural net using this tutorial:<\/p>\n\n<p><a href=\"https:\/\/gallery.cortanaintelligence.com\/Experiment\/Neural-Network-Convolution-and-pooling-deep-net-2\" rel=\"noreferrer\">https:\/\/gallery.cortanaintelligence.com\/Experiment\/Neural-Network-Convolution-and-pooling-deep-net-2<\/a><\/p>\n\n<p>The differences between mine and the tutorial is I'm doing regression with 35 features and 1 label and they're doing classification with 28x28 features and 10 labels. <\/p>\n\n<p>I start with the basic and 2nd example and get them working with:<\/p>\n\n<pre><code>input Data [35];\n\nhidden H1 [100]\n    from Data all;\n\nhidden H2 [100]\n    from H1 all;\n\noutput Result [1] linear\n    from H2 all;\n<\/code><\/pre>\n\n<p>Now the transformation to convolution I misunderstand. In the tutorial and documentation here: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/machine-learning-azure-ml-netsharp-reference-guide\" rel=\"noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/machine-learning-azure-ml-netsharp-reference-guide<\/a> it doesn't mention how the node tuple values are calculated for the hidden layers. The tutorial says:<\/p>\n\n<pre><code>hidden C1 [5, 12, 12]\n  from Picture convolve {\n    InputShape  = [28, 28];\n    KernelShape = [ 5,  5];\n    Stride      = [ 2,  2];\n    MapCount = 5;\n  }\n\nhidden C2 [50, 4, 4]\n   from C1 convolve {\n     InputShape  = [ 5, 12, 12];\n     KernelShape = [ 1,  5,  5];\n     Stride      = [ 1,  2,  2];\n     Sharing     = [ F,  T,  T];\n     MapCount = 10;\n  }\n<\/code><\/pre>\n\n<p>Seems like the [5, 12, 12] and [50,4,4] pop out of no where along with the KernalShape, Stride, and MapCount. How do I know what values are valid for my example? I tried using the same values, but it didn't work and I have a feeling since he has a [28,28] input and I have a [35], I need tuples with 2 integers not 3. <\/p>\n\n<p>I just tried with random values that seem to correlate with the tutorial:<\/p>\n\n<pre><code>const { T = true; F = false; }\n\ninput Data [35];\n\nhidden C1 [7, 23]\n  from Data convolve {\n    InputShape  = [35];\n    KernelShape = [7];\n    Stride      = [2];\n    MapCount = 7;\n  }\n\nhidden C2 [200, 6]\n   from C1 convolve {\n     InputShape  = [ 7, 23];\n     KernelShape = [ 1,  7];\n     Stride      = [ 1,  2];\n     Sharing     = [ F,  T];\n     MapCount = 14;\n  }\n\nhidden H3 [100]\n  from C2 all;\n\noutput Result [1] linear\n  from H3 all;\n<\/code><\/pre>\n\n<p>Right now it seems impossible to debug because the only error code Azure Machine Learning Studio ever gives is:<\/p>\n\n<pre><code>Exception\":{\"ErrorId\":\"LibraryException\",\"ErrorCode\":\"1000\",\"ExceptionType\":\"ModuleException\",\"Message\":\"Error 1000: TLC library exception: Exception of type 'Microsoft.Numerics.AFxLibraryException' was thrown.\",\"Exception\":{\"Library\":\"TLC\",\"ExceptionType\":\"LibraryException\",\"Message\":\"Exception of type 'Microsoft.Numerics.AFxLibraryException' was thrown.\"}}}Error: Error 1000: TLC library exception: Exception of type 'Microsoft.Numerics.AFxLibraryException' was thrown. Process exited with error code -2\n<\/code><\/pre>\n\n<p>Lastly my setup is <a href=\"https:\/\/i.stack.imgur.com\/PBN9L.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/PBN9L.png\" alt=\"Azure Machine Learning Setup\"><\/a> <\/p>\n\n<p>Thanks for the help!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2017-08-09 05:37:28.253000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2017-08-14 04:53:38.057000 UTC",
        "Question_score":9,
        "Question_tags":"machine-learning|convolution|azure-machine-learning-studio|net#",
        "Question_view_count":1268,
        "Owner_creation_date":"2014-12-13 21:25:26.277000 UTC",
        "Owner_last_access_date":"2022-09-24 17:32:50.893000 UTC",
        "Owner_location":"Missouri",
        "Owner_reputation":1454,
        "Owner_up_votes":117,
        "Owner_down_votes":15,
        "Owner_views":328,
        "Answer_body":"<p>The correct network definition for 35-column length input with given kernels and strides would be following:<\/p>\n\n<pre><code>const { T = true; F = false; }\n\ninput Data [35];\n\nhidden C1 [7, 15]\n  from Data convolve {\n    InputShape  = [35];\n    KernelShape = [7];\n    Stride      = [2];\n    MapCount = 7;\n  }\n\nhidden C2 [14, 7, 5]\n   from C1 convolve {\n     InputShape  = [ 7, 15];\n     KernelShape = [ 1,  7];\n     Stride      = [ 1,  2];\n     Sharing     = [ F,  T];\n     MapCount = 14;\n  }\n\nhidden H3 [100]\n  from C2 all;\n\noutput Result [1] linear\n  from H3 all;\n<\/code><\/pre>\n\n<p>First, the C1 = [7,15]. The first dimension is simply the MapCount. For the second dimension, the kernel shape defines the length of the \"window\" that's used to scan the input columns, and the stride defines how much it moves at each step. So the kernel windows would cover columns 1-7, 3-9, 5-11,...,29-35, yielding the second dimension of 15 when you tally the windows.<\/p>\n\n<p>Next, the C2 = [14,7,5]. The first dimension is again the MapCount. For the second and third dimension, the 1-by-7 kernel \"window\" has to cover the input size of 7-by-15, using steps of 1 and 2 along corresponding dimensions. <\/p>\n\n<p>Note that you could specify C2 hidden layer shape of [98,5] or even [490], if you wanted to flatten the outputs. <\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2017-08-21 19:04:16.370000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":72376872,
        "Question_title":"entry_point file using XGBoost as a framework in sagemaker",
        "Question_body":"<p>Looking at the following source code taken from <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost.html\" rel=\"nofollow noreferrer\">here<\/a> (SDK v2):<\/p>\n<pre><code>import boto3\nimport sagemaker\nfrom sagemaker.xgboost.estimator import XGBoost\nfrom sagemaker.session import Session\nfrom sagemaker.inputs import TrainingInput\n\n# initialize hyperparameters\nhyperparameters = {\n        &quot;max_depth&quot;:&quot;5&quot;,\n        &quot;eta&quot;:&quot;0.2&quot;,\n        &quot;gamma&quot;:&quot;4&quot;,\n        &quot;min_child_weight&quot;:&quot;6&quot;,\n        &quot;subsample&quot;:&quot;0.7&quot;,\n        &quot;verbosity&quot;:&quot;1&quot;,\n        &quot;objective&quot;:&quot;reg:linear&quot;,\n        &quot;num_round&quot;:&quot;50&quot;}\n\n# set an output path where the trained model will be saved\nbucket = sagemaker.Session().default_bucket()\nprefix = 'DEMO-xgboost-as-a-framework'\noutput_path = 's3:\/\/{}\/{}\/{}\/output'.format(bucket, prefix, 'abalone-xgb-framework')\n\n# construct a SageMaker XGBoost estimator\n# specify the entry_point to your xgboost training script\nestimator = XGBoost(entry_point = &quot;your_xgboost_abalone_script.py&quot;, \n                    framework_version='1.2-2',\n                    hyperparameters=hyperparameters,\n                    role=sagemaker.get_execution_role(),\n                    instance_count=1,\n                    instance_type='ml.m5.2xlarge',\n                    output_path=output_path)\n\n# define the data type and paths to the training and validation datasets\ncontent_type = &quot;libsvm&quot;\ntrain_input = TrainingInput(&quot;s3:\/\/{}\/{}\/{}\/&quot;.format(bucket, prefix, 'train'), content_type=content_type)\nvalidation_input = TrainingInput(&quot;s3:\/\/{}\/{}\/{}\/&quot;.format(bucket, prefix, 'validation'), content_type=content_type)\n\n# execute the XGBoost training job\nestimator.fit({'train': train_input, 'validation': validation_input})\n<\/code><\/pre>\n<p>I wonder where the your_xgboost_abalone_script.py file has to be placed please? So far I used XGBoost as a built-in algorithm from my local machine with similar code (i.e. I span up a training job remotely). Thanks!<\/p>\n<p>PS:<\/p>\n<p>Looking at <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/estimators.html\" rel=\"nofollow noreferrer\">this<\/a>, and source_dir, I wonder if one can upload Python files to S3. In this case, I take it is has to be tar.gz? Thanks!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-05-25 11:34:26.700000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-05-25 12:39:54.637000 UTC",
        "Question_score":0,
        "Question_tags":"python|xgboost|amazon-sagemaker",
        "Question_view_count":74,
        "Owner_creation_date":"2010-03-01 10:53:04.443000 UTC",
        "Owner_last_access_date":"2022-09-24 18:56:19.313000 UTC",
        "Owner_location":"Somewhere",
        "Owner_reputation":15705,
        "Owner_up_votes":2171,
        "Owner_down_votes":91,
        "Owner_views":2150,
        "Answer_body":"<p><code>your_xgboost_abalone_script.py<\/code> can be created locally. The path you provide is relative to where the code is running.<\/p>\n<p>I.e. <code>your_xgboost_abalone_script.py<\/code> can be located in the same directory where you are running the SageMaker SDK (&quot;source code&quot;).<\/p>\n<p>For example if you have <code>your_xgboost_abalone_script.py<\/code> in the same directory as the source code:<\/p>\n<pre><code>.\n\u251c\u2500\u2500 source_code.py\n\u2514\u2500\u2500 your_xgboost_abalone_script.py\n<\/code><\/pre>\n<p>Then you can point to this file exactly how the documentation depicts:<\/p>\n<pre><code>estimator = XGBoost(entry_point = &quot;your_xgboost_abalone_script.py&quot;, \n.\n.\n.\n)\n<\/code><\/pre>\n<p>The SDK will take <code>your_xgboost_abalone_script.py<\/code> repackage it into a model tar ball and upload it to S3 on your behalf.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-06-08 19:01:50.813000 UTC",
        "Answer_last_edit_date":"2022-06-09 17:44:18.207000 UTC",
        "Answer_score":1.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":59150100,
        "Question_title":"Sagemaker and Tensorflow model not saved",
        "Question_body":"<p>I am learning Sagemaker and I have this entry point:<\/p>\n\n<pre><code>import os\nimport tensorflow as tf\nfrom tensorflow.python.estimator.model_fn import ModeKeys as Modes\n\nINPUT_TENSOR_NAME = 'inputs'\nSIGNATURE_NAME = 'predictions'\n\nLEARNING_RATE = 0.001\n\n\ndef model_fn(features, labels, mode, params):\n    # Input Layer\n    input_layer = tf.reshape(features[INPUT_TENSOR_NAME], [-1, 28, 28, 1])\n\n    # Convolutional Layer #1\n    conv1 = tf.layers.conv2d(\n        inputs=input_layer,\n        filters=32,\n        kernel_size=[5, 5],\n        padding='same',\n        activation=tf.nn.relu)\n\n    # Pooling Layer #1\n    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n\n    # Convolutional Layer #2 and Pooling Layer #2\n    conv2 = tf.layers.conv2d(\n        inputs=pool1,\n        filters=64,\n        kernel_size=[5, 5],\n        padding='same',\n        activation=tf.nn.relu)\n    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n\n    # Dense Layer\n    pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\n    dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n    dropout = tf.layers.dropout(\n        inputs=dense, rate=0.4, training=(mode == Modes.TRAIN))\n\n    # Logits Layer\n    logits = tf.layers.dense(inputs=dropout, units=10)\n\n    # Define operations\n    if mode in (Modes.PREDICT, Modes.EVAL):\n        predicted_indices = tf.argmax(input=logits, axis=1)\n        probabilities = tf.nn.softmax(logits, name='softmax_tensor')\n\n    if mode in (Modes.TRAIN, Modes.EVAL):\n        global_step = tf.train.get_or_create_global_step()\n        label_indices = tf.cast(labels, tf.int32)\n        loss = tf.losses.softmax_cross_entropy(\n            onehot_labels=tf.one_hot(label_indices, depth=10), logits=logits)\n        tf.summary.scalar('OptimizeLoss', loss)\n\n    if mode == Modes.PREDICT:\n        predictions = {\n            'classes': predicted_indices,\n            'probabilities': probabilities\n        }\n        export_outputs = {\n            SIGNATURE_NAME: tf.estimator.export.PredictOutput(predictions)\n        }\n        return tf.estimator.EstimatorSpec(\n            mode, predictions=predictions, export_outputs=export_outputs)\n\n    if mode == Modes.TRAIN:\n        optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n        train_op = optimizer.minimize(loss, global_step=global_step)\n        return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n\n    if mode == Modes.EVAL:\n        eval_metric_ops = {\n            'accuracy': tf.metrics.accuracy(label_indices, predicted_indices)\n        }\n        return tf.estimator.EstimatorSpec(\n            mode, loss=loss, eval_metric_ops=eval_metric_ops)\n\n\ndef serving_input_fn(params):\n    inputs = {INPUT_TENSOR_NAME: tf.placeholder(tf.float32, [None, 784])}\n    return tf.estimator.export.ServingInputReceiver(inputs, inputs)\n\n\ndef read_and_decode(filename_queue):\n    reader = tf.TFRecordReader()\n    _, serialized_example = reader.read(filename_queue)\n\n    features = tf.parse_single_example(\n        serialized_example,\n        features={\n            'image_raw': tf.FixedLenFeature([], tf.string),\n            'label': tf.FixedLenFeature([], tf.int64),\n        })\n\n    image = tf.decode_raw(features['image_raw'], tf.uint8)\n    image.set_shape([784])\n    image = tf.cast(image, tf.float32) * (1. \/ 255)\n    label = tf.cast(features['label'], tf.int32)\n\n    return image, label\n\n\ndef train_input_fn(training_dir, params):\n    return _input_fn(training_dir, 'train.tfrecords', batch_size=100)\n\n\ndef eval_input_fn(training_dir, params):\n    return _input_fn(training_dir, 'test.tfrecords', batch_size=100)\n\n\ndef _input_fn(training_dir, training_filename, batch_size=100):\n    test_file = os.path.join(training_dir, training_filename)\n    filename_queue = tf.train.string_input_producer([test_file])\n\n    image, label = read_and_decode(filename_queue)\n    images, labels = tf.train.batch(\n        [image, label], batch_size=batch_size,\n        capacity=1000 + 3 * batch_size)\n\n    return {INPUT_TENSOR_NAME: images}, labels\n\ndef neo_preprocess(payload, content_type):\n    import logging\n    import numpy as np\n    import io\n\n    logging.info('Invoking user-defined pre-processing function')\n\n    if content_type != 'application\/x-image' and content_type != 'application\/vnd+python.numpy+binary':\n        raise RuntimeError('Content type must be application\/x-image or application\/vnd+python.numpy+binary')\n\n    f = io.BytesIO(payload)\n    image = np.load(f)*255\n\n    return image\n\n### NOTE: this function cannot use MXNet\ndef neo_postprocess(result):\n    import logging\n    import numpy as np\n    import json\n\n    logging.info('Invoking user-defined post-processing function')\n\n    # Softmax (assumes batch size 1)\n    result = np.squeeze(result)\n    result_exp = np.exp(result - np.max(result))\n    result = result_exp \/ np.sum(result_exp)\n\n    response_body = json.dumps(result.tolist())\n    content_type = 'application\/json'\n\n    return response_body, content_type\n<\/code><\/pre>\n\n<p>And I am training it <\/p>\n\n<pre><code>estimator = TensorFlow(entry_point='cnn_fashion_mnist.py',\n                       role=role,\n                       input_mode='Pipe',\n                       training_steps=1, \n                       evaluation_steps=1,\n                       train_instance_count=1,\n                       output_path=output_path,\n                       train_instance_type='ml.c5.2xlarge',\n                       base_job_name='mnist')\n<\/code><\/pre>\n\n<p>so far it is trying correctly and it tells me that everything when well, but when I check the output there is nothing there or if I try to deploy it I get the error saying it couldn't find the model because there is nothing in the bucker, any ideas or extra configurations? Thank you<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2019-12-03 04:14:10.973000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"tensorflow|amazon-sagemaker",
        "Question_view_count":658,
        "Owner_creation_date":"2016-05-09 05:03:09.773000 UTC",
        "Owner_last_access_date":"2021-07-16 12:54:52.163000 UTC",
        "Owner_location":null,
        "Owner_reputation":45,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":15,
        "Answer_body":"<p>Looks like you are using one of the older Tensorflow versions.\nWe would recommend switching to a newer more straight-forward way of running Tensorflow in SageMaker (script mode) by switching to a more recent Tensorflow version.<\/p>\n\n<p>You can read more about it in our documentation:\n<a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_tf.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_tf.html<\/a><\/p>\n\n<p>Here is an example that might help:\n<a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_script_mode_training_and_serving\/tensorflow_script_mode_training_and_serving.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_script_mode_training_and_serving\/tensorflow_script_mode_training_and_serving.ipynb<\/a> <\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2019-12-17 23:52:25.670000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":64653042,
        "Question_title":"Control tracked version of external dependency",
        "Question_body":"<p>I am trying to set up a DVC repository for machine learning data with different tagged versions of the dataset. I do this with something like:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>$ cd \/raid\/ml_data  # folder on a data drive\n$ git init\n$ dvc init\n$ [add data]\n$ [commit to dvc, git]\n$ git tag -a 1.0.0\n$ [add or change data]\n$ [commit to dvc, git]\n$ git tag -a 1.1.0\n<\/code><\/pre>\n<p>I have multiple projects that each need to reference some version of this dataset. The problem is I can't figure out how to set up those projects to reference a specific version. I'm able to track the <code>HEAD<\/code> of the repo with something like:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>$ cd ~\/my_proj  # different drive than the remote\n$ mkdir data\n$ git init\n$ dvc init\n$ dvc remote add -d local \/raid\/ml_data  # add the remote on my data drive\n$ dvc cache dir \/raid\/ml_data\/.dvc\/cache  # tell DVC to use the remote cache\n$ dvc checkout\n$ dvc run --external -d \/raid\/ml_data -o data\/ cp -r \/raid\/ml_data data\n<\/code><\/pre>\n<p>This gets me the latest version of the dataset, symlinked into my <code>data<\/code> folder, but what if I want some projects to use the <code>1.0.0<\/code> version and some to use the <code>1.1.0<\/code> version, or another version? Or for that matter, if I update the dataset to <code>2.0.0<\/code> but don't want my existing projects to necessarily track <code>HEAD<\/code> and instead keep the version with which they were set up?<\/p>\n<p>It's important to me to not create a ton of local copies of my dataset as the <code>\/home<\/code> drive is much smaller than the <code>\/raid<\/code> drive and some of these datasets are huge.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-11-02 20:42:34.297000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-11-02 23:50:23.337000 UTC",
        "Question_score":2,
        "Question_tags":"version-control|dvc",
        "Question_view_count":139,
        "Owner_creation_date":"2013-06-07 18:26:33.700000 UTC",
        "Owner_last_access_date":"2022-09-14 18:00:36.650000 UTC",
        "Owner_location":"Colorado Springs, CO",
        "Owner_reputation":11685,
        "Owner_up_votes":2855,
        "Owner_down_votes":47,
        "Owner_views":1329,
        "Answer_body":"<p>I think you are looking for the <a href=\"https:\/\/dvc.org\/doc\/start\/data-access\" rel=\"nofollow noreferrer\">data access<\/a> set of commands.<\/p>\n<p>In your particular case, <code>dvc import<\/code> makes sense:<\/p>\n<pre><code>$ dvc import \/raid\/ml_data data\n<\/code><\/pre>\n<p>if you want to get the most recent version (HEAD). Then you will be able to update it with the <code>dvc update<\/code> command (if 2.0.0 is released, for example).<\/p>\n<pre><code>$ dvc import \/raid\/ml_data data --rev 1.0.0\n<\/code><\/pre>\n<p>if you'd like to &quot;fix&quot; it to the specific version.<\/p>\n<h3>Avoiding copies<\/h3>\n<p>Make sure also, that <code>symlinks<\/code> are set for the second project, as described in the <a href=\"https:\/\/dvc.org\/doc\/user-guide\/large-dataset-optimization\" rel=\"nofollow noreferrer\">Large Dataset Optimization<\/a>:<\/p>\n<pre><code>$ dvc config cache.type reflink,hardlink,symlink,copy\n<\/code><\/pre>\n<p>(there are config modifiers <code>--global<\/code>, <code>--local<\/code>, <code>--system<\/code> to set this setting for everyone at once, or just for one project, etc)<\/p>\n<p>Check the details instruction <a href=\"https:\/\/dvc.org\/doc\/user-guide\/large-dataset-optimization#configuring-dvc-cache-file-link-type\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n<hr \/>\n<p>Overall, it's a great setup, and looks like you got pretty much everything right. Please, don't hesitate to follow up and\/or create other questions here- we'll help you with this.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2020-11-02 21:12:41.433000 UTC",
        "Answer_last_edit_date":"2020-11-03 00:16:03.343000 UTC",
        "Answer_score":1.0,
        "Question_valid_tags":[
            "dvc"
        ]
    },
    {
        "Question_id":58040933,
        "Question_title":"Error in connecting Azure SQL database from Azure Machine Learning Service using python",
        "Question_body":"<p>I am trying to connect <strong>Azure SQL Database<\/strong> from <strong>Azure Machine Learning service<\/strong>, but I got the below error.<\/p>\n\n<p><strong>Please check Error: -<\/strong><\/p>\n\n<pre><code>**('IM002', '[IM002] [unixODBC][Driver Manager]Data source name not found and no default driver specified (0) (SQLDriverConnect)')**\n<\/code><\/pre>\n\n<p>Please Check the below code that I have used for database connection: -<\/p>\n\n<pre><code>import pyodbc\n\nclass DbConnect:\n    # This class is used for azure database connection using pyodbc\n    def __init__(self):\n        try:\n            self.sql_db = pyodbc.connect(SERVER=&lt;servername&gt;;PORT=1433;DATABASE=&lt;databasename&gt;;UID=&lt;username&gt;;PWD=&lt;password&gt;')\n\n            get_name_query = \"select name from contacts\"\n            names = self.sql_db.execute(get_name_query)\n            for name in names:\n                print(name)\n\n        except Exception as e:\n            print(\"Error in azure sql server database connection : \", e)\n            sys.exit()\n\nif __name__ == \"__main__\":\n    class_obj = DbConnect()\n<\/code><\/pre>\n\n<p>Is there any way to solve the above error? Please let me know if there is any way.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2019-09-21 13:49:49.150000 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"python|sql|azure|azure-sql-database|azure-machine-learning-service",
        "Question_view_count":1013,
        "Owner_creation_date":"2019-04-05 12:07:30.937000 UTC",
        "Owner_last_access_date":"2020-01-07 14:57:03.530000 UTC",
        "Owner_location":null,
        "Owner_reputation":219,
        "Owner_up_votes":9,
        "Owner_down_votes":0,
        "Owner_views":35,
        "Answer_body":"<p>I'd consider using <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-dataprep\/azureml.dataprep?view=azure-dataprep-py\" rel=\"nofollow noreferrer\"><code>azureml.dataprep<\/code><\/a> over pyodbc for this task (the API may change, but this worked last time I tried):<\/p>\n\n<pre><code>import azureml.dataprep as dprep\n\nds = dprep.MSSQLDataSource(server_name=&lt;server-name,port&gt;,\n                           database_name=&lt;database-name&gt;,\n                           user_name=&lt;username&gt;,\n                           password=&lt;password&gt;)\n<\/code><\/pre>\n\n<p>You should then be able to collect the result of an SQL query in pandas e.g. via<\/p>\n\n<pre><code>dataflow = dprep.read_sql(ds, \"SELECT top 100 * FROM [dbo].[MYTABLE]\")\ndataflow.to_pandas_dataframe()\n<\/code><\/pre>",
        "Answer_comment_count":3.0,
        "Answer_creation_date":"2019-09-21 23:35:21.710000 UTC",
        "Answer_last_edit_date":"2019-09-22 10:05:09.727000 UTC",
        "Answer_score":1.0,
        "Question_valid_tags":[
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":72399408,
        "Question_title":"How to get list of compute instance size under Azure Machine Learning and Azure Databricks?",
        "Question_body":"<p>Goal here is to query a list of frequently used compute instance size under Azure Machine Learning and Azure Databricks using Azure Resource Graph Explorer from Azure Portal using Kusto query. From the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/governance\/resource-graph\/reference\/supported-tables-resources#resources\" rel=\"nofollow noreferrer\">documentation<\/a> here, there is a list of resources can be queried but there isn't any compute under <code>microsoft.machinelearningservices\/<\/code>(not classic studio) and <code>Microsoft.Databricks\/workspaces<\/code>.<\/p>\n<p>Below is what was tried, to get VM instance size but not showing what we have under Azure Machine Learning\/Azure Databricks.<\/p>\n<pre><code>Resources\n| project name, location, type, vmSize=tostring(properties.hardwareProfile.vmSize)\n| where type =~ 'Microsoft.Compute\/virtualMachines'\n| order by name desc\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-05-27 00:55:49.060000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"azure|size|azure-databricks|azure-machine-learning-service|azure-resource-graph",
        "Question_view_count":153,
        "Owner_creation_date":"2019-09-11 07:07:53.007000 UTC",
        "Owner_last_access_date":"2022-09-15 16:01:06.153000 UTC",
        "Owner_location":"Kuala Lumpur, Federal Territory of Kuala Lumpur, Malaysia",
        "Owner_reputation":383,
        "Owner_up_votes":70,
        "Owner_down_votes":1,
        "Owner_views":35,
        "Answer_body":"<blockquote>\n<p>Unfortunately, Azure Resource Graph Explorer doesn't provide any query\nto get any compute related information from both, Azure Machine\nLearning and Databricks.<\/p>\n<\/blockquote>\n<p>Though Azure Resource Graph Explorer supports join functionality, allowing for more advanced exploration of your Azure environment by enabling you to correlate between resources and their properties. But these services only applicable on few Azure resources like VM, storage account, Cosmos DB, SQL databases, Network Security Groups, public IP addresses, etc.<\/p>\n<p><strong>Hence, there is no such Kusto query available in Azure Resource Graph Explorer which can list compute instance size of Machine Learning service and Databricks.<\/strong><\/p>\n<p><strong>Workarounds<\/strong><\/p>\n<p>Machine Learning Service<\/p>\n<p>For machine learning service you can manage the compute instance directly from ML service by using Python SDK. Refer <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-manage-compute-instance?tabs=python#manage\" rel=\"nofollow noreferrer\">Python SDK azureml v1<\/a> to know more.<\/p>\n<p>Azure Databricks<\/p>\n<p>Cluster is the computational resource in Databricks. You can <strong>filter the cluster list<\/strong> from Databricks UI and manage the same. Features like cluster configuration, cluster cloning, access control, etc. are available which you can used based on your requirement. For more details, please check <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/databricks\/clusters\/clusters-manage#filter-cluster-list\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-05-27 04:42:21.543000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":72994988,
        "Question_title":"How to mlflow-autolog a sklearn ConfusionMatrixDisplay?",
        "Question_body":"<p>I'm trying to log the plot of a <a href=\"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.ConfusionMatrixDisplay.html#sklearn.metrics.ConfusionMatrixDisplay.from_estimator\" rel=\"nofollow noreferrer\">confusion matrix generated with scikit-learn<\/a> for a <em>test<\/em> set using <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.sklearn.html\" rel=\"nofollow noreferrer\">mlflow's support for scikit-learn<\/a>.<\/p>\n<p>For this, I tried something that resemble the code below (I'm using mlflow hosted on Databricks, and <code>sklearn==1.0.1<\/code>)<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import sklearn.datasets\nimport pandas as pd\nimport numpy as np\nimport mlflow\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\n\nmlflow.set_tracking_uri(&quot;databricks&quot;)\nmlflow.set_experiment(&quot;\/Users\/name.surname\/plotcm&quot;)\n\ndata = sklearn.datasets.fetch_20newsgroups(categories=['alt.atheism', 'sci.space'])\n\ndf = pd.DataFrame(data = np.c_[data['data'], data['target']])\\\n       .rename({0:'text', 1:'class'}, axis = 'columns')\n\ntrain, test = train_test_split(df)\n\nmy_pipeline = Pipeline([\n    ('vectorizer', TfidfVectorizer()),\n    ('classifier', SGDClassifier(loss='modified_huber')),\n])\n\nmlflow.sklearn.autolog()\n\nfrom sklearn.metrics import ConfusionMatrixDisplay # should I import this after the call to `.autolog()`?\n\nmy_pipeline.fit(train['text'].values, train['class'].values)\n\ncm = ConfusionMatrixDisplay.from_predictions(\n      y_true=test[&quot;class&quot;], y_pred=my_pipeline.predict(test[&quot;text&quot;])\n  )\n<\/code><\/pre>\n<p>while the confusion matrix for the training set is saved in my mlflow run, no png file is created in the mlflow frontend for the <code>test<\/code> set.<\/p>\n<p>If I try to add<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>cm.figure_.savefig('test_confusion_matrix.png')\nmlflow.log_artifact('test_confusion_matrix.png')\n<\/code><\/pre>\n<p>that does the job, but requires explicitly logging the artifact.<\/p>\n<p>Is there an idiomatic\/proper way to autolog the confusion matrix computed using a test set after <code>my_pipeline.fit()<\/code>?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-07-15 13:44:41.357000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-07-17 18:51:20.967000 UTC",
        "Question_score":0,
        "Question_tags":"python|scikit-learn|confusion-matrix|mlflow",
        "Question_view_count":157,
        "Owner_creation_date":"2014-11-11 16:17:30.717000 UTC",
        "Owner_last_access_date":"2022-09-24 20:31:18.173000 UTC",
        "Owner_location":"Verona, VR, Italy",
        "Owner_reputation":4811,
        "Owner_up_votes":376,
        "Owner_down_votes":73,
        "Owner_views":713,
        "Answer_body":"<p>The proper way to do this is to use <code>mlflow.log_figure<\/code> as a fluent API announced in <code>MLflow 1.13.0<\/code>. You can read the documentation <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.log_figure\" rel=\"nofollow noreferrer\">here<\/a>. This code will do the job.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>mlflow.log_figure(cm.figure_, 'test_confusion_matrix.png')\n<\/code><\/pre>\n<p>This function implicitly store the image, and then calls <code>log_artifact<\/code> against that path, something like you did.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-07-20 08:15:34.100000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "mlflow"
        ]
    },
    {
        "Question_id":73279794,
        "Question_title":"[Catboost][ClearML] Error: if loss-function is Logloss, then class weights should be given for 0 and 1 classes",
        "Question_body":"<p>Having recently started using ClearML to manage the MLOps, I am facing the following problem:\nWhen running a script that trains a CatBoost in a binary classification problem using different class weights from my computer, it works perfectly, logs the results and no issues at all.\nOnce I try to run that remotely using the ClearML agent, it results in the following error:<\/p>\n<pre><code>&lt;!-- language: lang-none --&gt;\nTraceback (most recent call last):\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/lib\/python3.9\/site-packages\/clearml\/binding\/frameworks\/catboost_bind.py&quot;, line 102, in _fit\n    return original_fn(obj, *args, **kwargs)\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/lib\/python3.9\/site-packages\/catboost\/core.py&quot;, line 5007, in fit\n    self._fit(X, y, cat_features, text_features, embedding_features, None, sample_weight, None, None, None, None, baseline, use_best_model,\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/lib\/python3.9\/site-packages\/catboost\/core.py&quot;, line 2262, in _fit\n    train_params = self._prepare_train_params(\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/lib\/python3.9\/site-packages\/catboost\/core.py&quot;, line 2194, in _prepare_train_params\n    _check_train_params(params)\n  File &quot;_catboost.pyx&quot;, line 6032, in _catboost._check_train_params\n  File &quot;_catboost.pyx&quot;, line 6051, in _catboost._check_train_params\n**_catboost.CatBoostError: catboost\/private\/libs\/options\/catboost_options.cpp:607: if loss-function is Logloss, then class weights should be given for 0 and 1 classes\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):**\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/task_repository\/RecSys.git\/src\/cli\/model_training_remote.py&quot;, line 313, in &lt;module&gt;\n    rfs.run(\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/task_repository\/RecSys.git\/src\/cli\/model_training_remote.py&quot;, line 232, in run\n    model.fit(\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/lib\/python3.9\/site-packages\/clearml\/binding\/frameworks\/__init__.py&quot;, line 36, in _inner_patch\n    raise ex\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/lib\/python3.9\/site-packages\/clearml\/binding\/frameworks\/__init__.py&quot;, line 34, in _inner_patch\n    ret = patched_fn(original_fn, *args, **kwargs)\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/lib\/python3.9\/site-packages\/clearml\/binding\/frameworks\/catboost_bind.py&quot;, line 110, in _fit\n    return original_fn(obj, *args, **kwargs)\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/lib\/python3.9\/site-packages\/catboost\/core.py&quot;, line 5007, in fit\n    self._fit(X, y, cat_features, text_features, embedding_features, None, sample_weight, None, None, None, None, baseline, use_best_model,\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/lib\/python3.9\/site-packages\/catboost\/core.py&quot;, line 2262, in _fit\n    train_params = self._prepare_train_params(\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/lib\/python3.9\/site-packages\/catboost\/core.py&quot;, line 2194, in _prepare_train_params\n    _check_train_params(params)\n  File &quot;_catboost.pyx&quot;, line 6032, in _catboost._check_train_params\n  File &quot;_catboost.pyx&quot;, line 6051, in _catboost._check_train_params\n**_catboost.CatBoostError: catboost\/private\/libs\/options\/catboost_options.cpp:607: if loss-function is Logloss, then class weights should be given for 0 and 1 classes**\n\n<\/code><\/pre>\n<p>I do have the dictionary being connected:<\/p>\n<pre><code>    model_params = {\n        &quot;loss_function&quot;: &quot;Logloss&quot;,\n        &quot;eval_metric&quot;: &quot;AUC&quot;,\n        &quot;class_weights&quot;: {0: 1, 1: 60},\n        &quot;learning_rate&quot;: 0.1\n    }\n<\/code><\/pre>\n<p>registered in the ClearML task as<\/p>\n<pre><code>task.connect(model_params, 'model_params')\n<\/code><\/pre>\n<p>and used as parameters for the model in the following call:<\/p>\n<pre><code>model = CatBoostClassifier(**model_params)\n<\/code><\/pre>\n<p>When running it from the container in ClearML interactive mode, it also works fine.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-08-08 14:47:32.813000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"python|catboost|clearml",
        "Question_view_count":54,
        "Owner_creation_date":"2022-08-08 14:30:03.173000 UTC",
        "Owner_last_access_date":"2022-09-23 21:14:51.430000 UTC",
        "Owner_location":null,
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Answer_body":"<p>Disclaimer: I'm a team members of ClearML<\/p>\n<p>I think I understand the problem, basically I think the issue is:<\/p>\n<pre><code>task.connect(model_params, 'model_params')\n<\/code><\/pre>\n<p>Since this is a nested dict:<\/p>\n<pre><code>    model_params = {\n        &quot;loss_function&quot;: &quot;Logloss&quot;,\n        &quot;eval_metric&quot;: &quot;AUC&quot;,\n        &quot;class_weights&quot;: {0: 1, 1: 60},\n        &quot;learning_rate&quot;: 0.1\n    }\n<\/code><\/pre>\n<p>The class_weights is stored as a <code>String<\/code> key, but <code>catboost<\/code> expects <code>int<\/code> key, hence failing.\nOne option would be to remove the <code>task.connect(model_params, 'model_params')<\/code><\/p>\n<p>Another solution (until we fix it) would be to do:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>task.connect(model_params, 'model_params')\nmodel_params[&quot;class_weights&quot;] = {\n0: model_params[&quot;class_weights&quot;].get(&quot;0&quot;, model_params[&quot;class_weights&quot;].get(0))\n1: model_params[&quot;class_weights&quot;].get(&quot;1&quot;, model_params[&quot;class_weights&quot;].get(1))\n}\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-08-08 20:52:18.293000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_valid_tags":[
            "clearml"
        ]
    },
    {
        "Question_id":60801292,
        "Question_title":"View Neptune Graph Schema using Jupyter notebook",
        "Question_body":"<p>Is there a way to view the schema of a graph in a Neptune cluster using Jupyter Notebook? <\/p>\n\n<p>Like you would do a \"select * from tablename limit 10\" in an RDS using SQL, similarly is there a way to get a sense of the graph data through Jupyter Notebook?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_date":"2020-03-22 15:36:34.693000 UTC",
        "Question_favorite_count":2.0,
        "Question_last_edit_date":"2021-11-23 23:00:19.890000 UTC",
        "Question_score":2,
        "Question_tags":"gremlin|amazon-sagemaker|amazon-neptune|gremlinpython|graph-notebook",
        "Question_view_count":831,
        "Owner_creation_date":"2020-03-22 15:31:06.787000 UTC",
        "Owner_last_access_date":"2022-09-17 07:01:39.723000 UTC",
        "Owner_location":null,
        "Owner_reputation":99,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":12,
        "Answer_body":"<p>It depends on how large your graph is as to how well this will perform but you can get a sense of the type of nodes and edges you have using something like the example below. From the tags you used I assume you are using Gremlin:<\/p>\n\n<pre><code>g.V().groupCount().by(label)\ng.E().groupCount().by(label)\n<\/code><\/pre>\n\n<p>If you have a very large graph try putting something like <code>limit(100000)<\/code> before the <code>groupCount<\/code> step.<\/p>\n\n<p>If you are using a programming language like Python (with gremlin python installed) then you will need to add a <code>next()<\/code> terminal step to the queries as in:<\/p>\n\n<pre><code>g.V().groupCount().by(label).next()\ng.E().groupCount().by(label).next()\n<\/code><\/pre>\n\n<p>Having found the labels and distribution of the labels you could use one of them to explore some properties. Let's imagine there is a label called \"person\".<\/p>\n\n<pre><code>g.V().hasLabel('person').limit(10).valueMap().toList()\n<\/code><\/pre>\n\n<p>Remember with Gremlin property graphs vertices with the same label may not necessarily have all the same properties so it's good to look at more than one vertex to get a sense for that as well.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-03-22 18:40:12.783000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":4.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":73471486,
        "Question_title":"How to prevent storing data in Jupyter project tree when writing data from Sagemaker to S3",
        "Question_body":"<p>I am new to AWS Sagemaker and I wrote data to my S3 bucket.\nBut these datasets also appear in the working tree of my jupyter instance.<\/p>\n<p>How can I move data directly to S3 without saving it &quot;locally&quot;?<\/p>\n<p>My code:<\/p>\n<pre><code>import os\nimport pandas as pd\n\nimport sagemaker, boto3\nfrom sagemaker import get_execution_role\nfrom sagemaker.inputs import TrainingInput\nfrom sagemaker.serializers import CSVSerializer\n\n# please provide your own bucket and folder path of your bucket here\nbucket = &quot;test-bucket2342343&quot;\nsm_sess = sagemaker.Session(default_bucket=bucket)\nfile_path = &quot;Use Cases\/Sagemaker Demo\/xgboost&quot;\n\n# data \ndf_train = pd.DataFrame({'X':[0,100,200,400,450,  550,600,800,1600],\n                         'y':[0,0,  0,  0,  0,    1,  1,  1,  1]})\n\ndf_test = pd.DataFrame({'X':[10,90,240,459,120,  650,700,1800,1300],\n                        'y':[0,0,  0,  0,  0,    1,  1,  1,  1]})\n\n# move to S3 \ndf_train[['y','X']].to_csv('train.csv', header=False, index=False)\n\ndf_val = df_test.copy()\ndf_val[['y','X']].to_csv('val.csv', header=False, index=False)\n\nboto3.Session().resource(&quot;s3&quot;).Bucket(bucket) \\\n.Object(os.path.join(file_path, &quot;train.csv&quot;)).upload_file(&quot;train.csv&quot;)\n\nboto3.Session().resource(&quot;s3&quot;).Bucket(bucket) \\\n.Object(os.path.join(file_path, &quot;val.csv&quot;)).upload_file(&quot;val.csv&quot;)\n\n<\/code><\/pre>\n<p>It successfully appears in my S3 bucket.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/d1yCy.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/d1yCy.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>But it also appears here:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/RjGZr.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/RjGZr.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-08-24 10:24:43.943000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-08-24 10:33:21.250000 UTC",
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|amazon-s3|amazon-sagemaker",
        "Question_view_count":17,
        "Owner_creation_date":"2019-08-16 07:41:01.083000 UTC",
        "Owner_last_access_date":"2022-09-24 16:45:09.263000 UTC",
        "Owner_location":null,
        "Owner_reputation":188,
        "Owner_up_votes":134,
        "Owner_down_votes":4,
        "Owner_views":43,
        "Answer_body":"<p>with Pandas you can save to S3 directly (<a href=\"https:\/\/stackoverflow.com\/a\/56275519\/121956\">relevant answer<\/a>). For example:<\/p>\n<pre><code>import pandas as pd\ndf = pd.DataFrame( [ [1, 1, 1], [2, 2, 2] ], columns=['a', 'b', 'c'])\ndf.to_csv('s3:\/\/test-bucket2342343\/\/tmp.csv', index=False)\n<\/code><\/pre>\n<p>Or, use what you currently do and delete the local files:<\/p>\n<pre><code>import os\nos.remove('train.csv')\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-08-25 17:19:20.213000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":30068341,
        "Question_title":"How to write the results of Azure ML web service to the azure sql database (The output of Azure ML web service is in Json structure)",
        "Question_body":"<p>The results can be written to SQL Azure using the writer module in the experiment but after publishing the web service the output comes in the Json Structure and it doesn't go to the writer module <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2015-05-06 05:36:06.837000 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":"2015-05-08 06:16:55.883000 UTC",
        "Question_score":0,
        "Question_tags":"azure-sql-database|azure-scheduler|azure-machine-learning-studio",
        "Question_view_count":990,
        "Owner_creation_date":"2014-06-23 16:37:06.413000 UTC",
        "Owner_last_access_date":"2020-06-18 08:44:47.883000 UTC",
        "Owner_location":"Bengaluru, India",
        "Owner_reputation":191,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":22,
        "Answer_body":"<p>Don't set output port and use Batch execution service - details are provided here - <a href=\"http:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-publish-a-machine-learning-web-service\/\" rel=\"nofollow\">Publish web service<\/a> and <a href=\"http:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-consume-web-services\/\" rel=\"nofollow\">consume web service<\/a><\/p>",
        "Answer_comment_count":5.0,
        "Answer_creation_date":"2015-05-06 07:34:50.130000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":57386269,
        "Question_title":"How to use Azure ML Scoring URI?",
        "Question_body":"<p>I'm completely new to Azure ML, but I wanted to try out their automated ML UX. So I've followed the instructions to finally deploy my app (<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-create-portal-experiments#deploy-your-model\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-create-portal-experiments#deploy-your-model<\/a>). Now I've got my \"Scoring URI\", but I don't know how to use it? <strong>How can I test an input and get an output - can I do it with Postman?<\/strong><\/p>\n\n<ul>\n<li>the tutorial doesn't tell me what to do with this \"Scoring URI\", and so I am stuck<\/li>\n<\/ul>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-08-07 02:38:49.467000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2019-08-07 07:31:26.470000 UTC",
        "Question_score":0,
        "Question_tags":"azure|azure-machine-learning-service",
        "Question_view_count":826,
        "Owner_creation_date":"2018-05-21 00:50:14.910000 UTC",
        "Owner_last_access_date":"2020-09-11 03:56:16.180000 UTC",
        "Owner_location":null,
        "Owner_reputation":45,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":26,
        "Answer_body":"<p>On the bottom of the page that you have linked above, there is a link:<\/p>\n\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-consume-web-service\" rel=\"nofollow noreferrer\">Learn how to consume a web service.<\/a><\/p>\n\n<p>This is exactly on that topic on how to use the deployed web service for scoring (sending an input and getting an output).<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2019-08-07 07:31:15.267000 UTC",
        "Answer_last_edit_date":"2019-08-18 01:14:21.313000 UTC",
        "Answer_score":1.0,
        "Question_valid_tags":[
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":67123040,
        "Question_title":"How to tell programmatically that an AWS Step Function execution has been completed?",
        "Question_body":"<p>I am triggering a Step Function execution via a Python cell in a SageMaker Notebook, like this:<\/p>\n<pre><code>state_machine_arn = 'arn:aws:states:us-west-1:1234567891:stateMachine:alexanderMyPackageStateMachineE3411O13-A1vQWERTP9q9'\nsfn = boto3.client('stepfunctions')\n..\nsfn.start_execution(**kwargs)  # Non Blocking Call\nrun_arn = response['executionArn']\nprint(f&quot;Started run {run_name}. ARN is {run_arn}.&quot;)\n<\/code><\/pre>\n<p>and then in order to check that the execution (which might take hours to complete depending on the input) has been completed, before I start doing some custom post-analysis on the results, I manually execute a cell with:<\/p>\n<pre><code>response = sfn.list_executions(\n    stateMachineArn=state_machine_arn,\n    maxResults=1\n)\nprint(response)\n<\/code><\/pre>\n<p>where I can see from the output the status of the execution, e.g. <code>'status': 'RUNNING'<\/code>.<\/p>\n<p>How can I automate this, i.e. trigger the Step Function and continue the execution on my post-analysis custom logic only after the execution has finished? Is there for example a blocking call to start the execution, or a callback method I could use?<\/p>\n<p>I can think of putting a sleep method, so that the Python Notebook cell would periodically call <code>list_executions()<\/code> and check the status, and only when the execution is completed, continue to rest of the code. I can statistically determine the sleep period, but I was wondering if there is a simpler\/more accurate way.<\/p>\n<hr \/>\n<p>PS: Related: <a href=\"https:\/\/stackoverflow.com\/questions\/46878423\/how-to-avoid-simultaneous-execution-in-aws-step-function\">How to avoid simultaneous execution in aws step function<\/a>, however I would like to avoid creating any new AWS resource, just for this, I would like to do everything from within the Notebook.<\/p>\n<p>PPS: I cannot make any change to <code>MyPackage<\/code> and the Step Function definition.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2021-04-16 09:53:03.027000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-04-16 14:57:49.787000 UTC",
        "Question_score":4,
        "Question_tags":"python|amazon-web-services|asynchronous|amazon-sagemaker|aws-step-functions",
        "Question_view_count":1216,
        "Owner_creation_date":"2013-05-22 21:25:42.213000 UTC",
        "Owner_last_access_date":"2022-09-21 16:01:33.950000 UTC",
        "Owner_location":"London, UK",
        "Owner_reputation":70285,
        "Owner_up_votes":7595,
        "Owner_down_votes":12100,
        "Owner_views":13121,
        "Answer_body":"<p>Based on the comments.<\/p>\n<p>If no new resources are to be created (no CloudWatch Event rules, lambda functions) nor any changes to existing Step Function are allowed, then <strong>pooling iteratively<\/strong> <code>list_executions<\/code> would be the best solution.<\/p>\n<p>AWS CLI and boto3 have implemented similar solutions (not for Step Functions), but for some other services. They are called <code>waiters<\/code> (e.g. <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/ec2.html#waiters\" rel=\"nofollow noreferrer\">ec2 waiters<\/a>). So basically you would have to create your own <strong>waiter for Step Function<\/strong>, as AWS does not provide one for that. AWS uses <strong>15 seconds<\/strong> sleep time from what I recall for its waiters.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-04-16 10:18:25.233000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":61987233,
        "Question_title":"How to construct a \"text\/csv\" payload when invoking a sagemaker endpoint",
        "Question_body":"<p>My training data looks like <\/p>\n\n<pre><code>df = pd.DataFrame({'A' : [2, 5], 'B' : [1, 7]})\n<\/code><\/pre>\n\n<p>I have trained a model in AWS Sagemaker and I deployed the model behind an endpoint.\nThe endpoint accepts the payload as \"text\/csv\".<\/p>\n\n<p>to invoke the endpoint using boto3 you can do:<\/p>\n\n<pre><code>import boto3\nclient = boto3.client('sagemaker-runtime')\nresponse = client.invoke_endpoint(\n    EndpointName=\"my-sagemaker-endpoint-name\",\n    Body= my_payload_as_csv,\n    ContentType = 'text\/csv')\n<\/code><\/pre>\n\n<p>How do i construct the payload \"my_payload_as_csv\" from my Dataframe in order to invoke the Sagemaker Endpoint correctly?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2020-05-24 14:13:56.843000 UTC",
        "Question_favorite_count":2.0,
        "Question_last_edit_date":null,
        "Question_score":4,
        "Question_tags":"pandas|amazon-web-services|boto3|amazon-sagemaker",
        "Question_view_count":2627,
        "Owner_creation_date":"2012-10-24 12:12:59.277000 UTC",
        "Owner_last_access_date":"2022-09-25 05:49:23.950000 UTC",
        "Owner_location":"Leuven, Belgium",
        "Owner_reputation":3126,
        "Owner_up_votes":1817,
        "Owner_down_votes":2,
        "Owner_views":262,
        "Answer_body":"<p>if you start from the dataframe example<\/p>\n\n<pre><code>df = pd.DataFrame({'A' : [2, 5], 'B' : [1, 7]})\n<\/code><\/pre>\n\n<p>you take a row<\/p>\n\n<pre><code>df_1_record = df[:1]\n<\/code><\/pre>\n\n<p>and convert <code>df_1_record<\/code> to a csv like this:<\/p>\n\n<pre><code>import io\nfrom io import StringIO\ncsv_file = io.StringIO()\n# by default sagemaker expects comma seperated\ndf_1_record.to_csv(csv_file, sep=\",\", header=False, index=False)\nmy_payload_as_csv = csv_file.getvalue()\n<\/code><\/pre>\n\n<p><code>my_payload_as_csv<\/code> looks like<\/p>\n\n<pre><code>'2,1\\n'\n<\/code><\/pre>\n\n<p>then you can invoke the sagemaker endpoint<\/p>\n\n<pre><code>import boto3\nclient = boto3.client('sagemaker-runtime')\nresponse = client.invoke_endpoint(\n    EndpointName=\"my-sagemaker-endpoint-name\",\n    Body= my_payload_as_csv,\n    ContentType = 'text\/csv')\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2020-05-24 14:13:56.843000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":7.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":54992434,
        "Question_title":"MissingRequiredParameter: Missing required key 'FunctionName' in params",
        "Question_body":"<p>I'm working on a supervised machine learning problem, and I am setting up a custom labeling task to send out to Amazon Mechanical Turk for human annotation.<\/p>\n\n<p>I have uploaded the data to AWS S3 in the json-lines (<code>.jsonl<\/code>) format as follows, pursuant to the instructions as specified in the AWS documentation <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-data-input.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-data-input.html<\/a>: <\/p>\n\n<pre><code>{\"source\": \"value0\"}\n{\"source\": \"value1\"}    \n{\"source\": \"value2\"}\n...\n{\"source\": \"value2\"}\n<\/code><\/pre>\n\n<p>When I click on the default text classification template, I can see my data come through and everything appears to work.<\/p>\n\n<p>However, I am getting the following error when I attempt to use the custom annotation task template interface: <code>MissingRequiredParameter: Missing required key 'FunctionName' in params<\/code> <\/p>\n\n<p>The error resembles an AWS Lambda error, except the strange thing is that I am not using AWS Lambda. Suggestions for how to proceed? <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-03-04 22:17:36.450000 UTC",
        "Question_favorite_count":0.0,
        "Question_last_edit_date":null,
        "Question_score":3,
        "Question_tags":"amazon-web-services|mechanicalturk|amazon-sagemaker",
        "Question_view_count":7776,
        "Owner_creation_date":"2012-05-07 22:27:38.227000 UTC",
        "Owner_last_access_date":"2021-05-01 20:42:42.860000 UTC",
        "Owner_location":"San Francisco, CA, USA",
        "Owner_reputation":947,
        "Owner_up_votes":70,
        "Owner_down_votes":11,
        "Owner_views":61,
        "Answer_body":"<p>I am from the engineering team and happy to help you here. I think the issue is not related to the manifest as it looks correct to me. The error suggests that you may haven't provided a correct lambda ARN for pre or post labeling task. Please see this doc for more details: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-custom-templates-step3.html\" rel=\"noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-custom-templates-step3.html<\/a><\/p>\n\n<p>I can also help further if you can send me details on how you starting the job and what parameters you are sending.  <\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2019-03-11 22:11:47.057000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":5.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":58600732,
        "Question_title":"Is there a way to hide mlflow ui header when start the server with mlflow server?",
        "Question_body":"<p>I want to integrate mlflow ui to our website by using an iframe, but with the header hidden if possible. I found there is an environment variable setting in the source code \/mlflow\/server\/js\/components\/HomeView.js:\n<code>const headerHeight = process.env.HIDE_HEADER === 'true' ? 0 : 60;<\/code> But how can I specify this environment by running the server with <code>mlflow server<\/code>? I tried with <code>HIDE_HEADER=true mlflow server<\/code>, but this doesn't work. Or is there any other way to solve this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-10-29 02:30:04.850000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"node.js|reactjs|mlflow",
        "Question_view_count":296,
        "Owner_creation_date":"2015-09-01 23:50:47.410000 UTC",
        "Owner_last_access_date":"2022-09-23 03:26:58.597000 UTC",
        "Owner_location":null,
        "Owner_reputation":626,
        "Owner_up_votes":75,
        "Owner_down_votes":1,
        "Owner_views":79,
        "Answer_body":"<p>@Jason good question, those environment variables are read at build-time for the MLflow UI's Javascript assets. Since the PyPI MLflow wheel comes with pre-built Javascript assets, it's difficult to achieve your use case using a PyPI installation of <code>mlflow<\/code>.<\/p>\n\n<p>However, you can build a custom MLflow wheel from source with the UI header hidden by following the instructions <a href=\"https:\/\/github.com\/mlflow\/mlflow\/blob\/branch-1.3\/CONTRIBUTING.rst#building-a-distributable-artifact\" rel=\"nofollow noreferrer\">here<\/a>, replacing the <code>npm run build<\/code> step with <code>HIDE_HEADER=true npm run build<\/code> (basically, the idea is to set the desired environment variables prior to building Javascript assets via <code>npm run build<\/code>). You can then pip-install that wheel on the node hosting your MLflow server &amp; launch the server via <code>mlflow server<\/code>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2019-10-30 20:52:32.163000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_valid_tags":[
            "mlflow"
        ]
    },
    {
        "Question_id":68802388,
        "Question_title":"Why do I get an error that Sagemaker Endpoint does not have multiple models when it does?",
        "Question_body":"<p>Invoking a multimodel Sagemaker Endpoint, I get an error that it is not multimodel. I create it like this.<\/p>\n<pre><code>create_endpoint_config_response = client.create_endpoint_config(\n    EndpointConfigName=endpoint_config_name,\n    ProductionVariants=[\n        {\n            &quot;InstanceType&quot;: &quot;ml.m5.large&quot;,\n            &quot;InitialVariantWeight&quot;: 0.5,\n            &quot;InitialInstanceCount&quot;: 1,\n            &quot;ModelName&quot;: model_name1,\n            &quot;VariantName&quot;: model_name1,\n        },\n         {\n            &quot;InstanceType&quot;: &quot;ml.m5.large&quot;,\n            &quot;InitialVariantWeight&quot;: 0.5,\n            &quot;InitialInstanceCount&quot;: 1,\n            &quot;ModelName&quot;: model_name2,\n            &quot;VariantName&quot;: model_name2,\n        }\n    ]\n)\n<\/code><\/pre>\n<p>I confirm in the GUI that it in fact has multiple models. I invoke it like this:<\/p>\n<pre><code>response = client.invoke_endpoint(\n    EndpointName=endpoint_name, \n    TargetModel=model_name1,\n    ContentType=&quot;text\/x-libsvm&quot;, \n    Body=payload\n)\n<\/code><\/pre>\n<p>and get this error:<\/p>\n<blockquote>\n<p>ValidationError: An error occurred (ValidationError) when calling the\nInvokeEndpoint operation: Endpoint\nmy-endpoint1 is not a multi-model endpoint\nand does not support target model header.<\/p>\n<\/blockquote>\n<p>The same problem was discussed <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/issues\/1026\" rel=\"nofollow noreferrer\">here<\/a> with no resolution.<\/p>\n<p>How can I invoke a multimodel endpoint?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-08-16 11:55:33.853000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-08-16 13:05:02.993000 UTC",
        "Question_score":0,
        "Question_tags":"machine-learning|amazon-sagemaker",
        "Question_view_count":247,
        "Owner_creation_date":"2008-11-20 08:57:51.293000 UTC",
        "Owner_last_access_date":"2022-09-24 19:18:28.080000 UTC",
        "Owner_location":"Israel",
        "Owner_reputation":17500,
        "Owner_up_votes":463,
        "Owner_down_votes":87,
        "Owner_views":1561,
        "Answer_body":"<p>The answer (see <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/issues\/1026\" rel=\"nofollow noreferrer\">GitHub<\/a> discussion) is that this error message is simply false.<\/p>\n<p>To avoid this error, the model's local filename (usually for the form <code>model_filename.tar.gz<\/code>) must be used, not the model name.<\/p>\n<p>The <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/invoke-multi-model-endpoint.html\" rel=\"nofollow noreferrer\">documentation<\/a> does say this, though it lacks essential detail.<\/p>\n<p>I found <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/multi_model_xgboost_home_value\/xgboost_multi_model_endpoint_home_value.ipynb\" rel=\"nofollow noreferrer\">this to be the best example<\/a>.  See the last part  of that Notebook, in which <code>invoke_endpoint<\/code> is used (rather than a predictor as used earlier in the Notebook).<\/p>\n<p>As to the location of that model file: This <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/multi_model_bring_your_own\/multi_model_endpoint_bring_your_own.ipynb\" rel=\"nofollow noreferrer\">Notebook<\/a> says:<\/p>\n<blockquote>\n<p>When creating the Model entity for multi-model endpoints, the container's ModelDataUrl is the S3 prefix where the model\nartifacts that are invokable by the endpoint are located. The rest of the S3 path will be specified when invoking the model.<\/p>\n<\/blockquote>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-08-18 10:47:03.050000 UTC",
        "Answer_last_edit_date":"2021-08-26 06:34:26.337000 UTC",
        "Answer_score":0.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":69203143,
        "Question_title":"Using Tesla A100 GPU with Kubeflow Pipelines on Vertex AI",
        "Question_body":"<p>I'm using the following lines of code to specify the desired machine type and accelerator\/GPU on a Kubeflow Pipeline (KFP) that I will be running on a serverless manner through Vertex AI\/Pipelines.<\/p>\n<pre><code>op().\nset_cpu_limit(8).\nset_memory_limit(50G).\nadd_node_selector_constraint('cloud.google.com\/gke-accelerator', 'nvidia-tesla-k80').\nset_gpu_limit(1)\n<\/code><\/pre>\n<p>and it works for other GPUs as well i.e. Pascal, Tesla, Volta cards.<\/p>\n<p>However, I can't do the same with the latest accelerator type which is the <code>Tesla A100<\/code> as it requires a special machine type, which is as least an <code>a2-highgpu-1g<\/code>.<\/p>\n<p>How do I make sure that this particular component will run on top of <code>a2-highgpu-1g<\/code> when I run it on Vertex?<\/p>\n<p>If i simply follow the method for older GPUs:<\/p>\n<pre><code>op().\nset_cpu_limit(12). # max for A2-highgpu-1g\nset_memory_limit(85G). # max for A2-highgpu-1g\nadd_node_selector_constraint('cloud.google.com\/gke-accelerator', 'nvidia-tesla-a100').\nset_gpu_limit(1)\n<\/code><\/pre>\n<p>It throws an error when run\/deployed since the machine type that is being spawned is the general type i.e. N1-Highmem-*<\/p>\n<p>Same thing happened when I did not specify the cpu and memory limits, in hope that it will automatically select the right machnie type based on the accelerator constraint.<\/p>\n<pre><code>    op().\n    add_node_selector_constraint('cloud.google.com\/gke-accelerator', 'nvidia-tesla-a100').\n    set_gpu_limit(1)\n<\/code><\/pre>\n<p>Error:\n<code>&quot;NVIDIA_TESLA_A100&quot; is not supported for machine type &quot;n1-highmem-2&quot;,<\/code><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2021-09-16 06:09:14.497000 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"google-kubernetes-engine|kubeflow-pipelines|google-cloud-vertex-ai",
        "Question_view_count":473,
        "Owner_creation_date":"2015-12-04 05:40:05.093000 UTC",
        "Owner_last_access_date":"2022-09-22 02:24:42.530000 UTC",
        "Owner_location":"Manila, NCR, Philippines",
        "Owner_reputation":185,
        "Owner_up_votes":11,
        "Owner_down_votes":0,
        "Owner_views":14,
        "Answer_body":"<p>Currently, GCP don't support A2 Machine type for normal KF Components. A potential workaround right now is to use <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/create-custom-job\" rel=\"nofollow noreferrer\"><strong>GCP custom job component<\/strong><\/a> that you can explicitly specify the machine type.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-09-20 02:13:08.633000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_valid_tags":[
            "google-cloud-vertex-ai"
        ]
    },
    {
        "Question_id":69577270,
        "Question_title":"Google Cloud Vertex AI - 400 'dedicated_resources' is not supported for Model",
        "Question_body":"<p>I'm attempting to deploy a text classification model I trained with Vertex AI on the Google Cloud Platform using the Python SDK.<\/p>\n<pre><code>from google.cloud import aiplatform\n\nimport os\n\nos.environ[&quot;GOOGLE_APPLICATION_CREDENTIALS&quot;] = &quot;&lt;key location&gt;&quot;\n\ndef create_endpoint(\n    project_id: str,\n    display_name: str,\n    location: str,\n    sync: bool = True,\n):\n    endpoint = aiplatform.Endpoint.create(\n        display_name=display_name, project=project_id, location=location,\n    )\n\n    print(endpoint.display_name)\n    print(endpoint.resource_name)\n    return endpoint\n\ndef deploy_model(project_id, location, model_id):\n    model_location = &quot;projects\/{}\/locations\/{}\/models\/{}&quot;.format(project_id, location, model_id)\n\n    print(&quot;Initializing Vertex AI&quot;)\n    aiplatform.init(project=project_id, location=location)\n\n    print(&quot;Getting model from {}&quot;.format(model_location))\n    model = aiplatform.Model(model_location)\n\n    print(&quot;Creating endpoint.&quot;)\n    endpoint = create_endpoint(project_id, &quot;{}_endpoint&quot;.format(model_id), location)\n\n    print(&quot;Deploying endpoint&quot;)\n    endpoint.deploy(\n        model,\n        machine_type=&quot;n1-standard-4&quot;,\n        min_replica_count=1,\n        max_replica_count=5,\n        accelerator_type='NVIDIA_TESLA_K80',\n        accelerator_count=1\n    )\n\n    return endpoint\n\nendpoint = deploy_model(\n    &quot;&lt;project name&gt;&quot;,\n    &quot;us-central1&quot;,\n    &quot;&lt;model id&gt;&quot;,\n)\n<\/code><\/pre>\n<p>Unfortunately, when I run this code, I receive this error after the endpoint.deploy is triggered:\n<code>google.api_core.exceptions.InvalidArgument: 400 'dedicated_resources' is not supported for Model...<\/code> followed by the model location.<\/p>\n<p>Note the places I've swapped my values with &lt;***&gt; to hide my local workspace variables.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2021-10-14 20:57:29.023000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-10-14 21:57:40.770000 UTC",
        "Question_score":0,
        "Question_tags":"python|google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":616,
        "Owner_creation_date":"2014-11-26 14:46:22.680000 UTC",
        "Owner_last_access_date":"2022-09-23 13:33:34.890000 UTC",
        "Owner_location":"Boston, MA",
        "Owner_reputation":1256,
        "Owner_up_votes":391,
        "Owner_down_votes":2,
        "Owner_views":245,
        "Answer_body":"<p>You encounter the error since it is not possible to assign a custom machine to a Text Classification model. Only custom models and AutoML tabular models can use custom machines as per <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/configure-compute#specify\" rel=\"nofollow noreferrer\">documentation<\/a>. For AutoML models aside from tabular models, Vertex AI automatically configures the machine type.<\/p>\n<blockquote>\n<p>If you want to use a custom-trained model or an AutoML tabular model\nto serve online predictions, you must specify a machine type when you\ndeploy the Model resource as a DeployedModel to an Endpoint. <strong>For other<\/strong>\n<strong>types of AutoML models, Vertex AI configures the machine types<\/strong>\n<strong>automatically.<\/strong><\/p>\n<\/blockquote>\n<p>The workaround for this is to remove the machine related parameters on your <code>endpoint.deploy()<\/code> and you should be able to deploy the model.<\/p>\n<pre><code>print(&quot;Deploying endpoint&quot;)\nendpoint.deploy(model)\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2021-10-15 04:13:37.373000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_valid_tags":[
            "google-cloud-vertex-ai"
        ]
    },
    {
        "Question_id":69538469,
        "Question_title":"Is there a way to pass arguments to our own docker container in sagemaker?",
        "Question_body":"<p>I am trying to train my model using Bring your own container technique in sagemaker. My model training runs correctly without any issues locally. But my docker image takes env-file as an input that could change at different runs. But in sagemaker when passing the ECR image, I don't know how to pass this env-file. So instead, inside the <code>train<\/code> script, which is called by the sagemaker, I added <code>export KEY=value<\/code> statements to create my variables. Even that did not expose my variables. Another way I tried it was by executing <code>RUN source file.env<\/code> while building my image. Even this approach did not work out as I got an error <code>\/bin\/sh: 1: source: not found<\/code>.<\/p>\n<p>I could try <code>ENV<\/code> while building my image and that would probably work but this approach won't be flexible as my variables could change at different runs. Is there any way to pass docker run arguments from a sagemaker estimator or notebook? I checked out the documentation but I couldn't find anything.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2021-10-12 09:53:42.110000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"docker|amazon-sagemaker|env-file",
        "Question_view_count":292,
        "Owner_creation_date":"2021-10-05 11:38:25.427000 UTC",
        "Owner_last_access_date":"2022-09-16 13:13:37.810000 UTC",
        "Owner_location":null,
        "Owner_reputation":43,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Answer_body":"<p>I've been passing environment variables along with the Docker image URL when creating the Training job using the SageMaker Python SDK. Documentation of the <code>train<\/code> method states that:<\/p>\n<pre><code>environment (dict[str, str]) : Environment variables to be set for\n            use during training job (default: ``None``): \n<\/code><\/pre>\n<p>For reference, the <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/5bc3ccf\/src\/sagemaker\/session.py#L569\" rel=\"nofollow noreferrer\">SDK source<\/a>.<\/p>\n<p>Because the SDK is a wrapper on top of <a href=\"https:\/\/pypi.org\/project\/boto3\/\" rel=\"nofollow noreferrer\">Boto3<\/a>, I'm pretty sure that the same can be implemented with Boto3 alone, and that there is an equivalent for every other <a href=\"https:\/\/aws.amazon.com\/developer\/tools\/#SDKs\" rel=\"nofollow noreferrer\">Amazon Services SDK<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-09-08 18:24:57.360000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":70599052,
        "Question_title":"AWS CreateDeviceFleet operation fail because \"the account id does not have ownership on bucket\"",
        "Question_body":"<p>I'm having an issue with AWS when I try to create a device fleet with sagemaker :<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import boto3\n\nsagemaker_client = boto3.client('sagemaker', region_name=AWS_REGION)\nsagemaker_client.create_device_fleet(\n    DeviceFleetName=device_fleet_name,\n    RoleArn=iot_role_arn,\n    OutputConfig={\n        'S3OutputLocation': s3_device_fleet_output\n    }\n)\n\n<\/code><\/pre>\n<p>It raises the following exception:<\/p>\n<blockquote>\n<p>ClientError: An error occurred (ValidationException) when calling the CreateDeviceFleet operation: The account id &lt;my-account-id&gt; does not have ownership on bucket: &lt;bucket-name&gt;<\/p>\n<\/blockquote>\n<p>I dont get it because I created the bucket so I should be the owner. I have not found how to check or change bucket ownership.<\/p>\n<p>I tried changing the bucket policy as follows but it didn't help.<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n    &quot;Version&quot;: &quot;2012-10-17&quot;,\n    &quot;Statement&quot;: [\n        {\n            &quot;Sid&quot;: &quot;Statement1&quot;,\n            &quot;Principal&quot;: {\n                &quot;AWS&quot;: &quot;arn:aws:iam::&lt;id&gt;:user\/&lt;user&gt;&quot;\n            },\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: &quot;*&quot;,\n            &quot;Resource&quot;: [\n                &quot;arn:aws:s3:::&lt;bucket-name&gt;&quot;,\n                &quot;arn:aws:s3:::&lt;bucket-name&gt;\/*&quot;\n            ]\n        }\n    ]\n}\n\n<\/code><\/pre>\n<p>I also tried with sagemaker's GUI, it fails for the same reason (ValidationException, the account id &lt;my-account-id&gt; does not have ownership on bucket : &lt;bucket-name&gt;).<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2022-01-05 20:10:29.643000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-s3|amazon-sagemaker|aws-iot",
        "Question_view_count":48,
        "Owner_creation_date":"2020-03-16 17:16:50.350000 UTC",
        "Owner_last_access_date":"2022-04-26 10:55:17.747000 UTC",
        "Owner_location":null,
        "Owner_reputation":36,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Answer_body":"<p>This bucket policy made it work :<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n    &quot;Version&quot;: &quot;2012-10-17&quot;,\n    &quot;Statement&quot;: [\n        {\n            &quot;Sid&quot;: &quot;Statement1&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Principal&quot;: {\n                &quot;AWS&quot;: &quot;arn:aws:iam::&lt;account-id&gt;:role\/&lt;iot-role&gt;&quot;\n            },\n            &quot;Action&quot;: &quot;*&quot;,\n            &quot;Resource&quot;: [\n                &quot;arn:aws:s3:::&lt;bucket-name&gt;&quot;,\n                &quot;arn:aws:s3:::&lt;bucket-name&gt;\/*&quot;\n            ]\n        }\n    ]\n}\n<\/code><\/pre>\n<p>I still don't fully get it, because the role had full access on s3 buckets so i don't know why editing the bucket's policy changed something, but it works.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-01-06 14:38:47.650000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":54465049,
        "Question_title":"Getting Out of Memory error when using Image Classification in Sage Maker",
        "Question_body":"<p>When using a p2.xlarge or p3.2xlarge with up to 1TB of memory trying to use the predefined SageMaker Image Classification algorithm in a training job I\u2019m getting the following error:<\/p>\n\n<p><code>ClientError: Out of Memory. Please use a larger instance and\/or reduce the values of other parameters (e.g. batch size, number of layers etc.) if applicable<\/code><\/p>\n\n<p>I\u2019m using 450+ images, I\u2019ve tried resizing them from their original 2000x3000px size to a 244x244px size down to a 24x24px size and keep getting the same error.<\/p>\n\n<p>I\u2019ve tried adjusting my hyper parameters: num_classes, num_layers, num_training_samples, optimizer, image_shape, checkpoint frequency, batch_size and epochs. Also tried using pretrained model. But the same error keeps occurring.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-01-31 16:27:28.437000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2019-02-07 04:20:17.233000 UTC",
        "Question_score":1,
        "Question_tags":"amazon-web-services|artificial-intelligence|amazon-sagemaker",
        "Question_view_count":2360,
        "Owner_creation_date":"2013-02-25 19:50:19.380000 UTC",
        "Owner_last_access_date":"2022-09-23 22:29:29.277000 UTC",
        "Owner_location":"Puebla",
        "Owner_reputation":147,
        "Owner_up_votes":148,
        "Owner_down_votes":0,
        "Owner_views":54,
        "Answer_body":"<p>Would've added this as a comment but I don't have enough rep yet.<\/p>\n\n<p>A few clarifying questions so that I can have some more context:<\/p>\n\n<p><em>How exactly are you achieving 1TB of RAM?<\/em><\/p>\n\n<ol>\n<li><a href=\"https:\/\/aws.amazon.com\/ec2\/instance-types\/p2\/\" rel=\"nofollow noreferrer\"><code>p2.xlarge<\/code><\/a> servers have 61GB of RAM, and <a href=\"https:\/\/aws.amazon.com\/ec2\/instance-types\/p3\/\" rel=\"nofollow noreferrer\"><code>p3.2xlarge<\/code><\/a> servers have 61GB memory + 16GB onboard the Tesla V100 GPU. <\/li>\n<\/ol>\n\n<p><em>How are you storing, resizing, and ingesting the images into the SageMaker algorithm?<\/em><\/p>\n\n<ol start=\"2\">\n<li>The memory error seems suspect considering it still occurs when downsizing images to 24x24. If you are resizing your original images (450 images at 2000x3000 resolution) as in-memory objects and aren't performing the transformations in-place (ie: not creating new images), you may have a substantial bit of memory pre-allocated, causing the SageMaker training algorithm to throw an OOM error.<\/li>\n<\/ol>",
        "Answer_comment_count":5.0,
        "Answer_creation_date":"2019-01-31 18:44:45.577000 UTC",
        "Answer_last_edit_date":"2019-01-31 19:17:47.733000 UTC",
        "Answer_score":2.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":42228715,
        "Question_title":"Running h2o in R script in Azure machine learning",
        "Question_body":"<p>I know how to access packages in <code>R<\/code> scripts in <code>Azure machine<\/code> learning by either using the <code>Azure<\/code> supported ones or by zipping up the packages.<\/p>\n\n<p>My problem now is that <code>Azure<\/code> machine learning does not support the <code>h2o package<\/code> and when I tried using the zipped file - it gave an <code>error<\/code>. <\/p>\n\n<p>Has anyone figured out how to use <code>h2o<\/code> in <code>R<\/code> in <code>Azure machine<\/code> learning?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2017-02-14 14:29:19.403000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2017-02-14 14:35:46.530000 UTC",
        "Question_score":0,
        "Question_tags":"r|azure|h2o|azure-machine-learning-studio",
        "Question_view_count":361,
        "Owner_creation_date":"2012-10-29 18:22:48.770000 UTC",
        "Owner_last_access_date":"2022-09-21 14:01:44.350000 UTC",
        "Owner_location":null,
        "Owner_reputation":823,
        "Owner_up_votes":16,
        "Owner_down_votes":4,
        "Owner_views":114,
        "Answer_body":"<p>So since there was no reply to my question, I made some research and came up with the following:<\/p>\n\n<p>H2O cannot be ran in a straightforward manner in Azure machine learning embedded R scripts. A workaround the problem is to consider using an Azure created environment - specially for H2O. The options available are:<\/p>\n\n<ol>\n<li>Spinning up an H2O Artificial Intelligence Virtual Machine solution<\/li>\n<li>Using an H2O application for HDInsight<\/li>\n<\/ol>\n\n<p>For more reading, you can go to: <a href=\"http:\/\/docs.h2o.ai\/h2o\/latest-stable\/h2o-docs\/azure.html\" rel=\"nofollow noreferrer\">http:\/\/docs.h2o.ai\/h2o\/latest-stable\/h2o-docs\/azure.html<\/a> <\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2017-02-16 09:36:45.673000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":3.0,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":54757598,
        "Question_title":"Add model description when registering model after hyperdrive successful run",
        "Question_body":"<p>I have successfully trained a model on Azure Machine Learning Service using Hyperdrive that has now yielded a hyperdrive run instance<\/p>\n\n<pre><code>hyperdrive_run = exp.submit(config=hypertune_config)\nhyperdrive_run\nbest_run = hyperdrive_run.get_best_run_by_primary_metric()\n<\/code><\/pre>\n\n<p>As a next step, I would like to register a model while adding a description to the model.:<\/p>\n\n<pre><code>pumps_rf = best_run.register_model(model_name='pumps_rf', model_path='outputs\/rf.pkl')\n<\/code><\/pre>\n\n<p>There is a <code>description<\/code> column in the Models section of my AML Workspace on Azure portal but the <code>register_model<\/code> method does not seem to have a <code>description<\/code> flag. So how do I go about adding a description to the model so I see it in Azure Portal?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2019-02-19 01:23:00.057000 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"azure|azure-machine-learning-studio",
        "Question_view_count":453,
        "Owner_creation_date":"2014-08-20 22:42:51.227000 UTC",
        "Owner_last_access_date":"2022-02-11 02:02:02.143000 UTC",
        "Owner_location":"Toronto, Canada",
        "Owner_reputation":2754,
        "Owner_up_votes":189,
        "Owner_down_votes":1,
        "Owner_views":124,
        "Answer_body":"<p>Good question :).<\/p>\n\n<p>Looking at the current version of the API, it doesn't look like you can add the description using <code>Run.register_model<\/code>, as confirmed <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.run.run?view=azure-ml-py#register-model-model-name--model-path-none--tags-none--properties-none----kwargs-\" rel=\"nofollow noreferrer\">by the docs<\/a>. <\/p>\n\n<p>You can go around this however by registering the model using the <code>Model.register<\/code> method which, fortunately, includes an argument for <code>description<\/code> as detailed <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.model.model?view=azure-ml-py#register-workspace--model-path--model-name--tags-none--properties-none--description-none-\" rel=\"nofollow noreferrer\">here<\/a>. In your case, you also need to <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.run(class)?view=azure-ml-py#download-file-name--output-file-path-none-\" rel=\"nofollow noreferrer\">download the files<\/a> first.<\/p>\n\n<p>In short, use something like:<\/p>\n\n<pre class=\"lang-python prettyprint-override\"><code>best_run.download_file('outputs\/rf.pkl', output_file_path='.\/rf.pkl')\n\nModel.register(workspace=ws, model_path='.\/rf.pkl', model_name=\"pumps_rf\", description=\"There are many models like it, but this one is mine.\")\n<\/code><\/pre>",
        "Answer_comment_count":3.0,
        "Answer_creation_date":"2019-02-20 13:15:17.447000 UTC",
        "Answer_last_edit_date":"2019-02-20 18:14:19.150000 UTC",
        "Answer_score":1.0,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":62137347,
        "Question_title":"How to get a Sagemaker notebook onto GitHub?",
        "Question_body":"<p>I have a Sagemaker notebook that I would like to move to a GitHub repository. I thought perhaps I should download the files locally, then I can easily push to git. But I cannot figure out how to download the folder to my local computer. Then I though, perhaps there is a way using the AWS CLI to move directly from Sagemaker to git? I've made many google searches that are unable to answer my question.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2020-06-01 17:10:37.107000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":4,
        "Question_tags":"amazon-web-services|github|amazon-sagemaker",
        "Question_view_count":2743,
        "Owner_creation_date":"2018-06-26 13:36:22.170000 UTC",
        "Owner_last_access_date":"2022-09-16 19:14:40.053000 UTC",
        "Owner_location":"McLean, VA, USA",
        "Owner_reputation":936,
        "Owner_up_votes":23,
        "Owner_down_votes":7,
        "Owner_views":153,
        "Answer_body":"<p>Building off the answer given by @mokugo-devops, I was able to link my existing notebook to my GitHub account.<\/p>\n\n<p>First, I followed the directions posted in the link provided in his answer to set up my GitHub repo with my AWS account on the CLI, then I used the following command to edit my existing notebook:<\/p>\n\n<pre><code>aws sagemaker  update-notebook-instance \\\n--notebook-instance-name &lt;value&gt; \\\n--default-code-repository &lt;saved-github-repo-name-in-AWS&gt;\n<\/code><\/pre>\n\n<p>my notebook instance is now linked my GitHub repo.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-06-02 12:45:32.273000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":57811873,
        "Question_title":"Can H2o AutoML benefit from a GPU instance on Sagemaker platform?",
        "Question_body":"<p>I'm running some projects with H2o AutoML using Sagemaker notebook instances, and I would like to know if H2o AutoML can benefit from a GPU Sagemaker instance, if so, how should I configure the notebook? <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":4,
        "Question_creation_date":"2019-09-05 19:25:33.230000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"gpu|h2o|amazon-sagemaker|automl",
        "Question_view_count":462,
        "Owner_creation_date":"2017-10-26 10:07:59.113000 UTC",
        "Owner_last_access_date":"2021-09-21 19:53:06.417000 UTC",
        "Owner_location":"Belo Horizonte, MG, Brasil",
        "Owner_reputation":97,
        "Owner_up_votes":15,
        "Owner_down_votes":0,
        "Owner_views":25,
        "Answer_body":"<p><a href=\"http:\/\/docs.h2o.ai\/h2o\/latest-stable\/h2o-docs\/automl.html\" rel=\"nofollow noreferrer\">H2O AutoML<\/a> contains a handful of algorithms and one of them is <a href=\"http:\/\/docs.h2o.ai\/h2o\/latest-stable\/h2o-docs\/data-science\/xgboost.html\" rel=\"nofollow noreferrer\">XGBoost<\/a>, which has been part of H2O AutoML since H2O version 3.22.0.1.  XGBoost is the only GPU-capable algorithm inside of H2O AutoML, however, a lot of the models that are trained in AutoML are XGBoost models, so it still can be useful to utilize a GPU. Keep in mind that you must use H2O 3.22 or above to use this feature.<\/p>\n\n<p>My suggestion is to test it on a GPU-enabled instance and compare the results to a non-GPU instance and see if it's worth the extra cost.  <\/p>",
        "Answer_comment_count":5.0,
        "Answer_creation_date":"2019-09-05 23:49:07.090000 UTC",
        "Answer_last_edit_date":"2019-09-06 05:30:43.127000 UTC",
        "Answer_score":1.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":73760033,
        "Question_title":"How to transfer a csv file from notebook folder to a datastore",
        "Question_body":"<p>I want to transfer a generated csv file <code>test_df.csv<\/code> from my Azure ML notebook folder which has a path <code>\/Users\/Ankit19.Gupta\/test_df.csv<\/code> to a datastore which has a web path <code>https:\/\/abc.blob.core.windows.net\/azureml\/LocalUpload\/f3db18b6<\/code>. I have written the python code as<\/p>\n<pre><code>from azureml.core import Workspace\nws = Workspace.from_config()\ndatastore = ws.get_default_datastore()\n    \ndatastore.upload_files('\/Users\/Ankit19.Gupta\/test_df.csv',\n                  target_path='https:\/\/abc.blob.core.windows.net\/azureml\/LocalUpload\/f3db18b6',\n                  overwrite=True)\n<\/code><\/pre>\n<p>But it is showing the following error message:<\/p>\n<pre><code>UserErrorException: UserErrorException:\n    Message: '\/' does not point to a file. Please upload the file to cloud first if running in a cloud notebook.\n    InnerException None\n    ErrorResponse \n{\n    &quot;error&quot;: {\n        &quot;code&quot;: &quot;UserError&quot;,\n        &quot;message&quot;: &quot;'\/' does not point to a file. Please upload the file to cloud first if running in a cloud notebook.&quot;\n    }\n}\n<\/code><\/pre>\n<p>I have tried <a href=\"https:\/\/stackoverflow.com\/questions\/67897947\/how-to-transfer-data-from-azure-ml-notebooks-to-a-storage-container\">this<\/a> but it is not working for me. Can anyone please help me to resolve this issue. Any help would be appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-09-18 04:00:33.117000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|azure|file-upload|azure-machine-learning-service|movefile",
        "Question_view_count":43,
        "Owner_creation_date":"2018-10-21 20:43:54.483000 UTC",
        "Owner_last_access_date":"2022-09-24 14:48:13.903000 UTC",
        "Owner_location":null,
        "Owner_reputation":237,
        "Owner_up_votes":60,
        "Owner_down_votes":0,
        "Owner_views":173,
        "Answer_body":"<p>The way the path was mentioned is not accurate. The datastore path will be different manner.\nReplace the below code for the small change in the calling path.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/4o1WU.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/4o1WU.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<pre><code>from azureml.core import Workspace\nws = Workspace.from_config()\ndatastore = ws.get_default_datastore()\n    \ndatastore.upload_files('.\/Users\/foldername\/filename.csv',\n                  target_path=\u2019your targetfolder',\n                  overwrite=True)\n<\/code><\/pre>\n<p>We need to call all the parent folders before the folder.  <strong><code>\u201c.\/\u201d<\/code><\/strong> is the way we can call the dataset from datastore.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-09-20 07:20:53.173000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":65832720,
        "Question_title":"Cannot find folder that Python os module shows exists",
        "Question_body":"<p>This is definitely a first for me. Using the <code>os.listdir()<\/code> method, I'm able to view files \/ folders from a directory that doesn't seem to exist. Below is a lightly redacted snippet from the console showing the effect:<\/p>\n<pre><code>sh-4.2$ python\nPython 3.6.11 | packaged by conda-forge | (default, Aug  5 2020, 20:09:42)\n[GCC 7.5.0] on linuxType &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.\n&gt;&gt;&gt; import os\n&gt;&gt;&gt; os.listdir(&lt;full_file_path&gt;)\n['file01', 'file02', 'file03', 'file04', 'file05']\n&gt;&gt;&gt; exit()\nsh-4.2$ ls &lt;full_file_path&gt;\nsh-4.2$ ls -a &lt;full_file_path&gt;\n.  ..\nsh-4.2$\n<\/code><\/pre>\n<p>From the graphical file explorer, I am unable to see anything in the parent folder for the files I'm searching for. Python insists that the files are real and exist, but they cannot be accessed without using python to do so. They should not be hidden, or having special permissions to be able to view them. Any help is appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-01-21 17:31:48.190000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python-3.x|amazon-sagemaker|python-os",
        "Question_view_count":88,
        "Owner_creation_date":"2019-09-10 12:23:41.763000 UTC",
        "Owner_last_access_date":"2022-08-08 16:55:01.517000 UTC",
        "Owner_location":"Seaside, CA, USA",
        "Owner_reputation":125,
        "Owner_up_votes":6,
        "Owner_down_votes":0,
        "Owner_views":10,
        "Answer_body":"<p>This issue has been solved.<\/p>\n<p>The directory that I'm looking for was created with <code>os.makedirs()<\/code>. On closer inspection, I can see that it created the filepath from <code>os.getcwd()<\/code> exactly as it was entered.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>&gt;&gt;&gt; import os\n&gt;&gt;&gt; os.getcwd()\n'\/ec2-user\/SageMaker\/path\/to\/current\/'\n\n&gt;&gt;&gt; os.listdir('.\/results') # Should show me 5 different folders\n[]\n\n&gt;&gt;&gt; os.listdir('.\/~') # Uh oh\n['SageMaker']\n<\/code><\/pre>\n<p>So what happened was that the full file path was created from the original working directory, contrary to what was expected.<\/p>\n<pre><code>sh-4.2 $ ls ~\/SageMaker\/path\/to\/current\/~\/SageMaker\/path\/to\/current\/results\nfolder01 folder02 folder03 folder04 folder05\n<\/code><\/pre>\n<p><strong>TL;DR<\/strong>\nI did not confirm the location of the directory was being created from root as expected, and it was created in the wrong location. <code>os.listdir()<\/code> still showed the files in the &quot;correct location&quot; because it wasn't starting in root, but in the current working directory.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2021-01-21 18:19:48.847000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":41451123,
        "Question_title":"An example to create a training model on real data in AzureML",
        "Question_body":"<p>Can you introduce a real sample for azure ML and show how can it be possible to see the result of the training?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2017-01-03 19:40:53.410000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"machine-learning|azure-machine-learning-studio",
        "Question_view_count":76,
        "Owner_creation_date":"2014-06-23 20:02:17.940000 UTC",
        "Owner_last_access_date":"2022-09-24 13:03:15.667000 UTC",
        "Owner_location":"Belgium",
        "Owner_reputation":17835,
        "Owner_up_votes":636,
        "Owner_down_votes":1612,
        "Owner_views":2203,
        "Answer_body":"<p><a href=\"http:\/\/blog.learningtree.com\/how-to-build-a-predictive-model-using-azure-machine-learning\/\" rel=\"nofollow noreferrer\">Here<\/a> is a good sample to create your first model.\nI should notice that I can't load data from url, as there is a forbidden error to load from url, and I don't know why! \nAnyhow, you can import data manually by copy the data from <a href=\"http:\/\/blog.learningtree.com\/wp-content\/uploads\/2015\/01\/breast-cancer-wisconsin.data_.arff_.txt\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n\n<p>Also, you can find the created model which is published here: \n<a href=\"https:\/\/gallery.cortanaintelligence.com\/Experiment\/Cancer-Model-1\" rel=\"nofollow noreferrer\">https:\/\/gallery.cortanaintelligence.com\/Experiment\/Cancer-Model-1<\/a><\/p>\n\n<p>About see the result of the training model, you can right click on the tick (highlighted by a red circle in the following picture) of Evaluation Model. Then, in the opened menu, go to \"Evaluation Result -> Visualization\".\n<a href=\"https:\/\/i.stack.imgur.com\/vhJWE.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/vhJWE.jpg\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>After that you can see a window like the following (which shows ROC curve and some related result such as accuracy of the training model):<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/FAuzU.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/FAuzU.png\" alt=\"enter image description here\"><\/a>\n<a href=\"https:\/\/i.stack.imgur.com\/B39lI.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/B39lI.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Besides, you can see <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/machine-learning-azure-ml-customer-churn-scenario\" rel=\"nofollow noreferrer\">this example<\/a> as an another sample.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2017-01-03 19:40:53.410000 UTC",
        "Answer_last_edit_date":"2017-01-04 13:50:37.233000 UTC",
        "Answer_score":0.0,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":50334735,
        "Question_title":"How to generate Recordio from Java object",
        "Question_body":"<p>I am trying to serialize a list of java Objects (POJO) into RecordIO format. I have seen this BeanIO (<a href=\"http:\/\/beanio.org\/\" rel=\"nofollow noreferrer\">http:\/\/beanio.org\/<\/a>) but it seems to be outdated. Is there any other Java library that could be used or a different way to do this ?<\/p>\n\n<p>Once list of objects is serialized it will be used to train a model with SageMaker.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2018-05-14 16:10:57.120000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"aws-sdk|amazon-sagemaker",
        "Question_view_count":160,
        "Owner_creation_date":"2015-04-08 00:54:56.053000 UTC",
        "Owner_last_access_date":"2019-10-03 00:46:15.877000 UTC",
        "Owner_location":null,
        "Owner_reputation":35,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":10,
        "Answer_body":"<p>Solving my own problem. I decided to use Apache Avro instead of BeanIO. Spark allow to serialize using Avro (c.f. Spark-Avro). This seems to work however it did not fit my use case has I was trying to serialize an array of numbers.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2018-05-18 14:14:47.110000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":63268849,
        "Question_title":"ERR_HTTP2_PROTOCOL_ERROR when opening Notebook in JUPYTERLAB Azure ML Studio",
        "Question_body":"<p>So our team created a new Azure <strong>Machine Learning<\/strong> resource, but whenever I try to add a new notebook and try to edit it using &quot;JUPYTERLAB&quot; i get <code>ERR_HTTP2_PROTOCOL_ERROR<\/code> error, but the same notebook, when edited using <code>EDIT IN JUPYTER<\/code> works perfectly.<\/p>\n<p>This is a blank and clean notebook, I also tried 2 different laptops and multiple browsers per laptop, same error. I also tried incognito and clearing cookies, but to no avail.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/IevSG.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/IevSG.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>update: I seem to have accidentally replicated the issue and I now know what is causing it, the situation is that Im using my work laptop and constantly switching VPN connections, and some times, connecting to the AZURE PORTAl OUTSIDE the VPN. So, when you've worked on a notebook while inside a VPN, then you disconnected, and tried loading the notebook sometime later, you will encounter this<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2020-08-05 15:47:33.543000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-08-10 10:48:21.367000 UTC",
        "Question_score":1,
        "Question_tags":"jupyter-notebook|azure-machine-learning-studio|azure-machine-learning-service",
        "Question_view_count":246,
        "Owner_creation_date":"2012-06-18 23:56:44.277000 UTC",
        "Owner_last_access_date":"2022-09-24 14:51:42.627000 UTC",
        "Owner_location":null,
        "Owner_reputation":298,
        "Owner_up_votes":10,
        "Owner_down_votes":1,
        "Owner_views":38,
        "Answer_body":"<p>This problem has stomped me for hours, but I was finally able to fix it. What I did was I opened a terminal and did a Jupyter lab rebuild &quot;jupyter lab build&quot;<\/p>\n<p><a href=\"https:\/\/imgur.com\/aRB8GWS.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/imgur.com\/aRB8GWS.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/IceQO.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/IceQO.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2020-08-05 16:08:25.983000 UTC",
        "Answer_last_edit_date":"2020-08-05 23:10:31.917000 UTC",
        "Answer_score":1.0,
        "Question_valid_tags":[
            "azure-machine-learning-studio",
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":68004538,
        "Question_title":"Is dvc.yaml supposed to be written or generated by dvc run command?",
        "Question_body":"<p>Trying to understand <a href=\"https:\/\/dvc.org\/doc\/start\" rel=\"nofollow noreferrer\">dvc<\/a>, most tutorials mention generation of dvc.yaml by running <code>dvc run<\/code> command.<\/p>\n<p>But at the same time, dvc.yaml which defines the DAG is also <a href=\"https:\/\/dvc.org\/doc\/user-guide\/project-structure\/pipelines-files\" rel=\"nofollow noreferrer\">well documented<\/a>. Also the fact that it is a yaml format and human readable\/writable would point to the fact that it is meant to be a DSL for specifying your data pipeline.<\/p>\n<p>Can somebody clarify which is the better practice?\nWriting the dvc.yaml or let it be generated by <code>dvc run<\/code> command?\nOr is it left to user's choice and there is no technical difference?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2021-06-16 14:19:55.940000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-06-16 19:58:05.370000 UTC",
        "Question_score":5,
        "Question_tags":"directed-acyclic-graphs|data-pipeline|dvc",
        "Question_view_count":1101,
        "Owner_creation_date":"2009-05-27 17:42:14.993000 UTC",
        "Owner_last_access_date":"2022-09-23 06:17:58.327000 UTC",
        "Owner_location":"Gothenburg, Sweden",
        "Owner_reputation":1547,
        "Owner_up_votes":28,
        "Owner_down_votes":9,
        "Owner_views":212,
        "Answer_body":"<p>I'd recommend manual editing as the main route! (I believe that's officially recommended since <a href=\"https:\/\/dvc.org\/blog\/dvc-2-0-release\" rel=\"nofollow noreferrer\">DVC 2.0<\/a>)<\/p>\n<p><code>dvc stage add<\/code> can still be very helpful for programmatic generation of pipelines files, but it doesn't support all the features of <code>dvc.yaml<\/code>, for example setting <code>vars<\/code> values or defining <a href=\"https:\/\/dvc.org\/doc\/user-guide\/project-structure\/pipelines-files#foreach-stages\" rel=\"nofollow noreferrer\"><code>foreach<\/code> stages<\/a>.<\/p>",
        "Answer_comment_count":7.0,
        "Answer_creation_date":"2021-06-16 16:00:05.940000 UTC",
        "Answer_last_edit_date":"2021-06-16 20:39:37.407000 UTC",
        "Answer_score":4.0,
        "Question_valid_tags":[
            "dvc"
        ]
    },
    {
        "Question_id":72251787,
        "Question_title":"Permission \"artifactregistry.repositories.downloadArtifacts\" denied on resource",
        "Question_body":"<p>While the artifact repository was successfully creating, running a docker push to push the image to the google artifact registry fails with a permissions error even after granting all artifact permissions to the accounting I am using on gcloud cli.<\/p>\n<p><strong>Command used to push image:<\/strong><\/p>\n<pre><code>docker push us-central1-docker.pkg.dev\/project-id\/repo-name:v2\n<\/code><\/pre>\n<p><strong>Error message:<\/strong><\/p>\n<pre><code>The push refers to repository [us-central1-docker.pkg.dev\/project-id\/repo-name]\n6f6f4a472f31: Preparing\nbc096d7549c4: Preparing\n5f70bf18a086: Preparing\n20bed28d4def: Preparing\n2a3255c6d9fb: Preparing\n3f5d38b4936d: Waiting\n7be8268e2fb0: Waiting\nb889a93a79dd: Waiting\n9d4550089a93: Waiting\na7934564e6b9: Waiting\n1b7cceb6a07c: Waiting\nb274e8788e0c: Waiting\n78658088978a: Waiting\ndenied: Permission &quot;artifactregistry.repositories.downloadArtifacts&quot; denied on resource &quot;projects\/project-id\/locations\/us-central1\/repositories\/repo-name&quot; (or it may not exist)\n\n\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":3,
        "Question_creation_date":"2022-05-15 20:00:57.243000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-05-16 02:21:18.670000 UTC",
        "Question_score":12,
        "Question_tags":"python|google-cloud-platform|google-cloud-ml|google-cloud-vertex-ai|docker-push",
        "Question_view_count":5722,
        "Owner_creation_date":"2015-03-21 06:55:29.353000 UTC",
        "Owner_last_access_date":"2022-09-02 13:58:42.187000 UTC",
        "Owner_location":"Bangkok",
        "Owner_reputation":194,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":12,
        "Answer_body":"<p>I was able to recreate your use case. This happens when you are trying to push an image on a <code>repository<\/code> in which its specific hostname (associated with it's repository location) is not yet  added to the credential helper configuration for authentication. You may refer to this <a href=\"https:\/\/cloud.google.com\/artifact-registry\/docs\/docker\/authentication\" rel=\"noreferrer\">Setting up authentication for Docker <\/a> as also provided by @DazWilkin in the comments for more details.<\/p>\n<p>In my example, I was trying to push an image on a repository that has a location of <code>us-east1<\/code> and got the same error since it is not yet added to the credential helper configuration.\n<a href=\"https:\/\/i.stack.imgur.com\/NQeIf.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/NQeIf.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>And after I ran the authentication using below command (specifically for us-east1 since it is the <code>location<\/code> of my repository), the image was successfully pushed:<\/p>\n<pre><code>gcloud auth configure-docker us-east1-docker.pkg.dev\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/q2Q9x.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/q2Q9x.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><em><strong>QUICK TIP<\/strong><\/em>: You may  get your authentication command specific for your repository when you open your desired repository in the <a href=\"https:\/\/console.cloud.google.com\/artifacts\" rel=\"noreferrer\">console<\/a>, and then click on the <code>SETUP INSTRUCTIONS<\/code>.\n<a href=\"https:\/\/i.stack.imgur.com\/KBjqa.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/KBjqa.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_date":"2022-05-16 06:40:03.067000 UTC",
        "Answer_last_edit_date":"2022-06-05 22:36:23.460000 UTC",
        "Answer_score":23.0,
        "Question_valid_tags":[
            "google-cloud-vertex-ai"
        ]
    },
    {
        "Question_id":55621967,
        "Question_title":"Feature Importance for XGBoost in Sagemaker",
        "Question_body":"<p>I have built an XGBoost model using Amazon Sagemaker, but I was unable to find anything which will help me interpret the model and validate if it has learned the right dependencies.<\/p>\n\n<p>Generally, we can see Feature Importance for XGBoost by get_fscore() function in the python API (<a href=\"https:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_api.html\" rel=\"nofollow noreferrer\">https:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_api.html<\/a>) I see nothing of that sort in the sagemaker api(<a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/estimators.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/estimators.html<\/a>).<\/p>\n\n<p>I know I can build my own model and then deploy that using sagemaker but I am curious if anyone has faced this problem and how they overcame it.<\/p>\n\n<p>Thanks.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_date":"2019-04-10 22:06:42.317000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":4,
        "Question_tags":"xgboost|amazon-sagemaker",
        "Question_view_count":3637,
        "Owner_creation_date":"2014-12-23 09:45:25.267000 UTC",
        "Owner_last_access_date":"2022-09-22 22:39:33.930000 UTC",
        "Owner_location":"Mountain View, CA, USA",
        "Owner_reputation":123,
        "Owner_up_votes":35,
        "Owner_down_votes":0,
        "Owner_views":17,
        "Answer_body":"<p>SageMaker XGBoost currently does not provide interface to retrieve feature importance from the model. You can write some code to get the feature importance from the XGBoost model. You have to get the booster object artifacts from the model in S3 and then use the following snippet <\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import pickle as pkl\nimport xgboost\nbooster = pkl.load(open(model_file, 'rb'))\nbooster.get_score()\nbooster.get_fscore()\n<\/code><\/pre>\n\n<p>Refer <a href=\"https:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_api.html\" rel=\"nofollow noreferrer\">XGBoost doc<\/a> for methods to get feature importance from the Booster object such as <code>get_score()<\/code> or <code>get_fscore()<\/code>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2019-04-11 16:54:55.857000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":3.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":55525445,
        "Question_title":"How to overcome TrainingException when training a large model with Azure Machine Learning service?",
        "Question_body":"<p>I'm training a large-ish model, trying to use for the purpose <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/tutorial-train-models-with-aml#create-an-estimator\" rel=\"nofollow noreferrer\">Azure Machine Learning service<\/a> in Azure notebooks.<\/p>\n\n<p>I thus create an <code>Estimator<\/code> to train locally:<\/p>\n\n<pre><code>from azureml.train.estimator import Estimator\n\nestimator = Estimator(source_directory='.\/source_dir',\n                      compute_target='local',\n                      entry_script='train.py')\n<\/code><\/pre>\n\n<p>(my <code>train.py<\/code> should load and train starting from a large word vector file).<\/p>\n\n<p>When running with <\/p>\n\n<pre><code>run = experiment.submit(config=estimator)\n<\/code><\/pre>\n\n<p>I get <\/p>\n\n<blockquote>\n  <p>TrainingException: <\/p>\n  \n  <p>====================================================================<\/p>\n  \n  <p>While attempting to take snapshot of\n  \/data\/home\/username\/notebooks\/source_dir Your total\n  snapshot size exceeds the limit of 300.0 MB. Please see\n  <a href=\"http:\/\/aka.ms\/aml-largefiles\" rel=\"nofollow noreferrer\">http:\/\/aka.ms\/aml-largefiles<\/a> on how to work with large files.<\/p>\n  \n  <p>====================================================================<\/p>\n<\/blockquote>\n\n<p>The link provided in the error is likely <a href=\"https:\/\/github.com\/MicrosoftDocs\/azure-docs\/issues\/26076\" rel=\"nofollow noreferrer\">broken<\/a>. \nContents in my <code>.\/source_dir<\/code> indeed exceed 300 MB.<br>\nHow can I solve this?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2019-04-04 21:50:42.303000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2019-04-07 13:10:37.940000 UTC",
        "Question_score":1,
        "Question_tags":"python|azure|azure-machine-learning-service",
        "Question_view_count":1127,
        "Owner_creation_date":"2014-11-11 16:17:30.717000 UTC",
        "Owner_last_access_date":"2022-09-24 20:31:18.173000 UTC",
        "Owner_location":"Verona, VR, Italy",
        "Owner_reputation":4811,
        "Owner_up_votes":376,
        "Owner_down_votes":73,
        "Owner_views":713,
        "Answer_body":"<p>You can place the training files outside <code>source_dir<\/code> so that they don't get uploaded as part of submitting the experiment, and then upload them separately to the data store (which is basically using the Azure storage associated with your workspace). All you need to do then is reference the training files from <code>train.py<\/code>. <\/p>\n\n<p>See the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/tutorial-train-models-with-aml\" rel=\"nofollow noreferrer\">Train model tutorial<\/a> for an example of how to upload data to the data store and then access it from the training file.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2019-04-05 06:29:53.380000 UTC",
        "Answer_last_edit_date":"2019-04-05 07:25:03.930000 UTC",
        "Answer_score":3.0,
        "Question_valid_tags":[
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":52684987,
        "Question_title":"AWS NoCredentials in training",
        "Question_body":"<p>I am attempting to run the <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/mxnet_gluon_mnist\/mxnet_mnist_with_gluon_local_mode.ipynb\" rel=\"nofollow noreferrer\">example code<\/a> for Amazon Sagemaker on a local GPU.  I have copied the code from the Jupyter notebook to the following Python script:<\/p>\n\n<pre><code>import boto3\nimport subprocess\nimport sagemaker\nfrom sagemaker.mxnet import MXNet\nfrom mxnet import gluon\nfrom sagemaker import get_execution_role\nimport os\n\nsagemaker_session = sagemaker.Session()\ninstance_type = 'local'\nif subprocess.call('nvidia-smi') == 0:\n    # Set type to GPU if one is present\n    instance_type = 'local_gpu'\n# role = get_execution_role()\n\ngluon.data.vision.MNIST('.\/data\/train', train=True)\ngluon.data.vision.MNIST('.\/data\/test', train=False)\n\n# successfully connects and uploads data\ninputs = sagemaker_session.upload_data(path='data', key_prefix='data\/mnist')\n\nhyperparameters = {\n    'batch_size': 100,\n    'epochs': 20,\n    'learning_rate': 0.1,\n    'momentum': 0.9,\n    'log_interval': 100\n}\n\nm = MXNet(\"mnist.py\",\n          role=role,\n          train_instance_count=1,\n          train_instance_type=instance_type,\n          framework_version=\"1.1.0\",\n          hyperparameters=hyperparameters)\n\n# fails in Docker container\nm.fit(inputs)\npredictor = m.deploy(initial_instance_count=1, instance_type=instance_type)\nm.delete_endpoint()\n<\/code><\/pre>\n\n<p>where the referenced <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/mxnet_gluon_mnist\/mnist.py\" rel=\"nofollow noreferrer\">mnist.py<\/a> file is exactly as specified on Github. The script fails on <code>m.fit<\/code> in Docker container with the following error: <\/p>\n\n<pre><code>algo-1-1DUU4_1  | Downloading s3:\/\/&lt;S3-BUCKET&gt;\/sagemaker-mxnet-2018-10-07-00-47-10-435\/source\/sourcedir.tar.gz to \/tmp\/script.tar.gz\nalgo-1-1DUU4_1  | 2018-10-07 00:47:29,219 ERROR - container_support.training - uncaught exception during training: Unable to locate credentials\nalgo-1-1DUU4_1  | Traceback (most recent call last):\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/container_support\/training.py\", line 36, in start\nalgo-1-1DUU4_1  |     fw.train()\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/mxnet_container\/train.py\", line 169, in train\nalgo-1-1DUU4_1  |     mxnet_env.download_user_module()\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/container_support\/environment.py\", line 89, in download_user_module\nalgo-1-1DUU4_1  |     cs.download_s3_resource(self.user_script_archive, tmp)\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/container_support\/utils.py\", line 37, in download_s3_resource\nalgo-1-1DUU4_1  |     script_bucket.download_file(script_key_name, target)\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/boto3\/s3\/inject.py\", line 246, in bucket_download_file\nalgo-1-1DUU4_1  |     ExtraArgs=ExtraArgs, Callback=Callback, Config=Config)\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/boto3\/s3\/inject.py\", line 172, in download_file\nalgo-1-1DUU4_1  |     extra_args=ExtraArgs, callback=Callback)\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/boto3\/s3\/transfer.py\", line 307, in download_file\nalgo-1-1DUU4_1  |     future.result()\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/s3transfer\/futures.py\", line 73, in result\nalgo-1-1DUU4_1  |     return self._coordinator.result()\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/s3transfer\/futures.py\", line 233, in result\nalgo-1-1DUU4_1  |     raise self._exception\nalgo-1-1DUU4_1  | NoCredentialsError: Unable to locate credentials\n<\/code><\/pre>\n\n<p>I am confused that I can authenticate to S3 outside of the container (to pload the training\/test data) but I cannot within the Docker container.  So I am guessing the issues has to do with passing the AWS credentials to the Docker container.  Here is the generated Docker-compose file:<\/p>\n\n<pre><code>networks:\n  sagemaker-local:\n    name: sagemaker-local\nservices:\n  algo-1-1DUU4:\n    command: train\n    environment:\n    - AWS_REGION=us-west-2\n    - TRAINING_JOB_NAME=sagemaker-mxnet-2018-10-07-00-47-10-435\n    image: 123456789012.dkr.ecr.us-west-2.amazonaws.com\/sagemaker-mxnet:1.1.0-gpu-py2\n    networks:\n      sagemaker-local:\n        aliases:\n        - algo-1-1DUU4\n    stdin_open: true\n    tty: true\n    volumes:\n    - \/tmp\/tmpSkaR3x\/algo-1-1DUU4\/input:\/opt\/ml\/input\n    - \/tmp\/tmpSkaR3x\/algo-1-1DUU4\/output:\/opt\/ml\/output\n    - \/tmp\/tmpSkaR3x\/algo-1-1DUU4\/output\/data:\/opt\/ml\/output\/data\n    - \/tmp\/tmpSkaR3x\/model:\/opt\/ml\/model\nversion: '2.1'\n<\/code><\/pre>\n\n<p>Should the AWS credentials be passed in as enviromental variables?<\/p>\n\n<p>I upgraded my <code>sagemaker<\/code> install to after reading <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/403\" rel=\"nofollow noreferrer\">Using boto3 in install local mode?<\/a>, but that had no effect.  I checked the credentials that are being fetched in the Sagemaker session (outside the container) and they appear to be blank, even though I have an <code>~\/.aws\/config<\/code> and <code>~\/.aws\/credentials<\/code> file:<\/p>\n\n<pre><code>{'_token': None, '_time_fetcher': &lt;function _local_now at 0x7f4dbbe75230&gt;, '_access_key': None, '_frozen_credentials': None, '_refresh_using': &lt;bound method AssumeRoleCredentialFetcher.fetch_credentials of &lt;botocore.credentials.AssumeRoleCredentialFetcher object at 0x7f4d2de48bd0&gt;&gt;, '_secret_key': None, '_expiry_time': None, 'method': 'assume-role', '_refresh_lock': &lt;thread.lock object at 0x7f4d9f2aafd0&gt;}\n<\/code><\/pre>\n\n<p>I am new to AWS so I do not know how to diagnose the issue regarding AWS credentials.  My <code>.aws\/config<\/code> file has the following information (with placeholder values):<\/p>\n\n<pre><code>[default]\noutput = json\nregion = us-west-2\nrole_arn = arn:aws:iam::123456789012:role\/SageMakers\nsource_profile = sagemaker-test\n\n[profile sagemaker-test]\noutput = json\nregion = us-west-2\n<\/code><\/pre>\n\n<p>Where the <code>sagemaker-test<\/code> profile has <code>AmazonSageMakerFullAccess<\/code> in the IAM Management Console.<\/p>\n\n<p>The <code>.aws\/credentials<\/code> file has the following information (represented by placeholder values):<\/p>\n\n<pre><code>[default]\naws_access_key_id = 1234567890\naws_secret_access_key = zyxwvutsrqponmlkjihgfedcba\n[sagemaker-test]\naws_access_key_id = 0987654321\naws_secret_access_key = abcdefghijklmopqrstuvwxyz\n<\/code><\/pre>\n\n<p>Lastly, these are versions of the applicable libraries from a <code>pip freeze<\/code>:<\/p>\n\n<pre><code>awscli==1.16.19\nboto==2.48.0\nboto3==1.9.18\nbotocore==1.12.18\ndocker==3.5.0\ndocker-compose==1.22.0\nmxnet-cu91==1.1.0.post0\nsagemaker==1.11.1\n<\/code><\/pre>\n\n<p>Please let me know if I left out any relevant information and thanks for any help\/feedback that you can provide.<\/p>\n\n<p><strong>UPDATE<\/strong>: Thanks for your help, everyone! While attempting some of your suggested fixes, I noticed that <code>boto3<\/code> was out of date, and update it (to <code>boto3-1.9.26<\/code> and <code>botocore-1.12.26<\/code>) which appeared to resolve the issue.  I was not able to find any documentation on that being an issue with <code>boto3==1.9.18<\/code>.  If someone could help me understand what the issue was with <code>boto3<\/code>, I would happy to make mark their answer as correct.<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":3,
        "Question_creation_date":"2018-10-07 02:32:13.133000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2018-10-18 20:09:43.687000 UTC",
        "Question_score":1,
        "Question_tags":"python|python-2.7|amazon-web-services|amazon-s3|amazon-sagemaker",
        "Question_view_count":1374,
        "Owner_creation_date":"2013-09-29 23:23:04.177000 UTC",
        "Owner_last_access_date":"2022-09-19 18:01:07.730000 UTC",
        "Owner_location":"Gloucester, VA, USA",
        "Owner_reputation":544,
        "Owner_up_votes":174,
        "Owner_down_votes":1,
        "Owner_views":85,
        "Answer_body":"<p>SageMaker local mode is designed to pick up whatever credentials are available in your boto3 session, and pass them into the docker container as environment variables. <\/p>\n\n<p>However, the version of the sagemaker sdk that you are using (1.11.1 and earlier) will ignore the credentials if they include a token, because that usually indicates short-lived credentials that won't remain valid long enough for a training job to complete or endpoint to be useful.<\/p>\n\n<p>If you are using temporary credentials, try replacing them with permanent ones, or running from an ec2 instance (or SageMaker notebook!) that has an appropriate instance role assigned.<\/p>\n\n<p>Also, the sagemaker sdk's handling of credentials changed in v1.11.2 and later -- temporary credentials will be passed to local mode containers, but with a warning message. So you could just upgrade to a newer version and try again (<code>pip install -U sagemaker<\/code>). <\/p>\n\n<p>Also, try upgrading <code>boto3<\/code> can change, so try using the latest version.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2018-10-17 23:26:48.057000 UTC",
        "Answer_last_edit_date":"2018-11-18 08:20:34.207000 UTC",
        "Answer_score":1.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":68463080,
        "Question_title":"how create azure machine learning scoring image using local package",
        "Question_body":"<p>I have pkl package saved in my azure devops repository<\/p>\n<p>using below code it searches for package in workspace.\nHow to provide package saved in repository<\/p>\n<pre><code> ws = Workspace.get(\n         name=workspace_name,\n         subscription_id=subscription_id,\n        resource_group=resource_group,\n        auth=cli_auth)\n\nimage_config = ContainerImage.image_configuration(\n    execution_script=&quot;score.py&quot;,\n    runtime=&quot;python-slim&quot;,\n    conda_file=&quot;conda.yml&quot;,\n    description=&quot;Image with ridge regression model&quot;,\n    tags={&quot;area&quot;: &quot;ml&quot;, &quot;type&quot;: &quot;dev&quot;},\n)\n\nimage = Image.create(\n    name=image_name,  models=[model], image_config=image_config, workspace=ws\n)\n\nimage.wait_for_creation(show_output=True)\n\nif image.creation_state != &quot;Succeeded&quot;:\n    raise Exception(&quot;Image creation status: {image.creation_state}&quot;)\n\nprint(\n    &quot;{}(v.{} [{}]) stored at {} with build log {}&quot;.format(\n        image.name,\n        image.version,\n        image.creation_state,\n        image.image_location,\n        image.image_build_log_uri,\n    )\n)\n\n# Writing the image details to \/aml_config\/image.json\nimage_json = {}\nimage_json[&quot;image_name&quot;] = image.name\nimage_json[&quot;image_version&quot;] = image.version\nimage_json[&quot;image_location&quot;] = image.image_location\nwith open(&quot;aml_config\/image.json&quot;, &quot;w&quot;) as outfile:\n    json.dump(image_json, outfile)\n<\/code><\/pre>\n<p>I tried to provide path to models but its fails saying package not found<\/p>\n<p>models = $(System.DefaultWorkingDirectory)\/package_model.pkl<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-07-21 01:44:56.507000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"azure|machine-learning|azure-machine-learning-studio|azure-machine-learning-service",
        "Question_view_count":140,
        "Owner_creation_date":"2019-08-31 00:00:56.790000 UTC",
        "Owner_last_access_date":"2022-09-16 17:26:27.993000 UTC",
        "Owner_location":null,
        "Owner_reputation":417,
        "Owner_up_votes":53,
        "Owner_down_votes":0,
        "Owner_views":233,
        "Answer_body":"<p>Register model:\nRegister a file or folder as a model by calling <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.model.model?view=azure-ml-py#register-workspace--model-path--model-name--tags-none--properties-none--description-none--datasets-none--model-framework-none--model-framework-version-none--child-paths-none-\" rel=\"nofollow noreferrer\">Model.register()<\/a>.<\/p>\n<p>In addition to the content of the model file itself, your registered model will also store model metadata -- model description, tags, and framework information -- that will be useful when managing and deploying models in your workspace. Using tags, for instance, you can categorize your models and apply filters when listing models in your workspace.<\/p>\n<pre><code>model = Model.register(workspace=ws,\n                       model_name='',                # Name of the registered model in your workspace.\n                       model_path='',  # Local file to upload and register as a model.\n                       model_framework=Model.Framework.SCIKITLEARN,  # Framework used to create the model.\n                       model_framework_version=sklearn.__version__,  # Version of scikit-learn used to create the model.\n                       sample_input_dataset=input_dataset,\n                       sample_output_dataset=output_dataset,\n                       resource_configuration=ResourceConfiguration(cpu=1, memory_in_gb=0.5),\n                       description='Ridge regression model to predict diabetes progression.',\n                       tags={'area': 'diabetes', 'type': 'regression'})\n\nprint('Name:', model.name)\nprint('Version:', model.version)\n<\/code><\/pre>\n<p>Deploy machine learning models to Azure: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-and-where?tabs=python\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-and-where?tabs=python<\/a><\/p>\n<p>To Troubleshooting remote model deployment Please follow the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-troubleshoot-deployment?tabs=azcli#function-fails-get_model_path\" rel=\"nofollow noreferrer\">document<\/a>.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/BL0Nm.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/BL0Nm.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-07-26 05:09:30.737000 UTC",
        "Answer_last_edit_date":"2021-07-26 05:54:33.550000 UTC",
        "Answer_score":1.0,
        "Question_valid_tags":[
            "azure-machine-learning-studio",
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":65509754,
        "Question_title":"Can ClearML (formerly Trains) work a local server?",
        "Question_body":"<p>I am trying to start my way with <a href=\"https:\/\/github.com\/allegroai\/clearml\" rel=\"nofollow noreferrer\">ClearML<\/a> (formerly known as Trains).<\/p>\n<p>I see on the <a href=\"https:\/\/allegro.ai\/clearml\/docs\/rst\/getting_started\/index.html\" rel=\"nofollow noreferrer\">documentation<\/a> that I need to have server running, either on the ClearML platform itself, or on a remote machine using AWS etc.<\/p>\n<p>I would really like to bypass this restriction and run experiments on my local machine, not connecting to any remote destination.<\/p>\n<p>According to <a href=\"https:\/\/allegro.ai\/clearml\/docs\/rst\/deploying_clearml\/index.html\" rel=\"nofollow noreferrer\">this<\/a> I can install the <code>trains-server<\/code> on any remote machine, so in theory I should also be able to install it on my local machine, but it still requires me to have Kubernetes or Docker, but I am not using any of them.<\/p>\n<p>Anyone had any luck using ClearML (or Trains, I think it's still quite the same API and all) on a local server?<\/p>\n<ul>\n<li>My OS is Ubuntu 18.04.<\/li>\n<\/ul>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2020-12-30 15:54:39.217000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-12-31 15:03:29.850000 UTC",
        "Question_score":5,
        "Question_tags":"trains|clearml",
        "Question_view_count":740,
        "Owner_creation_date":"2013-10-29 21:50:14.230000 UTC",
        "Owner_last_access_date":"2022-09-23 12:45:41.383000 UTC",
        "Owner_location":null,
        "Owner_reputation":2801,
        "Owner_up_votes":371,
        "Owner_down_votes":0,
        "Owner_views":131,
        "Answer_body":"<p>Disclaimer: I'm a member of the ClearML team (formerly Trains)<\/p>\n<blockquote>\n<p>I would really like to bypass this restriction and run experiments on my local machine, not connecting to any remote destination.<\/p>\n<\/blockquote>\n<p>A few options:<\/p>\n<ol>\n<li>The Clearml Free trier offers free hosting for your experiments, these experiment are only accessible to you, unless you specifically want to share them among your colleagues. This is probably the easiest way to <a href=\"https:\/\/app.community.clear.ml\" rel=\"nofollow noreferrer\">get started<\/a>.<\/li>\n<li>Install the ClearML-Server basically all you need is docker installed and you should be fine. There are full instructions <a href=\"https:\/\/clear.ml\/docs\/latest\/docs\/deploying_clearml\/clearml_server_linux_mac\/\" rel=\"nofollow noreferrer\">here<\/a> , this is the summary:<\/li>\n<\/ol>\n<pre class=\"lang-bash prettyprint-override\"><code>echo &quot;vm.max_map_count=262144&quot; &gt; \/tmp\/99-trains.conf\nsudo mv \/tmp\/99-trains.conf \/etc\/sysctl.d\/99-trains.conf\nsudo sysctl -w vm.max_map_count=262144\nsudo service docker restart\n\nsudo curl -L &quot;https:\/\/github.com\/docker\/compose\/releases\/latest\/download\/docker-compose-$(uname -s)-$(uname -m)&quot; -o \/usr\/local\/bin\/docker-compose\nsudo chmod +x \/usr\/local\/bin\/docker-compose\n\nsudo mkdir -p \/opt\/trains\/data\/elastic_7\nsudo mkdir -p \/opt\/trains\/data\/mongo\/db\nsudo mkdir -p \/opt\/trains\/data\/mongo\/configdb\nsudo mkdir -p \/opt\/trains\/data\/redis\nsudo mkdir -p \/opt\/trains\/logs\nsudo mkdir -p \/opt\/trains\/config\nsudo mkdir -p \/opt\/trains\/data\/fileserver\n\nsudo curl https:\/\/raw.githubusercontent.com\/allegroai\/trains-server\/master\/docker-compose.yml -o \/opt\/trains\/docker-compose.yml\ndocker-compose -f \/opt\/trains\/docker-compose.yml up -d\n<\/code><\/pre>\n<ol start=\"3\">\n<li>ClearML also supports full offline mode (i.e. no outside connection is made). Once your experiment completes, you can manually import the run to your server (either self hosted or free tier server)<\/li>\n<\/ol>\n<pre class=\"lang-py prettyprint-override\"><code>from clearml import Task\nTask.set_offline(True)\ntask = Task.init(project_name='examples', task_name='offline mode experiment')\n<\/code><\/pre>\n<p>When the process ends you will get a link to a zip file containing the output of the entire offline session:<\/p>\n<pre><code>ClearML Task: Offline session stored in \/home\/user\/.clearml\/cache\/offline\/offline-2d061bb57d9e408a9420c4fe81e26ad0.zip\n<\/code><\/pre>\n<p>Later you can import the session with:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from clearml import Task\nTask.import_offline_session('\/home\/user\/.clearml\/cache\/offline\/offline-2d061bb57d9e408a9420c4fe81e26ad0.zip')\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-12-30 16:18:59.523000 UTC",
        "Answer_last_edit_date":"2022-08-24 07:33:21.413000 UTC",
        "Answer_score":6.0,
        "Question_valid_tags":[
            "clearml"
        ]
    },
    {
        "Question_id":62980380,
        "Question_title":"How to load trained model in amazon sagemaker?",
        "Question_body":"<p>I am following <a href=\"https:\/\/github.com\/mtm12\/SageMakerDemo\" rel=\"noreferrer\">this example<\/a> on how to train a machine learning model in Amazon-sagemaker.<\/p>\n<pre><code>data_location = 's3:\/\/{}\/kmeans_highlevel_example\/data'.format(bucket)\noutput_location = 's3:\/\/{}\/kmeans_highlevel_example\/output'.format(bucket)\n\nprint('training data will be uploaded to: {}'.format(data_location))\nprint('training artifacts will be uploaded to: {}'.format(output_location))\n\nkmeans = KMeans(role=role,\n                train_instance_count=2,\n                train_instance_type='ml.c4.8xlarge',\n                output_path=output_location,\n                k=10,\n                epochs=100,\n                data_location=data_location)\n<\/code><\/pre>\n<p>So after calling the fit function the model should be saved in the S3 bucket?? How can you load this model next time?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-07-19 12:36:37.333000 UTC",
        "Question_favorite_count":2.0,
        "Question_last_edit_date":null,
        "Question_score":6,
        "Question_tags":"amazon-web-services|amazon-s3|amazon-sagemaker",
        "Question_view_count":4776,
        "Owner_creation_date":"2011-07-16 13:02:36.880000 UTC",
        "Owner_last_access_date":"2022-09-24 20:19:39.590000 UTC",
        "Owner_location":"Slovenia",
        "Owner_reputation":14913,
        "Owner_up_votes":307,
        "Owner_down_votes":1,
        "Owner_views":1093,
        "Answer_body":"<p>This can be done by using the sagemaker library combined with the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/model.html\" rel=\"noreferrer\">Inference Model<\/a>.<\/p>\n<pre><code>model = sagemaker.model.Model(\n    image=image\n    model_data='s3:\/\/bucket\/model.tar.gz',\n    role=role_arn)\n<\/code><\/pre>\n<p>The options you're passing in are:<\/p>\n<ul>\n<li><code>image<\/code> - This is the ECR image you're using for inference (which should be for the algorithm you're trying to use). Paths are available <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-algo-docker-registry-paths.html\" rel=\"noreferrer\">here<\/a>.<\/li>\n<li><code>model_data<\/code> - This is the path of where your model is stored (in a <code>tar.gz<\/code> compressed archive).<\/li>\n<li><code>role<\/code> - This is the arn of a role that is capable of both pulling the image from ECR and getting the s3 archive.<\/li>\n<\/ul>\n<p>Once you've successfully done this you will need to setup an endpoint, this can be done by performing the following in your notebook through the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/model.html#sagemaker.model.Model.deploy\" rel=\"noreferrer\">deploy function<\/a>.<\/p>\n<pre><code>model.deploy(\n   initial_instance_count=1,\n   instance_type='ml.p2.xlarge'\n)\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2020-07-19 12:49:45.810000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":8.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":33708725,
        "Question_title":"Azure Machine learning: error with multiclass classification algo",
        "Question_body":"<p>I have <a href=\"https:\/\/yadi.sk\/i\/--Vkm7FTkTAxY\" rel=\"nofollow noreferrer\">training set<\/a> and <a href=\"https:\/\/yadi.sk\/i\/wYu0ZsmukTAw3\" rel=\"nofollow noreferrer\">test set<\/a> (csv files with header), in which I have to classify each value. There is 118.000 uniq values in X column, and only about 13000 in y1 column, so there will be 13000 categories.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/Qc1i8.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Qc1i8.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>From Training set I need only <code>X<\/code> and <code>y1<\/code> column to train model. I need to classify X value to one of categories (find normal from of initial word). I tried all multi algo but failed trying to evaluate model.<\/p>\n\n<p>Visualizing Score model return this:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/EnDZq.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/EnDZq.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>What can be a problem, it just returns -2 code as error and this <a href=\"https:\/\/yadi.sk\/i\/I6WiEoCGkTCXc\" rel=\"nofollow noreferrer\">log<\/a><\/p>\n\n<p>UPD1: By Metadata Editor module under Project Column Module made column y1  as categorical,  nothing seems to be changed<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2015-11-14 13:08:11.170000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2016-02-01 04:23:57.020000 UTC",
        "Question_score":0,
        "Question_tags":"machine-learning|azure-machine-learning-studio",
        "Question_view_count":75,
        "Owner_creation_date":"2012-10-11 20:05:08.313000 UTC",
        "Owner_last_access_date":"2022-05-31 20:17:36.077000 UTC",
        "Owner_location":"Moscow, Russia",
        "Owner_reputation":4886,
        "Owner_up_votes":1215,
        "Owner_down_votes":15,
        "Owner_views":1228,
        "Answer_body":"<p><strong>Moncef<\/strong> provided <a href=\"https:\/\/social.msdn.microsoft.com\/Forums\/azure\/en-US\/aabb9080-adf8-4fc0-b332-2a3c8ee29a28\/azure-machine-learning-error-with-multiclass-classification-algo?forum=MachineLearning\" rel=\"nofollow\">here<\/a> the solution to my problem:<\/p>\n\n<p>The point is that Azure has limitations on maximum categories 8192, this is why the number should be decreased by R or python modules or own evaluation module may be created. Or there is another way, evaluation step may be skipped, because model`ve been trained successfully. <\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2015-11-23 09:44:23.193000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":68251533,
        "Question_title":"Why is AWS Sagemaker notebook instance designed to only persist data under ~\/Sagemaker?",
        "Question_body":"<p>In my current job we use AWS managed notebooks on Sagemaker EC2. I am largely okay with the user experience but the lack of data persistency outside <code>~\/Sagemaker<\/code> has been quite inconvenient. Every time should the instance need restarting, I'd lose all the settings and python packages. Wonder why AWS would make this particular decision for Sagemaker. Have used Google Cloud's AI platform before and it does not have such settings and my configurations would always persist.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-07-05 06:23:56.780000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":266,
        "Owner_creation_date":"2014-08-22 21:51:20.997000 UTC",
        "Owner_last_access_date":"2022-09-25 01:52:01.297000 UTC",
        "Owner_location":"Vancouver, BC, Canada",
        "Owner_reputation":2758,
        "Owner_up_votes":603,
        "Owner_down_votes":2,
        "Owner_views":122,
        "Answer_body":"<p>I faced a similar issue on other AWS services. Usually for managed services AWS uses read-only containers approach and leave just one folder of the filesystem for read\/write that persist across the stop\/restart cycle.\nReguarding the packages installation the seems to be to install your custom environment on the notebook instance's Amazon EBS volume, as described <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/nbi-add-external.html\" rel=\"nofollow noreferrer\">here<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-07-05 12:24:52.607000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":62261222,
        "Question_title":"Workspace Authentication: More than one token matches the criteria",
        "Question_body":"<p>I constantly run into problems when working on Azure Compute Instances and trying to connect from the Jupyter Lab to the workspace.<\/p>\n<p>With InteractiveLoginAuthentication I get the following message:<\/p>\n<pre><code>AuthenticationException: AuthenticationException:\n    Message: Could not retrieve user token. Please run 'az login'\n    InnerException More than one token matches the criteria. The result is ambiguous.\n    ErrorResponse \n{\n    &quot;error&quot;: {\n        &quot;code&quot;: &quot;UserError&quot;,\n        &quot;inner_error&quot;: {\n            &quot;code&quot;: &quot;Authentication&quot;\n        },\n        &quot;message&quot;: &quot;Could not retrieve user token. Please run 'az login'&quot;\n    }\n}\n<\/code><\/pre>\n<p>With a Service Principal this one (SP is owner in the ML Workspace):<\/p>\n<pre><code>WorkspaceException: WorkspaceException:\n    Message: No workspaces found with name=xxx in all the subscriptions that you have access to.\n    InnerException None\n    ErrorResponse \n{\n    &quot;error&quot;: {\n        &quot;message&quot;: &quot;No workspaces found with name=xxx in all the subscriptions that you have access to.&quot;\n    }\n}\n<\/code><\/pre>\n<p>I had another workspace in a different subscription where I could resolve it by giving the tennant as an extra input to the InteractiveLoginAuthentication. This time, no chance.<\/p>\n<p>The funny thing is, though, that I can login to the workspace via InteractiveLoginAuthentication when doing it from my local computer.<\/p>\n<p>I supsected that some old tokens are cached somewhere so I tried to use the &quot;Private browsing&quot; function of my browser. Furthermore, I deleted <code>\/home\/azureuser\/.azure\/accessTokens.json<\/code> but no effect.<\/p>\n<p>Maybe some of you had this problem before and have an idea?<\/p>\n<p>For reference some sites I checked:<\/p>\n<ul>\n<li><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-setup-authentication\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-setup-authentication<\/a><\/li>\n<li><a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/manage-azureml-service\/authentication-in-azureml\/authentication-in-azureml.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/manage-azureml-service\/authentication-in-azureml\/authentication-in-azureml.ipynb<\/a><\/li>\n<li><a href=\"https:\/\/github.com\/Azure\/azure-cli\/issues\/4618\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/azure-cli\/issues\/4618<\/a><\/li>\n<li><a href=\"https:\/\/github.com\/Azure\/azure-cli\/issues\/6147\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/azure-cli\/issues\/6147<\/a><\/li>\n<\/ul>\n<h1>Update<\/h1>\n<p>When I run this code:<\/p>\n<pre><code>from azureml.core.authentication import InteractiveLoginAuthentication\ninteractive_auth = InteractiveLoginAuthentication(tenant_id='xxx')\n\nws = Workspace.get(name='xxx',\n                   subscription_id='xxx',\n                   resource_group='xxx',\n                   auth=interactive_auth)\n<\/code><\/pre>\n<p>I get the following trace:<\/p>\n<pre><code>---------------------------------------------------------------------------\nAdalError                                 Traceback (most recent call last)\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in _get_arm_token_with_refresh(profile_object, cloud_type, account_object, config_object, session_object, config_directory, force_reload, resource)\n   1820         auth, _, _ = profile_object.get_login_credentials(resource)\n-&gt; 1821         access_token = auth._token_retriever()[1]\n   1822         if (_get_exp_time(access_token) - time.time()) &lt; _TOKEN_REFRESH_THRESHOLD_SEC:\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_vendor\/azure_cli_core\/_profile.py in _retrieve_token()\n    525                     return self._creds_cache.retrieve_token_for_user(username_or_sp_id,\n--&gt; 526                                                                      account[_TENANT_ID], resource)\n    527                 use_cert_sn_issuer = account[_USER_ENTITY].get(_SERVICE_PRINCIPAL_CERT_SN_ISSUER_AUTH)\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_vendor\/azure_cli_core\/_profile.py in retrieve_token_for_user(self, username, tenant, resource)\n    889         context = self._auth_ctx_factory(self._cloud_type, tenant, cache=self.adal_token_cache)\n--&gt; 890         token_entry = context.acquire_token(resource, username, _CLIENT_ID)\n    891         if not token_entry:\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/authentication_context.py in acquire_token(self, resource, user_id, client_id)\n    144 \n--&gt; 145         return self._acquire_token(token_func)\n    146 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/authentication_context.py in _acquire_token(self, token_func, correlation_id)\n    127         self.authority.validate(self._call_context)\n--&gt; 128         return token_func(self)\n    129 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/authentication_context.py in token_func(self)\n    142             token_request = TokenRequest(self._call_context, self, client_id, resource)\n--&gt; 143             return token_request.get_token_from_cache_with_refresh(user_id)\n    144 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/token_request.py in get_token_from_cache_with_refresh(self, user_id)\n    346         self._user_id = user_id\n--&gt; 347         return self._find_token_from_cache()\n    348 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/token_request.py in _find_token_from_cache(self)\n    126         cache_query = self._create_cache_query()\n--&gt; 127         return self._cache_driver.find(cache_query)\n    128 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/cache_driver.py in find(self, query)\n    195                         {&quot;query&quot;: log.scrub_pii(query)})\n--&gt; 196         entry, is_resource_tenant_specific = self._load_single_entry_from_cache(query)\n    197         if entry:\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/cache_driver.py in _load_single_entry_from_cache(self, query)\n    123             else:\n--&gt; 124                 raise AdalError('More than one token matches the criteria. The result is ambiguous.')\n    125 \n\nAdalError: More than one token matches the criteria. The result is ambiguous.\n\nDuring handling of the above exception, another exception occurred:\n\nAuthenticationException                   Traceback (most recent call last)\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in wrapper(self, *args, **kwargs)\n    288                     module_logger.debug(&quot;{} acquired lock in {} s.&quot;.format(type(self).__name__, duration))\n--&gt; 289                 return test_function(self, *args, **kwargs)\n    290             except Exception as e:\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in _get_arm_token(self)\n    474         else:\n--&gt; 475             return self._get_arm_token_using_interactive_auth()\n    476 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in _get_arm_token_using_interactive_auth(self, force_reload, resource)\n    589         arm_token = _get_arm_token_with_refresh(profile_object, cloud_type, ACCOUNT, CONFIG, SESSION,\n--&gt; 590                                                 get_config_dir(), force_reload=force_reload, resource=resource)\n    591         # If a user has specified a tenant id then we need to check if this token is for that tenant.\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in connection_aborted_wrapper(*args, **kwargs)\n    325                 try:\n--&gt; 326                     return function(*args, **kwargs)\n    327                 except AuthenticationException as e:\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in _get_arm_token_with_refresh(profile_object, cloud_type, account_object, config_object, session_object, config_directory, force_reload, resource)\n   1829             raise AuthenticationException(&quot;Could not retrieve user token. Please run 'az login'&quot;,\n-&gt; 1830                                           inner_exception=e)\n   1831 \n\nAuthenticationException: AuthenticationException:\n    Message: Could not retrieve user token. Please run 'az login'\n    InnerException More than one token matches the criteria. The result is ambiguous.\n    ErrorResponse \n{\n    &quot;error&quot;: {\n        &quot;code&quot;: &quot;UserError&quot;,\n        &quot;inner_error&quot;: {\n            &quot;code&quot;: &quot;Authentication&quot;\n        },\n        &quot;message&quot;: &quot;Could not retrieve user token. Please run 'az login'&quot;\n    }\n}\n\nDuring handling of the above exception, another exception occurred:\n\nAdalError                                 Traceback (most recent call last)\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in _get_arm_token_with_refresh(profile_object, cloud_type, account_object, config_object, session_object, config_directory, force_reload, resource)\n   1820         auth, _, _ = profile_object.get_login_credentials(resource)\n-&gt; 1821         access_token = auth._token_retriever()[1]\n   1822         if (_get_exp_time(access_token) - time.time()) &lt; _TOKEN_REFRESH_THRESHOLD_SEC:\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_vendor\/azure_cli_core\/_profile.py in _retrieve_token()\n    525                     return self._creds_cache.retrieve_token_for_user(username_or_sp_id,\n--&gt; 526                                                                      account[_TENANT_ID], resource)\n    527                 use_cert_sn_issuer = account[_USER_ENTITY].get(_SERVICE_PRINCIPAL_CERT_SN_ISSUER_AUTH)\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_vendor\/azure_cli_core\/_profile.py in retrieve_token_for_user(self, username, tenant, resource)\n    889         context = self._auth_ctx_factory(self._cloud_type, tenant, cache=self.adal_token_cache)\n--&gt; 890         token_entry = context.acquire_token(resource, username, _CLIENT_ID)\n    891         if not token_entry:\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/authentication_context.py in acquire_token(self, resource, user_id, client_id)\n    144 \n--&gt; 145         return self._acquire_token(token_func)\n    146 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/authentication_context.py in _acquire_token(self, token_func, correlation_id)\n    127         self.authority.validate(self._call_context)\n--&gt; 128         return token_func(self)\n    129 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/authentication_context.py in token_func(self)\n    142             token_request = TokenRequest(self._call_context, self, client_id, resource)\n--&gt; 143             return token_request.get_token_from_cache_with_refresh(user_id)\n    144 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/token_request.py in get_token_from_cache_with_refresh(self, user_id)\n    346         self._user_id = user_id\n--&gt; 347         return self._find_token_from_cache()\n    348 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/token_request.py in _find_token_from_cache(self)\n    126         cache_query = self._create_cache_query()\n--&gt; 127         return self._cache_driver.find(cache_query)\n    128 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/cache_driver.py in find(self, query)\n    195                         {&quot;query&quot;: log.scrub_pii(query)})\n--&gt; 196         entry, is_resource_tenant_specific = self._load_single_entry_from_cache(query)\n    197         if entry:\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/cache_driver.py in _load_single_entry_from_cache(self, query)\n    123             else:\n--&gt; 124                 raise AdalError('More than one token matches the criteria. The result is ambiguous.')\n    125 \n\nAdalError: More than one token matches the criteria. The result is ambiguous.\n\nDuring handling of the above exception, another exception occurred:\n\nAuthenticationException                   Traceback (most recent call last)\n&lt;ipython-input-2-fd1276999d15&gt; in &lt;module&gt;\n      5                    subscription_id='00c983e5-d766-480b-be75-abf95d1a46c3',\n      6                    resource_group='BusinessIntelligence',\n----&gt; 7                    auth=interactive_auth)\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/workspace.py in get(name, auth, subscription_id, resource_group)\n    547 \n    548         result_dict = Workspace.list(\n--&gt; 549             subscription_id, auth=auth, resource_group=resource_group)\n    550         result_dict = {k.lower(): v for k, v in result_dict.items()}\n    551         name = name.lower()\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/workspace.py in list(subscription_id, auth, resource_group)\n    637         elif subscription_id and resource_group:\n    638             workspaces_list = Workspace._list_legacy(\n--&gt; 639                 auth, subscription_id=subscription_id, resource_group_name=resource_group)\n    640 \n    641             Workspace._process_autorest_workspace_list(\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/workspace.py in _list_legacy(auth, subscription_id, resource_group_name, ignore_error)\n   1373                 return None\n   1374             else:\n-&gt; 1375                 raise e\n   1376 \n   1377     @staticmethod\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/workspace.py in _list_legacy(auth, subscription_id, resource_group_name, ignore_error)\n   1367             # azureml._base_sdk_common.workspace.models.workspace.Workspace\n   1368             workspace_autorest_list = _commands.list_workspace(\n-&gt; 1369                 auth, subscription_id=subscription_id, resource_group_name=resource_group_name)\n   1370             return workspace_autorest_list\n   1371         except Exception as e:\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_project\/_commands.py in list_workspace(auth, subscription_id, resource_group_name)\n    386         if resource_group_name:\n    387             list_object = WorkspacesOperations.list_by_resource_group(\n--&gt; 388                 auth._get_service_client(AzureMachineLearningWorkspaces, subscription_id).workspaces,\n    389                 resource_group_name)\n    390             workspace_list = list_object.value\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in _get_service_client(self, client_class, subscription_id, subscription_bound, base_url)\n    155         # in the multi-tenant case, which causes confusion.\n    156         if subscription_id:\n--&gt; 157             all_subscription_list, tenant_id = self._get_all_subscription_ids()\n    158             self._check_if_subscription_exists(subscription_id, all_subscription_list, tenant_id)\n    159 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in _get_all_subscription_ids(self)\n    497         :rtype: list, str\n    498         &quot;&quot;&quot;\n--&gt; 499         arm_token = self._get_arm_token()\n    500         return self._get_all_subscription_ids_internal(arm_token)\n    501 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in wrapper(self, *args, **kwargs)\n    293                     InteractiveLoginAuthentication(force=True, tenant_id=self._tenant_id)\n    294                     # Try one more time\n--&gt; 295                     return test_function(self, *args, **kwargs)\n    296                 else:\n    297                     raise e\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in _get_arm_token(self)\n    473             return self._ambient_auth._get_arm_token()\n    474         else:\n--&gt; 475             return self._get_arm_token_using_interactive_auth()\n    476 \n    477     @_login_on_failure_decorator(_interactive_auth_lock)\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in _get_arm_token_using_interactive_auth(self, force_reload, resource)\n    588         profile_object = Profile(async_persist=False, cloud_type=cloud_type)\n    589         arm_token = _get_arm_token_with_refresh(profile_object, cloud_type, ACCOUNT, CONFIG, SESSION,\n--&gt; 590                                                 get_config_dir(), force_reload=force_reload, resource=resource)\n    591         # If a user has specified a tenant id then we need to check if this token is for that tenant.\n    592         if self._tenant_id and fetch_tenantid_from_aad_token(arm_token) != self._tenant_id:\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in connection_aborted_wrapper(*args, **kwargs)\n    324             while True:\n    325                 try:\n--&gt; 326                     return function(*args, **kwargs)\n    327                 except AuthenticationException as e:\n    328                     if &quot;Connection aborted.&quot; in str(e) and attempt &lt;= retries:\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in _get_arm_token_with_refresh(profile_object, cloud_type, account_object, config_object, session_object, config_directory, force_reload, resource)\n   1828         if not token_about_to_expire:\n   1829             raise AuthenticationException(&quot;Could not retrieve user token. Please run 'az login'&quot;,\n-&gt; 1830                                           inner_exception=e)\n   1831 \n   1832     try:\n\nAuthenticationException: AuthenticationException:\n    Message: Could not retrieve user token. Please run 'az login'\n    InnerException More than one token matches the criteria. The result is ambiguous.\n    ErrorResponse \n{\n    &quot;error&quot;: {\n        &quot;code&quot;: &quot;UserError&quot;,\n        &quot;inner_error&quot;: {\n            &quot;code&quot;: &quot;Authentication&quot;\n        },\n        &quot;message&quot;: &quot;Could not retrieve user token. Please run 'az login'&quot;\n    }\n}\n<\/code><\/pre>\n<ul>\n<li><code>azureml-sdk<\/code> is on version 1.9.0<\/li>\n<li>I can connect an authenticate from my local machine. Problems only occur when I want to work on a compute instance.<\/li>\n<\/ul>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_date":"2020-06-08 11:38:19.803000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-07-09 15:28:37.017000 UTC",
        "Question_score":3,
        "Question_tags":"azure|azure-machine-learning-service",
        "Question_view_count":2737,
        "Owner_creation_date":"2020-05-11 13:50:20.747000 UTC",
        "Owner_last_access_date":"2022-09-15 09:14:33.430000 UTC",
        "Owner_location":"Germany",
        "Owner_reputation":163,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":16,
        "Answer_body":"<p>Okay, here is the answer:<\/p>\n<ul>\n<li>You work for company A which is on Azure.<\/li>\n<li>You get access to company B's subscription.<\/li>\n<li>Problem is: You are associated to A's AAD in ML-Studio.<\/li>\n<li>You need to specify the tenant ID in the <code>InteractiveLoginAuthentication<\/code> like so:<\/li>\n<\/ul>\n<pre><code>interactive_auth = InteractiveLoginAuthentication(tenant_id=tenant_id)\n\nworkspace = Workspace.get(name=workspace_name,\n                          subscription_id=subscription_id,\n                          resource_group=resource_group,\n                          auth=interactive_auth)\n<\/code><\/pre>\n<ul>\n<li>Now the <strong>important<\/strong> part: You need to use company B's <code>tenant_id<\/code> (I used company A's all the time since I thought that was my authentication point)<\/li>\n<li>Of course, this is obvious while you read it...as it is to me now :)<\/li>\n<\/ul>\n<p>Hope this helps you. Took me some time but learned a lot ;)<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2020-07-29 14:52:50.720000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_valid_tags":[
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":37183494,
        "Question_title":"Azure machine learning predict order for customer",
        "Question_body":"<p>I have created a new experiment in Azure Machine Learning and added two datasets by manually uploading csv's.<\/p>\n\n<ul>\n<li>One is from a customer of which I'd like to predict which products he will order next. <\/li>\n<li>The second dataset has the same type of data, only then from all other customers as reference for learning.<\/li>\n<\/ul>\n\n<p>I have <code>productid<\/code>, <code>amount<\/code>, and <code>orderdate<\/code> and <code>orderid<\/code> for grouping and putting it on a timeframe.\nThe customer (dataset one) is always several months behind with ordering the latest products. therefor I added the dataset two with all other customers as reference.<\/p>\n\n<p>Also because the reference can tell which products are more popular (ordered more and by several customers) so perhaps I should add a customerid column to the dataset.<\/p>\n\n<p>I know how to start and get the data in, and I do know that it is common to split the data for training, feed it to the train model with a <code>Ilearnerdotnet<\/code> type and give the output to the score model and evaluate the model.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/qH7lb.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/qH7lb.png\" alt=\"current experiment\"><\/a>\nI do not know how to choose a classification type and how this can give an output for the next three months of order. I have read some tutorials, but I just need someone who can give me some pointers.<\/p>\n\n<p><strong>edit<\/strong> I have added the customerid to the dataset so that I have just one set now which I should split to focus on a specific customer.\n<strong>edit2<\/strong> found these templates. will look into it <a href=\"https:\/\/stackoverflow.com\/a\/36552849\/169714\">https:\/\/stackoverflow.com\/a\/36552849\/169714<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2016-05-12 09:58:06.927000 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":"2017-05-23 12:06:43.567000 UTC",
        "Question_score":2,
        "Question_tags":"azure|azure-machine-learning-studio",
        "Question_view_count":845,
        "Owner_creation_date":"2009-09-07 13:35:28.847000 UTC",
        "Owner_last_access_date":"2022-01-31 12:39:52.117000 UTC",
        "Owner_location":"Netherlands",
        "Owner_reputation":5867,
        "Owner_up_votes":2127,
        "Owner_down_votes":9,
        "Owner_views":1692,
        "Answer_body":"<p>Go over this <a href=\"http:\/\/download.microsoft.com\/download\/0\/5\/A\/05AE6B94-E688-403E-90A5-6035DBE9EEC5\/machine-learning-basics-infographic-with-algorithm-examples.pdf\" rel=\"nofollow\">http:\/\/download.microsoft.com\/download\/0\/5\/A\/05AE6B94-E688-403E-90A5-6035DBE9EEC5\/machine-learning-basics-infographic-with-algorithm-examples.pdf<\/a><\/p>\n\n<p>If above infographic doesn't help, then you can try all of the learners by going over this experiment and use the one with best results - <a href=\"https:\/\/gallery.cortanaintelligence.com\/Experiment\/Algo-Evaluater-Compare-Performance-of-Multiple-Algos-against-Your-Data-1\" rel=\"nofollow\">https:\/\/gallery.cortanaintelligence.com\/Experiment\/Algo-Evaluater-Compare-Performance-of-Multiple-Algos-against-Your-Data-1<\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2016-05-12 21:56:26.117000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":3.0,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":62111580,
        "Question_title":"Trouble opening audio files stored on S3 in SageMaker",
        "Question_body":"<p>I stored like 300 GB of audio data (mp3\/wav mostly) on Amazon S3 and am trying to access it in a SageMaker notebook instance to do some data transformations. I'm trying to use either torchaudio or librosa to load a file as a waveform. torchaudio expects the file path as the input, librosa can either use a file path or file-like object. I tried using s3fs to get the url to the file but torchaudio doesn't recognize it as a file. And apparently SageMaker has problems installing librosa so I can't use that. What should I do?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_date":"2020-05-31 04:28:02.700000 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-s3|pytorch|amazon-sagemaker",
        "Question_view_count":749,
        "Owner_creation_date":"2020-05-31 04:23:53.110000 UTC",
        "Owner_last_access_date":"2022-06-17 22:34:42.813000 UTC",
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Answer_body":"<p>I ended up not using SageMaker for this, but for anybody else having similar problems, I solved this by opening the file using s3fs and writing it to a <code>tempfile.NamedTemporaryFile<\/code>. This gave me a file path that I could pass into either <code>torchaudio.load<\/code> or <code>librosa.core.load<\/code>. This was also important because I wanted the extra resampling functionality of <code>librosa.core.load<\/code>, but it doesn't accept file-like objects for loading mp3s.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-06-15 21:46:02.683000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":72166808,
        "Question_title":"OpenCV issue while using DNN implementation with any version in Machine Learning Services",
        "Question_body":"<p>I am using Machine Learning Services and when I am trying to implement Deep Neural Network, I am getting CV2 issue. The CV2 library is being bothering the code block. The following is the error I am getting when I am trying to use CV2 for DNN_BACKEND_CUDA.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/hTSXM.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/hTSXM.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Any help is appreciable.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":4,
        "Question_creation_date":"2022-05-09 03:50:34.317000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-05-09 10:31:18.250000 UTC",
        "Question_score":1,
        "Question_tags":"python|opencv|computer-vision|azure-machine-learning-service",
        "Question_view_count":86,
        "Owner_creation_date":"2022-04-27 21:21:09.217000 UTC",
        "Owner_last_access_date":"2022-08-17 18:50:07.473000 UTC",
        "Owner_location":null,
        "Owner_reputation":41,
        "Owner_up_votes":7,
        "Owner_down_votes":0,
        "Owner_views":30,
        "Answer_body":"<p>The issue raised is very rare and there are less chances of getting the success rate even after the proper installation of libraries. When the code was deployed in Azure Machine Learning some of the issues might be resolved. Checkout the following steps to be taken care of:<\/p>\n<ol>\n<li>Check with the version of Open CV<\/li>\n<\/ol>\n<p><code>import cv2<\/code><\/p>\n<p><code>cv2.__version__<\/code><\/p>\n<ol start=\"2\">\n<li>After installation, implement the following steps<\/li>\n<\/ol>\n<p>these steps are very much time taking.<\/p>\n<pre><code>%cd \/content\n!git clone https:\/\/github.com\/opencv\/opencv\n!git clone https:\/\/github.com\/opencv_contrib\n!mkdir \/content\/build\n%cd \/content\/build\n!cmake -DOPENCV_EXTRA_MODULES_PATH=\/content\/opencv_contrib\/modules  -DBUILD_SHARED_LIBS=OFF  -DBUILD_TESTS=OFF  -DBUILD_PERF_TESTS=OFF -DBUILD_EXAMPLES=OFF -DWITH_OPENEXR=OFF -DWITH_CUDA=ON -DWITH_CUBLAS=ON -DWITH_CUDNN=ON -DOPENCV_DNN_CUDA=ON \/content\/opencv\n!make -j8 install\n<\/code><\/pre>\n<ol start=\"3\">\n<li>Check the version of Open CV again.<\/li>\n<\/ol>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2022-05-09 04:51:23.273000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":65556574,
        "Question_title":"How to make prediction after model registration in azure?",
        "Question_body":"<p>I created a simply model and then registered in azure. How can I make a prediction?<\/p>\n<pre><code>from sklearn import svm\nimport joblib\nimport numpy as np\n\n# customer ages\nX_train = np.array([50, 17, 35, 23, 28, 40, 31, 29, 19, 62])\nX_train = X_train.reshape(-1, 1)\n# churn y\/n\ny_train = [&quot;yes&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;yes&quot;, &quot;yes&quot;, &quot;yes&quot;, &quot;no&quot;, &quot;no&quot;, &quot;yes&quot;]\n\nclf = svm.SVC(gamma=0.001, C=100.)\nclf.fit(X_train, y_train)\n\njoblib.dump(value=clf, filename=&quot;churn-model.pkl&quot;)\n<\/code><\/pre>\n<p>Registration:<\/p>\n<pre><code>from azureml.core import Workspace\nws = Workspace.get(name=&quot;myworkspace&quot;, subscription_id='My_subscription_id', resource_group='ML_Lingaro')\n\nfrom azureml.core.model import Model\nmodel = Model.register(workspace=ws, model_path=&quot;churn-model.pkl&quot;, model_name=&quot;churn-model-test&quot;)\n<\/code><\/pre>\n<p>Prediction:<\/p>\n<pre><code>from azureml.core.model import Model\nimport os\n\nmodel = Model(workspace=ws, name=&quot;churn-model-test&quot;)\nX_test = np.array([50, 17, 35, 23, 28, 40, 31, 29, 19, 62])\nmodel.predict(X_test) ???? \n<\/code><\/pre>\n<p>Error: <code>AttributeError: 'Model' object has no attribute 'predict'<\/code><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-01-04 01:09:35.690000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-01-04 01:24:17.017000 UTC",
        "Question_score":3,
        "Question_tags":"azure|azure-machine-learning-service",
        "Question_view_count":540,
        "Owner_creation_date":"2020-11-20 01:00:01.337000 UTC",
        "Owner_last_access_date":"2021-04-10 20:06:50.227000 UTC",
        "Owner_location":null,
        "Owner_reputation":73,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":24,
        "Answer_body":"<p>great question -- I also had the same misconception starting out. The missing piece is that there's a difference between model 'registration' and model 'deployment'. Registration is simply for tracking and for easy downloading at a later place and time. Deployment is what you're after, making a model available to be scored against.<\/p>\n<p>There's a <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-and-where?tabs=python&amp;WT.mc_id=AI-MVP-5003930\" rel=\"nofollow noreferrer\">whole section in the docs about deployment<\/a>. My suggestion would be to deploy it locally first for testing.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-01-04 05:20:26.627000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_valid_tags":[
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":60122070,
        "Question_title":"AWS SageMaker PyTorch: no module named 'sagemaker'",
        "Question_body":"<p>I have deployed a PyTorch model on AWS with SageMaker, and I try to send a request to test the service. However, I got a very vague error message saying \"no module named 'sagemaker'\". I have tried to search online, but cannot find posts about similar message.<\/p>\n\n<p>My client code: <\/p>\n\n<pre><code>import numpy as np\nfrom sagemaker.pytorch.model import PyTorchPredictor\n\nENDPOINT = '&lt;endpoint name&gt;'\n\npredictor = PyTorchPredictor(ENDPOINT)\npredictor.predict(np.random.random_sample([1, 3, 224, 224]).tobytes())\n<\/code><\/pre>\n\n<p>Detailed error message:<\/p>\n\n<pre><code>Traceback (most recent call last):\n  File \"client.py\", line 7, in &lt;module&gt;\n    predictor.predict(np.random.random_sample([1, 3, 224, 224]).tobytes())\n  File \"\/Users\/jiashenc\/Env\/py3\/lib\/python3.7\/site-packages\/sagemaker\/predictor.py\", line 110, in predict\n    response = self.sagemaker_session.sagemaker_runtime_client.invoke_endpoint(**request_args)\n  File \"\/Users\/jiashenc\/Env\/py3\/lib\/python3.7\/site-packages\/botocore\/client.py\", line 276, in _api_call\n    return self._make_api_call(operation_name, kwargs)\n  File \"\/Users\/jiashenc\/Env\/py3\/lib\/python3.7\/site-packages\/botocore\/client.py\", line 586, in _make_api_call\n    raise error_class(parsed_response, operation_name)\nbotocore.errorfactory.ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from model with message \"No module named 'sagemaker'\". See https:\/\/us-east-2.console.aws.amazon.com\/cloudwatch\/home?region=us-east-2#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/&lt;endpoint name&gt; in account xxxxxxxxxxxxxx for more information.\n<\/code><\/pre>\n\n<p>This bug is because I merge both the serving script and my deploy script together, see below<\/p>\n\n<pre><code>import os\nimport torch\nimport numpy as np\nfrom sagemaker.pytorch.model import PyTorchModel\nfrom torch import cuda\nfrom torchvision.models import resnet50\n\n\ndef model_fn(model_dir):\n    device = torch.device('cuda' if cuda.is_available() else 'cpu')\n    model = resnet50()\n    with open(os.path.join(model_dir, 'model.pth'), 'rb') as f:\n        model.load_state_dict(torch.load(f, map_location=device))\n    return model.to(device)\n\ndef predict_fn(input_data, model):\n    device = torch.device('cuda' if cuda.is_available() else 'cpu')\n    model.eval()\n    with torch.no_grad():\n        return model(input_data.to(device))\n\n\nif __name__ == '__main__':\n    pytorch_model = PyTorchModel(model_data='s3:\/\/&lt;bucket name&gt;\/resnet50\/model.tar.gz',\n                                    entry_point='serve.py', role='jiashenC-sagemaker',\n                                    py_version='py3', framework_version='1.3.1')\n    predictor = pytorch_model.deploy(instance_type='ml.t2.medium', initial_instance_count=1)\n    print(predictor.predict(np.random.random_sample([1, 3, 224, 224]).astype(np.float32)))\n<\/code><\/pre>\n\n<p>The root cause is the 4th line in my code. It tries to import sagemaker, which is an unavailable library. <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_date":"2020-02-07 22:41:06.463000 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":"2020-02-09 15:07:22.870000 UTC",
        "Question_score":1,
        "Question_tags":"python|amazon-web-services|amazon-sagemaker",
        "Question_view_count":5568,
        "Owner_creation_date":"2017-03-16 03:15:12.740000 UTC",
        "Owner_last_access_date":"2022-09-25 01:39:19.960000 UTC",
        "Owner_location":"Atlanta, GA, USA",
        "Owner_reputation":1653,
        "Owner_up_votes":60,
        "Owner_down_votes":0,
        "Owner_views":72,
        "Answer_body":"<p><em>(edit 2\/9\/2020 with extra code snippets)<\/em><\/p>\n\n<p>Your serving code tries to use the <code>sagemaker<\/code> module internally. The <code>sagemaker<\/code> module (also called <a href=\"http:\/\/sagemaker.readthedocs.io\" rel=\"nofollow noreferrer\">SageMaker Python SDK<\/a>, one of the numerous orchestration SDKs for SageMaker) is not designed to be used in model containers, but instead out of models, to orchestrate their activity (train, deploy, bayesian tuning, etc). In your specific example, you shouldn't include the deployment and model call code to server code, as those are actually actions that will be conducted from outside the server to orchestrate its lifecyle and interact with it. For model deployment with the Sagemaker Pytorch container, your entry point script just needs to contain the required <code>model_fn<\/code> function for model deserialization, and optionally an <code>input_fn<\/code>, <code>predict_fn<\/code> and <code>output_fn<\/code>, respectively for pre-processing, inference and post-processing (<a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_pytorch.html#the-sagemaker-pytorch-model-server\" rel=\"nofollow noreferrer\">detailed in the documentation here<\/a>). This logic is beautiful :) : you don't need anything else to deploy a production-ready deep learning server! (MMS in the case of Pytorch and MXNet, Flask+Gunicorn in the case of sklearn).<\/p>\n\n<p>In summary, this is how your code should be split:<\/p>\n\n<p>An entry_point script <code>serve.py<\/code> that contains model serving code and looks like this:<\/p>\n\n<pre><code>import os\n\nimport numpy as np\nimport torch\nfrom torch import cuda\nfrom torchvision.models import resnet50\n\ndef model_fn(model_dir):\n    # TODO instantiate a model from its artifact stored in model_dir\n    return model\n\ndef predict_fn(input_data, model):\n    # TODO apply model to the input_data, return result of interest\n    return result\n<\/code><\/pre>\n\n<p>and some orchestration code to instantiate a SageMaker Model object, deploy it to a server and query it. This is run from the orchestration runtime of your choice, which could be a SageMaker Notebook, your laptop, an AWS Lambda function, an Apache Airflow operator, etc - and with the SDK for your choice; don't need to use python for this.<\/p>\n\n<pre><code>import numpy as np\nfrom sagemaker.pytorch.model import PyTorchModel\n\npytorch_model = PyTorchModel(\n    model_data='s3:\/\/&lt;bucket name&gt;\/resnet50\/model.tar.gz',\n    entry_point='serve.py',\n    role='jiashenC-sagemaker',\n    py_version='py3',\n    framework_version='1.3.1')\n\npredictor = pytorch_model.deploy(instance_type='ml.t2.medium', initial_instance_count=1)\n\nprint(predictor.predict(np.random.random_sample([1, 3, 224, 224]).astype(np.float32)))\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2020-02-08 19:23:03.900000 UTC",
        "Answer_last_edit_date":"2020-02-09 17:29:35.957000 UTC",
        "Answer_score":4.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":62919671,
        "Question_title":"conda env build fails with \"[Errno 28] No space left on device\"",
        "Question_body":"<p>I'm trying to build a new conda environment in our Sagemaker ec2 environment in a terminal session.  Packages in the original copy of the environment were corrupted, and the environment became unusable. The issue couldn't be fixed by removing packages and re-installing or using <code>conda update<\/code>.<\/p>\n<p>I nuked the environment with <code>conda env remove -n python3-cn<\/code> and then attempted to recreate the environment with:<\/p>\n<pre><code>conda env create -p \/home\/ec2-user\/SageMaker\/anaconda3\/envs\/python3-cn --file=${HOME}\/SageMaker\/efs\/.sagemaker\/python3-cn_environment.yml --force\n<\/code><\/pre>\n<p>This environment has been created a number of times in several ec2 instances for individual Sagemaker users.<\/p>\n<p>Conda logs the following:<\/p>\n<pre><code>Collecting package metadata (repodata.json): done\nSolving environment: done\n\nDownloading and Extracting Packages\npytest-arraydiff-0.2 | 14 KB     | ##################################################################################################### | 100% \npartd-0.3.8          | 32 KB     | ##################################################################################################### | 100% \n\n... several progress bar lines later...\n\npsycopg2-2.7.5       | 507 KB    | ##################################################################################################### | 100% \nPreparing transaction: done\nVerifying transaction: done\nExecuting transaction: done\nERROR conda.core.link:_execute(700): An error occurred while installing package 'defaults::mkl-2018.0.3-1'.\nRolling back transaction: done\n\n[Errno 28] No space left on device\n()\n<\/code><\/pre>\n<p>The <code>No space left on device<\/code> error is consistent. I've tried<\/p>\n<ul>\n<li><code>conda clean --all<\/code>, removing the environment, re-building the environment<\/li>\n<li>removing the caches, removing the environment, re-building the environment<\/li>\n<li>removing the environment, shutting down and restarting JuypiterLab (our Sagemaker is configured to create <code>python3-cn<\/code> if the environment doesn't exist when JupyterLab starts)<\/li>\n<\/ul>\n<p>In the first two, I get <code>Errno 28<\/code>.<\/p>\n<p>In the last one, the instance is not created, <code>conda env list<\/code> does not show the <code>python3-cn<\/code>, but I see there is a <code>python3-cn<\/code> directory in the <code>anaconda\/envs\/<\/code> directory. If I do <code>conda activate python3-cn<\/code>, I see the prompt change, but the environment is unusuable. If I try <code>conda update --all<\/code>, I get a notification that one of the package files has been corrupted.<\/p>\n<p>Not really sure what to do here. I'm looking for space hogs, but not really finding anything significant.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2020-07-15 16:38:26.687000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"conda|amazon-sagemaker",
        "Question_view_count":4104,
        "Owner_creation_date":"2011-07-19 03:38:29.507000 UTC",
        "Owner_last_access_date":"2022-09-06 01:14:00.390000 UTC",
        "Owner_location":null,
        "Owner_reputation":56,
        "Owner_up_votes":14,
        "Owner_down_votes":0,
        "Owner_views":25,
        "Answer_body":"<p>Try increasing the ebs volume amount of your notebook ... this blog explains it well: <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/customize-your-notebook-volume-size-up-to-16-tb-with-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/customize-your-notebook-volume-size-up-to-16-tb-with-amazon-sagemaker\/<\/a><\/p>\n<p>Also, best practice is to use lifecycle configuration scripts to build\/add new dependencies ... official docs: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/notebook-lifecycle-config.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/notebook-lifecycle-config.html<\/a><\/p>\n<p>This github page has some great template examples ... for example setting up specific configs like conda, etc: <a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/tree\/master\/scripts\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/tree\/master\/scripts<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-07-16 02:37:49.027000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":62180798,
        "Question_title":"How to increase number of tested images in MS Azure Custom Vision?",
        "Question_body":"<p>I've created a project in Azure Custom Vision (Object Detection, General Compact, Tier S0). I uploaded about 70 images, 35 images per tag then started training my model.<\/p>\n\n<p>Checked tags in the Iterations screen after training (Quick Training) was done. For my surprise, only 7 images were tested per tag.<\/p>\n\n<p>Tried to run Advanced Training for 1 hour. Nothing has changed. Only 7 images per tag were tested.<\/p>\n\n<p>Am I doing something wrong?<\/p>\n\n<p>Is there a way to use all images for object detection training so it can give me a better accuracy?<\/p>\n\n<p>Thanks,\n+ftex<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-06-03 19:08:10.130000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"azure|microsoft-custom-vision|azure-machine-learning-service|azure-ai",
        "Question_view_count":159,
        "Owner_creation_date":"2020-04-30 16:18:28.697000 UTC",
        "Owner_last_access_date":"2022-09-22 19:07:59.483000 UTC",
        "Owner_location":null,
        "Owner_reputation":15,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Answer_body":"<p>What you are seeing in the test interface after the training is only a part of the total images because these metrics are calculated using k-fold cross validation.<\/p>\n<p>You are not doing something wrong. It would not be logic to test all the images because it would mean testing with your training images.<\/p>\n<p>To have a better accuracy, there's no magic: add more images, relevant to your use-case<\/p>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/cognitive-services\/custom-vision-service\/getting-started-build-a-classifier#evaluate-the-classifier?WT.mc_id=AI-MVP-5003365\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/cognitive-services\/custom-vision-service\/getting-started-build-a-classifier#evaluate-the-classifier<\/a><\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2020-06-07 14:56:47.697000 UTC",
        "Answer_last_edit_date":"2020-09-18 21:38:49.973000 UTC",
        "Answer_score":0.0,
        "Question_valid_tags":[
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":50985138,
        "Question_title":"Sagemaker Hyperparameter Optimization XGBoost",
        "Question_body":"<p>I am trying to build a hyperparameter optimization job in Amazon Sagemaker, in python, but something is not working. Here is what I have:<\/p>\n\n<pre><code>sess = sagemaker.Session()\n\nxgb = sagemaker.estimator.Estimator(containers[boto3.Session().region_name],\n                                    role, \n                                    train_instance_count=1, \n                                    train_instance_type='ml.m4.4xlarge',\n                                    output_path=output_path_1,\n                                    base_job_name='HPO-xgb',\n                                    sagemaker_session=sess)\n\nfrom sagemaker.tuner import HyperparameterTuner, IntegerParameter, CategoricalParameter, ContinuousParameter    \n\nhyperparameter_ranges = {'eta': ContinuousParameter(0.01, 0.2),\n                         'num_rounds': ContinuousParameter(100, 500),\n                         'num_class':  4,\n                         'max_depth': IntegerParameter(3, 9),\n                         'gamma': IntegerParameter(0, 5),\n                         'min_child_weight': IntegerParameter(2, 6),\n                         'subsample': ContinuousParameter(0.5, 0.9),\n                         'colsample_bytree': ContinuousParameter(0.5, 0.9)}\n\nobjective_metric_name = 'validation:mlogloss'\nobjective_type='minimize'\nmetric_definitions = [{'Name': 'validation-mlogloss',\n                       'Regex': 'validation-mlogloss=([0-9\\\\.]+)'}]\n\ntuner = HyperparameterTuner(xgb,\n                            objective_metric_name,\n                            objective_type,\n                            hyperparameter_ranges,\n                            metric_definitions,\n                            max_jobs=9,\n                            max_parallel_jobs=3)\n\ntuner.fit({'train': s3_input_train, 'validation': s3_input_validation}) \n<\/code><\/pre>\n\n<p>And the error I get is: <\/p>\n\n<pre><code>AttributeError: 'str' object has no attribute 'keys'\n<\/code><\/pre>\n\n<p>The error seems to come from the <code>tuner.py<\/code> file:<\/p>\n\n<pre><code>----&gt; 1 tuner.fit({'train': s3_input_train, 'validation': s3_input_validation})\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/tuner.py in fit(self, inputs, job_name, **kwargs)\n    144             self.estimator._prepare_for_training(job_name)\n    145 \n--&gt; 146         self._prepare_for_training(job_name=job_name)\n    147         self.latest_tuning_job = _TuningJob.start_new(self, inputs)\n    148 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/tuner.py in _prepare_for_training(self, job_name)\n    120 \n    121         self.static_hyperparameters = {to_str(k): to_str(v) for (k, v) in self.estimator.hyperparameters().items()}\n--&gt; 122         for hyperparameter_name in self._hyperparameter_ranges.keys():\n    123             self.static_hyperparameters.pop(hyperparameter_name, None)\n    124 \n\nAttributeError: 'list' object has no attribute 'keys'                           \n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2018-06-22 09:42:02.477000 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":"2018-06-28 08:11:28.990000 UTC",
        "Question_score":3,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":1523,
        "Owner_creation_date":"2015-05-26 22:53:10.120000 UTC",
        "Owner_last_access_date":"2019-06-03 07:51:39.510000 UTC",
        "Owner_location":null,
        "Owner_reputation":455,
        "Owner_up_votes":7,
        "Owner_down_votes":0,
        "Owner_views":58,
        "Answer_body":"<p>Your arguments when initializing the HyperparameterTuner object are in the wrong order. The constructor has the following signature:<\/p>\n\n<pre><code>HyperparameterTuner(estimator, \n                    objective_metric_name, \n                    hyperparameter_ranges, \n                    metric_definitions=None, \n                    strategy='Bayesian', \n                    objective_type='Maximize', \n                    max_jobs=1, \n                    max_parallel_jobs=1, \n                    tags=None, \n                    base_tuning_job_name=None)\n<\/code><\/pre>\n\n<p>so in this case, your <code>objective_type<\/code> is in the wrong position. See <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/latest\/tuner.html#sagemaker.tuner.HyperparameterTuner\" rel=\"nofollow noreferrer\">the docs<\/a> for more details.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2018-06-27 00:04:50.673000 UTC",
        "Answer_last_edit_date":"2018-07-10 18:56:01.193000 UTC",
        "Answer_score":5.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":64979752,
        "Question_title":"Unable to access python packages installed in Azure ML",
        "Question_body":"<p>I am trying to deploy a pre-trained ML model (saved as .h5 file) to Azure ML. I have created an AKS cluster and trying to deploy the model as shown below:<\/p>\n<pre><code>from azureml.core import Workspace\nfrom azureml.core.model import Model\n\nfrom azureml.core.environment import Environment\nfrom azureml.core.conda_dependencies import CondaDependencies\nfrom azureml.core.model import InferenceConfig\n\nfrom azureml.core.webservice import AksWebservice, LocalWebservice\nfrom azureml.core.compute import ComputeTarget\n\nworkspace = Workspace.from_config(path=&quot;config.json&quot;)\n\nenv = Environment.get(workspace, name='AzureML-TensorFlow-1.13-GPU')\n\n# Installing packages present in my requirements file\nwith open('requirements.txt') as f:\n    dependencies = f.readlines()\ndependencies = [x.strip() for x in dependencies if '# ' not in x]\ndependencies.append(&quot;azureml-defaults&gt;=1.0.45&quot;)\n\nenv.python.conda_dependencies = CondaDependencies.create(conda_packages=dependencies)\n\n# Including the source folder so that all helper scripts are included in my deployment\ninference_config = InferenceConfig(entry_script='app.py', environment=env, source_directory='.\/ProcessImage')\n\naks_target = ComputeTarget(workspace=workspace, name='sketch-ppt-vm')\n\n# Deployment with suitable config\ndeployment_config = AksWebservice.deploy_configuration(cpu_cores=4, memory_gb=32)\nmodel = Model(workspace, 'sketch-inference')\nservice = Model.deploy(workspace, &quot;process-sketch-dev&quot;, [model], inference_config, deployment_config, deployment_target=aks_target, overwrite=True)\nservice.wait_for_deployment(show_output = True)\nprint(service.state)\n<\/code><\/pre>\n<p>My main entry script requires some additional helper scripts, which I include by mentioning the source folder in my inference config.<\/p>\n<p>I was expecting that the helper scripts I add should be able to access the packages installed while setting up the environment during deployment, but I get ModuleNotFoundError.<\/p>\n<p>Here is the error output, along with the a couple of environment variables I printed while executing entry script:<\/p>\n<pre><code>    AZUREML_MODEL_DIR ----  azureml-models\/sketch-inference\/1\n    PYTHONPATH ----  \/azureml-envs\/azureml_6dc005c11e151f8d9427c0c6091a1bb9\/lib\/python3.6\/site-packages:\/var\/azureml-server:\n    PATH ----  \/azureml-envs\/azureml_6dc005c11e151f8d9427c0c6091a1bb9\/bin:\/opt\/miniconda\/bin:\/usr\/local\/nvidia\/bin:\/usr\/local\/cuda\/bin:\/usr\/local\/sbin:\/usr\/local\/bin:\/usr\/sbin:\/usr\/bin:\/sbin:\/bin:\/opt\/intel\/compilers_and_libraries\/linux\/mpi\/bin64\n    Exception in worker process\n    Traceback (most recent call last):\n    File &quot;\/azureml-envs\/azureml_6dc005c11e151f8d9427c0c6091a1bb9\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py&quot;, line 583, in spawn_worker\n        worker.init_process()\n    File &quot;\/azureml-envs\/azureml_6dc005c11e151f8d9427c0c6091a1bb9\/lib\/python3.6\/site-packages\/gunicorn\/workers\/base.py&quot;, line 129, in init_process\n        self.load_wsgi()\n    File &quot;\/azureml-envs\/azureml_6dc005c11e151f8d9427c0c6091a1bb9\/lib\/python3.6\/site-packages\/gunicorn\/workers\/base.py&quot;, line 138, in load_wsgi\n        self.wsgi = self.app.wsgi()\n    File &quot;\/azureml-envs\/azureml_6dc005c11e151f8d9427c0c6091a1bb9\/lib\/python3.6\/site-packages\/gunicorn\/app\/base.py&quot;, line 67, in wsgi\n        self.callable = self.load()\n    File &quot;\/azureml-envs\/azureml_6dc005c11e151f8d9427c0c6091a1bb9\/lib\/python3.6\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 52, in load\n        return self.load_wsgiapp()\n    File &quot;\/azureml-envs\/azureml_6dc005c11e151f8d9427c0c6091a1bb9\/lib\/python3.6\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 41, in load_wsgiapp\n        return util.import_app(self.app_uri)\n    File &quot;\/azureml-envs\/azureml_6dc005c11e151f8d9427c0c6091a1bb9\/lib\/python3.6\/site-packages\/gunicorn\/util.py&quot;, line 350, in import_app\n        __import__(module)\n    File &quot;\/var\/azureml-server\/wsgi.py&quot;, line 1, in &lt;module&gt;\n        import create_app\n    File &quot;\/var\/azureml-server\/create_app.py&quot;, line 3, in &lt;module&gt;\n        from app import main\n    File &quot;\/var\/azureml-server\/app.py&quot;, line 32, in &lt;module&gt;\n        from aml_blueprint import AMLBlueprint\n    File &quot;\/var\/azureml-server\/aml_blueprint.py&quot;, line 25, in &lt;module&gt;\n        import main\n    File &quot;\/var\/azureml-app\/main.py&quot;, line 12, in &lt;module&gt;\n        driver_module_spec.loader.exec_module(driver_module)\n    File &quot;\/structure\/azureml-app\/ProcessImage\/app.py&quot;, line 16, in &lt;module&gt;\n        from ProcessImage.samples.coco.inference import run as infer\n    File &quot;\/var\/azureml-app\/ProcessImage\/samples\/coco\/inference.py&quot;, line 1, in &lt;module&gt;\n        import skimage.io\n    ModuleNotFoundError: No module named 'skimage'\n<\/code><\/pre>\n<p>The existing answers related to this aren't of much help. I believe there must be a simpler way to fix this, since AzureML specifically provides the feature to setup environment with pip\/conda packages installed either by supplying requirements.txt file or individually.<\/p>\n<p>What am I missing here? Kindly help.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-11-24 03:16:03.027000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":3,
        "Question_tags":"azure|azure-machine-learning-studio|azure-machine-learning-service",
        "Question_view_count":1381,
        "Owner_creation_date":"2015-04-23 17:51:38.813000 UTC",
        "Owner_last_access_date":"2022-09-23 21:23:01.117000 UTC",
        "Owner_location":"Boston, MA, USA",
        "Owner_reputation":1910,
        "Owner_up_votes":85,
        "Owner_down_votes":6,
        "Owner_views":316,
        "Answer_body":"<p>So, after some trial and error, creating a fresh environment and then adding the packages solved the problem for me. I am still not clear on why this didn't work when I tried to use <a href=\"http:\/\/from%20azureml.core%20import%20Workspace%20from%20azureml.core.model%20import%20Model%20from%20azureml.core.environment%20import%20Environment,%20DEFAULT_GPU_IMAGE%20from%20azureml.core.conda_dependencies%20import%20CondaDependencies%20from%20azureml.core.model%20import%20InferenceConfig%20from%20azureml.core.webservice%20import%20AksWebservice,%20LocalWebservice%20from%20azureml.core.compute%20import%20ComputeTarget%20%20%20#%201.%20Instantiate%20the%20workspace%20workspace%20=%20Workspace.from_config(path=%22config.json%22)%20%20#%202.%20Setup%20the%20environment%20env%20=%20Environment(%27sketchenv%27)%20with%20open(%27requirements.txt%27)%20as%20f:%20#%20Fetch%20all%20dependencies%20as%20a%20list%20%20%20%20%20dependencies%20=%20f.readlines()%20dependencies%20=%20%5Bx.strip()%20for%20x%20in%20dependencies%20if%20%27#%20%27%20not%20in%20x%5D%20env.docker.base_image%20=%20DEFAULT_GPU_IMAGE%20env.python.conda_dependencies%20=%20CondaDependencies.create(conda_packages=%5B%27numpy==1.17.4%27,%20%27Cython%27%5D,%20pip_packages=dependencies)%20%20#%203.%20Inference%20Config%20inference_config%20=%20InferenceConfig(entry_script=%27app.py%27,%20environment=env,%20source_directory=%27.\/ProcessImage%27)%20%20#%204.%20Compute%20target%20(using%20existing%20cluster%20from%20the%20workspacke)%20aks_target%20=%20ComputeTarget(workspace=workspace,%20name=%27sketch-ppt-vm%27)%20%20#%205.%20Deployment%20config%20deployment_config%20=%20AksWebservice.deploy_configuration(cpu_cores=6,%20memory_gb=100)%20%20#%206.%20Model%20deployment%20model%20=%20Model(workspace,%20%27sketch-inference%27)%20#%20Registered%20model%20(which%20contains%20model%20files\/folders)%20service%20=%20Model.deploy(workspace,%20%22process-sketch-dev%22,%20%5Bmodel%5D,%20inference_config,%20deployment_config,%20deployment_target=aks_target,%20overwrite=True)%20service.wait_for_deployment(show_output%20=%20True)%20print(service.state)\" rel=\"nofollow noreferrer\">Environment.from_pip_requirements()<\/a>. A detailed answer in this regard would be interesting to read.<\/p>\n<p>My primary task was inference - object detection given an image, and we have our own model developed by our team. There are two types of imports I wanted to have:<\/p>\n<p><strong>1. Standard python packages (installed through pip)<\/strong><br \/>\nThis was solved by creating conda dependencies and add it to env object (Step 2)<\/p>\n<p><strong>2. Methods\/vars from helper scripts<\/strong> (if you have pre\/post processing to be done during model inference):<br \/>\nThis was done by mentioning <code>source_directory<\/code> in InferenceConfig (step 3)<\/p>\n<p>Here is my updated script which combines Environment creation, Inference and Deployment configs and using existing compute in the workspace (created through portal).<\/p>\n<pre><code>from azureml.core import Workspace\nfrom azureml.core.model import Model\nfrom azureml.core.environment import Environment, DEFAULT_GPU_IMAGE\nfrom azureml.core.conda_dependencies import CondaDependencies\nfrom azureml.core.model import InferenceConfig\nfrom azureml.core.webservice import AksWebservice, LocalWebservice\nfrom azureml.core.compute import ComputeTarget\n\n\n# 1. Instantiate the workspace\nworkspace = Workspace.from_config(path=&quot;config.json&quot;)\n\n# 2. Setup the environment\nenv = Environment('sketchenv')\nwith open('requirements.txt') as f: # Fetch all dependencies as a list\n    dependencies = f.readlines()\ndependencies = [x.strip() for x in dependencies if '# ' not in x]\nenv.docker.base_image = DEFAULT_GPU_IMAGE\nenv.python.conda_dependencies = CondaDependencies.create(conda_packages=['numpy==1.17.4', 'Cython'], pip_packages=dependencies)\n\n# 3. Inference Config\ninference_config = InferenceConfig(entry_script='app.py', environment=env, source_directory='.\/ProcessImage')\n\n# 4. Compute target (using existing cluster from the workspacke)\naks_target = ComputeTarget(workspace=workspace, name='sketch-ppt-vm')\n\n# 5. Deployment config\ndeployment_config = AksWebservice.deploy_configuration(cpu_cores=6, memory_gb=100)\n\n# 6. Model deployment\nmodel = Model(workspace, 'sketch-inference') # Registered model (which contains model files\/folders)\nservice = Model.deploy(workspace, &quot;process-sketch-dev&quot;, [model], inference_config, deployment_config, deployment_target=aks_target, overwrite=True)\nservice.wait_for_deployment(show_output = True)\nprint(service.state)\n<\/code><\/pre>\n<hr \/>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-11-24 20:27:00.653000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "azure-machine-learning-studio",
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":73282180,
        "Question_title":"In AzureML, start_logging will start asynchronous execution or synchronous execution?",
        "Question_body":"<p>It was written in the Microsoft AzureML documentation, &quot;A run represents a single trial of an experiment. Runs are used to monitor the asynchronous execution of a trial&quot; and A Run object is also created when you submit or start_logging with the Experiment class.&quot;<\/p>\n<p>Related to <code>start_logging<\/code>, as far as I know, when we have simply started the run by executing this <code>start logging<\/code> method. We have to stop, or complete by <code>complete<\/code> method when the run is completed. This is because  <code>start_logging<\/code> is a synchronized way of creating an experiment. However, Run object created from <code>start_logging<\/code> is to monitor the asynchronous execution of a trial.<\/p>\n<p>Can anyone clarify whether <code>start_logging<\/code> will start asynchronous execution or synchronous execution?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-08-08 17:55:30.190000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-08-12 01:22:46.787000 UTC",
        "Question_score":0,
        "Question_tags":"azure|azure-machine-learning-studio",
        "Question_view_count":61,
        "Owner_creation_date":"2018-05-02 19:00:43.000000 UTC",
        "Owner_last_access_date":"2022-09-16 13:48:06.800000 UTC",
        "Owner_location":"Ottawa, ON, Canada",
        "Owner_reputation":27,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Answer_body":"<p><strong>start_logging<\/strong> will be considered as <strong>asynchronous<\/strong> execution as this generates the multiple interactive run sessions. In a specific experiment, there is a chance of multiple interactive sessions, that work parallelly and there will be no scenario to be followed in sequential.<\/p>\n<p>The individual operation can be performed and recognized based on the parameters like <strong>args<\/strong>  and <strong>kwargs<\/strong>.<\/p>\n<p>When the start_logging is called, then an interactive run like <strong>jupyter notebook<\/strong> was created. The complete metrics and components which are created when the start_logging was called will be utilized. When the output directory was mentioned for each interactive run, based on the args value, the output folder will be called seamlessly.<\/p>\n<p>The following code block will help to define the operation of start_logging<\/p>\n<pre><code>experiment = Experiment(your_workspace, &quot;your_experiment_name&quot;)\n   run = experiment.start_logging(outputs=None, snapshot_directory=&quot;.&quot;, display_name=&quot;test&quot;)\n   ...\n   run.log_metric(&quot;Accuracy_Value&quot;, accuracy)\n   run.complete()\n<\/code><\/pre>\n<p>the below code block will be defining the basic syntax of start_logging<\/p>\n<pre><code>start_logging(*args, **kwargs)\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2022-08-09 12:29:07.607000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":64118186,
        "Question_title":"Getting an anomaly score for every datapoint in SageMaker?",
        "Question_body":"<p>I'm very new to SageMaker, and I've run into a bit of confusion as to how to achieve the output I am looking for. I am currently attempting to use the built-in RCF algorithm to perform anomaly detection on a list of stock volumes, like this:<\/p>\n<pre><code>apple_stock_volumes = [123412, 465125, 237564, 238172]\n<\/code><\/pre>\n<p>I have created a training job, model, and endpoint, and I'm trying now to invoke the endpoint using boto3. My current code looks like this:<\/p>\n<pre><code>apple_stock_volumes = [123412, 465125, 237564, 238172]\ndef inference():\n    client = boto3.client('sagemaker-runtime')\n    \n    body = &quot; &quot;.join(apple_stock_volumes)\n    response = client.invoke_endpoint(\n        EndpointName='apple-volume-endpoint',\n        Body=body,\n        ContentType='text\/csv'\n    )\n    inference = json.loads(response['Body'].read())\n    print(inference)\n\ninference()\n<\/code><\/pre>\n<p>What I wanted was to get an anomaly score for every datapoint, and then to alert if the anomaly score was a few standard deviations above the mean. However, what I'm actually receiving is just a single anomaly score. The following is my output:<\/p>\n<pre><code>{'scores': [{'score': 0.7164874384}]}\n<\/code><\/pre>\n<p>Can anyone explain to me what's going on here? Is this an average anomaly score? Why can't I seem to get SageMaker to output a list of anomaly scores corresponding to my data? Thanks in advance!<\/p>\n<p>Edit: I have already trained the model on a csv of historical volume data for the last year, and I have created an endpoint to hit.<\/p>\n<p>Edit 2: I've accepted @maafk's answer, although the actual answer to my question was provided in one of his comments. The piece I was missing was that each data point must be on a new line in your csv input to the endpoint. Once I substituted <code>body = &quot; &quot;.join(apple_stock_volumes)<\/code> for <code>body = &quot;\\n&quot;.join(apple_stock_volumes)<\/code>, everything worked as expected.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-09-29 10:56:43.873000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-09-29 15:44:02.040000 UTC",
        "Question_score":0,
        "Question_tags":"amazon-web-services|random-forest|amazon-sagemaker",
        "Question_view_count":54,
        "Owner_creation_date":"2020-05-13 03:31:33.533000 UTC",
        "Owner_last_access_date":"2022-07-26 19:57:01.630000 UTC",
        "Owner_location":null,
        "Owner_reputation":51,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":"<p>In your case, you'll want to get the standard deviation from getting the scores from historical stock volumes, and figuring out what your anomaly score is by calculating <code>3 * standard deviation<\/code><\/p>\n<p>Update your code to do inference on <em>multiple<\/em> records at once<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>apple_stock_volumes = [123412, 465125, 237564, 238172]\ndef inference():\n    client = boto3.client('sagemaker-runtime')\n    \n    body = &quot;\\n&quot;.join(apple_stock_volumes). # New line for each record\n    response = client.invoke_endpoint(\n        EndpointName='apple-volume-endpoint',\n        Body=body,\n        ContentType='text\/csv'\n    )\n    inference = json.loads(response['Body'].read())\n    print(inference)\n\ninference()\n<\/code><\/pre>\n<p>This will return a list of scores<\/p>\n<p>Assuming <code>apple_stock_volumes_df<\/code> has your volumes and the scores (after running inference on each record):<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>score_mean = apple_stock_volumes_df['score'].mean()\nscore_std = apple_stock_volumes_df['score'].std()\nscore_cutoff = score_mean + 3*score_std\n<\/code><\/pre>\n<p>There is a great example <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/random_cut_forest\/random_cut_forest.ipynb\" rel=\"nofollow noreferrer\">here<\/a> showing this<\/p>",
        "Answer_comment_count":6.0,
        "Answer_creation_date":"2020-09-29 11:10:32.670000 UTC",
        "Answer_last_edit_date":"2020-09-29 16:58:49.940000 UTC",
        "Answer_score":0.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":60202189,
        "Question_title":"How to create azure machine learning resource using terraform resource providers?",
        "Question_body":"<p>I wants to create azure machine learning workspace using terraform scripts.Is there any terraform provider to achieve this.<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":1,
        "Question_creation_date":"2020-02-13 07:04:51.623000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"azure|terraform|azure-machine-learning-service|azure-rm",
        "Question_view_count":1152,
        "Owner_creation_date":"2019-08-12 18:04:59.383000 UTC",
        "Owner_last_access_date":"2021-10-29 06:22:06.150000 UTC",
        "Owner_location":null,
        "Owner_reputation":110,
        "Owner_up_votes":22,
        "Owner_down_votes":0,
        "Owner_views":12,
        "Answer_body":"<p>In the meantime Microsoft has added a Terraform resource for ML Workspace in the Azure Provider. This should make any custom scripting obsolete.<\/p>\n<p><a href=\"https:\/\/www.terraform.io\/docs\/providers\/azurerm\/r\/machine_learning_workspace.html\" rel=\"nofollow noreferrer\">https:\/\/www.terraform.io\/docs\/providers\/azurerm\/r\/machine_learning_workspace.html<\/a><\/p>\n<pre><code>resource &quot;azurerm_machine_learning_workspace&quot; &quot;example&quot; {\n  name                    = &quot;example-workspace&quot;\n  location                = azurerm_resource_group.example.location\n  resource_group_name     = azurerm_resource_group.example.name\n  application_insights_id = azurerm_application_insights.example.id\n  key_vault_id            = azurerm_key_vault.example.id\n  storage_account_id      = azurerm_storage_account.example.id\n\n  identity {\n    type = &quot;SystemAssigned&quot;\n  }\n}\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-09-16 19:42:13.650000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":4.0,
        "Question_valid_tags":[
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":64097278,
        "Question_title":"Why is the field \"compute target\" for data drift monitoring in Azure ML studio still blank whereas I have a compute instance?",
        "Question_body":"<p>I have created a compute instance:<\/p>\n<p>Virtual machine size\nSTANDARD_DS3_V2 (4 Cores, 14 GB RAM, 28 GB Disk)<\/p>\n<p>Processing Unit\nCPU - General purpose<\/p>\n<p>But, I'm not able to access it when trying to set it for data drift monitoring.\nThe dropdown list is empty. I can't understand why. Can you help me please?<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Pf10h.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Pf10h.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-09-28 06:55:03.867000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"azure|machine-learning|azure-machine-learning-studio",
        "Question_view_count":88,
        "Owner_creation_date":"2015-02-11 07:34:40.283000 UTC",
        "Owner_last_access_date":"2022-09-23 14:32:37.963000 UTC",
        "Owner_location":"Lyon, France",
        "Owner_reputation":457,
        "Owner_up_votes":19,
        "Owner_down_votes":2,
        "Owner_views":125,
        "Answer_body":"<p>I found the answer. You must give a <strong>cluster<\/strong> compute instance to do data drift in Azure Machine Learning Studio. As it is not clear, I'm planning to add something in the documentation of Azure.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-09-28 07:43:05.530000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":62737008,
        "Question_title":"How to get Sagemaker Batch Transform Job status printed out in my python notebook?",
        "Question_body":"<p>I am running a python notebook which is initiating a Batch Transform Job in Sagemaker. However, I want to also print the status &quot;Failed&quot;, &quot;In Progress&quot; and &quot;Completed&quot; once the job is complete running. As of now, I am only able to start the Batch Transform Job (rf=random forest) but I am not certain how to get the job status print outs. Can someone help with that given my script below?<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/SU0ln.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/SU0ln.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<pre><code>rf_transformer = rf.transformer(\n                                instance_count,\n                                instance_type,\n                                strategy=strategy,\n                                output_path=output_path,\n                                max_payload=max_payload)\n\nrf_transformer.transform(\n                                str('s3:\/\/batch_scoring\/rf_output),\n                                content_type='text\/csv',\n                                compression_type='Gzip'\n                         )\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-07-05 04:59:20.127000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python-3.x|amazon-s3|amazon-sagemaker",
        "Question_view_count":612,
        "Owner_creation_date":"2020-06-03 22:21:17.740000 UTC",
        "Owner_last_access_date":"2020-09-13 17:40:31.793000 UTC",
        "Owner_location":null,
        "Owner_reputation":91,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":54,
        "Answer_body":"<p>You can do it with:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>job_name = rf_transformer.latest_transform_job.name\nrf_transformer.sagemaker_session.describe_transform_job(job_name)['TransformJobStatus']\n<\/code><\/pre>\n<p>You can also use the AWS SDK directly, if you wish:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import boto3\n\nsagemaker_client = boto3.client('sagemaker')\nsagemaker_client.describe_transform_job(job_name)['TransformJobStatus']\n<\/code><\/pre>\n<p>API documentation: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_DescribeTransformJob.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_DescribeTransformJob.html<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-07-07 20:37:11.750000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":72860901,
        "Question_title":"How can I change the security setting and enable terminal for a Vertex AI managed notebook?",
        "Question_body":"<p>I created a notebook using Vertex AI without enabling terminal first, but I want to enable terminal now so that I can run a Python file from a terminal. Is there any way I can change the setting retrospectively?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-07-04 18:44:02.433000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-07-06 04:37:07.957000 UTC",
        "Question_score":2,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai|gcp-ai-platform-notebook",
        "Question_view_count":110,
        "Owner_creation_date":"2022-06-30 04:56:56.230000 UTC",
        "Owner_last_access_date":"2022-09-18 19:07:37.860000 UTC",
        "Owner_location":null,
        "Owner_reputation":23,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":"<p>As of now, when you create a Notebook instance with unchecked <em>&quot;Enable terminal&quot;<\/em> like the below screenshot, you <strong>cannot re-enable this option once the Notebook instance is already created<\/strong>.\n<a href=\"https:\/\/i.stack.imgur.com\/QM6xp.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/QM6xp.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>The only workaround is to <strong>recreate the Notebook instance<\/strong> and then enable it.<\/p>\n<p>Right now, there is already a <a href=\"https:\/\/issuetracker.google.com\/222694899\" rel=\"nofollow noreferrer\">Feature Request<\/a> for this. You can <strong>star<\/strong> the public issue tracker feature request and add <strong>\u2018Me too\u2019<\/strong> in the thread. This will bring more attention to the request as more users request support for it.<\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_date":"2022-07-08 03:33:52.893000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "google-cloud-vertex-ai"
        ]
    },
    {
        "Question_id":61380051,
        "Question_title":"Sagemaker usage of EC2 instances",
        "Question_body":"<p>Is there a way to view\/monitor AWS Sagemaker's usage of EC2 instances?\nI am running a Sagemaker endpoint and tried to find its instances (ml.p3.2xlarge in this case) in the EC2 UI, but couldn't find them. <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_date":"2020-04-23 05:37:43.500000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":5,
        "Question_tags":"amazon-ec2|amazon-sagemaker",
        "Question_view_count":615,
        "Owner_creation_date":"2016-03-21 08:49:39.920000 UTC",
        "Owner_last_access_date":"2022-07-17 11:53:14.840000 UTC",
        "Owner_location":null,
        "Owner_reputation":383,
        "Owner_up_votes":10,
        "Owner_down_votes":0,
        "Owner_views":19,
        "Answer_body":"<p>ml EC2 instances do not appear in the EC2 console. You can find their metrics in Cloudwatch though, and create dashboards to monitor what you need:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/BgUkm.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/BgUkm.png\" alt=\"enter image description here\"><\/a>\n<a href=\"https:\/\/i.stack.imgur.com\/wD4w0.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/wD4w0.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2020-05-04 07:02:18.650000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":3.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":59773503,
        "Question_title":"Using Sagemaker predictor in a Spark UDF function",
        "Question_body":"<p>I am trying to run inference on a Tensorflow model deployed on SageMaker from a Python Spark job.\nI am running a (Databricks) notebook which has the following cell:<\/p>\n\n<pre><code>def call_predict():\n        batch_size = 1\n        data = [[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.2]]\n        tensor_proto = tf.make_tensor_proto(values=np.asarray(data), shape=[batch_size, len(data[0])], dtype=tf.float32)      \n        prediction = predictor.predict(tensor_proto)\n        print(\"Process time: {}\".format((time.clock() - start)))\n        return prediction\n<\/code><\/pre>\n\n<p>If I just call call_predict() it works fine:<\/p>\n\n<pre><code>call_predict()\n<\/code><\/pre>\n\n<p>and I get the output:<\/p>\n\n<pre><code>Process time: 65.261396\nOut[61]: {'model_spec': {'name': u'generic_model',\n  'signature_name': u'serving_default',\n  'version': {'value': 1578909324L}},\n 'outputs': {u'ages': {'dtype': 1,\n   'float_val': [5.680944442749023],\n   'tensor_shape': {'dim': [{'size': 1L}]}}}}\n<\/code><\/pre>\n\n<p>but when I try to call from a Spark context (in a UDF) I get a serialization error.\nThe code I'm trying to run is:<\/p>\n\n<pre><code>dataRange = range(1, 10001)\nrangeRDD = sc.parallelize(dataRange, 8)\nnew_data = rangeRDD.map(lambda x : call_predict())\nnew_data.count()\n<\/code><\/pre>\n\n<p>and the error I get is:<\/p>\n\n<pre><code>---------------------------------------------------------------------------\nPicklingError                             Traceback (most recent call last)\n&lt;command-2282434&gt; in &lt;module&gt;()\n      2 rangeRDD = sc.parallelize(dataRange, 8)\n      3 new_data = rangeRDD.map(lambda x : call_predict())\n----&gt; 4 new_data.count()\n      5 \n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in count(self)\n   1094         3\n   1095         \"\"\"\n-&gt; 1096         return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n   1097 \n   1098     def stats(self):\n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in sum(self)\n   1085         6.0\n   1086         \"\"\"\n-&gt; 1087         return self.mapPartitions(lambda x: [sum(x)]).fold(0, operator.add)\n   1088 \n   1089     def count(self):\n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in fold(self, zeroValue, op)\n    956         # zeroValue provided to each partition is unique from the one provided\n    957         # to the final reduce call\n--&gt; 958         vals = self.mapPartitions(func).collect()\n    959         return reduce(op, vals, zeroValue)\n    960 \n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in collect(self)\n    829         # Default path used in OSS Spark \/ for non-credential passthrough clusters:\n    830         with SCCallSiteSync(self.context) as css:\n--&gt; 831             sock_info = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())\n    832         return list(_load_from_socket(sock_info, self._jrdd_deserializer))\n    833 \n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in _jrdd(self)\n   2573 \n   2574         wrapped_func = _wrap_function(self.ctx, self.func, self._prev_jrdd_deserializer,\n-&gt; 2575                                       self._jrdd_deserializer, profiler)\n   2576         python_rdd = self.ctx._jvm.PythonRDD(self._prev_jrdd.rdd(), wrapped_func,\n   2577                                              self.preservesPartitioning, self.is_barrier)\n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in _wrap_function(sc, func, deserializer, serializer, profiler)\n   2475     assert serializer, \"serializer should not be empty\"\n   2476     command = (func, profiler, deserializer, serializer)\n-&gt; 2477     pickled_command, broadcast_vars, env, includes = _prepare_for_python_RDD(sc, command)\n   2478     return sc._jvm.PythonFunction(bytearray(pickled_command), env, includes, sc.pythonExec,\n   2479                                   sc.pythonVer, broadcast_vars, sc._javaAccumulator)\n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in _prepare_for_python_RDD(sc, command)\n   2461     # the serialized command will be compressed by broadcast\n   2462     ser = CloudPickleSerializer()\n-&gt; 2463     pickled_command = ser.dumps(command)\n   2464     if len(pickled_command) &gt; sc._jvm.PythonUtils.getBroadcastThreshold(sc._jsc):  # Default 1M\n   2465         # The broadcast will have same life cycle as created PythonRDD\n\n\/databricks\/spark\/python\/pyspark\/serializers.pyc in dumps(self, obj)\n    709                 msg = \"Could not serialize object: %s: %s\" % (e.__class__.__name__, emsg)\n    710             cloudpickle.print_exec(sys.stderr)\n--&gt; 711             raise pickle.PicklingError(msg)\n    712 \n    713 \n\nPicklingError: Could not serialize object: TypeError: can't pickle _ssl._SSLSocket objects\n<\/code><\/pre>\n\n<p>Not sure what is this serialization error - does is complain about failing to deserialize the Predictor<\/p>\n\n<p>My notebook has a cell which was called prior to the above cells with the following imports:<\/p>\n\n<pre><code>import sagemaker\nimport boto3\nfrom sagemaker.tensorflow.model import TensorFlowPredictor\nimport tensorflow as tf\nimport numpy as np\nimport time\n<\/code><\/pre>\n\n<p>The Predictor was created with the following code:<\/p>\n\n<pre><code>sagemaker_client = boto3.client('sagemaker', aws_access_key_id=ACCESS_KEY,\n                                aws_secret_access_key=SECRET_KEY, region_name='us-east-1')\nsagemaker_runtime_client = boto3.client('sagemaker-runtime', aws_access_key_id=ACCESS_KEY,\n                                        aws_secret_access_key=SECRET_KEY, region_name='us-east-1')\n\nboto_session = boto3.Session(region_name='us-east-1')\nsagemaker_session = sagemaker.Session(boto_session, sagemaker_client=sagemaker_client, sagemaker_runtime_client=sagemaker_runtime_client)\n\npredictor = TensorFlowPredictor('endpoint-poc', sagemaker_session)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-01-16 16:00:15.880000 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"tensorflow|pyspark|amazon-sagemaker",
        "Question_view_count":322,
        "Owner_creation_date":"2016-03-21 08:49:39.920000 UTC",
        "Owner_last_access_date":"2022-07-17 11:53:14.840000 UTC",
        "Owner_location":null,
        "Owner_reputation":383,
        "Owner_up_votes":10,
        "Owner_down_votes":0,
        "Owner_views":19,
        "Answer_body":"<p>The udf function will be executed by multiple spark tasks in parallel. Those tasks run in completely isolated python processes and they are scheduled to physically different machines. Hence each data, those functions reference, must be on the same node. This is the case for everything created within the udf.<\/p>\n\n<p>Whenever you reference any object outside of the udf from the function, this data structure needs to be serialised (pickled) to each executor. Some object state, like open connections to a socket, cannot be pickled.<\/p>\n\n<p>You need to make sure, that connections are lazily opened each executor. It must happen only on the first function call on that executor. The <a href=\"https:\/\/spark.apache.org\/docs\/latest\/streaming-programming-guide.html#design-patterns-for-using-foreachrdd\" rel=\"nofollow noreferrer\">connection pooling topic<\/a> is covered in the docs, however only in the spark streaming guide (though it also applies for normal batch jobs).<\/p>\n\n<p>Normally one can use the Singleton Pattern for this. But in python people use the Borgh pattern.<\/p>\n\n<pre><code>class Env:\n    _shared_state = {\n        \"sagemaker_client\": None\n        \"sagemaker_runtime_client\": None\n        \"boto_session\": None\n        \"sagemaker_session\": None\n        \"predictor\": None\n    }\n    def __init__(self):\n        self.__dict__ = self._shared_state\n        if not self.predictor:\n            self.sagemaker_client = boto3.client('sagemaker', aws_access_key_id=ACCESS_KEY, aws_secret_access_key=SECRET_KEY, region_name='us-east-1')\n            self.sagemaker_runtime_client = boto3.client('sagemaker-runtime', aws_access_key_id=ACCESS_KEY, aws_secret_access_key=SECRET_KEY, region_name='us-east-1')\n\n            self.boto_session = boto3.Session(region_name='us-east-1')\n            self.sagemaker_session = sagemaker.Session(self.boto_session, sagemaker_client=self.sagemaker_client, sagemaker_runtime_client=self.sagemaker_runtime_client)\n\n            self.predictor = TensorFlowPredictor('endpoint-poc', self.sagemaker_session)\n\n\n#....\ndef call_predict():\n   env = Env()\n   batch_size = 1\n   data = [[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.2]]\n   tensor_proto = tf.make_tensor_proto(values=np.asarray(data), shape=[batch_size, len(data[0])], dtype=tf.float32)      \n   prediction = env.predictor.predict(tensor_proto)\n\n   print(\"Process time: {}\".format((time.clock() - start)))\n        return prediction\n\nnew_data = rangeRDD.map(lambda x : call_predict())\n<\/code><\/pre>\n\n<p>The Env class is defined on the master node. Its <code>_shared_state<\/code> has empty entries. When then Env object is instantiated first time, it shares the state with all further instances of Env on any subsequent call to the udf. On each separate parallel running process this will happen exactly one time. This way the sessions are shared and do not need to pickled. <\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2020-01-16 20:30:04.950000 UTC",
        "Answer_last_edit_date":"2020-01-20 07:54:05.700000 UTC",
        "Answer_score":1.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":61074798,
        "Question_title":"Deploy pre-trained tensorflow model on the aws sagemaker - ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation",
        "Question_body":"<p>This is the first time I am using amazon web services to deploy my machine learning pre-trained model. I want to deploy my pre-trained TensorFlow model to Aws-Sagemaker. I am somehow able to deploy the endpoints successfully But whenever I call the <code>predictor.predict(some_data)<\/code> method to make prediction to invoking the endpoints it's throwing an error.<\/p>\n\n<pre><code>ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from model with message \"\". See https:\/\/us-west-2.console.aws.amazon.com\/cloudwatch\/home?region=us-west-2#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/sagemaker-tensorflow-2020-04-07-04-25-27-055 in account 453101909370 for more information.\n<\/code><\/pre>\n\n<p>After going through the cloud watch logs I found this error.<\/p>\n\n<pre><code>#011details = \"NodeDef mentions attr 'explicit_paddings' not in Op&lt;name=Conv2D; signature=input:T, filter:T -&gt; output:T; attr=T:type,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE]; attr=strides:list(int); attr=use_cudnn_on_gpu:bool,default=true; attr=padding:string,allowed=[\"SAME\", \"VALID\"]; attr=data_format:string,default=\"NHWC\",allowed=[\"NHWC\", \"NCHW\"]; attr=dilations:list(int),default=[1, 1, 1, 1]&gt;; NodeDef: {{node conv1_conv\/convolution}} = Conv2D[T=DT_FLOAT, _output_shapes=[[?,112,112,64]], data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 2, 2, 1], use_cudnn_on_gpu=true, _device=\"\/job:localhost\/replica:0\/task:0\/device:CPU:0\"](conv1_pad\/Pad, conv1_conv\/kernel\/read). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).\n<\/code><\/pre>\n\n<p>I don't know where I am wrong and I have wasted 2 days already to solve this error and couldn't find out the information regarding this. The detailed logs I have shared <a href=\"https:\/\/docs.google.com\/document\/d\/1NXsLRd6cfbNE55xSVq5d63ETt-cBmwZsOaic8IyF1Qw\/edit?usp=sharing\" rel=\"nofollow noreferrer\">here<\/a>. <\/p>\n\n<p>Tensorflow version of my notebook instance is 1.15<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-04-07 07:26:24.423000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-04-07 10:38:27.797000 UTC",
        "Question_score":2,
        "Question_tags":"amazon-web-services|tensorflow|tensorflow-serving|amazon-sagemaker",
        "Question_view_count":740,
        "Owner_creation_date":"2016-12-03 17:35:32.470000 UTC",
        "Owner_last_access_date":"2022-09-20 06:57:36.320000 UTC",
        "Owner_location":"Nagpur, Sitabuldi, Nagpur, Maharashtra, India",
        "Owner_reputation":111,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":44,
        "Answer_body":"<p>After a lot of searching and try &amp; error, I was able to solve this problem. In many cases, the problem arises because of the TensorFlow and Python versions.<\/p>\n<p><strong>Cause of the problem:<\/strong>\nTo deploy the endpoints, I was using the <code>TensorflowModel<\/code> on TF 1.12 and python 3 and which exactly caused the problem.<\/p>\n<blockquote>\n<pre><code>sagemaker_model = TensorFlowModel(model_data = model_data,\n                                  role = role,\n                                  framework_version = '1.12',\n                                  entry_point = 'train.py')\n<\/code><\/pre>\n<\/blockquote>\n<p>Apparently, <code>TensorFlowModel<\/code> only allows python 2 on TF version 1.11, 1.12. 2.1.0.<\/p>\n<p><strong>How I fixed it:<\/strong> There are two TensorFlow solutions that handle serving in the Python SDK. They have different class representations and documentation as shown here.<\/p>\n<ol>\n<li><strong>TensorFlowModel<\/strong> - <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/model.py#L47\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/model.py#L47<\/a><\/li>\n<\/ol>\n<ul>\n<li>Doc:\n<a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/tree\/v1.12.0\/src\/sagemaker\/tensorflow#deploying-directly-from-model-artifacts\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk\/tree\/v1.12.0\/src\/sagemaker\/tensorflow#deploying-directly-from-model-artifacts<\/a><\/li>\n<li>Key difference: Uses a proxy GRPC client to send requests<\/li>\n<li>Container impl:\n<a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-container\/blob\/master\/src\/tf_container\/serve.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-tensorflow-container\/blob\/master\/src\/tf_container\/serve.py<\/a><\/li>\n<\/ul>\n<ol start=\"2\">\n<li><strong>Model<\/strong> - <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/serving.py#L96\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/serving.py#L96<\/a><\/li>\n<\/ol>\n<ul>\n<li>Doc: <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/deploying_tensorflow_serving.rst\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/deploying_tensorflow_serving.rst<\/a><\/li>\n<li>Key difference: Utilizes the TensorFlow serving rest API<\/li>\n<li>Container impl: <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-serving-container\/blob\/master\/container\/sagemaker\/serve.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-tensorflow-serving-container\/blob\/master\/container\/sagemaker\/serve.py<\/a><\/li>\n<\/ul>\n<p>Python 3 isn't supported using the <code>TensorFlowModel<\/code> object, as the container uses the TensorFlow serving API library in conjunction with the GRPC client to handle making inferences, however, the TensorFlow serving API isn't supported in Python 3 officially, so there are only Python 2 versions of the containers when using the <code>TensorFlowModel<\/code> object.\nIf you need Python 3 then you will need to use the <code>Model<\/code> object defined in #2 above.<\/p>\n<p>Finally, I used the <code>Model<\/code> with the TensorFlow version 1.15.1.<\/p>\n<blockquote>\n<pre><code>sagemaker_model = Model(model_data = model_data,\n                        role = role,\n                        framework_version='1.15.2',\n                        entry_point = 'train.py')\n<\/code><\/pre>\n<\/blockquote>\n<p>Also, here are the successful results.\n<a href=\"https:\/\/i.stack.imgur.com\/OMsEf.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/OMsEf.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-06-30 07:46:24.160000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":4.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":61090530,
        "Question_title":"Using tidyverse to read data from s3 bucket",
        "Question_body":"<p>I'm trying to read a <code>.csv<\/code> file stored in an s3 bucket, and I'm getting errors. I'm following the instructions <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/r_kernel\/using_r_with_amazon_sagemaker.ipynb\" rel=\"nofollow noreferrer\">here<\/a>, but either it does not work or I am making a mistake and I'm not getting what I'm doing wrong.<\/p>\n\n<p>Here's what I'm trying to do:<\/p>\n\n<pre><code># I'm working on a SageMaker notebook instance\nlibrary(reticulate)\nlibrary(tidyverse)\n\nsagemaker &lt;- import('sagemaker')\nsagemaker.session &lt;- sagemaker$Session()\n\nregion &lt;- sagemaker.session$boto_region_name\nbucket &lt;- \"my-bucket\"\nprefix &lt;- \"data\/staging\"\nbucket.path &lt;- sprintf(\"https:\/\/s3-%s.amazonaws.com\/%s\", region, bucket)\nrole &lt;- sagemaker$get_execution_role()\n\nclient &lt;- sagemaker.session$boto_session$client('s3')\nkey &lt;- sprintf(\"%s\/%s\", prefix, 'my_file.csv')\n\nmy.obj &lt;- client$get_object(Bucket=bucket, Key=key)\n\nmy.df &lt;- read_csv(my.obj$Body) # This is where it all breaks down:\n## \n## Error: `file` must be a string, raw vector or a connection.\n## Traceback:\n## \n## 1. read_csv(my.obj$Body)\n## 2. read_delimited(file, tokenizer, col_names = col_names, col_types = col_types, \n##  .     locale = locale, skip = skip, skip_empty_rows = skip_empty_rows, \n##  .     comment = comment, n_max = n_max, guess_max = guess_max, \n##  .     progress = progress)\n## 3. col_spec_standardise(data, skip = skip, skip_empty_rows = skip_empty_rows, \n##  .     comment = comment, guess_max = guess_max, col_names = col_names, \n##  .     col_types = col_types, tokenizer = tokenizer, locale = locale)\n## 4. datasource(file, skip = skip, skip_empty_rows = skip_empty_rows, \n##  .     comment = comment)\n## 5. stop(\"`file` must be a string, raw vector or a connection.\", \n##  .     call. = FALSE)\n<\/code><\/pre>\n\n<p>When working with Python, I can read a CSV file using someting like this:<\/p>\n\n<pre class=\"lang-python prettyprint-override\"><code>import pandas as pd\n# ... Lots of boilerplate code\nmy_data = pd.read_csv(client.get_object(Bucket=bucket, Key=key)['Body'])\n<\/code><\/pre>\n\n<p>This is very similar to what I'm trying to do in R, and it works with Python... so why does it not work on R?<\/p>\n\n<p>Can you point me in the right path?<\/p>\n\n<p><strong>Note:<\/strong> Although I could use a Python kernel for this, I'd like to stick to R, because I'm more fluent with it than with Python, at least when it comes to dataframe crunching.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2020-04-07 22:38:37.960000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"r|amazon-s3|amazon-sagemaker|readr",
        "Question_view_count":1735,
        "Owner_creation_date":"2010-06-01 23:01:17.097000 UTC",
        "Owner_last_access_date":"2022-09-24 15:18:33.307000 UTC",
        "Owner_location":"Mexico",
        "Owner_reputation":20017,
        "Owner_up_votes":1826,
        "Owner_down_votes":1932,
        "Owner_views":2754,
        "Answer_body":"<p>I'd recommend trying the <code>aws.s3<\/code> package instead:<\/p>\n\n<p><a href=\"https:\/\/github.com\/cloudyr\/aws.s3\" rel=\"nofollow noreferrer\">https:\/\/github.com\/cloudyr\/aws.s3<\/a><\/p>\n\n<p>Pretty simple - set your env variables:<\/p>\n\n<pre><code>Sys.setenv(\"AWS_ACCESS_KEY_ID\" = \"mykey\",\n           \"AWS_SECRET_ACCESS_KEY\" = \"mysecretkey\",\n           \"AWS_DEFAULT_REGION\" = \"us-east-1\",\n           \"AWS_SESSION_TOKEN\" = \"mytoken\")\n<\/code><\/pre>\n\n<p>and then once that is out of the way:<\/p>\n\n<p><code>aws.s3::s3read_using(read.csv, object = \"s3:\/\/bucket\/folder\/data.csv\")<\/code><\/p>\n\n<p>Update: I see you're also already familiar with boto and trying to use reticulate so leaving this easy wrapper for that here:\n<a href=\"https:\/\/github.com\/cloudyr\/roto.s3\" rel=\"nofollow noreferrer\">https:\/\/github.com\/cloudyr\/roto.s3<\/a><\/p>\n\n<p>Looks like it has a great api for example the variable layout you're aiming to use:<\/p>\n\n<pre><code>download_file(\n  bucket = \"is.rud.test\", \n  key = \"mtcars.csv\", \n  filename = \"\/tmp\/mtcars-again.csv\", \n  profile_name = \"personal\"\n)\n\nread_csv(\"\/tmp\/mtcars-again.csv\")\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2020-04-10 23:44:37.483000 UTC",
        "Answer_last_edit_date":"2020-04-13 01:16:35.877000 UTC",
        "Answer_score":1.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":68682085,
        "Question_title":"Assume Sagemaker Notebook instance role from Docker container with default network mode",
        "Question_body":"<p>I have an interesting use case and a problem.<\/p>\n<p>We are leveraging <strong>Sagemaker Notebooks<\/strong> as a development environment for our data science teams. These notebooks are essentially EC2 instances with a (relatively) nice IDE (not as good as Cloud9, though).<\/p>\n<p>In addition, we are running some docker containers on these instances. However, we are forced to use <code>--network=host<\/code> mode, otherwise, the role assigned to the Notebook Instance is not assumed inside the docker container.<\/p>\n<p>On the host (here <code>1234567890<\/code> is our account number, and <code>DataScientist<\/code> is the role attached to the Sagemaker Notebook instance):<\/p>\n<pre><code>$ aws sts get-caller-identity\n{\n    &quot;UserId&quot;: &quot;AROAU2P5VGYMMVxxxxxJ:SageMaker&quot;,\n    &quot;Account&quot;: &quot;1234567890&quot;,\n    &quot;Arn&quot;: &quot;arn:aws:sts::1234567890:assumed-role\/DataScientist\/SageMaker&quot;\n}\n<\/code><\/pre>\n<p>Running the same command inside a Docker container with <code>--network=host<\/code> produces the same result:<\/p>\n<pre><code>$ docker run --network host amazon\/aws-cli sts get-caller-identity\n{\n    &quot;UserId&quot;: &quot;AROAU2P5VGYMMVxxxxxJ:SageMaker&quot;,\n    &quot;Account&quot;: &quot;1234567890&quot;,\n    &quot;Arn&quot;: &quot;arn:aws:sts::1234567890:assumed-role\/DataScientist\/SageMaker&quot;\n}\n<\/code><\/pre>\n<p>However, it doesn't work with Docker <code>--network=bridge<\/code>:<\/p>\n<pre><code>$ docker run amazon\/aws-cli sts get-caller-identity\n{\n    &quot;UserId&quot;: &quot;AROAIMGPPFPT5T6N7BYX6:i-0b2a9080d5ed1cb98&quot;,\n    &quot;Account&quot;: &quot;366152344081&quot;,\n    &quot;Arn&quot;: &quot;arn:aws:sts::366152344081:assumed-role\/BaseNotebookInstanceEc2InstanceRole\/i-0b2a9080d5ed1cb98&quot;\n}\n<\/code><\/pre>\n<p>As you can see, it's a completely different role being assumed. Notice the account number 366152344081 and the role ARN - it's sth internal to AWS.<\/p>\n<p>We would like to keep the default networking option for Docker (bridge) and at the same time be able to assume the correct role (the one attached to SageMaker Notebook instance e.g. <code>DataScientist<\/code> in our case) attached to the host system (Sagemaker Notebook). Are there any hacks (e.g. iptable rules, etc.) to achieve that?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-08-06 13:06:25.970000 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":"2021-12-10 12:39:06.700000 UTC",
        "Question_score":0,
        "Question_tags":"amazon-web-services|docker|amazon-iam|amazon-sagemaker",
        "Question_view_count":195,
        "Owner_creation_date":"2013-11-15 15:40:39.387000 UTC",
        "Owner_last_access_date":"2022-09-24 12:00:52.630000 UTC",
        "Owner_location":"Ljubljana, Slovenia",
        "Owner_reputation":2470,
        "Owner_up_votes":910,
        "Owner_down_votes":11,
        "Owner_views":285,
        "Answer_body":"<p>If we look at the networks that were created on a clean Sagemaker Notebook instance, we can notice a user-defined bridge network named <code>sagemaker-local<\/code>:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>$ docker network ls\nNETWORK ID          NAME                DRIVER              SCOPE\nf1d5a59a8c9e        bridge              bridge              local\n6142e6764495        host                host                local\n194adfb00f0a        none                null                local\n99de6c086aa8        sagemaker-local     bridge              local\n<\/code><\/pre>\n<p>If we then attach to this custom bridge, we will be able to assume the correct role (the one attached to the Sagemaker Notebook instance itself):<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>$ docker run --network sagemaker-local amazon\/aws-cli sts get-caller-identity\n{\n    &quot;UserId&quot;: &quot;AROAU2P5VGYMMVxxxxxJ:SageMaker&quot;,\n    &quot;Account&quot;: &quot;1234567890&quot;,\n    &quot;Arn&quot;: &quot;arn:aws:sts::1234567890:assumed-role\/DataScientist\/SageMaker&quot;\n}\n<\/code><\/pre>\n<hr \/>\n<p><strong>UPDATE<\/strong><\/p>\n<p>As of this writing (10 Dec 2021) you don't need to attach to <code>sagemaker-local<\/code> bridge network anymore, the default <code>bridge<\/code> will work as well (note <code>--network bridge<\/code> is implicit in this call):<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>$ docker run amazon\/aws-cli sts get-caller-identity\n{\n    &quot;UserId&quot;: &quot;AROAU2P5VGYMMVxxxxxJ:SageMaker&quot;,\n    &quot;Account&quot;: &quot;1234567890&quot;,\n    &quot;Arn&quot;: &quot;arn:aws:sts::1234567890:assumed-role\/DataScientist\/SageMaker&quot;\n}\n<\/code><\/pre>\n<p>Make sure you restart your SageMaker Notebook instance.<\/p>\n<p>Also, <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/mxnet_gluon_mnist\/setup.sh\" rel=\"nofollow noreferrer\">here<\/a> I found some manual patching (iptables etc.), but with the update it's already patched.<\/p>\n<p>Thanks to AWS who fixed this :)<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-12-08 08:24:37.413000 UTC",
        "Answer_last_edit_date":"2021-12-10 10:14:48.447000 UTC",
        "Answer_score":0.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":56450223,
        "Question_title":"Image Classification in Azure Machine Learning",
        "Question_body":"<p>I'm preparing for the Azure Machine Learning exam, and here is a question confuses me.<\/p>\n<blockquote>\n<p>You are designing an Azure Machine Learning workflow. You have a\ndataset that contains two million large digital photographs. You plan\nto detect the presence of trees in the photographs. You need to ensure\nthat your model supports the following:<\/p>\n<p>Solution: You create a Machine\nLearning experiment that implements the Multiclass Decision Jungle\nmodule. Does this meet the goal?<\/p>\n<p>Solution: You create a Machine Learning experiment that implements the\nMulticlass Neural Network module. Does this meet the goal?<\/p>\n<\/blockquote>\n<p>The answer for the first question is No while for second is Yes, but I cannot understand why Multiclass Decision Jungle doesn't meet the goal since it is a classifier. Can someone explain to me the reason?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":3,
        "Question_creation_date":"2019-06-04 19:33:42.253000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-06-20 09:12:55.060000 UTC",
        "Question_score":1,
        "Question_tags":"machine-learning|azure-machine-learning-studio|azure-machine-learning-workbench|azure-machine-learning-service",
        "Question_view_count":538,
        "Owner_creation_date":"2015-12-07 02:00:13.690000 UTC",
        "Owner_last_access_date":"2022-06-01 02:54:45.017000 UTC",
        "Owner_location":null,
        "Owner_reputation":388,
        "Owner_up_votes":84,
        "Owner_down_votes":2,
        "Owner_views":322,
        "Answer_body":"<p>I suppose that this is part of a series of questions that present the same scenario. And there should be definitely some constraints in the scenario. \nMoreover if you have a look on the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/multiclass-neural-network\" rel=\"nofollow noreferrer\">Azure documentation<\/a>:<\/p>\n\n<blockquote>\n  <p>However, recent research has shown that deep neural networks (DNN)\n  with many layers can be very effective in complex tasks such as image\n  or speech recognition. The successive layers are used to model\n  increasing levels of semantic depth.<\/p>\n<\/blockquote>\n\n<p>Thus, Azure recommends using Neural Networks for image classification. Remember, that the goal of the exam is to test your capacity to design data science solution <strong>using Azure<\/strong> so better to use their official documentation as a reference.<\/p>\n\n<p>And comparing to the other solutions:<\/p>\n\n<ol>\n<li>You create an Azure notebook that supports the Microsoft Cognitive\nToolkit.<\/li>\n<li>You create a Machine Learning experiment that implements\nthe Multiclass Decision Jungle module.<\/li>\n<li>You create an endpoint to the\nComputer vision API. <\/li>\n<li>You create a Machine Learning experiment that\nimplements the Multiclass Neural Network module.<\/li>\n<li>You create an Azure\nnotebook that supports the Microsoft Cognitive Toolkit.<\/li>\n<\/ol>\n\n<p>There are only 2 Azure ML Studio modules, and as the question is about constructing a <strong>workflow<\/strong> I guess we can only choose between them. (CNTK is actually the best solution as it allows constructing a deep neural network with ReLU whereas AML Studio doesn't, and API call is not about data science at all). <\/p>\n\n<p>Finally, I do agree with the other contributors that the question is absurd. Hope this helps.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2019-06-07 08:15:14.607000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_valid_tags":[
            "azure-machine-learning-studio",
            "azure-machine-learning-workbench",
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":67496760,
        "Question_title":"Mounting an S3 bucket in docker in a clearml agent",
        "Question_body":"<p>What is the best practice for mounting an S3 container inside a docker image that will be using as a ClearML agent?  I can think of 3 solutions, but have been unable to get any to work currently:<\/p>\n<ol>\n<li>Use <a href=\"https:\/\/allegro.ai\/clearml\/docs\/docs\/use_cases\/clearml_agent_use_case_examples.html?highlight=docker\" rel=\"nofollow noreferrer\">prefabbed configuration in ClearML<\/a>, specifically CLEARML_AGENT_K8S_HOST_MOUNT.  For this to work, the S3 bucket would be mounted separately on the host using <a href=\"https:\/\/rclone.org\/\" rel=\"nofollow noreferrer\">rclone<\/a> and then remapped into docker. This appears to only apply to Kubernetes and not Docker - and therefore would not work.<\/li>\n<li>Mount using s3fuse as specified <a href=\"https:\/\/stackoverflow.com\/questions\/35189251\/docker-mount-s3-container\">here<\/a>.  The issue is will it work with the S3 bucket secret stored in ClearML browser sessions?  This would also appear to be complicated and require custom docker images, not to mention running the docker image as --privileged or similar.<\/li>\n<li>Pass arguments to docker using &quot;docker_args and docker_bash_setup_script arguments to Task.create()&quot; as specified in the <a href=\"https:\/\/allegro.ai\/clearml\/docs\/docs\/release_notes\/ver_1_0.html\" rel=\"nofollow noreferrer\">1.0 release notes<\/a>.  This would be similar to (1), but the arguments would be for <a href=\"https:\/\/docs.docker.com\/storage\/bind-mounts\/\" rel=\"nofollow noreferrer\">bind-mounting the volume<\/a>.  I do not see much documentation or examples on how this new feature may be used for this end.<\/li>\n<\/ol>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2021-05-12 02:58:51.577000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-05-14 19:57:30.560000 UTC",
        "Question_score":1,
        "Question_tags":"docker|amazon-s3|wsl-2|rclone|clearml",
        "Question_view_count":770,
        "Owner_creation_date":"2013-03-06 14:43:00.910000 UTC",
        "Owner_last_access_date":"2022-09-16 18:05:09.150000 UTC",
        "Owner_location":"Akron, OH, USA",
        "Owner_reputation":4013,
        "Owner_up_votes":94,
        "Owner_down_votes":0,
        "Owner_views":72,
        "Answer_body":"<p>I was able to get another option entirely to work, namely, mount a drive on in WSL and then pass it to Docker.  Let's get to it:<\/p>\n<p>Why not host in Windows itself, why rclone in WSL?<\/p>\n<ul>\n<li>Docker running on WSL <a href=\"https:\/\/github.com\/billziss-gh\/winfsp\/issues\/61\" rel=\"nofollow noreferrer\">cannot access drives mounted through winfsp<\/a> (what rclone uses)<\/li>\n<\/ul>\n<p>Steps to mount the drive in ClearML in Windows:<\/p>\n<ul>\n<li>You can install rclone in WSL and the mount will be accessible to docker\n<ul>\n<li>create the folder <code>\/data\/my-mount<\/code> (this needs to be in <code>\/data<\/code> - I don't know why and I can't find out with a Google search, but I found out about it <a href=\"https:\/\/forum.rclone.org\/t\/fusermount-permission-denied-in-docker-rclone\/13914\/5\" rel=\"nofollow noreferrer\">here<\/a>)<\/li>\n<li>You can put the configuration file in windows (use the <code>--config<\/code> option).<\/li>\n<li>Note: ClearML will not support spaces in mounted paths, even though docker will.  Therefore your path has to be <code>\/data\/my-mount<\/code> rather than <code>\/data\/my mount<\/code>.  There is a <a href=\"https:\/\/github.com\/allegroai\/clearml\/issues\/358\" rel=\"nofollow noreferrer\">bug that I opened about this<\/a>.<\/li>\n<\/ul>\n<\/li>\n<li>You can test mounting by calling docker and mounting the file.\n<ul>\n<li>Example: <code>docker run -it -v \\\\wsl$\\Ubuntu\\data:\/data my-docker-image:latest ls \/data\/my-mount<\/code><\/li>\n<li>Note: You will have to mount \/data rather than \/data\/my-mount, otherwise you may get this error: <code>docker: Error response from daemon: error while creating mount source path<\/code><\/li>\n<\/ul>\n<\/li>\n<li>Now, you can setup the clearml.conf file in <code>C:\\Users\\Myself\\clearml.conf<\/code> such that:<\/li>\n<\/ul>\n<pre><code>default_docker: {\n   # default docker image to use when running in docker mode\n   image: &quot;my-docker-image:latest&quot;\n\n   # optional arguments to pass to docker image\n   arguments: [&quot;-v&quot;,&quot;\\\\wsl$\\Ubuntu\\data:\/data&quot;, ]\n}\n<\/code><\/pre>\n<ul>\n<li>Note that you can also run clearml-agent out of WSL and then would only need to specify <code>[&quot;-v&quot;,&quot;\/data:\/data&quot;, ]<\/code>.<\/li>\n<li>Run clearml agent in cmd: <code>clearml-agent daemon --docker<\/code><\/li>\n<\/ul>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-05-14 16:30:41.940000 UTC",
        "Answer_last_edit_date":"2021-05-14 17:07:32.113000 UTC",
        "Answer_score":0.0,
        "Question_valid_tags":[
            "clearml"
        ]
    },
    {
        "Question_id":61803031,
        "Question_title":"Azure ML: Include additional files during model deployment",
        "Question_body":"<p>In my AML pipeline, I've got a model built and deployed to the AciWebservice. I now have a need to include some additional data that would be used by score.py. This data is in json format (~1mb) and is specific to the model that's built. To accomplish this, I was thinking of sticking this file in blob store and updating some \"placholder\" vars in the score.py during deployment, but it seems hacky. <\/p>\n\n<p>Here are some options I was contemplating but wasn't sure on the practicality<\/p>\n\n<p><strong>Option 1:<\/strong>\nIs it possible to include this file, during the model deployment itself so that it's part of the docker image? <\/p>\n\n<p><strong>Option 2:<\/strong>\nAnother possibility I was contemplating, would it be possible to include this json data part of the Model artifacts?<\/p>\n\n<p><strong>Option 3:<\/strong>\nHow about registering it as a dataset and pull that in the score file?<\/p>\n\n<p>What is the recommended way to deploy dependent files in a model deployment scenario?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2020-05-14 16:56:48.817000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-05-14 18:07:01.850000 UTC",
        "Question_score":3,
        "Question_tags":"azure-machine-learning-service",
        "Question_view_count":866,
        "Owner_creation_date":"2012-02-23 16:54:25.410000 UTC",
        "Owner_last_access_date":"2022-09-02 23:23:03.830000 UTC",
        "Owner_location":null,
        "Owner_reputation":1704,
        "Owner_up_votes":61,
        "Owner_down_votes":7,
        "Owner_views":232,
        "Answer_body":"<p>There are few ways to accomplish this:<\/p>\n\n<ol>\n<li><p>Put the additional file in the same folder as your model file, and <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.model.model?view=azure-ml-py#register-workspace--model-path--model-name--tags-none--properties-none--description-none--datasets-none--model-framework-none--model-framework-version-none--child-paths-none--sample-input-dataset-none--sample-output-dataset-none--resource-configuration-none-\" rel=\"nofollow noreferrer\">register<\/a> the whole folder as the model. In this approach the file is stored alongside the model.<\/p><\/li>\n<li><p>Put the file in a local folder, and specify that folder as source_directory in <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.model.inferenceconfig?view=azure-ml-py\" rel=\"nofollow noreferrer\">InferenceConfig<\/a>. In this approach the file is re-uploaded every time you deploy a new endpoint.<\/p><\/li>\n<li><p>Use custom base image in InferenceConfig to bake the file into Docker image itself.<\/p><\/li>\n<\/ol>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-05-14 19:28:31.663000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_valid_tags":[
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":68422680,
        "Question_title":"how to write to Azure PipelineData properly?",
        "Question_body":"<p>Im trying to learn Azure, with little luck (yet). All the tutorials show using PipelineData just as a file, when configured in &quot;upload&quot; mode. However, im getting &quot;FileNotFoundError: [Errno 2] No such file or directory: ''&quot; error. I would love to ask a more specific question, but i just can't see what im doing wrong.<\/p>\n<pre><code>from azureml.core import Workspace, Datastore,Dataset,Environment\nfrom azureml.core.compute import ComputeTarget, AmlCompute\nfrom azureml.core.compute_target import ComputeTargetException\nfrom azureml.core.runconfig import RunConfiguration\nfrom azureml.core.conda_dependencies import CondaDependencies\nfrom azureml.pipeline.steps import PythonScriptStep\nfrom azureml.pipeline.core import Pipeline, PipelineData\nimport os\n\nws = Workspace.from_config()\ndatastore = ws.get_default_datastore()\n\ncompute_name = &quot;cpucluster&quot;\ncompute_target = ComputeTarget(workspace=ws, name=compute_name)\naml_run_config = RunConfiguration()\naml_run_config.target = compute_target\naml_run_config.environment.python.user_managed_dependencies = False\naml_run_config.environment.python.conda_dependencies = CondaDependencies.create(\n    conda_packages=['pandas','scikit-learn'], \n    pip_packages=['azureml-sdk', 'azureml-dataprep[fuse,pandas]'], \n    pin_sdk_version=False)\n\noutput1 = PipelineData(&quot;processed_data1&quot;,datastore=datastore, output_mode=&quot;upload&quot;)\nprep_step = PythonScriptStep(\n    name=&quot;dataprep&quot;,\n    script_name=&quot;dataprep.py&quot;,\n    source_directory=os.path.join(os.getcwd(),'dataprep'),\n    arguments=[&quot;--output&quot;, output1],\n    outputs = [output1],\n    compute_target=compute_target,\n    runconfig=aml_run_config,\n    allow_reuse=True\n)\n<\/code><\/pre>\n<p>In the dataprep.py i hve the following:<\/p>\n<pre><code>import numpy, argparse, pandas\nfrom azureml.core import Run\nrun = Run.get_context()\nparser = argparse.ArgumentParser()\nparser.add_argument('--output', dest='output', required=True)\nargs = parser.parse_args()\ndf = pandas.DataFrame(numpy.random.rand(100,3))\ndf.iloc[:, 2] = df.iloc[:,0] + df.iloc[:,1]\nprint(df.iloc[:5,:])\ndf.to_csv(args.output)\n\n<\/code><\/pre>\n<p>So, yeah. pd is supposed to write to the output, but my compute cluster says the following:<\/p>\n<pre><code>&quot;User program failed with FileNotFoundError: [Errno 2] No such file or directory: ''\\&quot;.\n<\/code><\/pre>\n<p>When i dont include the to_csv() function, the cluster does not complain<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2021-07-17 17:11:28.290000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-07-17 22:09:08.577000 UTC",
        "Question_score":4,
        "Question_tags":"python|azure|azure-machine-learning-service",
        "Question_view_count":404,
        "Owner_creation_date":"2020-05-02 13:08:31.653000 UTC",
        "Owner_last_access_date":"2022-09-25 01:27:33.107000 UTC",
        "Owner_location":null,
        "Owner_reputation":59,
        "Owner_up_votes":7,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Answer_body":"<p>Here is an <a href=\"https:\/\/github.com\/james-tn\/highperformance_python_in_azure\/blob\/master\/parallel_python_processing\/pipeline_definition.ipynb\" rel=\"nofollow noreferrer\">example<\/a> for PRS.\n<a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-pipeline-core\/azureml.pipeline.core.pipelinedata?view=azure-ml-py\" rel=\"nofollow noreferrer\">PipelineData<\/a> was intended to represent &quot;transient&quot; data from one step to the next one, while OutputDatasetConfig was intended for capturing the final state of a dataset (and hence why you see features like lineage, ADLS support, etc). PipelineData always outputs data in a folder structure like {run_id}{output_name}. OutputDatasetConfig allows to decouple the data from the run and hence it allows you to control where to land the data (although by default it will produce similar folder structure). The OutputDatasetConfig allows even to register the output as a Dataset, where getting rid of such folder structure makes sense. From the docs itself: &quot;Represent how to copy the output of a run and be promoted as a FileDataset. The OutputFileDatasetConfig allows you to specify how you want a particular local path on the compute target to be uploaded to the specified destination&quot;.<\/p>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-pipeline-batch-scoring-classification#create-dataset-objects\" rel=\"nofollow noreferrer\">OutFileDatasetConfig<\/a> is a control plane concept to pass data between pipeline steps.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2021-07-19 04:05:19.373000 UTC",
        "Answer_last_edit_date":"2021-07-19 04:14:27.887000 UTC",
        "Answer_score":2.0,
        "Question_valid_tags":[
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":66923216,
        "Question_title":"Change Disk Type Azure ML",
        "Question_body":"<p>I have azure ml , I created compute for learning.\nCost for instance is 2-5usd with my use. But cost for p10(premium SSD) Disk 17usd.<\/p>\n<p>I don't know how change it because its not appear in azure Disk and in ML studio i cant find option for manage storage type for compute.<\/p>\n<p>Some one know how change it ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-04-02 17:45:41.497000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"azure|azure-machine-learning-service",
        "Question_view_count":150,
        "Owner_creation_date":"2019-06-20 07:29:43.553000 UTC",
        "Owner_last_access_date":"2022-09-25 05:21:17.457000 UTC",
        "Owner_location":null,
        "Owner_reputation":97,
        "Owner_up_votes":167,
        "Owner_down_votes":0,
        "Owner_views":15,
        "Answer_body":"<p>There is no possible way to change the compute disk type if you use the Azure ML compute cluster and compute instance. Only when you use the extra computer, you can manage the separate resources such as the disk, network, and so on. For example, you attach a VM as the target computer to the Azure ML. Then when you create the VM you can set the disk type with HDD.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-04-07 06:57:37.403000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_valid_tags":[
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":64302986,
        "Question_title":"How to highlight custom extractions using a2i's crowd-textract-analyze-document?",
        "Question_body":"<p>I would like to create a human review loop for images that undergone OCR using Amazon Textract and Entity Extraction using Amazon Comprehend.<\/p>\n<p>My process is:<\/p>\n<ol>\n<li>send image to Textract to extract the text<\/li>\n<li>send text to Comprehend to extract entities<\/li>\n<li>find the Block IDs in Textract's output of the entities extracted by Comprehend<\/li>\n<li>add new Blocks of type <code>KEY_VALUE_SET<\/code> to textract's JSON output <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/a2i-crowd-textract-detection.html\" rel=\"nofollow noreferrer\">per the docs<\/a><\/li>\n<li>create a Human Task with <code>crowd-textract-analyze-document<\/code> element in the template and feed it the modified textract output<\/li>\n<\/ol>\n<p>What fails to work in this process is step 5. My custom entities are not rendered properly. By &quot;fails to work&quot; I mean that the entities are not highlighted on the image when I click them on the sidebar. There is no error in the browser's console.<\/p>\n<p>Has anyone tried such a thing?<\/p>\n<p><em>Sorry for not including examples. I will remove secrets\/PII from my files and attach them to the question<\/em><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-10-11 10:29:04.293000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-06-19 08:28:21.367000 UTC",
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker|amazon-textract|amazon-comprehend",
        "Question_view_count":252,
        "Owner_creation_date":"2012-11-28 14:00:42.423000 UTC",
        "Owner_last_access_date":"2022-09-24 22:55:26.030000 UTC",
        "Owner_location":"Israel",
        "Owner_reputation":2162,
        "Owner_up_votes":390,
        "Owner_down_votes":16,
        "Owner_views":307,
        "Answer_body":"<p>I used the AWS documentation of the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/a2i-crowd-textract-detection.html\" rel=\"nofollow noreferrer\">a2i-crowd-textract-detection human task element<\/a> to generate the value of the <code>initialValue<\/code> attribute. It appears the doc for that attribute is incorrect. While the the doc shows that the value should be in the same format as the output of Textract, namely:<\/p>\n<pre><code>[\n        {\n            &quot;BlockType&quot;: &quot;KEY_VALUE_SET&quot;,\n            &quot;Confidence&quot;: 38.43309020996094,\n            &quot;Geometry&quot;: { ... }\n            &quot;Id&quot;: &quot;8c97b240-0969-4678-834a-646c95da9cf4&quot;,\n            &quot;Relationships&quot;: [\n                { &quot;Type&quot;: &quot;CHILD&quot;, &quot;Ids&quot;: [...]},\n                { &quot;Type&quot;: &quot;VALUE&quot;, &quot;Ids&quot;: [...]}\n            ],\n            &quot;EntityTypes&quot;: [&quot;KEY&quot;],\n            &quot;Text&quot;: &quot;Foo bar&quot;\n        },\n]\n<\/code><\/pre>\n<p>the <code>a2i-crowd-textract-detection<\/code> expects the input to have lowerCamelCase attribute names (rather than UpperCamelCase). For example:<\/p>\n<pre><code>[\n        {\n            &quot;blockType&quot;: &quot;KEY_VALUE_SET&quot;,\n            &quot;confidence&quot;: 38.43309020996094,\n            &quot;geometry&quot;: { ... }\n            &quot;id&quot;: &quot;8c97b240-0969-4678-834a-646c95da9cf4&quot;,\n            &quot;relationships&quot;: [\n                { &quot;Type&quot;: &quot;CHILD&quot;, &quot;ids&quot;: [...]},\n                { &quot;Type&quot;: &quot;VALUE&quot;, &quot;ids&quot;: [...]}\n            ],\n            &quot;entityTypes&quot;: [&quot;KEY&quot;],\n            &quot;text&quot;: &quot;Foo bar&quot;\n        },\n]\n<\/code><\/pre>\n<p>I opened a support case about this documentation error to AWS.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-10-18 09:15:33.233000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":57199472,
        "Question_title":"Is it possible to set\/change mlflow run name after run initial creation?",
        "Question_body":"<p>I could not find a way yet of setting the runs name after the first start_run for that run (we can pass a name there). <\/p>\n\n<p>I Know we can use tags but that is not the same thing. I would like to add a run relevant name, but very often we know the name only after run evaluation or while we're running the run interactively in notebook for example.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_date":"2019-07-25 10:09:51.490000 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":"2019-07-31 00:38:47.097000 UTC",
        "Question_score":11,
        "Question_tags":"mlflow",
        "Question_view_count":7689,
        "Owner_creation_date":"2014-02-22 09:53:28.773000 UTC",
        "Owner_last_access_date":"2021-04-14 22:23:03.790000 UTC",
        "Owner_location":"Portugal",
        "Owner_reputation":133,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Answer_body":"<p>It is possible to edit run names from the MLflow UI. First, click into the run whose name you'd like to edit.<\/p>\n\n<p>Then, edit the run name by clicking the dropdown next the run name (i.e. the downward-pointing caret in this image):<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/sl6Qs.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/sl6Qs.png\" alt=\"Rename run dropdown\"><\/a><\/p>\n\n<p>There's currently no stable public API for setting run names - however, you can programmatically set\/edit run names by setting the tag with key <code>mlflow.runName<\/code>, which is what the UI (currently) does under the hood.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2019-07-29 06:02:35.503000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":9.0,
        "Question_valid_tags":[
            "mlflow"
        ]
    },
    {
        "Question_id":57471129,
        "Question_title":"Unable to use GPU to train a NN model in azure machine learning service using P100-NC6s-V2 compute. Fails wth CUDA error",
        "Question_body":"<p>I\u2019ve recently started working with azure for ML and am trying to use machine learning service workspace.\nI\u2019ve set up a workspace with the compute set to NC6s-V2 machines since I need train a NN using images on GPU. <\/p>\n\n<p>The issue is that the training still happens on the CPU \u2013 the logs say it\u2019s not able to find CUDA. Here\u2019s the warning log when running my script.\nAny clues how to solve this issue?<\/p>\n\n<p>I\u2019ve also mentioned explicitly tensorflow-gpu package in the conda packages option of the estimator. <\/p>\n\n<p>Here's my code for the estimator,<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>script_params = {\n         '--input_data_folder': ds.path('dataset').as_mount(),\n         '--zip_file_name': 'train.zip',\n         '--run_mode': 'train'\n    }\n\n\nest = Estimator(source_directory='.\/scripts',\n                     script_params=script_params,\n                     compute_target=compute_target,\n                     entry_script='main.py',\n                     conda_packages=['scikit-image', 'keras', 'tqdm', 'pillow', 'matplotlib', 'scipy', 'tensorflow-gpu']\n                     )\n\nrun = exp.submit(config=est)\n\nrun.wait_for_completion(show_output=True)\n<\/code><\/pre>\n\n<p>The compute target was made as per the sample code on github:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>compute_name = \"P100-NC6s-V2\"\ncompute_min_nodes = 0\ncompute_max_nodes = 4\n\nvm_size = \"STANDARD_NC6S_V2\"\n\nif compute_name in ws.compute_targets:\n    compute_target = ws.compute_targets[compute_name]\n    if compute_target and type(compute_target) is AmlCompute:\n        print('found compute target. just use it. ' + compute_name)\nelse:\n    print('creating a new compute target...')\n    provisioning_config = AmlCompute.provisioning_configuration(vm_size=vm_size,\n                                                                min_nodes=compute_min_nodes,\n                                                                max_nodes=compute_max_nodes)\n\n    # create the cluster\n    compute_target = ComputeTarget.create(\n        ws, compute_name, provisioning_config)\n\n    # can poll for a minimum number of nodes and for a specific timeout.\n    # if no min node count is provided it will use the scale settings for the cluster\n    compute_target.wait_for_completion(\n        show_output=True, min_node_count=None, timeout_in_minutes=20)\n\n    # For a more detailed view of current AmlCompute status, use get_status()\n    print(compute_target.get_status().serialize())\n\n<\/code><\/pre>\n\n<p>This is the warning with which it fails to use the GPU:<\/p>\n\n<pre><code>2019-08-12 14:50:16.961247: I tensorflow\/compiler\/xla\/service\/service.cc:168] XLA service 0x55a7ce570830 executing computations on platform Host. Devices:\n2019-08-12 14:50:16.961278: I tensorflow\/compiler\/xla\/service\/service.cc:175]   StreamExecutor device (0): &lt;undefined&gt;, &lt;undefined&gt;\n2019-08-12 14:50:16.971025: I tensorflow\/stream_executor\/platform\/default\/dso_loader.cc:53] Could not dlopen library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: \/opt\/intel\/compilers_and_libraries_2018.3.222\/linux\/mpi\/intel64\/lib:\/opt\/intel\/compilers_and_libraries_2018.3.222\/linux\/mpi\/mic\/lib:\/opt\/intel\/compilers_and_libraries_2018.3.222\/linux\/mpi\/intel64\/lib:\/opt\/intel\/compilers_and_libraries_2018.3.222\/linux\/mpi\/mic\/lib:\/azureml-envs\/azureml_5fdf05c5671519f307e0f43128b8610e\/lib:\n2019-08-12 14:50:16.971054: E tensorflow\/stream_executor\/cuda\/cuda_driver.cc:318] failed call to cuInit: UNKNOWN ERROR (303)\n2019-08-12 14:50:16.971081: I tensorflow\/stream_executor\/cuda\/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: 4bd815dfb0e74e3da901861a4746184f000000\n2019-08-12 14:50:16.971089: I tensorflow\/stream_executor\/cuda\/cuda_diagnostics.cc:176] hostname: 4bd815dfb0e74e3da901861a4746184f000000\n2019-08-12 14:50:16.971164: I tensorflow\/stream_executor\/cuda\/cuda_diagnostics.cc:200] libcuda reported version is: Not found: was unable to find libcuda.so DSO loaded into this program\n2019-08-12 14:50:16.971202: I tensorflow\/stream_executor\/cuda\/cuda_diagnostics.cc:204] kernel reported version is: 418.40.4\nDevice mapping:\n\/job:localhost\/replica:0\/task:0\/device:XLA_CPU:0 -&gt; device: XLA_CPU device\n2019-08-12 14:50:16.973301: I tensorflow\/core\/common_runtime\/direct_session.cc:296] Device mapping:\n\/job:localhost\/replica:0\/task:0\/device:XLA_CPU:0 -&gt; device: XLA_CPU device\n\n<\/code><\/pre>\n\n<p>It's currently using the CPU as per the logs. Any clues how to resolve the issue here?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_date":"2019-08-13 04:29:57.500000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2019-08-13 04:31:41.317000 UTC",
        "Question_score":5,
        "Question_tags":"python|tensorflow|azure-machine-learning-service",
        "Question_view_count":1402,
        "Owner_creation_date":"2014-08-15 23:27:51.463000 UTC",
        "Owner_last_access_date":"2021-05-13 06:30:45.853000 UTC",
        "Owner_location":"India",
        "Owner_reputation":65,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Answer_body":"<p>Instead of base Estimator, you can use the Tensorflow Estimator with Keras and other libraries layered on top. That way you don't have to worry about setting up and configuring the GPU libraries, as the Tensorflow Estimator uses a Docker image with GPU libraries pre-configured. <\/p>\n\n<p>See here for documentation:<\/p>\n\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-train-core\/azureml.train.dnn.tensorflow?view=azure-ml-py\" rel=\"nofollow noreferrer\">API Reference<\/a> You can use <code>conda_packages<\/code> argument to specify additional libraries. Also set argument <code>use_gpu = True<\/code>.<\/p>\n\n<p><a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/training-with-deep-learning\/train-hyperparameter-tune-deploy-with-keras\/train-hyperparameter-tune-deploy-with-keras.ipynb\" rel=\"nofollow noreferrer\">Example Notebook<\/a><\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2019-08-13 14:10:12.680000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":71962260,
        "Question_title":"Reading Data in Vertex AI Pipelines",
        "Question_body":"<p>This is my first time using Google's Vertex AI Pipelines. I checked <a href=\"https:\/\/codelabs.developers.google.com\/vertex-pipelines-intro?hl=en#0\" rel=\"nofollow noreferrer\">this codelab<\/a> as well as <a href=\"https:\/\/towardsdatascience.com\/how-to-set-up-custom-vertex-ai-pipelines-step-by-step-467487f81cad\" rel=\"nofollow noreferrer\">this post<\/a> and <a href=\"https:\/\/medium.com\/google-cloud\/google-vertex-ai-the-easiest-way-to-run-ml-pipelines-3a41c5ed153\" rel=\"nofollow noreferrer\">this post<\/a>, on top of some links derived from the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/pipelines\/introduction?hl=es-419\" rel=\"nofollow noreferrer\">official documentation<\/a>. I decided to put all that knowledge to work, in some toy example: I was planning to build a pipeline consisting of 2 components: &quot;get-data&quot; (which reads some .csv file stored in Cloud Storage) and &quot;report-data&quot; (which basically returns the shape of the .csv data read in the previous component). Furthermore, I was cautious to include <a href=\"https:\/\/stackoverflow.com\/questions\/71351821\/reading-file-from-vertex-ai-and-google-cloud-storage\">some suggestions<\/a> provided in this forum. The code I currently have, goes as follows:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>\nfrom kfp.v2 import compiler\nfrom kfp.v2.dsl import pipeline, component, Dataset, Input, Output\nfrom google.cloud import aiplatform\n\n# Components section   \n\n@component(\n    packages_to_install=[\n        &quot;google-cloud-storage&quot;,\n        &quot;pandas&quot;,\n    ],\n    base_image=&quot;python:3.9&quot;,\n    output_component_file=&quot;get_data.yaml&quot;\n)\ndef get_data(\n    bucket: str,\n    url: str,\n    dataset: Output[Dataset],\n):\n    import pandas as pd\n    from google.cloud import storage\n    \n    storage_client = storage.Client(&quot;my-project&quot;)\n    bucket = storage_client.get_bucket(bucket)\n    blob = bucket.blob(url)\n    blob.download_to_filename('localdf.csv')\n    \n    # path = &quot;gs:\/\/my-bucket\/program_grouping_data.zip&quot;\n    df = pd.read_csv('localdf.csv', compression='zip')\n    df['new_skills'] = df['new_skills'].apply(ast.literal_eval)\n    df.to_csv(dataset.path + &quot;.csv&quot; , index=False, encoding='utf-8-sig')\n\n\n@component(\n    packages_to_install=[&quot;pandas&quot;],\n    base_image=&quot;python:3.9&quot;,\n    output_component_file=&quot;report_data.yaml&quot;\n)\ndef report_data(\n    inputd: Input[Dataset],\n):\n    import pandas as pd\n    df = pd.read_csv(inputd.path)\n    return df.shape\n\n\n# Pipeline section\n\n@pipeline(\n    # Default pipeline root. You can override it when submitting the pipeline.\n    pipeline_root=PIPELINE_ROOT,\n    # A name for the pipeline.\n    name=&quot;my-pipeline&quot;,\n)\ndef my_pipeline(\n    url: str = &quot;test_vertex\/pipeline_root\/program_grouping_data.zip&quot;,\n    bucket: str = &quot;my-bucket&quot;\n):\n    dataset_task = get_data(bucket, url)\n\n    dimensions = report_data(\n        dataset_task.output\n    )\n\n# Compilation section\n\ncompiler.Compiler().compile(\n    pipeline_func=my_pipeline, package_path=&quot;pipeline_job.json&quot;\n)\n\n# Running and submitting job\n\nfrom datetime import datetime\n\nTIMESTAMP = datetime.now().strftime(&quot;%Y%m%d%H%M%S&quot;)\n\nrun1 = aiplatform.PipelineJob(\n    display_name=&quot;my-pipeline&quot;,\n    template_path=&quot;pipeline_job.json&quot;,\n    job_id=&quot;mlmd-pipeline-small-{0}&quot;.format(TIMESTAMP),\n    parameter_values={&quot;url&quot;: &quot;test_vertex\/pipeline_root\/program_grouping_data.zip&quot;, &quot;bucket&quot;: &quot;my-bucket&quot;},\n    enable_caching=True,\n)\n\nrun1.submit()\n<\/code><\/pre>\n<p>I was happy to see that the pipeline compiled with no errors, and managed to submit the job. However &quot;my happiness lasted short&quot;, as when I went to Vertex AI Pipelines, I stumbled upon some &quot;error&quot;, which goes like:<\/p>\n<blockquote>\n<p>The DAG failed because some tasks failed. The failed tasks are: [get-data].; Job (project_id = my-project, job_id = 4290278978419163136) is failed due to the above error.; Failed to handle the job: {project_number = xxxxxxxx, job_id = 4290278978419163136}<\/p>\n<\/blockquote>\n<p>I did not find any related info on the web, neither could I find any log or something similar, and I feel a bit overwhelmed that the solution to this (seemingly) easy example, is still eluding me.<\/p>\n<p>Quite obviously, I don't what or where I am mistaking. Any suggestion?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":5,
        "Question_creation_date":"2022-04-22 00:40:56.410000 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":"2022-04-25 00:09:49.253000 UTC",
        "Question_score":4,
        "Question_tags":"python|google-cloud-platform|google-cloud-storage|google-cloud-vertex-ai|kubeflow-pipelines",
        "Question_view_count":892,
        "Owner_creation_date":"2021-08-19 14:58:58.957000 UTC",
        "Owner_last_access_date":"2022-09-23 17:13:29.400000 UTC",
        "Owner_location":null,
        "Owner_reputation":395,
        "Owner_up_votes":18,
        "Owner_down_votes":4,
        "Owner_views":38,
        "Answer_body":"<p>With some suggestions provided in the comments, I think I managed to make my demo pipeline work. I will first include the updated code:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from kfp.v2 import compiler\nfrom kfp.v2.dsl import pipeline, component, Dataset, Input, Output\nfrom datetime import datetime\nfrom google.cloud import aiplatform\nfrom typing import NamedTuple\n\n\n# Importing 'COMPONENTS' of the 'PIPELINE'\n\n@component(\n    packages_to_install=[\n        &quot;google-cloud-storage&quot;,\n        &quot;pandas&quot;,\n    ],\n    base_image=&quot;python:3.9&quot;,\n    output_component_file=&quot;get_data.yaml&quot;\n)\ndef get_data(\n    bucket: str,\n    url: str,\n    dataset: Output[Dataset],\n):\n    &quot;&quot;&quot;Reads a csv file, from some location in Cloud Storage&quot;&quot;&quot;\n    import ast\n    import pandas as pd\n    from google.cloud import storage\n    \n    # 'Pulling' demo .csv data from a know location in GCS\n    storage_client = storage.Client(&quot;my-project&quot;)\n    bucket = storage_client.get_bucket(bucket)\n    blob = bucket.blob(url)\n    blob.download_to_filename('localdf.csv')\n    \n    # Reading the pulled demo .csv data\n    df = pd.read_csv('localdf.csv', compression='zip')\n    df['new_skills'] = df['new_skills'].apply(ast.literal_eval)\n    df.to_csv(dataset.path + &quot;.csv&quot; , index=False, encoding='utf-8-sig')\n\n\n@component(\n    packages_to_install=[&quot;pandas&quot;],\n    base_image=&quot;python:3.9&quot;,\n    output_component_file=&quot;report_data.yaml&quot;\n)\ndef report_data(\n    inputd: Input[Dataset],\n) -&gt; NamedTuple(&quot;output&quot;, [(&quot;rows&quot;, int), (&quot;columns&quot;, int)]):\n    &quot;&quot;&quot;From a passed csv file existing in Cloud Storage, returns its dimensions&quot;&quot;&quot;\n    import pandas as pd\n    \n    df = pd.read_csv(inputd.path+&quot;.csv&quot;)\n    \n    return df.shape\n\n\n# Building the 'PIPELINE'\n\n@pipeline(\n    # i.e. in my case: PIPELINE_ROOT = 'gs:\/\/my-bucket\/test_vertex\/pipeline_root\/'\n    # Can be overriden when submitting the pipeline\n    pipeline_root=PIPELINE_ROOT,\n    name=&quot;readcsv-pipeline&quot;,  # Your own naming for the pipeline.\n)\ndef my_pipeline(\n    url: str = &quot;test_vertex\/pipeline_root\/program_grouping_data.zip&quot;,\n    bucket: str = &quot;my-bucket&quot;\n):\n    dataset_task = get_data(bucket, url)\n\n    dimensions = report_data(\n        dataset_task.output\n    )\n    \n\n# Compiling the 'PIPELINE'    \n\ncompiler.Compiler().compile(\n    pipeline_func=my_pipeline, package_path=&quot;pipeline_job.json&quot;\n)\n\n\n# Running the 'PIPELINE'\n\nTIMESTAMP = datetime.now().strftime(&quot;%Y%m%d%H%M%S&quot;)\n\nrun1 = aiplatform.PipelineJob(\n    display_name=&quot;my-pipeline&quot;,\n    template_path=&quot;pipeline_job.json&quot;,\n    job_id=&quot;mlmd-pipeline-small-{0}&quot;.format(TIMESTAMP),\n    parameter_values={\n        &quot;url&quot;: &quot;test_vertex\/pipeline_root\/program_grouping_data.zip&quot;,\n        &quot;bucket&quot;: &quot;my-bucket&quot;\n    },\n    enable_caching=True,\n)\n\n# Submitting the 'PIPELINE'\n\nrun1.submit()\n<\/code><\/pre>\n<p>Now, I will add some complementary comments, which in sum, managed to solve my problem:<\/p>\n<ul>\n<li>First, having the &quot;Logs Viewer&quot; (roles\/logging.viewer) enabled for your user, will greatly help to troubleshoot any existing error in your pipeline (Note: that role worked for me, however you might want to look for a better matching role for you own purposes <a href=\"https:\/\/cloud.google.com\/logging\/docs\/access-control?&amp;_ga=2.212939101.-833851896.1650314090&amp;_gac=1.182768084.1650632490.Cj0KCQjwpImTBhCmARIsAKr58cw9X0TGy2YeN-CFsM4RvEXDH_vL54Ce1ECrWh4_aJNoPEhgtXusUhwaAlIUEALw_wcB#permissions_and_roles\" rel=\"nofollow noreferrer\">here<\/a>). Those errors will appear as &quot;Logs&quot;, which can be accessed by clicking the corresponding button:<\/li>\n<\/ul>\n<p><a href=\"https:\/\/i.stack.imgur.com\/jpgfU.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/jpgfU.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<ul>\n<li>NOTE: In the picture above, when the &quot;Logs&quot; are displayed, it might be helpful to carefully check each log (close to the time when you created you pipeline), as generally each eof them corresponds with a single warning or error line:<\/li>\n<\/ul>\n<p><a href=\"https:\/\/i.stack.imgur.com\/CpTkZ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/CpTkZ.png\" alt=\"Verte AI Pipelines Logs\" \/><\/a><\/p>\n<ul>\n<li>Second, the output of my pipeline was a tuple. In my original approach, I just returned the plain tuple, but it is advised to return a <a href=\"https:\/\/docs.python.org\/3\/library\/typing.html#typing.NamedTuple\" rel=\"nofollow noreferrer\">NamedTuple<\/a> instead. In general, if you need to input \/ output one or more &quot;<em>small values<\/em>&quot; (int or str, for any reason), pick a NamedTuple to do so.<\/li>\n<li>Third, when the connection between your pipelines is <code>Input[Dataset]<\/code> or <code>Ouput[Dataset]<\/code>, adding the file extension is needed (and quite easy to forget). Take for instance the ouput of the <code>get_data<\/code> component, and notice how the data is recorded by specifically adding the file extension, i.e. <code>dataset.path + &quot;.csv&quot;<\/code>.<\/li>\n<\/ul>\n<p>Of course, this is a very tiny example, and projects can easily scale to huge projects, however as some sort of &quot;Hello Vertex AI Pipelines&quot; it will work well.<\/p>\n<p>Thank you.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-04-22 19:57:49.577000 UTC",
        "Answer_last_edit_date":"2022-04-25 07:07:54.733000 UTC",
        "Answer_score":4.0,
        "Question_valid_tags":[
            "google-cloud-vertex-ai"
        ]
    },
    {
        "Question_id":37140987,
        "Question_title":"power by dashboard with machine learning azure data",
        "Question_body":"<p>Hi I have a web service which is the result of my machine learning azure training. I would like to set a new datasource in power bi, which calls the web service with the current datetime as a parameter in order to create a report with the result predictions. I cannot find a way to call the api. Is this any? I am thinking another solution of creating a service and execute the api, and insert the result in a table in order to connect to this table. But, I would like to avoid doing this.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2016-05-10 14:09:39.010000 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"azure|powerbi|azure-machine-learning-studio",
        "Question_view_count":444,
        "Owner_creation_date":"2016-01-05 14:27:32.637000 UTC",
        "Owner_last_access_date":"2018-11-22 19:35:34.697000 UTC",
        "Owner_location":"Buenos Aires, Argentina",
        "Owner_reputation":606,
        "Owner_up_votes":64,
        "Owner_down_votes":1,
        "Owner_views":168,
        "Answer_body":"<p>I used something called Azure Data Factory (ADF). It allows you to schedule a job by defining a pipeline with activities. There are activities for training your model or scoring your predictive ML. The scoring result, I am storing it in Azure DB (it could be another storage) and connected it to Power BI.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2016-05-13 14:43:12.243000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":3.0,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":36563769,
        "Question_title":"How to download the entire scored dataset from Azure machine studio?",
        "Question_body":"<p>I have an experiment in azure machine learning studio, and I would like to the see entire scored dataset.<\/p>\n\n<p>Naturally I used the 'visualise' option on the scored dataset but these yields only 100 rows (the test dataset is around 500 rows)<\/p>\n\n<p>I also tired the 'save as dataset' option, but then file does not open well with excel or text editor (special character encoding)<\/p>\n\n<p>Basically, I want to see the entire test data with scored labels as table or download as .csv maybe<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2016-04-12 04:42:39.040000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"azure-machine-learning-studio",
        "Question_view_count":5296,
        "Owner_creation_date":"2014-07-25 05:27:39.940000 UTC",
        "Owner_last_access_date":"2022-09-15 08:56:29.147000 UTC",
        "Owner_location":"Link\u00f6ping, Sweden",
        "Owner_reputation":1677,
        "Owner_up_votes":82,
        "Owner_down_votes":2,
        "Owner_views":221,
        "Answer_body":"<p>Please try the Convert to CSV module: <a href=\"https:\/\/msdn.microsoft.com\/library\/azure\/faa6ba63-383c-4086-ba58-7abf26b85814\" rel=\"noreferrer\">https:\/\/msdn.microsoft.com\/library\/azure\/faa6ba63-383c-4086-ba58-7abf26b85814<\/a><\/p>\n\n<p>After you run the experiment, right click on the output of the module to download the CSV file.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2016-04-12 04:57:38.280000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":14.0,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":71127858,
        "Question_title":"Cant install imbalanced-learn on an Azure ML Environment",
        "Question_body":"<p>I have an Azure ML Workspace which comes by default with some pre-installed packages.<\/p>\n<p>I tried to install<\/p>\n<pre><code>!pip install -U imbalanced-learn\n<\/code><\/pre>\n<p>But I got this error<\/p>\n<pre><code>Requirement already up-to-date: scikit-learn in \/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages (0.24.2)\nRequirement already satisfied, skipping upgrade: scipy&gt;=0.19.1 in \/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages (from scikit-learn) (1.4.1)\nRequirement already satisfied, skipping upgrade: joblib&gt;=0.11 in \/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages (from scikit-learn) (0.14.1)\nRequirement already satisfied, skipping upgrade: numpy&gt;=1.13.3 in \/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages (from scikit-learn) (1.18.5)\nRequirement already satisfied, skipping upgrade: threadpoolctl&gt;=2.0.0 in \/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages (from scikit-learn) (2.1.0)\nCollecting imbalanced-learn\n  Using cached imbalanced_learn-0.9.0-py3-none-any.whl (199 kB)\nRequirement already satisfied, skipping upgrade: threadpoolctl&gt;=2.0.0 in \/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages (from imbalanced-learn) (2.1.0)\nRequirement already satisfied, skipping upgrade: joblib&gt;=0.11 in \/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages (from imbalanced-learn) (0.14.1)\nRequirement already satisfied, skipping upgrade: scipy&gt;=1.1.0 in \/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages (from imbalanced-learn) (1.4.1)\nERROR: Could not find a version that satisfies the requirement scikit-learn&gt;=1.0.1 (from imbalanced-learn) (from versions: 0.9, 0.10, 0.11, 0.12, 0.12.1, 0.13, 0.13.1, 0.14, 0.14.1, 0.15.0b1, 0.15.0b2, 0.15.0, 0.15.1, 0.15.2, 0.16b1, 0.16.0, 0.16.1, 0.17b1, 0.17, 0.17.1, 0.18, 0.18.1, 0.18.2, 0.19b2, 0.19.0, 0.19.1, 0.19.2, 0.20rc1, 0.20.0, 0.20.1, 0.20.2, 0.20.3, 0.20.4, 0.21rc2, 0.21.0, 0.21.1, 0.21.2, 0.21.3, 0.22rc2.post1, 0.22rc3, 0.22, 0.22.1, 0.22.2, 0.22.2.post1, 0.23.0rc1, 0.23.0, 0.23.1, 0.23.2, 0.24.dev0, 0.24.0rc1, 0.24.0, 0.24.1, 0.24.2)\nERROR: No matching distribution found for scikit-learn&gt;=1.0.1 (from imbalanced-\n<\/code><\/pre>\n<p>learn)<\/p>\n<p>Not sure how to solve this, I have read in other posts to use conda, but that didnt work either.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2022-02-15 14:06:37.077000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-02-15 21:26:00.047000 UTC",
        "Question_score":1,
        "Question_tags":"python|scikit-learn|pip|azure-machine-learning-service",
        "Question_view_count":219,
        "Owner_creation_date":"2011-04-05 19:05:03.093000 UTC",
        "Owner_last_access_date":"2022-09-16 12:42:27.473000 UTC",
        "Owner_location":"Brussels, B\u00e9lgica",
        "Owner_reputation":30340,
        "Owner_up_votes":1667,
        "Owner_down_votes":79,
        "Owner_views":2937,
        "Answer_body":"<p><a href=\"https:\/\/pypi.org\/project\/scikit-learn\/1.0.1\/\" rel=\"nofollow noreferrer\"><code>scikit-learn<\/code> 1.0.1<\/a> and up require Python &gt;= 3.7; you use Python 3.6. You need to upgrade Python or downgrade <code>imbalanced-learn<\/code>. <a href=\"https:\/\/pypi.org\/project\/imbalanced-learn\/0.8.1\/\" rel=\"nofollow noreferrer\"><code>imbalanced-learn<\/code> 0.8.1<\/a> allows Python 3.6 so<\/p>\n<pre><code>!pip install -U &quot;imbalanced-learn &lt; 0.9&quot;\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2022-02-15 14:23:56.627000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_valid_tags":[
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":69036277,
        "Question_title":"Run independent `PythonScriptStep` steps in parallel",
        "Question_body":"<p>In my pipeline multiple steps are independent and so I would like them to run in parallel based on input dependencies.<\/p>\n<p>As the compute I use has multiple nodes I would have expected this to be the default.<\/p>\n<p>For example:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Iye85.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Iye85.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>All 3 upper steps should run in parallel, then both <code>finetune<\/code> steps in parallel as soon as their inputs are satisfied and the same for <code>rgb_test<\/code>.<\/p>\n<p>Currently only 1 step runs at a time, the other are <code>Queued<\/code>.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_date":"2021-09-02 19:49:26.280000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"azure-machine-learning-service",
        "Question_view_count":156,
        "Owner_creation_date":"2012-01-26 14:27:40.553000 UTC",
        "Owner_last_access_date":"2022-09-24 16:26:41.580000 UTC",
        "Owner_location":null,
        "Owner_reputation":802,
        "Owner_up_votes":288,
        "Owner_down_votes":0,
        "Owner_views":91,
        "Answer_body":"<p>It ended up being because of vCPU quota.<\/p>\n<p>After increasing the quota, parallel tasks can run at the same time as expected.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-09-27 19:02:52.630000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":73230096,
        "Question_title":"How to create a new VertexAI Workbench Managed Notebook using gcloud",
        "Question_body":"<p>There is a gcloud command to create a user-managed notebook instance.<\/p>\n<pre><code>gcloud notebooks instances create \n<\/code><\/pre>\n<p>Is is possible to create a managed notebook with gcloud?<\/p>\n<p>It looks to be possible in the <a href=\"https:\/\/stackoverflow.com\/questions\/70223161\/how-to-create-a-new-workbench-managed-notebook-using-rest-api\">API<\/a>. I can't find a gcloud reference.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_date":"2022-08-04 03:37:25.230000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"jupyter-notebook|gcloud|google-cloud-vertex-ai|gcp-ai-platform-notebook",
        "Question_view_count":107,
        "Owner_creation_date":"2013-01-30 04:13:24.743000 UTC",
        "Owner_last_access_date":"2022-09-22 00:56:52.800000 UTC",
        "Owner_location":"Melbourne, Australia",
        "Owner_reputation":3943,
        "Owner_up_votes":747,
        "Owner_down_votes":5,
        "Owner_views":339,
        "Answer_body":"<p>As mentioned by @gogasca, the gcloud SDK for creating managed notebook is currently under work. Meanwhile you can try client libraries and REST API to create the same.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-08-11 07:51:47.547000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_valid_tags":[
            "google-cloud-vertex-ai"
        ]
    },
    {
        "Question_id":72097417,
        "Question_title":"Segfault using htop on AWS Sagemaker pytorch-1.10-cpu-py38 app",
        "Question_body":"<p>I am trying to launch the htop command in the Pytorch 1.10 - Python 3.8 CPU optimized AWS Sagemaker container. This works fine in other images I have used till now, but in this one, the command fails with a segfault:<\/p>\n<pre><code>htop \nhtop: \/opt\/conda\/lib\/libncursesw.so.6: no version information available (required by htop) \nhtop: \/opt\/conda\/lib\/libncursesw.so.6: no version information available (required by htop) \nhtop: \/opt\/conda\/lib\/libncursesw.so.6: no version information available (required by htop) \nSegmentation fault (core dumped)\n<\/code><\/pre>\n<p>More info :<\/p>\n<pre><code>htop --version\nhtop: \/opt\/conda\/lib\/libncursesw.so.6: no version information available (required by htop)\nhtop: \/opt\/conda\/lib\/libncursesw.so.6: no version information available (required by htop)\nhtop: \/opt\/conda\/lib\/libncursesw.so.6: no version information available (required by htop)\nhtop 2.2.0 - (C) 2004-2019 Hisham Muhammad\nReleased under the GNU GPL.\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-05-03 09:33:22.183000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"python|amazon-web-services|conda|amazon-sagemaker|htop",
        "Question_view_count":147,
        "Owner_creation_date":"2019-03-01 09:06:03.127000 UTC",
        "Owner_last_access_date":"2022-08-11 13:18:40.803000 UTC",
        "Owner_location":"Paris, France",
        "Owner_reputation":494,
        "Owner_up_votes":133,
        "Owner_down_votes":0,
        "Owner_views":24,
        "Answer_body":"<p>I fixed this with<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code># Note: add sudo if needed:\nln -fs \/lib\/x86_64-linux-gnu\/libncursesw.so.6 \/opt\/conda\/lib\/libncursesw.so.6\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-06-17 09:56:35.693000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":61389632,
        "Question_title":"Import error while Executing AWS Predictive Maintenance Using Machine Learning Sample",
        "Question_body":"<p>We are trying to execute and check what kind of output is provided by Predictive Maintenance Using Machine Learning on AWS sample data. We are referring <a href=\"https:\/\/aws.amazon.com\/solutions\/predictive-maintenance-using-machine-learning\/\" rel=\"nofollow noreferrer\">Predictive Maintenance Using Machine Learning<\/a> and <a href=\"https:\/\/docs.aws.amazon.com\/solutions\/latest\/predictive-maintenance-using-machine-learning\/welcome.html\" rel=\"nofollow noreferrer\">AWS Guide<\/a> to launch the sample template provided by the AWS. The template is executed properly and we can see the resources in account. Whenever we run the sagemaker notebook for the given example we are getting the error in CloudWatch logs as follows<\/p>\n\n<pre><code>ImportError: cannot import name 'replace_file' on line from mxnet.gluon.utils import download, check_sha1, _get_repo_file_url, replace_file.\n<\/code><\/pre>\n\n<p>This is the stage where the invoke the training job. We have tried following options to resolve the issue.<\/p>\n\n<ul>\n<li>Upgrading the mxnet module<\/li>\n<li>Upgrading the tensorflow module<\/li>\n<\/ul>\n\n<p>But no success.<\/p>\n\n<p>Thanks in advance.<\/p>\n\n<p>Error Traceback is as follows<\/p>\n\n<pre><code>  File \"\/usr\/lib\/python3.5\/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"\/usr\/lib\/python3.5\/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"\/opt\/ml\/code\/sagemaker_predictive_maintenance_entry_point.py\", line 10, in &lt;module&gt;\n    import gluonnlp\n  File \"\/usr\/local\/lib\/python3.5\/dist-packages\/gluonnlp\/__init__.py\", line 25, in &lt;module&gt;\n    from . import data\n  File \"\/usr\/local\/lib\/python3.5\/dist-packages\/gluonnlp\/data\/__init__.py\", line 23, in &lt;module&gt;\n    from . import (batchify, candidate_sampler, conll, corpora, dataloader,\n  File \"\/usr\/local\/lib\/python3.5\/dist-packages\/gluonnlp\/data\/question_answering.py\", line 31, in &lt;module&gt;\n    from mxnet.gluon.utils import download, check_sha1, _get_repo_file_url, replace_file\n    ImportError: cannot import name 'replace_file'\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-04-23 14:27:38.893000 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"python|amazon-web-services|tensorflow|amazon-sagemaker",
        "Question_view_count":398,
        "Owner_creation_date":"2016-09-13 12:35:38.817000 UTC",
        "Owner_last_access_date":"2022-09-12 09:54:06.233000 UTC",
        "Owner_location":null,
        "Owner_reputation":23,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":"<p>A fix for this issue is being deployed to the official solution. In the meantime, you can make the changes described <a href=\"https:\/\/github.com\/awslabs\/predictive-maintenance-using-machine-learning\/pull\/7\/files\" rel=\"nofollow noreferrer\">here<\/a> in your SageMaker environment by following the instructions below:<\/p>\n\n<p>1) In the notebook, please change the <code>framework_version<\/code> to <code>1.6.0<\/code>.<\/p>\n\n<pre><code>MXNet(entry_point='sagemaker_predictive_maintenance_entry_point.py',\n          source_dir='sagemaker_predictive_maintenance_entry_point',\n          py_version='py3',\n          role=role, \n          train_instance_count=1, \n          train_instance_type=train_instance_type,\n          output_path=output_location,\n          hyperparameters={'num-datasets' : len(train_df),\n                           'num-gpus': 1,\n                           'epochs': 500,\n                           'optimizer': 'adam',\n                           'batch-size':1,\n                           'log-interval': 100},\n         input_mode='File',\n         train_max_run=7200,\n         framework_version='1.6.0')  &lt;- Change this to 1.6.0.\n<\/code><\/pre>\n\n<p>2) This will likely fix things, but just to be sure you don't have any stale packages, change the <code>requirements.txt<\/code> file as well.<\/p>\n\n<p>You'll need to open up a terminal in SageMaker.\n<a href=\"https:\/\/i.stack.imgur.com\/0Vn6l.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/0Vn6l.png\" alt=\"enter image description here\"><\/a>\nimage taken from <a href=\"https:\/\/medium.com\/swlh\/jupyter-notebook-on-amazon-sagemaker-getting-started-55489f500439\" rel=\"nofollow noreferrer\">https:\/\/medium.com\/swlh\/jupyter-notebook-on-amazon-sagemaker-getting-started-55489f500439<\/a><\/p>\n\n<p>and run<\/p>\n\n<pre><code>cd SageMaker\/sagemaker_predictive_maintenance_entry_point\/\nsudo vim requirements.txt  # (or sudo nano requirements.txt)\n<\/code><\/pre>\n\n<p>Change the contents to:<\/p>\n\n<pre><code>gluonnlp==0.9.1\npandas==0.22\n<\/code><\/pre>\n\n<p>Save it, and then run the example again.<\/p>\n\n<p>Feel free to comment on the issue as well:\n<a href=\"https:\/\/github.com\/awslabs\/predictive-maintenance-using-machine-learning\/issues\/6\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/predictive-maintenance-using-machine-learning\/issues\/6<\/a> <\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-04-28 08:50:49.633000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":70191668,
        "Question_title":"What are SageMaker pipelines actually?",
        "Question_body":"<p>Sagemaker pipelines are rather unclear to me, I'm not experienced in the field of ML but I'm working on figuring out the pipeline definitions.<\/p>\n<p>I have a few questions:<\/p>\n<ul>\n<li><p>Is sagemaker pipelines a stand-alone service\/feature? Because I don't see any option to create them through the console, though I do see CloudFormation and CDK resources.<\/p>\n<\/li>\n<li><p>Is a sagemaker pipeline essentially codepipeline? How do these integrate, how do these differ?<\/p>\n<\/li>\n<li><p>There's also a Python SDK, how does this differ from the CDK and CloudFormation?<\/p>\n<\/li>\n<\/ul>\n<p>I can't seem to find any examples besides the Python SDK usage, how come?<\/p>\n<p>The docs and workshops seem only to properly describe the Python SDK usage,it would be really helpful if someone could clear this up for me!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-12-01 21:50:43.060000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":3,
        "Question_tags":"amazon-web-services|amazon-sagemaker|mlops",
        "Question_view_count":716,
        "Owner_creation_date":"2020-01-05 18:52:39.257000 UTC",
        "Owner_last_access_date":"2022-09-18 18:55:51.313000 UTC",
        "Owner_location":"Amsterdam",
        "Owner_reputation":197,
        "Owner_up_votes":53,
        "Owner_down_votes":7,
        "Owner_views":49,
        "Answer_body":"<p>SageMaker has two things called Pipelines: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/pipelines.html\" rel=\"nofollow noreferrer\">Model Building Pipelines<\/a> and <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-pipelines.html\" rel=\"nofollow noreferrer\">Serial Inference Pipelines<\/a>. I believe you're referring to the former<\/p>\n<p>A model building pipeline defines steps in a machine learning workflow, such as pre-processing, hyperparameter tuning, batch transformations, and setting up endpoints<\/p>\n<p>A serial inference pipeline is two or more SageMaker models run one after the other<\/p>\n<p>A model building pipeline is defined in JSON, and is hosted\/run in some sort of proprietary, serverless fashion by SageMaker<\/p>\n<blockquote>\n<p>Is sagemaker pipelines a stand-alone service\/feature? Because I don't see any option to create them through the console, though I do see CloudFormation and CDK resources.<\/p>\n<\/blockquote>\n<p>You can create\/modify them using the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreatePipeline.html\" rel=\"nofollow noreferrer\">API<\/a>, which can also be called via the <a href=\"https:\/\/docs.aws.amazon.com\/cli\/latest\/reference\/sagemaker\/create-pipeline.html\" rel=\"nofollow noreferrer\">CLI<\/a>, <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/workflows\/pipelines\/sagemaker.workflow.pipelines.html#sagemaker.workflow.pipeline.Pipeline.create\" rel=\"nofollow noreferrer\">Python SDK<\/a>, or <a href=\"https:\/\/docs.aws.amazon.com\/AWSCloudFormation\/latest\/UserGuide\/aws-resource-sagemaker-pipeline.html\" rel=\"nofollow noreferrer\">CloudFormation<\/a>. These all use the AWS API under the hood<\/p>\n<p>You can start\/stop\/view them in SageMaker Studio:<\/p>\n<pre><code>Left-side Navigation bar &gt; SageMaker resources &gt; Drop-down menu &gt; Pipelines\n<\/code><\/pre>\n<blockquote>\n<p>Is a sagemaker pipeline essentially codepipeline? How do these integrate, how do these differ?<\/p>\n<\/blockquote>\n<p>Unlikely. CodePipeline is more for building and deploying code, not specific to SageMaker. There is no direct integration as far as I can tell, other than that you can start a SM pipeline with CP<\/p>\n<blockquote>\n<p>There's also a Python SDK, how does this differ from the CDK and CloudFormation?<\/p>\n<\/blockquote>\n<p>The Python SDK is a stand-alone library to interact with SageMaker in a developer-friendly fashion. It's more dynamic than CloudFormation. Let's you build pipelines using code. Whereas CloudFormation takes a static JSON string<\/p>\n<p>A very simple example of Python SageMaker SDK usage:<\/p>\n\n<pre class=\"lang-python prettyprint-override\"><code>processor = SKLearnProcessor(\n    framework_version=&quot;0.23-1&quot;,\n    instance_count=1,\n    instance_type=&quot;ml.m5.large&quot;,\n    role=&quot;role-arn&quot;,\n)\n\nprocessing_step = ProcessingStep(\n    name=&quot;processing&quot;,\n    processor=processor,\n    code=&quot;preprocessor.py&quot;\n)\n\npipeline = Pipeline(name=&quot;foo&quot;, steps=[processing_step])\npipeline.upsert(role_arn = ...)\npipeline.start()\n<\/code><\/pre>\n<p><code>pipeline.definition()<\/code> produces rather verbose JSON like this:<\/p>\n\n<pre class=\"lang-json prettyprint-override\"><code>{\n&quot;Version&quot;: &quot;2020-12-01&quot;,\n&quot;Metadata&quot;: {},\n&quot;Parameters&quot;: [],\n&quot;PipelineExperimentConfig&quot;: {\n    &quot;ExperimentName&quot;: {\n        &quot;Get&quot;: &quot;Execution.PipelineName&quot;\n    },\n    &quot;TrialName&quot;: {\n        &quot;Get&quot;: &quot;Execution.PipelineExecutionId&quot;\n    }\n},\n&quot;Steps&quot;: [\n    {\n        &quot;Name&quot;: &quot;processing&quot;,\n        &quot;Type&quot;: &quot;Processing&quot;,\n        &quot;Arguments&quot;: {\n            &quot;ProcessingResources&quot;: {\n                &quot;ClusterConfig&quot;: {\n                    &quot;InstanceType&quot;: &quot;ml.m5.large&quot;,\n                    &quot;InstanceCount&quot;: 1,\n                    &quot;VolumeSizeInGB&quot;: 30\n                }\n            },\n            &quot;AppSpecification&quot;: {\n                &quot;ImageUri&quot;: &quot;246618743249.dkr.ecr.us-west-2.amazonaws.com\/sagemaker-scikit-learn:0.23-1-cpu-py3&quot;,\n                &quot;ContainerEntrypoint&quot;: [\n                    &quot;python3&quot;,\n                    &quot;\/opt\/ml\/processing\/input\/code\/preprocessor.py&quot;\n                ]\n            },\n            &quot;RoleArn&quot;: &quot;arn:aws:iam::123456789012:role\/foo&quot;,\n            &quot;ProcessingInputs&quot;: [\n                {\n                    &quot;InputName&quot;: &quot;code&quot;,\n                    &quot;AppManaged&quot;: false,\n                    &quot;S3Input&quot;: {\n                        &quot;S3Uri&quot;: &quot;s3:\/\/bucket\/preprocessor.py&quot;,\n                        &quot;LocalPath&quot;: &quot;\/opt\/ml\/processing\/input\/code&quot;,\n                        &quot;S3DataType&quot;: &quot;S3Prefix&quot;,\n                        &quot;S3InputMode&quot;: &quot;File&quot;,\n                        &quot;S3DataDistributionType&quot;: &quot;FullyReplicated&quot;,\n                        &quot;S3CompressionType&quot;: &quot;None&quot;\n                    }\n                }\n            ]\n        }\n    }\n  ]\n}\n<\/code><\/pre>\n<p>You could <em>use<\/em> the above JSON with CloudFormation\/CDK, but you <em>build<\/em> the JSON with the SageMaker SDK<\/p>\n<p>You can also define model building workflows using Step Function State Machines, using the <a href=\"https:\/\/aws-step-functions-data-science-sdk.readthedocs.io\/en\/stable\/\" rel=\"nofollow noreferrer\">Data Science SDK<\/a>, or <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/workflows\/airflow\/index.html\" rel=\"nofollow noreferrer\">Airflow<\/a><\/p>",
        "Answer_comment_count":6.0,
        "Answer_creation_date":"2021-12-01 22:01:10.903000 UTC",
        "Answer_last_edit_date":"2022-06-02 19:24:10.167000 UTC",
        "Answer_score":2.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":52053776,
        "Question_title":"A\/B Test feature in SageMaker: variant assignment is random?",
        "Question_body":"<p>A\/B test feature in SageMaker sounds so intriguing but the more I looked into, the more I am confused whether this is a useful feature. For this to be useful, you need to get the variant assignment data back and join with some internal data to figure out the best performing variant.<\/p>\n\n<p>How is this assignment done? Is it purely random? Or am I supposed to pass some kind of ID (or hashed ID) which can indicate a person or a browser so that the same model is picked up for the same person.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2018-08-28 08:51:33.390000 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":614,
        "Owner_creation_date":"2011-05-13 06:51:53.437000 UTC",
        "Owner_last_access_date":"2022-09-25 04:56:44.010000 UTC",
        "Owner_location":null,
        "Owner_reputation":10317,
        "Owner_up_votes":268,
        "Owner_down_votes":1,
        "Owner_views":595,
        "Answer_body":"<blockquote>\n  <p>For this to be useful, you need to get the variant assignment data back and join with some internal data to figure out the best performing variant. <\/p>\n<\/blockquote>\n\n<p>The InvokeEndpoint response includes the \"InvokedProductionVariant\", in order to support the kind of analysis you describe. Details can be found in the API documentation: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_runtime_InvokeEndpoint.html#API_runtime_InvokeEndpoint_ResponseSyntax\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_runtime_InvokeEndpoint.html#API_runtime_InvokeEndpoint_ResponseSyntax<\/a><\/p>\n\n<blockquote>\n  <p>How is this assignment done? Is it purely random? <\/p>\n<\/blockquote>\n\n<p>Traffic is distributed randomly while remaining proportional to the weight of the production variant.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2018-08-28 21:51:17.687000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":60506398,
        "Question_title":"How do I use an environment in an ML Azure Pipeline",
        "Question_body":"<p><strong>Background<\/strong><\/p>\n\n<p>I have created an ML Workspace environment from a conda <code>environment.yml<\/code> plus some docker config and environment variables. I can access it from within a Python notebook:<\/p>\n\n<pre><code>env = Environment.get(workspace=ws, name='my-environment', version='1')\n<\/code><\/pre>\n\n<p>I can use this successfully to run a Python script as an experiment, i.e.<\/p>\n\n<pre><code>runconfig = ScriptRunConfig(source_directory='script\/', script='my-script.py', arguments=script_params)\nrunconfig.run_config.target = compute_target\nrunconfig.run_config.environment = env\nrun = exp.submit(runconfig)\n<\/code><\/pre>\n\n<p><strong>Problem<\/strong><\/p>\n\n<p>I would now like to run this same script as a Pipeline, so that I can trigger multiple runs with different parameters. I have created the Pipeline as follows:<\/p>\n\n<pre><code>pipeline_step = PythonScriptStep(\n    source_directory='script', script_name='my-script.py',\n    arguments=['-a', param1, '-b', param2],\n    compute_target=compute_target,\n    runconfig=runconfig\n)\nsteps = [pipeline_step]\npipeline = Pipeline(workspace=ws, steps=steps)\npipeline.validate()\n<\/code><\/pre>\n\n<p>When I then try to run the Pipeline:<\/p>\n\n<pre><code>pipeline_run = Experiment(ws, 'my_pipeline_run').submit(\n    pipeline, pipeline_parameters={...}\n)\n<\/code><\/pre>\n\n<p>I get the following error: <code>Response status code does not indicate success: 400 (Conda dependencies were not specified. Please make sure that all conda dependencies were specified i).<\/code><\/p>\n\n<p>When I view the pipeline run in the Azure Portal it seems that the environment has not been picked up: none of my conda dependencies are configured, hence the code does not run. What am I doing wrong?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-03-03 11:37:15.267000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-03-03 23:14:33.840000 UTC",
        "Question_score":3,
        "Question_tags":"python|azure|azure-machine-learning-studio|azure-machine-learning-service",
        "Question_view_count":1972,
        "Owner_creation_date":"2014-06-25 12:11:55.160000 UTC",
        "Owner_last_access_date":"2022-09-24 10:12:50.483000 UTC",
        "Owner_location":"London, UK",
        "Owner_reputation":1534,
        "Owner_up_votes":171,
        "Owner_down_votes":3,
        "Owner_views":56,
        "Answer_body":"<p>You're almost there, but you need to use <code>RunConfiguration<\/code> instead of <code>ScriptRunConfig<\/code>. More info <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/machine-learning-pipelines\/intro-to-pipelines\/aml-pipelines-getting-started.ipynb\" rel=\"noreferrer\">here<\/a><\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core.runconfig import RunConfiguration\n\nenv = Environment.get(workspace=ws, name='my-environment', version='1')\n# create a new runconfig object\nrunconfig = RunConfiguration()\nrunconfig.environment = env\n\npipeline_step = PythonScriptStep(\n    source_directory='script', script_name='my-script.py',\n    arguments=['-a', param1, '-b', param2],\n    compute_target=compute_target,\n    runconfig=runconfig\n)\n\npipeline = Pipeline(workspace=ws, steps=[pipeline_step])\n\npipeline_run = Experiment(ws, 'my_pipeline_run').submit(pipeline)\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-03-03 17:07:50.993000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":8.0,
        "Question_valid_tags":[
            "azure-machine-learning-studio",
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":56851463,
        "Question_title":"How do I specify mlflow MLproject with zero parameters?",
        "Question_body":"<p>I tried to create MLproject with zero parameters as:<\/p>\n\n<pre><code>name: test\n\nconda_env: conda.yaml\n\nentry_points:\n  main:\n    parameters:\n    command: \"python test.py\"\n<\/code><\/pre>\n\n<p>when I get an error:<\/p>\n\n<pre><code>  Traceback (most recent call last):\n File \"\/home\/ubuntu\/.local\/bin\/mlflow\", line 11, in &lt;module&gt;\n    sys.exit(cli())\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/click\/core.py\", line 764, in __call__\n    return self.main(*args, **kwargs)\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/click\/core.py\", line 717, in main\n    rv = self.invoke(ctx)\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/click\/core.py\", line 1137, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/click\/core.py\", line 956, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/click\/core.py\", line 555, in invoke\n    return callback(*args, **kwargs)\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/mlflow\/cli.py\", line 137, in run\n    run_id=run_id,\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/mlflow\/projects\/__init__.py\", line 230, in run\n    use_conda=use_conda, storage_dir=storage_dir, synchronous=synchronous, run_id=run_id)\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/mlflow\/projects\/__init__.py\", line 85, in _run\n    project = _project_spec.load_project(work_dir)\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/mlflow\/projects\/_project_spec.py\", line 40, in load_project\n    entry_points[name] = EntryPoint(name, parameters, command)\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/mlflow\/projects\/_project_spec.py\", line 87, in __init__\n    self.parameters = {k: Parameter(k, v) for (k, v) in parameters.items()}\nAttributeError: 'NoneType' object has no attribute 'items'\n<\/code><\/pre>\n\n<p>Am I missing something or mlflow does not allow project with  zero parameters?<\/p>\n\n<p>I have also posted this at my public repo of: <a href=\"https:\/\/github.com\/sameermahajan\/mlflow-try\" rel=\"nofollow noreferrer\">https:\/\/github.com\/sameermahajan\/mlflow-try<\/a> if someone would like to try out:<\/p>\n\n<pre><code>mlflow run https:\/\/github.com\/sameermahajan\/mlflow-try.git\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-07-02 11:29:36.100000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"mlflow",
        "Question_view_count":353,
        "Owner_creation_date":"2013-11-13 11:51:02.317000 UTC",
        "Owner_last_access_date":"2022-09-23 07:08:32.330000 UTC",
        "Owner_location":"Pune, Maharashtra, India",
        "Owner_reputation":478,
        "Owner_up_votes":65,
        "Owner_down_votes":4,
        "Owner_views":118,
        "Answer_body":"<p>For this, you completely drop the 'parameters' section as below:<\/p>\n\n<pre><code>name: test\n\nconda_env: conda.yaml\n\nentry_points:\n  main:\n    command: \"python test.py\"\n<\/code><\/pre>\n\n<p>(I thought I had tried it earlier but I was trying too many different ways to may be miss out on this one)<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2019-07-04 11:42:23.613000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_valid_tags":[
            "mlflow"
        ]
    },
    {
        "Question_id":69681031,
        "Question_title":"how to concatenate the OutputPathPlaceholder with a string with Kubeflow pipelines?",
        "Question_body":"<p>I am using Kubeflow pipelines (KFP) with GCP Vertex AI pipelines. I am using <code>kfp==1.8.5<\/code> (kfp SDK) and <code>google-cloud-pipeline-components==0.1.7<\/code>. Not sure if I can find which version of Kubeflow is used on GCP.<\/p>\n<p>I am bulding a component (yaml) using python inspired form this <a href=\"https:\/\/github.com\/kubeflow\/pipelines\/issues\/3748#issuecomment-627698554\" rel=\"nofollow noreferrer\">Github issue<\/a>. I am defining an output like:<\/p>\n<pre><code>outputs=[(OutputSpec(name='drt_model', type='Model'))]\n<\/code><\/pre>\n<p>This will be a base output directory to store few artifacts on Cloud Storage like model checkpoints and model.<\/p>\n<p>I would to keep one base output directory but add sub directories depending of the artifact:<\/p>\n<ul>\n<li>&lt;output_dir_base&gt;\/model<\/li>\n<li>&lt;output_dir_base&gt;\/checkpoints<\/li>\n<li>&lt;output_dir_base&gt;\/tensorboard<\/li>\n<\/ul>\n<p>but I didn't find how to concatenate the <strong>OutputPathPlaceholder('drt_model')<\/strong> with a string like <strong>'\/model'<\/strong>.<\/p>\n<p>How can append extra folder structure like \/model or \/tensorboard to the OutputPathPlaceholder that KFP will set during run time ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2021-10-22 17:36:32.977000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"kubeflow-pipelines|google-cloud-vertex-ai",
        "Question_view_count":312,
        "Owner_creation_date":"2016-06-06 14:08:12.253000 UTC",
        "Owner_last_access_date":"2022-09-22 14:59:43.617000 UTC",
        "Owner_location":"Z\u00fcrich, Switzerland",
        "Owner_reputation":1414,
        "Owner_up_votes":258,
        "Owner_down_votes":3,
        "Owner_views":478,
        "Answer_body":"<p>I didn't realized in the first place that <code>ConcatPlaceholder<\/code> accept both Artifact and string. This is exactly what I wanted to achieve:<\/p>\n<pre><code>ConcatPlaceholder([OutputPathPlaceholder('drt_model'), '\/model'])\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-11-18 19:26:06.353000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "google-cloud-vertex-ai"
        ]
    },
    {
        "Question_id":53609409,
        "Question_title":"Automatically \"stop\" Sagemaker notebook instance after inactivity?",
        "Question_body":"<p>I have a Sagemaker Jupyter notebook instance that I keep leaving online overnight by mistake, unnecessarily costing money... <\/p>\n\n<p>Is there any way to automatically stop the Sagemaker notebook instance when there is no activity for say, 1 hour? Or would I have to make a custom script?<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_date":"2018-12-04 09:18:11.383000 UTC",
        "Question_favorite_count":2.0,
        "Question_last_edit_date":null,
        "Question_score":19,
        "Question_tags":"amazon-web-services|aws-lambda|amazon-cloudwatch|amazon-sagemaker",
        "Question_view_count":12683,
        "Owner_creation_date":"2011-03-10 10:26:00.990000 UTC",
        "Owner_last_access_date":"2022-09-24 23:25:14.187000 UTC",
        "Owner_location":null,
        "Owner_reputation":2563,
        "Owner_up_votes":121,
        "Owner_down_votes":0,
        "Owner_views":167,
        "Answer_body":"<p>You can use <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/customize-your-amazon-sagemaker-notebook-instances-with-lifecycle-configurations-and-the-option-to-disable-internet-access\/\" rel=\"noreferrer\">Lifecycle configurations<\/a> to set up an automatic job that will stop your instance after inactivity.<\/p>\n\n<p>There's <a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\" rel=\"noreferrer\">a GitHub repository<\/a> which has samples that you can use. In the repository, there's a <a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/blob\/master\/scripts\/auto-stop-idle\/on-start.sh\" rel=\"noreferrer\">auto-stop-idle<\/a> script which will shutdown your instance once it's idle for more than 1 hour.<\/p>\n\n<p>What you need to do is<\/p>\n\n<ol>\n<li>to create a Lifecycle configuration using the script and<\/li>\n<li>associate the configuration with the instance. You can do this when you edit or create a Notebook instance.<\/li>\n<\/ol>\n\n<p>If you think 1 hour is too long you can tweak the script. <a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/blob\/master\/scripts\/auto-stop-idle\/on-start.sh#L17\" rel=\"noreferrer\">This line<\/a> has the value.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2019-10-15 09:46:13.200000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":26.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":66196815,
        "Question_title":"Using Logistic Regression For Timeseries Data in Amazon SageMaker",
        "Question_body":"<p>For a project I am working on, which uses annual financial reports data (of multiple categories) from companies which have been successful or gone bust\/into liquidation, I previously created a (fairly well performing) model on AWS Sagemaker using a multiple linear regression algorithm (specifically, the AWS stock algorithm for logistic regression\/classification problems - the 'Linear Learner' algorithm)<\/p>\n<p>This model just produces a simple &quot;company is in good health&quot; or &quot;company looks like it will go bust&quot; binary prediction, based on one set of annual data fed in; e.g.<\/p>\n<pre><code>query input: {data:[{\n&quot;Gross Revenue&quot;: -4000,\n&quot;Balance Sheet&quot;: 10000,\n&quot;Creditors&quot;: 4000,\n&quot;Debts&quot;: 1000000 \n}]}\n\ninference output: &quot;in good health&quot; \/ &quot;in bad health&quot;\n<\/code><\/pre>\n<p>I trained this model by just ignoring what year for each company the values were from and pilling in all of the annual financial reports data (i.e. one years financial data for one company = one input line) for the training, along with the label of &quot;good&quot; or &quot;bad&quot; - a good company was one which has existed for a while, but hasn't gone bust, a bad company is one which was found to have eventually gone bust; e.g.:<\/p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>label<\/th>\n<th>Gross Revenue<\/th>\n<th>Balance Sheet<\/th>\n<th>Creditors<\/th>\n<th>Debts<\/th>\n<\/tr>\n<\/thead>\n<tbody>\n<tr>\n<td>good<\/td>\n<td>10000<\/td>\n<td>20000<\/td>\n<td>0<\/td>\n<td>0<\/td>\n<\/tr>\n<tr>\n<td>bad<\/td>\n<td>0<\/td>\n<td>5<\/td>\n<td>100<\/td>\n<td>10000<\/td>\n<\/tr>\n<tr>\n<td>bad<\/td>\n<td>20000<\/td>\n<td>0<\/td>\n<td>4<\/td>\n<td>100000000<\/td>\n<\/tr>\n<\/tbody>\n<\/table>\n<\/div>\n<p>I hence used these multiple features (gross revenue, balance sheet...) along with the label (good\/bad) in my training input, to create my first model.<\/p>\n<p>I would like to use the same features as before as input (gross revenue, balance sheet..) but over multiple years; e.g take the values from 2020 &amp; 2019 and use these (along with the eventual company status of &quot;good&quot; or &quot;bad&quot;) as the singular input for my new model. However I'm unsure of the following:<\/p>\n<ul>\n<li>is this an inappropriate use of logistic regression Machine learning? i.e. is there a more suitable algorithm I should consider?<\/li>\n<li>is it fine, or terribly wrong to try and just use the same technique as before, but combine the data for both years into one input line like:<\/li>\n<\/ul>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>label<\/th>\n<th>Gross Revenue(2019)<\/th>\n<th>Balance Sheet(2019)<\/th>\n<th>Creditors(2019)<\/th>\n<th>Debts(2019)<\/th>\n<th>Gross Revenue(2020)<\/th>\n<th>Balance Sheet(2020)<\/th>\n<th>Creditors(2020)<\/th>\n<th>Debts(2020)<\/th>\n<\/tr>\n<\/thead>\n<tbody>\n<tr>\n<td>good<\/td>\n<td>10000<\/td>\n<td>20000<\/td>\n<td>0<\/td>\n<td>0<\/td>\n<td>30000<\/td>\n<td>10000<\/td>\n<td>40<\/td>\n<td>500<\/td>\n<\/tr>\n<tr>\n<td>bad<\/td>\n<td>100<\/td>\n<td>50<\/td>\n<td>200<\/td>\n<td>50000<\/td>\n<td>100<\/td>\n<td>5<\/td>\n<td>100<\/td>\n<td>10000<\/td>\n<\/tr>\n<tr>\n<td>bad<\/td>\n<td>5000<\/td>\n<td>0<\/td>\n<td>2000<\/td>\n<td>800000<\/td>\n<td>2000<\/td>\n<td>0<\/td>\n<td>4<\/td>\n<td>100000000<\/td>\n<\/tr>\n<\/tbody>\n<\/table>\n<\/div>\n<p>I would personally expect that a company which has gotten worse over time (i.e. companies finances are worse in 2020 than in 2019) should be more likely to be found to be a &quot;bad&quot;\/likely to go bust, so I would hope that, if I feed in data like in the above example (i.e. earlier years data comes before later years data, on an input line) my training job ends up creating a model which gives greater weighting to the earlier years data, when making predictions<\/p>\n<p>Any advice or tips would be greatly appreciated - I'm pretty new to machine learning and would like to learn more<\/p>\n<p>UPDATE:<\/p>\n<p>Using Long-Short-Term-Memory Recurrent Neural Networks (LSTM RNN) is one potential route I think I could try taking, but this seems to commonly just be used with multivariate data over many dates; my data only has 2 or 3 dates worth of multivariate data, per company. I would want to try using the data I have for all the companies, over the few dates worth of data there are, in training<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2021-02-14 15:15:31.557000 UTC",
        "Question_favorite_count":0.0,
        "Question_last_edit_date":"2021-02-24 12:09:13.343000 UTC",
        "Question_score":1,
        "Question_tags":"amazon-web-services|machine-learning|logistic-regression|recurrent-neural-network|amazon-sagemaker",
        "Question_view_count":183,
        "Owner_creation_date":"2019-08-26 17:17:56.033000 UTC",
        "Owner_last_access_date":"2022-01-09 18:01:35.493000 UTC",
        "Owner_location":null,
        "Owner_reputation":65,
        "Owner_up_votes":23,
        "Owner_down_votes":0,
        "Owner_views":15,
        "Answer_body":"<p>I once developed a so called Genetic Time Series in R. I used a Genetic Algorithm which sorted out the best solutions from multivariate data, which were fitted on a VAR in differences or a VECM. Your data seems more macro economic or financial than user-centric and VAR or VECM seems appropriate. (Surely it is possible to treat time-series data in the same way so that we can use LSTM or other approaches, but these are very common) However, I do not know if VAR in differences or VECM works with binary classified labels. Perhaps if you would calculate a metric outcome, which you later label encode to a categorical feature (or label it first to a categorical) than VAR or VECM may also be appropriate.<\/p>\n<p>However you may add all yearly data points to one data points per firm to forecast its survival, but you would loose a lot of insight. If you are interested in time series ML which works a little bit different than for neural networks or elastic net (which could also be used with time series) let me know. And we can work something out. Or I'll paste you some sources.<\/p>\n<p>Summary:\n1.)\nIt is possible to use LSTM, elastic NEt (time points may be dummies or treated as cross sectional panel) or you use VAR in differences and VECM with a slightly different out come variable<\/p>\n<p>2.)\nIt is possible but you will loose information over time.<\/p>\n<p>All the best,\nPatrick<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2021-02-27 18:21:16.540000 UTC",
        "Answer_last_edit_date":"2021-02-27 18:46:22.833000 UTC",
        "Answer_score":2.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":72473641,
        "Question_title":"How do launch experiments in DVC?",
        "Question_body":"<p>I want to launch some experiments in DVC. But when I set values of experiment parameters, DVC deletes file 'params.yaml', and experiment doesn't set in queue.<\/p>\n<p>Simplified code for example:\nPython file 'test.py':<\/p>\n<pre><code>import numpy as np\nimport json\nimport yaml\n\nparams = yaml.safe_load(open('params.yaml'))[&quot;test&quot;]\n\nprecision = np.random.random()\nrecall = params['value']\naccuracy = np.random.random()\n \n\nrows = {'precision': precision,\n        'recall': recall,\n        'accuracy': accuracy}\n\n\nwith open(params['metrics_path'], 'w') as outfile:\n    json.dump(rows, outfile)\n\nfpr = 10*np.random.random((1,10)).tolist()\ntpr = 10*np.random.random((1,10)).tolist()\n\nwith open('plot.json', 'w') as outfile2:\n    json.dump(\n      {\n        &quot;roc&quot;: [ {&quot;fpr&quot;: f, &quot;tpr&quot;: t} for f, t in zip(fpr, tpr) ]\n      }, \n      outfile2\n      )\n<\/code><\/pre>\n<p>params.yaml:<\/p>\n<pre><code>test:\n  metrics_path: &quot;scores.json&quot;\n  value: 1\n<\/code><\/pre>\n<p>dvc.yaml:<\/p>\n<pre><code>stages:\n  test:\n    cmd: python test.py\n    deps:\n    - test.py\n    params:\n    - test.metrics_path\n    - test.value\n    metrics:\n    - scores.json:\n        cache: false\n    plots:\n    - plot.json:\n        cache: false\n        x: fpr\n        y: tpr\n<\/code><\/pre>\n<p>It is strange behavior. Is it possible to fix it?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":8,
        "Question_creation_date":"2022-06-02 09:10:42.840000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"python|dvc",
        "Question_view_count":167,
        "Owner_creation_date":"2021-06-12 15:13:14.510000 UTC",
        "Owner_last_access_date":"2022-09-23 11:35:31.300000 UTC",
        "Owner_location":null,
        "Owner_reputation":95,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Answer_body":"<p>I solved my problem. It is necessary, that all files (executable scripts, 'dvc.yaml', 'params.yaml') be tracked by git. In this case <code>dvc exp run<\/code> command works correctly.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-06-03 08:28:54.587000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_valid_tags":[
            "dvc"
        ]
    },
    {
        "Question_id":53143396,
        "Question_title":"What is the difference between Azure Databricks and Azure Batch AI?",
        "Question_body":"<p>I have went throught the documentation of both Microsoft AI services <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/batch-ai\/overview\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/batch-ai\/overview<\/a> and <a href=\"https:\/\/azure.microsoft.com\/en-us\/services\/databricks\/\" rel=\"nofollow noreferrer\">https:\/\/azure.microsoft.com\/en-us\/services\/databricks\/<\/a> but I still can't understand the difference or when each should be used(purpose).<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2018-11-04 17:20:54.317000 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":"2018-11-05 20:36:05.280000 UTC",
        "Question_score":2,
        "Question_tags":"azure|artificial-intelligence|azure-machine-learning-studio|databricks|azure-cognitive-services",
        "Question_view_count":1267,
        "Owner_creation_date":"2018-08-12 12:36:04.407000 UTC",
        "Owner_last_access_date":"2020-04-19 10:48:37.467000 UTC",
        "Owner_location":null,
        "Owner_reputation":49,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":11,
        "Answer_body":"<p>Microsoft AI is a first party solution Microsoft built, Databricks is based off of Apache Spark that we will manage for you in Azure.  One of the things we are trying to do in Azure is meet the customer where they are most comfortable.  In a way it's similar to how we have Service Fabric (Microsoft Service) and Azure Kubernetes Service (AKS).  Both allow you to run microservices but one is a service we developed and the other is a managed Open Source Project we support.  <\/p>\n\n<p>When to use each is more of a question of preference and skillset. <\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2018-11-06 10:41:58.237000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":3.0,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":40577607,
        "Question_title":"AzureML: How to Keep leading zeros in dataset (.CSV)",
        "Question_body":"<p>My data: <code>0671001795<\/code><\/p>\n\n<p>Dataset in Microsoft AzureML: <a href=\"https:\/\/i.stack.imgur.com\/gfCtu.png\" rel=\"nofollow noreferrer\">https:\/\/i.stack.imgur.com\/gfCtu.png<\/a><\/p>\n\n<p>How to Keep leading zeros? <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2016-11-13 18:58:10.300000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"azure-machine-learning-studio",
        "Question_view_count":162,
        "Owner_creation_date":"2015-03-04 10:32:55.007000 UTC",
        "Owner_last_access_date":"2017-04-22 15:39:27.547000 UTC",
        "Owner_location":null,
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Answer_body":"<p>Keeping leading zeros when the data column is in integer format is not possible. Here's an alternative way of keeping the leading zeros. (The String data type is used instead of int)<\/p>\n\n<ol>\n<li>Add an special character('\/' etc) before the number <\/li>\n<li>Pipe the contentthrough 'Preprocess Text' module.<\/li>\n<li>Make sure only to tick \"Remove Special Characters\" option.<\/li>\n<li>Output would be in a string with your desired format.<\/li>\n<\/ol>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/H0IBg.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/H0IBg.png\" alt=\"Remove Special Characters option\"><\/a><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/ML1YM.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ML1YM.png\" alt=\"Output as a string\"><\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2016-11-15 10:25:02.147000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":51117133,
        "Question_title":"AWS Sagemaker - Install External Library and Make it Persist",
        "Question_body":"<p>I have a sagemaker instance up and running and I have a few libraries that I frequently use with it but each time I restart the instance they get wiped and I have to reinstall them. Is it possible to install my libraries to one of the anaconda environments and have the change remain?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2018-06-30 17:34:14.783000 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":13,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":10578,
        "Owner_creation_date":"2017-03-23 00:32:36.263000 UTC",
        "Owner_last_access_date":"2021-03-22 21:38:40.833000 UTC",
        "Owner_location":null,
        "Owner_reputation":401,
        "Owner_up_votes":17,
        "Owner_down_votes":0,
        "Owner_views":13,
        "Answer_body":"<p>The supported way to do this for Sagemaker notebook instances is with <strong>Lifecycle Configurations<\/strong>.<\/p>\n\n<p>You can create an <strong>onStart<\/strong> lifecycle hook that can install the required packages into the respective Conda environments each time your notebook instance starts.<\/p>\n\n<p>Please see the following blog post for more details<\/p>\n\n<p><a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/customize-your-amazon-sagemaker-notebook-instances-with-lifecycle-configurations-and-the-option-to-disable-internet-access\/\" rel=\"noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/customize-your-amazon-sagemaker-notebook-instances-with-lifecycle-configurations-and-the-option-to-disable-internet-access\/<\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2018-07-02 18:19:26.567000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":12.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":61997914,
        "Question_title":"How to edit\/patch kubernetes deployment to add label using python",
        "Question_body":"<p>I am fairly new to kubernetes - I have developed web UI\/API that automates model deployment using Azure Machine Learning Services to Azure Kubernetes Services (AKS). As a hardening measure, I am tying to set up managed identity for deployed pods in AKS using <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-use-azure-ad-identity\" rel=\"nofollow noreferrer\">this documentation<\/a>. One of the step is to edit the deployment to add identity-feature label at <code>\/spec\/template\/metadata\/labels<\/code> for the deployment (see para starting like <code>Edit the deployment to add ...<\/code> in <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-use-azure-ad-identity#assign-azure-identity-to-machine-learning-web-service\" rel=\"nofollow noreferrer\">this section<\/a>). <\/p>\n\n<p>I wish to automate this step using python kubernetes client (<a href=\"https:\/\/github.com\/kubernetes-client\/python\" rel=\"nofollow noreferrer\">https:\/\/github.com\/kubernetes-client\/python<\/a>). Browsing the available API, I was wondering that perhaps <code>patch_namespaced_deployment<\/code> will allow me to edit deployment and add label at <code>\/spec\/template\/metadata\/labels<\/code>. I was looking for some example code using the python client for the same - any help to achieve above will be appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":5,
        "Question_creation_date":"2020-05-25 07:44:23.393000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":3,
        "Question_tags":"kubernetes|azure-aks|azure-machine-learning-service|kubernetes-python-client|aad-pod-identity",
        "Question_view_count":6338,
        "Owner_creation_date":"2010-08-11 09:27:43.807000 UTC",
        "Owner_last_access_date":"2022-09-24 03:00:34.580000 UTC",
        "Owner_location":"Mumbai, India",
        "Owner_reputation":45542,
        "Owner_up_votes":2319,
        "Owner_down_votes":29,
        "Owner_views":3034,
        "Answer_body":"<p>Have a look at this example:<\/p>\n<p><a href=\"https:\/\/github.com\/kubernetes-client\/python\/blob\/master\/examples\/deployment_crud.py#L62-L70\" rel=\"nofollow noreferrer\">https:\/\/github.com\/kubernetes-client\/python\/blob\/master\/examples\/deployment_crud.py#L62-L70<\/a><\/p>\n<pre class=\"lang-py prettyprint-override\"><code>def update_deployment(api_instance, deployment):\n    # Update container image\n    deployment.spec.template.spec.containers[0].image = &quot;nginx:1.16.0&quot;\n    # Update the deployment\n    api_response = api_instance.patch_namespaced_deployment(\n        name=DEPLOYMENT_NAME,\n        namespace=&quot;default&quot;,\n        body=deployment)\n    print(&quot;Deployment updated. status='%s'&quot; % str(api_response.status))\n<\/code><\/pre>\n<p>The Labels are on the deployment object, from the App v1 API,<\/p>\n<pre><code>kind: Deployment\nmetadata:\n  name: deployment-example\nspec:\n  replicas: 3\n  revisionHistoryLimit: 10\n  template:\n    metadata:\n      labels:\n        app: nginx\n<\/code><\/pre>\n<p>which means you need to update the following:<\/p>\n<p><code>deployment.spec.template.metadata.labels.app = &quot;nginx&quot;<\/code><\/p>",
        "Answer_comment_count":7.0,
        "Answer_creation_date":"2020-05-26 11:41:48.890000 UTC",
        "Answer_last_edit_date":"2020-12-11 17:43:35.473000 UTC",
        "Answer_score":5.0,
        "Question_valid_tags":[
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":57692681,
        "Question_title":"Sagemaker to use processed pickled ndarray instead of csv files from S3",
        "Question_body":"<p>I understand that you can pass a CSV file from S3 into a Sagemaker XGBoost container using the following code<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>train_channel = sagemaker.session.s3_input(train_data, content_type='text\/csv')\nvalid_channel = sagemaker.session.s3_input(validation_data, content_type='text\/csv')\n\ndata_channels = {'train': train_channel, 'validation': valid_channel}\nxgb_model.fit(inputs=data_channels,  logs=True)\n<\/code><\/pre>\n\n<p>But I have an ndArray stored in S3 bucket. These are processed, label encoded, feature engineered arrays. I would want to pass this into the container instead of the csv. I do understand I can always convert my ndarray into csv files before saving it in S3. Just checking if there is an array option.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-08-28 12:41:13.187000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|amazon-s3|amazon-sagemaker",
        "Question_view_count":92,
        "Owner_creation_date":"2013-03-16 12:40:41.347000 UTC",
        "Owner_last_access_date":"2020-11-27 07:21:03.280000 UTC",
        "Owner_location":"Chennai, India",
        "Owner_reputation":468,
        "Owner_up_votes":24,
        "Owner_down_votes":4,
        "Owner_views":97,
        "Answer_body":"<p>There are multiple options for algorithms in SageMaker:<\/p>\n\n<ol>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algos.html\" rel=\"nofollow noreferrer\">Built-in algorithms<\/a>, like the SageMaker XGBoost you mention<\/li>\n<li>Custom, user-created algorithm code, which can be:\n\n<ul>\n<li>Written for a pre-built docker image, available for Sklearn, TensorFlow, Pytorch, MXNet<\/li>\n<li>Written in your own container<\/li>\n<\/ul><\/li>\n<\/ol>\n\n<p>When you use built-ins (option 1), your choice of data format options is limited to what the built-ins support, <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost.html#InputOutput-XGBoost\" rel=\"nofollow noreferrer\">which is only csv and libsvm in the case of the built-in XGBoost<\/a>. If you want to use custom data formats and pre-processing logic before XGBoost, it is absolutely possible if you use your own script leveraging the open-source XGBoost. You can get inspiration from the <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_randomforest\/Sklearn_on_SageMaker_end2end.ipynb\" rel=\"nofollow noreferrer\">Random Forest demo<\/a> to see how to create custom models in pre-built containers<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2019-09-13 23:37:02.250000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":54432761,
        "Question_title":"Amazon SageMaker hyperparameter tuning error for built-in algorithm using the Python SDK",
        "Question_body":"<p>When using the Python SDK to start a SageMaker hyperparameter tuning job using one of the built-in algorithms (in this case, the Image Classifier) with the following code:<\/p>\n\n<pre><code># [...] Some lines elided for brevity\n\nfrom sagemaker.tuner import HyperparameterTuner, IntegerParameter, CategoricalParameter, ContinuousParameter\nhyperparameter_ranges = {'optimizer': CategoricalParameter(['sgd', 'adam']),\n                         'learning_rate': ContinuousParameter(0.0001, 0.2),\n                         'mini_batch_size': IntegerParameter(2, 30),}\n\nobjective_metric_name = 'validation:accuracy'\n\ntuner = HyperparameterTuner(image_classifier,\n                            objective_metric_name,\n                            hyperparameter_ranges,\n\n                            max_jobs=50,\n                            max_parallel_jobs=3)\n\ntuner.fit(inputs=data_channels, logs=True)\n<\/code><\/pre>\n\n<p>The job fails and I get this error when checking on the job status in the SageMaker web console:<\/p>\n\n<pre><code>ClientError: Additional hyperparameters are not allowed (u'sagemaker_estimator_module', u'sagemaker_estimator_class_name' were unexpected) (caused by ValidationError) \n\nCaused by: Additional properties are not allowed (u'sagemaker_estimator_module', u'sagemaker_estimator_class_name' were unexpected) \n\nFailed validating u'additionalProperties' in schema: {u'$schema': u'http:\/\/json-schema.org\/schema#', u'additionalProperties': False, u'definitions': {u'boolean_0_1': {u'oneOf': [{u'enum': [u'0', u'1'], u'type': u'string'}, {u'enum': [0, 1], u'type': u'number'}]}, u'boolean_true_false_0_1': {u'oneOf': [{u'enum': [u'true', u'false',\n<\/code><\/pre>\n\n<p>I'm not explicitly passing the <code>sagemaker_estimator_module<\/code> or <code>sagemaker_estimator_class_name<\/code> properties anywhere, so I'm not sure why it's returning this error. <\/p>\n\n<p>What's the right way to start this tuning job?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-01-30 02:58:11.420000 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":3,
        "Question_tags":"python|amazon-web-services|amazon-sagemaker|hyperparameters",
        "Question_view_count":595,
        "Owner_creation_date":"2008-10-23 03:43:42.317000 UTC",
        "Owner_last_access_date":"2022-08-22 05:29:27.157000 UTC",
        "Owner_location":"Singapore",
        "Owner_reputation":7707,
        "Owner_up_votes":162,
        "Owner_down_votes":6,
        "Owner_views":583,
        "Answer_body":"<p>I found the answer via <a href=\"https:\/\/translate.google.com\/translate?hl=en&amp;sl=ja&amp;u=https:\/\/dev.classmethod.jp\/machine-learning\/sagemaker-tuning-stack\/&amp;prev=search\" rel=\"nofollow noreferrer\">this post translated from Japanese<\/a>.<\/p>\n\n<p>When starting hyperparameter tuning jobs using the built-in algorithms in the Python SDK, <strong>you need to explicitly pass <code>include_cls_metadata=False<\/code><\/strong> as a keyword argument to <code>tuner.fit()<\/code> like this:<\/p>\n\n<p><code>tuner.fit(inputs=data_channels, logs=True, include_cls_metadata=False)<\/code><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2019-01-30 02:58:11.420000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":60716202,
        "Question_title":"SageMaker experiments store",
        "Question_body":"<p>I just started using aws sagemaker for running and maintaining models, experiments. just wanted to know is there any persistent layer for the sagemaker from where i can get data of my experiments\/models instead of looking into the sagemaker studio. Does sagemaker saves the experiments or its data like s3 location in any table  something like modelsdb? <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-03-17 04:18:32.033000 UTC",
        "Question_favorite_count":0.0,
        "Question_last_edit_date":"2020-03-17 04:34:04.403000 UTC",
        "Question_score":1,
        "Question_tags":"python|amazon-web-services|machine-learning|amazon-sagemaker",
        "Question_view_count":229,
        "Owner_creation_date":"2015-03-23 17:08:17.093000 UTC",
        "Owner_last_access_date":"2022-04-22 14:16:10.363000 UTC",
        "Owner_location":null,
        "Owner_reputation":187,
        "Owner_up_votes":59,
        "Owner_down_votes":0,
        "Owner_views":151,
        "Answer_body":"<p>SageMaker Studio is using the SageMaker API to pull all of the data its displaying.  Essentially there's no secret API here getting invoked.<\/p>\n\n<p>Quite a bit of what's being displayed with respect to experiments is from the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_Search.html\" rel=\"nofollow noreferrer\">search results<\/a>, the rest coming from either List* or Describe* calls.  Studio is taking the results from the search request and displaying it in the table format that you're seeing.  Search results when searching over resource ExperimentTrialComponent that have a source (such as a training job) will be enhanced with the original sources data ([result]::SourceDetail::TrainingJob) if supported (work is ongoing to add additional source detail resource types).<\/p>\n\n<p>All of the metadata that is related to resources in SageMaker is available via the APIs; there is no other location (in the cloud) like s3 for that data.<\/p>\n\n<p>As of this time there is no effort to determine if it's possible to add support into <a href=\"https:\/\/github.com\/VertaAI\/modeldb\" rel=\"nofollow noreferrer\">modeldb<\/a> for SageMaker that I'm aware of.  Given that modeldb appears to make some assumptions about it's talking to a relational database it would appear unlikely to be something that would be doable. (I only read the overview very quickly so this might be inaccurate.)<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-03-18 15:42:56.820000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":62844211,
        "Question_title":"Updating Sagemaker Endpoint with new Endpoint Configuration",
        "Question_body":"<p>A bit confused with automatisation of Sagemaker retraining the model.<\/p>\n<p>Currently I have a notebook instance with Sagemaker <code>LinearLerner<\/code> model making the classification task. So using <code>Estimator<\/code> I'm making training, then deploying the model creating <code>Endpoint<\/code>. Afterwards using <code>Lambda<\/code> function for invoke this endpoint, I add it to the <code>API Gateway<\/code> receiving the api endpoint which can be used for POST requests and sending back response with class.<\/p>\n<p>Now I'm facing with the problem of retraining. For that I use <code>serverless<\/code> approach and <code>lambda<\/code> function getting environment variables for training_jobs. But the problem that Sagemaker not allow to rewrite training job and you can only create new one. My goal is to automatise the part when the new training job and the new endpoint config will apply to the existing endpoint that I don't need to change anything in API gateway. Is that somehow possible to automatically attach new endpoint config with existing endpoint?<\/p>\n<p>Thanks<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2020-07-11 01:19:50.427000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-07-11 21:22:56.883000 UTC",
        "Question_score":5,
        "Question_tags":"python|endpoint|amazon-sagemaker",
        "Question_view_count":3443,
        "Owner_creation_date":"2013-12-08 08:33:34.717000 UTC",
        "Owner_last_access_date":"2022-09-21 18:08:28.293000 UTC",
        "Owner_location":null,
        "Owner_reputation":2778,
        "Owner_up_votes":138,
        "Owner_down_votes":1,
        "Owner_views":352,
        "Answer_body":"<p>If I am understanding the question correctly, you should be able to use <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateEndpointConfig.html\" rel=\"nofollow noreferrer\">CreateEndpointConfig<\/a> near the end of the training job, then use <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_UpdateEndpoint.html\" rel=\"nofollow noreferrer\">UpdateEndpoint<\/a>:<\/p>\n<p><code>Deploys the new EndpointConfig specified in the request, switches to using newly created endpoint, and then deletes resources provisioned for the endpoint using the previous EndpointConfig (there is no availability loss).<\/code><\/p>\n<p>If the API Gateway \/ Lambda is routed via the endpoint ARN, that should not change after using <code>UpdateEndpoint<\/code>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-07-11 16:38:36.257000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":3.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":52664415,
        "Question_title":"Why we need ML batch execution and update resource option in azure data factory",
        "Question_body":"<p>Why we need to ML Batch Execution and ML Update resource option in Data factory ? How this can be used to retrain machine learning when updating a blob file ?<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/8KWH0.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/8KWH0.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2018-10-05 11:16:38.597000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2018-10-06 17:24:47.100000 UTC",
        "Question_score":1,
        "Question_tags":"c#|azure|azure-blob-storage|azure-data-factory|azure-machine-learning-studio",
        "Question_view_count":141,
        "Owner_creation_date":"2018-10-05 11:11:35.717000 UTC",
        "Owner_last_access_date":"2018-11-29 09:07:46.350000 UTC",
        "Owner_location":null,
        "Owner_reputation":78,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":17,
        "Answer_body":"<p>Ml Batch Execution- to call retraining experiment and get a .ilearner file as output.\nML Update Resource- Use the above .ilearner as input and call patch endpoint of predictive web service to Update resource.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2018-10-17 11:10:04.663000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":62002183,
        "Question_title":"Use tensorboard with object detection API in sagemaker",
        "Question_body":"<p>with <a href=\"https:\/\/github.com\/svpino\/tensorflow-object-detection-sagemaker\" rel=\"nofollow noreferrer\">this<\/a> I successfully created a training job on sagemaker using the Tensorflow Object Detection API in a docker container. Now I'd like to monitor the training job using sagemaker, but cannot find anything explaining how to do it. I don't use a sagemaker notebook.\nI think I can do it by saving the logs into a S3 bucket and point there a local tensorboard instance .. but don't know how to tell the tensorflow object detection API where to save the logs (is there any command line argument for this ?).\nSomething like <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/keras_script_mode_pipe_mode_horovod\/tensorflow_keras_CIFAR10.ipynb\" rel=\"nofollow noreferrer\">this<\/a>, but the script <code>generate_tensorboard_command.py<\/code> fails because my training job don't have the <code>sagemaker_submit_directory<\/code> parameter..<\/p>\n<p>The fact is when I start the training job nothing is created on my s3 until the job finish and upload everything. There should be a way tell tensorflow where to save the logs (s3) during the training, hopefully without modifying the API source code..<\/p>\n<p><strong>Edit<\/strong><\/p>\n<p>I can finally make it works with the accepted solution (tensorflow natively supports read\/write to s3), there are however additional steps to do:<\/p>\n<ol>\n<li>Disable network isolation in the training job configuration<\/li>\n<li>Provide credentials to the docker image to write to S3 bucket<\/li>\n<\/ol>\n<p>The only thing is that Tensorflow continuously polls filesystem (i.e. looking for an updated model in serving mode) and this cause useless requests to S3, that you will have to pay (together with a buch of errors in the console). I opened a new question <a href=\"https:\/\/stackoverflow.com\/q\/64969198\/4267439\">here<\/a> for this. At least it works.<\/p>\n<p><strong>Edit 2<\/strong><\/p>\n<p>I was wrong, TF just write logs, is not polling so it's an expected behavior and the extra costs are minimal.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-05-25 12:12:39.343000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-11-25 16:53:18.013000 UTC",
        "Question_score":2,
        "Question_tags":"object-detection|tensorboard|amazon-sagemaker|object-detection-api|tensorflow-model-garden",
        "Question_view_count":311,
        "Owner_creation_date":"2014-11-18 21:32:30.293000 UTC",
        "Owner_last_access_date":"2022-09-24 17:06:59.437000 UTC",
        "Owner_location":"Jesi, Italy",
        "Owner_reputation":2302,
        "Owner_up_votes":51,
        "Owner_down_votes":4,
        "Owner_views":227,
        "Answer_body":"<p>Looking through the example you posted, it looks as though the <code>model_dir<\/code> passed to the TensorFlow Object Detection package is configured to <code>\/opt\/ml\/model<\/code>:<\/p>\n<pre><code># These are the paths to where SageMaker mounts interesting things in your container.\nprefix = '\/opt\/ml\/'\ninput_path = os.path.join(prefix, 'input\/data')\noutput_path = os.path.join(prefix, 'output')\nmodel_path = os.path.join(prefix, 'model')\nparam_path = os.path.join(prefix, 'input\/config\/hyperparameters.json')\n<\/code><\/pre>\n<p>During the training process, tensorboard logs will be written to <code>\/opt\/ml\/model<\/code>, and then uploaded to s3 as a final model artifact AFTER training: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-envvariables.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-envvariables.html<\/a>.<\/p>\n<p>You <em>might<\/em> be able to side-step the SageMaker artifact upload step and point the <code>model_dir<\/code> of TensorFlow Object Detection API directly at an s3 location during training:<\/p>\n<pre><code>model_path = &quot;s3:\/\/your-bucket\/path\/here\n<\/code><\/pre>\n<p>This means that the TensorFlow library within the SageMaker job is directly writing to S3 instead of the filesystem inside of it's container. Assuming the underlying TensorFlow Object Detection code can write directly to S3 (something you'll have to verify), you should be able to see the tensorboard logs and checkpoints there in realtime.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-07-07 16:06:22.057000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":38505336,
        "Question_title":"The \"perfect error\": untraceable, unnamed, from neverland",
        "Question_body":"<p>I have an experiment running without problems when I run single modules as selected parts.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/aVsSC.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/aVsSC.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>The situation is pretty different when I run the entire experiment. In that case it fails, but I cannot know why.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/N2h11.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/N2h11.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>The experiment returns an error and obviously it doesn't let me deploy the web service, while:<\/p>\n\n<p>1) I cannot know on <strong>which module<\/strong> my error is.<\/p>\n\n<p>2) I don't have an overall description of the error.<\/p>\n\n<p>3) It could be related to the error here but I cannot know because I don't have any feedback about that. I know that it could be a <strong>bug<\/strong> Azure is trying to solve but this is not reported anywhere.<\/p>\n\n<p>I really need to know if that's a bug and if I can do something about that.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":4,
        "Question_creation_date":"2016-07-21 13:16:15.653000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"debugging|azure|feedback|azure-machine-learning-studio",
        "Question_view_count":75,
        "Owner_creation_date":"2015-07-09 09:05:28.610000 UTC",
        "Owner_last_access_date":"2022-09-19 17:14:25.487000 UTC",
        "Owner_location":"Colleferro, Italy",
        "Owner_reputation":809,
        "Owner_up_votes":109,
        "Owner_down_votes":0,
        "Owner_views":361,
        "Answer_body":"<p>This issue has been resolved. Please let me know if this happens again<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2016-07-28 06:57:13.830000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":58117200,
        "Question_title":"How to get status of Azure machine learning service pipeline run using Rest Api?",
        "Question_body":"<p>I have created an Azure Machine Learning Service Pipeline which i am invoking externally using its rest endpoint.\nBut i also need to monitor its run , whether it got completed or failed, periodically.\n<strong>Is there a methodinside a machine learning pipeline's rest endpoint, which i can hit to check its run status?<\/strong>\nI have tried the steps mentioned in the link here \n<a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/machine-learning-pipelines\/pipeline-batch-scoring\/pipeline-batch-scoring.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/machine-learning-pipelines\/pipeline-batch-scoring\/pipeline-batch-scoring.ipynb<\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2019-09-26 12:49:43.603000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2019-09-26 12:51:23.203000 UTC",
        "Question_score":1,
        "Question_tags":"azure|rest|azure-machine-learning-service",
        "Question_view_count":898,
        "Owner_creation_date":"2017-10-22 09:05:10.973000 UTC",
        "Owner_last_access_date":"2020-11-06 16:28:37.523000 UTC",
        "Owner_location":null,
        "Owner_reputation":55,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":16,
        "Answer_body":"<p>For getting status of run, you can use REST APIs described here <a href=\"https:\/\/github.com\/Azure\/azure-rest-api-specs\/tree\/master\/specification\/machinelearningservices\/data-plane\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/azure-rest-api-specs\/tree\/master\/specification\/machinelearningservices\/data-plane<\/a> <\/p>\n\n<p>Specifically you need <a href=\"https:\/\/github.com\/Azure\/azure-rest-api-specs\/blob\/master\/specification\/machinelearningservices\/data-plane\/Microsoft.MachineLearningServices\/preview\/2019-08-01\/runHistory.json\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/azure-rest-api-specs\/blob\/master\/specification\/machinelearningservices\/data-plane\/Microsoft.MachineLearningServices\/preview\/2019-08-01\/runHistory.json<\/a><\/p>\n\n<p>use this call to get run information including status:<\/p>\n\n<blockquote>\n  <p>\/history\/v1.0\/subscriptions\/{subscriptionId}\/resourceGroups\/{resourceGroupName}\/providers\/Microsoft.MachineLearningServices\/workspaces\/{workspaceName}\/experiments\/{experimentName}\/runs\/{runId}\/details<\/p>\n<\/blockquote>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2019-09-26 16:56:32.870000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":63127521,
        "Question_title":"Deploying Model to Kubernetes",
        "Question_body":"<p>I am trying to deploy a model to Kubernetes in Azure Machine Learning Studio, it was working for a while, but now, it fails during deployment, the error message is as follows:<\/p>\n<pre><code>Deploy: Failed on step WaitServiceCreating. Details: AzureML service API error. \nYour container application crashed. This may be caused by errors in your scoring file's init() function.\nPlease check the logs for your container instance: pipeline-created-on-07-28-2020-r.\nFrom the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs.\nYou can also try to run image viennaglobal.azurecr.io\/azureml\/azureml_6ae744633f749472feb283065055dc2c:latest locally.\nPlease refer to http:\/\/aka.ms\/debugimage#service-launch-fails for more information.\n<\/code><\/pre>\n<pre class=\"lang-js prettyprint-override\"><code>{\n    &quot;code&quot;: &quot;KubernetesDeploymentFailed&quot;,\n    &quot;statusCode&quot;: 400,\n    &quot;message&quot;: &quot;Kubernetes Deployment failed&quot;,\n    &quot;details&quot;: [\n        {\n            &quot;code&quot;: &quot;CrashLoopBackOff&quot;,\n            &quot;message&quot;: &quot;Your container application crashed. This may be caused by errors in your scoring file's init() function.\nPlease check the logs for your container instance: pipeline-created-on-07-28-2020-r. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs. \\nYou can also try to run image viennaglobal.azurecr.io\/azureml\/azureml_6ae744633f749472feb283065055dc2c:latest locally. Please refer to http:\/\/aka.ms\/debugimage#service-launch-fails for more information.&quot;\n        }\n    ]\n}\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_date":"2020-07-28 05:23:57.627000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-07-28 14:09:26.093000 UTC",
        "Question_score":1,
        "Question_tags":"azure|azure-machine-learning-studio|azure-machine-learning-service",
        "Question_view_count":186,
        "Owner_creation_date":"2017-09-12 05:36:25.677000 UTC",
        "Owner_last_access_date":"2022-09-18 07:02:14.870000 UTC",
        "Owner_location":null,
        "Owner_reputation":35,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":9,
        "Answer_body":"<p>It seems it was a bug, got corrected by itself today. Closing this question now<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-07-30 10:23:55.683000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_valid_tags":[
            "azure-machine-learning-studio",
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":65881699,
        "Question_title":"SageMaker failed to extract model data archive tar.gz for container when deploying",
        "Question_body":"<p>I am trying in Amazon Sagemaker to deploy an existing Scikit-Learn model. So a model that wasn't trained on SageMaker, but locally on my machine.<\/p>\n<p>On my local (windows) machine I've saved my model as model.joblib and tarred the model to model.tar.gz.<\/p>\n<p>Next, I've uploaded this model to my S3 bucket ('my_bucket') in the following path s3:\/\/my_bucket\/models\/model.tar.gz. I can see the tar file in S3.<\/p>\n<p>But when I'm trying to deploy the model, it keeps giving the error message &quot;Failed to extract model data archive&quot;.<\/p>\n<p>The .tar.gz is generated on my local machine by running 'tar -czf model.tar.gz model.joblib' in a powershell command window.<\/p>\n<p>The code for uploading to S3<\/p>\n<pre><code>import boto3\ns3 = boto3.client(&quot;s3&quot;, \n              region_name='eu-central-1', \n              aws_access_key_id=AWS_KEY_ID, \n              aws_secret_access_key=AWS_SECRET)\ns3.upload_file(Filename='model.tar.gz', Bucket=my_bucket, Key='models\/model.tar.gz')\n<\/code><\/pre>\n<p>The code for creating the estimator and deploying:<\/p>\n<pre><code>import boto3\nfrom sagemaker.sklearn.estimator import SKLearnModel\n\n...\n\nmodel_data = 's3:\/\/my_bucket\/models\/model.tar.gz'\nsklearn_model = SKLearnModel(model_data=model_data,\n                             role=role,\n                             entry_point=&quot;my-script.py&quot;,\n                             framework_version=&quot;0.23-1&quot;)\npredictor = sklearn_model.deploy(instance_type=&quot;ml.t2.medium&quot;, initial_instance_count=1)                             \n<\/code><\/pre>\n<p>The error message:<\/p>\n<blockquote>\n<p>error message: UnexpectedStatusException: Error hosting endpoint\nsagemaker-scikit-learn-2021-01-24-17-24-42-204: Failed. Reason: Failed\nto extract model data archive for container &quot;container_1&quot; from URL\n&quot;s3:\/\/my_bucket\/models\/model.tar.gz&quot;. Please ensure that the object\nlocated at the URL is a valid tar.gz archive<\/p>\n<\/blockquote>\n<p>Is there a way to see why the archive is invalid?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":5,
        "Question_creation_date":"2021-01-25 09:03:31.923000 UTC",
        "Question_favorite_count":2.0,
        "Question_last_edit_date":"2021-01-28 08:48:27.607000 UTC",
        "Question_score":4,
        "Question_tags":"machine-learning|scikit-learn|deployment|amazon-sagemaker",
        "Question_view_count":1859,
        "Owner_creation_date":"2016-09-04 06:28:40.890000 UTC",
        "Owner_last_access_date":"2022-08-13 10:06:28.500000 UTC",
        "Owner_location":"Amersfoort, Nederland",
        "Owner_reputation":424,
        "Owner_up_votes":15,
        "Owner_down_votes":3,
        "Owner_views":21,
        "Answer_body":"<p>I had a similar issue as well, along with a similar fix to Bas (per comment above).<\/p>\n<p>I was finding I wasn't necessarily having issues with the .tar.gz step, this command does work fine:<\/p>\n<p><code>tar -czf &lt;filename&gt; .\/&lt;directory-with-files&gt;<\/code><\/p>\n<p>but rather with the uploading step.<\/p>\n<p>Manually uploading to S3 should take care of this, however, if you're doing this step programmatically, you might need to double check the steps taken. Bas appears to have had filename issues, mine were around using boto properly. Here's some code that works (Python only here, but watch for similar issues with other libraries):<\/p>\n<pre><code>bucket = 'bucket-name'\nkey = 'directory-inside-bucket'\nfile = 'the file name of the .tar.gz'\n\ns3_client = boto3.client('s3')\ns3_client.upload_file(file, bucket, key)\n<\/code><\/pre>\n<p>Docs: <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/s3.html#S3.Client.upload_file\" rel=\"nofollow noreferrer\">https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/s3.html#S3.Client.upload_file<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-01-28 16:41:09.143000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":67407702,
        "Question_title":"Corrupted dvc.lock",
        "Question_body":"<p>I'm using DAGsHub storage as a remote and running into the following error message (when trying to DVC pull):<\/p>\n<blockquote>\n<p>ERROR: Lockfile 'bias_tagging_model\/dvc.lock' is corrupted.<\/p>\n<\/blockquote>\n<p>I thought I might have messed something up, but when cloning the git repo again and DVC pulling I am still running into this.\nThe data looks ok when viewed in the browser.\nIf you have any ideas, I would appreciate your help!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2021-05-05 19:35:18.287000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":3,
        "Question_tags":"dvc",
        "Question_view_count":362,
        "Owner_creation_date":"2021-05-04 12:52:20.443000 UTC",
        "Owner_last_access_date":"2022-01-11 19:04:00.700000 UTC",
        "Owner_location":"New York, NY, USA",
        "Owner_reputation":75,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":"<p>Usually, the reason for this error is the DVC version.<\/p>\n<p>If the dvc.lock file has a DVC 2.* schema and you are using a lower version, it will throw this error.<\/p>\n<p>Upgrade your DVC version, and it should work.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-05-06 09:13:21.493000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":4.0,
        "Question_valid_tags":[
            "dvc"
        ]
    },
    {
        "Question_id":64513552,
        "Question_title":"How to have multiple MLFlow runs in parallel?",
        "Question_body":"<p>I'm not very familiar with parallelization in Python and I'm getting an error when trying to train a model on multiple training folds in parallel. Here's a simplified version of my code:<\/p>\n<pre><code>def train_test_model(fold):\n    # here I train the model etc...\n    \n    # now I want to save the parameters and metrics\n    with mlflow.start_run():\n        mlflow.log_param(&quot;run_name&quot;, run_name)\n        mlflow.log_param(&quot;modeltype&quot;, modeltype)\n        # and so on...\n\nif __name__==&quot;__main__&quot;:\n    pool = ThreadPool(processes = num_trials)\n    # run folds in parallel\n    pool.map(lambda fold:train_test_model(fold), folds)\n<\/code><\/pre>\n<p>I'm getting the following error:<\/p>\n<pre><code>Exception: Run with UUID 23e9bb6d22674a518e48af9c51252860 is already active. To start a new run, first end the current run with mlflow.end_run(). To start a nested run, call start_run with nested=True\n<\/code><\/pre>\n<p>The <a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.start_run\" rel=\"nofollow noreferrer\">documentation<\/a> says that <code>mlflow.start_run()<\/code> starts a new run and makes it active which is the root of my problem. Every thread starts a MLFlow run for its corresponding fold and makes it active while I need the runs to run in parallel i.e. all be active(?) and save parameters\/metrics of the corresponding fold. How can I solve that issue?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-10-24 12:58:40.573000 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"python|pyspark|parallel-processing|mlflow",
        "Question_view_count":2275,
        "Owner_creation_date":"2018-01-14 12:10:19.517000 UTC",
        "Owner_last_access_date":"2022-09-24 19:20:31.110000 UTC",
        "Owner_location":null,
        "Owner_reputation":177,
        "Owner_up_votes":126,
        "Owner_down_votes":1,
        "Owner_views":7,
        "Answer_body":"<p>I found a solution, maybe it will be useful for someone else. You can see details with code examples here: <a href=\"https:\/\/github.com\/mlflow\/mlflow\/issues\/3592\" rel=\"nofollow noreferrer\">https:\/\/github.com\/mlflow\/mlflow\/issues\/3592<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-10-28 09:26:02.673000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_valid_tags":[
            "mlflow"
        ]
    },
    {
        "Question_id":64137409,
        "Question_title":"How can I create an Azure dataset in Azure ML studio (through the GUI) from a parquet file created with Azure Spark",
        "Question_body":"<p>I'm trying to load files as a dataset in the GUI of Azure ML Studio. These parquet files have been created through Spark.<\/p>\n<p>In my folder, Spark creates files such as &quot;_SUCCESS&quot; or &quot;_committed_8998000&quot;.<\/p>\n<p>Azure ML Studio is not able to read them or ignore them and tells me:<\/p>\n<pre><code>The provided file(s) have invalid byte(s) for the specified file encoding.\n{\n  &quot;message&quot;: &quot; &quot;\n}\n<\/code><\/pre>\n<p>I selected &quot;Ignore unmatched files path&quot; and yet, it still does not work.<\/p>\n<p>If I remove the &quot;_SUCCESS&quot; and other Spark files, it works.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2020-09-30 12:18:12.143000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"azure|apache-spark|parquet|azure-machine-learning-studio",
        "Question_view_count":178,
        "Owner_creation_date":"2015-02-11 07:34:40.283000 UTC",
        "Owner_last_access_date":"2022-09-23 14:32:37.963000 UTC",
        "Owner_location":"Lyon, France",
        "Owner_reputation":457,
        "Owner_up_votes":19,
        "Owner_down_votes":2,
        "Owner_views":125,
        "Answer_body":"<p>Thanks for the feedback. You can use globing in path. e.g. path = '**\/*.parquet' to select only the parquet files<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2020-09-30 16:39:04.070000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":42766263,
        "Question_title":"How to Find Azure ML Experiment based on Deployed Web Service",
        "Question_body":"<p>I have a list of ML experiments which I have created in Azure Machine Learning Studio.  I have deployed them as web services (the new version, not classic).  <\/p>\n\n<p>How can I go into Azure Machine Learning Web Services, click on a web service (which was deployed from an experiment), then navigate back to the experiment \/ predictive model which feeds it?<\/p>\n\n<p>The only link I can find between the two is by updating the web service from the predictive experiment, which then confirms what the web service is. I can see that the \"ExperimentId\" is a GUID in the URL when in the experiment and the web service, so hopefully this is possible.<\/p>\n\n<p>My reasoning is that relying on matching naming conventions, etc., to select the appropriate model to update is subject to human error.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2017-03-13 14:35:50.577000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":7,
        "Question_tags":"web-services|azure|azure-machine-learning-studio",
        "Question_view_count":224,
        "Owner_creation_date":"2013-08-28 15:24:36.330000 UTC",
        "Owner_last_access_date":"2021-07-14 22:35:47.190000 UTC",
        "Owner_location":"Bristol, United Kingdom",
        "Owner_reputation":592,
        "Owner_up_votes":18,
        "Owner_down_votes":0,
        "Owner_views":43,
        "Answer_body":"<p>The <em>new<\/em> web service does not store any information about the experiment or workspace that was deployed (not all <em>new<\/em> web services are deployed from an experiment).<\/p>\n\n<p>Here are the options available to track the relationship between the experiment and a <em>new<\/em> web service.<\/p>\n\n<h2>last deployment<\/h2>\n\n<p>However, the experiment keeps track of the <strong>last<\/strong> <em>new<\/em> web service that was deployed from the experiment. each deployment to a <em>new<\/em> web service overwrites this value.<\/p>\n\n<p>The value is stored in the experiment graph. One way to get the graph is to use the powershell module <a href=\"http:\/\/aka.ms\/amlps\" rel=\"nofollow noreferrer\">amlps<\/a><\/p>\n\n<p><code>Export-AmlExperimentGraph -ExperimentId &lt;Experiment Id&gt; -OutputFile e.json<\/code><\/p>\n\n<p><strong>e.json<\/strong><\/p>\n\n<pre><code>{\n\"ExperimentId\":\"&lt;Experiment Id&gt;\",\n\/\/ . . .\n\"WebService\":{\n\/\/ . . .\n\"ArmWebServiceId\":\"&lt;Arm Id&gt;\"\n},\n\/\/ . . . \n}\n<\/code><\/pre>\n\n<h2>azure resource tags<\/h2>\n\n<p>The tags feature for Azure resources is supported by the <em>new<\/em> web services. Setting a <code>tag<\/code> on the web service programmatically, with powershell or via the azure portal UX can be used to store a reference to the experiment on the <em>new<\/em> web service.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2017-03-23 15:38:46.690000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":64684503,
        "Question_title":"Does AWS Sagemaker charges you per API request?",
        "Question_body":"<p>AWS pricing page describes how much it costs per hour to run AWS Sagemaker for online realtime inference.\n<a href=\"https:\/\/aws.amazon.com\/sagemaker\/pricing\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/sagemaker\/pricing\/<\/a><\/p>\n<p>But AWS usually also charges for API requests.\nDo they charge extra per every API inference request to the Sagemaker model?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-11-04 17:01:34.833000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":187,
        "Owner_creation_date":"2017-08-02 21:51:50.163000 UTC",
        "Owner_last_access_date":"2022-09-22 16:56:11.783000 UTC",
        "Owner_location":"London, \u0412\u0435\u043b\u0438\u043a\u043e\u0431\u0440\u0438\u0442\u0430\u043d\u0438\u044f",
        "Owner_reputation":404,
        "Owner_up_votes":55,
        "Owner_down_votes":2,
        "Owner_views":25,
        "Answer_body":"<p>I am on the AWS SageMaker team.  For &quot;Real-Time Inference&quot; you are only charged for:<\/p>\n<ol>\n<li>usage of the instance types you choose (instance hours)<\/li>\n<li>storage attached to those instance (GB storage hours)<\/li>\n<li>data in and out of your Endpoint (Bytes in\/out)<\/li>\n<\/ol>\n<p>See &quot;Pricing Example #6: Real-Time Inference&quot; as well.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-12-16 22:55:11.613000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":65075228,
        "Question_title":"How could I resolve ImportError: no module named 'azureml', I am using Azure databricks notebook",
        "Question_body":"<p>I am trying to creating similar machine learning experiment as on found on github at below link.<\/p>\n<p><a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/automated-machine-learning\/forecasting-orange-juice-sales\/auto-ml-forecasting-orange-juice-sales.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/automated-machine-learning\/forecasting-orange-juice-sales\/auto-ml-forecasting-orange-juice-sales.ipynb<\/a><\/p>\n<p>what could I do to resolve the ImportEror:?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2020-11-30 14:15:28.693000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"azure-databricks|azure-machine-learning-service",
        "Question_view_count":1063,
        "Owner_creation_date":"2017-01-09 16:18:30.217000 UTC",
        "Owner_last_access_date":"2021-07-12 16:45:28.897000 UTC",
        "Owner_location":null,
        "Owner_reputation":25,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":17,
        "Answer_body":"<blockquote>\n<p>To resolve this issue, I would request you to install azureml library from PyPi packages.<\/p>\n<\/blockquote>\n<p>To make third-party or locally-built code available to notebooks and jobs running on your clusters, you can install a library. Libraries can be written in Python, Java, Scala, and R. You can upload Java, Scala, and Python libraries and point to external packages in PyPI, Maven, and CRAN repositories.<\/p>\n<p><strong>Steps to install third-party libraries:<\/strong><\/p>\n<p><strong>Step1:<\/strong> Create Databricks Cluster.<\/p>\n<p><strong>Step2:<\/strong> Select the cluster created.<\/p>\n<p><strong>Step3:<\/strong> Select Libraries =&gt; Install New =&gt; Select Library Source = &quot;PYPI&quot; =&gt; Package = &quot;azureml-sdk[databricks]&quot;.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/DafqR.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/DafqR.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/wCjUG.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/wCjUG.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/HD6VC.gif\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/HD6VC.gif\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><strong>Reference:<\/strong> <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-configure-databricks-automl-environment\" rel=\"nofollow noreferrer\">Set up a development environment with Azure Databricks and autoML in Azure Machine Learning<\/a><\/p>\n<p>For different methods to install packages in Azure Databricks: <a href=\"https:\/\/stackoverflow.com\/questions\/60543850\/how-to-install-a-library-on-a-databricks-cluster-using-some-command-in-the-noteb\/60557852#60557852\">How to install a library on a databricks cluster using some command in the notebook?<\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2020-12-01 06:21:04.677000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":67209146,
        "Question_title":"DVC - make scheduled csv dumps",
        "Question_body":"<p>Suppose we got some database (any database, that supports csv dumping), collecting raw data in real time for further usage in ML.\nOn the other side, we got DVC, that can work with csv files.<\/p>\n<p>I want to organize a scheduled run of stored SELECT to that DB with datetime parameters (and also support a manual run), to make a new csv files, and send them to DVC.<\/p>\n<p>In DVC documentation and examples I found, csv file already exists.<\/p>\n<p>Can I make this interaction with database with DVC itself, or I got something wrong, and there is a separate tool for csv dump?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-04-22 08:01:27.020000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"export-to-csv|dvc",
        "Question_view_count":65,
        "Owner_creation_date":"2020-02-05 20:03:01.007000 UTC",
        "Owner_last_access_date":"2022-09-21 07:19:14.923000 UTC",
        "Owner_location":null,
        "Owner_reputation":55,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Answer_body":"<p>There are 3 steps in this process:<\/p>\n<ol>\n<li>Create a CSV dump. Many DBs have these tools but DVC does not support this natively.<\/li>\n<li>Version the CSV dump and move it to some storage. DVC does this job.<\/li>\n<li>Schedule periodical dump. You can use Cron (easy), AirFlow (not easy) or <a href=\"https:\/\/docs.github.com\/en\/actions\/reference\/events-that-trigger-workflows\" rel=\"nofollow noreferrer\">periodical jobs in GitHub Actions<\/a>\/<a href=\"https:\/\/docs.gitlab.com\/ee\/ci\/pipelines\/schedules.html\" rel=\"nofollow noreferrer\">GitLab CI\/CD<\/a>. Another project from the DVC team can help with CI\/CD option: <a href=\"https:\/\/cml.dev\" rel=\"nofollow noreferrer\">https:\/\/cml.dev<\/a>.<\/li>\n<\/ol>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-04-22 08:56:43.750000 UTC",
        "Answer_last_edit_date":"2021-04-22 09:49:00.030000 UTC",
        "Answer_score":4.0,
        "Question_valid_tags":[
            "dvc"
        ]
    },
    {
        "Question_id":51792005,
        "Question_title":"Sagemaker: DeepAR Hyperparameter Tuning Error",
        "Question_body":"<p>Running into a new issue with tuning DeepAR on SageMaker when trying to initialize a hyperparameter tuning job - this error also occurs when calling the test:mean_wQuantileLoss. I've upgraded the sagemaker package, restarted my instance, restarted the kernel (using a juptyer notebook), and yet the problem persists. <\/p>\n\n<pre><code>ClientError: An error occurred (ValidationException) when calling the \nCreateHyperParameterTuningJob operation: The objective metric type, [Maximize], that you specified for objective metric, [test:RMSE], isn\u2019t valid for the [156387875391.dkr.ecr.us-west-2.amazonaws.com\/forecasting-deepar:1] algorithm. Choose a valid objective metric type.\n<\/code><\/pre>\n\n<p>Code:<\/p>\n\n<pre><code>my_tuner = HyperparameterTuner(estimator=estimator,\n                               objective_metric_name=\"test:RMSE\",\n                               hyperparameter_ranges=hyperparams,\n                               max_jobs=20,\n                               max_parallel_jobs=2)\n\n# Start hyperparameter tuning job\nmy_tuner.fit(inputs=data_channels)\n\nStack Trace:\nClientError                               Traceback (most recent call last)\n&lt;ipython-input-66-9d6d8de89536&gt; in &lt;module&gt;()\n      7 \n      8 # Start hyperparameter tuning job\n----&gt; 9 my_tuner.fit(inputs=data_channels)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/tuner.py in fit(self, inputs, job_name, include_cls_metadata, **kwargs)\n    255 \n    256         self._prepare_for_training(job_name=job_name, include_cls_metadata=include_cls_metadata)\n--&gt; 257         self.latest_tuning_job = _TuningJob.start_new(self, inputs)\n    258 \n    259     @classmethod\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/tuner.py in start_new(cls, tuner, inputs)\n    525                                                output_config=(config['output_config']),\n    526                                                resource_config=(config['resource_config']),\n--&gt; 527                                                stop_condition=(config['stop_condition']), tags=tuner.tags)\n    528 \n    529         return cls(tuner.sagemaker_session, tuner._current_job_name)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/session.py in tune(self, job_name, strategy, objective_type, objective_metric_name, max_jobs, max_parallel_jobs, parameter_ranges, static_hyperparameters, image, input_mode, metric_definitions, role, input_config, output_config, resource_config, stop_condition, tags)\n    348         LOGGER.info('Creating hyperparameter tuning job with name: {}'.format(job_name))\n    349         LOGGER.debug('tune request: {}'.format(json.dumps(tune_request, indent=4)))\n--&gt; 350         self.sagemaker_client.create_hyper_parameter_tuning_job(**tune_request)\n    351 \n    352     def stop_tuning_job(self, name):\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/client.py in _api_call(self, *args, **kwargs)\n    312                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n    313             # The \"self\" in this scope is referring to the BaseClient.\n--&gt; 314             return self._make_api_call(operation_name, kwargs)\n    315 \n    316         _api_call.__name__ = str(py_operation_name)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/client.py in _make_api_call(self, operation_name, api_params)\n    610             error_code = parsed_response.get(\"Error\", {}).get(\"Code\")\n    611             error_class = self.exceptions.from_code(error_code)\n--&gt; 612             raise error_class(parsed_response, operation_name)\n    613         else:\n    614             return parsed_response\n\nClientError: An error occurred (ValidationException) when calling the CreateHyperParameterTuningJob operation: \nThe objective metric type, [Maximize], that you specified for objective metric, [test:RMSE], isn\u2019t valid for the [156387875391.dkr.ecr.us-west-2.amazonaws.com\/forecasting-deepar:1] algorithm. \nChoose a valid objective metric type.\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2018-08-10 18:14:21.517000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":1355,
        "Owner_creation_date":"2013-09-11 21:34:25.347000 UTC",
        "Owner_last_access_date":"2018-11-07 18:08:35.827000 UTC",
        "Owner_location":null,
        "Owner_reputation":27,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Answer_body":"<p>It looks like you are trying to maximize this metric, <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/deepar-tuning.html\" rel=\"nofollow noreferrer\">test:RMSE can only be minimized<\/a> by SageMaker HyperParameter Tuning. <\/p>\n\n<p>To achieve this in the SageMaker Python SDK, create your HyperparameterTuner with objective_type='Minimize'. You can see the signature of the init method <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tuner.py#L158\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n\n<p>Here is the change you should make to your call to HyperparameterTuner:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>my_tuner = HyperparameterTuner(estimator=estimator,\n                               objective_metric_name=\"test:RMSE\",\n                               objective_type='Minimize',\n                               hyperparameter_ranges=hyperparams,\n                               max_jobs=20,\n                               max_parallel_jobs=2)\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2018-08-10 19:39:04.953000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":73244442,
        "Question_title":"HuggingFace Trainer() cannot report to wandb",
        "Question_body":"<p>I am trying to set trainer with arguments <code>report_to<\/code> to <code>wandb<\/code>, refer to <a href=\"https:\/\/docs.wandb.ai\/guides\/integrations\/huggingface#getting-started-track-experiments\" rel=\"nofollow noreferrer\">this docs<\/a>\nwith config:<\/p>\n<pre><code>training_args = TrainingArguments(\n    output_dir=&quot;test_trainer&quot;,\n    evaluation_strategy=&quot;steps&quot;,\n    learning_rate=config.learning_rate,\n    num_train_epochs=config.epochs,\n    weight_decay=config.weight_decay,\n    logging_dir=config.logging_dir,\n    report_to=&quot;wandb&quot;,\n    save_total_limit=1,\n    per_device_train_batch_size=config.batch_size,\n    per_device_eval_batch_size=config.batch_size,\n    fp16=True,\n    load_best_model_at_end=True,\n    seed=42\n)\n<\/code><\/pre>\n<p>yet when I set trainer with:<\/p>\n<pre><code>trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    compute_metrics=compute_metrics\n)\n<\/code><\/pre>\n<p>it shows:<\/p>\n<pre><code>---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-68-b009351ab52d&gt; in &lt;module&gt;\n      4     train_dataset=train_dataset,\n      5     eval_dataset=eval_dataset,\n----&gt; 6     compute_metrics=compute_metrics\n      7 )\n\n~\/.virtualenvs\/transformers_lab\/lib\/python3.7\/site-packages\/transformers\/trainer.py in __init__(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers)\n    286                 &quot;You should subclass `Trainer` and override the `create_optimizer_and_scheduler` method.&quot;\n    287             )\n--&gt; 288         default_callbacks = DEFAULT_CALLBACKS + get_reporting_integration_callbacks(self.args.report_to)\n    289         callbacks = default_callbacks if callbacks is None else default_callbacks + callbacks\n    290         self.callback_handler = CallbackHandler(\n\n~\/.virtualenvs\/transformers_lab\/lib\/python3.7\/site-packages\/transformers\/integrations.py in get_reporting_integration_callbacks(report_to)\n    794         if integration not in INTEGRATION_TO_CALLBACK:\n    795             raise ValueError(\n--&gt; 796                 f&quot;{integration} is not supported, only {', '.join(INTEGRATION_TO_CALLBACK.keys())} are supported.&quot;\n    797             )\n    798     return [INTEGRATION_TO_CALLBACK[integration] for integration in report_to]\n\nValueError: w is not supported, only azure_ml, comet_ml, mlflow, tensorboard, wandb are supported.\n<\/code><\/pre>\n<p>Have anyone got same error before?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-08-05 03:48:25.703000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-08-09 08:29:23.877000 UTC",
        "Question_score":0,
        "Question_tags":"python|huggingface-transformers|wandb",
        "Question_view_count":47,
        "Owner_creation_date":"2020-04-18 05:07:48.850000 UTC",
        "Owner_last_access_date":"2022-09-23 07:29:12.847000 UTC",
        "Owner_location":"Taipei, Taiwan R.O.C",
        "Owner_reputation":163,
        "Owner_up_votes":13,
        "Owner_down_votes":0,
        "Owner_views":15,
        "Answer_body":"<p>Although the documentation states that the <code>report_to<\/code> parameter can receive both <code>List[str]<\/code> or <code>str<\/code> I have always used a list with 1! element for this purpose.<\/p>\n<p>Therefore, even if you report only to wandb, the solution to your problem is to replace:<\/p>\n<pre><code> report_to = 'wandb'\n<\/code><\/pre>\n<p>with<\/p>\n<pre><code>report_to = [&quot;wandb&quot;]\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2022-08-06 10:28:14.627000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "wandb"
        ]
    },
    {
        "Question_id":73676684,
        "Question_title":"How to run SageMaker Distributed training from SageMaker Studio?",
        "Question_body":"<p>The sample notebooks for <strong>SageMaker Distributed training<\/strong>, like here:\u00a0https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/advanced_functionality\/distributed_tensorflow_mask_rcnn\/mask-rcnn-scriptmode-s3.ipynb rely on the\u00a0<code>docker build .<\/code> and\u00a0<code>docker push .<\/code> commands, which are not available or installable in <a href=\"https:\/\/aws.amazon.com\/sagemaker\/studio\/\" rel=\"nofollow noreferrer\">Amazon SageMaker Studio<\/a>.<\/p>\n<p>Are there alternatives of these notebooks that are compatible with the SageMaker Studio?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-09-11 03:49:16.347000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker|distributed-training|amazon-machine-learning|amazon-sagemaker-studio",
        "Question_view_count":16,
        "Owner_creation_date":"2014-01-16 15:43:59.673000 UTC",
        "Owner_last_access_date":"2022-09-25 03:22:08.463000 UTC",
        "Owner_location":"Singapore",
        "Owner_reputation":5854,
        "Owner_up_votes":155,
        "Owner_down_votes":70,
        "Owner_views":794,
        "Answer_body":"<p>SageMaker Studio does not support Docker, since the Studio apps are containers themselves. You can use the SageMaker Docker Build tool to build docker images from Studio (uses CodeBuild in the backend). See the blog <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/using-the-amazon-sagemaker-studio-image-build-cli-to-build-container-images-from-your-studio-notebooks\/\" rel=\"nofollow noreferrer\">Using the Amazon SageMaker Studio Image Build CLI to build container images from your Studio notebooks<\/a> and the <a href=\"https:\/\/github.com\/aws-samples\/sagemaker-studio-image-build-cli\" rel=\"nofollow noreferrer\">Github repo<\/a> for details.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-09-13 18:56:21.000000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":72551630,
        "Question_title":"How to setup a DVC shared cache without git repository between different services in minikube?",
        "Question_body":"<p>I need to setup a shared cache in minikube in such a way that different services can use that cache to pull and update DVC models and data needed for training Machine Learning models. The structure of the project is to use 1 pod to periodically update the cache with new models and outputs. Then, multiple pods can read the cache to recreate the updated models and data. So I need to be able to update the local cache directory and pull from it using DVC commands, so that all the services have consistent view on the latest models and data created by a service.<\/p>\n<p>More specifically, I have a docker image called <code>inference-service<\/code> that should only <code>dvc pull<\/code> or some how use the info in the shared dvc cache to get the latest model and data locally in <code>models<\/code> and <code>data<\/code> folders (see dockerfile) in minikube. I have another image called <code>test-service<\/code> that\nruns the ML pipeline using <code>dvc repro<\/code> which creates the models and data that DVC needs (dvc.yaml) to track and store in the shared cache. So <code>test-service<\/code> should push created outputs from the ML pipeline into the shared cache so that <code>inference-service<\/code> can pull it and use it instead of running dvc repro by itself. <code>test-service<\/code> should only re-train and write the updated models and data into the shared cache while <code>inference-service<\/code> should only read and recreate the updated\/latest models and data from the shared cache.<\/p>\n<p><em><strong>Problem: the cache does get mounted on the minikube VM, but the inference service does not pull (using <code>dvc pull -f<\/code>) the data and models after the test service is done with <code>dvc repro<\/code> and results the following warnings and failures:<\/strong><\/em><\/p>\n<p><em>relevant kubernetes pod log of inference-service<\/em><\/p>\n<pre><code>WARNING: Output 'data\/processed\/train_preprocessed.pkl'(stage: 'preprocess') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nYou can also use `dvc commit preprocess` to associate existing 'data\/processed\/train_preprocessed.pkl' with stage: 'preprocess'.\nWARNING: Output 'data\/processed\/validation_preprocessed.pkl'(stage: 'preprocess') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nYou can also use `dvc commit preprocess` to associate existing 'data\/processed\/validation_preprocessed.pkl' with stage: 'preprocess'.\nWARNING: Output 'data\/processed\/test_preprocessed.pkl'(stage: 'preprocess') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nYou can also use `dvc commit preprocess` to associate existing 'data\/processed\/test_preprocessed.pkl' with stage: 'preprocess'.\nWARNING: Output 'data\/interim\/train_featurized.pkl'(stage: 'featurize') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nYou can also use `dvc commit featurize` to associate existing 'data\/interim\/train_featurized.pkl' with stage: 'featurize'.\nWARNING: Output 'data\/interim\/validation_featurized.pkl'(stage: 'featurize') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nYou can also use `dvc commit featurize` to associate existing 'data\/interim\/validation_featurized.pkl' with stage: 'featurize'.\nWARNING: Output 'data\/interim\/test_featurized.pkl'(stage: 'featurize') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nYou can also use `dvc commit featurize` to associate existing 'data\/interim\/test_featurized.pkl' with stage: 'featurize'.\nWARNING: Output 'models\/mlb.pkl'(stage: 'featurize') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nWARNING: Output 'models\/tfidf_vectorizer.pkl'(stage: 'featurize') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nWARNING: Output 'models\/model.pkl'(stage: 'train') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nWARNING: Output 'reports\/scores.json'(stage: 'evaluate') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nWARNING: No file hash info found for '\/root\/models\/model.pkl'. It won't be created.\nWARNING: No file hash info found for '\/root\/reports\/scores.json'. It won't be created.\nWARNING: No file hash info found for '\/root\/data\/processed\/train_preprocessed.pkl'. It won't be created.\nWARNING: No file hash info found for '\/root\/data\/processed\/validation_preprocessed.pkl'. It won't be created.\nWARNING: No file hash info found for '\/root\/data\/processed\/test_preprocessed.pkl'. It won't be created.\nWARNING: No file hash info found for '\/root\/data\/interim\/train_featurized.pkl'. It won't be created.\nWARNING: No file hash info found for '\/root\/data\/interim\/validation_featurized.pkl'. It won't be created.\nWARNING: No file hash info found for '\/root\/data\/interim\/test_featurized.pkl'. It won't be created.\nWARNING: No file hash info found for '\/root\/models\/mlb.pkl'. It won't be created.\nWARNING: No file hash info found for '\/root\/models\/tfidf_vectorizer.pkl'. It won't be created.\n10 files failed\nERROR: failed to pull data from the cloud - Checkout failed for following targets:\n\/root\/models\/model.pkl\n\/root\/reports\/scores.json\n\/root\/data\/processed\/train_preprocessed.pkl\n\/root\/data\/processed\/validation_preprocessed.pkl\n\/root\/data\/processed\/test_preprocessed.pkl\n\/root\/data\/interim\/train_featurized.pkl\n\/root\/data\/interim\/validation_featurized.pkl\n\/root\/data\/interim\/test_featurized.pkl\n\/root\/models\/mlb.pkl\n\/root\/models\/tfidf_vectorizer.pkl\nIs your cache up to date?\n<\/code><\/pre>\n<p><em>relevant kubernetes pod log of test-service<\/em><\/p>\n<pre><code>Stage 'preprocess' is cached - skipping run, checking out outputs\nGenerating lock file 'dvc.lock'\nUpdating lock file 'dvc.lock'\nStage 'featurize' is cached - skipping run, checking out outputs\nUpdating lock file 'dvc.lock'\nStage 'train' is cached - skipping run, checking out outputs\nUpdating lock file 'dvc.lock'\nStage 'evaluate' is cached - skipping run, checking out outputs\nUpdating lock file 'dvc.lock'\nUse `dvc push` to send your updates to remote storage.\n<\/code><\/pre>\n<p><strong>Project Tree<\/strong><\/p>\n<pre><code>\u251c\u2500 .dvc\n\u2502  \u251c\u2500 .gitignore\n\u2502  \u251c\u2500 config\n\u2502  \u2514\u2500 tmp\n\u251c\u2500 deployment\n\u2502  \u251c\u2500 docker-compose\n\u2502  \u2502  \u251c\u2500 docker-compose.yml\n\u2502  \u251c\u2500 minikube-dep\n\u2502  \u2502  \u251c\u2500 inference-test-services_dep.yaml\n\u2502  \u251c\u2500 startup_minikube_with_mount.sh.sh\n\u251c\u2500 Dockerfile # for inference service\n\u251c\u2500 dvc-cache # services should push and pull from this cache folder and see this as the DVC repo\n\u251c- dvc.yaml\n\u251c- params.yaml\n\u251c\u2500 src\n\u2502  \u251c\u2500 build_features.py\n|  \u251c\u2500 preprocess_data.py\n|  \u251c\u2500 serve_model.py\n|  \u251c\u2500 startup.sh  \n|  \u251c\u2500 requirements.txt\n\u251c\u2500 test_dep\n\u2502  \u251c\u2500 .dvc # same as .dvc in the root folder\n|  |  \u251c\u2500...\n\u2502  \u251c\u2500 Dockerfile # for test service\n\u2502  \u251c\u2500 dvc.yaml\n|  \u251c\u2500 params.yaml\n\u2502  \u2514\u2500 src\n\u2502     \u251c\u2500 build_features.py # same as root src folder\n|     \u251c\u2500 preprocess_data.py # same as root src folder\n|     \u251c\u2500 serve_model.py # same as root src folder\n|     \u251c\u2500 startup_test.sh  \n|     \u251c\u2500 requirements.txt  # same as root src folder\n<\/code><\/pre>\n<p><strong>dvc.yaml<\/strong><\/p>\n<pre><code>stages:\n  preprocess:\n    cmd: python ${preprocess.script}\n    params:\n      - preprocess\n    deps:\n      - ${preprocess.script}\n      - ${preprocess.input_train}\n      - ${preprocess.input_val}\n      - ${preprocess.input_test}\n    outs:\n      - ${preprocess.output_train}\n      - ${preprocess.output_val}\n      - ${preprocess.output_test}\n  featurize:\n    cmd: python ${featurize.script}\n    params:\n      - preprocess\n      - featurize\n    deps:\n      - ${featurize.script}\n      - ${preprocess.output_train}\n      - ${preprocess.output_val}\n      - ${preprocess.output_test}\n    outs:\n      - ${featurize.output_train}\n      - ${featurize.output_val}\n      - ${featurize.output_test}\n      - ${featurize.mlb_out}\n      - ${featurize.tfidf_vectorizer_out}\n  train:\n    cmd: python ${train.script}\n    params:\n      - featurize\n      - train\n    deps:\n      - ${train.script}\n      - ${featurize.output_train}\n    outs:\n      - ${train.model_out}\n  evaluate:\n    cmd: python ${evaluate.script}\n    params:\n      - featurize\n      - train\n      - evaluate\n    deps:\n      - ${evaluate.script}\n      - ${train.model_out}\n      - ${featurize.output_val}\n    metrics:\n      - ${evaluate.scores_path}\n<\/code><\/pre>\n<p><strong>params.yaml<\/strong><\/p>\n<pre><code>preprocess:\n  script: src\/preprocess\/preprocess_data.py\n  input_train: data\/raw\/train.tsv\n  input_val: data\/raw\/validation.tsv\n  input_test: data\/raw\/test.tsv\n  output_train: data\/processed\/train_preprocessed.pkl\n  output_val: data\/processed\/validation_preprocessed.pkl\n  output_test: data\/processed\/test_preprocessed.pkl\n\nfeaturize:\n  script: src\/features\/build_features.py\n  output_train: data\/interim\/train_featurized.pkl\n  output_val: data\/interim\/validation_featurized.pkl\n  output_test: data\/interim\/test_featurized.pkl\n  mlb_out: models\/mlb.pkl\n  tfidf_vectorizer_out: models\/tfidf_vectorizer.pkl\n\ntrain:\n  script: src\/models\/train_model.py\n  model_out: models\/model.pkl\n\nevaluate:\n  script: src\/models\/evaluate_model.py\n  scores_path: reports\/scores.json\n  roc_json: reports\/roc_plot.json\n  prc_json: reports\/prc_plot.json\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-06-08 20:07:48.090000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-06-13 21:50:56.467000 UTC",
        "Question_score":0,
        "Question_tags":"docker|kubernetes|minikube|dvc",
        "Question_view_count":196,
        "Owner_creation_date":"2019-07-06 08:50:39.337000 UTC",
        "Owner_last_access_date":"2022-08-02 18:08:37.997000 UTC",
        "Owner_location":null,
        "Owner_reputation":298,
        "Owner_up_votes":14,
        "Owner_down_votes":1,
        "Owner_views":47,
        "Answer_body":"<p>After running <code>dvc repro<\/code> in <code>test-service<\/code>, a new <code>dvc.lock<\/code> will be created, containing the file hashes relative to your pipeline (i.e. the hash for <code>models\/model.pkl<\/code> etc).<\/p>\n<p>If you're running a shared cache, <code>inference-service<\/code> should have access to the updated <code>dvc.lock<\/code>. If that is present, it will be sufficient to run <code>dvc checkout<\/code> to populate the workspace with the files corresponding to the hashes in the shared cache.<\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_date":"2022-06-10 15:37:00.463000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_valid_tags":[
            "dvc"
        ]
    },
    {
        "Question_id":61832086,
        "Question_title":"Having issues reading S3 bucket when transitioning a tensorflow model from local machine to AWS SageMaker",
        "Question_body":"<p>When testing on a local machine in Python I would normally use the following to read a training set with sub-directories of all the classes and files\/class:<\/p>\n\n<pre><code>train_path = r\"C:\\temp\\coins\\PCGS - Gold\\train\"\n\ntrain_batches = ImageDataGenerator().flow_from_directory(train_path, target_size=(100,100), classes=['0','1',2','3' etc...], batch_size=32)\n<\/code><\/pre>\n\n<p><strong>Found 4100 images belonging to 22 classes.<\/strong><\/p>\n\n<p>but on AWS SageMaker's Jupyter notebook I am now pulling the files from an S3 bucket.  I tried the following: <\/p>\n\n<pre><code>bucket = \"coinpath\"\n\ntrain_path = 's3:\/\/{}\/{}\/train'.format(bucket, \"v1\")   #note that the directory structure is coinpath\/v1\/train where coinpath is the bucket\n\ntrain_batches = ImageDataGenerator().flow_from_directory(train_path, target_size=(100,100), classes=\n['0','1',2','3' etc...], batch_size=32)\n<\/code><\/pre>\n\n<p>but I get: ** Found 0 images belonging to 22 classes.**<\/p>\n\n<p>Looking for some guidance on the right way to pull training data from S3.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2020-05-16 04:57:46.783000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"tensorflow|amazon-sagemaker",
        "Question_view_count":608,
        "Owner_creation_date":"2020-03-02 04:35:49.267000 UTC",
        "Owner_last_access_date":"2022-09-22 20:14:01.340000 UTC",
        "Owner_location":"Washington D.C., DC, USA",
        "Owner_reputation":317,
        "Owner_up_votes":60,
        "Owner_down_votes":0,
        "Owner_views":35,
        "Answer_body":"<p>From <a href=\"https:\/\/stackoverflow.com\/questions\/54736505\/ideal-way-to-read-data-in-bucket-stored-batches-of-data-for-keras-ml-training-in\">Ideal way to read data in bucket stored batches of data for Keras ML training in Google Cloud Platform?<\/a> \"ImageDataGenerator.flow_from_directory() currently does not allow you to stream data directly from a GCS bucket. \"<\/p>\n\n<p>I had to download the image from S3 first.  This is best for latency reasons as well. <\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2020-05-18 02:23:32.457000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":69373666,
        "Question_title":"Deployment with customer handler on Google Cloud Vertex AI",
        "Question_body":"<p>I'm trying to deploy a TorchServe instance on Google Vertex AI platform but as per their documentation (<a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/custom-container-requirements#response_requirements\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/custom-container-requirements#response_requirements<\/a>), it requires the responses to be of the following shape:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n  &quot;predictions&quot;: PREDICTIONS\n}\n<\/code><\/pre>\n<p>Where <strong>PREDICTIONS<\/strong> is an array of JSON values representing the predictions that your container has generated.<\/p>\n<p>Unfortunately, when I try to return such a shape in the <code>postprocess()<\/code> method of my custom handler, as such:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>def postprocess(self, data):\n    return {\n        &quot;predictions&quot;: data\n    }\n<\/code><\/pre>\n<p>TorchServe returns:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n  &quot;code&quot;: 503,\n  &quot;type&quot;: &quot;InternalServerException&quot;,\n  &quot;message&quot;: &quot;Invalid model predict output&quot;\n}\n<\/code><\/pre>\n<p>Please note that <code>data<\/code> is a list of lists, for example: [[1, 2, 1], [2, 3, 3]]. (Basically, I am generating embeddings from sentences)<\/p>\n<p>Now if I simply return <code>data<\/code> (and not a Python dictionary), it works with TorchServe but when I deploy the container on Vertex AI, it returns the following error:  <code>ModelNotFoundException<\/code>. I assumed Vertex AI throws this error since the return shape does not match what's expected (c.f. documentation).<\/p>\n<p>Did anybody successfully manage to deploy a TorchServe instance with custom handler on Vertex AI?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2021-09-29 09:23:26.933000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-09-29 11:28:26.017000 UTC",
        "Question_score":0,
        "Question_tags":"google-cloud-platform|pytorch|google-cloud-ml|google-cloud-vertex-ai|torchserve",
        "Question_view_count":433,
        "Owner_creation_date":"2011-03-18 10:16:38.977000 UTC",
        "Owner_last_access_date":"2022-02-21 16:31:56.407000 UTC",
        "Owner_location":"Switzerland",
        "Owner_reputation":681,
        "Owner_up_votes":72,
        "Owner_down_votes":7,
        "Owner_views":34,
        "Answer_body":"<p>Actually, making sure that the TorchServe processes correctly the input dictionary (instances) solved the issue. It seems like what's on the <a href=\"https:\/\/cloud.google.com\/blog\/topics\/developers-practitioners\/pytorch-google-cloud-how-deploy-pytorch-models-vertex-ai\" rel=\"nofollow noreferrer\">article<\/a> did not work for me.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-10-03 16:29:02.093000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "google-cloud-vertex-ai"
        ]
    },
    {
        "Question_id":69158205,
        "Question_title":"Differences between using Sagemaker notebook vs Glue (Sagemaker) notebook",
        "Question_body":"<p>I have a Machine Learning job I want to run with Sagemaker. For data preparation and transformation, I am using some numpy and pandas steps to transform them with notebook.<\/p>\n<p>I noticed AWS Glue have both <a href=\"https:\/\/docs.aws.amazon.com\/glue\/latest\/dg\/aws-glue-programming-python-samples-legislators.html\" rel=\"nofollow noreferrer\">Sagemaker and Zeppelin notebook which can be created via development endpoint<\/a><\/p>\n<p>There isn't much info online i could find what's the difference and benefit of using one over another (i.e. Sagemaker notebook and import from s3 vs creating notebook from Glue)<\/p>\n<p>From what i researched and tried it seems that i can achieve same thing with both:<\/p>\n<ul>\n<li>Sagemaker notebook and import directly from s3 + further python code to process the data<\/li>\n<li>Glue (need to crawl and create dataset) as shown <a href=\"https:\/\/docs.aws.amazon.com\/glue\/latest\/dg\/aws-glue-programming-python-samples-legislators.html#aws-glue-programming-python-samples-legislators-crawling\" rel=\"nofollow noreferrer\">here<\/a>, create dev endpoint and use similar script to process the data.<\/li>\n<\/ul>\n<p>Anyone able to shed light on this?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2021-09-13 06:30:02.627000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"jupyter-notebook|aws-glue|amazon-sagemaker",
        "Question_view_count":1517,
        "Owner_creation_date":"2015-04-29 01:25:44.417000 UTC",
        "Owner_last_access_date":"2022-09-25 05:47:18.430000 UTC",
        "Owner_location":null,
        "Owner_reputation":441,
        "Owner_up_votes":166,
        "Owner_down_votes":2,
        "Owner_views":42,
        "Answer_body":"<p>The question isn't clear but let me explain this point.<\/p>\n<p>When you launch a Glue Development endpoint you can attach either a SageMaker notebook or Zeppelin notebook. Both will be created and configured by Glue and your script will be executed on the Glue Dev endpoint.<\/p>\n<p><strong>If your question is &quot;what is the difference between a SageMaker notebook created from Glue console and a SageMaker notebook created from SageMaker console?<\/strong><\/p>\n<p>When you create a notebook instance from Glue console, the created notebook will always have public internet access enabled. <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/understanding-amazon-sagemaker-notebook-instance-networking-configurations-and-advanced-routing-options\/\" rel=\"nofollow noreferrer\">This blog<\/a> explains the difference between the networking configurations with SM notebooks. You cannot also create the notebook with a specific disk size but you can stop the notebook once it's created and increase disk size.<\/p>\n<p><strong>If your question is &quot;what is the difference between SageMaker notebook and Zeppelin notebooks?&quot;<\/strong><\/p>\n<p>The answer is the first one used Jupter (very popular) while the second one uses Zeppelin.<\/p>\n<p><strong>If your question is &quot;what is the difference between using only a SageMaker notebook versus using SM notebook + Glue dev Endpoint?&quot;<\/strong><\/p>\n<p>The answer is: if you are running normal pandas + numpy without using Spark, SM notebook is much cheaper (if you use small instance type and if your data is relatively small). However, if you are trying to process a large dataset and you are planning to use spark, then SM notebook + Glue Dev endpoint will be the best option to develop the job which will be executed later as a Glue Job (transformation job) (server less).<\/p>\n<p>SM notebook is like running python code on an EC2 instance versus SM notebook + Glue which is used to develop ETL jobs which you can launch to process deltas.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2021-09-14 21:46:38.580000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":3.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":71101070,
        "Question_title":"How to properly extract endpoint id from gcp_resources of a Vertex AI pipeline on GCP?",
        "Question_body":"<p>I am using GCP Vertex AI pipeline (KFP) and using <code>google-cloud-aiplatform==1.10.0<\/code>, <code>kfp==1.8.11<\/code>, <code>google-cloud-pipeline-components==0.2.6<\/code>\nIn a component I am getting a gcp_resources <a href=\"https:\/\/github.com\/kubeflow\/pipelines\/blob\/master\/components\/google-cloud\/google_cloud_pipeline_components\/proto\/README.md\" rel=\"nofollow noreferrer\">documentation<\/a> :<\/p>\n<pre><code>gcp_resources (str):\n            Serialized gcp_resources proto tracking the create endpoint's long running operation.\n<\/code><\/pre>\n<p>To extract the endpoint_id to do online prediction of my deployed model, I am doing:<\/p>\n<pre><code>from google_cloud_pipeline_components.proto.gcp_resources_pb2 import GcpResources\nfrom google.protobuf.json_format import Parse\ninput_gcp_resources = Parse(endpoint_ressource_name, GcpResources())\ngcp_resources=input_gcp_resources.resources.__getitem__(0).resource_uri.split('\/')\nendpoint_id=gcp_resources[gcp_resources.index('endpoints')+1]\n<\/code><\/pre>\n<p>Is there a better\/native way of extracting such info ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-02-13 13:26:50.230000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"google-cloud-vertex-ai",
        "Question_view_count":225,
        "Owner_creation_date":"2016-06-06 14:08:12.253000 UTC",
        "Owner_last_access_date":"2022-09-22 14:59:43.617000 UTC",
        "Owner_location":"Z\u00fcrich, Switzerland",
        "Owner_reputation":1414,
        "Owner_up_votes":258,
        "Owner_down_votes":3,
        "Owner_views":478,
        "Answer_body":"<p>In this case is the best way to extract the information. But, I recommend using the <a href=\"http:\/\/ttps:\/\/github.com\/aio-libs\/yarl\" rel=\"nofollow noreferrer\">yarl<\/a> library for complex uri to parse.<\/p>\n<p>You can see this example:<\/p>\n<pre><code>&gt;&gt;&gt; from yarl import URL\n&gt;&gt;&gt; url = URL('https:\/\/www.python.org\/~guido?arg=1#frag')\n&gt;&gt;&gt; url\nURL('https:\/\/www.python.org\/~guido?arg=1#frag')\n<\/code><\/pre>\n<p>All URL parts can be accessed by these properties.<\/p>\n<pre><code>&gt;&gt;&gt; url.scheme\n'https'\n&gt;&gt;&gt; url.host\n'www.python.org'\n&gt;&gt;&gt; url.path\n'\/~guido'\n&gt;&gt;&gt; url.query_string\n'arg=1'\n&gt;&gt;&gt; url.query\n&lt;MultiDictProxy('arg': '1')&gt;\n&gt;&gt;&gt; url.fragment\n'frag'\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-02-14 21:24:49.257000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "google-cloud-vertex-ai"
        ]
    },
    {
        "Question_id":37947524,
        "Question_title":"AzureML: taking parameters as input",
        "Question_body":"<p>I have a code R (manipulateXY.R) taking parameters X (from picklist), Y (a \"not constrained\" value) from a text file (parameter.txt) and producing n images.\nI want to put this code as a \"R script\" in Azure ML, and to produce a web service pointing to that logic (manipulateXY). The question is: how can I pass parameters to the Azure code? I need it because I want a web app with the following outfit<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/dq0Kr.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/dq0Kr.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>such that I choose the X and Y and press \"Run\", it calls the logic in Azure ML, it takes the generated images and put them on the web-app. <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2016-06-21 14:41:20.903000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"azure|web-applications|azure-web-app-service|azure-machine-learning-studio",
        "Question_view_count":91,
        "Owner_creation_date":"2015-07-09 09:05:28.610000 UTC",
        "Owner_last_access_date":"2022-09-19 17:14:25.487000 UTC",
        "Owner_location":"Colleferro, Italy",
        "Owner_reputation":809,
        "Owner_up_votes":109,
        "Owner_down_votes":0,
        "Owner_views":361,
        "Answer_body":"<p>You can use web service parameters as shown here - <a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-web-service-parameters\/\" rel=\"nofollow\">https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-web-service-parameters<\/a>\/<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2016-06-26 17:46:56.443000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":37540703,
        "Question_title":"Test multiple algorithms in one experiment",
        "Question_body":"<p>Is there any way to test multiple algorithms rather than doing it once for each and every algorithm; then checking the result? There are a lot of times where I don\u2019t really know which one to use, so I would like to test multiple and get the result (error rate) fairly quick in Azure Machine Learning Studio.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":2,
        "Question_creation_date":"2016-05-31 08:33:50.827000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2016-06-15 08:05:20.520000 UTC",
        "Question_score":1,
        "Question_tags":"azure|machine-learning|azure-machine-learning-studio",
        "Question_view_count":425,
        "Owner_creation_date":"2016-02-24 10:28:58.853000 UTC",
        "Owner_last_access_date":"2016-08-22 12:20:52.243000 UTC",
        "Owner_location":null,
        "Owner_reputation":39,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Answer_body":"<p>The module you are looking for, is the one called \u201c<strong>Cross-Validate Model<\/strong>\u201d. It basically splits whatever comes in from the input-port (dataset) into 10 pieces, then reserves the last piece as the \u201canswer\u201d; and trains the nine other subset models and returns a set of accuracy statistics measured towards the last subset. What you would look at is the column called \u201cMean absolute error\u201d which is the average error for the trained models. You can connect whatever algorithm you want to one of the ports, and subsequently you will receive the result for that algorithm in particular after you \u201cright-click\u201d the port which gives the score.<\/p>\n\n<p>After that you can assess which algorithm did the best. And as a pro-tip; you could use the <strong>Filter-based-feature selection<\/strong> to actually see which column had a significant impact on the result.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2016-05-31 08:58:13.653000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":67505781,
        "Question_title":"Loading Pretrained Keras to Sagemaker - local classification works but sagemaker classification changes",
        "Question_body":"<p>EDIT: Found a solution, see bottom of post.<\/p>\n<p>I have a pre-trained keras model (model.h5) which is a CNN for image classification. My goal is to deploy the model on sagemaker and use a lambda function to interface with the sagemaker endpoint and make predictions. When I predict with the model on my local machine using the following code, I get results I would expect:<\/p>\n<pre><code>model = load_model(r'model.h5')\nphoto_fp = r'\/path\/to\/photo.jpg'\n\nimg = Image.open(photo_fp).resize((128,128))\nimage_array = np.array(img) \/ 255.\nimg_batch = np.expand_dims(image_array, axis=0)\n\nprint(model.predict(img_batch))\n# [[9.9984562e-01 1.5430539e-04 2.2775747e-14 9.5851349e-16]]\n<\/code><\/pre>\n<p>However, when I deploy the model as an endpoint on sagemaker, I get different results. Below is my code to deploy the model as an endpoint:<\/p>\n<pre><code>model = load_model(r'model.h5')\n\nimport tensorflow as tf\nfrom tensorflow import keras\nimport sagemaker\nimport boto3, re\nfrom sagemaker import get_execution_role\ndef convert_h5_to_aws(loaded_model):\n    # Interpreted from 'Data Liam'\n    from tensorflow.python.saved_model import builder\n    from tensorflow.python.saved_model.signature_def_utils import predict_signature_def\n    from tensorflow.python.saved_model import tag_constants\n    \n    model_version = '1'\n    export_dir = 'export\/Servo\/' + model_version\n    \n    # Build the Protocol Buffer SavedModel at 'export_dir'\n    builder = builder.SavedModelBuilder(export_dir)\n    \n    # Create prediction signature to be used by TensorFlow Serving Predict API\n    signature = predict_signature_def(\n        inputs={&quot;inputs&quot;: loaded_model.input}, outputs={&quot;score&quot;: loaded_model.output})\n\n    with tf.compat.v1.Session() as sess:\n        init = tf.global_variables_initializer()\n        sess.run(init)\n        # Save the meta graph and variables\n        builder.add_meta_graph_and_variables(\n            sess=sess, tags=[tag_constants.SERVING], signature_def_map={&quot;serving_default&quot;: signature})\n        builder.save()\n    \n    #create a tarball\/tar file and zip it\n    import tarfile\n    with tarfile.open('model.tar.gz', mode='w:gz') as archive:\n        archive.add('export', recursive=True)\n        \nconvert_h5_to_aws(model)\n\nsagemaker_session = sagemaker.Session()\ninputs = sagemaker_session.upload_data(path='model.tar.gz', key_prefix='model')\n\n!touch train.py # from notebook\n# the (default) IAM role\nrole = get_execution_role()\nframework_version = tf.__version__\n\n# Create Sagemaker model\nfrom sagemaker.tensorflow.model import TensorFlowModel\nsagemaker_model = TensorFlowModel(model_data = 's3:\/\/' + sagemaker_session.default_bucket() + '\/model\/model.tar.gz',\n                                  role = role,\n                                  framework_version = framework_version,\n                                  entry_point = 'train.py')\n\npredictor = sagemaker_model.deploy(initial_instance_count=1,\n                                   instance_type='ml.m4.xlarge')\n<\/code><\/pre>\n<p>This deploys fine and saves as an endpoint. Then, I invoke the endpoint:<\/p>\n<pre><code>runtime = boto3.client('runtime.sagemaker')\nendpoint_name = 'endpoint-name-for-stackoverflow'\n\nimg = Image.open(photo_fp).resize((128,128))\nimage_array = np.array(img) \/ 255.\nimg_batch = np.expand_dims(image_array, axis=0)\npredictor = TensorFlowPredictor(endpoint_name)\nresult = predictor.predict(data=img_batch)\nprint(result)\n# {'predictions': [[0.199595317, 0.322404563, 0.209394112, 0.268606]]}\n<\/code><\/pre>\n<p>As you can see, the classifier is predicting all of the outputs as nearly equal probabilities, which is not what was predicted on the local machine. This leads me to believe that something is going wrong in my deployment.<\/p>\n<p>I have tried loading the model weights and json model structure to sagemaker rather than the entire h5 model but that yielded the same results. I also used invoke endpoint instead of the predictor API with the following code:<\/p>\n<pre><code>payload = json.dumps(img_batch.tolist())\nresponse = runtime.invoke_endpoint(EndpointName=endpoint_name,\n                                   ContentType='application\/json',\n                                   Body=payload)\nresult = json.loads(response['Body'].read().decode())\nprint(result)\n# {'predictions': [[0.199595317, 0.322404563, 0.209394112, 0.268606]]}\n<\/code><\/pre>\n<p>But yet again, the same results.<\/p>\n<p>Any ideas why I'm getting different results with the sagemaker than on my local machine with the same model?\nThanks!<\/p>\n<p>EDIT: Found a solution. The problem was with the TensorflowModel framework version argument. I changed the framework_version to '1.12' and installed version 1.12 in the Sagemaker Jupyter instance and retrained my model locally using TF 1.12. I'm not totally sure why this works but all of the blogs I found (e.g. <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/deploy-trained-keras-or-tensorflow-models-using-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">this one<\/a>) used 1.12. Hope this helps.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-05-12 14:37:03.953000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-05-21 14:34:44.923000 UTC",
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|tensorflow|keras|amazon-sagemaker",
        "Question_view_count":169,
        "Owner_creation_date":"2020-11-25 03:30:44.957000 UTC",
        "Owner_last_access_date":"2022-09-07 18:33:16.370000 UTC",
        "Owner_location":null,
        "Owner_reputation":59,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Answer_body":"<p>For the benefit of community providing solution in answer section<\/p>\n<blockquote>\n<p>The problem was with the <code>TensorflowModel<\/code> framework version argument. After\nchanging the <code>framework_version<\/code> to <code>1.12<\/code> and installed version <code>TF 1.12<\/code> in\nthe <code>Sagemaker Jupyter<\/code> instance and retrained model locally using <code>TF 1.12<\/code> got same results. (paraphrased from Peter Van Katwyk)<\/p>\n<\/blockquote>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-05-25 07:03:45.083000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":64033258,
        "Question_title":"How to explicitly set sagemaker autopilot's validation set?",
        "Question_body":"<p>The example notebook: <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/autopilot\/autopilot_customer_churn.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/autopilot\/autopilot_customer_churn.ipynb<\/a> states that in the Analyzing Data step:<\/p>\n<p><code>The dataset is analyzed and Autopilot comes up with a list of ML pipelines that should be tried out on the dataset. The dataset is also split into train and validation sets.<\/code><\/p>\n<p>Presumably, autopilot uses this validation set to select the best performing model candidates to return to the user. However, I have not found a way to manually set this validation set used by sagemaker autopilot.<\/p>\n<p>For example, google automl, allows users to add TRAIN, VALIDATE,TEST keywords to a data_split column to manually set which data points are in which set.<\/p>\n<p>Is something like this currently possible which sagemaker autopilot?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-09-23 17:26:01.153000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker|automl",
        "Question_view_count":159,
        "Owner_creation_date":"2014-10-09 02:10:54.710000 UTC",
        "Owner_last_access_date":"2022-09-10 14:48:43.730000 UTC",
        "Owner_location":null,
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":"<p>I'm afraid you can't do this at the moment. The validation set is indeed built by Autopilot itself.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-09-24 07:04:57.000000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":69640534,
        "Question_title":"How to log artifacts in wandb while using saimpletransformers?",
        "Question_body":"<p>I am creating a Question Answering model using <a href=\"https:\/\/simpletransformers.ai\/docs\/qa-specifics\/\" rel=\"nofollow noreferrer\">simpletransformers<\/a>. I would also like to use wandb to track model artifacts. As I understand from <a href=\"https:\/\/docs.wandb.ai\/guides\/integrations\/other\/simpletransformers\" rel=\"nofollow noreferrer\">wandb docs<\/a>, there is an integration touchpoint for simpletransformers but there is no mention of logging artifacts.<\/p>\n<p>I would like to log artifacts generated at the train, validation, and test phase such as train.json, eval.json, test.json, output\/nbest_predictions_test.json and best performing model.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-10-20 04:55:43.473000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"nlp-question-answering|simpletransformers|wandb",
        "Question_view_count":53,
        "Owner_creation_date":"2018-06-12 01:08:24.783000 UTC",
        "Owner_last_access_date":"2021-10-29 05:38:36.997000 UTC",
        "Owner_location":"Ahmedabad, Gujarat, India",
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":"<p>Currently simpleTransformers doesn't support logging artifacts within the training\/testing scripts. But you can do it manually:<\/p>\n<pre><code>import os \n\nwith wandb.init(id=model.wandb_run_id, resume=&quot;allow&quot;, project=wandb_project) as training_run:\n    for dir in sorted(os.listdir(&quot;outputs&quot;)):\n        if &quot;checkpoint&quot; in dir:\n            artifact = wandb.Artifact(&quot;model-checkpoints&quot;, type=&quot;checkpoints&quot;)\n            artifact.add_dir(&quot;outputs&quot; + &quot;\/&quot; + dir)\n            training_run.log_artifact(artifact)\n<\/code><\/pre>\n<p>For more info, you can follow along with the W&amp;B notebook in the SimpleTransofrmer's README.md<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2021-10-20 11:26:44.573000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_valid_tags":[
            "wandb"
        ]
    },
    {
        "Question_id":50418501,
        "Question_title":"Sagemaker model evaluation",
        "Question_body":"<p>The Amazon documentation lists several approaches to evaluate a model (e.g. cross validation, etc.) however these methods does not seem to be available in the Sagemaker Java SDK. \nCurrently if we want to do 5-fold cross validation it seems the only option is to create 5 models (and also deploy 5 endpoints) one model for each subset of data and manually compute the performance metric (recall, precision, etc.). <\/p>\n\n<p>This approach is not very efficient and can also be expensive need to deploy k-endpoints, based on the number of folds in the k-fold validation.<\/p>\n\n<p>Is there another way to test the performance of a model?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2018-05-18 20:01:39.553000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"aws-sdk|aws-java-sdk|amazon-sagemaker",
        "Question_view_count":1442,
        "Owner_creation_date":"2015-04-08 00:54:56.053000 UTC",
        "Owner_last_access_date":"2019-10-03 00:46:15.877000 UTC",
        "Owner_location":null,
        "Owner_reputation":35,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":10,
        "Answer_body":"<p>Amazon SageMaker is a set of multiple components that you can choose which ones to use. <\/p>\n\n<p>The built-in algorithms are designed for (infinite) scale, which means that you can have huge datasets and be able to build a model with them quickly and with low cost. Once you have large datasets you usually don't need to use techniques such as cross-validation, and the recommendation is to have a clear split between training data and validation data. Each of these parts will be defined with an input channel when you are submitting a training job.  <\/p>\n\n<p>If you have a small amount of data and you want to train on all of it and use cross-validation to allow it, you can use a different part of the service (interactive notebook instance). You can bring your own algorithm or even container image to be used in the development, training or hosting. You can have any python code based on any machine learning library or framework, including scikit-learn, R, TensorFlow, MXNet etc. In your code, you can define cross-validation based on the training data that you copy from S3 to the worker instances. <\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_date":"2018-05-19 19:29:38.327000 UTC",
        "Answer_last_edit_date":"2018-05-28 17:13:42.773000 UTC",
        "Answer_score":3.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":73146779,
        "Question_title":"ML Studio language studio failing to detect the source language",
        "Question_body":"<p>I am running a program in python to detect a language and translate that to English using azure machine learning studio. The code block mentioned below throwing error when trying to detect the language.<\/p>\n<blockquote>\n<p>Error 0002: Failed to parse parameter.<\/p>\n<\/blockquote>\n<pre><code>def sample_detect_language():\n    print(\n        &quot;This sample statement will be translated to english from any other foreign language&quot;\n       \n    )\n    \n    from azure.core.credentials import AzureKeyCredential\n    from azure.ai.textanalytics import TextAnalyticsClient\n\n    endpoint = os.environ[&quot;AZURE_LANGUAGE_ENDPOINT&quot;]\n    key = os.environ[&quot;AZURE_LANGUAGE_KEY&quot;]\n\n    text_analytics_client = TextAnalyticsClient(endpoint=endpoint)\n    documents = [\n        &quot;&quot;&quot;\n        The feedback was awesome\n        &quot;&quot;&quot;,\n        &quot;&quot;&quot;\n        la recensione \u00e8 stata fantastica\n        &quot;&quot;&quot;\n    ]\n\n    result = text_analytics_client.detect_language(documents)\n    reviewed_docs = [doc for doc in result if not doc.is_error]\n\n    print(&quot;Check the languages we got review&quot;)\n\n    for idx, doc in enumerate(reviewed_docs):\n        print(&quot;Number#{} is in '{}', which has ISO639-1 name '{}'\\n&quot;.format(\n            idx, doc.primary_language.name, doc.primary_language.iso6391_name\n        ))\n        if doc.is_error:\n            print(doc.id, doc.error)\n    \n    print(\n        &quot;Storing reviews and mapping to their respective ISO639-1 name &quot;\n        \n    )\n\n    review_to_language = {}\n    for idx, doc in enumerate(reviewed_docs):\n        review_to_language[documents[idx]] = doc.primary_language.iso6391_name\n\n\nif __name__ == '__main__':\n    sample_detect_language()\n<\/code><\/pre>\n<p>Any help to solve the issue is appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-07-28 03:02:43.490000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"azure|azure-machine-learning-studio",
        "Question_view_count":50,
        "Owner_creation_date":"2022-05-09 19:08:30.643000 UTC",
        "Owner_last_access_date":"2022-09-16 03:29:45.173000 UTC",
        "Owner_location":null,
        "Owner_reputation":31,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":41,
        "Answer_body":"<p>The issue was raised because of missing the called parameters in the function. While doing language detection in machine learning studio, we need to assign end point and key credentials. In the code mentioned above, endpoint details were mentioned, but missed <strong>AzureKeyCredential.<\/strong><\/p>\n<pre><code>endpoint = os.environ[&quot;AZURE_LANGUAGE_ENDPOINT&quot;]\nkey = os.environ[&quot;AZURE_LANGUAGE_KEY&quot;]\ntext_analytics_client = TextAnalyticsClient(endpoint=endpoint)\n<\/code><\/pre>\n<p>replace the above line with the code block mentioned below<\/p>\n<pre><code>text_analytics_client = TextAnalyticsClient(endpoint=endpoint, credential= AzureKeyCredential(key))\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2022-07-28 11:21:24.270000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":73042521,
        "Question_title":"How to use scikit learn model from inside sagemaker 'model.tar.gz' file?",
        "Question_body":"<p>New to Sagemaker..<\/p>\n<p>Trained a &quot;linear-learner&quot; classification model using the Sagemaker API, and it saved a &quot;model.tar.gz&quot; file in my s3 path. From what I understand SM just used an image of a scikit logreg model.<\/p>\n<p>Finally, I'd like to gain access to the model object itself, so I unpacked the &quot;model.tar.gz&quot; file only to find another file called &quot;model_algo-1&quot; with no extension.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/DPUMO.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/DPUMO.png\" alt=\"contents of unknown file\" \/><\/a><\/p>\n<p>Can anyone tell me how I can find the &quot;real&quot; modeling object without using the inference\/Endpoint delpoy API provided by Sagemaker? There are some things I want to look at manually.<\/p>\n<p>Thanks,\nCraig<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-07-19 19:28:20.593000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|amazon-sagemaker",
        "Question_view_count":42,
        "Owner_creation_date":"2016-09-22 14:43:34.617000 UTC",
        "Owner_last_access_date":"2022-09-22 14:50:58.827000 UTC",
        "Owner_location":null,
        "Owner_reputation":129,
        "Owner_up_votes":7,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Answer_body":"<p>Linear-Learner is a built in algorithm written using MX-net and the binary is also MXNET compatible. You can't use this model outside of SageMaker as there is no open source implementation for this.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-07-21 23:57:06.900000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":57526707,
        "Question_title":"How to parallelize work on an Azure ML Service Compute cluster?",
        "Question_body":"<p>I am able to submit jobs to Azure ML services using a compute cluster. It works well, and the autoscaling combined with good flexibility for custom environments seems to be exactly what I need. However, so far all these jobs seem to only use one compute node of the cluster. Ideally I would like to use multiple nodes for a computation, but all methods that I see rely on rather deep integration with azure ML services.<\/p>\n\n<p>My modelling case is a bit atypical. From previous experiments I identified a group of architectures (pipelines of preprocessing steps + estimators in Scikit-learn) that worked well. \nHyperparameter tuning for one of these estimators can be performed reasonably fast (couple of minutes) with <a href=\"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.RandomizedSearchCV.html#sklearn.model_selection.RandomizedSearchCV\" rel=\"nofollow noreferrer\">RandomizedSearchCV<\/a>. So it seems less effective to parallelize this step.<\/p>\n\n<p>Now I want to tune and train this entire list of architectures.\nThis should be very easily to parallelize since all architectures can be trained independently. <\/p>\n\n<p>Ideally I would like something like (in pseudocode)<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>tuned = AzurePool.map(tune_model, [model1, model2,...])\n<\/code><\/pre>\n\n<p>However, I could not find any resources on how I could achieve this with an Azure ML Compute cluster.\nAn acceptable alternative would come in the form of a plug-and-play substitute for sklearn's CV-tuning methods, similar to the ones provided in <a href=\"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.RandomizedSearchCV.html#sklearn.model_selection.RandomizedSearchCV\" rel=\"nofollow noreferrer\">dask<\/a> or <a href=\"https:\/\/databricks.github.io\/spark-sklearn-docs\/#spark_sklearn.GridSearchCV\" rel=\"nofollow noreferrer\">spark<\/a>.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2019-08-16 14:48:33.733000 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":"2019-08-16 20:32:31.453000 UTC",
        "Question_score":3,
        "Question_tags":"python|azure|scikit-learn|cluster-computing|azure-machine-learning-service",
        "Question_view_count":818,
        "Owner_creation_date":"2018-05-01 15:15:47.290000 UTC",
        "Owner_last_access_date":"2022-09-23 13:50:22.170000 UTC",
        "Owner_location":"Belgium",
        "Owner_reputation":1466,
        "Owner_up_votes":361,
        "Owner_down_votes":11,
        "Owner_views":118,
        "Answer_body":"<p>There are a number of ways you could tackle this with AzureML. The simplest would be to just launch a number of jobs using the AzureML Python SDK (the underlying example is taken from <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/4170a394edd36413edebdbab347afb0d833c94ee\/how-to-use-azureml\/training\/train-hyperparameter-tune-deploy-with-sklearn\/train-hyperparameter-tune-deploy-with-sklearn.ipynb\" rel=\"nofollow noreferrer\">here<\/a>)<\/p>\n\n<pre><code>from azureml.train.sklearn import SKLearn\n\nruns = []\n\nfor kernel in ['linear', 'rbf', 'poly', 'sigmoid']:\n    for penalty in [0.5, 1, 1.5]:\n        print ('submitting run for kernel', kernel, 'penalty', penalty)\n        script_params = {\n            '--kernel': kernel,\n            '--penalty': penalty,\n        }\n\n        estimator = SKLearn(source_directory=project_folder, \n                            script_params=script_params,\n                            compute_target=compute_target,\n                            entry_script='train_iris.py',\n                            pip_packages=['joblib==0.13.2'])\n\n        runs.append(experiment.submit(estimator))\n<\/code><\/pre>\n\n<p>The above requires you to factor your training out into a script (or a set of scripts in a folder) along with the python packages required. The above estimator is a convenience wrapper for using Scikit Learn. There are also estimators for Tensorflow, Pytorch, Chainer and a generic one (<code>azureml.train.estimator.Estimator<\/code>) -- they all differ in the Python packages and base docker they use.<\/p>\n\n<p>A second option, if you are actually tuning parameters, is to use the HyperDrive service like so (using the same <code>SKLearn<\/code> Estimator as above):<\/p>\n\n<pre><code>from azureml.train.sklearn import SKLearn\nfrom azureml.train.hyperdrive.runconfig import HyperDriveConfig\nfrom azureml.train.hyperdrive.sampling import RandomParameterSampling\nfrom azureml.train.hyperdrive.run import PrimaryMetricGoal\nfrom azureml.train.hyperdrive.parameter_expressions import choice\n\nestimator = SKLearn(source_directory=project_folder, \n                    script_params=script_params,\n                    compute_target=compute_target,\n                    entry_script='train_iris.py',\n                    pip_packages=['joblib==0.13.2'])\n\nparam_sampling = RandomParameterSampling( {\n    \"--kernel\": choice('linear', 'rbf', 'poly', 'sigmoid'),\n    \"--penalty\": choice(0.5, 1, 1.5)\n    }\n)\n\nhyperdrive_run_config = HyperDriveConfig(estimator=estimator,\n                                         hyperparameter_sampling=param_sampling, \n                                         primary_metric_name='Accuracy',\n                                         primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\n                                         max_total_runs=12,\n                                         max_concurrent_runs=4)\n\nhyperdrive_run = experiment.submit(hyperdrive_run_config)\n<\/code><\/pre>\n\n<p>Or you could use DASK to schedule the work as you were mentioning. Here is a sample of how to set up DASK on and AzureML Compute Cluster so you can do interactive work on it: <a href=\"https:\/\/github.com\/danielsc\/azureml-and-dask\" rel=\"nofollow noreferrer\">https:\/\/github.com\/danielsc\/azureml-and-dask<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2019-08-16 21:18:45.970000 UTC",
        "Answer_last_edit_date":"2019-09-05 07:54:33.437000 UTC",
        "Answer_score":2.0,
        "Question_valid_tags":[
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":55854377,
        "Question_title":"comprehend.start_topics_detection_job Fails with Silent Error?",
        "Question_body":"<p>I have <a href=\"https:\/\/docs.aws.amazon.com\/code-samples\/latest\/catalog\/python-comprehend-TopicModeling.py.html\" rel=\"nofollow noreferrer\">Amazon sample code<\/a> for running <code>comprehend.start_topics_detection_job<\/code>. Here is the code with the variables filled in for my job:<\/p>\n\n<pre><code>import re\nimport csv\nimport pytz\nimport boto3\nimport json\n\n# https:\/\/docs.aws.amazon.com\/code-samples\/latest\/catalog\/python-comprehend-TopicModeling.py.html\n# https:\/\/docs.aws.amazon.com\/comprehend\/latest\/dg\/API_InputDataConfig.html\n\n# Set these values before running the program\ninput_s3_url = \"s3:\/\/comprehend-topic-modelling-bucket\/input_800_cleaned_articles\/\"\ninput_doc_format = \"ONE_DOC_PER_LINE\"\noutput_s3_url = \"s3:\/\/comprehend-topic-modelling-bucket\/output\"\ndata_access_role_arn = \"arn:aws:iam::372656143103:role\/access-aws-services-from-sagemaker\"\nnumber_of_topics = 30\n\n# Set up job configuration\ninput_data_config = {\"S3Uri\": input_s3_url, \"InputFormat\": input_doc_format}\noutput_data_config = {\"S3Uri\": output_s3_url}\n\n# Begin a job to detect the topics in the document collection\ncomprehend = boto3.client('comprehend')\nstart_result = comprehend.start_topics_detection_job(\n    NumberOfTopics=number_of_topics,\n    InputDataConfig=input_data_config,\n    OutputDataConfig=output_data_config,\n    DataAccessRoleArn=data_access_role_arn)\n\n# Output the results\nprint('Start Topic Detection Job: ' + json.dumps(start_result))\njob_id = start_result['JobId']\nprint(f'job_id: {job_id}')\n\n# Retrieve and output information about the job\ndescribe_result = comprehend.describe_topics_detection_job(JobId=job_id)\nprint('Describe Job: ' + json.dumps(describe_result)) . #&lt;===LINE 36\n\n# List and output information about current jobs\nlist_result = comprehend.list_topics_detection_jobs()\nprint('list_topics_detection_jobs_result: ' + json.dumps(list_result))\n<\/code><\/pre>\n\n<p>It's failing with the error:<\/p>\n\n<pre><code>---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n&lt;ipython-input-8-840a7ee043d4&gt; in &lt;module&gt;()\n     34 # Retrieve and output information about the job\n     35 describe_result = comprehend.describe_topics_detection_job(JobId=job_id)\n---&gt; 36 print('Describe Job: ' + json.dumps(describe_result))\n     37 \n     38 # List and output information about current jobs\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/json\/__init__.py in dumps(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\n    229         cls is None and indent is None and separators is None and\n    230         default is None and not sort_keys and not kw):\n--&gt; 231         return _default_encoder.encode(obj)\n    232     if cls is None:\n    233         cls = JSONEncoder\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/json\/encoder.py in encode(self, o)\n    197         # exceptions aren't as detailed.  The list call should be roughly\n    198         # equivalent to the PySequence_Fast that ''.join() would do.\n--&gt; 199         chunks = self.iterencode(o, _one_shot=True)\n    200         if not isinstance(chunks, (list, tuple)):\n    201             chunks = list(chunks)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/json\/encoder.py in iterencode(self, o, _one_shot)\n    255                 self.key_separator, self.item_separator, self.sort_keys,\n    256                 self.skipkeys, _one_shot)\n--&gt; 257         return _iterencode(o, 0)\n    258 \n    259 def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/json\/encoder.py in default(self, o)\n    178         \"\"\"\n    179         raise TypeError(\"Object of type '%s' is not JSON serializable\" %\n--&gt; 180                         o.__class__.__name__)\n    181 \n    182     def encode(self, o):\n\nTypeError: Object of type 'datetime' is not JSON serializable\n<\/code><\/pre>\n\n<p>It fails instantly, the second I pus \"run\". It seems to me that the call to <code>comprehend.start_topics_detection_job<\/code> may be failing, leading to an error line 36, <code>print('Describe Job: ' + json.dumps(describe_result))<\/code>.<\/p>\n\n<p>What am I missing?<\/p>\n\n<p><strong>UPDATE<\/strong><\/p>\n\n<p>The same IAM role is being used for the notebook, as well as in the above code. Here are the permissions currently assigned to that IAM role:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/6ihIr.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/6ihIr.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_date":"2019-04-25 17:03:45.007000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-04-07 05:03:44.610000 UTC",
        "Question_score":0,
        "Question_tags":"python|django|machine-learning|amazon-sagemaker|amazon-comprehend",
        "Question_view_count":232,
        "Owner_creation_date":"2010-06-11 22:17:02.427000 UTC",
        "Owner_last_access_date":"2022-09-24 23:17:42.980000 UTC",
        "Owner_location":null,
        "Owner_reputation":4334,
        "Owner_up_votes":409,
        "Owner_down_votes":1,
        "Owner_views":496,
        "Answer_body":"<p>It turns out that there was nothing wrong with the call to <code>comprehend.describe_topics_detection_job<\/code> -- it was just returning, in <code>describe_result<\/code>, something that could not be json serialized, so <code>json.dumps(describe_result))<\/code> was throwing an error. <\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2019-05-01 08:07:25.340000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":68143997,
        "Question_title":"Automate Docker Run command on Sagemaker's Notebook Instance",
        "Question_body":"<p>I have a Docker image in AWS ECR and I open my Sagemaker Notebook instance---&gt;go to terminal--&gt;docker run....\nThis is how I start my Docker container.<\/p>\n<p>Now, I want to automate this process(running my docker image on Sagemaker Notebook Instance) instead of typing the docker run commands.<\/p>\n<p>Can I create a cron job on Sagemaker? or Is there any other approach?<\/p>\n<p>Thanks<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2021-06-26 15:48:40.977000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|docker|containers|amazon-sagemaker",
        "Question_view_count":393,
        "Owner_creation_date":"2017-06-16 14:03:57.833000 UTC",
        "Owner_last_access_date":"2022-09-20 06:42:37.060000 UTC",
        "Owner_location":"Bangalore, Karnataka, India",
        "Owner_reputation":230,
        "Owner_up_votes":11,
        "Owner_down_votes":4,
        "Owner_views":27,
        "Answer_body":"<p>For this you can create an inline Bash shell in your SageMaker notebook as follows. This will take your Docker container, create the image, ECR repo if it does not exist and push the image.<\/p>\n<pre><code>%%sh\n\n# Name of algo -&gt; ECR\nalgorithm_name=your-algo-name\n\ncd container #your directory with dockerfile and other sm components\n\nchmod +x randomForest-Petrol\/train #train file for container\nchmod +x randomForest-Petrol\/serve #serve file for container\n\naccount=$(aws sts get-caller-identity --query Account --output text)\n\n# Region, defaults to us-west-2\nregion=$(aws configure get region)\nregion=${region:-us-west-2}\n\nfullname=&quot;${account}.dkr.ecr.${region}.amazonaws.com\/${algorithm_name}:latest&quot;\n\n# If the repository doesn't exist in ECR, create it.\naws ecr describe-repositories --repository-names &quot;${algorithm_name}&quot; &gt; \/dev\/null 2&gt;&amp;1\n\nif [ $? -ne 0 ]\nthen\n    aws ecr create-repository --repository-name &quot;${algorithm_name}&quot; &gt; \/dev\/null\nfi\n\n# Get the login command from ECR and execute it directly\naws ecr get-login-password --region ${region}|docker login --username AWS --password-stdin ${fullname}\n\n# Build the docker image locally with the image name and then push it to ECR\n# with the full name.\n\ndocker build  -t ${algorithm_name} .\ndocker tag ${algorithm_name} ${fullname}\n\ndocker push ${fullname}\n<\/code><\/pre>\n<p>I am contributing this on behalf of my employer, AWS. My contribution is licensed under the MIT license. See here for a more detailed explanation\n<a href=\"https:\/\/aws-preview.aka.amazon.com\/tools\/stackoverflow-samples-license\/\" rel=\"nofollow noreferrer\">https:\/\/aws-preview.aka.amazon.com\/tools\/stackoverflow-samples-license\/<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-07-22 18:10:03.510000 UTC",
        "Answer_last_edit_date":"2021-07-22 18:41:04.577000 UTC",
        "Answer_score":0.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":60334889,
        "Question_title":"\"No Kernel!\" error Azure ML compute JupyterLab",
        "Question_body":"<p>When using the JupyterLab found within the azure ML compute instance, every now and then, I run into an issue where it will say that network connection is lost. <\/p>\n\n<p>I have confirmed that the computer is still running.\nthe notebook itself can be edited and saved, so the computer\/VM is definitely running\nOf course, the internet is fully functional<\/p>\n\n<p>On the top right corner <em>next to the now blank circle<\/em> it will say \"No Kernel!\"<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_date":"2020-02-21 08:39:54.260000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"azure|azure-machine-learning-studio|azure-machine-learning-service|azure-machine-learning-workbench",
        "Question_view_count":1026,
        "Owner_creation_date":"2015-09-15 16:27:17.953000 UTC",
        "Owner_last_access_date":"2022-09-24 06:49:58.907000 UTC",
        "Owner_location":"Bangalore, Karnataka, India",
        "Owner_reputation":2272,
        "Owner_up_votes":1340,
        "Owner_down_votes":67,
        "Owner_views":516,
        "Answer_body":"<p>We can't repro the issue, can you help gives us more details? One possibility is that the kernel has bugs and hangs (could be due to extensions, widgets installed) or the resources on the machine are exhausted and kernel dies. What VM type are you using? If it's a small VM you may ran out of resources.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2020-02-21 22:12:37.643000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "azure-machine-learning-studio",
            "azure-machine-learning-workbench",
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":73200116,
        "Question_title":"AWS Sagemaker integration with mongodb and lambda",
        "Question_body":"<p>I'm looking for some advice from anyone who's tried aws Sagemaker. I'm very new to this and would appreciate anyone kind enough to help me out.<\/p>\n<p>I have created a basic time series project in a Sagemaker notebook. It trains the model on CSV file data and tests it, with good results.<\/p>\n<p>The data I am using is based on store profits. I am predicting the profit each week.<\/p>\n<p>However, my question is, how can I pass new store sales data into this model each week (only one day a week), and retrain it with the new weeks data (so it can notice any new patterns), then for it to predict the next week profit for each store?<\/p>\n<p>All my store data is synced into mongodb, so I'm presuming I would need a lambda function to get this data and pass it over to the Sagemaker model.<\/p>\n<p>Is it worth retraining the model every week? As I have years worth of store data? Or should I just pass over the old data with the new data added in for it to predict? How do I pass over this data? In a lambda function with a cloud event to make it run automatically every week?<\/p>\n<p>Can I write the predictions back into mongodb in a new table, or are they saved somewhere else first and this would have to be another lambda function?<\/p>\n<p>I have looked at so many tutorials, but none of them seem to explain how I can connect everything up and have the model make predictions automatically and then save them in a dB.<\/p>\n<p>Many thanks in advance to anyone who can explain this to me! Sorry for such a long question!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2022-08-01 22:17:07.390000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"mongodb|amazon-web-services|aws-lambda|time-series|amazon-sagemaker",
        "Question_view_count":58,
        "Owner_creation_date":"2022-05-31 07:56:46.887000 UTC",
        "Owner_last_access_date":"2022-09-21 19:38:27.443000 UTC",
        "Owner_location":null,
        "Owner_reputation":67,
        "Owner_up_votes":13,
        "Owner_down_votes":0,
        "Owner_views":24,
        "Answer_body":"<p>Recently I have completed similar use case and here's my answers -<\/p>\n<p><strong>Q1  : Is there need of retraining every week?<\/strong><\/p>\n<p>Ans : Yes, you need to do continuous training and continuous forecasting steps (tie using sagemaker pipeline) in prod to make it work perfectly automated for stable MAE, MAPE etc.<\/p>\n<p><strong>Q2  : How can I pass new data and forecast for next week? How to get input data from mongodb?<\/strong><\/p>\n<p>Ans : You could use Lambda, or Glue job (designed for ETL so better) to drop in S3 bucket. This will could become input raw data bucket for sagemaker pipeline.<\/p>\n<p><strong>Q3  : Can I write the predictions back into mongodb in a new table, or are they saved somewhere else first and this would have to be another lambda function?<\/strong><\/p>\n<p>Ans : Yes you can, both ways.<\/p>\n<p>I would suggest to start small i.e. First drop a csv file to s3 location in say YYMMDD folder. Use this as input and develop completely in one notebook (continuous train, continuous forecast).<\/p>\n<p>Later, learn about pipelines - how to write different steps, pass objects between steps etc and go modify your code to fit in pipeline.<\/p>\n<p>Create a sagemaker pipeline with steps : (Refer links below )<\/p>\n<ol>\n<li>Preprocess ( any transformations, cleansing )<\/li>\n<li>Training ( use prebuilt image or need to build one, depends)<\/li>\n<li>Forecast ( either do batch transform or deploy to endpoint and later delete )<\/li>\n<li>Post processing ( if required )<\/li>\n<\/ol>\n<p>Take the output the from sagemaker pipelines to mongodb. <strong>Sagemaker Pipelines help automate scheduled execution using AWS Event Bridge<\/strong><\/p>\n<p>Some references :<\/p>\n<p><a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/sagemaker-pipelines\/tabular\/abalone_build_train_deploy\/sagemaker-pipelines-preprocess-train-evaluate-batch-transform.ipynb\" rel=\"nofollow noreferrer\">Example Pipelines<\/a> Look here in to know about pipelines<\/p>\n<p><a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-forecasting-air-pollution-with-deepar\/blob\/5e29057a8c9d7f8db6cb1143f7838b6614f44ef1\/01_train_and_evaluate_air_quality_deepar_model.ipynb\" rel=\"nofollow noreferrer\">Example1 DeepAR<\/a><\/p>\n<p><a href=\"https:\/\/sagemaker-examples.readthedocs.io\/en\/latest\/introduction_to_amazon_algorithms\/deepar_electricity\/DeepAR-Electricity.html?highlight=deepar\" rel=\"nofollow noreferrer\">Example2 DeepAR<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-08-03 11:36:34.223000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":36967126,
        "Question_title":"Why do I get good accuracy with IRIS dataset with a single hidden node?",
        "Question_body":"<p>I have a minimal example of a neural network with a back-propagation trainer, testing it on the IRIS data set. I started of with 7 hidden nodes and it worked well.<\/p>\n\n<p>I lowered the number of nodes in the hidden layer to 1 (expecting it to fail), but was surprised to see that the accuracy went up.<\/p>\n\n<p>I set up the experiment in azure ml, just to validate that it wasn't my code. Same thing there, 98.3333% accuracy with a single hidden node.<\/p>\n\n<p>Can anyone explain to me what is happening here?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2016-05-01 13:18:34.660000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":4,
        "Question_tags":"machine-learning|neural-network|backpropagation|azure-machine-learning-studio",
        "Question_view_count":4488,
        "Owner_creation_date":"2015-03-16 10:34:07.333000 UTC",
        "Owner_last_access_date":"2022-09-23 14:45:47.530000 UTC",
        "Owner_location":null,
        "Owner_reputation":825,
        "Owner_up_votes":46,
        "Owner_down_votes":0,
        "Owner_views":82,
        "Answer_body":"<p>First, it has been well established that a variety of classification models yield incredibly good results on Iris (Iris is very predictable); see <a href=\"http:\/\/lab.fs.uni-lj.si\/lasin\/wp\/IMIT_files\/neural\/doc\/seminar8.pdf\" rel=\"noreferrer\">here<\/a>, for example.<\/p>\n\n<p>Secondly, we can observe that there are relatively few features in the Iris dataset. Moreover, if you look at the <a href=\"https:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/iris\/iris.names\" rel=\"noreferrer\">dataset description<\/a> you can see that two of the features are very highly correlated with the class outcomes.<\/p>\n\n<p>These correlation values are linear, single-feature correlations, which indicates that one can most likely apply a linear model and observe good results. Neural nets are highly nonlinear; they become more and more complex and capture greater and greater nonlinear feature combinations as the number of hidden nodes and hidden layers is increased.<\/p>\n\n<p>Taking these facts into account, that (a) there are few features to begin with and (b) that there are high linear correlations with class, would all point to a less complex, linear function as being the appropriate predictive model-- by using a single hidden node, you are very nearly using a linear model.<\/p>\n\n<p>It can also be noted that, in the absence of any hidden layer (i.e., just input and output nodes), and when the logistic transfer function is used, this is equivalent to logistic regression.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2016-05-01 16:22:44.790000 UTC",
        "Answer_last_edit_date":"2016-05-01 23:14:29.997000 UTC",
        "Answer_score":6.0,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":70486162,
        "Question_title":"Conflicting Python versions in SageMaker Studio notebook with Python 3.8 kernel",
        "Question_body":"<p>I'm trying to run a SageMaker kernel with Python 3.8 in SageMaker Studio, and the notebook appears to use a separate distribution of Python 3.7. The <em>running app<\/em> is indicated as <em>tensorflow-2.6-cpu-py38-ubuntu20.04-v1<\/em>. When I run <code>!python3 -V<\/code> I get <em>Python 3.8.2<\/em>. However, the Python instance inside the notebook is different:<\/p>\n<pre><code>import sys\nsys.version\n<\/code><\/pre>\n<p>gives <code>'3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21) \\n[GCC 9.4.0]'<\/code><\/p>\n<p>Similarly, running <code>%pip -V<\/code> and <code>%conda info<\/code> indicates Python 3.7.<\/p>\n<p>Also, <code>import tensorflow<\/code> fails, as it isn't preinstalled in the Python environment that the notebook invokes.<\/p>\n<p>I'm running in the <em>eu-west-2<\/em> region. Is there anything I can do to address this short of opening a support ticket?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2021-12-26 11:42:01.197000 UTC",
        "Question_favorite_count":3.0,
        "Question_last_edit_date":null,
        "Question_score":5,
        "Question_tags":"python-3.x|amazon-web-services|jupyter-notebook|amazon-sagemaker|anaconda3",
        "Question_view_count":768,
        "Owner_creation_date":"2012-03-14 12:18:03.733000 UTC",
        "Owner_last_access_date":"2022-09-23 12:09:26.313000 UTC",
        "Owner_location":"London, UK",
        "Owner_reputation":1060,
        "Owner_up_votes":102,
        "Owner_down_votes":0,
        "Owner_views":139,
        "Answer_body":"<p>are you still facing this issue?<\/p>\n<p>I am in eu-west-2 using a SageMaker Studio notebook and the TensorFlow 2.6 Python 3.8 CPU Optimized image (running app is tensorflow-2.6-cpu-py38-ubuntu20.04-v1).<\/p>\n<p>When I run the below commands, I get the right outputs.<\/p>\n<pre><code>!python3 -V\n<\/code><\/pre>\n<p>returns Python 3.8.2<\/p>\n<pre><code>import sys\nsys.version \n<\/code><\/pre>\n<p>returns\n3.8.2 (default, Dec  9 2021, 06:26:16) \\n[GCC 9.3.0]'<\/p>\n<pre><code>import tensorflow as tf\nprint(tf.__version__)\n<\/code><\/pre>\n<p>returns 2.6.2<\/p>\n<p>It seems this has now been fixed<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-02-25 13:00:46.503000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":50414639,
        "Question_title":"How to Publish an Azure Bot",
        "Question_body":"<p>Just learning how to use <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/bot-service\/?view=azure-bot-service-3.0\" rel=\"nofollow noreferrer\">Azure Bot Service<\/a> and <code>Azure Bot Framework<\/code>. I created a Bot in Azure portal following <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/bot-service\/bot-service-quickstart?view=azure-bot-service-3.0\" rel=\"nofollow noreferrer\">this<\/a> Official Azure tutorial. Does this bot need to be published somewhere? I read somewhere that you <code>Build--&gt;Test--&gt;Publish--&gt;Evaluate<\/code>. I've tested it in Azure portal itself as explained <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/bot-service\/bot-service-quickstart?view=azure-bot-service-3.0\" rel=\"nofollow noreferrer\">here<\/a>. Not sure about the Publish part of it.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_date":"2018-05-18 15:25:22.620000 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":3,
        "Question_tags":"azure|botframework|azure-machine-learning-studio|azure-bot-service",
        "Question_view_count":844,
        "Owner_creation_date":"2012-02-25 04:28:19.340000 UTC",
        "Owner_last_access_date":"2022-09-24 17:06:32.277000 UTC",
        "Owner_location":null,
        "Owner_reputation":19815,
        "Owner_up_votes":2703,
        "Owner_down_votes":22,
        "Owner_views":2272,
        "Answer_body":"<p>How do you intend to use your bot? Azure Bots work by connecting them to existing channels like Skype, Facebook Messenger, SMS, etc or making REST calls from a custom application.<\/p>\n\n<p>However you can also reach your bot directly from: <code>https:\/\/webchat.botframework.com\/embed\/YOUR_BOT_ID?t=YOUR_TOKEN_HERE<\/code><\/p>\n\n<p>You can embed it on any web page with this HTML tag:<\/p>\n\n<pre><code>&lt;iframe src=\"https:\/\/webchat.botframework.com\/embed\/YOUR_BOT_ID?t=YOUR_TOKEN_HERE\"&gt;&lt;\/iframe&gt;\n<\/code><\/pre>\n\n<p>Please note that both of these methods expose your token and would allow other developers to add your bot to their pages as well.<\/p>\n\n<p>Bot ID is the name of your bot and you can get the token from the portal by going to your bot and choosing \"Channel\" blade and then clicking the \"Get bot embed codes\" link.<\/p>\n\n<p>Edit: I went ahead and wrote a blog post on this topic <a href=\"https:\/\/medium.com\/@joelatwar\/how-to-embed-your-azure-web-app-bot-in-any-web-page-120dfda91fdc\" rel=\"nofollow noreferrer\">https:\/\/medium.com\/@joelatwar\/how-to-embed-your-azure-web-app-bot-in-any-web-page-120dfda91fdc<\/a><\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2018-05-18 16:11:16.240000 UTC",
        "Answer_last_edit_date":"2018-05-25 21:12:34.360000 UTC",
        "Answer_score":5.0,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":65464181,
        "Question_title":"An alternative to tf.distribute.cluster_resolver.TPUClusterResolver( tpu_name) to be used in Sagemaker?",
        "Question_body":"<ol>\n<li><p>task : object_detection<\/p>\n<\/li>\n<li><p>environment: AWS sagemaker<\/p>\n<\/li>\n<li><p>instance type: 'ml.p2.xlarge' | num_instances = 1<\/p>\n<\/li>\n<li><p>Main file to be run: <a href=\"https:\/\/github.com\/tensorflow\/models\/blob\/master\/research\/object_detection\/model_main_tf2.py\" rel=\"nofollow noreferrer\">original<\/a><\/p>\n<\/li>\n<li><p>Problematic code segment from the main file:<\/p>\n<pre><code>    resolver = tf.distribute.cluster_resolver.TPUClusterResolver(\n    FLAGS.tpu_name)\n    tf.config.experimental_connect_to_cluster(resolver)\n    tf.tpu.experimental.initialize_tpu_system(resolver)\n    strategy = tf.distribute.experimental.TPUStrategy(resolver)\n    elif FLAGS.num_workers &gt; 1:\n        strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\n    else:\n        strategy = tf.compat.v2.distribute.MirroredStrategy()\n<\/code><\/pre>\n<\/li>\n<li><p>Problem : Can't find the proper value to be given as <code>tpu_name<\/code> argument.<\/p>\n<\/li>\n<li><p>My research on the problem:<\/p>\n<\/li>\n<\/ol>\n<p>According to the tensorflow documentation in <a href=\"https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/distribute\/cluster_resolver\/TPUClusterResolver\" rel=\"nofollow noreferrer\">tf.distribute.cluster_resolver.TPUClusterResolver<\/a>, it says that this resolver works only on Google Cloud platform.<\/p>\n<blockquote>\n<p>This is an implementation of cluster resolvers for the Google Cloud\nTPU service.<\/p>\n<p>TPUClusterResolver supports the following distinct environments:\nGoogle Compute Engine Google Kubernetes Engine Google internal<\/p>\n<p>It can be passed into tf.distribute.TPUStrategy to support TF2\ntraining on Cloud TPUs.<\/p>\n<\/blockquote>\n<p>But from <a href=\"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/39721\" rel=\"nofollow noreferrer\">this issue in github<\/a>, I found out that a similar code also works in Azure.<\/p>\n<ol start=\"8\">\n<li>My question :<\/li>\n<\/ol>\n<p>Is there a way I can bypass this resolver and initialize my tpu in <strong>sagemaker<\/strong> ?<\/p>\n<p>Even better, if I can find a way to insert the name or url of sagemaker gpu to the resolver and initiate it from there ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-12-27 08:55:38.347000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-12-27 10:12:35.520000 UTC",
        "Question_score":0,
        "Question_tags":"python|tensorflow|gpu|amazon-sagemaker|tpu",
        "Question_view_count":355,
        "Owner_creation_date":"2018-01-28 13:47:46.417000 UTC",
        "Owner_last_access_date":"2022-09-23 19:02:42.520000 UTC",
        "Owner_location":null,
        "Owner_reputation":65,
        "Owner_up_votes":89,
        "Owner_down_votes":0,
        "Owner_views":18,
        "Answer_body":"<p>Let me clarify some confusion here. TPUs are only offered on Google Cloud and the <code>TPUClusterResolver<\/code> implementation queries GCP APIs to get the cluster config for the TPU node. Thus, no you can't use <code>TPUClusterResolver<\/code> with AWS sagemaker, but you should try it out with TPUs on GCP instead or try find some other documentation on Sagemaker's end on how they enable cluster resolving on their end (if they do).<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-02-22 15:24:19.500000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":50078161,
        "Question_title":"Azure ML Prediction Is Constant",
        "Question_body":"<p>I am using the Azure ML model available at <a href=\"https:\/\/gallery.azure.ai\/Experiment\/Weather-prediction-model-1\" rel=\"nofollow noreferrer\">https:\/\/gallery.azure.ai\/Experiment\/Weather-prediction-model-1<\/a> to design a prediction mechanism based on temperature and humidity. I haven't done any changes to the existing model and feeding in data from a simulator. The prediction output is stuck at 0.489944100379944. I have taken over 17k samples and still, the prediction is constant at this value. <\/p>\n\n<p>Any help will be highly appreciated.<\/p>\n\n<p><em>N.B. - This is my first stint with ML<\/em><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":10,
        "Question_creation_date":"2018-04-28 15:32:06.273000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"azure|machine-learning|data-science|azure-machine-learning-studio|machine-learning-model",
        "Question_view_count":163,
        "Owner_creation_date":"2012-05-30 13:51:11.600000 UTC",
        "Owner_last_access_date":"2022-09-24 15:08:32.660000 UTC",
        "Owner_location":null,
        "Owner_reputation":118,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":52,
        "Answer_body":"<p>This was caused by the training dataset. The dataset had characters in the humidity and temperature columns. This led to the model expecting characters but operating on floating point numbers. I cleaned the dataset and ensured that there are only floats in the temperature and humidity columns. Then I used this training data for the model and phew!!!! Everything's working now. <\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2018-04-30 17:04:20.377000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":62889537,
        "Question_title":"TensorFlow Serving vs. TensorFlow Inference (container type for SageMaker model)",
        "Question_body":"<p>I am fairly new to TensorFlow (and SageMaker) and am stuck in the process of deploying a SageMaker endpoint. I have just recently succeeded in creating a Saved Model type model, which is currently being used to service a sample endpoint (the model was created externally). However, when I checked the image I am using for the endpoint, it says '...\/tensorflow-inference', which is not the direction I want to go in because I want to use a SageMaker TensorFlow serving container (I followed tutorials from the official TensorFlow serving GitHub repo-using sample models, and they are deployed correcting using the TensorFlow serving framework).<\/p>\n<p>Am I encountering this issue because my Saved Model does not have the correct 'serving' tag? I have not checked my tag sets yet but wanted to know if this would be the core reason to the problem. Also, most importantly, <strong>what are the differences between the two container types<\/strong>-I think having a better understanding of these two concepts would show me why I am unable to produce the correct image.<\/p>\n<hr \/>\n<p>This is how I deployed the sample endpoint:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>model = Model(model_data =...)\n\npredictor = model.deploy(initial_instance_count=...)\n<\/code><\/pre>\n<p>When I run the code, I get a model, an endpoint configuration, and an endpoint. I got the container type by clicking on model details within the AWS SageMaker console.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":3,
        "Question_creation_date":"2020-07-14 06:50:03.670000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-07-15 04:44:21.467000 UTC",
        "Question_score":2,
        "Question_tags":"tensorflow-serving|amazon-sagemaker",
        "Question_view_count":1223,
        "Owner_creation_date":"2020-07-02 04:04:44.510000 UTC",
        "Owner_last_access_date":"2020-09-11 11:37:39.693000 UTC",
        "Owner_location":null,
        "Owner_reputation":41,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":11,
        "Answer_body":"<p>There are different versions for the framework containers. Since the framework version I'm using is 1.15, the image I got had to be in a tensorflow-inference container. If I used versions &lt;= 1.13, then I would get sagemaker-tensorflow-serving images. The two aren't the same, but there's no 'correct' container type.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-07-20 04:33:43.720000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":66477468,
        "Question_title":"Data Version Control with Google Drive Remote: \"googleapiclient.errors.UnknownApiNameOrVersion: name: drive version: v2\"",
        "Question_body":"<p>I'm trying to setup DVC with Google Drive storage as shown <a href=\"https:\/\/dvc.org\/doc\/user-guide\/setup-google-drive-remote#url-format\" rel=\"nofollow noreferrer\">here<\/a>. So far, I've been unsuccessful in pushing data to the remote. I tried both with and without the Google App setup.<\/p>\n<p>After running a <code>dvc push -v<\/code>, the following exception is shown:<\/p>\n<pre><code>  File &quot;(...)\/anaconda3\/lib\/python3.8\/site-packages\/googleapiclient\/discovery.py&quot;, line 387, in _retrieve_discovery_doc\n    raise UnknownApiNameOrVersion(&quot;name: %s  version: %s&quot; % (serviceName, version))\ngoogleapiclient.errors.UnknownApiNameOrVersion: name: drive  version: v2\n<\/code><\/pre>\n<p>DVC was installed via <code>pip install dvc[gdrive]<\/code>. The <code>pip freeze<\/code> of the concerning packages is:<\/p>\n<pre><code>oauth2client==4.1.3\ngoogle-api-python-client==2.0.1\ndvc==2.0.1\n<\/code><\/pre>\n<p>Any help is thoroughly appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-03-04 14:51:05.907000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-03-04 16:12:18.737000 UTC",
        "Question_score":4,
        "Question_tags":"google-client|dvc",
        "Question_view_count":979,
        "Owner_creation_date":"2020-06-24 13:54:59.210000 UTC",
        "Owner_last_access_date":"2022-09-25 01:13:01.880000 UTC",
        "Owner_location":null,
        "Owner_reputation":704,
        "Owner_up_votes":53,
        "Owner_down_votes":6,
        "Owner_views":33,
        "Answer_body":"<p>Can you try to install <code>google-api-python-client==1.12.8<\/code> and test in that way?<\/p>\n<p>Edit:<\/p>\n<p>It appears to be that, this was a bug in the 2.0.0-2.0.1 of google-api-client and resolved in 2.0.2. So this should also work <code>google-api-python-client&gt;=2.0.2<\/code><\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2021-03-04 15:58:09.453000 UTC",
        "Answer_last_edit_date":"2021-03-05 06:56:47.287000 UTC",
        "Answer_score":4.0,
        "Question_valid_tags":[
            "dvc"
        ]
    },
    {
        "Question_id":58188104,
        "Question_title":"Azure Machine Learning REST API: why is the prediction included in the Sample Request?",
        "Question_body":"<p>I followed Microsoft's tutorial on the German credit card risk model, step by step and without mistakes. The algorithm runs, it is deployed successfully, etc.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/tuyDt.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/tuyDt.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>I am using the <code>Select Columns in Dataset<\/code> to select the columns to input, and I do the same to select the output columns. <\/p>\n\n<p>I noticed that when I look at the <code>Request\/Response<\/code> tab of the deployed model, the Sample Request includes <em>all<\/em> columns, ignoring the selection I provided. This includes the field to be predicted, which is column 21:<\/p>\n\n<pre><code>{\n  \"Inputs\": {\n    \"input1\": {\n      \"ColumnNames\": [\n        \"Col1\",\n        \"Col2\",\n        \"Col3\",\n        \"Col4\",\n        \"Col5\",\n        \"Col6\",\n        \"Col7\",\n        \"Col8\",\n        \"Col9\",\n        \"Col10\",\n        \"Col11\",\n        \"Col12\",\n        \"Col13\",\n        \"Col14\",\n        \"Col15\",\n        \"Col16\",\n        \"Col17\",\n        \"Col18\",\n        \"Col19\",\n        \"Col20\",\n        \"Col21\"\n<\/code><\/pre>\n\n<p><strong>The problem<\/strong>: column 21 is the credit risk itself, so the API is expecting to receive that value. Instead, <strong>that is the value that should be predicted!<\/strong><\/p>\n\n<p>There clearly is a problem with the input schema, but how can I change that? How can I make sure that field is not requested by the API?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-10-01 15:31:40.453000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"azure|rest|schema|postman|azure-machine-learning-studio",
        "Question_view_count":119,
        "Owner_creation_date":"2015-10-26 16:55:20.940000 UTC",
        "Owner_last_access_date":"2022-07-29 13:54:20.753000 UTC",
        "Owner_location":"Humboldt, Saskatchewan, Canada",
        "Owner_reputation":1271,
        "Owner_up_votes":1079,
        "Owner_down_votes":0,
        "Owner_views":171,
        "Answer_body":"<p>Don't worry about the input schema for the <code>Col21<\/code> field. The <code>Col21<\/code> field in the input data just adapt for the <code>Edit Metadata<\/code> module which requires the <code>Col21<\/code> data in the training stage.<\/p>\n\n<p>You just fill an invalid value like <code>0<\/code> (<code>0<\/code> is an invalid classified value for risk) into <code>Col21<\/code> field, and then the web service will return a prediction classified value to replace the <code>Col21<\/code> value of your input data.<\/p>\n\n<p>At here, I use the first data record of the sample data with the <code>Col21<\/code> value <code>0<\/code> for testing via the link of <code>Test<\/code> feature on portal, it works fine and return <code>1<\/code> for <code>Credit risk<\/code><\/p>\n\n<p>Fig 1. To click <code>Test<\/code> link to test for <code>Col21<\/code> with <code>0<\/code><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/9jqyo.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/9jqyo.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Fig 2. Use the first record of sample to test<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/lIUiP.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/lIUiP.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Fig 3. The <code>Col21<\/code> value of <code>input1<\/code> is <code>0<\/code>, and the <code>Credit risk<\/code> value of <code>output1<\/code> is <code>1<\/code><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/h5OEH.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/h5OEH.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2019-10-08 17:04:23.830000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":71038823,
        "Question_title":"Cannot construct an Explanation object",
        "Question_body":"<p>Trying to construct an <code>Explanation<\/code> object for a unit test, but can't seem to get it to work. Here's what I'm trying:<\/p>\n<pre><code>from google.cloud import aiplatform\n\naiplatform.compat.types.explanation_v1.Explanation(\n    attributions=aiplatform.compat.types.explanation_v1.Attribution(\n        {\n            &quot;approximation_error&quot;: 0.010399332817679649,\n            &quot;baseline_output_value&quot;: 0.9280818700790405,\n            &quot;feature_attributions&quot;: {\n                &quot;feature_1&quot;: -0.0410824716091156,\n                &quot;feature_2&quot;: 0.01155053575833639,\n            },\n            &quot;instance_output_value&quot;: 0.6717480421066284,\n            &quot;output_display_name&quot;: &quot;true&quot;,\n            &quot;output_index&quot;: [0],\n            &quot;output_name&quot;: &quot;scores&quot;,\n        }\n    )\n)\n<\/code><\/pre>\n<p>which gives:<\/p>\n<pre><code>&quot;.venv\/lib\/python3.7\/site-packages\/proto\/message.py&quot;, line 521, in __init__\n    super().__setattr__(&quot;_pb&quot;, self._meta.pb(**params))\nTypeError: Value must be iterable\n<\/code><\/pre>\n<p>I found <a href=\"https:\/\/github.com\/googleapis\/gapic-generator-python\/issues\/413#issuecomment-872094378\" rel=\"nofollow noreferrer\">this<\/a> on github, but I'm not sure how to apply that workaround here.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-02-08 18:13:44.883000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|protocol-buffers|google-cloud-vertex-ai",
        "Question_view_count":74,
        "Owner_creation_date":"2018-03-01 19:41:46.780000 UTC",
        "Owner_last_access_date":"2022-09-25 03:26:23.160000 UTC",
        "Owner_location":null,
        "Owner_reputation":3576,
        "Owner_up_votes":42,
        "Owner_down_votes":7,
        "Owner_views":203,
        "Answer_body":"<p>As the error mentioned value to be passed at <code>attributions<\/code> should be <strong>iterable<\/strong>. See <a href=\"https:\/\/cloud.google.com\/python\/docs\/reference\/aiplatform\/latest\/google.cloud.aiplatform_v1.types.Explanation\" rel=\"nofollow noreferrer\">Explanation attributes documentation<\/a>.<\/p>\n<p>I tried your code and placed the <code>Attribution<\/code> object in a list and the error is gone. I assigned your objects in variables just so the code is readable.<\/p>\n<p>See code and testing below:<\/p>\n<pre><code>from google.cloud import aiplatform\n\ntest = {\n            &quot;approximation_error&quot;: 0.010399332817679649,\n            &quot;baseline_output_value&quot;: 0.9280818700790405,\n            &quot;feature_attributions&quot;: {\n                &quot;feature_1&quot;: -0.0410824716091156,\n                &quot;feature_2&quot;: 0.01155053575833639,\n            },\n            &quot;instance_output_value&quot;: 0.6717480421066284,\n            &quot;output_display_name&quot;: &quot;true&quot;,\n            &quot;output_index&quot;: [0],\n            &quot;output_name&quot;: &quot;scores&quot;,\n        }\n\nattributions=aiplatform.compat.types.explanation_v1.Attribution(test)\nx  = aiplatform.compat.types.explanation_v1.Explanation(\n    attributions=[attributions]\n)\nprint(x)\n<\/code><\/pre>\n<p>Output:<\/p>\n<pre><code>attributions {\n  baseline_output_value: 0.9280818700790405\n  instance_output_value: 0.6717480421066284\n  feature_attributions {\n    struct_value {\n      fields {\n        key: &quot;feature_1&quot;\n        value {\n          number_value: -0.0410824716091156\n        }\n      }\n      fields {\n        key: &quot;feature_2&quot;\n        value {\n          number_value: 0.01155053575833639\n        }\n      }\n    }\n  }\n  output_index: 0\n  output_display_name: &quot;true&quot;\n  approximation_error: 0.010399332817679649\n  output_name: &quot;scores&quot;\n}\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-02-09 05:52:12.890000 UTC",
        "Answer_last_edit_date":"2022-02-09 06:15:44.880000 UTC",
        "Answer_score":1.0,
        "Question_valid_tags":[
            "google-cloud-vertex-ai"
        ]
    },
    {
        "Question_id":55347910,
        "Question_title":"Why does my ML model deployment in Azure Container Instance still fail with \"current service state: Transitioning\"?",
        "Question_body":"<p>I am using Azure Machine Learning Service to deploy a ML model as web service.<\/p>\n<p>I <a href=\"https:\/\/stackoverflow.com\/a\/55281703\/4240413\">registered a <code>model<\/code><\/a> and now would like to deploy it as an ACI web service as in <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-deploy-and-where#aci\" rel=\"nofollow noreferrer\">the guide<\/a>.<\/p>\n<p>To do so I define<\/p>\n<pre><code>from azureml.core.webservice import Webservice, AciWebservice\nfrom azureml.core.image import ContainerImage\n\naciconfig = AciWebservice.deploy_configuration(cpu_cores=4, \n                      memory_gb=32, \n                      tags={&quot;data&quot;: &quot;text&quot;,  &quot;method&quot; : &quot;NB&quot;}, \n                      description='Predict something')\n<\/code><\/pre>\n<p>and<\/p>\n<pre><code>image_config = ContainerImage.image_configuration(execution_script=&quot;score.py&quot;, \n                      docker_file=&quot;Dockerfile&quot;,\n                      runtime=&quot;python&quot;, \n                      conda_file=&quot;myenv.yml&quot;)\n<\/code><\/pre>\n<p>and create an image with<\/p>\n<pre><code>image = ContainerImage.create(name = &quot;scorer-image&quot;,\n                      models = [model],\n                      image_config = image_config,\n                      workspace = ws\n                      )\n<\/code><\/pre>\n<p>Image creation succeeds with<\/p>\n<blockquote>\n<p>Creating image Image creation operation finished for image\nscorer-image:5, operation &quot;Succeeded&quot;<\/p>\n<\/blockquote>\n<p>Also, troubleshooting the image by running it locally on an Azure VM with<\/p>\n<pre><code>sudo docker run -p 8002:5001 myscorer0588419434.azurecr.io\/scorer-image:5\n<\/code><\/pre>\n<p>allows me to run (locally) queries successfully against <code>http:\/\/localhost:8002\/score<\/code>.<\/p>\n<p>However, deployment with<\/p>\n<pre><code>service_name = 'scorer-svc'\nservice = Webservice.deploy_from_image(deployment_config = aciconfig,\n                                        image = image,\n                                        name = service_name,\n                                        workspace = ws)\n<\/code><\/pre>\n<p>fails with<\/p>\n<blockquote>\n<p>Creating service<br \/>\nRunning.<br \/>\nFailedACI service creation operation finished, operation &quot;Failed&quot;<br \/>\nService creation polling reached terminal state, current service state: Transitioning<br \/>\nService creation polling reached terminal state, unexpected response received. Transitioning<\/p>\n<\/blockquote>\n<p>I tried setting in the <code>aciconfig<\/code> more generous <code>memory_gb<\/code>, but to no avail: the deployment stays in a <em>transitioning<\/em> state (like in the image below if monitored on the Azure portal):\n<a href=\"https:\/\/i.stack.imgur.com\/gCjI3.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/gCjI3.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Also, running <code>service.get_logs()<\/code> gives me<\/p>\n<blockquote>\n<p>WebserviceException: Received bad response from Model Management\nService: Response Code: 404<\/p>\n<\/blockquote>\n<p>What could possibly be the culprit?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2019-03-25 23:38:02.573000 UTC",
        "Question_favorite_count":3.0,
        "Question_last_edit_date":"2020-08-16 22:55:02.197000 UTC",
        "Question_score":4,
        "Question_tags":"python|docker|deployment|azure-container-instances|azure-machine-learning-service",
        "Question_view_count":3489,
        "Owner_creation_date":"2014-11-11 16:17:30.717000 UTC",
        "Owner_last_access_date":"2022-09-24 20:31:18.173000 UTC",
        "Owner_location":"Verona, VR, Italy",
        "Owner_reputation":4811,
        "Owner_up_votes":376,
        "Owner_down_votes":73,
        "Owner_views":713,
        "Answer_body":"<p>If ACI deployment fails, one solution is trying to allocate <em>less<\/em> resources, e.g.<\/p>\n\n<pre><code>aciconfig = AciWebservice.deploy_configuration(cpu_cores=1, \n                  memory_gb=8, \n                  tags={\"data\": \"text\",  \"method\" : \"NB\"}, \n                  description='Predict something')\n<\/code><\/pre>\n\n<p>While the error messages thrown are not particularly informative, this is actually clearly stated in the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/container-instances\/container-instances-region-availability\" rel=\"nofollow noreferrer\">documentation<\/a>:<\/p>\n\n<blockquote>\n  <p>When a region is under heavy load, you may experience a failure when\n  deploying instances. To mitigate such a deployment failure, try\n  deploying instances with lower resource settings [...]<\/p>\n<\/blockquote>\n\n<p>The documentation also states which are the maximum values of the CPU\/RAM resources available in the different regions (at the time of writing, requiring a deployment with <code>memory_gb=32<\/code> would likely fail in all regions because of insufficient resources).<\/p>\n\n<p>Upon requiring less resources, deployment should succeed with <\/p>\n\n<blockquote>\n  <p>Creating service<br>\n  Running......................................................<br>\n  SucceededACI service creation operation finished, operation<br>\n  \"Succeeded\" Healthy<\/p>\n<\/blockquote>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2019-03-29 00:06:58.050000 UTC",
        "Answer_last_edit_date":"2019-04-01 10:51:05.523000 UTC",
        "Answer_score":3.0,
        "Question_valid_tags":[
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":69269073,
        "Question_title":"How to assign two or more time series identifier columns in Vertex AI Tabular Forecasting",
        "Question_body":"<p>I was wondering if it is possible to have more than one time series identifier column in the model? Let's assume I'd like to create a forecast at a product and store level (which the documentation suggests should be possible).<\/p>\n<p>If I select product as the series identifier, the only options I have left for store is either a covariate or an attribute and neither is applicable in this scenario.<\/p>\n<p>Would concatenating product and store and using the individual product and store code values for that concatenated ID as attributes be a solution? It doesn't feel right, but I can't see any other option - am I missing something?<\/p>\n<p>Note: I understand that this feature of Vertex AI is currently in preview and that because of that the options may be limited.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2021-09-21 12:41:08.913000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"google-cloud-vertex-ai",
        "Question_view_count":269,
        "Owner_creation_date":"2018-02-26 07:37:25.620000 UTC",
        "Owner_last_access_date":"2022-09-13 18:52:26.407000 UTC",
        "Owner_location":"Northampton, UK",
        "Owner_reputation":333,
        "Owner_up_votes":107,
        "Owner_down_votes":0,
        "Owner_views":46,
        "Answer_body":"<p>There isn't an alternate way to assign 2 or more <strong>Time Series Identifiers<\/strong> in the <strong>Forecasting Model<\/strong> on <strong>Vertex AI<\/strong>. The &quot;<strong>Forecasting model<\/strong>&quot; is in the &quot;<strong>Preview<\/strong>&quot; <a href=\"https:\/\/cloud.google.com\/products#product-launch-stages\" rel=\"nofollow noreferrer\">Product launch stage<\/a>, as you are aware, with all consequences of that fact the options are limited. Please refer to this <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/datasets\/bp-tabular#data_preparation_best_practices\" rel=\"nofollow noreferrer\">doc<\/a> for more information about the best practices for data preparation to train the forecasting model.<\/p>\n<p>As a workaround, the two columns can be concatenated and assigned a Time Series Identifier on that concatenated column, as you have mentioned in the question. This way, the concatenated column carries more contextual information into the training of the model.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-09-23 05:48:16.770000 UTC",
        "Answer_last_edit_date":"2021-09-24 11:21:32.493000 UTC",
        "Answer_score":2.0,
        "Question_valid_tags":[
            "google-cloud-vertex-ai"
        ]
    },
    {
        "Question_id":59717227,
        "Question_title":"AWS SageMaker - submit button is not working with custom template",
        "Question_body":"<p>When I create a new job on AWS SageMaker, using my custom template with crowd form (see attached sample) the SUBMIT button is not working and is not even clickable. Is there anyway to make this work? Haven`t see a good response on AWS support.<\/p>\n\n<pre><code>$('#submitButton').onclick = function() {\n   $('crowd-form').submit(); \n};\n\n\n &lt;body&gt;\n    &lt;h2 id=\"hit\"&gt;test&lt;\/h2&gt;\n        &lt;canvas id=\"canvas\" width=1210 height=687&gt;&lt;\/canvas&gt;    \n        &lt;crowd-button id=\"submitButton3\"&gt;Test button&lt;\/crowd-button&gt;\n\n    &lt;crowd-form&gt;\n\n        &lt;input type=\"hidden\" name=\"path0\" id=\"input0123\" value=\"{{task.input.metadata.images.path0}}\" \/&gt;\n        &lt;crowd-input label=\"Please input the character you see in the image\" max-length=\"1\" name=\"workerInput0\"&gt;&lt;\/crowd-input&gt;\n\n        &lt;crowd-button id=\"submitButto3223n\"&gt;Submit123&lt;\/crowd-button&gt;\n\n    &lt;\/div&gt;&lt;\/div&gt;\n\n    &lt;crowd-button id=\"submitButton\"&gt;Submit123&lt;\/crowd-button&gt;\n\n    &lt;\/crowd-form&gt;\n    &lt;crowd-button id=\"submitButton1\"&gt;Submit1232&lt;\/crowd-button&gt;\n\n    &lt;script src=\"http:\/\/code.jquery.com\/jquery-1.11.0.min.js\"&gt;&lt;\/script&gt;\n &lt;\/body&gt;\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":4,
        "Question_creation_date":"2020-01-13 13:04:26.227000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-01-14 08:12:53.557000 UTC",
        "Question_score":2,
        "Question_tags":"amazon-sagemaker|mechanicalturk",
        "Question_view_count":917,
        "Owner_creation_date":"2018-12-09 09:05:30.127000 UTC",
        "Owner_last_access_date":"2020-04-30 07:25:44.867000 UTC",
        "Owner_location":"Lviv, Lviv Oblast, Ukraine",
        "Owner_reputation":23,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":15,
        "Answer_body":"<p>There are few issues with you code snippet.<\/p>\n<p>Here are the links to <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-ui-template-reference.html\" rel=\"nofollow noreferrer\">SageMaker's HTML Reference<\/a> and <a href=\"https:\/\/awsfeed.com\/whats-new\/machine-learning\/build-a-custom-data-labeling-workflow-with-amazon-sagemaker-ground-truth\/\" rel=\"nofollow noreferrer\">Example for building custom Labeling template<\/a><\/p>\n<p>First remove all those submit buttons (<code>&lt;crowd-button&gt;<\/code> elements) and the <code>onClick<\/code> event handler. From here you have two options use default SageMaker submit button or create your own in the template.<\/p>\n<h2>Use SageMaker's Submit Button<\/h2>\n<p>Leave out submit buttons (<code>crowd-button<\/code>) and SageMaker will automatically append one inside <code>crowd-form<\/code>. According to documentation <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-ui-template-crowd-form.html\" rel=\"nofollow noreferrer\">here<\/a><\/p>\n<h2>Use custom Submit Button<\/h2>\n<p>In this case you need to:<\/p>\n<ol>\n<li>Prevent SageMaker adding button by including <code>crowd-button<\/code> <strong>inside<\/strong> the <code>crowd-form<\/code> element and setting <code>style=&quot;display: none;<\/code><\/li>\n<li>Add your own Submit button elsewhere on the template and add <code>onclick<\/code> even handler that will execute <code>form.submit()<\/code><\/li>\n<\/ol>\n<p>Here is the working example of the template (taken from the Example mentioned above).<\/p>\n<pre><code>&lt;script src=&quot;https:\/\/assets.crowd.aws\/crowd-html-elements.js&quot;&gt;&lt;\/script&gt;\n\n&lt;link rel=&quot;stylesheet&quot; href=&quot;https:\/\/s3.amazonaws.com\/smgtannotation\/web\/static\/css\/1.3fc3007b.chunk.css&quot;&gt;\n&lt;link rel=&quot;stylesheet&quot; href=&quot;https:\/\/s3.amazonaws.com\/smgtannotation\/web\/static\/css\/main.9504782e.chunk.css&quot;&gt;\n\n&lt;div id='document-text' style=&quot;display: none;&quot;&gt;\n  {{ task.input.text }}\n&lt;\/div&gt;\n&lt;div id='document-image' style=&quot;display: none;&quot;&gt;\n        {{ task.input.taskObject | grant_read_access }}\n&lt;\/div&gt;\n&lt;div id=&quot;metadata&quot; style=&quot;display: none;&quot;&gt;\n  {{ task.input.metadata }}\n&lt;\/div&gt;\n\n&lt;crowd-form&gt;\n    &lt;input name=&quot;annotations&quot; id=&quot;annotations&quot; type=&quot;hidden&quot;&gt;\n\n     &lt;!-- Prevent crowd-form from creating its own button --&gt;\n    &lt;crowd-button form-action=&quot;submit&quot; style=&quot;display: none;&quot;&gt;&lt;\/crowd-button&gt;\n&lt;\/crowd-form&gt;\n\n&lt;!-- Custom annotation user interface is rendered here --&gt;\n&lt;div id=&quot;root&quot;&gt;&lt;\/div&gt;\n\n&lt;crowd-button id=&quot;submitButton&quot;&gt;Submit&lt;\/crowd-button&gt;\n\n&lt;script&gt;\n    document.querySelector('crowd-form').onsubmit = function() {\n        document.getElementById('annotations').value = JSON.stringify(JSON.parse(document.querySelector('pre').innerText));\n    };\n\n    document.getElementById('submitButton').onclick = function() {\n        document.querySelector('crowd-form').submit();\n    };\n&lt;\/script&gt;\n\n&lt;script src=&quot;https:\/\/s3.amazonaws.com\/smgtannotation\/web\/static\/js\/1.3e5a6849.chunk.js&quot;&gt;&lt;\/script&gt;\n&lt;script src=&quot;https:\/\/s3.amazonaws.com\/smgtannotation\/web\/static\/js\/main.96e12312.chunk.js&quot;&gt;&lt;\/script&gt;\n&lt;script src=&quot;https:\/\/s3.amazonaws.com\/smgtannotation\/web\/static\/js\/runtime~main.229c360f.js&quot;&gt;&lt;\/script&gt;\n<\/code><\/pre>\n<p>Code source<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-01-16 15:42:48.977000 UTC",
        "Answer_last_edit_date":"2020-06-20 09:12:55.060000 UTC",
        "Answer_score":2.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":63350039,
        "Question_title":"Add lifecycle configuration to existing notebook in SageMaker?",
        "Question_body":"<p>I started with SageMaker recently, and I'm loving it. However, I've been installing the same libraries over and over again to one of the in-built conda environments, and I want to create a life cycle configuration to do that automatically on startup. based on <a href=\"https:\/\/docs.aws.amazon.com\/en_us\/sagemaker\/latest\/dg\/notebook-lifecycle-config.html\" rel=\"nofollow noreferrer\">the bottom of this<\/a>:<\/p>\n<blockquote>\n<p>notebook instance lifecycle configurations are available when you create a new notebook instance.<\/p>\n<\/blockquote>\n<p>the trouble is, I already have a notebook I've been working in for a while. Is there any way to apply a life cycle configuration on startup to an already existing notebook?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2020-08-11 00:59:05.867000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-08-11 01:05:25.233000 UTC",
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-ec2|amazon-sagemaker",
        "Question_view_count":395,
        "Owner_creation_date":"2018-12-21 02:51:36.800000 UTC",
        "Owner_last_access_date":"2022-09-25 01:54:35.743000 UTC",
        "Owner_location":"Earth",
        "Owner_reputation":1011,
        "Owner_up_votes":218,
        "Owner_down_votes":5,
        "Owner_views":93,
        "Answer_body":"<p>You need to shut the instance down, then you can edit it. Then, if you use your eyes (which I neglected to do) you can see the &quot;Additional Configurations&quot; section contains lifecycle configurations<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-08-11 01:07:54.440000 UTC",
        "Answer_last_edit_date":"2021-04-18 03:31:54.507000 UTC",
        "Answer_score":2.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":71385524,
        "Question_title":"Sagemaker training job Fatal error: cannot open file 'train': No such file or directory",
        "Question_body":"<p>I am trying work on bring your own model. I have R code. when i try to run the job its failing.<\/p>\n<p><strong>Training Image:<\/strong><\/p>\n<pre><code>FROM r-base:3.6.3\n\nMAINTAINER Amazon SageMaker Examples &lt;amazon-sagemaker-examples@amazon.com&gt;\n\nRUN apt-get -y update &amp;&amp; apt-get install -y --no-install-recommends \\\n    wget \\\n    r-base \\\n    r-base-dev \\\n    apt-transport-https \\\n    ca-certificates \\\n    python3 python3-dev pip\n\nENV AWS_DEFAULT_REGION=&quot;us-east-2&quot;\nRUN R -e &quot;install.packages('reticulate', dependencies = TRUE, warning = function(w) stop(w))&quot;\nRUN R -e &quot;install.packages('readr', dependencies = TRUE, warning = function(w) stop(w))&quot;\nRUN R -e &quot;install.packages('dplyr', dependencies = TRUE, warning = function(w) stop(w))&quot;\n\nRUN pip install --quiet --no-cache-dir \\\n    'boto3&gt;1.0&lt;2.0' \\\n    'sagemaker&gt;2.0&lt;3.0'    \n\nENTRYPOINT [&quot;\/usr\/bin\/Rscript&quot;]\n<\/code><\/pre>\n<p><strong>Source code:<\/strong><\/p>\n<pre><code>rcode\n    \u2514\u2500\u2500 train.R\n    \u2514\u2500\u2500 train.tar.gz\n<\/code><\/pre>\n<p>Build<\/p>\n<pre><code>- aws s3 cp $CODEBUILD_SRC_DIR\/rcode\/ s3:\/\/${self:custom.deploymentBucket}\/${self:service}\/code\/training --recursive\n<\/code><\/pre>\n<p><strong>Serverless.com yaml<\/strong><\/p>\n<pre><code>           SagemakerRCodeTrainingStep:\n            Type: Task\n            Resource: ${self:custom.sageMakerTrainingJob}\n            Parameters:\n              TrainingJobName.$: &quot;$.sageMakerTrainingJobName&quot;\n              DebugHookConfig:\n                S3OutputPath: &quot;s3:\/\/${self:custom.deploymentBucket}\/${self:service}\/models\/rmodel&quot;\n              AlgorithmSpecification:\n                TrainingImage: ${self:custom.sagemakerRExecutionContainerURI}\n                TrainingInputMode: &quot;File&quot;\n              OutputDataConfig:\n                S3OutputPath: &quot;s3:\/\/${self:custom.deploymentBucket}\/${self:service}\/models\/rmodel&quot;\n              StoppingCondition:\n                MaxRuntimeInSeconds: ${self:custom.maxRuntime}\n              ResourceConfig:\n                InstanceCount: 1\n                InstanceType: &quot;ml.m5.xlarge&quot;\n                VolumeSizeInGB: 30\n              RoleArn: ${self:custom.stateMachineRoleARN}\n              InputDataConfig:\n                - DataSource:\n                    S3DataSource:\n                      S3DataType: &quot;S3Prefix&quot;\n                      S3Uri: &quot;s3:\/\/${self:custom.datasetsFilePath}\/data\/processed\/train&quot;\n                      S3DataDistributionType: &quot;FullyReplicated&quot;\n                  ChannelName: &quot;train&quot;\n              HyperParameters:\n                sagemaker_submit_directory: &quot;s3:\/\/${self:custom.deploymentBucket}\/${self:service}\/code\/training\/train.tar.gz&quot;\n                sagemaker_program: &quot;train.R&quot;\n                sagemaker_enable_cloudwatch_metrics: &quot;false&quot;\n                sagemaker_container_log_level: &quot;20&quot;\n                sagemaker_job_name: &quot;sagemaker-r-learn-2022-02-28-09-56-33-234&quot;\n                sagemaker_region: ${self:provider.region}\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-03-07 18:13:04.137000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-03-08 06:26:38.767000 UTC",
        "Question_score":1,
        "Question_tags":"r|amazon-sagemaker|serverless.com",
        "Question_view_count":369,
        "Owner_creation_date":"2010-09-29 12:36:11.347000 UTC",
        "Owner_last_access_date":"2022-07-20 12:26:21.990000 UTC",
        "Owner_location":"Bangalore, India",
        "Owner_reputation":6958,
        "Owner_up_votes":123,
        "Owner_down_votes":1,
        "Owner_views":787,
        "Answer_body":"<p>I am not sure which <code>TrainingImage<\/code> you are using and all the files in your container.\nThat being said, I suspect you are using a custom container.<\/p>\n<p>SageMaker Training Jobs look for a <code>train<\/code> file and run your container as <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-dockerfile.html\" rel=\"nofollow noreferrer\">follows<\/a>:<\/p>\n<pre><code>docker run image train\n<\/code><\/pre>\n<p>You can change this behavior by setting the <code>ENTRYPOINT<\/code> in your Dockerfile. Please see this example <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/r_examples\/r_byo_r_algo_hpo\/Dockerfile#L47\" rel=\"nofollow noreferrer\">Dockerfile<\/a> from the <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/tree\/main\/r_examples\/r_byo_r_algo_hpo\" rel=\"nofollow noreferrer\">r_byo_r_algo_hpo<\/a> example.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2022-03-07 22:06:30.177000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":68009703,
        "Question_title":"Can \"Invoke_endpoint\" calls timeout a lambda function?",
        "Question_body":"<p>I am attempting to pass json data into my sagemaker model through a lambda function. Currently, I am using a testing model that makes relatively quick inferences and returns them to the lambda function through the invoke_endpoint call. However, eventually a more advanced model will be implemented which might take longer than a lambda function can fun for (15 minutes maximum) to produce inferences. In the case that I call invoke_endpoint in one lambda function, can I return the response to another lambda function which is invoked by the sagemaker endpoint response? Even better, can I shut down the current lambda function after sending the data to sagemaker, and re-invoke it upon a response? I need to store the inference in DynamoDB, which is why I need a response (Unless I can update the saved model to store inferences directly, in which case I need the lambda function to not expect a response from invoke_endpoint). Sorry for my ignorance, I am a bit new to sagemaker.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-06-16 20:35:36.457000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|aws-lambda|amazon-sagemaker",
        "Question_view_count":181,
        "Owner_creation_date":"2021-05-26 21:07:02.450000 UTC",
        "Owner_last_access_date":"2022-07-23 02:32:24.890000 UTC",
        "Owner_location":null,
        "Owner_reputation":5,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":"<p>When calling <code>invoke_endpoint<\/code>, the underlying model invocation must take <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-inference-code.html#your-algorithms-inference-code-how-containe-serves-requests\" rel=\"nofollow noreferrer\">less than 1 minute<\/a>. If a single model execution needs more time to execute, consider running the model in Lambda itself, in SageMaker Training API (if its coldstart is acceptable) or in a custom service. If the invocation is made of several shorter calls you can also chain multiple services together with Step Functions.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-06-16 21:13:35.320000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":71077397,
        "Question_title":"Sagemaker Hyperparameter Tuning Job Mechanism",
        "Question_body":"<p>Does anyone know what's the mechanism behind hyperparameter tuning job in AWS Sagemaker?<\/p>\n<p>In specific, I am trying to do the following:<\/p>\n<ol>\n<li>Bring my own container<\/li>\n<li>Minimize cross entropy loss (this will be the objective metric of the tuner)<\/li>\n<\/ol>\n<p>My question is when we define the hyper parameter in <code>HyperParameterTuner<\/code> class, does that get copied into <code>\/opt\/ml\/input\/config\/hyperparameters.json<\/code>?<\/p>\n<p>If so, should one adjust the training image so that it uses the hyper parameters from <code>\/opt\/ml\/input\/config\/hyperparameters.json<\/code>?<\/p>\n<p>Edit: I've looked into some sample HPO notebooks that AWS provides and they seem to confuse me more. Sometimes they'd use <code>argparser<\/code> to pass in the HPs. How is that passed into the training code?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2022-02-11 08:57:35.830000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-02-11 09:12:31.860000 UTC",
        "Question_score":1,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":281,
        "Owner_creation_date":"2015-11-03 19:08:13.503000 UTC",
        "Owner_last_access_date":"2022-09-14 22:18:18.333000 UTC",
        "Owner_location":null,
        "Owner_reputation":361,
        "Owner_up_votes":15,
        "Owner_down_votes":0,
        "Owner_views":32,
        "Answer_body":"<p>So i finally figured it out and had it wrong all the time.<\/p>\n<p>The file <code>\/opt\/ml\/input\/config\/hyperparameters.json<\/code> is there. It just has slightly different content compared to a regular training-job. The params to be tuned as well as static params are contained there. As well as the metric-name.<\/p>\n<p>So here is the structure, i hope it helps:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n    '_tuning_objective_metric': 'your-metric', \n    'dynamic-param1': '0.3', \n    'dynamic-param2': '1',\n    'static-param1': 'some-value', \n    'static-paramN': 'another-value'\n}\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-08-19 14:42:46.467000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":61683506,
        "Question_title":"Azure ML: how to access logs of a failed Model deployment",
        "Question_body":"<p>I'm deploying a Keras model that is failing with the error below. The exception says that I can retrieve the logs by running \"print(service.get_logs())\", but that's giving me empty results. I am deploying the model from my AzureNotebook and I'm using the same \"service\" var to retrieve the logs. <\/p>\n\n<p>Also, how can i retrieve the logs from the container instance? I'm deploying to an AKS compute cluster I created. Sadly, the docs link in the exception also doesnt detail how to retrieve these logs.<\/p>\n\n<pre><code>More information can be found using '.get_logs()' Error: \n<\/code><\/pre>\n\n<pre><code>{   \"code\":\n\"KubernetesDeploymentFailed\",   \"statusCode\": 400,   \"message\":\n\"Kubernetes Deployment failed\",   \"details\": [\n    {\n      \"code\": \"CrashLoopBackOff\",\n      \"message\": \"Your container application crashed. This may be caused by errors in your scoring file's init() function.\\nPlease check\nthe logs for your container instance: my-model-service. From\nthe AML SDK, you can run print(service.get_logs()) if you have service\nobject to fetch the logs. \\nYou can also try to run image\nmlwks.azurecr.io\/azureml\/azureml_3c0c34b65cf18c8644e8d745943ab7d2:latest\nlocally. Please refer to http:\/\/aka.ms\/debugimage#service-launch-fails\nfor more information.\"\n    }   ] }\n<\/code><\/pre>\n\n<p>UPDATE<\/p>\n\n<p>Here's my code to deploy the model:<\/p>\n\n<pre><code>environment = Environment('my-environment')\nenvironment.python.conda_dependencies = CondaDependencies.create(pip_packages=[\"azureml-defaults\",\"azureml-dataprep[pandas,fuse]\",\"tensorflow\", \"keras\", \"matplotlib\"])\nservice_name = 'my-model-service'\n\n# Remove any existing service under the same name.\ntry:\n    Webservice(ws, service_name).delete()\nexcept WebserviceException:\n    pass\n\ninference_config = InferenceConfig(entry_script='score.py', environment=environment)\ncomp = ComputeTarget(workspace=ws, name=\"ml-inference-dev\")\nservice = Model.deploy(workspace=ws,\n                       name=service_name,\n                       models=[model],\n                       inference_config=inference_config,\n                       deployment_target=comp \n                      )\nservice.wait_for_deployment(show_output=True)\n<\/code><\/pre>\n\n<p>And my score.py<\/p>\n\n<pre><code>import joblib\nimport numpy as np\nimport os\n\nimport keras\n\nfrom keras.models import load_model\nfrom inference_schema.schema_decorators import input_schema, output_schema\nfrom inference_schema.parameter_types.numpy_parameter_type import NumpyParameterType\n\n\ndef init():\n    global model\n\n    model_path = Model.get_model_path('model.h5')\n    model = load_model(model_path)\n    model = keras.models.load_model(model_path)\n\n\n# The run() method is called each time a request is made to the scoring API.\n#\n# Shown here are the optional input_schema and output_schema decorators\n# from the inference-schema pip package. Using these decorators on your\n# run() method parses and validates the incoming payload against\n# the example input you provide here. This will also generate a Swagger\n# API document for your web service.\n@input_schema('data', NumpyParameterType(np.array([[0.1, 1.2, 2.3, 3.4, 4.5, 5.6, 6.7, 7.8, 8.9, 9.0]])))\n@output_schema(NumpyParameterType(np.array([4429.929236457418])))\ndef run(data):\n\n    return [123] #test\n<\/code><\/pre>\n\n<p><strong>Update 2:<\/strong><\/p>\n\n<p>Here is a screencap of the endpoint page. Is it normal for the CPU to be .1? Also, when i hit the swagger url in the browser, i get the error: \"No ready replicas for service doc-classify-env-service\"<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/Uvrfx.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Uvrfx.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><strong>Update 3<\/strong>\nAfter finally getting to the container logs, it turns out that it was choking  with this error on my score.py<\/p>\n\n<blockquote>\n  <p>ModuleNotFoundError: No module named 'inference_schema'<\/p>\n<\/blockquote>\n\n<p>I then ran a test that commented out the refs for \"input_schema\" and \"output_schema\" and also simplified my pip_packages and the REST endpoint come up! I was also able to get a prediction out of the model. <\/p>\n\n<pre><code>pip_packages=[\"azureml-defaults\",\"tensorflow\", \"keras\"])\n<\/code><\/pre>\n\n<p>So my question is, how should I have my pip_packages for the scoring file to utilize the inference_schema decorators? I'm assuming I need to include azureml-sdk[auotml] pip package, but when i do so, the image creation fails and I see several dependency conflicts.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2020-05-08 16:22:47.507000 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":"2020-05-13 22:45:55.880000 UTC",
        "Question_score":1,
        "Question_tags":"azure-machine-learning-service",
        "Question_view_count":1564,
        "Owner_creation_date":"2012-02-23 16:54:25.410000 UTC",
        "Owner_last_access_date":"2022-09-02 23:23:03.830000 UTC",
        "Owner_location":null,
        "Owner_reputation":1704,
        "Owner_up_votes":61,
        "Owner_down_votes":7,
        "Owner_views":232,
        "Answer_body":"<p>Try retrieving your service from the workspace directly <\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>ws.webservices[service_name].get_logs()\n<\/code><\/pre>\n\n<p>Also, I found deploying an image as an endpoint to be easier than inference+deploy model (depending on your use case) <\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>my_image = Image(ws, name='test', version='26')  \nservice = AksWebservice.deploy_from_image(ws, \"test1\", my_image, deployment_config, aks_target)\n<\/code><\/pre>",
        "Answer_comment_count":13.0,
        "Answer_creation_date":"2020-05-08 19:04:47.420000 UTC",
        "Answer_last_edit_date":"2020-05-08 20:40:02.070000 UTC",
        "Answer_score":3.0,
        "Question_valid_tags":[
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":69425994,
        "Question_title":"is there any way to scale axis of plots in wandb?",
        "Question_body":"<p>I am working with weight and bias(wandb).<br \/>\nHowever, it logs by step. And that makes plot disturbing when comparing runs.<br \/>\nFor example, I have a run A and run B(assume that they run with same dataset).<br \/>\nrun A: 30epochs, 4 batch, 200step\/epoch<br \/>\nrun B: 30epochs, 8 batch, 100step\/epoch<\/p>\n<p>then, the plot of run A gets longer(double, in this case) in axis x when it shows with run B.<\/p>\n<p>How can I scale x axis depend to runs AFTER training?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-10-03 14:53:08.153000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"wandb",
        "Question_view_count":604,
        "Owner_creation_date":"2020-10-22 15:00:31.587000 UTC",
        "Owner_last_access_date":"2022-09-19 00:28:34.893000 UTC",
        "Owner_location":null,
        "Owner_reputation":161,
        "Owner_up_votes":10,
        "Owner_down_votes":0,
        "Owner_views":29,
        "Answer_body":"<p>You can change the x-axis used via the chart settings by clicking on the pencil icon and then selecting a different x-axis. E.g. in your case you could select &quot;epoch&quot; instead of &quot;steps&quot;. Just make sure to log &quot;epoch&quot; to your charts, something like:<\/p>\n<pre><code>steps_per_epoch = n_samples \/ batch_size\nepoch = current_step \/ steps_per_epoch\nwandb.log({&quot;epoch&quot;:epoch, ...})\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-10-04 11:02:55.677000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":4.0,
        "Question_valid_tags":[
            "wandb"
        ]
    },
    {
        "Question_id":70109346,
        "Question_title":"Vertex AI was unable to import data into dataset. It says maximum 1M lines while my dataset only have 600k",
        "Question_body":"<p>I'm importing a text dataset to Google Vertex AI and got the following error:<\/p>\n<pre><code>Hello Vertex AI Customer,\n\nDue to an error, Vertex AI was unable to import data into \ndataset [dataset_name].\nAdditional Details:\nOperation State: Failed with errors\nResource Name: [resoure_link]\nError Messages: There are too many rows in the jsonl\/csv file. Currently we \nonly support 1000000 lines. Please cut your files to smaller size and run \nmultiple import data pipelines to import.\n<\/code><\/pre>\n<p>I checked my dataset which I generated from pandas and the actual CSV file, it only have 600k lines.<\/p>\n<p>Anyone got similar errors?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":4,
        "Question_creation_date":"2021-11-25 10:19:25.680000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-12-02 08:51:30.680000 UTC",
        "Question_score":2,
        "Question_tags":"python|google-cloud-vertex-ai",
        "Question_view_count":212,
        "Owner_creation_date":"2021-11-18 12:35:45.423000 UTC",
        "Owner_last_access_date":"2022-09-23 09:05:57.553000 UTC",
        "Owner_location":"Jakarta, Indonesia",
        "Owner_reputation":31,
        "Owner_up_votes":6,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Answer_body":"<p>So it turns out to be an error in my CSV formatting.<\/p>\n<p>I forgot to trim newlines and extra whitespaces in my text dataset. This solved the 1M lines count. But after doing that, I then get error telling me I have too much labels while it was only 2.<\/p>\n<pre><code>Error Messages: There are too many AnnotationSpecs in the dataset. Up to \n5000 AnnotationSpecs are allowed in one Dataset.\n<\/code><\/pre>\n<p>And this is because I created the text dataset using to_csv() method in Pandas dataframe. Creating a CSV file this way, it will automatically put quotes when your text include a &quot;,&quot; (comma character) only. So the CSV file will look like:<\/p>\n<pre><code>&quot;this is a sentence, with a comma&quot;, 0\nthis is a sentence without a comma, 1\n<\/code><\/pre>\n<p>Meanwhile, what Vertex AutoML Text wants the CSV is to look like this:<\/p>\n<pre><code>&quot;this is a sentence, with a comma&quot;, 0\n&quot;this is a sentence without a comma&quot;, 1\n<\/code><\/pre>\n<p>i.e. you have to put quotes on every line.<\/p>\n<p>Which you can achieve by writing your own CSV formatter, or if you insist on using Pandas to_csv(), you can pass csv.QUOTE_ALL to the quoting parameter. It will look like this:<\/p>\n<pre><code>import csv\ndf.to_csv(&quot;file.csv&quot;, index=False, quoting=csv.QUOTE_ALL, header=False)\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-12-09 05:19:20.677000 UTC",
        "Answer_last_edit_date":"2021-12-09 05:41:16.930000 UTC",
        "Answer_score":1.0,
        "Question_valid_tags":[
            "google-cloud-vertex-ai"
        ]
    },
    {
        "Question_id":68718719,
        "Question_title":"How can I retrive the model.pkl in the experiment in Databricks",
        "Question_body":"<p>I want to retrieve the pickle off my trained model, which I know is in the run file inside my experiments in Databricks.<\/p>\n<p>It seems that the <code>mlflow.pyfunc.load_model<\/code> can only do the <code>predict<\/code> method.<\/p>\n<p>There is an option to directly access the pickle?<\/p>\n<p>I also tried to use the path in the run using the <code>pickle.load(path)<\/code> (example of path: dbfs:\/databricks\/mlflow-tracking\/20526156406\/92f3ec23bf614c9d934dd0195\/artifacts\/model\/model.pkl).<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2021-08-09 21:26:51.170000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"python|azure|databricks|datastore|mlflow",
        "Question_view_count":3081,
        "Owner_creation_date":"2018-03-26 15:02:34.450000 UTC",
        "Owner_last_access_date":"2022-09-23 02:49:37.960000 UTC",
        "Owner_location":"S\u00e3o Paulo, State of S\u00e3o Paulo, Brazil",
        "Owner_reputation":96,
        "Owner_up_votes":10,
        "Owner_down_votes":0,
        "Owner_views":31,
        "Answer_body":"<p>I recently found the solution which can be done by the following two approaches:<\/p>\n<ol>\n<li>Use the customized predict function at the moment of saving the model (check <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/models.html#model-customization\" rel=\"nofollow noreferrer\">databricks<\/a> documentation for more details).<\/li>\n<\/ol>\n<p>example give by Databricks<\/p>\n<pre><code>class AddN(mlflow.pyfunc.PythonModel):\n\n    def __init__(self, n):\n        self.n = n\n\n    def predict(self, context, model_input):\n        return model_input.apply(lambda column: column + self.n)\n# Construct and save the model\nmodel_path = &quot;add_n_model&quot;\nadd5_model = AddN(n=5)\nmlflow.pyfunc.save_model(path=model_path, python_model=add5_model)\n\n# Load the model in `python_function` format\nloaded_model = mlflow.pyfunc.load_model(model_path)\n<\/code><\/pre>\n<ol start=\"2\">\n<li>Load the model artefacts as we are downloading the artefact:<\/li>\n<\/ol>\n<pre><code>from mlflow.tracking import MlflowClient\n\nclient = MlflowClient()\n\ntmp_path = client.download_artifacts(run_id=&quot;0c7946c81fb64952bc8ccb3c7c66bca3&quot;, path='model\/model.pkl')\n\nf = open(tmp_path,'rb')\n\nmodel = pickle.load(f)\n\nf.close()\n\n \n\nclient.list_artifacts(run_id=&quot;0c7946c81fb64952bc8ccb3c7c66bca3&quot;, path=&quot;&quot;)\n\nclient.list_artifacts(run_id=&quot;0c7946c81fb64952bc8ccb3c7c66bca3&quot;, path=&quot;model&quot;)\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-08-23 19:53:41.287000 UTC",
        "Answer_last_edit_date":"2022-01-24 20:08:25.513000 UTC",
        "Answer_score":1.0,
        "Question_valid_tags":[
            "mlflow"
        ]
    },
    {
        "Question_id":64875623,
        "Question_title":"Invoking an endpoint in AWS with a multidimensional array",
        "Question_body":"<p>I have deployed a Tensorflow-Model in SageMaker Studio following this tutorial:\n<a href=\"https:\/\/aws.amazon.com\/de\/blogs\/machine-learning\/deploy-trained-keras-or-tensorflow-models-using-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/de\/blogs\/machine-learning\/deploy-trained-keras-or-tensorflow-models-using-amazon-sagemaker\/<\/a>\nThe Model needs a Multidimensional Array as input. Invoking it from the Notebook itself is working:<\/p>\n<pre><code>import numpy as np\nimport json\ndata = np.load(&quot;testValues.npy&quot;)\npred=predictor.predict(data)\n<\/code><\/pre>\n<p>But I wasnt able to invoke it from a boto 3 client using this code:<\/p>\n<pre><code>import json\nimport boto3\nimport numpy as np\nimport io\n \nclient = boto3.client('runtime.sagemaker')\ndatain = np.load(&quot;testValues.npy&quot;)\ndata=datain.tolist();\nresponse = client.invoke_endpoint(EndpointName=endpoint_name, Body=json.dumps(data))\nresponse_body = response['Body']\nprint(response_body.read())\n<\/code><\/pre>\n<p>This throws the Error:<\/p>\n<pre><code>An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (415) from model with message &quot;{&quot;error&quot;: &quot;Unsupported Media Type: Unknown&quot;}&quot;.\n<\/code><\/pre>\n<p>I guess the reason is the json Media Type but i have no clue how to get it back in shape.\nI tried this:<a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/issues\/644\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/amazon-sagemaker-examples\/issues\/644<\/a> but it doesnt seem to change anything<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-11-17 12:50:35.417000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":550,
        "Owner_creation_date":"2018-11-19 11:55:42.703000 UTC",
        "Owner_last_access_date":"2022-01-24 23:25:29.970000 UTC",
        "Owner_location":"Germany",
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Answer_body":"<p>This fixed it for me:\nThe Content Type was missing.<\/p>\n<pre><code>import json\nimport boto3\nimport numpy as np\nimport io\n\nclient = boto3.client('runtime.sagemaker',aws_access_key_id=..., aws_secret_access_key=...,region_name=...)\nendpoint_name = '...'\n\ndata = np.load(&quot;testValues.npy&quot;)\n\n\npayload = json.dumps(data.tolist())\nresponse = client.invoke_endpoint(EndpointName=endpoint_name,\n                                  ContentType='application\/json',\n                                   Body=payload)\nresult = json.loads(response['Body'].read().decode())\nres = result['predictions']\nprint(&quot;test&quot;)\n\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-11-19 08:04:28.537000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":59829017,
        "Question_title":"Azure Machine Learning - Memory Error while creating dataframe",
        "Question_body":"<p>I am getting memory error while creating simple dataframe read from CSV file on Azure Machine Learning using notebook VM as compute instance. The VM has config of DS 13 56gb RAM, 8vcpu, 112gb storage on Ubuntu (Linux (ubuntu 16.04). CSV file is 5gb file. <\/p>\n\n<pre><code>blob_service = BlockBlobService(account_name,account_key)\nblobstring = blob_service.get_blob_to_text(container,filepath).content\ndffinaldata = pd.read_csv(StringIO(blobstring), sep=',')\n<\/code><\/pre>\n\n<p>What I am doing wrong here ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-01-20 18:25:49.467000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-01-20 21:35:26.093000 UTC",
        "Question_score":1,
        "Question_tags":"python|azure|azure-machine-learning-service",
        "Question_view_count":507,
        "Owner_creation_date":"2017-07-22 17:26:15.327000 UTC",
        "Owner_last_access_date":"2022-08-09 00:17:13.300000 UTC",
        "Owner_location":null,
        "Owner_reputation":255,
        "Owner_up_votes":20,
        "Owner_down_votes":0,
        "Owner_views":130,
        "Answer_body":"<p>you need to provide the right encoding when calling get_blob_to_text, please refer to the <a href=\"https:\/\/github.com\/Azure\/azure-storage-python\/blob\/master\/samples\/blob\/block_blob_usage.py#L390\" rel=\"nofollow noreferrer\">sample<\/a>.<\/p>\n\n<p>The code below is what  normally use for reading data file in blob storages. Basically, you can use blob\u2019s url along with sas token and use a request method. However, You might want to edit the \u2018for loop\u2019 depending what types of data you have (e.g. csv, jpg, and etc).<\/p>\n\n<p>-- Python code below --<\/p>\n\n<pre><code>import requests\nfrom azure.storage.blob import BlockBlobService, BlobPermissions\nfrom azure.storage.blob.baseblobservice import BaseBlobService\nfrom datetime import datetime, timedelta\n\naccount_name = '&lt;account_name&gt;'\naccount_key = '&lt;account_key&gt;'\ncontainer_name = '&lt;container_name&gt;'\n\nblob_service=BlockBlobService(account_name,account_key)\ngenerator = blob_service.list_blobs(container_name)\n\nfor blob in generator:\n    url = f\"https:\/\/{account_name}.blob.core.windows.net\/{container_name}\"\n    service = BaseBlobService(account_name=account_name, account_key=account_key)\n    token = service.generate_blob_shared_access_signature(container_name, img_name, permission=BlobPermissions.READ, expiry=datetime.utcnow() + timedelta(hours=1),)\n    url_with_sas = f\"{url}?{token}\"\n    response = requests.get(url_with_sas)\n<\/code><\/pre>\n\n<p>Please follow the below link to read data on Azure Blob Storage.\n<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-data\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-data<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-01-21 05:17:29.897000 UTC",
        "Answer_last_edit_date":"2020-01-21 05:56:40.473000 UTC",
        "Answer_score":0.0,
        "Question_valid_tags":[
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":63807950,
        "Question_title":"Letter Recognition Error in Azure ML Studio",
        "Question_body":"<p>I'm having troubles with a Letter Recognition model I'm creating in Azure ML Studio.<\/p>\n<p>I'm running a few algorithms - Decision Jungle, Neural Network, Decision Forest, Logistic Regression, One vs. All Multiclass, and then I append them using the Add rows method (Neural Network and Desicion Jungle\/ Decision Forest and Logistic Regression), until I append them all.<\/p>\n<p>However, appending Decision Forest and Logistic Regression I get the following error:<\/p>\n<pre><code>requestId = 9292bc066f51404eb5e0d0d219d3a072 errorComponent=Module. taskStatusCode=400. {&quot;Exception&quot;:{&quot;ErrorId&quot;:&quot;NotInRangeValue&quot;,&quot;ErrorCode&quot;:&quot;0008&quot;,&quot;ExceptionType&quot;:&quot;ModuleException&quot;,&quot;Message&quot;:&quot;Error 0008: Parameter \\&quot;Dataset2(number of columns)\\&quot; value should be in the range of [3, 3].&quot;}}Error: Error 0008: Parameter &quot;Dataset2(number of columns)&quot; value should be in the range of [3, 3]. Process exited with error code -2\n<\/code><\/pre>\n<p>Any advice what should I do? Huge thanks in advance<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-09-09 08:43:00.777000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-09-09 09:02:30.897000 UTC",
        "Question_score":0,
        "Question_tags":"azure|azure-machine-learning-studio",
        "Question_view_count":37,
        "Owner_creation_date":"2020-09-09 08:40:56.987000 UTC",
        "Owner_last_access_date":"2022-02-20 15:55:02.427000 UTC",
        "Owner_location":null,
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Answer_body":"<p>This error occurs when there is a mismatch of number of columns of the two dataset you are appending.<\/p>\n<p>Looking at the error :<\/p>\n<p>The output of one model is returning rows with 3 columns and other one is having either more or less than 3 columns.<\/p>\n<p>Before this step &quot;Add Rows&quot; step -&gt; Do quick Visualize<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/PsYQT.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/PsYQT.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>This will give a view of the dataset that you are planning to append.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/x442d.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/x442d.png\" alt=\"![enter image description here\" \/><\/a><\/p>\n<p>Ensure for both, the columns numbers are same.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2020-09-09 12:17:08.450000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":64305945,
        "Question_title":"pip install trains fails",
        "Question_body":"<p>upon running <code>pip install trains<\/code> in my virtual env<\/p>\n<p>I am getting<\/p>\n<pre><code>    ERROR: Command errored out with exit status 1:\n     command: \/home\/epdadmin\/noam\/code\/venv_linux\/bin\/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'\/tmp\/pip-install-owzh8lnl\/retrying\/setup.py'&quot;'&quot;'; __file__='&quot;'&quot;'\/tmp\/pip-install-owzh8lnl\/retrying\/setup.py'&quot;'&quot;';f=getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(__file__);code=f.read().replace('&quot;'&quot;'\\r\\n'&quot;'&quot;', '&quot;'&quot;'\\n'&quot;'&quot;');f.close();exec(compile(code, __file__, '&quot;'&quot;'exec'&quot;'&quot;'))' install --record \/tmp\/pip-record-lxz5t8pu\/install-record.txt --single-version-externally-managed --compile --install-headers \/home\/epdadmin\/noam\/code\/venv_linux\/include\/site\/python3.8\/retrying\n         cwd: \/tmp\/pip-install-owzh8lnl\/retrying\/\n    Complete output (10 lines):\n    running install\n    running build\n    running build_py\n    creating build\n    creating build\/lib\n    copying retrying.py -&gt; build\/lib\n    running install_lib\n    copying build\/lib\/retrying.py -&gt; \/home\/epdadmin\/noam\/code\/venv_linux\/lib\/python3.8\/site-packages\n    byte-compiling \/home\/epdadmin\/noam\/code\/venv_linux\/lib\/python3.8\/site-packages\/retrying.py to retrying.cpython-38.pyc\n    error: [Errno 13] Permission denied: '\/home\/epdadmin\/noam\/code\/venv_linux\/lib\/python3.8\/site-packages\/__pycache__\/retrying.cpython-38.pyc.139678407381360'\n    ----------------------------------------\nERROR: Command errored out with exit status 1: \/home\/epdadmin\/noam\/code\/venv_linux\/bin\/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'\/tmp\/pip-install-owzh8lnl\/retrying\/setup.py'&quot;'&quot;'; __file__='&quot;'&quot;'\/tmp\/pip-install-owzh8lnl\/retrying\/setup.py'&quot;'&quot;';f=getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(__file__);code=f.read().replace('&quot;'&quot;'\\r\\n'&quot;'&quot;', '&quot;'&quot;'\\n'&quot;'&quot;');f.close();exec(compile(code, __file__, '&quot;'&quot;'exec'&quot;'&quot;'))' install --record \/tmp\/pip-record-lxz5t8pu\/install-record.txt --single-version-externally-managed --compile --install-headers \/home\/epdadmin\/noam\/code\/venv_linux\/include\/site\/python3.8\/retrying Check the logs for full command output.\n<\/code><\/pre>\n<p>I know that <a href=\"https:\/\/stackoverflow.com\/questions\/15028648\/is-it-acceptable-and-safe-to-run-pip-install-under-sudo\">I am not supposed to run under sudo when using a venv<\/a>, so I don't really understand the problem<\/p>\n<p>running for example <code>pip install pandas<\/code> does work.<\/p>\n<p>Python 3.8<\/p>\n<p>How to install trains?<\/p>\n<hr \/>\n<p>EDIT:<\/p>\n<p>running <code>pip install trains --user<\/code> or <code>pip install --user trains<\/code> gives<\/p>\n<pre><code>ERROR: Can not perform a '--user' install. User site-packages are not visible in this virtualenv.\n<\/code><\/pre>",
        "Question_answer_count":3,
        "Question_comment_count":3,
        "Question_creation_date":"2020-10-11 15:44:31.093000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-12-30 18:45:56.110000 UTC",
        "Question_score":1,
        "Question_tags":"python|pip|trains|clearml",
        "Question_view_count":1031,
        "Owner_creation_date":"2011-08-25 22:58:29.233000 UTC",
        "Owner_last_access_date":"2022-09-24 23:30:23.147000 UTC",
        "Owner_location":"Technion, Israel",
        "Owner_reputation":18777,
        "Owner_up_votes":2376,
        "Owner_down_votes":137,
        "Owner_views":2000,
        "Answer_body":"<p>The problem was a permissions problem for the venv.\nAnother problem was trains required some packages that were not yet available with wheels on Python3.8, so I had to downgrade Python to 3.7<\/p>\n<p>That venv was created using Pycharm, and for some reason it was created with low permissions.<\/p>\n<p>There was probably a way to elevate its permissions, but instead I just deleted it and created another one using command line by<\/p>\n<pre><code>python -m virtualenv --python=\/usr\/bin\/python3.7 venv\n<\/code><\/pre>\n<p>And now <code>pip install trains<\/code> worked.<\/p>\n<p>Very annoying.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-10-15 13:18:50.970000 UTC",
        "Answer_last_edit_date":"2020-11-02 09:09:43.097000 UTC",
        "Answer_score":1.0,
        "Question_valid_tags":[
            "clearml"
        ]
    },
    {
        "Question_id":70644326,
        "Question_title":"wandb.plot.line does not work and it just shows a table",
        "Question_body":"<p>I used this example that was provided by WandB. However, the web interface just shows a table instead of a figure.<\/p>\n<pre><code>data = [[i, random.random() + math.sin(i \/ 10)] for i in range(100)]\n        table = wandb.Table(data=data, columns=[&quot;step&quot;, &quot;height&quot;])\n        wandb.log({'line-plot1': wandb.plot.line(table, &quot;step&quot;, &quot;height&quot;)})\n<\/code><\/pre>\n<p>This is a screenshot from WandB's web interface:\n<a href=\"https:\/\/i.stack.imgur.com\/INmPU.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/INmPU.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Also, I have the same problem with other kinds of figures and charts that use a table.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-01-09 18:36:40.317000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|deep-learning|pytorch|wandb",
        "Question_view_count":184,
        "Owner_creation_date":"2015-10-24 20:44:04.550000 UTC",
        "Owner_last_access_date":"2022-09-24 23:10:59.603000 UTC",
        "Owner_location":"Iran, Canada",
        "Owner_reputation":331,
        "Owner_up_votes":113,
        "Owner_down_votes":3,
        "Owner_views":40,
        "Answer_body":"<blockquote>\n<p>X-Post from the wandb forum<\/p>\n<\/blockquote>\n<p><a href=\"https:\/\/community.wandb.ai\/t\/wandb-plot-confusion-matrix-just-show-a-table\/1744\" rel=\"nofollow noreferrer\">https:\/\/community.wandb.ai\/t\/wandb-plot-confusion-matrix-just-show-a-table\/1744<\/a><\/p>\n<p>If you click the section called \u201cCustom Charts\u201d above the Table, it\u2019ll show the line plot that you\u2019ve logged.<\/p>\n<p>Logging the Table also is expected behaviour, because this will allow users to interactively explore the logged data in a W&amp;B Table after logging it.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-01-10 10:14:57.277000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_valid_tags":[
            "wandb"
        ]
    },
    {
        "Question_id":60037376,
        "Question_title":"Can't use Keras CSVLogger callbacks in Sagemaker script mode. It fails to write the log file on S3 ( error - No such file or directory )",
        "Question_body":"<p>I have this script where I want to get the callbacks to a separate CSV file in sagemaker custom script docker container. But when I try to run in local mode, it fails giving the following error. I have a hyper-parameter tuning job(HPO) to run and this keeps giving me errors. I need to get this local mode run correctly before doing the HPO. <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/de522.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/de522.jpg\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>In the notebook I use the following code.<\/p>\n\n<pre><code>from sagemaker.tensorflow import TensorFlow\n\ntf_estimator = TensorFlow(entry_point='lstm_model.py', \n                          role=role,\n                          code_location=custom_code_upload_location,\n                          output_path=model_artifact_location+'\/',\n                          train_instance_count=1, \n                          train_instance_type='local',\n                          framework_version='1.12', \n                          py_version='py3',\n                          script_mode=True,\n                          hyperparameters={'epochs': 1},\n                          base_job_name='hpo-lstm-local-test'\n                         )\n\ntf_estimator.fit({'training': training_input_path, 'validation': validation_input_path})\n<\/code><\/pre>\n\n<p>In my <strong>lstm_model.py<\/strong> script the following code is used.<\/p>\n\n<pre><code>lgdir = os.path.join(model_dir, 'callbacks_log.csv')\ncsv_logger = CSVLogger(lgdir, append=True)\n\nregressor.fit(x_train, y_train, batch_size=batch_size,\n              validation_data=(x_val, y_val), \n              epochs=epochs,\n              verbose=2,\n              callbacks=[csv_logger]\n              )\n<\/code><\/pre>\n\n<p>I tried creating a file before hand like shown below using tensorflow backend. But it doesn't create a file. ( K : tensorflow Backend, tf: tensorflow )<\/p>\n\n<pre><code>filename = tf.Variable(lgdir , tf.string)\ncontent = tf.Variable(\"\", tf.string)\nsess = K.get_session()\ntf.io.write_file(filename, content)\n<\/code><\/pre>\n\n<p>I can't use any other packages like pandas to create the file as the TensorFlow docker container in SageMaker for custom scripts doesn't provide them. They give only a limited amount of packages. <\/p>\n\n<p>Is there a way I can write the csv file to the S3 bucket location, before the fit method try to write the callback. Or is that the solution to the problem? I am not sure. <\/p>\n\n<p>If you can even suggest other suggestions to get callbacks, I would even accept that answer. But it should be worth the effort. <\/p>\n\n<p>This docker image is really narrowing the scope. <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-02-03 10:31:01.867000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"tensorflow|keras|amazon-sagemaker",
        "Question_view_count":412,
        "Owner_creation_date":"2018-01-28 13:47:46.417000 UTC",
        "Owner_last_access_date":"2022-09-23 19:02:42.520000 UTC",
        "Owner_location":null,
        "Owner_reputation":65,
        "Owner_up_votes":89,
        "Owner_down_votes":0,
        "Owner_views":18,
        "Answer_body":"<p>Well for starters, you can always make your own docker image using the Tensorflow image as a base. I work in Tensorflow 2.0 so this will be slightly different for you but here is an example of my image pattern:<\/p>\n\n<pre><code># Downloads the TensorFlow library used to run the Python script\nFROM tensorflow\/tensorflow:2.0.0a0 # you would use the equivalent for your TF version\n\n# Contains the common functionality necessary to create a container compatible with Amazon SageMaker\nRUN pip install sagemaker-containers -q \n\n# Wandb allows us to customize and centralize logging while maintaining open-source agility\nRUN pip install wandb -q # here you would install pandas\n\n# Copies the training code inside the container to the design pattern created by the Tensorflow estimator\n# here you could copy over a callbacks csv\nCOPY mnist-2.py \/opt\/ml\/code\/mnist-2.py \nCOPY callbacks.py \/opt\/ml\/code\/callbacks.py \nCOPY wandb_setup.sh \/opt\/ml\/code\/wandb_setup.sh\n\n# Set the login script as the entry point\nENV SAGEMAKER_PROGRAM wandb_setup.sh # here you would instead launch lstm_model.py\n<\/code><\/pre>\n\n<p>I believe you are looking for a pattern similar to this, but I prefer to log all of my model data using <a href=\"https:\/\/www.wandb.com\/\" rel=\"nofollow noreferrer\">Weights and Biases<\/a>. They're a little out of data on their SageMaker integration but I'm actually in the midst of writing an updated tutorial for them. It should certainly be finished this month and include logging and comparing runs from hyperparameter tuning jobs<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2020-03-10 17:15:47.413000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":60778546,
        "Question_title":"pip install azureml-sdk with latest patches to underlying libraries",
        "Question_body":"<p>How do I upgrade <code>azureml-sdk<\/code> such that the newest release of <code>azureml-core<\/code>, <code>1.1.5.5<\/code>, is installed? \nIf <code>azureml-sdk<\/code> is not installed, <code>pip install --upgrade azureml-sdk<\/code> will install <code>azureml-core==1.1.5.5<\/code>. If it is already installed, then it won't.<\/p>\n\n<pre><code>$ pip list --format=freeze | grep 'azureml-core'`\n&gt; azureml-core==1.1.5.1\n$ pip install --upgrade azureml-sdk[interpret,notebooks]\n$ pip list --format=freeze | grep 'azureml-core'`\n&gt; azureml-core==1.1.5.1\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_date":"2020-03-20 17:04:10.063000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-03-20 17:20:28.660000 UTC",
        "Question_score":0,
        "Question_tags":"python|pip|azure-machine-learning-service",
        "Question_view_count":1142,
        "Owner_creation_date":"2014-07-15 20:45:20.427000 UTC",
        "Owner_last_access_date":"2022-09-23 15:42:13.100000 UTC",
        "Owner_location":"Seattle, WA, USA",
        "Owner_reputation":3359,
        "Owner_up_votes":1187,
        "Owner_down_votes":14,
        "Owner_views":555,
        "Answer_body":"<p>You can use the eager strategy to force an upgrade of requirements:<\/p>\n\n<pre><code>pip install -U --upgrade-strategy eager azureml-sdk\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-03-20 20:23:55.707000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":41303697,
        "Question_title":"Use an existing Gateway with Azure Machine Learning?",
        "Question_body":"<p>We want to access an onprem SQL database with an existing Gateway, is that possible in AML?  The tool only seems to allow creating new gateways.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2016-12-23 15:03:02.510000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"azure-machine-learning-studio",
        "Question_view_count":39,
        "Owner_creation_date":"2011-02-03 15:24:02.860000 UTC",
        "Owner_last_access_date":"2022-09-14 18:59:29.800000 UTC",
        "Owner_location":null,
        "Owner_reputation":524,
        "Owner_up_votes":781,
        "Owner_down_votes":2,
        "Owner_views":48,
        "Answer_body":"<p>Confirmed that this is not possible, AML only allows use of AML-created gateways.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2017-03-01 19:48:19.940000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":62975940,
        "Question_title":"How to interpret the Throughput metric for Semantic Segmentation?",
        "Question_body":"<p>I am beginning to train a semantic segmentation model in AWS Sagermaker and they provide the following metrics for the output. I understand mIOU, loss, and pixel accuracy, but I do not know what throughput is or how to interpret it. Please see the image below and let me know if you need additional information.\n<a href=\"https:\/\/i.stack.imgur.com\/dpsDp.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/dpsDp.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-07-19 03:32:20.180000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-07-19 04:07:22.613000 UTC",
        "Question_score":0,
        "Question_tags":"metrics|evaluation|amazon-sagemaker|semantic-segmentation",
        "Question_view_count":67,
        "Owner_creation_date":"2015-12-07 18:34:11.820000 UTC",
        "Owner_last_access_date":"2021-08-20 05:18:27.117000 UTC",
        "Owner_location":null,
        "Owner_reputation":693,
        "Owner_up_votes":47,
        "Owner_down_votes":0,
        "Owner_views":56,
        "Answer_body":"<p>Throughput is reported in records per second (i.e. images per second). It shows how fast the algorithm can iterate over training or validation data. For example, with a throughput of 30 records\/sec it would take a minute to iterate over 1800 images.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-08-07 17:00:25.617000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":28590690,
        "Question_title":"Azure ML App - Complete Experince - Train automatically and Consume",
        "Question_body":"<p>I played a bit around with Azure ML studio. So as I understand the process goes like this:<\/p>\n\n<p>a) Create training experiment. Train it with data. <\/p>\n\n<p>b) Create Scoring experiment. This will include the trained model from the training experiment. Expose this as a service to be consumed over REST.<\/p>\n\n<p>Maybe a stupid question but what is the recommended way to get the complete experience like the one i get when I use an app like <a href=\"https:\/\/datamarket.azure.com\/dataset\/amla\/mba\" rel=\"nofollow\">https:\/\/datamarket.azure.com\/dataset\/amla\/mba<\/a> (Frequently Bought Together API built with Azure Machine Learning). <\/p>\n\n<p>I mean the following:<\/p>\n\n<p>a) Expose 2 or more services - one to train the model and the other to consume (test) the trained model. <\/p>\n\n<p>b) User periodically sends training data to train the model <\/p>\n\n<p>c) The trained model\/models now gets saved available for consumption<\/p>\n\n<p>d) User is now able to send a dataframe to get the predicted results.<\/p>\n\n<p>Is there an additional wrapper that needs to be built?<\/p>\n\n<p>If there is a link documenting this please point me to the same. <\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2015-02-18 18:05:24.780000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2015-02-18 19:04:32.933000 UTC",
        "Question_score":2,
        "Question_tags":"azure|azure-machine-learning-studio",
        "Question_view_count":769,
        "Owner_creation_date":"2015-02-18 17:59:15.667000 UTC",
        "Owner_last_access_date":"2016-07-08 12:44:42.013000 UTC",
        "Owner_location":null,
        "Owner_reputation":33,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Answer_body":"<p>The Azure ML retraining API is designed to handle the workflow you describe:<\/p>\n\n<p><a href=\"http:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-retrain-models-programmatically\/\" rel=\"nofollow\">http:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-retrain-models-programmatically\/<\/a><\/p>\n\n<p>Hope this helps,<\/p>\n\n<p>Roope - Microsoft Azure ML Team<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2015-03-03 23:52:28.317000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":66574569,
        "Question_title":"AWS Sagemaker KeyError: 'SM_CHANNEL_TRAINING' when tuning hyperparameters",
        "Question_body":"<p>When I try to use hyperparameters tuning on Sagemaker I get this error:<\/p>\n<pre><code>UnexpectedStatusException: Error for HyperParameterTuning job imageclassif-job-10-21-47-43: Failed. Reason: No training job succeeded after 5 attempts. Please take a look at the training job failures to get more details.\n<\/code><\/pre>\n<p>When I look up the logs on CloudWatch all 5 failed training jobs have the same error at the end:<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;\/usr\/lib\/python3.5\/runpy.py&quot;, line 184, in _run_module_as_main\n    &quot;__main__&quot;, mod_spec)\n  File &quot;\/usr\/lib\/python3.5\/runpy.py&quot;, line 85, in _run_code\n    exec(code, run_globals)\n  File &quot;\/opt\/ml\/code\/train.py&quot;, line 117, in &lt;module&gt;\n    parser.add_argument('--data-dir', type=str, default=os.environ['SM_CHANNEL_TRAINING'])\n  File &quot;\/usr\/lib\/python3.5\/os.py&quot;, line 725, in __getitem__\n    raise KeyError(key) from None\n<\/code><\/pre>\n<p>and<\/p>\n<pre><code>KeyError: 'SM_CHANNEL_TRAINING'\n<\/code><\/pre>\n<p>The problem is at the Step 4 of the project: <a href=\"https:\/\/github.com\/petrooha\/Deploying-LSTM\/blob\/main\/SageMaker%20Project.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/petrooha\/Deploying-LSTM\/blob\/main\/SageMaker%20Project.ipynb<\/a><\/p>\n<p>Would hihgly appreciate any hints on where to look next<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-03-10 23:53:21.190000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"python-3.x|deep-learning|lstm|amazon-sagemaker|hyperparameters",
        "Question_view_count":1121,
        "Owner_creation_date":"2020-05-02 14:27:01.780000 UTC",
        "Owner_last_access_date":"2021-07-19 22:38:45.667000 UTC",
        "Owner_location":"Miami Beach, \u0424\u043b\u043e\u0440\u0438\u0434\u0430, \u0421\u0428\u0410",
        "Owner_reputation":21,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":10,
        "Answer_body":"<p>In your <code>train.py<\/code> file, changing the environment variable from<\/p>\n<p><code>parser.add_argument('--data-dir', type=str, default=os.environ['SM_CHANNEL_TRAINING'])<\/code><\/p>\n<p>to<\/p>\n<p><code>parser.add_argument('--data-dir', type=str, default=os.environ['SM_CHANNEL_TRAIN'])<\/code> should address the issue.<\/p>\n<p>This is the case with Torch's framework_version 1.3.1 but other versions might also be affected. Here is the <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/1292\" rel=\"nofollow noreferrer\">link<\/a> for your reference.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-07-05 18:00:26.043000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":51391639,
        "Question_title":"Is it possible to predict in sagemaker without using s3",
        "Question_body":"<p>I have a .pkl which I would like to put into production. I would like to do a daily query of my SQL server and do a prediction on about 1000 rows. The <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/how-it-works-batch.html\" rel=\"nofollow noreferrer\">documentation<\/a> implies I have to load the daily data into s3. Is there a way around this? It should be able to fit in memory no problem. <\/p>\n\n<p>The answer to \" <a href=\"https:\/\/stackoverflow.com\/questions\/48319893\/is-there-some-kind-of-persistent-local-storage-in-aws-sagemaker-model-training\">is there some kind of persistent local storage in aws sagemaker model training?<\/a> \" says that \"<em>The notebook instance is coming with a local EBS (5GB) that you can use to copy some data into it and run the fast development iterations without copying the data every time from S3.<\/em>\" The 5GB could be enough but I am not sure you can actually run from a notebook in this manner. If I set up a VPN could I just query using pyodbc?<\/p>\n\n<p>Is there sagemaker integration with AWS Lambda? That in combination with a docker container would suit my needs.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_date":"2018-07-17 23:46:56.253000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2018-07-18 05:51:18.607000 UTC",
        "Question_score":2,
        "Question_tags":"amazon-web-services|amazon-s3|aws-lambda|amazon-sagemaker",
        "Question_view_count":1576,
        "Owner_creation_date":"2014-05-17 08:37:27.157000 UTC",
        "Owner_last_access_date":"2022-09-21 19:31:38.353000 UTC",
        "Owner_location":null,
        "Owner_reputation":4342,
        "Owner_up_votes":318,
        "Owner_down_votes":25,
        "Owner_views":315,
        "Answer_body":"<p>While you need to to specify a s3 \"folder\" as input, this folder can contain only a dummy file. \nAlso if you bring your own docker container for training like in <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/scikit_bring_your_own\" rel=\"nofollow noreferrer\">this example<\/a>, you can do pretty much everthing in it. So you could do your daily query inside your docker container, because they have access to the internet. <\/p>\n\n<p>Also inside this container you have access to all the other aws services. Your access is defined by the role you're passing to your training job.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2018-07-18 13:07:32.070000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":72665109,
        "Question_title":"dvc.exceptions.CyclicGraphError: Pipeline has a cycle involving: load_extract_save",
        "Question_body":"<pre><code>stages:\n  load_extract_save: \n    cmd: python src\/stage_01_load_extract_save.py --config=config\/config.yaml\n    deps:\n      - config\/config.yaml\n      - src\/utils\/all_utils.py\n      - src\/stage_01_load_extract_save.py\n      - artifacts\/data\n    outs:\n      - artifacts\/data\n      - artifacts\/clean_data\/X.npy\n      - artifacts\/clean_data\/Y.npy\n\n  train_test_split_save:\n    cmd: python src\/stage_02_train_test_split_save.py --config=config\/config.yaml --params=params.yaml\n    deps:\n      - artifacts\/clean_data\/X.npy\n      - artifacts\/clean_data\/Y.npy\n      - src\/utils\/all_utils.py\n      - params.yaml\n      - config\/config.yaml\n      - src\/stage_02_train_test_split_save.py\n    outs:\n      - artifacts\/train_data\/X_train.npy\n      - artifacts\/train_data\/Y_train.npy\n      - artifacts\/test_data\/X_test.npy\n      - artifacts\/test_data\/Y_test.npy\n  \n  train_model:\n    cmd:  python src\/stage_03_train.py --config=config\/config.yaml --params=params.yaml\n    deps:\n      - artifacts\/train_data\/X_train.npy\n      - artifacts\/train_data\/Y_train.npy\n      - artifacts\/test_data\/X_test.npy\n      - artifacts\/test_data\/Y_test.npy\n      - src\/stage_03_train.py\n      - src\/utils\/all_utils.py\n      - config\/config.yaml\n      - params.yaml\n    outs:\n      - artifacts\/checkpoints\n      - artifacts\/model\n  \n  metrics:\n    cmd: python src\/stage_04_metrics.py --config=config\/config.yaml\n    deps:\n      - src\/stage_04_metrics.py\n      - config\/config.yaml\n      - src\/utils\/all_utils.py\n      - artifacts\/test_data\/X_test.npy\n      - artifacts\/test_data\/Y_test.npy\n      - artifacts\/checkpoints\n      - artifacts\/model\n    outs:\n      - confusion_matrix.png\n<\/code><\/pre>\n<p>This is my DVC.yaml.<\/p>\n<p>I have created Github workflow to reproduce it, but whenever I run it it gives me the following error - <code>... ERROR: Pipeline has a cycle involving: load_extract_save.<\/code><\/p>\n<p>The error looks <a href=\"https:\/\/i.stack.imgur.com\/1mt1Y.png\" rel=\"nofollow noreferrer\">like this<\/a>.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-06-17 21:48:47.920000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-06-18 03:55:59.503000 UTC",
        "Question_score":2,
        "Question_tags":"python|dvc",
        "Question_view_count":38,
        "Owner_creation_date":"2020-04-19 15:08:15.270000 UTC",
        "Owner_last_access_date":"2022-09-22 19:13:07.327000 UTC",
        "Owner_location":null,
        "Owner_reputation":23,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":"<p>Stage <code>load_extract_save<\/code> both outputs and depends on the same path (<code>artifacts\/data<\/code>). That's a cycle.<\/p>\n<p>Pipeline structures should be <a href=\"https:\/\/dvc.org\/doc\/command-reference\/dag#directed-acyclic-graph\" rel=\"nofollow noreferrer\">directed <strong>acyclical<\/strong> graphs<\/a>, otherwise <code>dvc repro<\/code> could execute that stage over and over forever.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-06-18 03:51:17.717000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":3.0,
        "Question_valid_tags":[
            "dvc"
        ]
    },
    {
        "Question_id":65991587,
        "Question_title":"AzureDevOPS ML Error: We could not find config.json in: \/home\/vsts\/work\/1\/s or in its parent directories",
        "Question_body":"<p>I am trying to create an Azure DEVOPS ML Pipeline. The following code works 100% fine on Jupyter Notebooks, but when I run it in Azure Devops I get this error:<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;src\/my_custom_package\/data.py&quot;, line 26, in &lt;module&gt;\n    ws = Workspace.from_config()\n  File &quot;\/opt\/hostedtoolcache\/Python\/3.8.7\/x64\/lib\/python3.8\/site-packages\/azureml\/core\/workspace.py&quot;, line 258, in from_config\n    raise UserErrorException('We could not find config.json in: {} or in its parent directories. '\nazureml.exceptions._azureml_exception.UserErrorException: UserErrorException:\n    Message: We could not find config.json in: \/home\/vsts\/work\/1\/s or in its parent directories. Please provide the full path to the config file or ensure that config.json exists in the parent directories.\n    InnerException None\n    ErrorResponse \n{\n    &quot;error&quot;: {\n        &quot;code&quot;: &quot;UserError&quot;,\n        &quot;message&quot;: &quot;We could not find config.json in: \/home\/vsts\/work\/1\/s or in its parent directories. Please provide the full path to the config file or ensure that config.json exists in the parent directories.&quot;\n    }\n}\n<\/code><\/pre>\n<p>The code is:<\/p>\n<pre><code>#import\nfrom sklearn.model_selection import train_test_split\nfrom azureml.core.workspace import Workspace\nfrom azureml.train.automl import AutoMLConfig\nfrom azureml.core.compute import ComputeTarget, AmlCompute\nfrom azureml.core.compute_target import ComputeTargetException\nfrom azureml.core.experiment import Experiment\nfrom datetime import date\nfrom azureml.core import Workspace, Dataset\n\n\n\nimport pandas as pd\nimport numpy as np\nimport logging\n\n#getdata\nsubscription_id = 'mysubid'\nresource_group = 'myrg'\nworkspace_name = 'mlplayground'\nworkspace = Workspace(subscription_id, resource_group, workspace_name)\ndataset = Dataset.get_by_name(workspace, name='correctData')\n\n\n#auto ml\nws = Workspace.from_config()\n\n\nautoml_settings = {\n    &quot;iteration_timeout_minutes&quot;: 2880,\n    &quot;experiment_timeout_hours&quot;: 48,\n    &quot;enable_early_stopping&quot;: True,\n    &quot;primary_metric&quot;: 'spearman_correlation',\n    &quot;featurization&quot;: 'auto',\n    &quot;verbosity&quot;: logging.INFO,\n    &quot;n_cross_validations&quot;: 5,\n    &quot;max_concurrent_iterations&quot;: 4,\n    &quot;max_cores_per_iteration&quot;: -1,\n}\n\n\n\ncpu_cluster_name = &quot;computecluster&quot;\ncompute_target = ComputeTarget(workspace=ws, name=cpu_cluster_name)\nprint(compute_target)\nautoml_config = AutoMLConfig(task='regression',\n                             compute_target = compute_target,\n                             debug_log='automated_ml_errors.log',\n                             training_data = dataset,\n                             label_column_name=&quot;paidInDays&quot;,\n                             **automl_settings)\n\ntoday = date.today()\nd4 = today.strftime(&quot;%b-%d-%Y&quot;)\n\nexperiment = Experiment(ws, &quot;myexperiment&quot;+d4)\nremote_run = experiment.submit(automl_config, show_output = True)\n\nfrom azureml.widgets import RunDetails\nRunDetails(remote_run).show()\n\nremote_run.wait_for_completion()\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2021-02-01 11:05:33.493000 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"python|azure-machine-learning-service",
        "Question_view_count":2339,
        "Owner_creation_date":"2011-04-05 19:05:03.093000 UTC",
        "Owner_last_access_date":"2022-09-16 12:42:27.473000 UTC",
        "Owner_location":"Brussels, B\u00e9lgica",
        "Owner_reputation":30340,
        "Owner_up_votes":1667,
        "Owner_down_votes":79,
        "Owner_views":2937,
        "Answer_body":"<p>There is something weird happening on your code, you are getting the data from a first workspace (<code>workspace = Workspace(subscription_id, resource_group, workspace_name)<\/code>), then using the resources from a second one (<code>ws = Workspace.from_config()<\/code>). I would suggest avoiding having code relying on two different workspaces, especially when you know that an underlying datasource can be registered (linked) to multiple workspaces (<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-data#create-and-register-datastores\" rel=\"nofollow noreferrer\">documentation<\/a>).<\/p>\n<p>In general using a <code>config.json<\/code> file when instantiating a <code>Workspace<\/code> object will result in an interactive authentication. When your code will be processed and you will have a log asking you to reach a specific URL and enter a code. This will use your Microsoft account to verify that you are authorized to access the Azure resource (in this case your <code>Workspace('mysubid', 'myrg', 'mlplayground')<\/code>). This has its limitations when you start deploying the code onto virtual machines or agents, you will not always manually check the logs, access the URL and authenticate yourself.<\/p>\n<p>For this matter it is strongly recommended setting up more advanced authentication methods and personally I would suggest using the service principal one since it is simple, convinient and secure if done properly.\nYou can follow Azure's official documentation <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-setup-authentication#configure-a-service-principal\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-02-09 10:17:20.130000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":70928144,
        "Question_title":"Multiple users in DVC",
        "Question_body":"<p>I would like to ask if it is possible to use DVC with several accounts on the same machine. At the moment, all commands (<code>dvc pull<\/code>, <code>dvc push<\/code>, ...) are executed under my name. But after several people joined this project too, I do not want them to execute commands under my name.<\/p>\n<p>When I was alone on this project I generated ssh key:<\/p>\n<pre><code>ssh-keygen\n<\/code><\/pre>\n<p>Connected to server where DVC remote data is stored:<\/p>\n<pre><code>ssh-copy-id username@server_IP\n<\/code><\/pre>\n<p>Created config file which lets me execute all <code>dvc<\/code> commands using ssh:<\/p>\n<pre><code>[core]\n    remote = storage_server\n['remote &quot;storage_server&quot;']\n    url = ssh:\/\/username@server_IP:\/home\/DVC_remote\/DVC_project\n<\/code><\/pre>\n<p>What I should do so that several people could execute commands on their own name?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-01-31 15:03:42.880000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"dvc",
        "Question_view_count":181,
        "Owner_creation_date":"2016-03-08 20:35:01.700000 UTC",
        "Owner_last_access_date":"2022-09-15 06:24:00.440000 UTC",
        "Owner_location":null,
        "Owner_reputation":585,
        "Owner_up_votes":22,
        "Owner_down_votes":0,
        "Owner_views":54,
        "Answer_body":"<p>You need to make the &quot;username&quot; part of the config personalized based on who is running the command. There are a few options to do this (based on <a href=\"https:\/\/dvc.org\/doc\/command-reference\/remote\/modify#available-parameters-per-storage-type\" rel=\"nofollow noreferrer\">this document<\/a>, see the SSH part):<\/p>\n<h2>Basic options are:<\/h2>\n<ul>\n<li>User defined in the SSH config file (e.g. <code>~\/.ssh\/config<\/code>) for this host (URL);<\/li>\n<li>Current system user;<\/li>\n<\/ul>\n<p>So, the simplest even options could be just remove it from the URL and rely on the current system user?<\/p>\n<h2>Local (git-ignored or per-project DVC config) config<\/h2>\n<p>You could do is to remove the <code>username<\/code> part from the <code>url<\/code> and run something like this:<\/p>\n<pre><code>dvc remote modify --local storage_server user username\n<\/code><\/pre>\n<p><code>--local<\/code> here means that DVC will create a separate additional config that will be ignored by Git. This way if every user runs this command in every project they use they will customize the username.<\/p>\n<hr \/>\n<p>Let me know if that helps or something doesn't work. I'll try to help.<\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_date":"2022-01-31 17:45:43.317000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "dvc"
        ]
    },
    {
        "Question_id":70833499,
        "Question_title":"AzureML Environment for Inference : can't add pip packages to dependencies",
        "Question_body":"<p>I can't find the proper way to add dependencies to my Azure Container Instance for ML Inference.<\/p>\n<p>I basically started by following this tutorial : <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-train-deploy-notebook\" rel=\"nofollow noreferrer\">Train and deploy an image classification model with an example Jupyter Notebook<\/a><\/p>\n<p>It works fine.<\/p>\n<p>Now I want to deploy my trained TensorFlow model for inference. I tried many ways, but I was never able to add python dependencies to the Environment.<\/p>\n<h1>From the TensorFlow curated environment<\/h1>\n<p>Using <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/resource-curated-environments#inference-curated-environments-and-prebuilt-docker-images\" rel=\"nofollow noreferrer\">AzureML-tensorflow-2.4-ubuntu18.04-py37-cpu-inference<\/a> :<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core import Workspace\n\n\n# connect to your workspace\nws = Workspace.from_config()\n\n# names\nexperiment_name = &quot;my-experiment&quot;\nmodel_name = &quot;my-model&quot;\nenv_version=&quot;1&quot;\nenv_name=&quot;my-env-&quot;+env_version\nservice_name = str.lower(model_name + &quot;-service-&quot; + env_version)\n\n\n# create environment for the deploy\nfrom azureml.core.environment import Environment, DEFAULT_CPU_IMAGE\nfrom azureml.core.conda_dependencies import CondaDependencies\nfrom azureml.core.webservice import AciWebservice\n\n# get a curated environment\nenv = Environment.get(\n    workspace=ws, \n    name=&quot;AzureML-tensorflow-2.4-ubuntu18.04-py37-cpu-inference&quot;,\n# )\ncustom_env = env.clone(env_name)\ncustom_env.inferencing_stack_version='latest'\n\n# add packages\nconda_dep = CondaDependencies()\npython_packages = ['joblib', 'numpy', 'os', 'json', 'tensorflow']\nfor package in python_packages:\n    conda_dep.add_pip_package(package)\n    conda_dep.add_conda_package(package)\n\n# Adds dependencies to PythonSection of env\ncustom_env.python.user_managed_dependencies=True\ncustom_env.python.conda_dependencies=conda_dep\n\ncustom_env.register(workspace=ws)\n\n# create deployment config i.e. compute resources\naciconfig = AciWebservice.deploy_configuration(\n    cpu_cores=1,\n    memory_gb=1,\n    tags={&quot;experiment&quot;: experiment_name, &quot;model&quot;: model_name},\n)\n\nfrom azureml.core.model import InferenceConfig\nfrom azureml.core.model import Model\n\n# get the registered model\nmodel = Model(ws, model_name)\n\n# create an inference config i.e. the scoring script and environment\ninference_config = InferenceConfig(entry_script=&quot;score.py&quot;, environment=custom_env)\n\n# deploy the service\nservice = Model.deploy(\n    workspace=ws,\n    name=service_name,\n    models=[model],\n    inference_config=inference_config,\n    deployment_config=aciconfig,\n)\n\nservice.wait_for_deployment(show_output=True)\n<\/code><\/pre>\n<p>I get the following log :<\/p>\n<pre><code>\nAzureML image information: tensorflow-2.4-ubuntu18.04-py37-cpu-inference:20220110.v1\n\n\nPATH environment variable: \/opt\/miniconda\/envs\/amlenv\/bin:\/opt\/miniconda\/bin:\/usr\/local\/sbin:\/usr\/local\/bin:\/usr\/sbin:\/usr\/bin:\/sbin:\/bin\nPYTHONPATH environment variable: \n\nPip Dependencies\n---------------\nEdgeHubConnectionString and IOTEDGE_IOTHUBHOSTNAME are not set. Exiting...\n2022-01-24T10:21:09,855130300+00:00 - iot-server\/finish 1 0\n2022-01-24T10:21:09,856870100+00:00 - Exit code 1 is normal. Not restarting iot-server.\nabsl-py==0.15.0\napplicationinsights==0.11.10\nastunparse==1.6.3\nazureml-inference-server-http==0.4.2\ncachetools==4.2.4\ncertifi==2021.10.8\ncharset-normalizer==2.0.10\nclick==8.0.3\nFlask==1.0.3\nflatbuffers==1.12\ngast==0.3.3\ngoogle-auth==2.3.3\ngoogle-auth-oauthlib==0.4.6\ngoogle-pasta==0.2.0\ngrpcio==1.32.0\ngunicorn==20.1.0\nh5py==2.10.0\nidna==3.3\nimportlib-metadata==4.10.0\ninference-schema==1.3.0\nitsdangerous==2.0.1\nJinja2==3.0.3\nKeras-Preprocessing==1.1.2\nMarkdown==3.3.6\nMarkupSafe==2.0.1\nnumpy==1.19.5\noauthlib==3.1.1\nopt-einsum==3.3.0\npandas==1.1.5\nprotobuf==3.19.1\npyasn1==0.4.8\npyasn1-modules==0.2.8\npython-dateutil==2.8.2\npytz==2021.3\nrequests==2.27.1\nrequests-oauthlib==1.3.0\nrsa==4.8\nsix==1.15.0\ntensorboard==2.7.0\ntensorboard-data-server==0.6.1\ntensorboard-plugin-wit==1.8.1\ntensorflow==2.4.0\ntensorflow-estimator==2.4.0\ntermcolor==1.1.0\ntyping-extensions==3.7.4.3\nurllib3==1.26.8\nWerkzeug==2.0.2\nwrapt==1.12.1\nzipp==3.7.0\n\n\nEntry script directory: \/var\/azureml-app\/.\n\nDynamic Python package installation is disabled.\nStarting AzureML Inference Server HTTP.\n\nAzure ML Inferencing HTTP server v0.4.2\n\n\nServer Settings\n---------------\nEntry Script Name: score.py\nModel Directory: \/var\/azureml-app\/azureml-models\/my-model\/1\nWorker Count: 1\nWorker Timeout (seconds): 300\nServer Port: 31311\nApplication Insights Enabled: false\nApplication Insights Key: None\n\n\nServer Routes\n---------------\nLiveness Probe: GET   127.0.0.1:31311\/\nScore:          POST  127.0.0.1:31311\/score\n\nStarting gunicorn 20.1.0\nListening at: http:\/\/0.0.0.0:31311 (69)\nUsing worker: sync\nBooting worker with pid: 100\nException in worker process\nTraceback (most recent call last):\n  File &quot;\/opt\/miniconda\/envs\/amlenv\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 589, in spawn_worker\n    worker.init_process()\n  File &quot;\/opt\/miniconda\/envs\/amlenv\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py&quot;, line 134, in init_process\n    self.load_wsgi()\n  File &quot;\/opt\/miniconda\/envs\/amlenv\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py&quot;, line 146, in load_wsgi\n    self.wsgi = self.app.wsgi()\n  File &quot;\/opt\/miniconda\/envs\/amlenv\/lib\/python3.7\/site-packages\/gunicorn\/app\/base.py&quot;, line 67, in wsgi\n    self.callable = self.load()\n  File &quot;\/opt\/miniconda\/envs\/amlenv\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 58, in load\n    return self.load_wsgiapp()\n  File &quot;\/opt\/miniconda\/envs\/amlenv\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 48, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File &quot;\/opt\/miniconda\/envs\/amlenv\/lib\/python3.7\/site-packages\/gunicorn\/util.py&quot;, line 359, in import_app\n    mod = importlib.import_module(module)\n  File &quot;\/opt\/miniconda\/envs\/amlenv\/lib\/python3.7\/importlib\/__init__.py&quot;, line 127, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1006, in _gcd_import\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 983, in _find_and_load\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 967, in _find_and_load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 677, in _load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 728, in exec_module\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed\n  File &quot;\/opt\/miniconda\/envs\/amlenv\/lib\/python3.7\/site-packages\/azureml_inference_server_http\/server\/entry.py&quot;, line 1, in &lt;module&gt;\n    import create_app\n  File &quot;\/opt\/miniconda\/envs\/amlenv\/lib\/python3.7\/site-packages\/azureml_inference_server_http\/server\/create_app.py&quot;, line 4, in &lt;module&gt;\n    from routes_common import main\n  File &quot;\/opt\/miniconda\/envs\/amlenv\/lib\/python3.7\/site-packages\/azureml_inference_server_http\/server\/routes_common.py&quot;, line 32, in &lt;module&gt;\n    from aml_blueprint import AMLBlueprint\n  File &quot;\/opt\/miniconda\/envs\/amlenv\/lib\/python3.7\/site-packages\/azureml_inference_server_http\/server\/aml_blueprint.py&quot;, line 28, in &lt;module&gt;\n    main_module_spec.loader.exec_module(main)\n  File &quot;\/var\/azureml-app\/score.py&quot;, line 4, in &lt;module&gt;\n    import joblib\nModuleNotFoundError: No module named 'joblib'\nWorker exiting (pid: 100)\nShutting down: Master\nReason: Worker failed to boot.\n2022-01-24T10:21:13,851467800+00:00 - gunicorn\/finish 3 0\n2022-01-24T10:21:13,853259700+00:00 - Exit code 3 is not normal. Killing image.\n<\/code><\/pre>\n<h1>From a Conda specification<\/h1>\n<p>Same as before, but with a fresh environment from Conda specification and changing the <code>env_version<\/code> number :<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># ...\n\n\nenv_version=&quot;2&quot;\n\n# ...\n\ncustom_env = Environment.from_conda_specification(name=env_name, file_path=&quot;my-env.yml&quot;)\ncustom_env.docker.base_image = DEFAULT_CPU_IMAGE\n\n# ...\n\n<\/code><\/pre>\n<p>with <code>my-env.yml<\/code> :<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>name: my-env\ndependencies:\n- python\n\n- pip:\n  - azureml-defaults\n  - azureml-sdk\n  - sklearn\n  - numpy\n  - matplotlib\n  - joblib\n  - uuid\n  - requests\n  - tensorflow\n\n<\/code><\/pre>\n<p>I get this log :<\/p>\n<pre><code>2022-01-24T11:06:54,887886931+00:00 - iot-server\/run \n2022-01-24T11:06:54,891839877+00:00 - rsyslog\/run \n2022-01-24T11:06:54,893640998+00:00 - gunicorn\/run \n2022-01-24T11:06:54,912032812+00:00 - nginx\/run \nEdgeHubConnectionString and IOTEDGE_IOTHUBHOSTNAME are not set. Exiting...\n2022-01-24T11:06:55,398420960+00:00 - iot-server\/finish 1 0\n2022-01-24T11:06:55,414425146+00:00 - Exit code 1 is normal. Not restarting iot-server.\n\nPATH environment variable: \/opt\/miniconda\/bin:\/usr\/local\/sbin:\/usr\/local\/bin:\/usr\/sbin:\/usr\/bin:\/sbin:\/bin\nPYTHONPATH environment variable: \n\nPip Dependencies\n---------------\nbrotlipy==0.7.0\ncertifi==2020.6.20\ncffi @ file:\/\/\/tmp\/build\/80754af9\/cffi_1605538037615\/work\nchardet @ file:\/\/\/tmp\/build\/80754af9\/chardet_1605303159953\/work\nconda==4.9.2\nconda-package-handling @ file:\/\/\/tmp\/build\/80754af9\/conda-package-handling_1603018138503\/work\ncryptography @ file:\/\/\/tmp\/build\/80754af9\/cryptography_1605544449973\/work\nidna @ file:\/\/\/tmp\/build\/80754af9\/idna_1593446292537\/work\npycosat==0.6.3\npycparser @ file:\/\/\/tmp\/build\/80754af9\/pycparser_1594388511720\/work\npyOpenSSL @ file:\/\/\/tmp\/build\/80754af9\/pyopenssl_1605545627475\/work\nPySocks @ file:\/\/\/tmp\/build\/80754af9\/pysocks_1594394576006\/work\nrequests @ file:\/\/\/tmp\/build\/80754af9\/requests_1592841827918\/work\nruamel-yaml==0.15.87\nsix @ file:\/\/\/tmp\/build\/80754af9\/six_1605205313296\/work\ntqdm @ file:\/\/\/tmp\/build\/80754af9\/tqdm_1605303662894\/work\nurllib3 @ file:\/\/\/tmp\/build\/80754af9\/urllib3_1603305693037\/work\n\nStarting HTTP server\n2022-01-24T11:06:59,701365128+00:00 - gunicorn\/finish 127 0\n.\/run: line 127: exec: gunicorn: not found\n2022-01-24T11:06:59,706177784+00:00 - Exit code 127 is not normal. Killing image.\n    \n<\/code><\/pre>\n<p>I really don't know what I'm missing, and I've been searching for too long already (Azure docs, SO, ...).<\/p>\n<p>Thanks for your help !<\/p>\n<p>Edit : Non-exhaustive list of solutions I tried :<\/p>\n<ul>\n<li><a href=\"https:\/\/stackoverflow.com\/questions\/65159778\/how-to-create-azureml-environement-and-add-required-packages\">How to create AzureML environement and add required packages<\/a><\/li>\n<li><a href=\"https:\/\/stackoverflow.com\/questions\/65159308\/how-to-use-existing-conda-environment-as-a-azureml-environment\">how to use existing conda environment as a AzureML environment<\/a><\/li>\n<li>...<\/li>\n<li><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-environments#environment-building-caching-and-reuse\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-environments#environment-building-caching-and-reuse<\/a><\/li>\n<li><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-use-environments#add-packages-to-an-environment\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-use-environments#add-packages-to-an-environment<\/a><\/li>\n<li><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-inferencing-gpus\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-inferencing-gpus<\/a><\/li>\n<li><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-and-where?tabs=python#define-a-deployment-configuration\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-and-where?tabs=python#define-a-deployment-configuration<\/a><\/li>\n<li>...<\/li>\n<\/ul>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_date":"2022-01-24 11:57:28.957000 UTC",
        "Question_favorite_count":0.0,
        "Question_last_edit_date":"2022-01-26 02:17:34.173000 UTC",
        "Question_score":0,
        "Question_tags":"python|tensorflow|jupyter-notebook|azure-machine-learning-service",
        "Question_view_count":902,
        "Owner_creation_date":"2012-05-24 14:00:50.813000 UTC",
        "Owner_last_access_date":"2022-09-23 12:35:53.207000 UTC",
        "Owner_location":"Paris, France",
        "Owner_reputation":483,
        "Owner_up_votes":39,
        "Owner_down_votes":2,
        "Owner_views":105,
        "Answer_body":"<p>OK, I got it working : I started over from scratch and it worked.<\/p>\n<p>I have no idea what was wrong in all my preceding tries, and that is terrible.<\/p>\n<p>Multiple problems and how I (think I) solved them :<\/p>\n<ul>\n<li><code>joblib<\/code> : I actually didn't need it to load my Keras model. But the problem was not with this specific library, rather that I couldn't add dependencies to the inference environment.<\/li>\n<li><code>Environment<\/code> : finally, I was only able to make things work with a custom env : <code>Environment.from_conda_specification(name=version, file_path=&quot;conda_dependencies.yml&quot;)<\/code> . I haven't been able to add my libraries (or specify a specific package version) to a &quot;currated environment&quot;. I don't know why though...<\/li>\n<li><code>TensorFlow<\/code> : last problem I had was that I trained and registered my model in AzureML Notebook's <code>azureml_py38_PT_TF<\/code> kernel (<code>tensorflow==2.7.0<\/code>), and tried to load it in the inference Docker image (<code>tensorflow==2.4.0<\/code>). So I had to specify the version of TensorFlow I wanted to use in the inference image (which required the previous point to be solved).<\/li>\n<\/ul>\n<p>What finally worked :<\/p>\n<ul>\n<li>notebook.ipynb<\/li>\n<\/ul>\n<pre class=\"lang-py prettyprint-override\"><code>import uuid\nfrom azureml.core import Workspace, Environment, Model\nfrom azureml.core.webservice import AciWebservice\nfrom azureml.core.model import InferenceConfig\n\n\nversion = &quot;test-&quot;+str(uuid.uuid4())[:8]\n\nenv = Environment.from_conda_specification(name=version, file_path=&quot;conda_dependencies.yml&quot;)\ninference_config = InferenceConfig(entry_script=&quot;score.py&quot;, environment=env)\n\nws = Workspace.from_config()\nmodel = Model(ws, model_name)\n\naci_config = AciWebservice.deploy_configuration(\n    cpu_cores=1,\n    memory_gb=1,\n)\n\nservice = Model.deploy(\n    workspace=ws,\n    name=version,\n    models=[model],\n    inference_config=inference_config,\n    deployment_config=aci_config,\n    overwrite=True,\n)\n\nservice.wait_for_deployment(show_output=True)\n<\/code><\/pre>\n<ul>\n<li>conda_dependencies.yml<\/li>\n<\/ul>\n<pre class=\"lang-yaml prettyprint-override\"><code>channels:\n- conda-forge\ndependencies:\n- python=3.8\n- pip:\n  - azureml-defaults\n  - azureml-sdk\n  - numpy\n  - tensorflow==2.7.0\n\n<\/code><\/pre>\n<ul>\n<li>score.py<\/li>\n<\/ul>\n<pre class=\"lang-py prettyprint-override\"><code>import os\nimport json\nimport numpy as np\nimport tensorflow as tf\n\n\ndef init():\n    global model\n\n    model_path = os.path.join(os.getenv(&quot;AZUREML_MODEL_DIR&quot;), &quot;model\/data\/model&quot;)\n    model = tf.keras.models.load_model(model_path)\n\n\n\ndef run(raw_data):\n    data = np.array(json.loads(raw_data)[&quot;data&quot;])\n    y_hat = model.predict(data)\n\n    return y_hat.tolist()\n\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-01-26 09:14:08.820000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":35627916,
        "Question_title":"Azure machine learning even sampling",
        "Question_body":"<p>I'm trying to do some basic multi-label classification in Azure ML. I have some basic data in the following format:<\/p>\n\n<pre><code>value_x value_y label\nx1      y1      label1\nx2      y2      label1\nx3      y3      label2\n.....\n<\/code><\/pre>\n\n<p>My problem is that in my data certain labels (out of a total of five) are overrepresented, as about 40% of the data is label1, about 20% is label 2 and the rest around 10%. <\/p>\n\n<p>I would like to get a sampling out of these to train my model, so that each label is represented in equal amounts. <\/p>\n\n<p>Tried the stratification option in the Sampling module on the labels column, but that just gives me a sampling with the same distribution of labels as in the initial dataset.<\/p>\n\n<p>Any idea how I could do this with a module?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2016-02-25 12:53:42.827000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2016-02-25 14:36:19.577000 UTC",
        "Question_score":0,
        "Question_tags":"machine-learning|classification|sampling|multilabel-classification|azure-machine-learning-studio",
        "Question_view_count":539,
        "Owner_creation_date":"2013-10-10 11:47:41.540000 UTC",
        "Owner_last_access_date":"2022-09-22 10:35:54.783000 UTC",
        "Owner_location":null,
        "Owner_reputation":185,
        "Owner_up_votes":42,
        "Owner_down_votes":0,
        "Owner_views":11,
        "Answer_body":"<p>I was able to do this using a combination of <a href=\"https:\/\/msdn.microsoft.com\/library\/azure\/70530644-c97a-4ab6-85f7-88bf30a8be5f\" rel=\"nofollow\">Split Data<\/a>, <a href=\"https:\/\/msdn.microsoft.com\/library\/azure\/a8726e34-1b3e-4515-b59a-3e4a475654b8\" rel=\"nofollow\">Partition and Sample<\/a>, and <a href=\"https:\/\/msdn.microsoft.com\/library\/azure\/b2ebdabd-217d-4915-86cc-5b05972f7270\" rel=\"nofollow\">Add Rows<\/a> modules.  There may be an easier way to do it, but I did confirm it works.  :)  I published my work at <a href=\"http:\/\/gallery.azureml.net\/Details\/1245147fd7004e91bc7a3683cda19cc7\" rel=\"nofollow\">http:\/\/gallery.azureml.net\/Details\/1245147fd7004e91bc7a3683cda19cc7<\/a> so you can grab it directly from there, and run to confirm it does what you expect.  <\/p>\n\n<p>Since you said you wanted a sampling of the data, I just reduced each of the labels to 10% to have all labels represented equally.  Since you have a good understanding of the distribution in your dataset, leave label 3, 4, and 5 all at about 10%, and reduce label 1 by 1\/4 and label 2 by 1\/2 to get about 10% of them as well.  <\/p>\n\n<p>To explain what I did in the workspace linked above:<\/p>\n\n<ul>\n<li>I used some \"Split Data\" modules to filter out the label1 and label2 data.  In the Split Data module, change the Splitting mode to \"Regular Expression\" and set the regular expression to <strong>\\\"Label\" ^label1<\/strong> (to get the label1 data, for example).  <\/li>\n<li>Then I used some \"Partition and Sample\" modules to reduce the size of the label1 and label2 data appropriately.  <\/li>\n<li>Finally, I used some \"Add Rows\" modules to join all of the data back together again.  <\/li>\n<\/ul>\n\n<p>Finally, I didn't include this in my work, but you can also look at the <a href=\"https:\/\/msdn.microsoft.com\/library\/azure\/9f3fe1c4-520e-49ac-a152-2e104169912a\" rel=\"nofollow\">SMOTE<\/a> module.  It will increase the number of low-occurring samples using synthetic minority oversampling.  <\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2016-02-26 03:51:29.160000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":3.0,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":60156370,
        "Question_title":"how SageMaker to access s3 bucket data",
        "Question_body":"<p>I was using the code <code>pd.read_json('s3:\/\/example2020\/kaggle.json')<\/code> to access S3 bucket data, but it threw the error of <code>FileNotFoundError: example2020\/kaggle.json<\/code>. <\/p>\n\n<p>The methods I tried:<\/p>\n\n<p><strong>[Region]<\/strong>\nThe s3 bucket is in Ohio region while the SageMaker notebook instance is in Singapore. Not sure if this matters. I tried to recreate a s3 bucket in Singapore region but I still cannot access it and got the same file not found error. <\/p>\n\n<p><strong>[IAM Role]<\/strong>\nI checked the permission of IAM-SageMaker Execution role\n<a href=\"https:\/\/i.stack.imgur.com\/st4AR.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/st4AR.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-02-10 18:31:36.750000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"amazon-s3|amazon-sagemaker",
        "Question_view_count":1386,
        "Owner_creation_date":"2018-02-13 14:44:57.007000 UTC",
        "Owner_last_access_date":"2022-09-23 01:39:29.630000 UTC",
        "Owner_location":"Australia",
        "Owner_reputation":896,
        "Owner_up_votes":1273,
        "Owner_down_votes":3,
        "Owner_views":177,
        "Answer_body":"<p>The problem is still IAM permission. <\/p>\n\n<p>I created a new notebook instance and a new IAM role. You would be asked how to access s3 bucket. I chose <code>all s3 bucket<\/code>. Then the problem solved. \n<a href=\"https:\/\/i.stack.imgur.com\/B0qOO.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/B0qOO.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><br>\n<br>\n<strong>[Solution]<\/strong>\nIn Resource tab, check whether bucket name is general.\n <a href=\"https:\/\/i.stack.imgur.com\/LL6Fw.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/LL6Fw.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>If you changed old IAM and it is not working, you can create a new IAM role. And attach this role to the notebook.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-02-10 18:31:36.750000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":70838510,
        "Question_title":"Is it possible to request a Vertex AI endpoint from another GCP project?",
        "Question_body":"<p>I trained a model on GCP Vertex AI, and deployed it on an endpoint.<\/p>\n<p>I am able to execute predictions from a sample to my model with this python code <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/online-predictions-automl#aiplatform_predict_image_classification_sample-python\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/online-predictions-automl#aiplatform_predict_image_classification_sample-python<\/a><\/p>\n<p>It works within my GCP project.<\/p>\n<p>My question is, is it possible to request this endpoint from another GCP project ? If I set a service account and set IAM role in both projects ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-01-24 18:12:06.007000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python-3.x|google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":417,
        "Owner_creation_date":"2020-08-31 11:39:36.143000 UTC",
        "Owner_last_access_date":"2022-09-18 19:46:19.503000 UTC",
        "Owner_location":"Versailles, France",
        "Owner_reputation":140,
        "Owner_up_votes":9,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Answer_body":"<p>Yes it is possible. For example you have Project A and Project B, assuming that Project A hosts the model.<\/p>\n<ul>\n<li><p>Add service account of Project B in Project A and provide at least <code>roles\/aiplatform.user<\/code> predefined role. See <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/general\/access-control#predefined-roles\" rel=\"nofollow noreferrer\">predefined roles<\/a> and look for <code>roles\/aiplatform.user<\/code> to see complete roles it contains.<\/p>\n<\/li>\n<li><p>This role contains <strong>aiplatform.endpoints.<\/strong>* and <strong>aiplatform.batchPredictionJobs.<\/strong>* as these are the roles needed to run predictions.<\/p>\n<blockquote>\n<p>See <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/general\/iam-permissions\" rel=\"nofollow noreferrer\">IAM permissions for Vertex AI<\/a><\/p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>Resource<\/th>\n<th>Operation<\/th>\n<th>Permissions needed<\/th>\n<\/tr>\n<\/thead>\n<tbody>\n<tr>\n<td>batchPredictionJobs<\/td>\n<td>Create a batchPredictionJob<\/td>\n<td>aiplatform.batchPredictionJobs.create (permission needed on the parent resource)<\/td>\n<\/tr>\n<tr>\n<td>endpoints<\/td>\n<td>Predict an endpoint<\/td>\n<td>aiplatform.endpoints.predict (permission needed on the endpoint resource)<\/td>\n<\/tr>\n<\/tbody>\n<\/table>\n<\/div><\/blockquote>\n<\/li>\n<\/ul>\n<p>With this set up, Project B will be able to use the model in Project A to run predictions.<\/p>\n<p>NOTE: Just make sure that the script of Project B points to the resources in Project A like <code>project_id<\/code> and <code>endpoint_id<\/code>.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2022-01-25 03:21:58.690000 UTC",
        "Answer_last_edit_date":"2022-01-25 03:35:51.060000 UTC",
        "Answer_score":1.0,
        "Question_valid_tags":[
            "google-cloud-vertex-ai"
        ]
    },
    {
        "Question_id":70455676,
        "Question_title":"AWS Sagemaker Pipelines throws a \"No finished training job found associated with this estimator\" after introducing a register step",
        "Question_body":"<p>I am currently working on creating a Sagemaker Pipeline to train a Tensorflow model. I'm new to this area and I have been following <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker-pipelines\/tabular\/tensorflow2-california-housing-sagemaker-pipelines-deploy-endpoint\/tensorflow2-california-housing-sagemaker-pipelines-deploy-endpoint.ipynb\" rel=\"nofollow noreferrer\">this guide<\/a> created by AWS as well as the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/define-pipeline.html\" rel=\"nofollow noreferrer\">standard pipeline workflow<\/a> listed in the Sagemaker developer guide.<\/p>\n<p>I have a pipeline that runs without error when I only include the preprocessing, training, evaluation, and condition steps. When I add the register step:<\/p>\n<pre><code># Package evaluation metrics into an evaluation report `PropertyFile`\nevaluation_report = PropertyFile(\n        name=&quot;EvaluationReport&quot;, output_name=&quot;evaluation&quot;, path=&quot;evaluation_report.json&quot;\n)\n\n# Create ModelMetrics object using the evaluation report from the evaluation step\n# A ModelMetrics object contains metrics captured from a model.\nmodel_metrics = ModelMetrics(model_statistics=evaluation_report)\n\n# Create a RegisterModel step, which registers the model with Sagemaker Model Registry.\nregister_step = RegisterModel(\n    name=&quot;Foo&quot;,\n    estimator=estimator,\n    model_data=train_step.properties.ModelArtifacts.S3ModelArtifacts,\n    content_types=[&quot;text\/csv&quot;],\n    response_types=[&quot;text\/csv&quot;],\n    inference_instances=config[&quot;instance&quot;][&quot;inference&quot;],\n    transform_instances=config[&quot;instance&quot;][&quot;transform&quot;],\n    model_package_group_name=&quot;Bar&quot;,\n    model_metrics=model_metrics,\n    approval_status=&quot;approved&quot;,\n)\n<\/code><\/pre>\n<p>to the condition step's <code>if_steps<\/code>:<\/p>\n<pre><code># Create a Sagemaker Pipelines ConditionStep, using the condition above.\n# Enter the steps to perform if the condition returns True \/ False.\ncond_step = ConditionStep(\n    name=&quot;MSE-Lower-Than-Threshold-Condition&quot;,\n    conditions=[cond_lte],\n    if_steps=[register_step],\n    else_steps=[],\n)\n<\/code><\/pre>\n<p>I get the following trace:<\/p>\n<pre><code>PropertyFile(name='EvaluationReport', output_name='evaluation', path='evaluation_report.json')\nNo finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\nTraceback (most recent call last):\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/pipeline_definition.py&quot;, line 474, in &lt;module&gt;\n    main()\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/pipeline_definition.py&quot;, line 466, in main\n    pipeline = define_pipeline()\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/pipeline_definition.py&quot;, line 457, in define_pipeline\n    print(json.loads(pipeline.definition()))\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/venv\/lib\/python3.9\/site-packages\/sagemaker\/workflow\/pipeline.py&quot;, line 257, in definition\n    request_dict = self.to_request()\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/venv\/lib\/python3.9\/site-packages\/sagemaker\/workflow\/pipeline.py&quot;, line 89, in to_request\n    &quot;Steps&quot;: list_to_request(self.steps),\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/venv\/lib\/python3.9\/site-packages\/sagemaker\/workflow\/utilities.py&quot;, line 37, in list_to_request\n    request_dicts.append(entity.to_request())\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/venv\/lib\/python3.9\/site-packages\/sagemaker\/workflow\/steps.py&quot;, line 99, in to_request\n    &quot;Arguments&quot;: self.arguments,\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/venv\/lib\/python3.9\/site-packages\/sagemaker\/workflow\/condition_step.py&quot;, line 87, in arguments\n    IfSteps=list_to_request(self.if_steps),\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/venv\/lib\/python3.9\/site-packages\/sagemaker\/workflow\/utilities.py&quot;, line 39, in list_to_request\n    request_dicts.extend(entity.request_dicts())\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/venv\/lib\/python3.9\/site-packages\/sagemaker\/workflow\/step_collections.py&quot;, line 50, in request_dicts\n    return [step.to_request() for step in self.steps]\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/venv\/lib\/python3.9\/site-packages\/sagemaker\/workflow\/step_collections.py&quot;, line 50, in &lt;listcomp&gt;\n    return [step.to_request() for step in self.steps]\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/venv\/lib\/python3.9\/site-packages\/sagemaker\/workflow\/steps.py&quot;, line 209, in to_request\n    step_dict = super().to_request()\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/venv\/lib\/python3.9\/site-packages\/sagemaker\/workflow\/steps.py&quot;, line 99, in to_request\n    &quot;Arguments&quot;: self.arguments,\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/venv\/lib\/python3.9\/site-packages\/sagemaker\/workflow\/_utils.py&quot;, line 423, in arguments\n    model_package_args = get_model_package_args(\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/venv\/lib\/python3.9\/site-packages\/sagemaker\/session.py&quot;, line 4217, in get_model_package_args\n    model_package_args[&quot;model_metrics&quot;] = model_metrics._to_request_dict()\n  File &quot;\/Users\/&lt;user&gt;\/Code\/&lt;repo_name&gt;\/venv\/lib\/python3.9\/site-packages\/sagemaker\/model_metrics.py&quot;, line 66, in _to_request_dict\n    model_quality[&quot;Statistics&quot;] = self.model_statistics._to_request_dict()\nAttributeError: 'PropertyFile' object has no attribute '_to_request_dict'\n<\/code><\/pre>\n<p>From this trace I see two, potentially related, issues. The immediate issue is the <code>AttributeError: 'PropertyFile' object has no attribute '_to_request_dict'<\/code>. I haven't been able to find any information on why we might be receiving it between forums and Sagemaker documentation.<\/p>\n<p>I also see a sneaky issue towards the top of the trace that has plagued me all day. The line <code>No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config<\/code> tells me that the register step is using our estimator when it should be waiting until after the training step has run. I can't seem to find any reference to this error, besides a somewhat-similar <a href=\"https:\/\/datascience.stackexchange.com\/questions\/100113\/how-to-fix-sagemakers-no-finished-training-job-found-associated-with-this-esti\">stack exchange post<\/a>.<\/p>\n<p>I've compared my code to the AWS-published examples many times and I'm confident that I'm not doing anything taboo. Would anyone be able to shine some light on what these errors are suggesting? Is there any more information or code that would be relevant?<\/p>\n<p>Thanks so much!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2021-12-22 22:12:08.263000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"python|amazon-web-services|tensorflow|amazon-sagemaker",
        "Question_view_count":1006,
        "Owner_creation_date":"2021-04-26 02:01:43.747000 UTC",
        "Owner_last_access_date":"2022-06-23 14:27:02.190000 UTC",
        "Owner_location":null,
        "Owner_reputation":61,
        "Owner_up_votes":36,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":"<p>I was able to work through the issue! Here is the code for the register step that ended up working with my Tensorflow model:<\/p>\n<pre><code># Package the model\npipeline_model = PipelineModel(models=[model], role=params[&quot;role&quot;].default_value, sagemaker_session=session)\n\n# Create a RegisterModel step, which registers the model with Sagemaker Model Registry.\nregister_step = RegisterModel(\n    name=&quot;Bar&quot;,\n    model=pipeline_model,\n    model_data=train_step.properties.ModelArtifacts.S3ModelArtifacts,\n    content_types=[&quot;application\/json&quot;],\n    response_types=[&quot;application\/json&quot;],\n    inference_instances=[config[&quot;instance&quot;][&quot;inference&quot;]],\n    transform_instances=[config[&quot;instance&quot;][&quot;transform&quot;]],\n    model_package_group_name=&quot;Foo&quot;,\n    approval_status=&quot;Approved&quot;,\n)\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-01-19 19:26:46.233000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":71126832,
        "Question_title":"Amazon Sagemaker: User Input data validation in Inference Endpoint",
        "Question_body":"<p>I have successfully built a Sagemaker endpoint using a Tensorflow model. The pre and post processing is done inside &quot;inference.py&quot; which calls a handler function based on this tutorial: <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/using_tf.html#how-to-implement-the-pre-and-or-post-processing-handler-s\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/using_tf.html#how-to-implement-the-pre-and-or-post-processing-handler-s<\/a><\/p>\n<p>My questions are:<\/p>\n<ul>\n<li>Which method is good for validating user input data within inference.py?<\/li>\n<li>If such validation tests fail (e.g. wrong data types or data not in allowed range, etc.), how is it possible to return appropriate error messages with status codes to the user?<\/li>\n<li>How is this compatible with the API gateway placed above the endpoint?<\/li>\n<\/ul>\n<p>Here is the structure of the inference.py with the desired validation check as a comment:<\/p>\n<pre><code>import json\nimport requests\n\n\ndef handler(data, context):\n    &quot;&quot;&quot;Handle request.\n    Args:\n        data (obj): the request data\n        context (Context): an object containing request and configuration details\n    Returns:\n        (bytes, string): data to return to client, (optional) response content type\n    &quot;&quot;&quot;\n    processed_input = _process_input(data, context)\n    response = requests.post(context.rest_uri, data=processed_input)\n    return _process_output(response, context)\n\n\ndef _process_input(data, context):\n    if context.request_content_type == 'application\/json':\n        # pass through json (assumes it's correctly formed)\n        d = data.read().decode('utf-8')\n        data_dict = json.loads(data)\n\n\n        # -----&gt;   if data_dict['input_1'] &gt; 25000:\n        # -----&gt;       return some error specific message with status code 123\n\n\n        return some_preprocessing_function(data_dict)\n\n    raise ValueError('{{&quot;error&quot;: &quot;unsupported content type {}&quot;}}'.format(\n        context.request_content_type or &quot;unknown&quot;))\n\n\ndef _process_output(data, context):\n    if data.status_code != 200:\n        raise ValueError(data.content.decode('utf-8'))\n\n    response_content_type = context.accept_header\n    prediction = data.content\n    return prediction, response_content_type\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-02-15 12:56:47.890000 UTC",
        "Question_favorite_count":2.0,
        "Question_last_edit_date":"2022-02-15 13:25:22.733000 UTC",
        "Question_score":0,
        "Question_tags":"python|validation|amazon-sagemaker|endpoint|inference",
        "Question_view_count":245,
        "Owner_creation_date":"2021-02-18 15:25:28.947000 UTC",
        "Owner_last_access_date":"2022-09-23 10:35:08.913000 UTC",
        "Owner_location":null,
        "Owner_reputation":160,
        "Owner_up_votes":7,
        "Owner_down_votes":0,
        "Owner_views":14,
        "Answer_body":"<p>I will answer your questions inline below:<\/p>\n<ol>\n<li><em>Which method is good for validating user input data within inference.py?<\/em><\/li>\n<\/ol>\n<p>Seeing that you have a <code>handler<\/code> function, <code>input_handler<\/code> and <code>output_handler<\/code> are ignored. Thus, inside your <code>handler<\/code> function (as you are correctly doing) you can have the validation logic.<\/p>\n<ol start=\"2\">\n<li><em>If such validation tests fail (e.g. wrong data types or data not in allowed range, etc.), how is it possible to return appropriate error messages with status codes to the user?<\/em><\/li>\n<\/ol>\n<p>I like to think of my SageMaker endpoint as a web server. Thus, you can return any valid HTTP response code with a response message. Please see this example <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/sagemaker_batch_transform\/tensorflow_cifar-10_with_inference_script\/code\/inference.py#L47\" rel=\"nofollow noreferrer\">inference.py<\/a> file that I found as a reference.<\/p>\n<pre><code>_return_error(\n            415, 'Unsupported content type &quot;{}&quot;'.format(context.request_content_type or &quot;Unknown&quot;)\n        )\n\ndef _return_error(code, message):\n    raise ValueError(&quot;Error: {}, {}&quot;.format(str(code), message))\n<\/code><\/pre>\n<ol start=\"3\">\n<li><em>How is this compatible with the API gateway placed above the endpoint?<\/em><\/li>\n<\/ol>\n<p>Please see this <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/creating-a-machine-learning-powered-rest-api-with-amazon-api-gateway-mapping-templates-and-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">link<\/a> for details on Creating a machine learning-powered REST API with Amazon API Gateway mapping templates and Amazon SageMaker.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-02-22 21:59:21.520000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":69678393,
        "Question_title":"Creating docker file which installs python with sklearn and pandas that can be used on sagemaker",
        "Question_body":"<p>I am quite new to docker. My problem in short is to create a docker file that contains python with sklearn and pandas which can be used on aws sagemaker.<\/p>\n<p>My current docker file looks like the following:<\/p>\n<pre><code>FROM jupyter\/scipy-notebook\n\nRUN pip3 install sagemaker-training\n\nCOPY train.py \/opt\/ml\/code\/train.py\n\nENV SAGEMAKER_PROGRAM train.py\n<\/code><\/pre>\n<p>However when i try to create this image I get an error at line <code>pip3 install sagemaker-training<\/code>. The error is the following:<\/p>\n<pre><code>error: command 'gcc' failed: No such file or directory\n\nERROR: Command errored out with exit status 1: \/opt\/conda\/bin\/python3.9 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'\/tmp\/pip-install-fj0cb373\/sagemaker-training_66ca9935ed134c95ac11a32e118e4568\/setup.py'&quot;'&quot;'; __file__='&quot;'&quot;'\/tmp\/pip-install-fj0cb373\/sagemaker-training_66ca9935ed134c95ac11a32e118e4568\/setup.py'&quot;'&quot;';f = getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(__file__) if os.path.exists(__file__) else io.StringIO('&quot;'&quot;'from setuptools import setup; setup()'&quot;'&quot;');code = f.read().replace('&quot;'&quot;'\\r\\n'&quot;'&quot;', '&quot;'&quot;'\\n'&quot;'&quot;');f.close();exec(compile(code, __file__, '&quot;'&quot;'exec'&quot;'&quot;'))' install --record \/tmp\/pip-record-o5rzjscd\/install-record.txt --single-version-externally-managed --compile --install-headers \/opt\/conda\/include\/python3.9\/sagemaker-training Check the logs for full command output.\n\nThe command '\/bin\/bash -o pipefail -c pip3 install sagemaker-training' returned a non-zero code: 1\n<\/code><\/pre>\n<p>If there is a more suitable base image can someone point that out to me? I am generally trying to follow this page <a href=\"https:\/\/github.com\/aws\/sagemaker-training-toolkit\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-training-toolkit<\/a>.<\/p>\n<p>Note: I realise I can use some sagemaker pre-built containers without using my own docker file. However I am trying to do this for my own learning so I know what to do for projects that can't utilise them.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":5,
        "Question_creation_date":"2021-10-22 14:13:28.467000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-10-22 14:39:02.570000 UTC",
        "Question_score":0,
        "Question_tags":"docker|amazon-sagemaker",
        "Question_view_count":161,
        "Owner_creation_date":"2018-03-22 16:45:24.657000 UTC",
        "Owner_last_access_date":"2022-09-23 14:29:34.747000 UTC",
        "Owner_location":"Milton Keynes",
        "Owner_reputation":738,
        "Owner_up_votes":22,
        "Owner_down_votes":7,
        "Owner_views":69,
        "Answer_body":"<p>I adjusted your Dockerfile and it builds successfully for me.<\/p>\n<pre><code>FROM jupyter\/scipy-notebook\nARG defaultuser=jovyan\nUSER root\nENV DEBIAN_FRONTEND noninteractive\nRUN apt-get update &amp;&amp; \\\n    apt-get -y install gcc mono-mcs &amp;&amp; \\\n    rm -rf \/var\/lib\/apt\/lists\/*\nUSER $defaultuser\n\nRUN pip3 install sagemaker-training\n\nCOPY train.py \/opt\/ml\/code\/train.py\n\nENV SAGEMAKER_PROGRAM train.py\n<\/code><\/pre>\n<p>(I had to adjust for the fact that the default user from the base container isn't root, when installing GCC)<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-10-28 16:38:12.897000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":51198775,
        "Question_title":"How does Azure web service deployment work locally?",
        "Question_body":"<p>Azure ML provides client libraries (e.g. azureml for Python) for dataset management and model deploying. From what I understand, the custom algorithm would be serialized as a Pickle file, but I'm not sure what happens after that. If I have a custom model with a deep NN architecture and set up a web service for training and another for scoring, do I still need the machine that the model was developed on for the web services to run? I found this on the azureml documentation that was helpful:<\/p>\n<blockquote>\n<p>If a function has no source file associated with it (for example, you're developing inside of a REPL environment) then the functions byte code is serialized. If the function refers to any global variables those will also be serialized using Pickle. In this mode all of the state which you're referring to needs to be already defined (e.g. your published function should come after any other functions you are calling).<\/p>\n<p>If a function is saved on disk then the entire module the function is defined in will be serialized and re-executed on the server to get the function back. In this mode the entire contents of the file is serialized and the order of the function definitions don't matter.<\/p>\n<\/blockquote>\n<p>What if the function uses a library like TensorFlow or Keras? Can someone explain what happens after the Pickle model is created?<\/p>\n<p>Thanks!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2018-07-05 19:40:59.027000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-06-20 09:12:55.060000 UTC",
        "Question_score":1,
        "Question_tags":"python-3.x|azure|pickle|azure-machine-learning-studio",
        "Question_view_count":126,
        "Owner_creation_date":"2012-05-27 14:00:53.383000 UTC",
        "Owner_last_access_date":"2020-12-22 00:05:39.673000 UTC",
        "Owner_location":"Minneapolis, MN, United States",
        "Owner_reputation":115,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":13,
        "Answer_body":"<p>You need to take the model.pkl file, zip it, and upload it into Azure Machine Learning Studio as a new dataset. Then add the python module and connect it to your newly generated zip.<\/p>\n\n<p>You can now use it inside the AML Studio experiment. To use the model add the following code in your python module:<\/p>\n\n<pre><code>import pandas as pd\nimport sys\nimport pickle\n\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n    sys.path.insert(0,\".\\Script Bundle\")\n    model = pickle.load(open(\".\\Script Bundle\\model.pkl\", 'rb'))\n    pred = model.predict(dataframe1)\n    return pd.DataFrame([pred[0]]),\n<\/code><\/pre>\n\n<p><a href=\"https:\/\/blogs.technet.microsoft.com\/uktechnet\/2018\/04\/25\/deploying-externally-generated-pythonr-models-as-web-services-using-azure-machine-learning-studio\/\" rel=\"nofollow noreferrer\">You may find this post useful<\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2018-07-26 14:13:52.913000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":72214443,
        "Question_title":"Get notebook outline in sagemaker studio like in Visual Studio Code",
        "Question_body":"<p>When I use visual studio code with jupyter notebook, I have an &quot;outline&quot; tab in the left panels that display the Markdown section of my notebook for quick access.<\/p>\n<p>But in Sagemaker studio I don't have this and I would like to add it.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-05-12 11:06:44.940000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|jupyter-notebook|jupyter-lab|amazon-sagemaker",
        "Question_view_count":164,
        "Owner_creation_date":"2019-12-12 07:37:35.053000 UTC",
        "Owner_last_access_date":"2022-09-22 15:02:52.457000 UTC",
        "Owner_location":null,
        "Owner_reputation":795,
        "Owner_up_votes":37,
        "Owner_down_votes":1,
        "Owner_views":37,
        "Answer_body":"<p>The 'Outline' tab (Table of Contents extension in Jupyter) is not available for Studio yet.<\/p>\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/nbi.html\" rel=\"nofollow noreferrer\">SageMaker notebook instances<\/a> come with the extension prebuilt.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-05-13 16:32:09.953000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":47625056,
        "Question_title":"Amazon Machine Learning and SageMaker algorithms",
        "Question_body":"<p>1) According to <a href=\"http:\/\/docs.aws.amazon.com\/machine-learning\/latest\/dg\/learning-algorithm.html\" rel=\"nofollow noreferrer\">http:\/\/docs.aws.amazon.com\/machine-learning\/latest\/dg\/learning-algorithm.html<\/a> Amazon ML uses SGD. However I can't find how many hidden layers are used in the neural network?<\/p>\n\n<p>2) Can someone confirm that SageMaker would be able to do what Amazon ML does? i.e. SageMaker is more powerful than Amazon ML?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2017-12-04 00:55:11.920000 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":"2017-12-13 05:18:34.293000 UTC",
        "Question_score":5,
        "Question_tags":"amazon-web-services|amazon-machine-learning|amazon-sagemaker",
        "Question_view_count":2794,
        "Owner_creation_date":"2010-01-25 07:55:40.363000 UTC",
        "Owner_last_access_date":"2022-09-22 04:51:33.373000 UTC",
        "Owner_location":null,
        "Owner_reputation":2211,
        "Owner_up_votes":166,
        "Owner_down_votes":10,
        "Owner_views":176,
        "Answer_body":"<p>I'm not sure about Amazon ML but SageMaker uses the docker containers listed here for the built-in training: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-algo-docker-registry-paths.html\" rel=\"noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-algo-docker-registry-paths.html<\/a><\/p>\n\n<p>So, in general, anything you can do with Amazon ML you should be able to do with SageMaker (although Amazon ML has a pretty sweet schema editor).<\/p>\n\n<p>You can check out each of those containers to dive deep on how it all works.<\/p>\n\n<p>You can find an exhaustive list of available algorithms in SageMaker here:\n<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algos.html\" rel=\"noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algos.html<\/a><\/p>\n\n<p>For now, as of December 2017, these algorithms are all available:<\/p>\n\n<ul>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/linear-learner.html\" rel=\"noreferrer\">Linear Learner<\/a><\/li>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/fact-machines.html\" rel=\"noreferrer\">Factorization Machines<\/a><\/li>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost.html\" rel=\"noreferrer\">XGBoost Algorithm<\/a><\/li>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/image-classification.html\" rel=\"noreferrer\">Image Classification Algorithm<\/a><\/li>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/seq-2-seq.html\" rel=\"noreferrer\">Amazon SageMaker Sequence2Sequence<\/a><\/li>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/k-means.html\" rel=\"noreferrer\">K-Means Algorithm<\/a><\/li>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/pca.html\" rel=\"noreferrer\">Principal Component Analysis (PCA)<\/a><\/li>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/lda.html\" rel=\"noreferrer\">Latent Dirichlet Allocation (LDA)<\/a><\/li>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ntm.html\" rel=\"noreferrer\">Neural Topic Model (NTM)<\/a><\/li>\n<\/ul>\n\n<p>The general SageMaker SDK interface to these algorithms looks something like this:<\/p>\n\n<pre><code>from sagemaker import KMeans\nkmeans = KMeans(role=\"SageMakerRole\",\n                train_instance_count=2,\n                train_instance_type='ml.c4.8xlarge',\n                data_location=\"s3:\/\/training_data\/\",\n                output_path=\"s3:\/\/model_artifacts\/\",\n                k=10)\n<\/code><\/pre>\n\n<p>The libraries here: <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\" rel=\"noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples<\/a>\nand here: <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\" rel=\"noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk<\/a> are particularly useful for playing with SageMaker.<\/p>\n\n<p>You can also make use of Spark with SageMaker the Spark library here: <a href=\"https:\/\/github.com\/aws\/sagemaker-spark\" rel=\"noreferrer\">https:\/\/github.com\/aws\/sagemaker-spark<\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2017-12-05 11:00:55.643000 UTC",
        "Answer_last_edit_date":"2017-12-06 15:55:02.210000 UTC",
        "Answer_score":7.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":63138835,
        "Question_title":"How to save models trained locally in Amazon SageMaker?",
        "Question_body":"<p>I'm trying to use a local training job in SageMaker.<\/p>\n<p>Following this AWS notebook (<a href=\"http:\/\/mxnet_mnist_with_gluon_local_mode.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/mxnet_gluon_mnist\/mxnet_mnist_with_gluon_local_mode.ipynb<\/a>) I was able to train and predict locally.<\/p>\n<p>There is any way to train locally and save the trained model in the Amazon SageMaker Training Job section?\nOtherwise, how can I properly save trained models I trained using local mode?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2020-07-28 16:36:57.667000 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":"2020-07-28 17:59:02.730000 UTC",
        "Question_score":7,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":3195,
        "Owner_creation_date":"2016-05-27 23:31:32.937000 UTC",
        "Owner_last_access_date":"2022-07-18 14:07:45.653000 UTC",
        "Owner_location":"Rio de Janeiro, State of Rio de Janeiro, Brazil",
        "Owner_reputation":2243,
        "Owner_up_votes":497,
        "Owner_down_votes":32,
        "Owner_views":148,
        "Answer_body":"<p>There is no way to have your local mode training jobs appear in the AWS console. The intent of local mode is to allow for faster iteration\/debugging before using SageMaker for training your model.<\/p>\n<p>You can create SageMaker Models from local model artifacts. Compress your model artifacts into a <code>.tar.gz<\/code> file, upload that file to S3, and then create the Model (with the SDK or in the console).<\/p>\n<p>Documentation:<\/p>\n<ul>\n<li><a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html#using-models-trained-outside-of-amazon-sagemaker\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html#using-models-trained-outside-of-amazon-sagemaker<\/a><\/li>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateModel.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateModel.html<\/a><\/li>\n<\/ul>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-07-29 16:19:31.220000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":65099376,
        "Question_title":"Segmentation fault error in importing sentence_transformers in Azure Machine Learning Service Nvidia Compute",
        "Question_body":"<p>I would like to use sentence_transformers in AML to run XLM-Roberta model for sentence embedding. I have a script in which I import sentence_transformers:<\/p>\n<pre><code>from sentence_transformers import SentenceTransformer\n<\/code><\/pre>\n<p>Once I run my AML pipeline, the run fails on this script with the following error:<\/p>\n<pre><code>AzureMLCompute job failed.\nUserProcessKilledBySystemSignal: Job failed since the user script received system termination signal usually due to out-of-memory or segfault.\n    Cause: segmentation fault\n    TaskIndex: \n    NodeIp: #####\n    NodeId: #####\n<\/code><\/pre>\n<p>I'm pretty sure that this import is causing this error, because if I comment out this import, the rest of the script will run.\nThis is weird because the installation of the sentence_transformers succeed.<\/p>\n<p>This is the details of my compute:<\/p>\n<pre><code>Virtual machine size\nSTANDARD_NV24 (24 Cores, 224 GB RAM, 1440 GB Disk)\nProcessing Unit\nGPU - 4 x NVIDIA Tesla M60\n<\/code><\/pre>\n<p>Agent Pool:<\/p>\n<pre><code>Azure Pipelines\n<\/code><\/pre>\n<p>Agent Specification:<\/p>\n<pre><code>ubuntu-16.04\n<\/code><\/pre>\n<p>requirements.txt file:<\/p>\n<pre><code>torch==1.4.0\nsentence-transformers\n<\/code><\/pre>\n<p>Does anyone have a solution for this error?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-12-01 22:17:00.137000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-12-01 23:52:47.497000 UTC",
        "Question_score":1,
        "Question_tags":"azure|nvidia|azure-machine-learning-service|roberta-language-model|sentence-transformers",
        "Question_view_count":530,
        "Owner_creation_date":"2015-12-23 16:48:13.150000 UTC",
        "Owner_last_access_date":"2022-09-17 08:32:34.823000 UTC",
        "Owner_location":"Finland",
        "Owner_reputation":398,
        "Owner_up_votes":21,
        "Owner_down_votes":1,
        "Owner_views":28,
        "Answer_body":"<p>I fixed the issue by changing the pytorch version from 1.4.0 to 1.6.0.\nSo the requirements.txt looks like this:<\/p>\n<pre><code>torch==1.6.0\nsentence-transformers\n<\/code><\/pre>\n<p>At first I tried one of the older versions of sentence-transformers which was compatible with pytorch 1.4.0. But the older version doesn't support &quot;xml-roberta-base&quot; model, so I tried to upgrade the pytorch version.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-12-01 23:48:58.210000 UTC",
        "Answer_last_edit_date":"2020-12-01 23:57:49.223000 UTC",
        "Answer_score":2.0,
        "Question_valid_tags":[
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":72637756,
        "Question_title":"Azure Machine Learning Model deployment as AKS web service from multiple workspaces",
        "Question_body":"<p>I am trying to determine how the Az ML model deployment works with AKS. If you have a single AKS cluster but attach from two separate workspaces, will models from both workspaces get deployed into different azureml-fe's with different IP addresses OR a single azureml-fe with a single IP address? Reason I ask is because I want to purchase a certificate but am unsure if all the models (regardless of workspace) will get exposed under the same IP Address OR multiple IP Addresses? If its the former, I can do it with one certificate...otherwise I have to do it with multiple certificates or SAN based certificates. So if anyone has experience with this, please let me know!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-06-15 20:59:19.547000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"azure-aks|azure-machine-learning-service",
        "Question_view_count":127,
        "Owner_creation_date":"2013-08-15 14:39:30.773000 UTC",
        "Owner_last_access_date":"2022-09-23 17:28:32.673000 UTC",
        "Owner_location":null,
        "Owner_reputation":301,
        "Owner_up_votes":27,
        "Owner_down_votes":0,
        "Owner_views":52,
        "Answer_body":"<p>By checking AKS webservice class, we can do the multiple services links to single AKS cluster. The endpoint management was described in <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.webservice.aks.akswebservice?view=azure-ml-py\" rel=\"nofollow noreferrer\">document<\/a>, but this is representing one-to-one cluster and service. For multiple workspaces refer <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.webservice.aksendpoint?view=azure-ml-py#azureml-core-webservice-aksendpoint-create-version\" rel=\"nofollow noreferrer\">document<\/a>.<\/p>\n<p>Regarding <strong><code>azureml-fe<\/code><\/strong>. There will be one <strong>azureml-fe<\/strong> for one cluster. That means, when we are using different workspaces for deployment into one AKS, then only <strong>one<\/strong> <strong>azureml-fe<\/strong> and can be considered to take <strong>one certificate.<\/strong><\/p>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/v1\/how-to-deploy-azure-kubernetes-service?tabs=python#azure-ml-router\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/v1\/how-to-deploy-azure-kubernetes-service?tabs=python#azure-ml-router<\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2022-06-16 02:15:08.050000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":68709594,
        "Question_title":"Why does my sagemaker model need a larger instance for real-time inference",
        "Question_body":"<p>I have a sagemaker endpoint that serves a Random Forest Classifier SKLearn model. This model predicts the kind of user someone is based on there interaction on a website. <strong>I have had two releases for this model<\/strong>.<\/p>\n<p>The first model had 4 kinds of user: <code>'user-type-1', 'user-type-2', 'user-type-3', 'other'<\/code><\/p>\n<p>The second release differed from the first in that there was more training data used and there were 10 types of user <code>'user-type-1', 'user-type-2', 'user-type-3', 'user-type-4', 'user-type-5', 'user-type-6', 'user-type-7', 'user-type-8', 'user-type-9','other'<\/code>. To be more specific, in the first model many users got predicted as <code>'other'<\/code> whereas in the second model many of these users were in one of the new categories.<\/p>\n<p><strong>The parameters for the random forest was the same in both models<\/strong>.<\/p>\n<p><strong>Question:<\/strong> The second model uses a lot more memory than the first and I cannot figure out why. Both models are of similar size and the same number of calls to the endpoint are being made under the second release. Any ideas why I might need a larger instance with more memory in the second model than the first?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-08-09 09:15:59.870000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-08-09 09:27:48.237000 UTC",
        "Question_score":0,
        "Question_tags":"python|machine-learning|random-forest|amazon-sagemaker",
        "Question_view_count":52,
        "Owner_creation_date":"2018-03-22 16:45:24.657000 UTC",
        "Owner_last_access_date":"2022-09-23 14:29:34.747000 UTC",
        "Owner_location":"Milton Keynes",
        "Owner_reputation":738,
        "Owner_up_votes":22,
        "Owner_down_votes":7,
        "Owner_views":69,
        "Answer_body":"<p>The <code>values<\/code> attribute of classification-type SkLearn tree models is essentially a data matrix <code>(n_leaves, n_classes)<\/code> (probability distributions associated with each leaf node).<\/p>\n<p>In the first experiment this matrix has 4 columns, in the second experiment it has 10 columns - a 2.5X increase in size.<\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_date":"2021-08-09 10:43:55.360000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":69962965,
        "Question_title":"How to get an AWS Feature Store feature group into the ACTIVE state?",
        "Question_body":"<p>I am trying to ingest some rows into a Feature Store on AWS using:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>feature_group.ingest(data_frame=df, max_workers=8, wait=True)\n<\/code><\/pre>\n<p>but I am getting the following error:<\/p>\n<blockquote>\n<p>Failed to ingest row 1: An error occurred (ValidationError) when\ncalling the PutRecord operation: Validation Error: FeatureGroup\n[feature-group] is not in ACTIVE state.<\/p>\n<\/blockquote>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-11-14 12:25:51.657000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"amazon-web-services|aws-glue|amazon-sagemaker|data-ingestion|aws-feature-store",
        "Question_view_count":383,
        "Owner_creation_date":"2013-03-10 11:22:30.047000 UTC",
        "Owner_last_access_date":"2022-08-30 07:21:38.897000 UTC",
        "Owner_location":"Tel Aviv",
        "Owner_reputation":2791,
        "Owner_up_votes":163,
        "Owner_down_votes":13,
        "Owner_views":174,
        "Answer_body":"<p>It turns out the status of a feature group after its creation is <code>Created<\/code> but before you can ingest any rows you need to simply wait until it's <code>Active<\/code>:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>while status != 'Created':\n        try:\n            status = feature_group.describe()['OfflineStoreStatus']['Status']\n        except:\n            pass\n        print('Offline store status: {}'.format(status))    \n        sleep(15)\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-11-15 12:39:44.023000 UTC",
        "Answer_last_edit_date":"2022-03-03 07:01:53.523000 UTC",
        "Answer_score":0.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":58323029,
        "Question_title":"Azure ML deploy locally: tarfile.ReadError: file could not be opened successfully",
        "Question_body":"<p>I am trying to deploy(local) using this line:<\/p>\n\n<pre><code>local_service = Model.deploy(ws, \"test\", [model], inference_config, deployment_config)\n<\/code><\/pre>\n\n<p>Then I get this output in the terminal:<\/p>\n\n<pre><code>tarfile.ReadError: file could not be opened successfully\n<\/code><\/pre>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/WXbPe.png\" rel=\"nofollow noreferrer\">Screenshot of the output<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-10-10 12:35:41.163000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"python|deployment|azure-machine-learning-service",
        "Question_view_count":77,
        "Owner_creation_date":"2019-10-10 10:48:01.987000 UTC",
        "Owner_last_access_date":"2020-07-12 09:32:11.983000 UTC",
        "Owner_location":null,
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":"<p>There was a bug with the retry logic when files were being uploaded. That bug has since been fixed, so updating your SDK should fix the issue.<\/p>\n\n<p>Similar post: <a href=\"https:\/\/stackoverflow.com\/questions\/57854136\/registering-and-downloading-a-fasttext-bin-model-fails-with-azure-machine-learn\">Registering and downloading a fastText .bin model fails with Azure Machine Learning Service<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2019-10-10 16:46:03.837000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":70623713,
        "Question_title":"How can I pass parameters to a Vertex AI Platform Pipeline?",
        "Question_body":"<p>I have created a Vertex AI pipeline similar to <a href=\"https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/main\/notebooks\/official\/pipelines\/google_cloud_pipeline_components_automl_images.ipynb\" rel=\"nofollow noreferrer\">this.<\/a><\/p>\n<p>Now the pipeline has reference to a csv file. So if this csv file changes the pipeline needs to be recreated.<\/p>\n<p>Is there any way to pass a new csv as a parameter to the pipeline when it is re-run? That is without recreating the pipeline using the notebook?<\/p>\n<p>If not, is there a best practice way of auto updating the dataset, model and deployment?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-01-07 15:42:30.640000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":611,
        "Owner_creation_date":"2012-10-25 08:48:34.717000 UTC",
        "Owner_last_access_date":"2022-09-23 10:10:32.783000 UTC",
        "Owner_location":null,
        "Owner_reputation":2564,
        "Owner_up_votes":304,
        "Owner_down_votes":8,
        "Owner_views":451,
        "Answer_body":"<p>Have a look to <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/pipelines\/build-pipeline\" rel=\"nofollow noreferrer\">that documentation<\/a>.<\/p>\n<p>You can define your pipeline like that<\/p>\n<pre><code>...\n# Define the workflow of the pipeline.\n@kfp.dsl.pipeline(\n    name=&quot;automl-image-training-v2&quot;,\n    pipeline_root=pipeline_root_path)\ndef pipeline(project_id: str):\n...\n<\/code><\/pre>\n<p>(you have something very similar in your notebook sample)<\/p>\n<p>Then, when you invoke your pipeline, you can pass some parameter<\/p>\n<pre><code>import google.cloud.aiplatform as aip\n\njob = aip.PipelineJob(\n    display_name=&quot;automl-image-training-v2&quot;,\n    template_path=&quot;image_classif_pipeline.json&quot;,\n    pipeline_root=pipeline_root_path,\n    parameter_values={\n        'project_id': project_id\n    }\n)\n\njob.submit()\n<\/code><\/pre>\n<p>You can see the <code>project_id<\/code> a dict parameter in the parameter values, and in parameter of your pipeline function.<\/p>\n<p>Do the same for your CSV file name!<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-01-07 20:47:51.210000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":3.0,
        "Question_valid_tags":[
            "google-cloud-vertex-ai"
        ]
    },
    {
        "Question_id":65577286,
        "Question_title":"Dlr model gives notorious result for object detection model",
        "Question_body":"<p>I am following this <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker_neo_compilation_jobs\/gluoncv_ssd_mobilenet\/gluoncv_ssd_mobilenet_neo.ipynb\" rel=\"nofollow noreferrer\">link<\/a> to train an object detection model. I am able to successfully deploy the model on EC2 instance. The accuracy was good. I complied the same model file for m edge Device Jetson Nano. My inference code looks like below,<\/p>\n<pre><code>from dlr import DLRModel\nimport json\nimport cv2\n\nmodel = DLRModel('model', 'gpu')\n\nimg = cv2.imread('test.jpg')\nimg = cv2.resize(img, (512, 512), interpolation=cv2.INTER_AREA)\nimg = np.expand_dims(img, 0)\n\noutputs = model.run(img)\nobjects=outputs[0][0]\nscores=outputs[1][0]\nbounding_boxes=outputs[2][0]\n<\/code><\/pre>\n<p>When I look the result, it's not at all matched with SageMaker Notebook instance result. Boudning boxes' values are sometime in ~70000. I couldn't understand the format of result produced by DLR.<\/p>\n<p>Sample result for an image.<\/p>\n<p>Classes:\n[[10.0], [14.0], [4.0], [10.0], [14.0], [-1.0], [-1.0], [-1.0], [7.0], [-1.0], [6.0], [11.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [2.0], [-1.0], [-1.0], [0.0], [-1.0], [17.0], [-1.0], [-1.0], [6.0], [18.0], [-1.0], [-1.0], [-1.0], [18.0], [-1.0], [12.0], [-1.0], [-1.0], [13.0], [-1.0], [-1.0], [-1.0], [1.0], [-1.0], [-1.0], [5.0], [-1.0], [0.0], [-1.0], [-1.0], [-1.0], [-1.0], [3.0], [8.0], [5.0], [-1.0], [-1.0], [15.0], [-1.0], [9.0], [3.0], [-1.0], [10.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [16.0], [8.0], [-1.0], [16.0], [19.0], [-1.0], [9.0], [-1.0], [4.0], [-1.0], [-1.0], [15.0], [10.0], [-1.0], [-1.0], [4.0], [-1.0], [8.0], [2.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0]]<\/p>\n<p>scores:\n[[0.9527158737182617], [0.910746157169342], [0.28013306856155396], [0.059000786393880844], [0.04898739233613014], [-1.0], [-1.0], [-1.0], [0.04864144325256348], [-1.0], [0.04847110062837601], [0.04843416064977646], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [0.04821664094924927], [-1.0], [-1.0], [0.04808368161320686], [-1.0], [0.0479729063808918], [-1.0], [-1.0], [0.04782549664378166], [0.04778601974248886], [-1.0], [-1.0], [-1.0], [0.0475776381790638], [-1.0], [0.047538403421640396], [-1.0], [-1.0], [0.047468967735767365], [-1.0], [-1.0], [-1.0], [0.04737424850463867], [-1.0], [-1.0], [0.047330085188150406], [-1.0], [0.04730956256389618], [-1.0], [-1.0], [-1.0], [-1.0], [0.04710235074162483], [0.04710135608911514], [0.047083333134651184], [-1.0], [-1.0], [0.047033149749040604], [-1.0], [0.0469636432826519], [0.046939317137002945], [-1.0], [0.04687687009572983], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [0.046708278357982635], [0.046680934727191925], [-1.0], [0.04660974070429802], [0.046597886830568314], [-1.0], [0.04656397923827171], [-1.0], [0.046513814479112625], [-1.0], [-1.0], [0.04647510126233101], [0.04644943028688431], [-1.0], [-1.0], [0.046245038509368896], [-1.0], [0.01647786796092987], [0.010514501482248306], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0]]<\/p>\n<p>Bouding boxes:\n[[523.2319946289062, -777751.875, 523.5722045898438, 778992.625], [425.0546875, -1141093.0, 429.6099853515625, 1142524.5], [680.2073364257812, 542.9566650390625, 680.2202758789062, 543.36376953125], [425.0546875, -1141093.0, 429.6099853515625, 1142524.5], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [591.7652587890625, -1121890.0, 592.9493408203125, 1123299.75], [523.2319946289062, -777751.875, 523.5722045898438, 778992.625], [-1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0]]<\/p>\n<p>What causes this notorious result?\nIs there any issue while compiling model in Neo?\nAny issue in inference Code?<\/p>\n<p>Any hint would be appreciable.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-01-05 10:39:53.240000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|amazon-ec2|amazon-sagemaker|mxnet",
        "Question_view_count":98,
        "Owner_creation_date":"2015-03-18 10:49:38.223000 UTC",
        "Owner_last_access_date":"2022-09-24 14:02:18.917000 UTC",
        "Owner_location":"Coimbatore, Tamil Nadu, India",
        "Owner_reputation":10189,
        "Owner_up_votes":1483,
        "Owner_down_votes":261,
        "Owner_views":1471,
        "Answer_body":"<p>The reason behind the abnormal result is due to improper pre-processing method was applied. Here is the complete inference code for Mobilenet-ssd model.<\/p>\n<pre><code>def transform(img):\n    # Normalize\n    mean_vec = np.array([0.485, 0.456, 0.406])\n    stddev_vec = np.array([0.229, 0.224, 0.225])\n    image = (img \/ 255 - mean_vec) \/ stddev_vec\n\n    # Transpose\n    if len(image.shape) == 2:  # for greyscale image\n        image = np.expand_dims(image, axis=2)\n\n    image = np.rollaxis(image, axis=2, start=0)[np.newaxis, :]\n    return image\n\nmodel = DLRModel(model_dir, 'gpu')\n\n\nfor file_name in image_folder:\n    image = PIL.Image.open(file_name)\n    image = np.asarray(image.resize((512, 512)))\n    image = transform(image)\n    # flatten within a input array\n    input_data = {'data': image}\n    outputs = model.run(input_data)\n    objects = outputs[0]\n    scores = outputs[1]\n    bounding_boxes = outputs[2]\n    result = [objects.tolist(), scores.tolist(), bounding_boxes.tolist()]\n    print(json.dumps(result))\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-01-06 05:53:03.447000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":57971689,
        "Question_title":"Disk I\/O extremely slow on P100-NC6s-V2",
        "Question_body":"<p>I am training an image segmentation model on azure ML pipeline. During the testing step, I'm saving the output of the model to the associated blob storage. Then I want to find the IOU (Intersection over Union) between the calculated output and the ground truth. Both of these set of images lie on the blob storage. However, IOU calculation is extremely slow, and I think it's disk bound. In my IOU calculation code, I'm just loading the two images (commented out other code), still, it's taking close to 6 seconds per iteration, while training and testing were fast enough. <\/p>\n\n<p>Is this behavior normal? How do I debug this step?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-09-17 09:50:01.943000 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"tensorflow|azure-machine-learning-service",
        "Question_view_count":415,
        "Owner_creation_date":"2014-08-15 23:27:51.463000 UTC",
        "Owner_last_access_date":"2021-05-13 06:30:45.853000 UTC",
        "Owner_location":"India",
        "Owner_reputation":65,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Answer_body":"<p>A few notes on the drives that an AzureML remote run has available:<\/p>\n\n<p>Here is what I see when I run <code>df<\/code> on a remote run (in this one, I am using a blob <code>Datastore<\/code> via <code>as_mount()<\/code>):<\/p>\n\n<pre><code>Filesystem                             1K-blocks     Used  Available Use% Mounted on\noverlay                                103080160 11530364   86290588  12% \/\ntmpfs                                      65536        0      65536   0% \/dev\ntmpfs                                    3568556        0    3568556   0% \/sys\/fs\/cgroup\n\/dev\/sdb1                              103080160 11530364   86290588  12% \/etc\/hosts\nshm                                      2097152        0    2097152   0% \/dev\/shm\n\/\/danielscstorageezoh...-620830f140ab 5368709120  3702848 5365006272   1% \/mnt\/batch\/tasks\/...\/workspacefilestore\nblobfuse                               103080160 11530364   86290588  12% \/mnt\/batch\/tasks\/...\/workspaceblobstore\n<\/code><\/pre>\n\n<p>The interesting items are <code>overlay<\/code>, <code>\/dev\/sdb1<\/code>, <code>\/\/danielscstorageezoh...-620830f140ab<\/code> and <code>blobfuse<\/code>:<\/p>\n\n<ol>\n<li><code>overlay<\/code> and <code>\/dev\/sdb1<\/code> are both the mount of the <strong>local SSD<\/strong> on the machine (I am using a STANDARD_D2_V2 which has a 100GB SSD).<\/li>\n<li><code>\/\/danielscstorageezoh...-620830f140ab<\/code> is the mount of the <strong>Azure File Share<\/strong> that contains the project files (your script, etc.). It is also the <em>current working directory<\/em> for your run.<\/li>\n<li><strong><code>blobfuse<\/code><\/strong> is the blob store that I had requested to mount in the <code>Estimator<\/code> as I executed the run.<\/li>\n<\/ol>\n\n<p>I was curious about the performance differences between these 3 types of drives. My mini benchmark was to download and extract this file: <a href=\"http:\/\/download.tensorflow.org\/example_images\/flower_photos.tgz\" rel=\"nofollow noreferrer\">http:\/\/download.tensorflow.org\/example_images\/flower_photos.tgz<\/a> (it is a 220 MB tar file that contains about 3600 jpeg images of flowers).<\/p>\n\n<p>Here the results:<\/p>\n\n<pre><code>Filesystem\/Drive         Download_and_save       Extract\nLocal_SSD                               2s            2s  \nAzure File Share                        9s          386s\nPremium File Share                     10s          120s\nBlobfuse                               10s          133s\nBlobfuse w\/ Premium Blob                8s          121s\n<\/code><\/pre>\n\n<p>In summary, writing small files is much, much slower on the network drives, so it is highly recommended to use \/tmp or Python <code>tempfile<\/code> if you are writing smaller files. <\/p>\n\n<p>For reference, here the script I ran to measure: <a href=\"https:\/\/gist.github.com\/danielsc\/9f062da5e66421d48ac5ed84aabf8535\" rel=\"nofollow noreferrer\">https:\/\/gist.github.com\/danielsc\/9f062da5e66421d48ac5ed84aabf8535<\/a><\/p>\n\n<p>And this is how I ran it: <a href=\"https:\/\/gist.github.com\/danielsc\/6273a43c9b1790d82216bdaea6e10e5c\" rel=\"nofollow noreferrer\">https:\/\/gist.github.com\/danielsc\/6273a43c9b1790d82216bdaea6e10e5c<\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2019-09-17 15:56:27.297000 UTC",
        "Answer_last_edit_date":"2019-10-29 00:19:27.397000 UTC",
        "Answer_score":4.0,
        "Question_valid_tags":[
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":56418684,
        "Question_title":"Possible to access the internal representation of a neural network trained in Azure Machine Learning Service or Azure Machine Learning Studio?",
        "Question_body":"<p>I'm working with data scientists who would like to gain insight and understanding of the neural network models that they train using the visual interfaces in Azure Machine Learning Studio\/Service. Is it possible to dump out and inspect the internal representation of a neural network model? Is there a way that I could write code that accesses the nodes and weights of a trained neural network in order to visualize the network as a graph structure? Or if Azure Machine Learning Studio\/Service doesn't support this I'd appreciate advice on a different machine learning framework that might be more appropriate for this kind of analysis.<\/p>\n\n<p>Things I have tried:<\/p>\n\n<ul>\n<li>Train Model outputs an ILearnerDotNet (AML Studio) or Model (AML Service). I looked for items to drag into the workspace where I could write custom code such as Execute Python Script. They seem to accept datasets, but not ILearnerDotNet\/Model as input.<\/li>\n<li>I wasn't able to locate documentation about the ILearnerDotNet\/Model interfaces.<\/li>\n<li>Selecting the Train Model output offers the option to Save as Trained Model. This creates a trained model object and that would help me reference the trained model in other places, but I didn't find a way to use this to get at its internals.<\/li>\n<\/ul>\n\n<p>I'm new to the Azure Machine Learning landscape, and could use some help with how to get started on how to access this data.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-06-02 20:20:58.340000 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"neural-network|azure-machine-learning-studio|azure-machine-learning-service",
        "Question_view_count":167,
        "Owner_creation_date":"2015-03-29 15:33:13.810000 UTC",
        "Owner_last_access_date":"2022-09-20 21:41:22.517000 UTC",
        "Owner_location":null,
        "Owner_reputation":45,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Answer_body":"<p>Quote from Azure ML Exam reference:<\/p>\n\n<blockquote>\n  <p>By default, the architecture of neural networks is limited to a single\n  hidden layer with sigmoid as the activation function and softmax in\n  the last layer. You can change this in the properties of the model,\n  opening the Hidden layer specification dropdown list, and selecting a\n  Custom definition script. A text box will appear in which you will be\n  able to insert a Net# script. This script language allows you to\n  define neural networks architectures.<\/p>\n<\/blockquote>\n\n<p>For instance, if you want to create a two layer network, you may put the following code.<\/p>\n\n<pre><code>input Picture [28, 28];\nhidden H1 [200] from Picture all;\nhidden H2 [200] from H1 all;\noutput Result [10] softmax from H2 all;\n<\/code><\/pre>\n\n<p>Nevertheless, with Net# you will face certain limitations as, it does not accept regularization (neither L2 nor dropout). Also, there is no ReLU activation that are\ncommonly used in deep learning due to their benefits in backpropagation. You cannot modify the batch size of the Stochastic Gradient Descent (SGD). Besides that, you cannot use other optimization algorithms. You can use SGD with momentum, but not others like Adam, or RMSprop. You cannot define recurrent or recursive neural networks.<\/p>\n\n<p>Another great tool is CNTK (Cognitive Toolkit) that allows you defining your computational graph and create a fully customizable model.\nQuote from documentation<\/p>\n\n<blockquote>\n  <p>It is a Microsoft open source deep learning toolkit. Like other deep\n  learning tools, CNTK is based on the construction of computational\n  graphs and their optimization using automatic differentiation. The\n  toolkit is highly optimized and scales efficiently (from CPU, to GPU,\n  to multiple machines). CNTK is also very portable and flexible; you\n  can use it with programming languages like Python, C#, or C++, but you\n  can also use a model description language called BrainScript.<\/p>\n<\/blockquote>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2019-06-07 08:31:09.913000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_valid_tags":[
            "azure-machine-learning-studio",
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":60975078,
        "Question_title":"How to only load one portion of an AzureML tabular dataset (linked to Azure Blob Storage)",
        "Question_body":"<p>I have a DataSet defined in my AzureML workspace that is linked to an Azure Blob Storage csv file of 1.6Gb.  This file contains timeseries information of around 10000 devices.  So, I could've also created 10000 smaller files (since I use ADF for the transmission pipeline).<\/p>\n\n<p>My question now is: is it possible to load a part of the AzureML DataSet in my python notebook or script instead of loading the entire file?<br>\nThe only code I have now load the full file:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>dataset = Dataset.get_by_name(workspace, name='devicetelemetry')\ndf = dataset.to_pandas_dataframe()\n<\/code><\/pre>\n\n<p>The only concept of partitions I found with regards to the AzureML datasets was around time series and partitioning of timestamps &amp; dates.  However, here I would love to partition per device, so I can very easily just do a load of all telemetry of a specific device.<\/p>\n\n<p>Any pointers to docs or any suggestions? (I couldn't find any so far)<\/p>\n\n<p>Thanks already<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_date":"2020-04-01 15:53:54.767000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"python|azure-machine-learning-service",
        "Question_view_count":560,
        "Owner_creation_date":"2013-02-12 07:50:30.743000 UTC",
        "Owner_last_access_date":"2022-09-21 18:28:12.907000 UTC",
        "Owner_location":"Belgium",
        "Owner_reputation":2947,
        "Owner_up_votes":297,
        "Owner_down_votes":16,
        "Owner_views":355,
        "Answer_body":"<p>You're right there are the <code>.time_*()<\/code> filtering methods available with a <code>TabularDataset<\/code>.<\/p>\n\n<p>I'm not aware of anyway to do filtering as you suggest (but I agree it would be a useful feature). To get per-device partitioning, my recommendation would be to structure your container like so:<\/p>\n\n<pre><code>- device1\n    - 2020\n        - 2020-03-31.csv\n        - 2020-04-01.csv\n- device2\n   - 2020\n        - 2020-03-31.csv\n        - 2020-04-01.csv\n<\/code><\/pre>\n\n<p>In this way you can define an all-up Dataset, but also per-device Datasets by passing folder of the device to the DataPath<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code># all up dataset\nds_all = Dataset.Tabular.from_delimited_files(\n    path=DataPath(datastore, '*')\n)\n# device 1 dataset\nds_d1 = Dataset.Tabular.from_delimited_files(\n    path=DataPath(datastore, 'device1\/*')\n)\n<\/code><\/pre>\n\n<p><strong>CAVEAT<\/strong><\/p>\n\n<p>dataprep SDK is optimized for blobs around 200MB in size. So you can work with many small files, but sometimes it can be slower than expected, especially considering the overhead of enumerating all blobs in a container.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-04-01 17:12:16.673000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":64703268,
        "Question_title":"How to use AWS SageMaker and S3 for Object Detection?",
        "Question_body":"<p>I am looking to run a pre-trained object detection model onto a folder of ~400k images which is about 1.5GB. When I've tried running locally, it was estimated to take ~8 days to complete (with keras yolov3). Thus, I am looking to use AWS SageMaker and S3.<\/p>\n<p>When I have uploaded the zip folder of my images in the SageMaker jupyter notebook and tried to unzip by using bash command, an error pops ups saying that I have insufficient space. The volume assigned to my notebook is 5GB EBS, I do have other heavy datasets in my jupyter notebook space which could be causing this issue.<\/p>\n<p>To tackle that, I am looking for a way where I can upload my data to S3 and run SageMaker to read the images hosted and run an object detection model over. However, it does not look like there's a method to unzip folders on S3 without using an additional service (read that AWS Lambda may help) as these services are paid by my school.<\/p>\n<p>I could possibly re-run my code to extract my images from URL. In this case, how can I save these images to S3 directly in this case? Also, does anyone know if I am able to run yolov3 on SageMaker or if there is a better model I can look to use. Appreciate any advice that may help.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":5,
        "Question_creation_date":"2020-11-05 18:28:16.053000 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-s3|object-detection|amazon-sagemaker",
        "Question_view_count":256,
        "Owner_creation_date":"2017-10-13 07:37:10.153000 UTC",
        "Owner_last_access_date":"2022-09-20 08:00:20.060000 UTC",
        "Owner_location":"Singapore",
        "Owner_reputation":180,
        "Owner_up_votes":268,
        "Owner_down_votes":0,
        "Owner_views":29,
        "Answer_body":"<p>yes u are right u can upload thousands of images using aws cli using $aws s3 cp  ; or $aws s3 sync<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-11-06 02:23:45.800000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":73434003,
        "Question_title":"Read images from a bucket in GCP for ML",
        "Question_body":"<p>I have created a bucket in GCP containing my images dataset.<\/p>\n<p>The path to it is: xray-competition-bucket\/img_align_celeba<\/p>\n<p>How do I read it from GCP to Jupyter Lab in Vertex AI?<\/p>\n<p>My code is:<\/p>\n<pre><code>MAIN_PATH = '\/gcs\/xray-competition-bucket\/img_align_celeba'\n\nimage_paths = glob((MAIN_PATH + &quot;\/*.jpg&quot;))\n<\/code><\/pre>\n<p>and the result is that image_paths is an empty array.<\/p>\n<p>Note: I also tried the path gs:\/\/my_bucket\/...<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_date":"2022-08-21 11:30:10.723000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|jupyter-lab|bucket|google-cloud-vertex-ai",
        "Question_view_count":79,
        "Owner_creation_date":"2021-12-12 12:36:30.127000 UTC",
        "Owner_last_access_date":"2022-09-19 20:00:07.140000 UTC",
        "Owner_location":null,
        "Owner_reputation":35,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":"<p>You will need to <a href=\"https:\/\/cloud.google.com\/storage\/docs\/downloading-objects#storage-download-object-python\" rel=\"nofollow noreferrer\">download the GCS file locally<\/a> using <code>gsutil<\/code> or the python SDK if you want to use glob. There are also libraries like <a href=\"https:\/\/gcsfs.readthedocs.io\/en\/latest\/\" rel=\"nofollow noreferrer\">GCSFS<\/a> or TensorFlow's <a href=\"https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/io\/gfile\/GFile\" rel=\"nofollow noreferrer\">GFile<\/a> which offer a pythonic file-system interface for working with GCS. For example, here is <a href=\"https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/io\/gfile\/glob\" rel=\"nofollow noreferrer\">GFile.glob<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-08-23 15:00:06.083000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "google-cloud-vertex-ai"
        ]
    },
    {
        "Question_id":69931411,
        "Question_title":"Can't change virtual environment within Azure ML notebook",
        "Question_body":"<p>For some reason the jupyter notebooks on my VM are in the wrong environment (ie stuck in <code>(base)<\/code>). Furthermore, I can change the environment in the terminal but not in the notebook. Here is what happens when I attempt <code>!conda activate desired_env<\/code> in the notebook:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>CommandNotFoundError: Your shell has not been properly configured to use 'conda activate'.\nTo initialize your shell, run\n\n    $ conda init &lt;SHELL_NAME&gt;\n\nCurrently supported shells are:\n  - bash\n  - fish\n  - tcsh\n  - xonsh\n  - zsh\n  - powershell\n\nSee 'conda init --help' for more information and options.\n\nIMPORTANT: You may need to close and restart your shell after running 'conda init'.\n\n\n# conda environments:\n#\nbase                  *  \/anaconda\nazureml_py36             \/anaconda\/envs\/azureml_py36\nazureml_py38             \/anaconda\/envs\/azureml_py38\nazureml_py38_pytorch     \/anaconda\/envs\/azureml_py38_pytorch\nazureml_py38_tensorflow     \/anaconda\/envs\/azureml_py38_tensorflow\n<\/code><\/pre>\n<p>I tried the answers <a href=\"https:\/\/stackoverflow.com\/questions\/61915607\/commandnotfounderror-your-shell-has-not-been-properly-configured-to-use-conda\">here<\/a> (e.g., first running <code>!source \/anaconda\/etc\/profile.d\/conda.sh<\/code>).<\/p>\n<p>I also tried activating the environment using <code>source<\/code> rather than 'conda activate': <code>!source \/anaconda\/envs\/desired_env\/bin\/activate<\/code>. This runs but doesn't actually do anything when I see the current environment in <code>conda env list<\/code><\/p>\n<p>Edit: also adding that if I install a package in the <code>(base)<\/code> environment in the terminal, I still don't have access to it in jupyter notebook.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-11-11 16:04:54.700000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-11-11 16:10:28.003000 UTC",
        "Question_score":1,
        "Question_tags":"azure|jupyter-notebook|virtual-machine|conda|azure-machine-learning-studio",
        "Question_view_count":672,
        "Owner_creation_date":"2020-04-15 19:38:22.910000 UTC",
        "Owner_last_access_date":"2022-09-24 21:34:11.227000 UTC",
        "Owner_location":null,
        "Owner_reputation":361,
        "Owner_up_votes":18,
        "Owner_down_votes":2,
        "Owner_views":47,
        "Answer_body":"<p>I'm the PM that released AzureML Notebooks, you can't activate a Conda env from a cell, you have to create a new kernel will the Conda Env. Here are the instructions: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-terminal#add-new-kernels\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-terminal#add-new-kernels<\/a><\/p>",
        "Answer_comment_count":8.0,
        "Answer_creation_date":"2021-11-11 17:44:00.790000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":4.0,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":68658385,
        "Question_title":"Azure Machine Learning Studio designer - \"create new version\" unexpected when registering a data set",
        "Question_body":"<p>I am trying to register a data set as a Python step with the Azure Machine Learning Studio designer. Here is my code:<\/p>\n<pre><code>import pandas as pd\nfrom azureml.core import Workspace, Run, Dataset\n\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n    run = Run.get_context()\n    ws = run. experiment.workspace\n    ds = Dataset.from_pandas_dataframe(dataframe1)\n    ds.register(workspace = ws,\n                name = &quot;data set name&quot;,\n                description = &quot;example description&quot;,\n                create_new_version = True)\n    return dataframe1, \n<\/code><\/pre>\n<p>I get an error saying that &quot;create_new_version&quot; in the ds.register line was an unexpected keyword argument. However, this keyword appears in the documentation and I need it to keep track of new versions of the file.<\/p>\n<p>If I remove the argument, I get a different error: &quot;Local data source path not supported for this operation&quot;, so it still does not work. Any help is appreciated. Thanks!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_date":"2021-08-04 21:49:31.587000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"python|azure-machine-learning-studio|azure-machine-learning-service",
        "Question_view_count":565,
        "Owner_creation_date":"2013-06-17 20:00:29.817000 UTC",
        "Owner_last_access_date":"2022-09-24 13:07:45.427000 UTC",
        "Owner_location":null,
        "Owner_reputation":1111,
        "Owner_up_votes":124,
        "Owner_down_votes":2,
        "Owner_views":191,
        "Answer_body":"<h2>update<\/h2>\n<p>sharing OP's solution here for easier discovery<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\nfrom azureml.core import Workspace, Run, Dataset\n\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n    run = Run.get_context()\n    ws = run. experiment.workspace\n    datastore = ws.get_default_datastore()\n    ds = Dataset.Tabular.register_pandas_dataframe(\n        dataframe1, datastore, 'data_set_name',\n        description = 'data set description.')\n    return dataframe1,\n<\/code><\/pre>\n<h2>original answer<\/h2>\n<p>Sorry you're struggling. You're very close!<\/p>\n<p>A few things may be the culprit here.<\/p>\n<ol>\n<li>It looks like you're using the <code>Dataset<\/code> class, which has been deprecated. I recommend trying <code>Dataset.Tabular.register_pandas_dataframe()<\/code> (<a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data.dataset_factory.tabulardatasetfactory?view=azure-ml-py#register-pandas-dataframe-dataframe--target--name--description-none--tags-none--show-progress-true-\" rel=\"nofollow noreferrer\">docs link<\/a>) instead of <code>Dataset.from_pandas_dataframe()<\/code>. (<a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/work-with-data\/dataset-api-change-notice.md\" rel=\"nofollow noreferrer\">more about the Dataset API deprecation<\/a>)<\/li>\n<li>More conjectire here, but another thing is there might be some limitations to using dataset registration within an &quot;Execute Python Script&quot; (EPS) module due to:\n<ol>\n<li>the workspace object might not have the right permissions<\/li>\n<li>you might not be able to use the <code>register_pandas_dataframe<\/code> method inside the EPS module, but might have better luck with save the dataframe first to parquet, then calling <code>Dataset.Tabular.from_parquet_files<\/code><\/li>\n<\/ol>\n<\/li>\n<\/ol>\n<p>Hopefully something works here!<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2021-08-04 23:22:42.627000 UTC",
        "Answer_last_edit_date":"2021-08-05 16:21:12.980000 UTC",
        "Answer_score":2.0,
        "Question_valid_tags":[
            "azure-machine-learning-studio",
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":73720626,
        "Question_title":"How can I select a column product of a math operation in Azure Machine Learning Designer?",
        "Question_body":"<p>I have created a pipeline in Azure Machine Learning that includes a <strong>Math Operation<\/strong> (natural logarithm of a column named <em>charges<\/em>). The next pill to the <strong>Math Operatio<\/strong>n is <strong>Select Column in Dataset<\/strong>. Since the pipeline has not ben submitted and run I cannot access the column <em>ln(charges)<\/em> in the pill <strong>Select Column in Dataset<\/strong>.\nMy problem is that if I submit it I am able to run it and see the results in the pipeline once completed, but I have found no way of accessing those results (and thus the <em>ln(charges)<\/em> column in Designer.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/DOddA.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/DOddA.png\" alt=\"Pipeline Job after submitting and running\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Hp6Dc.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Hp6Dc.png\" alt=\"Pipeline in designer after submitting and running the job\" \/><\/a><\/p>\n<p><strong>UPDATE:<\/strong><\/p>\n<p><strong>I have found a workaround. Still in designer the column ln(charges) is not selectable but if I manually enter Ln(charges) in the select column fields it works.<\/strong><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-09-14 16:56:26.247000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-09-15 09:10:12.833000 UTC",
        "Question_score":1,
        "Question_tags":"azure|machine-learning|designer|azure-machine-learning-studio",
        "Question_view_count":55,
        "Owner_creation_date":"2018-05-15 15:20:25.170000 UTC",
        "Owner_last_access_date":"2022-09-23 12:16:24.777000 UTC",
        "Owner_location":"Spain",
        "Owner_reputation":47,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":17,
        "Answer_body":"<p>The following is the procedure of the math operation in Azure ML designer to select the column to be implemented. The following procedure will help to give the column name as well as we can also give the index number of the column. This answer contains both the procedures.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/9YV8f.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/9YV8f.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>We can click on edit column.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/bAnfq.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/bAnfq.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/cZFfF.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/cZFfF.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Based on the dataset which the experiment was running, both are options are mentioned in the above screen. We can choose either of the options.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/PblH7.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/PblH7.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>To access the data, right click and go to access data and click on result_dataset<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/8jHz5.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/8jHz5.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>The following page will open and click on any file mentioned in the box<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/4jWxT.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/4jWxT.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Click on download and open in the editor according to your wish.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/wfGPV.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/wfGPV.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>It looks like the above result screen.\nThe below screens are the designer created.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/bTWGR.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/bTWGR.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/6kUAw.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/6kUAw.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/I2Ej1.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/I2Ej1.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>To check the final model result. Go to evaluate model and get the results in visualization manner.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2022-09-21 08:01:03.323000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":73027674,
        "Question_title":"How does one move python and other types of files from one GCP notebook instance to another?",
        "Question_body":"<p>I have a Vertex AI notebook that contains a lot of python and jupyter notebook as well as pickled data files in it.  I need to move these files to another notebook.  There isn't a lot of documentation on google's help center.<\/p>\n<p>Has someone had to do this yet?  I'm new to GCP.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2022-07-18 19:32:54.733000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":114,
        "Owner_creation_date":"2016-02-12 09:56:16.143000 UTC",
        "Owner_last_access_date":"2022-09-22 18:01:16.733000 UTC",
        "Owner_location":"Grand Rapids, MI, USA",
        "Owner_reputation":1269,
        "Owner_up_votes":134,
        "Owner_down_votes":0,
        "Owner_views":261,
        "Answer_body":"<p>Can you try these steps in this <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/workbench\/user-managed\/migrate\" rel=\"nofollow noreferrer\">article<\/a>. It says you can copy your files to a <a href=\"https:\/\/cloud.google.com\/storage\/\" rel=\"nofollow noreferrer\">Google Cloud Storage Bucket<\/a> then move it to a new notebook by using gsutil tool.<\/p>\n<p>In your notebook's terminal run this code to copy an object to your Google Cloud storage bucket:<\/p>\n<pre><code>gsutil cp -R \/home\/jupyter\/* gs:\/\/BUCKET_NAMEPATH\n<\/code><\/pre>\n<p>Then open a new terminal to the target notebook and run this command to copy the directory to the notebook:<\/p>\n<pre><code>gsutil cp gs:\/\/BUCKET_NAMEPATH* \/home\/jupyter\/\n<\/code><\/pre>\n<p>Just change the <code>BUCKET_NAMEPATH<\/code> to the name of your cloud storage bucket.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-07-19 01:40:51.973000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "google-cloud-vertex-ai"
        ]
    },
    {
        "Question_id":62310010,
        "Question_title":"Azure ML PipelineData with DataTransferStep results in 0 bytes file",
        "Question_body":"<p>I am building an Azure ML pipeline with the azureml Python SDK. The pipeline calls a PythonScriptStep which stores data on the workspaceblobstore of the AML workspace. <\/p>\n\n<p>I would like to extend the pipeline to export the pipeline data to an Azure Data Lake (Gen 1). Connecting the output of the PythonScriptStep directly to Azure Data Lake (Gen 1) is not supported by Azure ML as far as I understand. Therefore, I added an extra DataTransferStep to the pipeline, which takes the output from the PythonScriptStep as input directly into the DataTransferStep. According to the Microsoft documentation this should be possible.<\/p>\n\n<p>So far I have built this solution, only this results in a file of 0 bytes on the Gen 1 Data Lake. I think the output_export_blob PipelineData does not correctly references the test.csv, and therefore the DataTransferStep cannot find the input. How can I connect the DataTransferStep correctly with the PipelineData output from the PythonScriptStep?<\/p>\n\n<p>Example I followed:\n<a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/machine-learning-pipelines\/intro-to-pipelines\/aml-pipelines-with-data-dependency-steps.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/machine-learning-pipelines\/intro-to-pipelines\/aml-pipelines-with-data-dependency-steps.ipynb<\/a><\/p>\n\n<p>pipeline.py<\/p>\n\n<pre><code>input_dataset = delimited_dataset(\n    datastore=prdadls_datastore,\n    folderpath=FOLDER_PATH_INPUT,\n    filepath=INPUT_PATH\n)\n\noutput_export_blob = PipelineData(\n    'export_blob',\n    datastore=workspaceblobstore_datastore,\n)\n\ntest_step = PythonScriptStep(\n    script_name=\"test_upload_stackoverflow.py\",\n    arguments=[\n        \"--output_extract\", output_export_blob,\n    ],\n    inputs=[\n        input_dataset.as_named_input('input'),\n    ],\n    outputs=[output_export_blob],\n    compute_target=aml_compute,\n    source_directory=\".\"\n)\n\noutput_export_adls = DataReference(\n    datastore=prdadls_datastore, \n    path_on_datastore=os.path.join(FOLDER_PATH_OUTPUT, 'test.csv'),\n    data_reference_name='export_adls'        \n)\n\nexport_to_adls = DataTransferStep(\n    name='export_output_to_adls',\n    source_data_reference=output_export_blob,\n    source_reference_type='file',\n    destination_data_reference=output_export_adls,\n    compute_target=adf_compute\n)\n\npipeline = Pipeline(\n    workspace=aml_workspace, \n    steps=[\n        test_step, \n        export_to_adls\n    ]\n)\n<\/code><\/pre>\n\n<p>test_upload_stackoverflow.py<\/p>\n\n<pre><code>import os\nimport pathlib\nfrom azureml.core import Datastore, Run\n\nparser = argparse.ArgumentParser(\"train\")\nparser.add_argument(\"--output_extract\", type=str)\nargs = parser.parse_args() \n\nrun = Run.get_context()\ndf_data_all = (\n    run\n    .input_datasets[\"input\"]\n    .to_pandas_dataframe()\n)\n\nos.makedirs(args.output_extract, exist_ok=True)\ndf_data_all.to_csv(\n    os.path.join(args.output_extract, \"test.csv\"), \n    index=False\n)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-06-10 17:54:20.073000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-06-10 18:10:59.793000 UTC",
        "Question_score":2,
        "Question_tags":"python|azure|azure-machine-learning-service",
        "Question_view_count":917,
        "Owner_creation_date":"2017-10-30 18:18:09.890000 UTC",
        "Owner_last_access_date":"2022-09-08 06:29:56.133000 UTC",
        "Owner_location":null,
        "Owner_reputation":73,
        "Owner_up_votes":15,
        "Owner_down_votes":0,
        "Owner_views":24,
        "Answer_body":"<p>The code example is immensely helpful. Thanks for that. You're right that it can be confusing to get <code>PythonScriptStep -&gt; PipelineData<\/code>. Working initially even without the <code>DataTransferStep<\/code>.<\/p>\n\n<p>I don't know 100% what's going on, but I thought I'd spitball some ideas:<\/p>\n\n<ol>\n<li>Does your <code>PipelineData<\/code>,  <code>export_blob<\/code>, contain the \"test.csv\" file? I would verify that before troubleshooting the <code>DataTransferStep<\/code>. You can verify this using the SDK, or more easily with the UI.\n\n<ol>\n<li>Go to the PipelineRun page, click on the <code>PythonScriptStep<\/code> in question.<\/li>\n<li>On \"Outputs + Logs\" page, there's a \"Data Outputs\" Section (that is slow to load initially)<\/li>\n<li>Open it and you'll see the output PipelineDatas then click on \"View Output\"<\/li>\n<li>Navigate to given path either in the Azure Portal or Azure Storage Explorer.\n<a href=\"https:\/\/i.stack.imgur.com\/9LaEq.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/9LaEq.png\" alt=\"enter image description here\"><\/a>\n<a href=\"https:\/\/i.stack.imgur.com\/XbnhC.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/XbnhC.png\" alt=\"enter image description here\"><\/a><\/li>\n<\/ol><\/li>\n<li>In <code>test_upload_stackoverflow.py<\/code> you are treating the <code>PipelineData<\/code> as a directory when call <code>.to_csv()<\/code> as opposed to a file which would be you just calling <code>df_data_all.to_csv(args.output_extract, index=False)<\/code>. Perhaps try defining the <code>PipelineData<\/code> with <code>is_directory=True<\/code>. Not sure if this is required though.<\/li>\n<\/ol>",
        "Answer_comment_count":7.0,
        "Answer_creation_date":"2020-06-10 18:21:40.647000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":47735839,
        "Question_title":"how to reduce the Run time in Azure ML for decision tree and decision forest",
        "Question_body":"<p>I am trying to run a regression model for a data set containing over 2000000 rows. I tried using linear regression and boosted decision tree regression without tuning model hyperparameter, I didn't get the expected accuracy. so I tried to use Tune model hyperparameter for the boosted decision tree, the model runs over 20 min. the decision forest also takes to0 long (even without tuning model hyperparameter). Is there any way to reduce the runtime without compromising the result accuracy too much?<\/p>\n\n<p>will sampling affect the output (say I took  0.5 as sampling rate)?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2017-12-10 04:24:43.917000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"azure-machine-learning-studio",
        "Question_view_count":127,
        "Owner_creation_date":"2017-12-10 03:38:46.070000 UTC",
        "Owner_last_access_date":"2018-01-21 14:22:02.587000 UTC",
        "Owner_location":"Portugal",
        "Owner_reputation":31,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":14,
        "Answer_body":"<p>The execution time on AzureML Studio depends on the pricing tier. The free version does one node execution at time while the standard pricing tier do the execute multiple execution at one time. <\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2017-12-19 04:14:26.753000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":65285203,
        "Question_title":"Why does AWS SageMaker create an S3 Bucket",
        "Question_body":"<p>Upon deploying a custom pytorch model with the boto3 client in python. I noticed that a new S3 bucket had been created with no visible objects. Is there a reason for this?<\/p>\n<p>The bucket that contained my model was named with the keyword &quot;sagemaker&quot; included, so I don't any issue there.<\/p>\n<p>Here is the code that I used for deployment:<\/p>\n<pre><code>remote_model = PyTorchModel(\n                     name = model_name, \n                     model_data=model_url,\n                     role=role,\n                     sagemaker_session = sess,\n                     entry_point=&quot;inference.py&quot;,\n                     # image=image, \n                     framework_version=&quot;1.5.0&quot;,\n                     py_version='py3'\n                    )\n\nremote_predictor = remote_model.deploy(\n                         instance_type='ml.g4dn.xlarge', \n                         initial_instance_count=1,\n                         #update_endpoint = True, # comment or False if endpoint doesns't exist\n                         endpoint_name=endpoint_name, # define a unique endpoint name; if ommited, Sagemaker will generate it based on used container\n                         wait=True\n                         )\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-12-14 07:35:28.823000 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"python|amazon-web-services|amazon-s3|boto3|amazon-sagemaker",
        "Question_view_count":1047,
        "Owner_creation_date":"2020-01-01 10:04:37.020000 UTC",
        "Owner_last_access_date":"2022-09-20 04:22:12.437000 UTC",
        "Owner_location":"Perth WA, Australia",
        "Owner_reputation":438,
        "Owner_up_votes":88,
        "Owner_down_votes":3,
        "Owner_views":67,
        "Answer_body":"<p>It was likely created as a default bucket by the SageMaker Python SDK. Note that the code you wrote about is not <code>boto3<\/code> (AWS python SDK), but <code>sagemaker<\/code> (<a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/index.html\" rel=\"nofollow noreferrer\">link<\/a>), the SageMaker-specific Python SDK, that is higher-level than boto3.<\/p>\n<p>The SageMaker Python SDK uses S3 at multiple places, for example to stage training code when using a Framework Estimator, and to stage inference code when deployment with a Framework Model (your case). It gives you control of the S3 location to use, but if you don't specify it, it may use an automatically generated bucket, if it has the permissions to do so.<\/p>\n<p>To control code staging S3 location, you can use the parameter <code>code_location<\/code> in either your <code>PyTorchEstimator<\/code> (training) or your <code>PyTorchModel<\/code> (serving)<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2020-12-16 15:34:33.677000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":30133814,
        "Question_title":"How to build an image classification dataset in Azure?",
        "Question_body":"<p>I've a set of images that have a single classification of OPEN (they show something that is open).  I couldn't find a way to directly add a status of open to the image reader dataset so I have FULL OUTER JOIN-ed a single ENTER DATA to an IMAGE READER as per the following.  This seems like a hack, does anyone know the \"right\" way to do this?\n<img src=\"https:\/\/i.stack.imgur.com\/Kt1Rv.png\" alt=\"enter image description here\"><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":4,
        "Question_creation_date":"2015-05-08 22:33:55.077000 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":"2015-10-30 08:06:38.650000 UTC",
        "Question_score":16,
        "Question_tags":"azure|machine-learning|azure-machine-learning-studio",
        "Question_view_count":1403,
        "Owner_creation_date":"2011-12-02 22:06:32.970000 UTC",
        "Owner_last_access_date":"2022-09-24 11:04:39.013000 UTC",
        "Owner_location":null,
        "Owner_reputation":373,
        "Owner_up_votes":119,
        "Owner_down_votes":4,
        "Owner_views":34,
        "Answer_body":"<p>Another way is to have R or python code that replicates the status for each image and then use add-columns. I think R\/Python code to just replicate the status for each image may be easier and faster than outer join.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2015-05-17 18:13:01.620000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":5.0,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":69721067,
        "Question_title":"GoogleAPICallError: None Unexpected state: Long-running operation had neither response nor error set",
        "Question_body":"<p>I'm new to Google Cloud Platform and I'm trying to create a Feature Store to fill with values from a csv file from Google Cloud Storage. The aim is to do that from a local notebook in Python.\nI'm basically following the code <a href=\"https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/master\/notebooks\/official\/feature_store\/gapic-feature-store.ipynb\" rel=\"nofollow noreferrer\">here<\/a>, making the appropriate changes since I'm working with the credit card public dataset.\nThe error that raises when I run the code is the following:<\/p>\n<pre><code>GoogleAPICallError: None Unexpected state: Long-running operation had neither response nor error set.\n<\/code><\/pre>\n<p>and it happens during the ingestion of the data from the csv file.<\/p>\n<p>Here it is the code I'm working on:<\/p>\n<pre><code>import os\nfrom datetime import datetime\nfrom google.cloud import bigquery\nfrom google.cloud import aiplatform\nfrom google.cloud.aiplatform_v1.types import feature as feature_pb2\nfrom google.cloud.aiplatform_v1.types import featurestore as featurestore_pb2\nfrom google.cloud.aiplatform_v1.types import \\\n    featurestore_service as featurestore_service_pb2\nfrom google.cloud.aiplatform_v1.types import entity_type as entity_type_pb2\nfrom google.cloud.aiplatform_v1.types import FeatureSelector, IdMatcher\n\ncredential_path = r&quot;C:\\Users\\...\\.json&quot;\nos.environ['GOOGLE_APPLICATION_CREDENTIALS'] = credential_path\n\n## Constants\nPROJECT_ID = &quot;my-project-ID&quot;\nREGION = &quot;us-central1&quot;\nAPI_ENDPOINT = &quot;us-central1-aiplatform.googleapis.com&quot;\nINPUT_CSV_FILE = &quot;my-input-file.csv&quot;\nFEATURESTORE_ID = &quot;fraud_detection&quot;\n\n## Output dataset\nDESTINATION_DATA_SET = &quot;fraud_predictions&quot;\nTIMESTAMP = datetime.now().strftime(&quot;%Y%m%d%H%M%S&quot;)\nDESTINATION_DATA_SET = &quot;{prefix}_{timestamp}&quot;.format(\n    prefix=DESTINATION_DATA_SET, timestamp=TIMESTAMP\n)\n\n## Output table. Make sure that the table does NOT already exist; \n## the BatchReadFeatureValues API cannot overwrite an existing table\nDESTINATION_TABLE_NAME = &quot;training_data&quot;\n\nDESTINATION_PATTERN = &quot;bq:\/\/{project}.{dataset}.{table}&quot;\nDESTINATION_TABLE_URI = DESTINATION_PATTERN.format(\n    project=PROJECT_ID, dataset=DESTINATION_DATA_SET, \n    table=DESTINATION_TABLE_NAME\n)\n\n## Create dataset\nclient = bigquery.Client(project=PROJECT_ID)\ndataset_id = &quot;{}.{}&quot;.format(client.project, DESTINATION_DATA_SET)\ndataset = bigquery.Dataset(dataset_id)\ndataset.location = REGION\ndataset = client.create_dataset(dataset)\nprint(&quot;Created dataset {}.{}&quot;.format(client.project, dataset.dataset_id))\n\n## Create client for CRUD and data_client for reading feature values.\nclient = aiplatform.gapic.FeaturestoreServiceClient(\n    client_options={&quot;api_endpoint&quot;: API_ENDPOINT})\ndata_client = aiplatform.gapic.FeaturestoreOnlineServingServiceClient(\n    client_options={&quot;api_endpoint&quot;: API_ENDPOINT})\nBASE_RESOURCE_PATH = client.common_location_path(PROJECT_ID, REGION)\n\n## Create featurestore (only the first time)\ncreate_lro = client.create_featurestore(\n    featurestore_service_pb2.CreateFeaturestoreRequest(\n        parent=BASE_RESOURCE_PATH,\n        featurestore_id=FEATURESTORE_ID,\n        featurestore=featurestore_pb2.Featurestore(\n            online_serving_config=featurestore_pb2.Featurestore.OnlineServingConfig(\n                fixed_node_count=1\n            ),\n        ),\n    )\n)\n\n## Wait for LRO to finish and get the LRO result.\nprint(create_lro.result())\n\nclient.get_featurestore(\n    name=client.featurestore_path(PROJECT_ID, REGION, FEATURESTORE_ID)\n)\n\n## Create credit card entity type (only the first time)\ncc_entity_type_lro = client.create_entity_type(\n    featurestore_service_pb2.CreateEntityTypeRequest(\n        parent=client.featurestore_path(PROJECT_ID, REGION, FEATURESTORE_ID),\n        entity_type_id=&quot;creditcards&quot;,\n        entity_type=entity_type_pb2.EntityType(\n            description=&quot;Credit card entity&quot;,\n        ),\n    )\n)\n\n## Create fraud entity type (only the first time)\nfraud_entity_type_lro = client.create_entity_type(\n    featurestore_service_pb2.CreateEntityTypeRequest(\n        parent=client.featurestore_path(PROJECT_ID, REGION, FEATURESTORE_ID),\n        entity_type_id=&quot;frauds&quot;,\n        entity_type=entity_type_pb2.EntityType(\n            description=&quot;Fraud entity&quot;,\n        ),\n    )\n)\n\n## Create features for credit card type (only the first time)\nclient.batch_create_features(\n    parent=client.entity_type_path(PROJECT_ID, REGION, FEATURESTORE_ID, &quot;creditcards&quot;),\n    requests=[\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v1&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v2&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v3&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v4&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v5&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v6&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v7&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v8&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v9&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v10&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v11&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v12&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v13&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v14&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v15&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v16&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v17&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v18&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v19&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v20&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v21&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v22&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v23&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v24&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v25&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v26&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v27&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v28&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;amount&quot;,\n        ),\n    ],\n).result()\n\n## Create features for fraud type (only the first time)\nclient.batch_create_features(\n    parent=client.entity_type_path(PROJECT_ID, REGION, FEATURESTORE_ID, &quot;frauds&quot;),\n    requests=[\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;class&quot;,\n        ),\n    ],\n).result()\n\n## Import features values for credit cards\nimport_cc_request = aiplatform.gapic.ImportFeatureValuesRequest(\n    entity_type=client.entity_type_path(\n        PROJECT_ID, REGION, FEATURESTORE_ID, &quot;creditcards&quot;),\n    csv_source=aiplatform.gapic.CsvSource(gcs_source=aiplatform.gapic.GcsSource(\n        uris=[&quot;gs:\/\/fraud-detection-19102021\/dataset\/cc_details_train.csv&quot;])),\n    entity_id_field=&quot;cc_id&quot;,\n    feature_specs=[\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v1&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v2&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v3&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v4&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v5&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v6&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v7&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v8&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v9&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v10&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v11&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v12&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v13&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v14&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v15&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v16&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v17&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v18&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v19&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v20&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v21&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v22&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v23&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v24&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v25&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v26&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v27&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v28&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;amount&quot;),\n    ],\n    feature_time_field='time',\n    worker_count=1,\n)\n\n## Start to import\ningestion_lro = client.import_feature_values(import_cc_request)\n\n## Polls for the LRO status and prints when the LRO has completed\ningestion_lro.result()\n\n## Import features values for frauds\nimport_fraud_request = aiplatform.gapic.ImportFeatureValuesRequest(\n    entity_type=client.entity_type_path(\n        PROJECT_ID, REGION, FEATURESTORE_ID, &quot;frauds&quot;),\n    csv_source=aiplatform.gapic.CsvSource(gcs_source=aiplatform.gapic.GcsSource(\n        uris=[&quot;gs:\/\/fraud-detection-19102021\/dataset\/data_fraud_train.csv&quot;])),\n    entity_id_field=&quot;fraud_id&quot;,\n    feature_specs=[\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;class&quot;),\n    ],\n    feature_time_field='time',\n    worker_count=1,\n)\n\n## Start to import\ningestion_lro = client.import_feature_values(import_fraud_request)\n\n## Polls for the LRO status and prints when the LRO has completed\ningestion_lro.result()\n<\/code><\/pre>\n<p>When I check the <code>Ingestion Jobs<\/code> from the <code>Feature<\/code> section of Google Cloud Console I see that the job has finished but no values are added to my features.<\/p>\n<p>Any advice it is really precious.<\/p>\n<p>Thank you all.<\/p>\n<p><strong>EDIT 1<\/strong>\nIn the image below there is an example of the first row of the csv file I used as input (<code>cc_details_train.csv<\/code>). All the unseen features  are similar, the feature <code>class<\/code> can assume 0 or 1 values.\nThe injection job lasts about 5 minutes to import (ideally) 3000 rows, but it ends without error and without importing any value.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Z34hG.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Z34hG.png\" alt=\"Rows of my csv file\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":7,
        "Question_creation_date":"2021-10-26 09:59:23.740000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-11-09 08:34:24.247000 UTC",
        "Question_score":1,
        "Question_tags":"google-cloud-platform|google-api-python-client|google-cloud-ml|google-cloud-vertex-ai",
        "Question_view_count":357,
        "Owner_creation_date":"2021-03-24 12:34:53.617000 UTC",
        "Owner_last_access_date":"2022-09-22 15:26:12.103000 UTC",
        "Owner_location":"Alatri, Frosinone, FR",
        "Owner_reputation":67,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":33,
        "Answer_body":"<p><strong>VERTEX AI recomendations when using CSV to ImportValues \/ using ImportFeatureValuesRequest<\/strong><\/p>\n<p>Its possible that when using this feature you might end not able to import any data at all. You must pay attention to the time field you are using as it must be in compliance with google time formats.<\/p>\n<ol>\n<li>feature_time_field, must follow the time constraint rule set by google which is RFC3339, ie: '2021-04-15T08:28:14Z'. You can check details about the field <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/reference\/rest\/v1\/projects.locations.featurestores.entityTypes\/importFeatureValues#request-body\" rel=\"nofollow noreferrer\">here<\/a> and details about timestamp format can be found <a href=\"https:\/\/developers.google.com\/protocol-buffers\/docs\/reference\/google.protobuf#timestamp\" rel=\"nofollow noreferrer\">here<\/a>.<\/li>\n<li>Other columns, fields must match is designed value. One exception is field entity_id_field, As it can be any value.<\/li>\n<\/ol>\n<p>Note: I my test i found that if i do not properly set up the time field as google recommended date format it will just not upload any feature value at all.<\/p>\n<p><em>test.csv<\/em><\/p>\n<pre><code>cc_id,time,v1,v2,v3,v4,v5,v6,v7,v8,v9,v10,v11,v12,v13,v14,v15,v16,v17,v18,v19,v20,v21,v22,v23,v24,v25,v26,v27,v28,amount\n100,2021-04-15T08:28:14Z,-1.359807,-0.072781,2.534897,1.872351,2.596267,0.465238,0.923123,0.347986,0.987354,1.234657,2.128645,1.958237,0.876123,-1.712984,-0.876436,1.74699,-1.645877,-0.936121,1.456327,0.087623,1.900872,2.876234,1.874123,0.923451,0.123432,0.000012,1.212121,0.010203,1000\n<\/code><\/pre>\n<p><em>output:<\/em><\/p>\n<pre><code>imported_entity_count: 1\nimported_feature_value_count: 29\n<\/code><\/pre>\n<p><strong>About optimization and working with features<\/strong><\/p>\n<p>You can check the official documentation <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/datasets\/prepare-text#single-label-classification\" rel=\"nofollow noreferrer\">here<\/a> to see the min and max amount of records recommended for processing. As a piece of advice you should only use the actual working features to run and the recommended amount of values for it.<\/p>\n<p><strong>See your running ingested job<\/strong><\/p>\n<p>Either if you use VertexUI or code to generated the ingested job. You can track its run by going into the UI to this path:<\/p>\n<pre><code>VertexAI &gt; Features &gt; View Ingested Jobs \n<\/code><\/pre>",
        "Answer_comment_count":3.0,
        "Answer_creation_date":"2021-11-08 09:36:18.673000 UTC",
        "Answer_last_edit_date":"2021-12-02 19:02:48.683000 UTC",
        "Answer_score":1.0,
        "Question_valid_tags":[
            "google-cloud-vertex-ai"
        ]
    },
    {
        "Question_id":56467434,
        "Question_title":"Making a Prediction Sagemaker Pytorch",
        "Question_body":"<p>I have trained and deployed a model in Pytorch with Sagemaker. I am able to call the endpoint and get a prediction. I am using the default input_fn() function (i.e. not defined in my serve.py).<\/p>\n\n<pre><code>model = PyTorchModel(model_data=trained_model_location,\n                     role=role,\n                     framework_version='1.0.0',\n                     entry_point='serve.py',\n                     source_dir='source')\npredictor = model.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')\n<\/code><\/pre>\n\n<p>A prediction can be made as follows:<\/p>\n\n<pre><code>input =\"0.12787057,  1.0612601,  -1.1081504\"\npredictor.predict(np.genfromtxt(StringIO(input), delimiter=\",\").reshape(1,3) )\n<\/code><\/pre>\n\n<p>I want to be able to serve the model with REST API and am HTTP POST using lambda and API gateway. I was able to use invoke_endpoint() for this with an XGBOOST model in Sagemaker this way. I am not sure what to send into the body for Pytorch.<\/p>\n\n<pre><code>client = boto3.client('sagemaker-runtime')\nresponse = client.invoke_endpoint(EndpointName=ENDPOINT  ,\nContentType='text\/csv',\nBody=???)\n<\/code><\/pre>\n\n<p>I believe I need to understand how to write the customer input_fn to accept and process the type of data I am able to send through invoke_client. Am I on the right track and if so, how could the input_fn be written to accept a csv from invoke_endpoint?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-06-05 20:19:23.530000 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"python|pytorch|amazon-sagemaker",
        "Question_view_count":2346,
        "Owner_creation_date":"2011-01-10 02:55:08.597000 UTC",
        "Owner_last_access_date":"2022-09-22 22:11:16.413000 UTC",
        "Owner_location":null,
        "Owner_reputation":1748,
        "Owner_up_votes":80,
        "Owner_down_votes":5,
        "Owner_views":393,
        "Answer_body":"<p>Yes you are on the right track. You can send csv-serialized input to the endpoint without using the <code>predictor<\/code> from the SageMaker SDK, and using other SDKs such as <code>boto3<\/code> which is installed in lambda:<\/p>\n\n<pre><code>import boto3\nruntime = boto3.client('sagemaker-runtime')\n\npayload = '0.12787057,  1.0612601,  -1.1081504'\n\nresponse = runtime.invoke_endpoint(\n    EndpointName=ENDPOINT_NAME,\n    ContentType='text\/csv',\n    Body=payload.encode('utf-8'))\n\nresult = json.loads(response['Body'].read().decode()) \n<\/code><\/pre>\n\n<p>This will pass to the endpoint a csv-formatted input, that you may need to reshape back in the <code>input_fn<\/code> to put in the appropriate dimension expected by the model.<\/p>\n\n<p>for example:<\/p>\n\n<pre><code>def input_fn(request_body, request_content_type):\n    if request_content_type == 'text\/csv':\n        return torch.from_numpy(\n            np.genfromtxt(StringIO(request_body), delimiter=',').reshape(1,3))\n<\/code><\/pre>\n\n<p><strong>Note<\/strong>: I wasn't able to test the specific <code>input_fn<\/code> above with your input content and shape but I used the approach on Sklearn RandomForest couple times, and looking at the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_pytorch.html#model-serving\" rel=\"nofollow noreferrer\">Pytorch SageMaker serving doc<\/a> the above rationale should work.<\/p>\n\n<p>Don't hesitate to use endpoint logs in Cloudwatch to diagnose any inference error (available from the endpoint UI in the console), those logs are usually <strong>much more verbose<\/strong> that the high-level logs returned by the inference SDKs<\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_date":"2019-06-06 00:42:46.690000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":68367348,
        "Question_title":"Azure ML Studio not showing datasets under models",
        "Question_body":"<p>I registered a model in an Azure ML notebook along with its datasets. In ML Studio I can see the model listed under the dataset, but no dataset gets listed under the model. What should I do to have datasets listed under models?<\/p>\n<ul>\n<li>Model listed under dataset:<\/li>\n<\/ul>\n<p><a href=\"https:\/\/i.stack.imgur.com\/fh3bV.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/fh3bV.png\" alt=\"model dataset\" \/><\/a><\/p>\n<ul>\n<li>Dataset not listed under the model:<\/li>\n<\/ul>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Ueys2.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Ueys2.png\" alt=\"dataset model\" \/><\/a><\/p>\n<ul>\n<li>Notebook code:<\/li>\n<\/ul>\n<pre><code>import pickle\nimport sys\nfrom azureml.core import Workspace, Dataset, Model\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils import assert_all_finite\n\nworkspace = Workspace('&lt;snip&gt;', '&lt;snip&gt;', '&lt;snip&gt;')\ndataset = Dataset.get_by_name(workspace, name='creditcard')\ndata = dataset.to_pandas_dataframe()\ndata.dropna(inplace=True)\nX = data.drop(labels=[&quot;Class&quot;], axis=1, inplace=False)\ny = data[&quot;Class&quot;]\n\nmodel = make_pipeline(StandardScaler(), GradientBoostingClassifier())\nmodel.fit(X, y)\n\nwith open('creditfraud_sklearn_model.pkl', 'wb') as outfile:\n    pickle.dump(model, outfile)\n\nModel.register(\n    Workspace = workspace,\n    model_name = 'creditfraud_sklearn_model',\n    model_path = 'creditfraud_sklearn_model.pkl',\n    description = 'Gradient Boosting classifier for Kaggle credit-card fraud',\n    model_framework = Model.Framework.SCIKITLEARN,\n    model_framework_version = sys.modules['sklearn'].__version__,\n    sample_input_dataset = dataset,\n    sample_output_dataset = dataset)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-07-13 18:00:30.667000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"azure-machine-learning-studio",
        "Question_view_count":29,
        "Owner_creation_date":"2014-06-27 16:09:59.457000 UTC",
        "Owner_last_access_date":"2022-09-25 00:49:45.530000 UTC",
        "Owner_location":null,
        "Owner_reputation":402,
        "Owner_up_votes":57,
        "Owner_down_votes":0,
        "Owner_views":90,
        "Answer_body":"<p>It looks like <code>add_dataset_references()<\/code> needs to be called to have datasets displayed under models:<\/p>\n<pre><code>model_registration.add_dataset_references([(&quot;input dataset&quot;, dataset)])\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-07-24 16:53:55.293000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":57172147,
        "Question_title":"'no SavedModel bundles found!' on tensorflow_hub model deployment to AWS SageMaker",
        "Question_body":"<p>I attempting to deploy the universal-sentence-encoder model to a aws Sagemaker endpoint and am getting the error <code>raise ValueError('no SavedModel bundles found!')<\/code><\/p>\n\n<p>I have shown my code below, I have a feeling that one of my paths is incorrect<\/p>\n\n<pre><code>import tensorflow as tf\nimport tensorflow_hub as hub\nimport numpy as np\nfrom sagemaker import get_execution_role\nfrom sagemaker.tensorflow.serving import Model\n\ndef tfhub_to_savedmodel(model_name,uri):\n    tfhub_uri = uri\n    model_path = 'encoder_model\/' + model_name\n\n    with tf.Session(graph=tf.Graph()) as sess:\n        module = hub.Module(tfhub_uri) \n        input_params = module.get_input_info_dict()\n        dtype = input_params['text'].dtype\n        shape = input_params['text'].get_shape()\n\n        # define the model inputs\n        inputs = {'text': tf.placeholder(dtype, shape, 'text')}\n\n        # define the model outputs\n        # we want the class ids and probabilities for the top 3 classes\n        logits = module(inputs['text'])\n        outputs = {\n            'vector': logits,\n        }\n\n        # export the model\n        sess.run([tf.global_variables_initializer(), tf.tables_initializer()])\n        tf.saved_model.simple_save(\n            sess,\n            model_path,\n            inputs=inputs,\n            outputs=outputs)  \n\n    return model_path\n\n\nsagemaker_role = get_execution_role()\n\n!tar -C \"$PWD\" -czf encoder.tar.gz encoder_model\/\nmodel_data = Session().upload_data(path='encoder.tar.gz',key_prefix='model')\n\nenv = {'SAGEMAKER_TFS_DEFAULT_MODEL_NAME': 'universal-sentence-encoder-large'}\n\nmodel = Model(model_data=model_data, role=sagemaker_role, framework_version=1.12, env=env)\npredictor = model.deploy(initial_instance_count=1, instance_type='ml.t2.medium')\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-07-23 21:01:20.943000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":5,
        "Question_tags":"amazon-web-services|tensorflow|amazon-sagemaker|tensorflow-hub",
        "Question_view_count":2621,
        "Owner_creation_date":"2018-07-17 15:14:49.147000 UTC",
        "Owner_last_access_date":"2021-11-04 23:15:34.717000 UTC",
        "Owner_location":"Berkeley, CA, USA",
        "Owner_reputation":425,
        "Owner_up_votes":12,
        "Owner_down_votes":1,
        "Owner_views":92,
        "Answer_body":"<p>I suppose you started from this example? <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/sagemaker-python-sdk\/tensorflow_serving_container\" rel=\"noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/sagemaker-python-sdk\/tensorflow_serving_container<\/a><\/p>\n\n<p>It looks like you're not saving the TF Serving bundle properly: the model version number is missing, because of this line:<\/p>\n\n<pre><code>model_path = 'encoder_model\/' + model_name\n<\/code><\/pre>\n\n<p>Replacing it with this should fix your problem:<\/p>\n\n<pre><code>model_path = '{}\/{}\/00000001'.format('encoder_model\/', model_name)\n<\/code><\/pre>\n\n<p>Your model artefact should look like this (I used the model in the notebook above):<\/p>\n\n<pre><code>mobilenet\/\nmobilenet\/mobilenet_v2_140_224\/\nmobilenet\/mobilenet_v2_140_224\/00000001\/\nmobilenet\/mobilenet_v2_140_224\/00000001\/saved_model.pb\nmobilenet\/mobilenet_v2_140_224\/00000001\/variables\/\nmobilenet\/mobilenet_v2_140_224\/00000001\/variables\/variables.data-00000-of-00001\nmobilenet\/mobilenet_v2_140_224\/00000001\/variables\/variables.index\n<\/code><\/pre>\n\n<p>Then, upload to S3 and deploy.<\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_date":"2019-07-24 14:34:18.283000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":6.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":71948090,
        "Question_title":"Best way to run 1000s of training jobs on sagemaker",
        "Question_body":"<p>I have thousands of training jobs that I want to run on sagemaker. Basically I have a list of hyperparameters and I want to train the model for <em>all<\/em> of those hyperparmeters in parallel (not a standard hyperparameter tuning where we just want to optimize the hyperparameter, here we want to train for all of the hyperparameters). I have searched the docs quite extensively but it surprises me that I couldn't find any info about this, even though it seems like a pretty basic functionality.<\/p>\n<p>For example, let's say I have 10,000 training jobs, and my quota is 20 instances, what is the best way to run these jobs utilizing all my available instances? In particular,<\/p>\n<ul>\n<li>Is there a &quot;queue manager&quot; functionality that takes the list of hyperparameters and runs the training jobs in batches of 20 until they are all done (even better if it could keep track of failed\/completed jobs).<\/li>\n<li>Is it best practice to run a single training job per instance? If that's the case do I need to ask for a much higher quota on the number of instance?<\/li>\n<li>If this functionality does not exist in sagemaker, is it worth using EC2 instead since it's a bit cheaper?<\/li>\n<\/ul>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2022-04-21 02:03:31.153000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-04-21 02:41:38.300000 UTC",
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":269,
        "Owner_creation_date":"2021-05-30 15:23:01.327000 UTC",
        "Owner_last_access_date":"2022-06-06 14:05:53.057000 UTC",
        "Owner_location":null,
        "Owner_reputation":103,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Answer_body":"<p>Your question is very broad and the best way forward would depend on other details of your use-case, so we will have to make some assumptions.<\/p>\n<p>[Queue manager]\nSageMaker does <em>not<\/em> have a queue manager. If at the end you decide you need a queue manager, I would suggest looking towards AWS Batch.<\/p>\n<p>[Single vs multiple training jobs]\nSince you need to run 10s of thousands job I assume you are training fairly lightweight models, so to save on time, you would be better off reusing instances for multiple training jobs. (Otherwise, with 20 instances limit, you need 500 rounds of training, with a 3 min start time - depending on instance type - you need 25 hours just for the wait time. Depending on the complexity of each individual model, this 25hours might be significant or totally acceptable).<\/p>\n<p>[Instance limit increase]\nYou can always ask for a limit increase, but going from a limit of 20 to 10k at once is likely that will not be accepted by the AWS support team, unless you are part of an organisation with a track record of usage on AWS, in which case this might be fine.<\/p>\n<p>[One possible option] (Assuming multiple lightweight models)\nYou could create a single training job, with instance count, the number of instances available to you.\nInside the training job, your code can run a for loop and perform all the individual training jobs you need.<\/p>\n<p>In this case, you will need to know which which instance is which so you can make the split of the HPOs. SageMaker writes this information on the file: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-running-container.html#your-algorithms-training-algo-running-container-dist-training\" rel=\"nofollow noreferrer\">\/opt\/ml\/input\/config\/resourceconfig.json<\/a> so using that you can easily have each instance run a subset of the trainings required.<\/p>\n<p>Another thing to think of, is if you need to save the generated models (which you probably need). You can either save everything in the output model directory - standard SM approach- but this would zip all models in a model.tar.gz file.\nIf you don't want this, and prefer to have each model individually saved, I'd suggest using the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-checkpoints.html\" rel=\"nofollow noreferrer\">checkpoints<\/a> directory that will sync anything written there to your s3 location.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2022-04-22 08:01:42.720000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":66264795,
        "Question_title":"Azure ML studio - Container Registry Error while trying to submit a pipeline",
        "Question_body":"<p>I'm having the following error while trying to submit an Azure ML Studio pipeline<\/p>\n<p><code>Get credentials or pull docker image failed with err: error response from daemon: get https:\/\/lgcrmldev.azurecr.io\/v2\/azureml\/azureml_977f5bda2f6f4f634482661c121c8959\/manifests\/latest: unauthorized: authentication required, visit https:\/\/aka.ms\/acr\/authorization for more information.<\/code><\/p>\n<p>The notebook python code I'm doing is something on these lines:<\/p>\n<pre><code># create a Python script to do the actual work and save it in the pipeline folder:\n\n%%writefile $experiment_folder\/batch_online_retail.py\nimport os\nimport numpy as np\nfrom azureml.core import Model\nimport joblib\n\n\n# Called when the service is loaded\ndef init():\n    global model\n    \n    # Load the model\n    model_path = Model.get_model_path('Random_Forest_model')\n    model = joblib.load(model_path)\n\ndef run(batch):\n    try:\n        result = []\n        \n    # Process each line\n    for in range (len(batch)):\n        # Read the comma-delimited data into an array\n        data = np.genfromtxt(f, delimiter=',')        \n        # Reshape into a 2-dimensional array for prediction (model expects multiple items)\n        prediction = model.predict(data.reshape(1, -1))        \n        # Append prediction to results\n        resultList.append(&quot;{}: {}&quot;.format(os.path.basename(f), prediction[0]))\n    return resultList      \n<\/code><\/pre>\n<pre><code># Creating the run context\nfrom azureml.core import Environment\nfrom azureml.core.runconfig import DEFAULT_CPU_IMAGE\nfrom azureml.core.runconfig import CondaDependencies\n\n# Add dependencies required by the model\n# For scikit-learn models, you need scikit-learn\n# For parallel pipeline steps, you need azureml-core and azureml-dataprep[fuse]\ncd = CondaDependencies.create(conda_packages=['scikit-learn','pip'],\n                              pip_packages=['azureml-defaults','azureml-core','azureml-dataprep[fuse,pandas]'])\n\nbatch_env = Environment(name='batch_environment')\nbatch_env.python.conda_dependencies = cd\nbatch_env.docker.enabled = True\nbatch_env.docker.base_image = DEFAULT_CPU_IMAGE\nprint('Configuration ready.')\n\n<\/code><\/pre>\n<pre><code># Creating the ParallelRunStep\nfrom azureml.pipeline.steps import ParallelRunConfig, ParallelRunStep\nfrom azureml.pipeline.core import PipelineData\n\ndefault_ds = ws.get_default_datastore()\n\noutput_dir = PipelineData(name='inferences', \n                          datastore=default_ds, \n                          output_path_on_compute='online-retail\/results')\n\nparallel_run_config = ParallelRunConfig(\n    source_directory=experiment_folder,\n    entry_script=&quot;batch_online_retail.py&quot;,\n    mini_batch_size=&quot;5&quot;,\n    error_threshold=10,\n    output_action=&quot;append_row&quot;,\n    environment=batch_env,\n    compute_target=inference_cluster,\n    node_count=2)\n\nparallelrun_step = ParallelRunStep(\n    name='batch-score-retail',\n    parallel_run_config=parallel_run_config,\n    inputs=[batch_data_set.as_named_input('online_retail_batch')],\n    output=output_dir,\n    arguments=[],\n    allow_reuse=True\n)\n\nprint('Steps defined')\n<\/code><\/pre>\n<p>and finally,<\/p>\n<pre><code># Create an Azure ML experiment in your workspace, put the step into a pipeline and run it\nfrom azureml.core import Experiment\nfrom azureml.pipeline.core import Pipeline\n\npipeline = Pipeline(workspace=ws, steps=[parallelrun_step])\npipeline_run = Experiment(ws, 'online-retail-deployment-cf').submit(pipeline)\npipeline_run.wait_for_completion(show_output=True)\n<\/code><\/pre>\n<p>It's in this final step that I keep getting the error above.<\/p>\n<p>My Container Registry has my user and Azure ML resource as a Contributor in the access control panel, so I don't think it's lack of permissions.<\/p>\n<p>I've found this Microsoft page that seems to have a fix for the error I'm having:\n<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/container-registry\/container-registry-faq#docker-push-succeeds-but-docker-pull-fails-with-error-unauthorized-authentication-required\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/container-registry\/container-registry-faq#docker-push-succeeds-but-docker-pull-fails-with-error-unauthorized-authentication-required<\/a><\/p>\n<p>But I don't understand how can I implement the suggested fix. This is because the Docker image the notebook uses is inside the Compute Instance created in Azure ML which we have limited access.<\/p>\n<p>Any ideas on what is the problem and how to fix it?<\/p>\n<p>Thank you in advance,\nCarla<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-02-18 17:11:59.863000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"azure|azure-container-registry|azure-machine-learning-service",
        "Question_view_count":869,
        "Owner_creation_date":"2021-02-18 16:28:16.487000 UTC",
        "Owner_last_access_date":"2021-03-29 15:35:38.400000 UTC",
        "Owner_location":null,
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":"<p>According to the example <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-train-with-custom-image#define-your-environment\" rel=\"nofollow noreferrer\">here<\/a>, I think you need to configure the environment variables for the docker images stored in the Azure Container Registry:<\/p>\n<pre><code>batch_env = Environment(name='batch_environment')\nbatch_env.python.conda_dependencies = cd\nbatch_env.docker.enabled = True\n# Set the container registry information.\nbatch_env.docker.base_image_registry.address = &quot;myregistry.azurecr.io&quot;\nbatch_env.docker.base_image_registry.username = &quot;username&quot;\nbatch_env.docker.base_image_registry.password = &quot;password&quot;\nbatch_env.docker.base_image = &quot;myregistry.azurecr.io\/DEFAULT_CPU_IMAGE&quot;\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-02-19 09:12:25.093000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_valid_tags":[
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":57660058,
        "Question_title":"Azure ML SDK DataReference - File Pattern - MANY files",
        "Question_body":"<p>I\u2019m building out a pipeline that should execute and train fairly frequently.  I\u2019m following this: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-create-your-first-pipeline\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-create-your-first-pipeline<\/a> <\/p>\n\n<p>Anyways, I\u2019ve got a stream analytics job dumping telemetry into .json files on blob storage (soon to be adls gen2).  Anyways, I want to find all .json files and use all of those files to train with.  I could possibly use just new .json files as well (interesting option honestly).<\/p>\n\n<p>Currently I just have the store mounted to a data lake and available; and it just iterates the mount for the data files and loads them up.<\/p>\n\n<ol>\n<li>How can I use data references for this instead?<\/li>\n<li>What does data references do for me that mounting time stamped data does not?\na.  From an audit perspective, I have version control, execution time and time stamped read only data.  Albeit, doing a replay on this would require additional coding, but is do-able.<\/li>\n<\/ol>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2019-08-26 14:37:54.697000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"azure-machine-learning-service",
        "Question_view_count":296,
        "Owner_creation_date":"2013-11-18 19:13:55.143000 UTC",
        "Owner_last_access_date":"2022-09-13 18:41:22.000000 UTC",
        "Owner_location":"Miami Beach, FL",
        "Owner_reputation":2682,
        "Owner_up_votes":75,
        "Owner_down_votes":4,
        "Owner_views":1006,
        "Answer_body":"<p>You could pass pointer to folder as an input parameter for the pipeline, and then your step can mount the folder to iterate over the json files.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2019-08-26 21:23:08.780000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":55892554,
        "Question_title":"Invoke endpoint after model deployment : [Err 104] Connection reset by peer",
        "Question_body":"<p>I am new to Sagemaker. I have deployed my well trained model in tensorflow  by using Json and Weight file. But it is strange that in my note book, I didn't see it says \"Endpoint successfully built\". Only the below is shown:<\/p>\n\n<pre><code>--------------------------------------------------------------------------------!\n<\/code><\/pre>\n\n<p>Instead, I found the endpoint number from my console. <\/p>\n\n<pre><code>import sagemaker\nfrom sagemaker.tensorflow.model import TensorFlowModel\n        predictor=sagemaker.tensorflow.model.TensorFlowPredictor(endpoint_name, sagemaker_session)\ndata= test_out2\npredictor.predict(data)\n<\/code><\/pre>\n\n<p>Then I try to invoke the endpoint by using 2D array:\n(1) If my 2D array is in size of (5000, 170), I am getting the error:<\/p>\n\n<pre><code>ConnectionResetError: [Errno 104] Connection reset by peer\n<\/code><\/pre>\n\n<p>(2) If reducing the array to size of (10,170), error is :<\/p>\n\n<pre><code>ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from model with message \"\". See https:\/\/us-east-2.console.aws.amazon.com\/cloudwatch\/home?region=us-east-2#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/sagemaker-tensorflow-2019-04-28-XXXXXXXXX in account 15XXXXXXXX for more information.\n<\/code><\/pre>\n\n<p>Any suggestion please? Found similar case in github, <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/issues\/589\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/issues\/589<\/a>.<\/p>\n\n<p>Is it the similar case please?<\/p>\n\n<p>Thank you very much in advance!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2019-04-28 16:54:16.677000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"tensorflow|amazon-sagemaker",
        "Question_view_count":465,
        "Owner_creation_date":"2014-05-28 14:38:02.530000 UTC",
        "Owner_last_access_date":"2021-01-12 13:58:29.283000 UTC",
        "Owner_location":null,
        "Owner_reputation":61,
        "Owner_up_votes":14,
        "Owner_down_votes":0,
        "Owner_views":13,
        "Answer_body":"<p>The first error with data size (5000, 170) might be due to a capacity issue. SageMaker endpoint prediction has a size limit of 5mb. So if your data is larger than 5mb, you need to chop it into pieces and call predict multiple times. <\/p>\n\n<p>For the second error with data size (10, 170), the error message asks you to look into logs. Did you find anything interesting in the cloudwatch log? Anything can be shared in this question?<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2019-05-01 18:24:23.680000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":55112494,
        "Question_title":"Install graphiz on AWS Sagemaker",
        "Question_body":"<p>I'm on a Jupyter notebook using Python3 and trying to plot a tree with code like this:<\/p>\n\n<pre><code>import xgboost as xgb\nfrom xgboost import plot_tree\n\nplot_tree(model, num_trees=4)\n<\/code><\/pre>\n\n<p>On the last line I get:<\/p>\n\n<pre><code>~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/xgboost\/plotting.py in to_graphviz(booster, fmap, num_trees, rankdir, yes_color, no_color, **kwargs)\n196         from graphviz import Digraph\n197     except ImportError:\n--&gt; 198         raise ImportError('You must install graphviz to plot tree')\n199 \n200     if not isinstance(booster, (Booster, XGBModel)):\n\nImportError: You must install graphviz to plot tree\n<\/code><\/pre>\n\n<p>How do I install graphviz so I can see the plot_tree?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-03-11 23:07:16.840000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"xgboost|amazon-sagemaker",
        "Question_view_count":572,
        "Owner_creation_date":"2013-01-04 01:30:05.087000 UTC",
        "Owner_last_access_date":"2022-09-22 03:07:08.883000 UTC",
        "Owner_location":"Granada Hills, Los Angeles, CA, United States",
        "Owner_reputation":6262,
        "Owner_up_votes":1258,
        "Owner_down_votes":9,
        "Owner_views":428,
        "Answer_body":"<p>I was finally able to learn that Conda has a package which can install it for you. I was able to get it installed by running the command:<\/p>\n\n<pre><code>!conda install python-graphviz --yes\n<\/code><\/pre>\n\n<p>Note the <code>--yes<\/code> is only needed if the installation needs to verify adding\/changing other packages since the Jupyter notebook is not interactive once it is running.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2019-03-12 05:02:06.347000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":3.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":71321757,
        "Question_title":"What are valid Azure ML Workspace connection argument options?",
        "Question_body":"<p>I want to build an Azure ML environment with two python packages that I have in Azure Devops.\nFor this I need a workspace connection to Azure Devops. One package is published to an artifact feed and I can access it using the python SDK using a personal access token:<\/p>\n<pre><code>ws.set_connection(name=&quot;ConnectionName&quot;, \n                  category= &quot;PythonFeed&quot;, \n                  target = &quot;https:\/\/pkgs.dev.azure.com\/&quot;, \n                  authType = &quot;PAT&quot;, \n                  value = PAT_TOKEN)\n<\/code><\/pre>\n<p>However, for the other I need to get the package from the git repository in Azure Devops. The documentation of the <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.workspace.workspace?view=azure-ml-py#azureml-core-workspace-workspace-set-connection\" rel=\"nofollow noreferrer\">Python SDK<\/a> and the underlying <a href=\"https:\/\/docs.microsoft.com\/en-us\/rest\/api\/azureml\/workspace-connections\/create\" rel=\"nofollow noreferrer\">REST API<\/a> don't give the options for the arguments, only that they need to be strings (see links).<\/p>\n<p>My question: what are the options for the following arguments:<\/p>\n<ul>\n<li>authType<\/li>\n<li>category<\/li>\n<li>valueFormat<\/li>\n<\/ul>\n<p>And what do I need to set for target argument, so that I can connect to the Azure DevOps repository with potentially different authentication?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-03-02 11:17:24.250000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|azure-devops|azure-machine-learning-studio|azureml-python-sdk|azure-python-sdk",
        "Question_view_count":93,
        "Owner_creation_date":"2022-03-02 11:07:10.530000 UTC",
        "Owner_last_access_date":"2022-09-22 13:13:56.163000 UTC",
        "Owner_location":null,
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":"<p>To get the package from a Azure DevOps git repository you can change the target to the repository URL:<\/p>\n<pre><code>ws.set_connection(\n    name=&quot;ConnectionName&quot;, \n    category = &quot;PythonFeed&quot;,\n    target = &quot;https:\/\/dev.azure.com\/&lt;MY-ORG&gt;\/&lt;MY-PROJECT&gt;\/_git\/&lt;MY-REPO&gt;&quot;, \n    authType = &quot;PAT&quot;, \n    value = &lt;PAT-TOKEN&gt;)\n<\/code><\/pre>\n<p>Note here that there is no user specified in the URL (the standard &quot;clone&quot; URL in Azure DevOps also contains &quot;DevOps-Vx@&quot;).<\/p>\n<p>As for any other options for &quot;authType&quot;, &quot;category&quot; and &quot;valueFormat&quot;, I don't know.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-08-02 10:10:13.617000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":67639665,
        "Question_title":"Azure ML not able to create conda environment (exit code: -15)",
        "Question_body":"<p>When I try to run the experiment defined in <a href=\"https:\/\/github.com\/MicrosoftLearning\/mslearn-dp100\/blob\/main\/06%20-%20Work%20with%20Data.ipynb\" rel=\"nofollow noreferrer\">this notebook<\/a> in  notebook, I encountered an error when it is creating the conda env. The error occurs when the below cell is executed:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core import Experiment, ScriptRunConfig, Environment\nfrom azureml.core.conda_dependencies import CondaDependencies\nfrom azureml.widgets import RunDetails\n\n\n# Create a Python environment for the experiment\nsklearn_env = Environment(&quot;sklearn-env&quot;)\n\n# Ensure the required packages are installed (we need scikit-learn, Azure ML defaults, and Azure ML dataprep)\npackages = CondaDependencies.create(conda_packages=['scikit-learn','pip'],\n                                    pip_packages=['azureml-defaults','azureml-dataprep[pandas]'])\nsklearn_env.python.conda_dependencies = packages\n\n# Get the training dataset\ndiabetes_ds = ws.datasets.get(&quot;diabetes dataset&quot;)\n\n# Create a script config\nscript_config = ScriptRunConfig(source_directory=experiment_folder,\n                              script='diabetes_training.py',\n                              arguments = ['--regularization', 0.1, # Regularizaton rate parameter\n                                           '--input-data', diabetes_ds.as_named_input('training_data')], # Reference to dataset\n                              environment=sklearn_env)\n\n# submit the experiment\nexperiment_name = 'mslearn-train-diabetes'\nexperiment = Experiment(workspace=ws, name=experiment_name)\nrun = experiment.submit(config=script_config)\nRunDetails(run).show()\nrun.wait_for_completion() \n<\/code><\/pre>\n<p>Everytime I run this, I always faced the issue of creating the conda env as below:<\/p>\n<pre><code>Creating conda environment...\nRunning: ['conda', 'env', 'create', '-p', '\/home\/azureuser\/.azureml\/envs\/azureml_000000000000', '-f', 'azureml-environment-setup\/mutated_conda_dependencies.yml']\nCollecting package metadata (repodata.json): ...working... done\nSolving environment: ...working... done\nPreparing transaction: ...working... done\nVerifying transaction: ...working... done\nExecuting transaction: ...working... done\n\nInstalling pip dependencies: ...working... \n\nAttempting to clean up partially built conda environment: \/home\/azureuser\/.azureml\/envs\/azureml_000000000000\nRemove all packages in environment \/home\/azureuser\/.azureml\/envs\/azureml_000000000000:\nCreating conda environment failed with exit code: -15\n<\/code><\/pre>\n<p>I could not find anything useful on the internet and this is not the only script where it fail. When I am try to run other experiments I have sometimes faced this issue. One solution which worked in the above case is I moved the pandas from pip to conda and it was able to create the coonda env. Example below:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code># Ensure the required packages are installed (we need scikit-learn, Azure ML defaults, and Azure ML dataprep)\npackages = CondaDependencies.create(conda_packages=['scikit-learn','pip'],\n                                    pip_packages=['azureml-defaults','azureml-dataprep[pandas]'])\n\n<\/code><\/pre>\n<pre class=\"lang-py prettyprint-override\"><code># Ensure the required packages are installed (we need scikit-learn, Azure ML defaults, and Azure ML dataprep)\npackages = CondaDependencies.create(conda_packages=['scikit-learn','pip','pandas'],\n                                    pip_packages=['azureml-defaults','azureml-dataprep'])\n\n<\/code><\/pre>\n\n<p>The error message (or the logs from Azure) is also not much help. Would apprecite if a proper solution is available.<\/p>\n<p>Edit: I have recently started learning to use Azure for Machine learning and so if I am not sure if I am missing something? I assume the example notebooks should work as is hence raised this question.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2021-05-21 15:19:28.967000 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":"2021-05-21 16:41:21.777000 UTC",
        "Question_score":4,
        "Question_tags":"anaconda|azure-machine-learning-service",
        "Question_view_count":2373,
        "Owner_creation_date":"2017-02-27 14:51:54.693000 UTC",
        "Owner_last_access_date":"2022-06-12 06:50:27.737000 UTC",
        "Owner_location":"India",
        "Owner_reputation":493,
        "Owner_up_votes":15,
        "Owner_down_votes":0,
        "Owner_views":9,
        "Answer_body":"<h2>short answer<\/h2>\n<p>Totally been in your shoes before. This code sample seems a smidge out of date. Using <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/work-with-data\/datasets-tutorial\/train-with-datasets\/train-with-datasets.ipynb\" rel=\"nofollow noreferrer\">this notebook<\/a> as a reference, can you try the following?<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>packages = CondaDependencies.create(\n    pip_packages=['azureml-defaults','scikit-learn']\n)\n<\/code><\/pre>\n<h2>longer  answer<\/h2>\n<p><a href=\"https:\/\/www.anaconda.com\/blog\/using-pip-in-a-conda-environment\" rel=\"nofollow noreferrer\">Using pip with Conda<\/a> is not always smooth sailing. In this instance, conda isn't reporting up the issue that pip is having. The solution is to create and test this environment locally where we can get more information, which will at least will give you a more informative error message.<\/p>\n<ol>\n<li>Install anaconda  or miniconda (or use an Azure ML Compute Instance which has conda pre-installed)<\/li>\n<li>Make a  file called environment.yml that looks like this<\/li>\n<\/ol>\n<pre class=\"lang-yaml prettyprint-override\"><code>name: aml_env\ndependencies:\n - python=3.8\n - pip=21.0.1\n - pip:\n    - azureml-defaults\n    - azureml-dataprep[pandas]\n    - scikit-learn==0.24.1\n<\/code><\/pre>\n<ol start=\"3\">\n<li>Create this environment with the command <code>conda env create -f environment.yml<\/code>.<\/li>\n<li>respond to any discovered error message<\/li>\n<li>If there' no error, use this new <code>environment.yml<\/code> with Azure ML like so<\/li>\n<\/ol>\n<pre class=\"lang-py prettyprint-override\"><code>sklearn_env = Environment.from_conda_specification(name = 'sklearn-env', file_path = '.\/environment.yml')\n<\/code><\/pre>\n<h2>more context<\/h2>\n<p>the error I'm guessing that's happening is when you reference a pip requirements file from a conda environment file. In this scenario, conda calls <code>pip install -r  requirements.txt<\/code> and if that command errors out, conda can't report the error.<\/p>\n<h3><code>requirements.txt<\/code><\/h3>\n<pre><code>scikit-learn==0.24.1\nazureml-dataprep[pandas]\n<\/code><\/pre>\n<h3><code>environment.yml<\/code><\/h3>\n<pre><code>name: aml_env\ndependencies:\n - python=3.8\n - pip=21.0.1\n - pip:\n    - -rrequirements.txt\n<\/code><\/pre>",
        "Answer_comment_count":6.0,
        "Answer_creation_date":"2021-05-21 16:37:34.950000 UTC",
        "Answer_last_edit_date":"2021-05-21 17:43:44.733000 UTC",
        "Answer_score":3.0,
        "Question_valid_tags":[
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":30172382,
        "Question_title":"How to write the body for HTTPS POST job in Azure Schedular without Azure Blob",
        "Question_body":"<p>I have created an experiment and successfully published a web service which requires inputs.<\/p>\n\n<p>When I schedule this web service as a HTTPS POST JOB it shows this error<\/p>\n\n<blockquote>\n  <p>Http Action - Response from host\n  'ussouthcentral.services.azureml.net': 'BadRequest' Response Headers:\n  x-ms-request-id: 51fb1d34-5bc7-4832-ad9f-b19826468ea0 Date: Mon, 11\n  May 2015 11:02:01 GMT Server: Microsoft-HTTPAPI\/2.0  Body:\n  {\"error\":{\"code\":\"BadArgument\",\"message\":\"Invalid argument\n  provided.\",\"details\":[{\"code\":\"MissingInputBlobInformation\",\"target\":\"Inputs\",\"message\":\"Missing\n  Azure storage blob information. Provide a valid connection string and\n  relative path or URI and try again.\"}]}}<\/p>\n<\/blockquote>\n\n<p>My data is not located in Azure Blob Storage. I am am trying to pass this web input as part as a HTTPS POST BODY.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2015-05-11 16:01:27.507000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2015-05-12 16:02:26.640000 UTC",
        "Question_score":1,
        "Question_tags":"azure|machine-learning|azure-scheduler|azure-machine-learning-studio",
        "Question_view_count":460,
        "Owner_creation_date":"2014-06-23 16:37:06.413000 UTC",
        "Owner_last_access_date":"2020-06-18 08:44:47.883000 UTC",
        "Owner_location":"Bengaluru, India",
        "Owner_reputation":191,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":22,
        "Answer_body":"<p>If you are using BES with web service input and output, you would need to provide the Storage information for the data. \nWith the Reader and Writer modules, you can remove the web service input and output ports.\nThen when the web service is called, it executes without using the Storage blob. It will read from the Reader and write to the destination specified in the Writer.\nI have uploaded a <a href=\"https:\/\/azuremlbesclienttemplate.codeplex.com\/documentation\" rel=\"nofollow\">Visual Studio template to CodePlex<\/a> that you can install. The NoInputOutput.aspx of that project does the above. And it should show you the workflow.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2015-05-15 21:16:20.933000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":47847736,
        "Question_title":"AWS Sagemaker Neural Topic Model",
        "Question_body":"<p>What is underlying algorithm for Sagemaker's <a href=\"http:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ntm.html\" rel=\"nofollow noreferrer\">Neural Topic Model<\/a>? I have hard time googling for details, and the documentation doesn't mention any paper.<\/p>\n\n<p>Googling for 'neural topic model' doesn't exactly answer my question, since a couple of methods seems to be called that.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2017-12-16 16:29:08.563000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"amazon-web-services|neural-network|amazon-sagemaker",
        "Question_view_count":222,
        "Owner_creation_date":"2013-07-23 09:40:39.353000 UTC",
        "Owner_last_access_date":"2022-09-21 17:49:28.250000 UTC",
        "Owner_location":"Poland",
        "Owner_reputation":2177,
        "Owner_up_votes":523,
        "Owner_down_votes":113,
        "Owner_views":262,
        "Answer_body":"<p>Seems like AWS SageMaker team answered the question, \n<a href=\"https:\/\/forums.aws.amazon.com\/thread.jspa?threadID=270493&amp;tstart=0\" rel=\"nofollow noreferrer\">https:\/\/forums.aws.amazon.com\/thread.jspa?threadID=270493&amp;tstart=0<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2018-01-06 17:35:39.877000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":68944750,
        "Question_title":"Mount a datalake storage in azure ML studio",
        "Question_body":"<p>I created a file dataset from a data lake folder on Azure ML Studio,  at the moment I\u00b4m able to download the data from the dataset to the compute instance with this code:<\/p>\n<pre><code>subscription_id = 'xxx'\nresource_group = 'luisdatapipelinetest'\nworkspace_name = 'ml-pipelines'\nworkspace = Workspace(subscription_id, resource_group, workspace_name)\ndataset = Dataset.get_by_name(workspace, name='files_test')\npath = &quot;\/mnt\/batch\/tasks\/shared\/LS_root\/mounts\/clusters\/demo1231\/code\/Users\/luis.rramirez\/test\/&quot;\ndataset.download(target_path=path, overwrite=True)\n<\/code><\/pre>\n<p>With that I'm able to access the files from the notebook.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/8q8y2.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/8q8y2.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>But copying the data from the data lake to the compute instance is not efficient, how can I mount the data lake directory in the vm instead of copying the data each time?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-08-26 20:08:50.263000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-08-27 06:10:24.417000 UTC",
        "Question_score":1,
        "Question_tags":"python|azure|azure-data-lake|azure-machine-learning-service|ml-studio",
        "Question_view_count":258,
        "Owner_creation_date":"2015-02-08 23:53:31.840000 UTC",
        "Owner_last_access_date":"2022-09-24 00:08:32.523000 UTC",
        "Owner_location":null,
        "Owner_reputation":8349,
        "Owner_up_votes":1489,
        "Owner_down_votes":6,
        "Owner_views":949,
        "Answer_body":"<p>MOUNTING ADLS2 to AML so you can save files into your mountPoint directly. <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-data#azure-data-lake-storage-generation-2\" rel=\"nofollow noreferrer\">Here<\/a> is the example of registering the storage and <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data.file_dataset.filedataset?view=azure-ml-py#mount-mount-point-none----kwargs-\" rel=\"nofollow noreferrer\">here<\/a> shows how to mount your registered datastore.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-08-30 04:49:53.133000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":73251212,
        "Question_title":"How do I retrieve a model in Vertex AI?",
        "Question_body":"<p>I defined a training job:<\/p>\n<pre><code>job = aiplatform.AutoMLTextTrainingJob(...\n<\/code><\/pre>\n<p>then I created a model by running the job:<\/p>\n<pre><code>model = job.run(...\n<\/code><\/pre>\n<p>It worked fine but it is now the next day and the variable <code>model<\/code> was in a Jupyter notebook and no longer exists. I have tried to get it back with:<\/p>\n<pre><code>from google.cloud import aiplatform_v1beta1\n\ndef sample_get_model():\n    client = aiplatform_v1beta1.ModelServiceClient()\n\n    model_id=id_of_training_pipeline\n    name= f'projects\/{PROJECT}\/locations\/{REGION}\/models\/{model_id}'\n    \n    request = aiplatform_v1beta1.GetModelRequest(name=name)\n    response = client.get_model(request=request)\n    print(response)\n\nsample_get_model()\n<\/code><\/pre>\n<p>I have also tried the id of v1 of the model created in place of <code>id_of_training_pipeline<\/code> and I have tried <code>\/pipelines\/pipeline_id<\/code><\/p>\n<p>but I get:\n<code>E0805 15:12:36.784008212   28406 hpack_parser.cc:1234]       Error parsing metadata: error=invalid value key=content-type value=text\/html; charset=UTF-8<\/code><\/p>\n<p>(<code>PROJECT<\/code> and <code>REGION<\/code> are set correctly).<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-08-05 14:19:46.783000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"google-cloud-vertex-ai",
        "Question_view_count":45,
        "Owner_creation_date":"2012-10-25 08:48:34.717000 UTC",
        "Owner_last_access_date":"2022-09-23 10:10:32.783000 UTC",
        "Owner_location":null,
        "Owner_reputation":2564,
        "Owner_up_votes":304,
        "Owner_down_votes":8,
        "Owner_views":451,
        "Answer_body":"<p>Found <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/samples\/aiplatform-get-model-sample#aiplatform_get_model_sample-python\" rel=\"nofollow noreferrer\">this<\/a> Google code which works.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-08-05 15:18:48.223000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_valid_tags":[
            "google-cloud-vertex-ai"
        ]
    },
    {
        "Question_id":73621446,
        "Question_title":"Log Any Type of Model in MLflow",
        "Question_body":"<p>I am trying to create a wrapper function that allows my Data Scientists to log their models in MLflow.<\/p>\n<p>This is what the function looks like,<\/p>\n<pre><code>def log_model(self, params, metrics, model, run_name, artifact_path, artifacts=None):\n\n    with mlflow.start_run(run_name=run_name):\n        run_id = mlflow.active_run().info.run_id\n        mlflow.log_params(params)\n        mlflow.log_metrics(metrics)\n\n        if model:\n            mlflow.lightgbm.log_model(model, artifact_path=artifact_path)\n\n        if artifacts:\n            for artifact in artifacts:\n                mlflow.log_artifact(artifact, artifact_path=artifact_path)\n\n    return run_id\n<\/code><\/pre>\n<p>It can be seen here that the model is being logged as a <code>lightgbm<\/code> model, however, the <code>model<\/code> parameter that is passed into this function can be of any type.<\/p>\n<p>How can I update this function, so that it will be able to log any kind of model?<\/p>\n<p>As far as I know, there is no <code>log_model<\/code> function that comes with <code>mlflow<\/code>. It's always <code>mlflow.&lt;model_type&gt;.log_model<\/code>.<\/p>\n<p>How can I go about handling this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-09-06 11:40:47.387000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"python|mlflow",
        "Question_view_count":39,
        "Owner_creation_date":"2018-03-24 01:53:05.820000 UTC",
        "Owner_last_access_date":"2022-09-24 16:46:35.903000 UTC",
        "Owner_location":"Sri Lanka",
        "Owner_reputation":820,
        "Owner_up_votes":389,
        "Owner_down_votes":1,
        "Owner_views":165,
        "Answer_body":"<p>I was able to solve this using the following approach,<\/p>\n<pre><code>def log_model(model, artifact_path):\n    model_class = get_model_class(model).split('.')[0]\n\n    try:\n        log_model = getattr(mlflow, model_class).log_model\n        log_model(model, artifact_path)\n    except AttributeError:\n        logger.info('The log_model function is not available as expected!')\n\ndef get_model_class(model):\n    klass = model.__class__\n    module = klass.__module__\n\n    if module == 'builtins':\n        return klass.__qualname__\n    return module + '.' + klass.__qualname__\n<\/code><\/pre>\n<p>From what I have seen, this will be able to handle most cases. The <code>get_model_class()<\/code> method will return the class used to develop the model and based on this, we can use the <code>getattr()<\/code> method to extract the relevant <code>log_model()<\/code> method.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-09-20 01:11:58.523000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_valid_tags":[
            "mlflow"
        ]
    },
    {
        "Question_id":41010551,
        "Question_title":"predicting the possibility of Active members becoming Inactive?",
        "Question_body":"<p>I have a database of members, some are active and some are inactive. <\/p>\n\n<p>I want to predict the possibility of Active members becoming Inactive?<\/p>\n\n<p>Should I run the AML on the inactive members (no splitting) and when I publish the model i pass in the active members?<\/p>\n\n<p>I have tried many AML datasets before however usually you will have a column that contains the values you want to predict (Active-Inactive) (True-False) (Red-Black-White) but i never tried having only one value to trina your model with.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2016-12-07 06:09:21.200000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2016-12-07 06:51:43.170000 UTC",
        "Question_score":0,
        "Question_tags":"azure|machine-learning|azure-machine-learning-studio",
        "Question_view_count":83,
        "Owner_creation_date":"2016-12-07 06:02:33.287000 UTC",
        "Owner_last_access_date":"2020-02-03 08:25:54.213000 UTC",
        "Owner_location":null,
        "Owner_reputation":15,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Answer_body":"<p>you will need to train your model with both active and inactive members.  I would split your dataset so there are examples of active and inactive members in both your training and your test set.  <\/p>\n\n<p>Let's discuss why we split the data.  Remember that with supervised learning, you need data with labeled examples.  For example, let\u2019s say I want to predict how much a house will cost based on its square footage and zip code.  To train my model, I need a dataset of existing houses with their square footage, zip codes, and prices, like this:<\/p>\n\n<p>SquareFootage ZipCode Price <br\/>\n2000          48075   200,000 <br\/>\n3000 48075 300,000 <br\/>\n4000 48075 400,000 <br\/>\n5000 48075 500,000 <br\/><\/p>\n\n<p>In this example, square footage and zip code are my features (things that influence the thing you want to predict) and price is my label (the thing that you want to predict).  I could train a model on some data like the above, and then use the trained model to predict prices, given only a square footage and zip code.  <\/p>\n\n<p>So, the reason I split the data is to provide most of the data to train the model (it will process the data to figure out correlations between the features and labels in the \u201ctrain model\u201d module), but we want to hold back some of that labeled data to test the model that we built.  Then, we can compare the price value that the trained model generates against the actual labeled price value in the test dataset (in the \u201cscore model\u201d module) to see how well the model is performing.  (We can\u2019t use the same data for both...the model is built using the training data, so it will perform pretty accurately with that; we hold back unused data to test.)  <\/p>\n\n<p>So, for your example, I would try a random split so there are examples of both active and inactive members (that is your label - inactive or active) and you will also need to provide relevant features that influence activity.  <\/p>",
        "Answer_comment_count":5.0,
        "Answer_creation_date":"2016-12-07 06:53:22.247000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":68775733,
        "Question_title":"How to choose from a list of Regions for GPU instances in Amazon SageMaker?",
        "Question_body":"<p>I'm currently trying to do distributed GPU training with 2 instances of ml.p3.8xlarge and after 4 attempts I have not been able to start a training job with the spot instances since AWS did not have any available instances in my region.<\/p>\n<p>How do I increase the number of regions I'm willing to choose from in SageMaker? At the moment I'm only using:<\/p>\n<p><code>sess.boto_region_name = us-east-1<\/code> (sagemaker session region)<\/p>\n<p>But I'm assuming if I allow SageMaker to choose from other regions, I will be able to start a training job with spot instances.<\/p>\n<p>Thanks!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-08-13 16:37:13.507000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-08-13 17:58:17.303000 UTC",
        "Question_score":0,
        "Question_tags":"amazon-web-services|boto3|amazon-sagemaker",
        "Question_view_count":120,
        "Owner_creation_date":"2016-07-19 00:48:21.237000 UTC",
        "Owner_last_access_date":"2022-09-13 07:31:55.037000 UTC",
        "Owner_location":null,
        "Owner_reputation":819,
        "Owner_up_votes":42,
        "Owner_down_votes":0,
        "Owner_views":87,
        "Answer_body":"<p>Most AWS services are regional-based, meaning they run in a given region and do not spread <em>beyond<\/em> one region.<\/p>\n<p>If you wish to run SageMaker in multiple regions, you would need to launch it <em>separately<\/em> in each region. So, you would only be 'choosing' <em>one<\/em> region when requesting SageMaker to perform some work.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2021-08-14 03:51:49.040000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":58879596,
        "Question_title":"Unable to call XG-Boost endpoint created in sagemaker using AWS-Lambda",
        "Question_body":"<p>I trained an xgboost model on AWS-Sagemaker and created an endpoint. Now I want to call the endpoint using AWS Lambda and AWS API. I created an lambda function and added the below mentioned code for my xgboost model. When I try to test it, the function is throwing a ParamValidation error. Here is my code<\/p>\n\n<pre><code>import json\nimport os\nimport csv\nimport io\nimport boto3\nendpointname =os.environ['endpointname'] #name of the endpoint I created in sagemaker\nruntime = boto3.client('runtime.sagemaker')\ndef lambda_handler(event, context):\n    print(\"Recieved Event: \"+json.dumps(event,indent=2))\n    data=json.loads(json.dumps(event))\n    print(data)\n    response = runtime.invoke_endpoint(EndpointName=endpointname,ContentType='text\/csv',Body=data)\n    print(response)\n    result = json.loads(response['Body'].read().decode())\n    print(int(float(result))) #sagemaker xgb returns bytes type for the test case\n<\/code><\/pre>\n\n<p>The test event I created is dict type. The function is throwing  <code>Invalid type for parameter Body, value: {'Time':'7'}, type: &lt;class 'dict'&gt;, valid types: &lt;class 'bytes'&gt;, &lt;class 'bytearray'&gt;, file-like object<\/code>\nIt means I should pass either byte or bytearray instead of dict type into my event. But when I read this <a href=\"https:\/\/docs.aws.amazon.com\/lambda\/latest\/dg\/python-programming-model-handler-types.html\" rel=\"nofollow noreferrer\">AWS Lambda doc<\/a> It says that my event type can only be dict,int,list,float,str, or None type. I followed the steps mentioned in <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda\/\" rel=\"nofollow noreferrer\">this<\/a> aws doc to create my lambda function. Can someone please explain why my code is throwing above mentioned error?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-11-15 14:58:08.267000 UTC",
        "Question_favorite_count":0.0,
        "Question_last_edit_date":"2019-11-15 17:05:37.747000 UTC",
        "Question_score":1,
        "Question_tags":"python|amazon-web-services|machine-learning|aws-lambda|amazon-sagemaker",
        "Question_view_count":519,
        "Owner_creation_date":"2019-09-12 20:07:41.627000 UTC",
        "Owner_last_access_date":"2022-09-21 14:35:39.230000 UTC",
        "Owner_location":"Hyderabad, Telangana, India",
        "Owner_reputation":486,
        "Owner_up_votes":28,
        "Owner_down_votes":2,
        "Owner_views":75,
        "Answer_body":"<p><code>data=json.loads(json.dumps(event))<\/code> is a redundant operation. <code>data=event<\/code> will return <code>True<\/code>. The event we provided for the test case is of type dict. It has a key value pair. key can be anything and the value should be a single string of all the predictor variables separated by comas. For predicting the output, we need value of the test case. So declare, for example, <code>payload=data['key']<\/code> then change <code>Body=payload<\/code> inside <code>response<\/code>. Then it will work.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2019-11-19 11:25:55.397000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":73773006,
        "Question_title":"ClientError: Annotation value 10 found in labels. This is greater than number of classes., exit code: 2",
        "Question_body":"<p>My total number of classes is 10<\/p>\n<p>1-&quot;Button_damage&quot;\n2-&quot;Cracks&quot;\n3-&quot;Edge_damage&quot;\n4-&quot;Frame_damage&quot;\n5-&quot;Hinge_damage&quot;\n6-&quot;Screen_damage&quot;\n7-&quot;Good_Button&quot;\n8-&quot;Good_Hinge&quot;\n9-&quot;Good_screen&quot;\n10-&quot;Good_frame&quot;<\/p>\n<p>When I give num_classes=10 in hyperparameter it throws a error.<\/p>\n<p>ClientError: Annotation value 10 found in labels. This is greater than number of classes., exit code: 2<\/p>\n<p>I changed num_classes=9 and tried, it shows same error.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-09-19 11:56:33.177000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":7,
        "Owner_creation_date":"2022-03-23 07:16:02.050000 UTC",
        "Owner_last_access_date":"2022-09-22 13:44:39.503000 UTC",
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":"<p>It was worked when I changed the num_classes=11. Because it adds one label &quot;0&quot; by default.<\/p>\n<p>0-&quot;BACKGROUND&quot;\n1-&quot;Button_damage&quot;\n2-&quot;Cracks&quot;\n3-&quot;Edge_damage&quot;\n4-&quot;Frame_damage&quot;\n5-&quot;Hinge_damage&quot;\n6-&quot;Screen_damage&quot;\n7-&quot;Good_Button&quot;\n8-&quot;Good_Hinge&quot;\n9-&quot;Good_screen&quot;\n10-&quot;Good_frame&quot;<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-09-19 11:56:33.177000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":73580703,
        "Question_title":"DVC | Permission denied ERROR: failed to reproduce stage: failed to run: .py, exited with 126",
        "Question_body":"<p>Goal: run <code>.py<\/code> files via. <code>dvc.yaml<\/code>.<\/p>\n<p>There are stages before it, in <code>dvc.yaml<\/code>, that don't produce the error.<\/p>\n<p><code>dvc exp run<\/code>:<\/p>\n<pre><code>(venv) me@ubuntu-pcs:~\/PycharmProjects\/project$ dvc exp run\nStage 'inference' didn't change, skipping\nRunning stage 'load_data':\n&gt; load_data.py\n\/bin\/bash: line 1: load_data.py: Permission denied\nERROR: failed to reproduce 'load_data': failed to run: load_data.py, exited with 126\n<\/code><\/pre>\n<p><code>dvc repro<\/code>:<\/p>\n<pre><code>(venv) me@ubuntu-pcs:~\/PycharmProjects\/project$ dvc repro\nStage 'predict' didn't change, skipping                                                                                                                                                                                                                        \nStage 'evaluate' didn't change, skipping\nStage 'inference' didn't change, skipping\nRunning stage 'load_data':\n&gt; load_data.py\n\/bin\/bash: line 1: load_data.py: Permission denied\nERROR: failed to reproduce 'load_data': failed to run: pdl1_lung_model\/load_data.py, exited with 126\n<\/code><\/pre>\n<hr \/>\n<p><code>dvc doctor<\/code>:<\/p>\n<pre><code>DVC version: 2.10.2 (pip)\n---------------------------------\nPlatform: Python 3.9.12 on Linux-5.15.0-46-generic-x86_64-with-glibc2.35\nSupports:\n        webhdfs (fsspec = 2022.5.0),\n        http (aiohttp = 3.8.1, aiohttp-retry = 2.5.2),\n        https (aiohttp = 3.8.1, aiohttp-retry = 2.5.2),\n        s3 (s3fs = 2022.5.0, boto3 = 1.21.21)\nCache types: hardlink, symlink\nCache directory: ext4 on \/dev\/nvme0n1p5\nCaches: local\nRemotes: s3\nWorkspace directory: ext4 on \/dev\/nvme0n1p5\nRepo: dvc, git\n<\/code><\/pre>\n<p><code>dvc exp run -v<\/code>:<\/p>\n<p><a href=\"https:\/\/gist.github.com\/danielbellsa\/3f2fe05c1535d494a8677e54cddf684a\" rel=\"nofollow noreferrer\">output.txt<\/a><\/p>\n<p><code>dvc exp run -vv<\/code>:<\/p>\n<p><a href=\"https:\/\/gist.github.com\/danielbellsa\/a124cf28b3f0252556deb90b042b7cec\" rel=\"nofollow noreferrer\">output2.txt<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2022-09-02 09:45:17.277000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-09-05 09:06:49.863000 UTC",
        "Question_score":1,
        "Question_tags":"python-3.x|permission-denied|dvc",
        "Question_view_count":77,
        "Owner_creation_date":"2021-09-07 12:58:02.980000 UTC",
        "Owner_last_access_date":"2022-09-23 15:24:35.073000 UTC",
        "Owner_location":null,
        "Owner_reputation":234,
        "Owner_up_votes":708,
        "Owner_down_votes":14,
        "Owner_views":155,
        "Answer_body":"<h3>Solution 1<\/h3>\n<p><code>.py<\/code> files weren't running as scripts.<\/p>\n<p>They need to be; if you want to run one <code>.py<\/code> file per <code>stage<\/code> in <code>dvc.yaml<\/code>.<\/p>\n<p>To do so, you want to append <strong>Boiler-plate code<\/strong>, at the bottom of each <code>.py<\/code> file.<\/p>\n<pre><code>if __name__ == &quot;__main__&quot;:\n    # invoke primary function() in .py file, w\/ params\n<\/code><\/pre>\n<h3>Solution 2<\/h3>\n<pre><code>chmod 777 ....py\n<\/code><\/pre>\n<h3>Soution 3<\/h3>\n<p>I forgot the <code>python<\/code> in <code>cmd:<\/code><\/p>\n<pre><code>  load_data:\n    cmd: python pdl1_lung_model\/load_data.py\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-09-02 10:34:04.133000 UTC",
        "Answer_last_edit_date":"2022-09-05 09:05:53.113000 UTC",
        "Answer_score":1.0,
        "Question_valid_tags":[
            "dvc"
        ]
    },
    {
        "Question_id":69316032,
        "Question_title":"Custom Container deployment in vertex ai",
        "Question_body":"<p>I am trying to deploy my custom container in vertex ai endpoint for predictions. The contents of the application are as follows.<\/p>\n<ol>\n<li>Flask - app.py<\/li>\n<\/ol>\n<pre><code>import pandas as pd\nfrom flask import Flask, jsonify,request\nimport tensorflow\nimport pre_process\nimport post_process\n\n\napp = Flask(__name__)\n\n\n@app.route('\/predict',methods=['POST'])\ndef predict():\n    req = request.json.get('instances')\n    \n    input_data = req[0]['email']\n\n    #preprocessing\n    text = pre_process.preprocess(input_data)\n    vector = pre_process.preprocess_tokenizing(text)\n\n    model = tensorflow.keras.models.load_model('model')\n\n    #predict\n    prediction = model.predict(vector)\n\n    #postprocessing\n    value = post_process.postprocess(list(prediction[0])) \n    \n    return jsonify({'output':{'doc_class':value}})\n\n\nif __name__=='__main__':\n    app.run(host='0.0.0.0')\n<\/code><\/pre>\n<ol start=\"2\">\n<li>Dockerfile<\/li>\n<\/ol>\n<pre><code>FROM python:3.7\n\nWORKDIR \/app\n\nCOPY . \/app\n\nRUN pip install --trusted-host pypi.python.org -r requirements.txt \n\n\nCMD [&quot;gunicorn&quot;, &quot;--bind&quot;, &quot;0.0.0.0:5000&quot;, &quot;app:app&quot;]\n\nEXPOSE 5050\n<\/code><\/pre>\n<ol start=\"3\">\n<li>pre_process.py<\/li>\n<\/ol>\n<pre><code>#import \nimport pandas as pd\nimport pickle\nimport re\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n\ndef preprocess(text):\n    &quot;&quot;&quot;Do all the Preprocessing as shown above and\n    return a tuple contain preprocess_email,preprocess_subject,preprocess_text for that Text_data&quot;&quot;&quot;\n         \n    \n    #After you store it in the list, Replace those sentances in original text by space.\n    text = re.sub(&quot;(Subject:).+&quot;,&quot; &quot;,text,re.I)\n    \n    #Delete all the sentances where sentence starts with &quot;Write to:&quot; or &quot;From:&quot;.\n    text = re.sub(&quot;((Write to:)|(From:)).+&quot;,&quot;&quot;,text,re.I)\n    \n    #Delete all the tags like &quot;&lt; anyword &gt;&quot;\n    text = re.sub(&quot;&lt;[^&gt;&lt;]+&gt;&quot;,&quot;&quot;,text)\n    \n    #Delete all the data which are present in the brackets.\n    text = re.sub(&quot;\\([^()]+\\)&quot;,&quot;&quot;,text)\n    \n    #Remove all the newlines('\\n'), tabs('\\t'), &quot;-&quot;, &quot;&quot;.\n    text = re.sub(&quot;[\\n\\t\\\\-]+&quot;,&quot;&quot;,text)\n    \n    #Remove all the words which ends with &quot;:&quot;.\n    text = re.sub(&quot;(\\w+:)&quot;,&quot;&quot;,text)\n    \n    #Decontractions, replace words like below to full words.\n\n    lines = re.sub(r&quot;n\\'t&quot;, &quot; not&quot;, text)\n    lines = re.sub(r&quot;\\'re&quot;, &quot; are&quot;, lines)\n    lines = re.sub(r&quot;\\'s&quot;, &quot; is&quot;, lines)\n    lines = re.sub(r&quot;\\'d&quot;, &quot; would&quot;, lines)\n    lines = re.sub(r&quot;\\'ll&quot;, &quot; will&quot;, lines)\n    lines = re.sub(r&quot;\\'t&quot;, &quot; not&quot;, lines)\n    lines = re.sub(r&quot;\\'ve&quot;, &quot; have&quot;, lines)\n    lines = re.sub(r&quot;\\'m&quot;, &quot; am&quot;, lines)\n    text = lines\n    \n        #replace numbers with spaces\n    text = re.sub(&quot;\\d+&quot;,&quot; &quot;,text)\n    \n        # remove _ from the words starting and\/or ending with _\n    text = re.sub(&quot;(\\s_)|(_\\s)&quot;,&quot; &quot;,text)\n    \n        #remove 1 or 2 letter word before _\n    text = re.sub(&quot;\\w{1,2}_&quot;,&quot;&quot;,text)\n    \n        #convert all letters to lowercase and remove the words which are greater \n        #than or equal to 15 or less than or equal to 2.\n    text = text.lower()\n    \n    text =&quot; &quot;.join([i for i in text.split() if len(i)&lt;15 and len(i)&gt;2])\n    \n    #replace all letters except A-Z,a-z,_ with space\n    preprocessed_text = re.sub(&quot;\\W+&quot;,&quot; &quot;,text)\n\n    return preprocessed_text\n\ndef preprocess_tokenizing(text):\n        \n    #from tf.keras.preprocessing.text import Tokenizer\n    #from tf.keras.preprocessing.sequence import pad_sequences\n    \n    tokenizer = pickle.load(open('tokenizer.pkl','rb'))\n\n    max_length = 1019\n    tokenizer.fit_on_texts([text])\n    encoded_docs = tokenizer.texts_to_sequences([text])\n    text_padded = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n    \n    return text_padded\n<\/code><\/pre>\n<ol start=\"4\">\n<li>post_process.py<\/li>\n<\/ol>\n<pre><code>def postprocess(vector):\n    index = vector.index(max(vector))\n    classes = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n    return classes[index]\n<\/code><\/pre>\n<ol start=\"4\">\n<li>requirements.txt<\/li>\n<\/ol>\n<pre><code>gunicorn\npandas==1.3.3\nnumpy==1.19.5\nflask\nflask-cors\nh5py==3.1.0\nscikit-learn==0.24.2\ntensorflow==2.6.0\n\n<\/code><\/pre>\n<ol start=\"5\">\n<li><p>model<\/p>\n<\/li>\n<li><p>tokenizer.pkl<\/p>\n<\/li>\n<\/ol>\n<p>I am following this blog <a href=\"https:\/\/medium.com\/mlearning-ai\/serverless-prediction-at-scale-part-2-custom-container-deployment-on-vertex-ai-103a43d0a290\" rel=\"nofollow noreferrer\">vertex ai deployment<\/a> for gcloud console commands to containerise and deploy the model to endpoint.But the model is taking forever to get deployed and ultimately fails to get deployed.<\/p>\n<p>After running the container in local host, it runs as expected but it is not getting deployed into vertex ai endpoint. I don't understand whether the problem is in flask app.py or Dockerfile or whether the problem lies somewhere else.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2021-09-24 13:42:01.653000 UTC",
        "Question_favorite_count":2.0,
        "Question_last_edit_date":null,
        "Question_score":3,
        "Question_tags":"flask|dockerfile|google-cloud-vertex-ai",
        "Question_view_count":629,
        "Owner_creation_date":"2021-09-08 09:22:34.063000 UTC",
        "Owner_last_access_date":"2022-05-19 04:33:00.303000 UTC",
        "Owner_location":null,
        "Owner_reputation":111,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":12,
        "Answer_body":"<p>I was able to resolve this issue by adding health route to http server. I added the following piece of code in my flask app.<\/p>\n<pre><code>@app.route('\/healthz')\ndef healthz():\n    return &quot;OK&quot;\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2021-09-28 05:16:11.387000 UTC",
        "Answer_last_edit_date":"2021-09-28 06:36:09.573000 UTC",
        "Answer_score":4.0,
        "Question_valid_tags":[
            "google-cloud-vertex-ai"
        ]
    },
    {
        "Question_id":60637170,
        "Question_title":"How to pass arguments to scoring file when deploying a Model in AzureML",
        "Question_body":"<p>I am deploying a trained model to an ACI endpoint on Azure Machine Learning, using the Python SDK.\nI have created my score.py file, but I would like that file to be called with an argument being passed (just like with a training file) that I can interpret using <code>argparse<\/code>.\nHowever, I don't seem to find how I can pass arguments\nThis is the code I have to create the InferenceConfig environment and which obviously does not work.  Should I fall back on using the extra Docker file steps or so?<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core.conda_dependencies import CondaDependencies\nfrom azureml.core.environment import Environment\nfrom azureml.core.model import InferenceConfig\n\nenv = Environment('my_hosted_environment')\nenv.python.conda_dependencies = CondaDependencies.create(\n    conda_packages=['scikit-learn'],\n    pip_packages=['azureml-defaults'])\nscoring_script = 'score.py --model_name ' + model_name\ninference_config = InferenceConfig(entry_script=scoring_script, environment=env)\n<\/code><\/pre>\n\n<p>Adding the score.py for reference on how I'd love to use the arguments in that script:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>#removed imports\nimport argparse\n\ndef init():\n    global model\n\n    parser = argparse.ArgumentParser(description=\"Load sklearn model\")\n    parser.add_argument('--model_name', dest=\"model_name\", required=True)\n    args, _ = parser.parse_known_args()\n\n    model_path = Model.get_model_path(model_name=args.model_name)\n    model = joblib.load(model_path)\n\ndef run(raw_data):\n    try:\n        data = json.loads(raw_data)['data']\n        data = np.array(data)\n        result = model.predict(data)\n        return result.tolist()\n\n    except Exception as e:\n        result = str(e)\n        return result\n<\/code><\/pre>\n\n<p>Interested to hear your thoughts<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":2,
        "Question_creation_date":"2020-03-11 13:27:40.433000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-03-12 09:38:40.357000 UTC",
        "Question_score":4,
        "Question_tags":"python|azure-machine-learning-service",
        "Question_view_count":1681,
        "Owner_creation_date":"2013-02-12 07:50:30.743000 UTC",
        "Owner_last_access_date":"2022-09-21 18:28:12.907000 UTC",
        "Owner_location":"Belgium",
        "Owner_reputation":2947,
        "Owner_up_votes":297,
        "Owner_down_votes":16,
        "Owner_views":355,
        "Answer_body":"<p>How to deploy using environments can be found here <a href=\"https:\/\/nam06.safelinks.protection.outlook.com\/?url=https%3A%2F%2Fgithub.com%2FAzure%2FMachineLearningNotebooks%2Fblob%2Fmaster%2Fhow-to-use-azureml%2Fdeployment%2Fdeploy-to-cloud%2Fmodel-register-and-deploy.ipynb&amp;data=02%7C01%7CRamprasad.Mula%40microsoft.com%7Ce06d310b0447416ab46b08d7bc836a81%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637185146436156499&amp;sdata=uQo332dpjuiNqWFCguvs3Kgg7UUMN8MBEzLxTPyH4MM%3D&amp;reserved=0\" rel=\"nofollow noreferrer\">model-register-and-deploy.ipynb<\/a> .  InferenceConfig class accepts  source_directory and entry_script <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.model.inferenceconfig?view=azure-ml-py#parameters\" rel=\"nofollow noreferrer\">parameters<\/a>, where source_directory  is a path to the folder that contains all files(score.py and any other additional files) to create the image. <\/p>\n\n<p>This <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/deployment\/deploy-multi-model\/multi-model-register-and-deploy.ipynb\" rel=\"nofollow noreferrer\">multi-model-register-and-deploy.ipynb<\/a> has code snippets on how to create InferenceConfig with source_directory and entry_script.<\/p>\n\n<pre><code>from azureml.core.webservice import Webservice\nfrom azureml.core.model import InferenceConfig\nfrom azureml.core.environment import Environment\n\nmyenv = Environment.from_conda_specification(name=\"myenv\", file_path=\"myenv.yml\")\ninference_config = InferenceConfig(entry_script=\"score.py\", environment=myenv)\n\nservice = Model.deploy(workspace=ws,\n                       name='sklearn-mnist-svc',\n                       models=[model], \n                       inference_config=inference_config,\n                       deployment_config=aciconfig)\n\nservice.wait_for_deployment(show_output=True)\n\nprint(service.scoring_uri)\n<\/code><\/pre>",
        "Answer_comment_count":4.0,
        "Answer_creation_date":"2020-03-12 09:36:25.480000 UTC",
        "Answer_last_edit_date":"2020-03-12 11:19:48.083000 UTC",
        "Answer_score":-2.0,
        "Question_valid_tags":[
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":62886435,
        "Question_title":"Using a custom docker with Azure ML",
        "Question_body":"<p>I'm following the guidelines (<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-use-environments\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-use-environments<\/a>) to use a custom docker file on Azure. My script to create the environment looks like this:<\/p>\n<pre><code>from azureml.core.environment import Environment\n\nmyenv = Environment(name = &quot;myenv&quot;)\nmyenv.docker.enabled = True\ndockerfile = r&quot;&quot;&quot;\nFROM mcr.microsoft.com\/azureml\/base:intelmpi2018.3-ubuntu16.04\nRUN apt-get update &amp;&amp; apt-get install -y libgl1-mesa-glx\nRUN echo &quot;Hello from custom container!&quot;\n&quot;&quot;&quot;\nmyenv.docker.base_image = None\nmyenv.docker.base_dockerfile = dockerfile\n<\/code><\/pre>\n<p>Upon execution, this is totally ignored and libgl1 is not installed. Any ideas why?<\/p>\n<p>EDIT: Here's the rest of my code:<\/p>\n<pre><code>est = Estimator(\n    source_directory = '.',\n    script_params = script_params,\n    use_gpu = True,\n    compute_target = 'gpu-cluster-1',\n    pip_packages = ['scipy==1.1.0', 'torch==1.5.1'],\n    entry_script = 'AzureEntry.py',\n    )\n\nrun = exp.submit(config = est)\nrun.wait_for_completion(show_output=True)\n<\/code><\/pre>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-use-environments\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-use-environments<\/a><\/p>",
        "Question_answer_count":5,
        "Question_comment_count":2,
        "Question_creation_date":"2020-07-14 00:45:13.407000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-07-14 13:00:46.430000 UTC",
        "Question_score":4,
        "Question_tags":"python|azure|azure-machine-learning-service",
        "Question_view_count":1819,
        "Owner_creation_date":"2013-04-17 22:31:48.703000 UTC",
        "Owner_last_access_date":"2021-02-08 16:39:27.677000 UTC",
        "Owner_location":null,
        "Owner_reputation":170,
        "Owner_up_votes":5,
        "Owner_down_votes":1,
        "Owner_views":23,
        "Answer_body":"<p>This should work :<\/p>\n<pre><code>from azureml.core import Workspace\nfrom azureml.core.environment import Environment\nfrom azureml.train.estimator import Estimator\nfrom azureml.core.conda_dependencies import CondaDependencies\nfrom azureml.core import Experiment\n\nws = Workspace (...)\nexp = Experiment(ws, 'test-so-exp')\n\nmyenv = Environment(name = &quot;myenv&quot;)\nmyenv.docker.enabled = True\ndockerfile = r&quot;&quot;&quot;\nFROM mcr.microsoft.com\/azureml\/base:intelmpi2018.3-ubuntu16.04\nRUN apt-get update &amp;&amp; apt-get install -y libgl1-mesa-glx\nRUN echo &quot;Hello from custom container!&quot;\n&quot;&quot;&quot;\nmyenv.docker.base_image = None\nmyenv.docker.base_dockerfile = dockerfile\n\n## You need to instead put your packages in the Environment definition instead... \n## see below for some changes too\n\nmyenv.python.conda_dependencies = CondaDependencies.create(pip_packages = ['scipy==1.1.0', 'torch==1.5.1'])\n<\/code><\/pre>\n<p>Finally you can build your estimator a bit differently :<\/p>\n<pre><code>est = Estimator(\n    source_directory = '.',\n#     script_params = script_params,\n#     use_gpu = True,\n    compute_target = 'gpu-cluster-1',\n#     pip_packages = ['scipy==1.1.0', 'torch==1.5.1'],\n    entry_script = 'AzureEntry.py',\n    environment_definition=myenv\n    )\n<\/code><\/pre>\n<p>And submit it :<\/p>\n<pre><code>run = exp.submit(config = est)\nrun.wait_for_completion(show_output=True)\n<\/code><\/pre>\n<p>Let us know if that works.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2020-07-14 18:13:46.867000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":3.0,
        "Question_valid_tags":[
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":35682879,
        "Question_title":"What is the name of the driver to connect to Azure SQL Database from pyodbc in Azure ML?",
        "Question_body":"<p>I'm trying to create a '<strong>Reader<\/strong>' alternative to read data from Azure SQL Database using the 'Execute python script' module in <strong>Azure ML<\/strong>.\nwhile doing so, I'm trying to connect to Azure Sql using pyodbc library.\nhere's my code:<\/p>\n\n<pre><code>def azureml_main(dataframe1 = None, dataframe2 = None):\n    import pyodbc   \n    import pandas as pd\n\n    conn = pyodbc.connect('DRIVER={SQL Server}; SERVER=server.database.windows.net; DATABASE=db_name; UID=user; PWD=Password')\n    SQLCommand = ('''select * from table1 ''')\n    data_frame = pd.read_sql(SQLCommand, conn)\n    return data_frame,\n<\/code><\/pre>\n\n<p>also tried to use a different driver name: {SQL Server Native Client 11.0}<\/p>\n\n<p>Here is the error i'm getting:<\/p>\n\n<pre><code>Error: ('IM002', '[IM002] [Microsoft][ODBC Driver Manager] Data source name not found and no default driver specified (0) (SQLDriverConnect)')\n<\/code><\/pre>\n\n<p>Does anybody know which driver should I use?<\/p>\n\n<p>just to make sure, I tried  \"{SQL Server}\", \"{SQL Server Native Client 11.0}\" and \"{SQL Server Native Client 10.0}\" and got the same error<\/p>\n\n<p>I also tried a different format: <\/p>\n\n<pre><code>conn = pyodbc.connect('DRIVER={SQL Server}; SERVER=server.database.windows.net; DATABASE=db_name; user=user@server; password=Password')\n<\/code><\/pre>\n\n<p>and <\/p>\n\n<pre><code>conn = pyodbc.connect('DRIVER={SQL Server Native Client 11.0}; SERVER=server.database.windows.net; DATABASE=db_name; user=user@server; password=Password')\n<\/code><\/pre>",
        "Question_answer_count":5,
        "Question_comment_count":1,
        "Question_creation_date":"2016-02-28 13:04:21.510000 UTC",
        "Question_favorite_count":2.0,
        "Question_last_edit_date":"2016-03-01 06:14:10.913000 UTC",
        "Question_score":6,
        "Question_tags":"python|pyodbc|azure-sql-database|azure-machine-learning-studio|cortana-intelligence",
        "Question_view_count":2168,
        "Owner_creation_date":"2011-10-31 11:53:18.253000 UTC",
        "Owner_last_access_date":"2022-06-28 13:56:00.827000 UTC",
        "Owner_location":null,
        "Owner_reputation":778,
        "Owner_up_votes":176,
        "Owner_down_votes":6,
        "Owner_views":89,
        "Answer_body":"<p>I got an answer from azure support:<\/p>\n\n<blockquote>\n  <p>Currently it is not possible to access sql azure dbs from within  an\n  \u201cexecute python script\u201d module. As you suspected this is due to\n  missing odbc drivers in the execution environment.   Suggested\n  workarounds are to  a) use reader module   or   b) export to blobs\n  and use the Azure Python SDK for accessing those blobs\n  <a href=\"http:\/\/blogs.msdn.com\/b\/bigdatasupport\/archive\/2015\/10\/02\/using-azure-sdk-for-python.aspx\" rel=\"nofollow\">http:\/\/blogs.msdn.com\/b\/bigdatasupport\/archive\/2015\/10\/02\/using-azure-sdk-for-python.aspx<\/a><\/p>\n<\/blockquote>\n\n<p>So currently it it <strong>impossible<\/strong> to connect to SQL server from \u201cexecute python script\u201d module in Azure-ML. If you like to change it, please vote <a href=\"https:\/\/feedback.azure.com\/forums\/257792-machine-learning\/suggestions\/12589266-enable-odbc-connection-from-excute-python-script\" rel=\"nofollow\">here<\/a> <\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2016-03-03 07:37:05.533000 UTC",
        "Answer_last_edit_date":"2016-03-03 11:01:52.110000 UTC",
        "Answer_score":2.0,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":49438903,
        "Question_title":"How to make parameters available to SageMaker Tensorflow Endpoint",
        "Question_body":"<p>I'm looking to make some hyper parameters available to the serving endpoint in SageMaker. The training instances is given access to input parameters using hyperparameters in:<\/p>\n\n<pre><code>estimator = TensorFlow(entry_point='autocat.py',\n                       role=role,\n                       output_path=params['output_path'],\n                       code_location=params['code_location'],\n                       train_instance_count=1,\n                       train_instance_type='ml.c4.xlarge',\n                       training_steps=10000,\n                       evaluation_steps=None,\n                       hyperparameters=params)\n<\/code><\/pre>\n\n<p>However, when the endpoint is deployed, there is no way to pass in parameters that are used to control the data processing in the <code>input_fn(serialized_input, content_type)<\/code> function.<\/p>\n\n<p>What would be the best way to pass parameters to the serving instance?? Is the <code>source_dir<\/code> parameter defined in the <code>sagemaker.tensorflow.TensorFlow<\/code> class copied to the serving instance? If so, I could use a config.yml or similar.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_date":"2018-03-22 21:37:03.920000 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":"2018-03-22 21:42:13.880000 UTC",
        "Question_score":1,
        "Question_tags":"amazon-web-services|tensorflow|amazon-sagemaker",
        "Question_view_count":1426,
        "Owner_creation_date":"2015-09-25 17:16:18.360000 UTC",
        "Owner_last_access_date":"2022-05-26 17:59:10.970000 UTC",
        "Owner_location":null,
        "Owner_reputation":749,
        "Owner_up_votes":11,
        "Owner_down_votes":0,
        "Owner_views":49,
        "Answer_body":"<p>Ah i have had a similar problem to you where I needed to download something off S3 to use in the input_fn for inference. In my case it was a dictionary.<\/p>\n\n<p>Three options:<\/p>\n\n<ol>\n<li>use your config.yml approach, and download and import the s3 file from within your entrypoint file before any function declarations. This would make it available to the input_fn <\/li>\n<li>Keep using the hyperparameter approach, download and import the vectorizer in <code>serving_input_fn<\/code> and make it available via a global variable so that <code>input_fn<\/code> has access to it.<\/li>\n<li>Download the file from s3 before training and include it in the source_dir directly.<\/li>\n<\/ol>\n\n<p>Option 3 would only work if you didnt need to make changes to the vectorizer seperately after initial training.<\/p>\n\n<p>Whatever you do, don't download the file directly in input_fn. I made that mistake and the performance is terrible as each invoking of the endpoint would result in the s3 file being downloaded.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2018-04-13 03:56:54.357000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":62614143,
        "Question_title":"AWS SageMaker: Create an endpoint using a trained model hosted in S3",
        "Question_body":"<p>I have following this <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/semantic_segmentation_pascalvoc\/semantic_segmentation_pascalvoc.ipynb\" rel=\"nofollow noreferrer\">tutorial<\/a>, which is mainly for jupyter notebook, and made some minimal modification for external processing. I've created a project that could prepare my dataset locally, upload it to S3, train, and finally deploy the model predictor to the same bucket. Perfect!<\/p>\n<p>So, after to train and saved it in S3 bucket:<\/p>\n<pre><code> ss_model.fit(inputs=data_channels, logs=True)\n<\/code><\/pre>\n<p>it failed while deploying as an endpoint. So, I have found tricks to host an endpoint in many ways, but not from a model already saved in S3. Because in order to host, you probably need to get the estimator, which in normal way is something like:<\/p>\n<pre><code> self.estimator = sagemaker.estimator.Estimator(self.training_image,\n                                                role,\n                                                train_instance_count=1,\n                                                train_instance_type='ml.p3.2xlarge',\n                                                train_volume_size=50,\n                                                train_max_run=360000,\n                                                output_path=output,\n                                                base_job_name='ss-training',\n                                                sagemaker_session=sess)\n<\/code><\/pre>\n<p>My question is: is there a way to load an estimator from a model saved in S3 (.tar)? Or, anyway, to create an endpoint without train it again?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_date":"2020-06-27 18:42:34.330000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"python|deep-learning|amazon-sagemaker|semantic-segmentation",
        "Question_view_count":1360,
        "Owner_creation_date":"2015-04-17 11:36:43.900000 UTC",
        "Owner_last_access_date":"2022-09-24 14:52:52.453000 UTC",
        "Owner_location":"S\u00e3o Jos\u00e9 dos Campos, Sao Jose dos Campos - State of S\u00e3o Paulo, Brazil",
        "Owner_reputation":138,
        "Owner_up_votes":40,
        "Owner_down_votes":0,
        "Owner_views":38,
        "Answer_body":"<p>So, after to run on many pages, just found a clue <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/blazingtext_hosting_pretrained_fasttext\/blazingtext_hosting_pretrained_fasttext.ipynb\" rel=\"nofollow noreferrer\">here<\/a>. And I finally found out how to load the model and create the endpoint:<\/p>\n<pre><code>def create_endpoint(self):\n    sess = sagemaker.Session()\n    training_image = get_image_uri(sess.boto_region_name, 'semantic-segmentation', repo_version=&quot;latest&quot;)        \n    role = &quot;YOUR_ROLE_ARN_WITH_SAGEMAKER_EXECUTION&quot;\n    model = &quot;s3:\/\/BUCKET\/PREFIX\/...\/output\/model.tar.gz&quot;\n\n    sm_model = sagemaker.Model(model_data=model, image=training_image, role=role, sagemaker_session=sess)\n    sm_model.deploy(initial_instance_count=1, instance_type='ml.p3.2xlarge')\n<\/code><\/pre>\n<p><strong>Please, do not forget to disable your endpoint after using. This is really important! Endpoints are charged by &quot;running&quot; not only by the use<\/strong><\/p>\n<p>I hope it also can help you out!<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-06-29 20:41:20.863000 UTC",
        "Answer_last_edit_date":"2020-07-27 13:14:15.040000 UTC",
        "Answer_score":3.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":59773167,
        "Question_title":"When will experiment be deleted with lifecycle_stage is set as deleted",
        "Question_body":"<p>I can see experiment 2 is in deleted, but when it will be deleted actually?<\/p>\n\n<pre><code>2   test    hdfs:\/\/\/1234\/mlflow deleted\n<\/code><\/pre>\n\n<p>If the experiment is not deleted automatically, how can I delete it?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-01-16 15:41:36.547000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"mlflow",
        "Question_view_count":412,
        "Owner_creation_date":"2014-08-18 14:07:01.673000 UTC",
        "Owner_last_access_date":"2022-09-15 04:40:38.707000 UTC",
        "Owner_location":"Berlin, Germany",
        "Owner_reputation":2521,
        "Owner_up_votes":447,
        "Owner_down_votes":13,
        "Owner_views":197,
        "Answer_body":"<p>I am assuming you use sql store?<\/p>\n\n<p>Currently there is no way to tell mlflow to hard-delete experiments. We are working with open source contributors to add a cli command that would perform garbage-collection of deleted experiments. This should be added soon in one of the upcoming mlflow releases. In the meantime, you can connect to your sql store and delete the experiments manually.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-01-22 23:47:09.917000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "mlflow"
        ]
    },
    {
        "Question_id":42145256,
        "Question_title":"Share dataset between two azureml environnement",
        "Question_body":"<p>a friend have sent me a python3 notebook with his dataset to validate his notebook.<\/p>\n\n<p>but when i try to use his dataset on my azureml workspace i have an error saying that the dataset does not exist<\/p>\n\n<p>he sent me his datset code :<\/p>\n\n<pre><code>from azureml import Workspace\n\nws = Workspace(\n    workspace_id='toto',\n    authorization_token='titi',\n    endpoint='https:\/\/studioapi.azureml.net'\n)\nds = ws.datasets['mini.csv00']\nframe = ds.to_dataframe()\n\nframe\n<\/code><\/pre>\n\n<p>when i try to use it i have a :<\/p>\n\n<pre><code>ndexError                                Traceback (most recent call last)\n&lt;ipython-input-7-5f41120e38e4&gt; in &lt;module&gt;()\n----&gt; 1 ds = ws.datasets['mini.csv00']\n      2 frame = ds.to_dataframe()\n      3 \n      4 frame\n\n\/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages\/azureml\/__init__.py in __getitem__(self, index)\n    461                     return self._create_dataset(dataset)\n    462 \n--&gt; 463         raise IndexError('A data set named \"{}\" does not exist'.format(index))\n    464 \n    465     def add_from_dataframe(self, dataframe, data_type_id, name, description):\n\nIndexError: A data set named \"mini.csv00\" does not exist\n<\/code><\/pre>\n\n<p>error ...<\/p>\n\n<p>But when i try it on my computer jupyter it works.\nAny ideas ?<\/p>\n\n<p>Thanks and regards<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2017-02-09 19:28:32.167000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|dataset|jupyter|azure-machine-learning-studio",
        "Question_view_count":108,
        "Owner_creation_date":"2010-08-05 09:07:28.530000 UTC",
        "Owner_last_access_date":"2022-09-11 14:03:07.340000 UTC",
        "Owner_location":null,
        "Owner_reputation":1048,
        "Owner_up_votes":199,
        "Owner_down_votes":1,
        "Owner_views":602,
        "Answer_body":"<p>I guess you are using Jupyter notebook on AzureML to do the experiment. In that case the <code>'mini.csv00'<\/code> should be in your experiments with <code>workspace_id='toto'<\/code>. <\/p>\n\n<p>Create a new experiment in your workspace named toto and put the dataset into it first. Then open the dataset using 'open in a new Notebook'. <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/ztIw0.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ztIw0.png\" alt=\"enter image description here\"><\/a> <\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2017-02-12 04:53:12.387000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":59069043,
        "Question_title":"LibraryExecutionError - testing a web service published via Rstudio RRS feed",
        "Question_body":"<p>I built a model in Rstudio, published in Azure ML Studio a web service using \"AzureML\" R package. When testing the web service on Azure ML Studio, I encountered an error:<\/p>\n\n<pre><code>Error: AzureML returns error code:\nHTTP status code : 400\nAzureML error code : LibraryExecutionError\nModule execution encountered an internal library error.\nThe following error occurred during evaluation of R script: R_tryEval: return error: Error: bad restore file magic number (file may be corrupted) -- no data loaded\n<\/code><\/pre>\n\n<p>Do you have any <strong>insights<\/strong> on how to solve such issue? Am I <strong>missing some important code in the R script<\/strong>?<\/p>\n\n<p>The model I used was a RandomForest to predicted Species on Iris dataset<\/p>\n\n<pre><code># Iris dataset\ndf = iris\nset.seed(100);\n\nindex = createDataPartition(df$Species, p = 0.7, list = FALSE)\nML.train = df[index,]; \nML.test = df[-index,];  rm(index)\n\nlibrary(randomForest)\nmodel = randomForest::randomForest(Species ~., data = ML.train)\n\nmypredict = function(newdata) {\n      require(randomForest)\n      predict(model, newdata, type = \"response\")\n}\n\n# Create workspace\nwsObj = AzureML::workspace(id = \"my Id\", auth = \"my token\")  # I omitted on purpose my Id and my token values\n\n# Publishing\nlibrary(devtools)\nlibrary(AzureML)\napi = AzureML::publishWebService(ws = wsObj,\n                                 fun = mypredict,\n                                 name = \"IrisWebService\",\n                                 inputSchema = ML.test %&gt;% select(-Species) )\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-11-27 11:20:11.397000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"r|web-services|azure-machine-learning-studio",
        "Question_view_count":54,
        "Owner_creation_date":"2019-11-27 11:11:38.927000 UTC",
        "Owner_last_access_date":"2020-10-05 07:36:02.550000 UTC",
        "Owner_location":"Dublin, Ireland",
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Answer_body":"<p>AzureML for RStudio is now not supported as the package is removed from CRAN repository from <a href=\"https:\/\/cran.r-project.org\/web\/packages\/AzureML\/index.html\" rel=\"nofollow noreferrer\">2019-07-29<\/a>.\nAzure ML studio with this package will not work as the package(AzureML) is removed.<\/p>\n\n<p>Azure Machine Learning SDK for R can be Downloaded from CRAN at <a href=\"https:\/\/cloud.r-project.org\/web\/packages\/azuremlsdk\/index.html\" rel=\"nofollow noreferrer\">https:\/\/cloud.r-project.org\/web\/packages\/azuremlsdk\/index.html<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2019-12-02 06:42:04.193000 UTC",
        "Answer_last_edit_date":"2019-12-02 08:06:50.873000 UTC",
        "Answer_score":0.0,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":51817494,
        "Question_title":"deploy a simple function to amazon sagemaker",
        "Question_body":"<p>I have been playing with Amazon Sagemaker. They have amazing sample notebooks in different areas. However, for testing purposes, I want to create an endpoint that returns the result from a function. From what I have seen so far, my understanding is that we can deploy only models but I would like to clarify it.<\/p>\n\n<p>Let's say I want to invoke the endpoint and it should give me the square of the input value. So, I will first create a function:<\/p>\n\n<pre><code>def my_square(x):\n    return x**2\n<\/code><\/pre>\n\n<p>Can we deploy this simple function in Amazon Sagemaker?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2018-08-13 07:45:57.657000 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":"2018-08-15 21:26:19.667000 UTC",
        "Question_score":3,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":216,
        "Owner_creation_date":"2015-04-16 01:27:21.930000 UTC",
        "Owner_last_access_date":"2022-09-21 00:22:16.033000 UTC",
        "Owner_location":null,
        "Owner_reputation":2282,
        "Owner_up_votes":123,
        "Owner_down_votes":10,
        "Owner_views":264,
        "Answer_body":"<p>Yes this is possible but it will need some overhead:\nYou can pass your own docker images for training and inference to sagemaker.<\/p>\n\n<p>Inside this containers you can do anything you want including return your <code>my_square<\/code> function. Keep in mind that you have to write your own flask microservice including proxy and wsgi server(if needed).<\/p>\n\n<p>In my opinion <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/scikit_bring_your_own\/scikit_bring_your_own.ipynb\" rel=\"nofollow noreferrer\">this example<\/a> is the most helpfull one.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2018-08-13 09:39:42.207000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":3.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":68465990,
        "Question_title":"How do I access AIP_STORAGE_URI in Vertex AI?",
        "Question_body":"<p>I uploaded a model with<\/p>\n<pre><code>gcloud beta ai models upload --artifact-uri\n<\/code><\/pre>\n<p>And in the docker I access <code>AIP_STORAGE_URI<\/code>.\nI see that <code>AIP_STORAGE_URI<\/code> is another Google Storage location so I try to download the files using <code>storage.Client()<\/code> but then it says that I don't have access:<\/p>\n<pre><code>google.api_core.exceptions.Forbidden: 403 GET https:\/\/storage.googleapis.com\/storage\/v1\/b\/caip-tenant-***-***-*-*-***?projection=noAcl&amp;prettyPrint=false: custom-online-prediction@**.iam.gserviceaccount.com does not have storage.buckets.get access to the Google Cloud Storage bucket\n<\/code><\/pre>\n<p>I am running this endpoint with the default service account.<\/p>\n<p><a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/custom-container-requirements#artifacts\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/custom-container-requirements#artifacts<\/a><\/p>\n<p>According to the above link:\n<code>The service account that your container uses by default has permission to read from this URI. <\/code><\/p>\n<p>What am I doing wrong?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-07-21 08:03:17.743000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-07-21 09:06:10.043000 UTC",
        "Question_score":3,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":742,
        "Owner_creation_date":"2016-06-26 21:49:44.157000 UTC",
        "Owner_last_access_date":"2022-09-24 07:38:31.393000 UTC",
        "Owner_location":null,
        "Owner_reputation":117,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":17,
        "Answer_body":"<p>The reason behind the error being, the default service account that Vertex AI uses has the \u201c<a href=\"https:\/\/cloud.google.com\/storage\/docs\/access-control\/iam-roles#standard-roles\" rel=\"nofollow noreferrer\">Storage Object Viewer<\/a>\u201d role which excludes the <code>storage.buckets.get<\/code> permission. At the same time, the <code>storage.Client()<\/code> part of the code makes a <code>storage.buckets.get<\/code> request to the Vertex AI managed bucket for which the default service account does not have permission to.<\/p>\n<p>To resolve the issue, I would suggest you to follow the below steps -<\/p>\n<ol>\n<li><p>Make changes in the custom code to access the bucket with the model artifacts in your project instead of using the environment variable <code>AIP_STORAGE_URI<\/code> which points to the model location in the Vertex AI managed bucket.<\/p>\n<\/li>\n<li><p>Create your own service account and grant the service account with all the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/general\/custom-service-account\" rel=\"nofollow noreferrer\">permissions<\/a> needed by the custom code. For this specific error, a role with the <code>storage.buckets.get<\/code> permission, eg. <a href=\"https:\/\/cloud.google.com\/storage\/docs\/access-control\/iam-roles#standard-roles\" rel=\"nofollow noreferrer\">Storage Admin<\/a> (&quot;roles\/storage.admin&quot;) has to be granted to the service account.<\/p>\n<\/li>\n<li><p>Provide the newly created service account in the &quot;Service Account&quot; field when deploying the model.<\/p>\n<\/li>\n<\/ol>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-07-30 17:29:41.500000 UTC",
        "Answer_last_edit_date":"2022-08-25 23:59:47.217000 UTC",
        "Answer_score":2.0,
        "Question_valid_tags":[
            "google-cloud-vertex-ai"
        ]
    },
    {
        "Question_id":59679192,
        "Question_title":"Hosting multiple models for multiple datasets in aws sagemaker",
        "Question_body":"<p>I read that there is a way to train and host multiple models using a single endpoint for a single dataset in AWS Sagemaker. But I have 2 different datasets in S3 and have to train a model for each dataset. Can these 2 different models be hosted using a single endpoint? <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-01-10 09:49:21.440000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":411,
        "Owner_creation_date":"2019-02-21 13:41:11.933000 UTC",
        "Owner_last_access_date":"2022-09-23 13:21:28.837000 UTC",
        "Owner_location":null,
        "Owner_reputation":67,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Answer_body":"<p>Yes, this is called a multi-model endpoint. You can use a large number of models on the same endpoint. They get loaded and unloaded dynamically as needed, and you simply have to pass the model name in your prediction request.<\/p>\n\n<p>Here are some resources:<\/p>\n\n<ul>\n<li><p>Blog post + example : <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/save-on-inference-costs-by-using-amazon-sagemaker-multi-model-endpoints\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/save-on-inference-costs-by-using-amazon-sagemaker-multi-model-endpoints\/<\/a><\/p><\/li>\n<li><p>Video explaining model deployment scenarios on SageMaker: <a href=\"https:\/\/youtu.be\/dT8jmdF-ZWw\" rel=\"nofollow noreferrer\">https:\/\/youtu.be\/dT8jmdF-ZWw<\/a><\/p><\/li>\n<\/ul>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-01-11 15:08:53.997000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":71708147,
        "Question_title":"MLFlow tracking ui not showing experiments on local machine (laptop)",
        "Question_body":"<p>I am a beginner in mlflow and was trying to set it up locally using Anaconda 3.\nI have created a new environment in anaconda and install mlflow and sklearn in it. Now I am using jupyter notebook to run my sample code for mlflow.<\/p>\n<p>'''<\/p>\n<pre><code>import os\nimport warnings\nimport sys\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import ElasticNet\nfrom urllib.parse import urlparse\nimport mlflow\nimport mlflow.sklearn\n\nimport logging\n\nlogging.basicConfig(level=logging.WARN)\nlogger = logging.getLogger(__name__)\n\nwarnings.filterwarnings(&quot;ignore&quot;)\nnp.random.seed(40)\n\n\nmlflow.set_tracking_uri(&quot;file:\/\/\/Users\/Swapnil\/Documents\/LocalPython\/MLFLowDemo\/mlrun&quot;)\n\nmlflow.get_tracking_uri()\n\nmlflow.get_experiment\n\n#experiment_id = mlflow.create_experiment(&quot;Mlflow_demo&quot;)\nexperiment_id = mlflow.create_experiment(&quot;Demo3&quot;)\nexperiment = mlflow.get_experiment(experiment_id)\nprint(&quot;Name: {}&quot;.format(experiment.name))\nprint(&quot;Experiment_id: {}&quot;.format(experiment.experiment_id))\nprint(&quot;Artifact Location: {}&quot;.format(experiment.artifact_location))\nprint(&quot;Tags: {}&quot;.format(experiment.tags))\nprint(&quot;Lifecycle_stage: {}&quot;.format(experiment.lifecycle_stage))\n\nmlflow.set_experiment(&quot;Demo3&quot;)\n\ndef eval_metrics(actual, pred):\n    rmse = np.sqrt(mean_squared_error(actual, pred))\n    mae = mean_absolute_error(actual, pred)\n    r2 = r2_score(actual, pred)\n    return rmse, mae, r2\n\n# Read the wine-quality csv file from the URL\ncsv_url =\\\n    'http:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/wine-quality\/winequality-red.csv'\ntry:\n    data = pd.read_csv(csv_url, sep=';')\nexcept Exception as e:\n    logger.exception(\n        &quot;Unable to download training &amp; test CSV, check your internet connection. Error: %s&quot;, e)\n\ndata.head(2)\n\n\ndef train_model(data, alpha, l1_ratio):\n    \n    # Split the data into training and test sets. (0.75, 0.25) split.\n    train, test = train_test_split(data)\n\n    # The predicted column is &quot;quality&quot; which is a scalar from [3, 9]\n    train_x = train.drop([&quot;quality&quot;], axis=1)\n    test_x = test.drop([&quot;quality&quot;], axis=1)\n    train_y = train[[&quot;quality&quot;]]\n    test_y = test[[&quot;quality&quot;]]\n\n    # Set default values if no alpha is provided\n    alpha = alpha\n    l1_ratio = l1_ratio\n\n\n    # Execute ElasticNet\n    lr = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=42)\n    lr.fit(train_x, train_y)\n\n    # Evaluate Metrics\n    predicted_qualities = lr.predict(test_x)\n    (rmse, mae, r2) = eval_metrics(test_y, predicted_qualities)\n\n    # Print out metrics\n    print(&quot;Elasticnet model (alpha=%f, l1_ratio=%f):&quot; % (alpha, l1_ratio))\n    print(&quot;  RMSE: %s&quot; % rmse)\n    print(&quot;  MAE: %s&quot; % mae)\n    print(&quot;  R2: %s&quot; % r2)\n    \n    # Log parameter, metrics, and model to MLflow\n    with mlflow.start_run(experiment_id = experiment_id):\n        mlflow.log_param(&quot;alpha&quot;, alpha)\n        mlflow.log_param(&quot;l1_ratio&quot;, l1_ratio)\n        mlflow.log_metric(&quot;rmse&quot;, rmse)\n        mlflow.log_metric(&quot;r2&quot;, r2)\n        mlflow.log_metric(&quot;mae&quot;, mae)\n        mlflow.sklearn.log_model(lr, &quot;model&quot;)\n        \n\ntrain_model(data, 0.5, 0.5)\n\ntrain_model(data, 0.5, 0.3)\n\ntrain_model(data, 0.4, 0.3)\n<\/code><\/pre>\n<p>'''<\/p>\n<p>using above code, I am successfully able to create 3 different experiment as I can see the folders created in my local directory as shown below:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/jKqgX.png\" rel=\"nofollow noreferrer\">enter image description here<\/a><\/p>\n<p>Now, I am trying to run the mlflow ui using the jupyter terminal in my chrome browser and I am able to open the mlflow ui but cannot see and experiments as shown below:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/6KaQK.png\" rel=\"nofollow noreferrer\">enter image description here<\/a><\/p>\n<p>Could you help me in finding where I am going wrong?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2022-04-01 14:01:52.310000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":3,
        "Question_tags":"python|windows|mlflow|mlops",
        "Question_view_count":936,
        "Owner_creation_date":"2022-04-01 13:44:18.173000 UTC",
        "Owner_last_access_date":"2022-09-23 11:29:25.810000 UTC",
        "Owner_location":null,
        "Owner_reputation":33,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Answer_body":"<p>Where do you run <code>mlflow ui<\/code> command?<\/p>\n<p>I think if you pass tracking ui path in the arguments, it would work:<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>mlflow ui --backend-store-uri file:\/\/\/Users\/Swapnil\/Documents\/LocalPython\/MLFLowDemo\/mlrun\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-04-24 00:46:51.590000 UTC",
        "Answer_last_edit_date":"2022-08-08 21:04:12.860000 UTC",
        "Answer_score":3.0,
        "Question_valid_tags":[
            "mlflow"
        ]
    },
    {
        "Question_id":59392060,
        "Question_title":"Azure ML platform wrongly interprets uploaded dataset",
        "Question_body":"<p>I have a data set with about 7999 attributes and 39 labels, with 3339 total observations (resulting in 3339x8038 data set), and I'm trying to upload id to Azure ML platform.\nI've selected the 'type' as 'tabular', encoding as 'utf-8', no row skipping, and use header from first file.\nThe problem is, that the headers are still not included and the data is interpreted as string with 0s, 1s, and commas (see pic <a href=\"https:\/\/imgur.com\/a\/QdQNt1y\" rel=\"nofollow noreferrer\">https:\/\/imgur.com\/a\/QdQNt1y<\/a>)<\/p>\n\n<p>Am I missing something? For smaller data sets it seemed to work. My headers are A1, ... A7999 for the attributes, and L1, ... L39 for the labels.<\/p>\n\n<p>Thanks for help in advance.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2019-12-18 12:28:28.560000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"azure|azure-machine-learning-service",
        "Question_view_count":49,
        "Owner_creation_date":"2018-05-23 16:05:07.810000 UTC",
        "Owner_last_access_date":"2022-09-22 18:43:42.817000 UTC",
        "Owner_location":"Stockholm, Sweden",
        "Owner_reputation":235,
        "Owner_up_votes":9,
        "Owner_down_votes":1,
        "Owner_views":43,
        "Answer_body":"<p>our system does our best guess over file settings when you try to create a dataset, but cannot guarantee perfect guesses in all cases. <\/p>\n\n<p>In such scenarios, you should be able to adjust the settings. We had a bug with the ability to change those settings, but rolled out a fix. Can you try to change those now?  <\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2019-12-20 03:50:12.910000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":55822637,
        "Question_title":"Is there a way to get log the descriptive stats of a dataset using MLflow?",
        "Question_body":"<p>Is there a way to get log the descriptive stats of a dataset using MLflow? If any could you please share the details?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2019-04-24 04:52:09.530000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":3,
        "Question_tags":"python|mlflow",
        "Question_view_count":4592,
        "Owner_creation_date":"2014-09-22 04:46:57.027000 UTC",
        "Owner_last_access_date":"2022-09-03 08:12:58.187000 UTC",
        "Owner_location":"Bengaluru, Karnataka, India",
        "Owner_reputation":569,
        "Owner_up_votes":41,
        "Owner_down_votes":3,
        "Owner_views":123,
        "Answer_body":"<p>Generally speaking you can log arbitrary output from your code using the mlflow_log_artifact() function.  From <a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.log_artifact\" rel=\"noreferrer\">the docs<\/a>:<\/p>\n<blockquote>\n<p><strong>mlflow.log_artifact(local_path, artifact_path=None)<\/strong>\nLog a local file or directory as an artifact of the currently active run.<\/p>\n<\/blockquote>\n<blockquote>\n<p><strong>Parameters:<\/strong><br \/>\n<em>local_path<\/em> \u2013 Path to the file to write.\n<em>artifact_path<\/em> \u2013 If provided, the directory in artifact_uri to write to.<\/p>\n<\/blockquote>\n<p>As an example, say you have your statistics in a pandas dataframe, <code>stat_df<\/code>.<\/p>\n<pre><code>## Write csv from stats dataframe\nstat_df.to_csv('dataset_statistics.csv')\n\n## Log CSV to MLflow\nmlflow.log_artifact('dataset_statistics.csv')\n<\/code><\/pre>\n<p>This will show up under the artifacts section of this MLflow run in the Tracking UI.  If you explore the docs further you'll see that you can also log an entire directory and the objects therein.  In general, MLflow provides you a lot of flexibility - anything you write to your file system you can track with MLflow.  Of course that doesn't mean you should. :)<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2019-05-08 01:32:42.457000 UTC",
        "Answer_last_edit_date":"2021-01-26 04:34:50.617000 UTC",
        "Answer_score":9.0,
        "Question_valid_tags":[
            "mlflow"
        ]
    },
    {
        "Question_id":48539564,
        "Question_title":"Tensorflow - Checkpoints not saving to Sagemaker Notebook Instance",
        "Question_body":"<p>I am running a Python script with Tensorflow in Amazon Sagemaker notebook instance.  I have no trouble writing to the storage in the notebook normally, but for some reason I am unsuccessful when trying to save Tensorflow model checkpoints.  This code previously worked before it was ported to Sagemaker.<\/p>\n\n<p>Below is a reduced version of my code:<\/p>\n\n<pre><code>bucket = 'sagemaker-complaints-data'    \nprefix = 'DeepTestV2' # place to upload training files within the bucket\ntimestamp = str(int(time()))\nout_dir = os.path.abspath(os.path.join(bucket, prefix, \"runs\", timestamp))\ncheckpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"model\")\npath = saver.save(sess, checkpoint_prefix, global_step=current_step)\nprint(\"Saved model checkpoint to {}\\n\".format(path))\n<\/code><\/pre>\n\n<p>No errors are being thrown and the print statement is outputting the correct path.  I have researched whether there are any known issues with using checkpoints in Sagemaker but have come across literally no posts describing this.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2018-01-31 10:13:02.673000 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"python|python-3.x|amazon-web-services|tensorflow|amazon-sagemaker",
        "Question_view_count":690,
        "Owner_creation_date":"2011-11-21 16:37:22.023000 UTC",
        "Owner_last_access_date":"2022-05-11 09:53:59.693000 UTC",
        "Owner_location":null,
        "Owner_reputation":1611,
        "Owner_up_votes":51,
        "Owner_down_votes":6,
        "Owner_views":189,
        "Answer_body":"<p>I have found out where this is - for some reason \"checkpoints\" seems to be a reserved word - changing the word to \"checks\" allowed me to write the folder.  Hope this helps someone!<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2018-02-15 17:26:35.183000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":73415182,
        "Question_title":"When do I use a glue job or a Sagemaker Processing job for an etl?",
        "Question_body":"<p>I am currently struggling to decide on what situations in which a glue job is preferable over a sagemaker processing job and vice versa? Some advice on this topic would be greatly appreciated.<\/p>\n<p>I can do the same on both, so why should I bother with the difference?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-08-19 10:13:08.283000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"amazon-web-services|aws-glue|amazon-sagemaker",
        "Question_view_count":34,
        "Owner_creation_date":"2022-02-16 03:15:56.940000 UTC",
        "Owner_last_access_date":"2022-09-23 15:06:17.550000 UTC",
        "Owner_location":null,
        "Owner_reputation":53,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":"<ul>\n<li>if you want to use a specific EC2 instance, use SageMaker<\/li>\n<li>Pricing: SageMaker is pro-rated per-second while Glue has minimum charge amount (1min or 10min depending on versions). You should measure how much would a workload cost you on each platform<\/li>\n<li>customization: in SageMaker Processing you can customize the execution environment, as you provide a Docker image (you could run more than Spark\/Python, such as C++ or R)<\/li>\n<\/ul>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-08-19 23:22:51.720000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":67393339,
        "Question_title":"dvc push, change the names of files on the remote storage",
        "Question_body":"<p>I'm working on a project with DVC (Data Version Control), when I push files in my remote storage, the name of the files are changed. How I can conserve the names?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-05-04 23:20:53.717000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-05-04 23:30:22.370000 UTC",
        "Question_score":2,
        "Question_tags":"dvc",
        "Question_view_count":447,
        "Owner_creation_date":"2013-09-20 14:02:00.450000 UTC",
        "Owner_last_access_date":"2022-09-21 22:02:08.603000 UTC",
        "Owner_location":null,
        "Owner_reputation":43,
        "Owner_up_votes":13,
        "Owner_down_votes":0,
        "Owner_views":13,
        "Answer_body":"<p>Short answer: there is no way to do that.<\/p>\n<p>Long answer:\nDvc remote is a content-based storage, so names are not preserved. Dvc creates metafiles (*.dvc files) in your workspace that contain names and those files are usually tracked by git, so you need to use git remote and dvc remote together to have both filenames and their contents. Here is a more detailed explanation about the format of local and remote storage <a href=\"https:\/\/dvc.org\/doc\/user-guide\/project-structure\/internal-files#structure-of-the-cache-directory\" rel=\"noreferrer\">https:\/\/dvc.org\/doc\/user-guide\/project-structure\/internal-files#structure-of-the-cache-directory<\/a> . Also, checkout <a href=\"https:\/\/dvc.org\/doc\/use-cases\/sharing-data-and-model-files\" rel=\"noreferrer\">https:\/\/dvc.org\/doc\/use-cases\/sharing-data-and-model-files<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-05-04 23:40:51.067000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":5.0,
        "Question_valid_tags":[
            "dvc"
        ]
    },
    {
        "Question_id":41136102,
        "Question_title":"Perform batch execution without using BLOB",
        "Question_body":"<p>Can I perform batch execution against Azure ML web service without using BLOB?<\/p>\n\n<p>I'm reading the data from SQL database into a CSV file stream. Can I stream this directly to the service without actually saving in and reading back the result from the BLOB?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2016-12-14 06:24:46.017000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"azure-machine-learning-studio",
        "Question_view_count":180,
        "Owner_creation_date":"2013-08-20 11:57:52.723000 UTC",
        "Owner_last_access_date":"2022-09-06 11:40:53.407000 UTC",
        "Owner_location":"Malaysia",
        "Owner_reputation":998,
        "Owner_up_votes":162,
        "Owner_down_votes":6,
        "Owner_views":136,
        "Answer_body":"<p>BES doesn't work with streams. The data needs to be stored somewhere for AML to access the data. Take a look at: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/machine-learning-consume-web-services#batch-execution-service-bes\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/machine-learning-consume-web-services#batch-execution-service-bes<\/a> for more information.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2016-12-14 09:40:21.107000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":50281188,
        "Question_title":"Sagemaker Java client generate IOrecord",
        "Question_body":"<p>I am trying to build a training set for Sagemaker using the Linear Learner algorithm. This algorithm supports recordIO wrapped protobuf and csv as format for the training data. As the training data is generated using spark I am having issues to generate a csv file from a dataframe (this seem broken for now), so I am trying to use protobuf. <\/p>\n\n<p>I managed to create a binary file for the training dataset using Protostuff which is a library that allows to generate protobuf messages from POJO objects. The problem is when triggering the training job I receive that message from SageMaker:\nClientError: No training data processed. Either the training channel is empty or the mini-batch size is too high. Verify that training data contains non-empty files and the mini-batch size is less than the number of records per training host.<\/p>\n\n<p>The training file is certainly not null. I suspect the way I generate the training data to be incorrect as I am able to train models using the libsvm format. Is there a way to generate IOrecord using the Sagemaker java client ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2018-05-10 20:39:10.483000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":222,
        "Owner_creation_date":"2015-04-08 00:54:56.053000 UTC",
        "Owner_last_access_date":"2019-10-03 00:46:15.877000 UTC",
        "Owner_location":null,
        "Owner_reputation":35,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":10,
        "Answer_body":"<p>Answering my own question. It was an issue in the algorithm configuration. I reduced mini batch size and it worked fine.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2018-05-18 14:17:53.370000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":39150834,
        "Question_title":"probability on azure recommendations api",
        "Question_body":"<p>I am using the azure recommendation api on <a href=\"http:\/\/recommendations.azurewebsites.net\/\" rel=\"nofollow noreferrer\">http:\/\/recommendations.azurewebsites.net\/<\/a>.\nI prepared the catalog to be like <code>&lt;Item Id&gt;<\/code>, <code>&lt;Item Name&gt;<\/code>, <code>&lt;Item Category&gt;<\/code>, <code>&lt;Features list&gt;<\/code> and the usage file : <code>&lt;userId&gt;<\/code>, <code>&lt;ItemId&gt;<\/code>.\nNow when I test the recommender, I always get a probability of 0.5 for all items, so I had to presume something is not right.\nIn order to know what's the problem I added two items to the catalog \none with same features as an other item but with different name and id,\nand an other item with different id and one different feature.\nI still get the 0.5 probability and now i'm sure something is not right but I still can figure out what the problem.<\/p>\n\n<p>here is a screenshot of what I get when I add the item to the cart<\/p>\n\n<p><img src=\"https:\/\/i.stack.imgur.com\/zhwiq.png\" alt=\"\"><\/p>\n\n<p>Is there any possibility to use the azure ml matchbox recommender with features and without ratings? <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2016-08-25 16:54:45.380000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2017-04-02 12:27:40.303000 UTC",
        "Question_score":0,
        "Question_tags":"recommendation-engine|azure-machine-learning-studio",
        "Question_view_count":98,
        "Owner_creation_date":"2015-11-11 15:15:48.300000 UTC",
        "Owner_last_access_date":"2022-09-21 14:00:23.953000 UTC",
        "Owner_location":"Tunis, Gouvernorat de Tunis, Tunisie",
        "Owner_reputation":31,
        "Owner_up_votes":61,
        "Owner_down_votes":0,
        "Owner_views":19,
        "Answer_body":"<p>Tayehi, <\/p>\n\n<p>Nice to meet you. I am the program manager in charge of the recommendations API.\n2 things:<\/p>\n\n<ol>\n<li><p>If you get a 0.5 probability you are most likely getting \"default recommendations\". This usually means that you do not have enough training data or that there are not enough co-occurrences for the item you are testing in the data. To describe the extreme case, imagine an item A that only gets purchased with an item B only one or two times -- it would be hard to say with confidence (statistical significance) that someone that likes item A is also likely to like item B.<\/p><\/li>\n<li><p>It looks like you are still using the old recommendations API. I would like to encourage you to use our newer version (the Recommendations API cognitive service). Please take a look at <a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/cognitive-services-migration-from-dm\/to\" rel=\"nofollow\">https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/cognitive-services-migration-from-dm\/to<\/a> help you in this process.<\/p><\/li>\n<\/ol>\n\n<p>Thanks!\nLuis Cabrera\nCortana Intelligence Applications.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2016-10-13 15:40:08.683000 UTC",
        "Answer_last_edit_date":"2016-10-14 20:13:37.407000 UTC",
        "Answer_score":0.0,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":69024005,
        "Question_title":"How to use SageMaker Estimator for model training and saving",
        "Question_body":"<p>The documentations of how to use SageMaker estimators are scattered around, sometimes obsolete, incorrect. Is there a one stop location which gives the comprehensive views of how to use SageMaker SDK Estimator to train and save models?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-09-02 04:01:47.170000 UTC",
        "Question_favorite_count":14.0,
        "Question_last_edit_date":null,
        "Question_score":27,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":6655,
        "Owner_creation_date":"2014-11-22 09:22:35.470000 UTC",
        "Owner_last_access_date":"2022-09-24 22:13:03.237000 UTC",
        "Owner_location":null,
        "Owner_reputation":14749,
        "Owner_up_votes":641,
        "Owner_down_votes":62,
        "Owner_views":968,
        "Answer_body":"<h1>Answer<\/h1>\n<p>There is no one such resource from AWS that provides the comprehensive view of how to use SageMaker SDK Estimator to train and save models.<\/p>\n<h2>Alternative Overview Diagram<\/h2>\n<p>I put a diagram and brief explanation to get the overview on how SageMaker Estimator runs a training.<\/p>\n<ol>\n<li><p>SageMaker sets up a docker container for a training job where:<\/p>\n<ul>\n<li>Environment variables are set as in <a href=\"https:\/\/github.com\/aws\/sagemaker-containers#important-environment-variables\" rel=\"noreferrer\">SageMaker Docker Container. Environment Variables<\/a>.<\/li>\n<li>Training data is setup under <code>\/opt\/ml\/input\/data<\/code>.<\/li>\n<li>Training script codes are setup under <code>\/opt\/ml\/code<\/code>.<\/li>\n<li><code>\/opt\/ml\/model<\/code> and <code>\/opt\/ml\/output<\/code> directories are setup to store training outputs.<\/li>\n<\/ul>\n<\/li>\n<\/ol>\n<pre><code>\/opt\/ml\n\u251c\u2500\u2500 input\n\u2502   \u251c\u2500\u2500 config\n\u2502   \u2502   \u251c\u2500\u2500 hyperparameters.json  &lt;--- From Estimator hyperparameter arg\n\u2502   \u2502   \u2514\u2500\u2500 resourceConfig.json\n\u2502   \u2514\u2500\u2500 data\n\u2502       \u2514\u2500\u2500 &lt;channel_name&gt;        &lt;--- From Estimator fit method inputs arg\n\u2502           \u2514\u2500\u2500 &lt;input data&gt;\n\u251c\u2500\u2500 code\n\u2502   \u2514\u2500\u2500 &lt;code files&gt;              &lt;--- From Estimator src_dir arg\n\u251c\u2500\u2500 model\n\u2502   \u2514\u2500\u2500 &lt;model files&gt;             &lt;--- Location to save the trained model artifacts\n\u2514\u2500\u2500 output\n    \u2514\u2500\u2500 failure                   &lt;--- Training job failure logs\n<\/code><\/pre>\n<ol start=\"2\">\n<li><p>SageMaker Estimator <code>fit(inputs)<\/code> method executes the training script. Estimator <code>hyperparameters<\/code> and <code>fit<\/code> method <code>inputs<\/code> are provided as its command line arguments.<\/p>\n<\/li>\n<li><p>The training script saves the model artifacts in the <code>\/opt\/ml\/model<\/code> once the training is completed.<\/p>\n<\/li>\n<li><p>SageMaker archives the artifacts under <code>\/opt\/ml\/model<\/code> into <code>model.tar.gz<\/code> and save it to the S3 location specified to <code>output_path<\/code> Estimator parameter.<\/p>\n<\/li>\n<li><p>You can set Estimator <code>metric_definitions<\/code> parameter to extract model metrics from the training logs. Then you can monitor the training progress in the SageMaker console metrics.<\/p>\n<\/li>\n<\/ol>\n<p><a href=\"https:\/\/i.stack.imgur.com\/gi8bU.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/gi8bU.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>I believe AWS needs to stop mass-producing verbose, redundant, wordy, scattered, and obsolete documents. AWS needs to understand <strong>A picture is worth thousand words<\/strong>.<\/p>\n<p>Have diagrams and piece document parts together in a <strong>context<\/strong> with a clear objective to achieve.<\/p>\n<hr \/>\n<h1>Problem<\/h1>\n<p>AWS documentations need serious re-design and re-structuring. Just to understand <strong>how to train and save a model<\/strong> forces us going through dozens of scattered,  fragmented, verbose, redundant documentations, which are often obsolete, incomplete, and sometime incorrect.<\/p>\n<p>It is well-summarized in <a href=\"https:\/\/nandovillalba.medium.com\/why-i-think-gcp-is-better-than-aws-ea78f9975bda\" rel=\"noreferrer\">Why I think GCP is better than AWS<\/a>:<\/p>\n<blockquote>\n<p>It\u2019s not that AWS is harder to use than GCP, it\u2019s that <strong>it is needlessly hard<\/strong>; a disjointed, sprawl of infrastructure primitives with poor cohesion between them.  <br><br>\nA challenge is nice, a confusing mess is not, and <strong>the problem with AWS is that a large part of your working hours will be spent untangling their documentation and weeding through features and products to find what you want<\/strong>, rather than focusing on cool interesting challenges.<\/p>\n<\/blockquote>\n<p>Especially the SageMaker team keeps changing implementations without updating documents. Its roll-out was also inconsistent, e.g. SDK version 2 was rolled out in the SageMaker Studio making the AWS examples in Github incompatible without announcing it. Whereas SageMaker instance still had SDK 1, hence code worked in Instance but not in Studio.<\/p>\n<p>It is mind-boggling that we have to go through these many documents below to understand how to use the SageMaker SDK Estimator for training.<\/p>\n<h2>Documents for Model Training<\/h2>\n<ul>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/how-it-works-training.html\" rel=\"noreferrer\">Train a Model with Amazon SageMaker<\/a><\/li>\n<\/ul>\n<p>This document gives 20,000 feet overview of how SageMaker training but does not give any clue what to do.<\/p>\n<ul>\n<li><a href=\"https:\/\/sagemaker-workshop.com\/custom\/containers.html\" rel=\"noreferrer\">Running a container for Amazon SageMaker training<\/a><\/li>\n<\/ul>\n<p>This document gives an overview of how SageMaker training looks like. However, this is not up-to-date as it is based on <a href=\"https:\/\/github.com\/aws\/sagemaker-containers\" rel=\"noreferrer\">SageMaker Containers<\/a> which is obsolete.<\/p>\n<blockquote>\n<p>WARNING: This package has been deprecated. Please use the SageMaker Training Toolkit for model training and the SageMaker Inference Toolkit for model serving.<\/p>\n<\/blockquote>\n<ul>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-train-model.html\" rel=\"noreferrer\">Step 4: Train a Model<\/a><\/li>\n<\/ul>\n<p>This document layouts the steps for training.<\/p>\n<blockquote>\n<p>The Amazon SageMaker Python SDK provides framework estimators and generic estimators to train your model while orchestrating the machine learning (ML) lifecycle accessing the SageMaker features for training and the AWS infrastructures<\/p>\n<\/blockquote>\n<ul>\n<li><a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html#train-a-model-with-the-sagemaker-python-sdk\" rel=\"noreferrer\">Train a Model with the SageMaker Python SDK<\/a><\/li>\n<\/ul>\n<blockquote>\n<p>To train a model by using the SageMaker Python SDK, you:<\/p>\n<ul>\n<li>Prepare a training script<\/li>\n<li>Create an estimator<\/li>\n<li>Call the fit method of the estimator<\/li>\n<\/ul>\n<\/blockquote>\n<p>Finally this document gives concrete steps and ideas. However still missing comprehensiv details about Environment Variables, Directory structure in the SageMaker docker container**, S3 for uploading code, placing data, S3 where the trained model is saved, etc.<\/p>\n<ul>\n<li><a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/using_tf.html\" rel=\"noreferrer\">Use TensorFlow with the SageMaker Python SDK<\/a><\/li>\n<\/ul>\n<p>This documents is focused on TensorFlow Estimator implementation steps. Use <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/frameworks\/tensorflow\/get_started_mnist_train.ipynb\" rel=\"noreferrer\">Training a Tensorflow Model on MNIST<\/a> Github example to accompany with to follow the actual implementation.<\/p>\n<h2>Documents for passing parameters and data locations<\/h2>\n<ul>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-running-container.html#your-algorithms-training-algo-running-container-inputdataconfig\" rel=\"noreferrer\">How Amazon SageMaker Provides Training Information<\/a><\/li>\n<\/ul>\n<blockquote>\n<p>This section explains how SageMaker makes training information, such as training data, hyperparameters, and other configuration information, available to your Docker container.<\/p>\n<\/blockquote>\n<p>This document finally gives the idea of how parameters and data are passed around but again, not comprehensive.<\/p>\n<ul>\n<li><a href=\"https:\/\/github.com\/aws\/sagemaker-containers#important-environment-variables\" rel=\"noreferrer\">SageMaker Docker Container Environment Variables<\/a><\/li>\n<\/ul>\n<p>This documentation is marked as <strong>deprecated<\/strong> but the only document which explains the SageMaker Environment Variables.<\/p>\n<blockquote>\n<h3>IMPORTANT ENVIRONMENT VARIABLES<\/h3>\n<ul>\n<li>SM_MODEL_DIR<\/li>\n<li>SM_CHANNELS<\/li>\n<li>SM_CHANNEL_{channel_name}<\/li>\n<li>SM_HPS<\/li>\n<li>SM_HP_{hyperparameter_name}<\/li>\n<li>SM_CURRENT_HOST<\/li>\n<li>SM_HOSTS<\/li>\n<li>SM_NUM_GPUS<\/li>\n<\/ul>\n<h3>List of provided environment variables by SageMaker Containers<\/h3>\n<ul>\n<li>SM_NUM_CPUS<\/li>\n<li>SM_LOG_LEVEL<\/li>\n<li>SM_NETWORK_INTERFACE_NAME<\/li>\n<li>SM_USER_ARGS<\/li>\n<li>SM_INPUT_DIR<\/li>\n<li>SM_INPUT_CONFIG_DIR<\/li>\n<li>SM_OUTPUT_DATA_DIR<\/li>\n<li>SM_RESOURCE_CONFIG<\/li>\n<li>SM_INPUT_DATA_CONFIG<\/li>\n<li>SM_TRAINING_ENV<\/li>\n<\/ul>\n<\/blockquote>\n<h2>Documents for SageMaker Docker Container Directory Structure<\/h2>\n<ul>\n<li><a href=\"https:\/\/sagemaker-workshop.com\/custom\/containers.html\" rel=\"noreferrer\">Running a container for Amazon SageMaker training<\/a><\/li>\n<\/ul>\n<pre><code>\/opt\/ml\n\u251c\u2500\u2500 input\n\u2502   \u251c\u2500\u2500 config\n\u2502   \u2502   \u251c\u2500\u2500 hyperparameters.json\n\u2502   \u2502   \u2514\u2500\u2500 resourceConfig.json\n\u2502   \u2514\u2500\u2500 data\n\u2502       \u2514\u2500\u2500 &lt;channel_name&gt;\n\u2502           \u2514\u2500\u2500 &lt;input data&gt;\n\u251c\u2500\u2500 model\n\u2502   \u2514\u2500\u2500 &lt;model files&gt;\n\u2514\u2500\u2500 output\n    \u2514\u2500\u2500 failure\n<\/code><\/pre>\n<p>This document explains the directory structure and purpose of each directory.<\/p>\n<blockquote>\n<h3>The input<\/h3>\n<ul>\n<li>\/opt\/ml\/input\/config contains information to control how your program runs. hyperparameters.json is a JSON-formatted dictionary of hyperparameter names to values. These values will always be strings, so you may need to convert them. resourceConfig.json is a JSON-formatted file that describes the network layout used for distributed training. Since scikit-learn doesn\u2019t support distributed training, we\u2019ll ignore it here.<\/li>\n<li>\/opt\/ml\/input\/data\/&lt;channel_name&gt;\/ (for File mode) contains the input data for that channel. The channels are created based on the call to CreateTrainingJob but it\u2019s generally important that channels match what the algorithm expects. The files for each channel will be copied from S3 to this directory, preserving the tree structure indicated by the S3 key structure.<\/li>\n<li>\/opt\/ml\/input\/data\/&lt;channel_name&gt;_&lt;epoch_number&gt; (for Pipe mode) is the pipe for a given epoch. Epochs start at zero and go up by one each time you read them. There is no limit to the number of epochs that you can run, but you must close each pipe before reading the next epoch.<\/li>\n<\/ul>\n<h3>The output<\/h3>\n<ul>\n<li>\/opt\/ml\/model\/ is the directory where you write the model that your algorithm generates. Your model can be in any format that you want. It can be a single file or a whole directory tree. SageMaker will package any files in this directory into a compressed tar archive file. This file will be available at the S3 location returned in the DescribeTrainingJob result.<\/li>\n<li>\/opt\/ml\/output is a directory where the algorithm can write a file failure that describes why the job failed. The contents of this file will be returned in the FailureReason field of the DescribeTrainingJob result. For jobs that succeed, there is no reason to write this file as it will be ignored.<\/li>\n<\/ul>\n<\/blockquote>\n<p>However, this is not up-to-date as it is based on <a href=\"https:\/\/github.com\/aws\/sagemaker-containers\" rel=\"noreferrer\">SageMaker Containers<\/a> which is obsolete.<\/p>\n<h2>Documents for Model Saving<\/h2>\n<p>The information on where the trained model is saved and in what format are fundamentally missing. The training script needs to save the model under <code>\/opt\/ml\/model<\/code> and the format and sub-directory structure depend on the frameworks e,g TensorFlow, Pytorch. This is because SageMaker deployment uses the Framework dependent model-serving, e,g. TensorFlow Serving for TensorFlow framework.<\/p>\n<p>This is not clearly documented and causing confusions. The developer needs to specify which format to use and under which sub-directory to save.<\/p>\n<p>To use TensorFlow Estimator training and deployment:<\/p>\n<ul>\n<li><a href=\"https:\/\/sagemaker-examples.readthedocs.io\/en\/latest\/aws_sagemaker_studio\/frameworks\/keras_pipe_mode_horovod\/keras_pipe_mode_horovod_cifar10.html#Deploy-the-trained-model\" rel=\"noreferrer\">Deploy the trained model<\/a><\/li>\n<\/ul>\n<blockquote>\n<p>Because <strong>we\u2019re using TensorFlow Serving for deployment<\/strong>, our training script <strong>saves the model in TensorFlow\u2019s SavedModel format<\/strong>.<\/p>\n<\/blockquote>\n<ul>\n<li><a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/frameworks\/tensorflow\/code\/train.py#L159-L166\" rel=\"noreferrer\">amazon-sagemaker-examples\/frameworks\/tensorflow\/code\/train.py <\/a><\/li>\n<\/ul>\n<pre><code>    # Save the model\n    # A version number is needed for the serving container\n    # to load the model\n    version = &quot;00000000&quot;\n    ckpt_dir = os.path.join(args.model_dir, version)\n    if not os.path.exists(ckpt_dir):\n        os.makedirs(ckpt_dir)\n    model.save(ckpt_dir)\n<\/code><\/pre>\n<p>The code is saving the model in <code>\/opt\/ml\/model\/00000000<\/code> because this is for TensorFlow serving.<\/p>\n<ul>\n<li><a href=\"https:\/\/www.tensorflow.org\/guide\/saved_model\" rel=\"noreferrer\">Using the SavedModel format<\/a><\/li>\n<\/ul>\n<blockquote>\n<p>The save-path follows a convention used by TensorFlow Serving where the last path component (1\/ here) is a version number for your model - it allows tools like Tensorflow Serving to reason about the relative freshness.<\/p>\n<\/blockquote>\n<ul>\n<li><a href=\"https:\/\/www.tensorflow.org\/tfx\/tutorials\/serving\/rest_simple#save_your_model\" rel=\"noreferrer\">Train and serve a TensorFlow model with TensorFlow Serving<\/a><\/li>\n<\/ul>\n<blockquote>\n<p>To load our trained model into TensorFlow Serving we first need to save it in SavedModel format. This will create a protobuf file in a well-defined directory hierarchy, and will include a version number. TensorFlow Serving allows us to select which version of a model, or &quot;servable&quot; we want to use when we make inference requests. Each version will be exported to a different sub-directory under the given path.<\/p>\n<\/blockquote>\n<h2>Documents for API<\/h2>\n<p>Basically the SageMaker SDK Estimator implements the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateTrainingJob.html\" rel=\"noreferrer\">CreateTrainingJob<\/a> API for training part. Hence, better to understand how it is designed and what parameters need to be defined. Otherwise working on Estimators are like walking in the dark.<\/p>\n<hr \/>\n<h1>Example<\/h1>\n<h2>Jupyter Notebook<\/h2>\n<pre><code>import sagemaker\nfrom sagemaker import get_execution_role\n\nsagemaker_session = sagemaker.Session()\nrole = get_execution_role()\nbucket = sagemaker_session.default_bucket()\n\nmetric_definitions = [\n    {&quot;Name&quot;: &quot;train:loss&quot;, &quot;Regex&quot;: &quot;.*loss: ([0-9\\\\.]+) - accuracy: [0-9\\\\.]+.*&quot;},\n    {&quot;Name&quot;: &quot;train:accuracy&quot;, &quot;Regex&quot;: &quot;.*loss: [0-9\\\\.]+ - accuracy: ([0-9\\\\.]+).*&quot;},\n    {\n        &quot;Name&quot;: &quot;validation:accuracy&quot;,\n        &quot;Regex&quot;: &quot;.*step - loss: [0-9\\\\.]+ - accuracy: [0-9\\\\.]+ - val_loss: [0-9\\\\.]+ - val_accuracy: ([0-9\\\\.]+).*&quot;,\n    },\n    {\n        &quot;Name&quot;: &quot;validation:loss&quot;,\n        &quot;Regex&quot;: &quot;.*step - loss: [0-9\\\\.]+ - accuracy: [0-9\\\\.]+ - val_loss: ([0-9\\\\.]+) - val_accuracy: [0-9\\\\.]+.*&quot;,\n    },\n    {\n        &quot;Name&quot;: &quot;sec\/sample&quot;,\n        &quot;Regex&quot;: &quot;.* - \\d+s (\\d+)[mu]s\/sample - loss: [0-9\\\\.]+ - accuracy: [0-9\\\\.]+ - val_loss: [0-9\\\\.]+ - val_accuracy: [0-9\\\\.]+&quot;,\n    },\n]\n\nimport uuid\n\ncheckpoint_s3_prefix = &quot;checkpoints\/{}&quot;.format(str(uuid.uuid4()))\ncheckpoint_s3_uri = &quot;s3:\/\/{}\/{}\/&quot;.format(bucket, checkpoint_s3_prefix)\n\nfrom sagemaker.tensorflow import TensorFlow\n\n# --------------------------------------------------------------------------------\n# 'trainingJobName' msut satisfy regular expression pattern: ^[a-zA-Z0-9](-*[a-zA-Z0-9]){0,62}\n# --------------------------------------------------------------------------------\nbase_job_name = &quot;fashion-mnist&quot;\nhyperparameters = {\n    &quot;epochs&quot;: 2, \n    &quot;batch-size&quot;: 64\n}\nestimator = TensorFlow(\n    entry_point=&quot;fashion_mnist.py&quot;,\n    source_dir=&quot;src&quot;,\n    metric_definitions=metric_definitions,\n    hyperparameters=hyperparameters,\n    role=role,\n    input_mode='File',\n    framework_version=&quot;2.3.1&quot;,\n    py_version=&quot;py37&quot;,\n    instance_count=1,\n    instance_type=&quot;ml.m5.xlarge&quot;,\n    base_job_name=base_job_name,\n    checkpoint_s3_uri=checkpoint_s3_uri,\n    model_dir=False\n)\nestimator.fit()\n<\/code><\/pre>\n<h2>fashion_mnist.py<\/h2>\n<pre><code>import os\nimport argparse\nimport json\nimport multiprocessing\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten, BatchNormalization\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D\nfrom tensorflow.keras.layers.experimental.preprocessing import Normalization\nfrom tensorflow.keras import backend as K\n\nprint(&quot;TensorFlow version: {}&quot;.format(tf.__version__))\nprint(&quot;Eager execution is: {}&quot;.format(tf.executing_eagerly()))\nprint(&quot;Keras version: {}&quot;.format(tf.keras.__version__))\n\n\nimage_width = 28\nimage_height = 28\n\n\ndef load_data():\n    fashion_mnist = tf.keras.datasets.fashion_mnist\n    (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n\n    number_of_classes = len(set(y_train))\n    print(&quot;number_of_classes&quot;, number_of_classes)\n\n    x_train = x_train \/ 255.0\n    x_test = x_test \/ 255.0\n    x_full = np.concatenate((x_train, x_test), axis=0)\n    print(x_full.shape)\n\n    print(type(x_train))\n    print(x_train.shape)\n    print(x_train.dtype)\n    print(y_train.shape)\n    print(y_train.dtype)\n\n    # ## Train\n    # * C: Convolution layer\n    # * P: Pooling layer\n    # * B: Batch normalization layer\n    # * F: Fully connected layer\n    # * O: Output fully connected softmax layer\n\n    # Reshape data based on channels first \/ channels last strategy.\n    # This is dependent on whether you use TF, Theano or CNTK as backend.\n    # Source: https:\/\/github.com\/keras-team\/keras\/blob\/master\/examples\/mnist_cnn.py\n    if K.image_data_format() == 'channels_first':\n        x = x_train.reshape(x_train.shape[0], 1, image_width, image_height)\n        x_test = x_test.reshape(x_test.shape[0], 1, image_width, image_height)\n        input_shape = (1, image_width, image_height)\n    else:\n        x_train = x_train.reshape(x_train.shape[0], image_width, image_height, 1)\n        x_test = x_test.reshape(x_test.shape[0], image_width, image_height, 1)\n        input_shape = (image_width, image_height, 1)\n\n    return x_train, y_train, x_test, y_test, input_shape, number_of_classes\n\n# tensorboard --logdir=\/full_path_to_your_logs\n\nvalidation_split = 0.2\nverbosity = 1\nuse_multiprocessing = True\nworkers = multiprocessing.cpu_count()\n\n\ndef train(model, x, y, args):\n    # SavedModel Output\n    tensorflow_saved_model_path = os.path.join(args.model_dir, &quot;tensorflow\/saved_model\/0&quot;)\n    os.makedirs(tensorflow_saved_model_path, exist_ok=True)\n\n    # Tensorboard Logs\n    tensorboard_logs_path = os.path.join(args.model_dir, &quot;tensorboard\/&quot;)\n    os.makedirs(tensorboard_logs_path, exist_ok=True)\n\n    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n        log_dir=tensorboard_logs_path,\n        write_graph=True,\n        write_images=True,\n        histogram_freq=1,  # How often to log histogram visualizations\n        embeddings_freq=1,  # How often to log embedding visualizations\n        update_freq=&quot;epoch&quot;,\n    )  # How often to write logs (default: once per epoch)\n\n    model.compile(\n        optimizer='adam',\n        loss=tf.keras.losses.sparse_categorical_crossentropy,\n        metrics=['accuracy']\n    )\n    history = model.fit(\n        x,\n        y,\n        shuffle=True,\n        batch_size=args.batch_size,\n        epochs=args.epochs,\n        validation_split=validation_split,\n        use_multiprocessing=use_multiprocessing,\n        workers=workers,\n        verbose=verbosity,\n        callbacks=[\n            tensorboard_callback\n        ]\n    )\n    return history\n\n\ndef create_model(input_shape, number_of_classes):\n    model = Sequential([\n        Conv2D(\n            name=&quot;conv01&quot;,\n            filters=32,\n            kernel_size=(3, 3),\n            strides=(1, 1),\n            padding=&quot;same&quot;,\n            activation='relu',\n            input_shape=input_shape\n        ),\n        MaxPooling2D(\n            name=&quot;pool01&quot;,\n            pool_size=(2, 2)\n        ),\n        Flatten(),  # 3D shape to 1D.\n        BatchNormalization(\n            name=&quot;batch_before_full01&quot;\n        ),\n        Dense(\n            name=&quot;full01&quot;,\n            units=300,\n            activation=&quot;relu&quot;\n        ),  # Fully connected layer\n        Dense(\n            name=&quot;output_softmax&quot;,\n            units=number_of_classes,\n            activation=&quot;softmax&quot;\n        )\n    ])\n    return model\n\n\ndef save_model(model, args):\n    # Save the model\n    # A version number is needed for the serving container\n    # to load the model\n    version = &quot;00000000&quot;\n    model_save_dir = os.path.join(args.model_dir, version)\n    if not os.path.exists(model_save_dir):\n        os.makedirs(model_save_dir)\n    print(f&quot;saving model at {model_save_dir}&quot;)\n    model.save(model_save_dir)\n\n\ndef parse_args():\n    # --------------------------------------------------------------------------------\n    # https:\/\/docs.python.org\/dev\/library\/argparse.html#dest\n    # --------------------------------------------------------------------------------\n    parser = argparse.ArgumentParser()\n\n    # --------------------------------------------------------------------------------\n    # hyperparameters Estimator argument are passed as command-line arguments to the script.\n    # --------------------------------------------------------------------------------\n    parser.add_argument('--epochs', type=int, default=10)\n    parser.add_argument('--batch-size', type=int, default=64)\n\n    # \/opt\/ml\/model\n    # sagemaker.tensorflow.estimator.TensorFlow override 'model_dir'.\n    # See https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/\\\n    # sagemaker.tensorflow.html#sagemaker.tensorflow.estimator.TensorFlow\n    parser.add_argument('--model_dir', type=str, default=os.environ['SM_MODEL_DIR'])\n\n    # \/opt\/ml\/output\n    parser.add_argument(&quot;--output_dir&quot;, type=str, default=os.environ[&quot;SM_OUTPUT_DIR&quot;])\n\n    args = parser.parse_args()\n    return args\n\n\nif __name__ == &quot;__main__&quot;:\n    args = parse_args()\n    print(&quot;---------- key\/value args&quot;)\n    for key, value in vars(args).items():\n        print(f&quot;{key}:{value}&quot;)\n\n    x_train, y_train, x_test, y_test, input_shape, number_of_classes = load_data()\n    model = create_model(input_shape, number_of_classes)\n\n    history = train(model=model, x=x_train, y=y_train, args=args)\n    print(history)\n    \n    save_model(model, args)\n    results = model.evaluate(x_test, y_test, batch_size=100)\n    print(&quot;test loss, test accuracy:&quot;, results)\n<\/code><\/pre>\n<h2>SageMaker Console<\/h2>\n<p><a href=\"https:\/\/i.stack.imgur.com\/ctcLy.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ctcLy.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<h2>Notebook output<\/h2>\n<pre><code>2021-09-03 03:02:04 Starting - Starting the training job...\n2021-09-03 03:02:16 Starting - Launching requested ML instancesProfilerReport-1630638122: InProgress\n......\n2021-09-03 03:03:17 Starting - Preparing the instances for training.........\n2021-09-03 03:04:59 Downloading - Downloading input data\n2021-09-03 03:04:59 Training - Downloading the training image...\n2021-09-03 03:05:23 Training - Training image download completed. Training in progress.2021-09-03 03:05:23.966037: W tensorflow\/core\/profiler\/internal\/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n2021-09-03 03:05:23.969704: W tensorflow\/core\/profiler\/internal\/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\n2021-09-03 03:05:24.118054: W tensorflow\/core\/profiler\/internal\/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n2021-09-03 03:05:26,842 sagemaker-training-toolkit INFO     Imported framework sagemaker_tensorflow_container.training\n2021-09-03 03:05:26,852 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n2021-09-03 03:05:27,734 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\n\/usr\/local\/bin\/python3.7 -m pip install -r requirements.txt\nWARNING: You are using pip version 21.0.1; however, version 21.2.4 is available.\nYou should consider upgrading via the '\/usr\/local\/bin\/python3.7 -m pip install --upgrade pip' command.\n\n2021-09-03 03:05:29,028 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n2021-09-03 03:05:29,045 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n2021-09-03 03:05:29,062 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n2021-09-03 03:05:29,072 sagemaker-training-toolkit INFO     Invoking user script\n\nTraining Env:\n\n{\n    &quot;additional_framework_parameters&quot;: {},\n    &quot;channel_input_dirs&quot;: {},\n    &quot;current_host&quot;: &quot;algo-1&quot;,\n    &quot;framework_module&quot;: &quot;sagemaker_tensorflow_container.training:main&quot;,\n    &quot;hosts&quot;: [\n        &quot;algo-1&quot;\n    ],\n    &quot;hyperparameters&quot;: {\n        &quot;batch-size&quot;: 64,\n        &quot;epochs&quot;: 2\n    },\n    &quot;input_config_dir&quot;: &quot;\/opt\/ml\/input\/config&quot;,\n    &quot;input_data_config&quot;: {},\n    &quot;input_dir&quot;: &quot;\/opt\/ml\/input&quot;,\n    &quot;is_master&quot;: true,\n    &quot;job_name&quot;: &quot;fashion-mnist-2021-09-03-03-02-02-305&quot;,\n    &quot;log_level&quot;: 20,\n    &quot;master_hostname&quot;: &quot;algo-1&quot;,\n    &quot;model_dir&quot;: &quot;\/opt\/ml\/model&quot;,\n    &quot;module_dir&quot;: &quot;s3:\/\/sagemaker-us-east-1-316725000538\/fashion-mnist-2021-09-03-03-02-02-305\/source\/sourcedir.tar.gz&quot;,\n    &quot;module_name&quot;: &quot;fashion_mnist&quot;,\n    &quot;network_interface_name&quot;: &quot;eth0&quot;,\n    &quot;num_cpus&quot;: 4,\n    &quot;num_gpus&quot;: 0,\n    &quot;output_data_dir&quot;: &quot;\/opt\/ml\/output\/data&quot;,\n    &quot;output_dir&quot;: &quot;\/opt\/ml\/output&quot;,\n    &quot;output_intermediate_dir&quot;: &quot;\/opt\/ml\/output\/intermediate&quot;,\n    &quot;resource_config&quot;: {\n        &quot;current_host&quot;: &quot;algo-1&quot;,\n        &quot;hosts&quot;: [\n            &quot;algo-1&quot;\n        ],\n        &quot;network_interface_name&quot;: &quot;eth0&quot;\n    },\n    &quot;user_entry_point&quot;: &quot;fashion_mnist.py&quot;\n}\n\nEnvironment variables:\n\nSM_HOSTS=[&quot;algo-1&quot;]\nSM_NETWORK_INTERFACE_NAME=eth0\nSM_HPS={&quot;batch-size&quot;:64,&quot;epochs&quot;:2}\nSM_USER_ENTRY_POINT=fashion_mnist.py\nSM_FRAMEWORK_PARAMS={}\nSM_RESOURCE_CONFIG={&quot;current_host&quot;:&quot;algo-1&quot;,&quot;hosts&quot;:[&quot;algo-1&quot;],&quot;network_interface_name&quot;:&quot;eth0&quot;}\nSM_INPUT_DATA_CONFIG={}\nSM_OUTPUT_DATA_DIR=\/opt\/ml\/output\/data\nSM_CHANNELS=[]\nSM_CURRENT_HOST=algo-1\nSM_MODULE_NAME=fashion_mnist\nSM_LOG_LEVEL=20\nSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\nSM_INPUT_DIR=\/opt\/ml\/input\nSM_INPUT_CONFIG_DIR=\/opt\/ml\/input\/config\nSM_OUTPUT_DIR=\/opt\/ml\/output\nSM_NUM_CPUS=4\nSM_NUM_GPUS=0\nSM_MODEL_DIR=\/opt\/ml\/model\nSM_MODULE_DIR=s3:\/\/sagemaker-us-east-1-316725000538\/fashion-mnist-2021-09-03-03-02-02-305\/source\/sourcedir.tar.gz\nSM_TRAINING_ENV={&quot;additional_framework_parameters&quot;:{},&quot;channel_input_dirs&quot;:{},&quot;current_host&quot;:&quot;algo-1&quot;,&quot;framework_module&quot;:&quot;sagemaker_tensorflow_container.training:main&quot;,&quot;hosts&quot;:[&quot;algo-1&quot;],&quot;hyperparameters&quot;:{&quot;batch-size&quot;:64,&quot;epochs&quot;:2},&quot;input_config_dir&quot;:&quot;\/opt\/ml\/input\/config&quot;,&quot;input_data_config&quot;:{},&quot;input_dir&quot;:&quot;\/opt\/ml\/input&quot;,&quot;is_master&quot;:true,&quot;job_name&quot;:&quot;fashion-mnist-2021-09-03-03-02-02-305&quot;,&quot;log_level&quot;:20,&quot;master_hostname&quot;:&quot;algo-1&quot;,&quot;model_dir&quot;:&quot;\/opt\/ml\/model&quot;,&quot;module_dir&quot;:&quot;s3:\/\/sagemaker-us-east-1-316725000538\/fashion-mnist-2021-09-03-03-02-02-305\/source\/sourcedir.tar.gz&quot;,&quot;module_name&quot;:&quot;fashion_mnist&quot;,&quot;network_interface_name&quot;:&quot;eth0&quot;,&quot;num_cpus&quot;:4,&quot;num_gpus&quot;:0,&quot;output_data_dir&quot;:&quot;\/opt\/ml\/output\/data&quot;,&quot;output_dir&quot;:&quot;\/opt\/ml\/output&quot;,&quot;output_intermediate_dir&quot;:&quot;\/opt\/ml\/output\/intermediate&quot;,&quot;resource_config&quot;:{&quot;current_host&quot;:&quot;algo-1&quot;,&quot;hosts&quot;:[&quot;algo-1&quot;],&quot;network_interface_name&quot;:&quot;eth0&quot;},&quot;user_entry_point&quot;:&quot;fashion_mnist.py&quot;}\nSM_USER_ARGS=[&quot;--batch-size&quot;,&quot;64&quot;,&quot;--epochs&quot;,&quot;2&quot;]\nSM_OUTPUT_INTERMEDIATE_DIR=\/opt\/ml\/output\/intermediate\nSM_HP_BATCH-SIZE=64\nSM_HP_EPOCHS=2\nPYTHONPATH=\/opt\/ml\/code:\/usr\/local\/bin:\/usr\/local\/lib\/python37.zip:\/usr\/local\/lib\/python3.7:\/usr\/local\/lib\/python3.7\/lib-dynload:\/usr\/local\/lib\/python3.7\/site-packages\n\nInvoking script with the following command:\n\n\/usr\/local\/bin\/python3.7 fashion_mnist.py --batch-size 64 --epochs 2\n\n\nTensorFlow version: 2.3.1\nEager execution is: True\nKeras version: 2.4.0\n---------- key\/value args\nepochs:2\nbatch_size:64\nmodel_dir:\/opt\/ml\/model\noutput_dir:\/opt\/ml\/output\n<\/code><\/pre>",
        "Answer_comment_count":4.0,
        "Answer_creation_date":"2021-09-02 04:01:47.170000 UTC",
        "Answer_last_edit_date":"2022-07-04 05:43:30.063000 UTC",
        "Answer_score":65.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":68764644,
        "Question_title":"Training Google-Cloud-Automl Model on multiple datasets",
        "Question_body":"<p>I would like to train an automl model on gcp's vertex ai using multiple datasets.  I would like to keep the datasets separate, since they come from different sources, want to train on them separately, etc.  Is that possible?  Or will I need to create a dataset containing both datasets? It looks like I can only select one dataset in the web UI.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-08-12 22:02:54.297000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"google-cloud-automl|google-cloud-vertex-ai",
        "Question_view_count":223,
        "Owner_creation_date":"2018-10-30 18:54:03.620000 UTC",
        "Owner_last_access_date":"2022-09-23 17:00:36.977000 UTC",
        "Owner_location":"Utah, USA",
        "Owner_reputation":1212,
        "Owner_up_votes":699,
        "Owner_down_votes":4,
        "Owner_views":110,
        "Answer_body":"<p>It is possible via the Vertex AI API as long as your sources are in Google Cloud Storage, just provide a list of training data which are in JSON or CSV format that qualifies with the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/datasets\/prepare-image\" rel=\"nofollow noreferrer\">best practices for formatting of training data<\/a>.<\/p>\n<p>See code for creating and importing datasets. See <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/datasets\/create-dataset-api#create-dataset\" rel=\"nofollow noreferrer\">documentation<\/a> for code reference and further details.<\/p>\n<pre><code>from typing import List, Union\nfrom google.cloud import aiplatform\n\n    def create_and_import_dataset_image_sample(\n        project: str,\n        location: str,\n        display_name: str,\n        src_uris: Union[str, List[str]], \/\/ example: [&quot;gs:\/\/bucket\/file1.csv&quot;, &quot;gs:\/\/bucket\/file2.csv&quot;]\n        sync: bool = True,\n    ):\n        aiplatform.init(project=project, location=location)\n    \n        ds = aiplatform.ImageDataset.create(\n            display_name=display_name,\n            gcs_source=src_uris,\n            import_schema_uri=aiplatform.schema.dataset.ioformat.image.single_label_classification,\n            sync=sync,\n        )\n    \n        ds.wait()\n    \n        print(ds.display_name)\n        print(ds.resource_name)\n        return ds\n<\/code><\/pre>\n<p>NOTE: The links provided are for Vertex AI AutoML Image. If you access the links there are options for other AutoML products like Text, Tabular and Video.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-08-13 04:16:45.167000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "google-cloud-vertex-ai"
        ]
    },
    {
        "Question_id":71308112,
        "Question_title":"How to return a float with 'text\/csv' as \"Content-Type\" from SageMaker endpoint that uses custom inference code?",
        "Question_body":"<p>I am trying to return output(or predictions) from a SageMaker endpoint with 'text\/csv' or 'text\/csv; charset=utf-8' as &quot;Content-Type&quot;. I have tried multiple ways, but the sagemaker always returns with 'text\/html; charset=utf-8' as the &quot;Content-Type&quot;, and I would like SageMaker to return 'text\/csv' or 'text\/csv; charset=utf-8'.<\/p>\n<p>Here's the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/sklearn\/using_sklearn.html#process-output\" rel=\"nofollow noreferrer\"><code>output_fn<\/code><\/a> from my inference-code:<\/p>\n<pre><code>** my other code **\ndef output_fn(prediction, content_type='text\/csv'):\n    ** my other code **\n    return output_float\n<\/code><\/pre>\n<p>above function returns number with float data-type and I got error(in cloudwatch logs) that  this function should only be returning string, tuple, dict or Respoonse instance.<\/p>\n<p>So, here are all the different ways I have tried to have SageMaker return my number with 'text\/csv' but only receives 'text\/html; charset=utf-8'<\/p>\n<ol>\n<li><code>return json.dumps(output_float)<\/code>. this sent 'text\/html; charset=utf-8'.<\/li>\n<li><code>return f&quot;{output_float}&quot;<\/code>. this sent 'text\/html; charset=utf-8'.<\/li>\n<li><code>return f&quot;{output_float},&quot;<\/code>. this sent 'text\/html; charset=utf-8'.<\/li>\n<li><code>return f&quot;{output_float}\\n&quot;<\/code>. this sent 'text\/html; charset=utf-8'.<\/li>\n<li><code>return f&quot;{output_float},\\n&quot;<\/code>. this sent 'text\/html; charset=utf-8'.<\/li>\n<li>by using <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/serializers.html\" rel=\"nofollow noreferrer\"><code>sagemaker.serializers.CSVSerializer<\/code><\/a> like this:\n<pre><code>from sagemaker.serializers import CSVSerializer\ncsv_serialiser = CSVSerializer(content_type='text\/csv')\n\ndef output_fn(prediction, content_type='text\/csv'):\n    ** my other code **\n    return csv_serialiser.serialize(output_float)\n<\/code><\/pre>\nI got <code>'NoneType' object has no attribute 'startswith'<\/code> error with this.<\/li>\n<li>as a tuple: <code>return (output_float,)<\/code>. I haven't noted down what this did, but it sure didn't return the number with 'text\/csv' as &quot;Content-Type&quot;.<\/li>\n<li>made changes in my model object to return a float on calling <code>.predict_proba<\/code> on my model-object and deployed it from sagemaker studio without using any custom inference code and deployed this from SageMaker studio. but got this error after sending a request to the endpoint: <code>'NoneType' object has no attribute 'startswith'<\/code>, but at my side, when I pass proper inputs to the unpicked model and call .predict_proba, i get float as expected.<\/li>\n<li>by returning <a href=\"https:\/\/tedboy.github.io\/flask\/generated\/generated\/flask.Response.html#flask.Response\" rel=\"nofollow noreferrer\"><code>flask.Response<\/code><\/a> like this:\n<pre><code>from flask import Response\n\ndef output_fn(prediction, content_type='text\/csv'):\n    ** my other code **\n    return Response(response=output_float, status=200, headers={'Content-Type':'text\/csv; charset=utf-8'})\n<\/code><\/pre>\nbut, I got some IndexError with this(I didn't notedown the traceback.)<\/li>\n<\/ol>\n<p>Some other info:<\/p>\n<ol>\n<li>Model I am using is completely outside of SageMaker, not from a training job or anything like that.<\/li>\n<li>All of the aforementioned endpoints have been deployed entirely from aws-cli with relevant .json files, except the one in point-8 above.<\/li>\n<\/ol>\n<p>How do I return number with &quot;text\/csv&quot; as content-type from sagemaker? I need my output in &quot;text\/csv&quot; content-type specifically for Model-Quality-Monitor. How do I do this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2022-03-01 11:54:47.137000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-03-07 11:21:09.630000 UTC",
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":461,
        "Owner_creation_date":"2019-06-07 12:24:06.180000 UTC",
        "Owner_last_access_date":"2022-09-24 17:19:11.323000 UTC",
        "Owner_location":"Bengaluru, Karnataka, India",
        "Owner_reputation":2046,
        "Owner_up_votes":2858,
        "Owner_down_votes":5,
        "Owner_views":369,
        "Answer_body":"<p>From the <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/advanced_functionality\/scikit_bring_your_own\/container\/decision_trees\/predictor.py#L88\" rel=\"nofollow noreferrer\">scikit_bring_your_own<\/a> example, I suggest testing by setting the Response as follows:<\/p>\n<pre><code>return flask.Response(response= output_float, status=200, mimetype=&quot;text\/csv&quot;)\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2022-03-07 20:45:03.080000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":50302810,
        "Question_title":"AWS SageMaker Minimum Configuration",
        "Question_body":"<p>Why do I need Container for AWS SageMaker? If I want to run Scikit Learn on SageMaker's Jupyter notebook for self learning purposes, do I still need to configure Container for it?<\/p>\n\n<p>What is the minimum configuration on SageMaker I will need if I just want to learn Scikit Learn? For example, I want to run Scikit Learn's Decision Tree algorithm with a set of training data and a set of test data. What do I need to do on SageMaker to perform the tasks? Thanks.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2018-05-12 04:20:34.910000 UTC",
        "Question_favorite_count":2.0,
        "Question_last_edit_date":"2018-07-10 08:33:12.337000 UTC",
        "Question_score":9,
        "Question_tags":"scikit-learn|amazon-sagemaker",
        "Question_view_count":849,
        "Owner_creation_date":"2016-04-20 00:33:54.223000 UTC",
        "Owner_last_access_date":"2022-04-02 22:37:53.663000 UTC",
        "Owner_location":"San Jose, CA, United States",
        "Owner_reputation":1075,
        "Owner_up_votes":22,
        "Owner_down_votes":0,
        "Owner_views":181,
        "Answer_body":"<p>You don't need much. Just an AWS Account with the correlated permissions on your role.\nInside the AWS SageMaker Console you can just run an AWS Notebook Instance with one click. There is Sklearn preinstalled and you can use it out of the box. No special container needed.<\/p>\n\n<p>As minimum you just need your AWS Account with the correlated permissions to create EC2 Instances and read \/ write from your S3. Thats all, just try it. :)<\/p>\n\n<p>Use this as a starting point: <a href=\"https:\/\/aws.amazon.com\/blogs\/aws\/sagemaker\/\" rel=\"noreferrer\">Amazon SageMaker \u2013 Accelerating Machine Learning<\/a><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/98gRb.png\" rel=\"noreferrer\">You can also access it via the Jupyter Terminal<\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2018-05-14 14:52:02.537000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":5.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":57273357,
        "Question_title":"Empty dictionary on AnnotationConsolidation lambda event for aws Sagemaker",
        "Question_body":"<p>I am starting to use aws sagemaker on the development of my machine learning model and I'm trying to build a lambda function to process the responses of a sagemaker labeling job. I already created my own lambda function but when I try to read the event contents I can see that the event dict is completely empty, so I'm not getting any data to read.<\/p>\n\n<p>I have already given enough permissions to the role of the lambda function. Including:\n- AmazonS3FullAccess.\n- AmazonSagemakerFullAccess.\n- AWSLambdaBasicExecutionRole<\/p>\n\n<p>I've tried using this code for the Post-annotation Lambda (adapted for python 3.6):<\/p>\n\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-custom-templates-step2-demo1.html#sms-custom-templates-step2-demo1-post-annotation\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-custom-templates-step2-demo1.html#sms-custom-templates-step2-demo1-post-annotation<\/a><\/p>\n\n<p>As well as this one in this git repository:<\/p>\n\n<p><a href=\"https:\/\/github.com\/aws-samples\/aws-sagemaker-ground-truth-recipe\/blob\/master\/aws_sagemaker_ground_truth_sample_lambda\/annotation_consolidation_lambda.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/aws-sagemaker-ground-truth-recipe\/blob\/master\/aws_sagemaker_ground_truth_sample_lambda\/annotation_consolidation_lambda.py<\/a><\/p>\n\n<p>But none of them seemed to work.<\/p>\n\n<p>For creating the labeling job I'm using boto3's functions for sagemaker:\n<a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.create_labeling_job\" rel=\"nofollow noreferrer\">https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.create_labeling_job<\/a><\/p>\n\n<p>This is the code i have for creating the labeling job:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>def create_labeling_job(client,bucket_name ,labeling_job_name, manifest_uri, output_path):\n\n    print(\"Creating labeling job with name: %s\"%(labeling_job_name))\n\n    response = client.create_labeling_job(\n        LabelingJobName=labeling_job_name,\n        LabelAttributeName='annotations',\n        InputConfig={\n            'DataSource': {\n                'S3DataSource': {\n                    'ManifestS3Uri': manifest_uri\n                }\n            },\n            'DataAttributes': {\n                'ContentClassifiers': [\n                    'FreeOfAdultContent',\n                ]\n            }\n        },\n        OutputConfig={\n            'S3OutputPath': output_path\n        },\n        RoleArn='arn:aws:myrolearn',\n        LabelCategoryConfigS3Uri='s3:\/\/'+bucket_name+'\/config.json',\n        StoppingConditions={\n            'MaxPercentageOfInputDatasetLabeled': 100,\n        },\n        LabelingJobAlgorithmsConfig={\n            'LabelingJobAlgorithmSpecificationArn': 'arn:image-classification'\n        },\n        HumanTaskConfig={\n            'WorkteamArn': 'arn:my-private-workforce-arn',\n            'UiConfig': {\n                'UiTemplateS3Uri':'s3:\/\/'+bucket_name+'\/templatefile'\n            },\n            'PreHumanTaskLambdaArn': 'arn:aws:lambda:us-east-1:432418664414:function:PRE-BoundingBox',\n            'TaskTitle': 'Title',\n            'TaskDescription': 'Description',\n            'NumberOfHumanWorkersPerDataObject': 1,\n            'TaskTimeLimitInSeconds': 600,\n            'AnnotationConsolidationConfig': {\n                'AnnotationConsolidationLambdaArn': 'arn:aws:my-custom-post-annotation-lambda'\n            }\n        }\n    )\n\n    return response\n<\/code><\/pre>\n\n<p>And this is the one i have for the lambda function:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>    print(\"Received event: \" + json.dumps(event, indent=2))\n    print(\"event: %s\"%(event))\n    print(\"context: %s\"%(context))\n    print(\"event headers: %s\"%(event[\"headers\"]))\n\n    parsed_url = urlparse(event['payload']['s3Uri']);\n    print(\"parsed_url: \",parsed_url)\n\n    labeling_job_arn = event[\"labelingJobArn\"]\n    label_attribute_name = event[\"labelAttributeName\"]\n\n    label_categories = None\n    if \"label_categories\" in event:\n        label_categories = event[\"labelCategories\"]\n        print(\" Label Categories are : \" + label_categories)\n\n    payload = event[\"payload\"]\n    role_arn = event[\"roleArn\"]\n\n    output_config = None # Output s3 location. You can choose to write your annotation to this location\n    if \"outputConfig\" in event:\n        output_config = event[\"outputConfig\"]\n\n    # If you specified a KMS key in your labeling job, you can use the key to write\n    # consolidated_output to s3 location specified in outputConfig.\n    kms_key_id = None\n    if \"kmsKeyId\" in event:\n        kms_key_id = event[\"kmsKeyId\"]\n\n    # Create s3 client object\n    s3_client = S3Client(role_arn, kms_key_id)\n\n    # Perform consolidation\n    return do_consolidation(labeling_job_arn, payload, label_attribute_name, s3_client)\n<\/code><\/pre>\n\n<p>I've tried debugging the event object with:<\/p>\n\n<pre><code>    print(\"Received event: \" + json.dumps(event, indent=2))\n<\/code><\/pre>\n\n<p>But it just prints an empty dictionary: <code>Received event: {}<\/code><\/p>\n\n<p>I expect the output to be something like:<\/p>\n\n<pre><code>    #Content of an example event:\n    {\n        \"version\": \"2018-10-16\",\n        \"labelingJobArn\": &lt;labelingJobArn&gt;,\n        \"labelCategories\": [&lt;string&gt;],  # If you created labeling job using aws console, labelCategories will be null\n        \"labelAttributeName\": &lt;string&gt;,\n        \"roleArn\" : \"string\",\n        \"payload\": {\n            \"s3Uri\": &lt;string&gt;\n        }\n        \"outputConfig\":\"s3:\/\/&lt;consolidated_output configured for labeling job&gt;\"\n    }\n<\/code><\/pre>\n\n<p>Lastly, when I try yo get the labeling job ARN with:<\/p>\n\n<pre><code>    labeling_job_arn = event[\"labelingJobArn\"]\n<\/code><\/pre>\n\n<p>I just get a KeyError (which makes sense because the dictionary is empty).<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2019-07-30 13:51:41.657000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2019-12-04 12:23:09.053000 UTC",
        "Question_score":3,
        "Question_tags":"python|amazon-web-services|aws-lambda|python-3.6|amazon-sagemaker",
        "Question_view_count":846,
        "Owner_creation_date":"2018-05-04 16:04:40.547000 UTC",
        "Owner_last_access_date":"2022-09-05 11:01:25.780000 UTC",
        "Owner_location":"Madrid, Spain",
        "Owner_reputation":81,
        "Owner_up_votes":54,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Answer_body":"<p>I found the problem, I needed to add the ARN of the role used by my Lamda function as a Trusted Entity on the Role used for the Sagemaker Labeling Job.<\/p>\n\n<p>I just went to <code>Roles &gt; MySagemakerExecutionRole &gt; Trust Relationships<\/code> and added:<\/p>\n\n<pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"AWS\": [\n          \"arn:aws:iam::xxxxxxxxx:role\/My-Lambda-Role\",\n           ...\n        ],\n        \"Service\": [\n          \"lambda.amazonaws.com\",\n          \"sagemaker.amazonaws.com\",\n           ...\n        ]\n      },\n      \"Action\": \"sts:AssumeRole\"\n    }\n  ]\n}\n<\/code><\/pre>\n\n<p>This made it work for me.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2019-08-02 10:28:09.013000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":71185505,
        "Question_title":"XGBClassifer, when de-serialized, gives 'XGBModel' object has no attribute 'enable_categorical'",
        "Question_body":"<p>I have a serialized XGBClassifier object, trained and generated using xgboost=1.5.2.<\/p>\n<pre><code>XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=0.5,\n              colsample_bynode=1, colsample_bytree=0.30140958911801474,\n              eval_metric='logloss', gamma=0.1203484640861413, gpu_id=-1,\n              importance_type='gain', interaction_constraints='',\n              learning_rate=0.1, max_bin=368, max_delta_step=0, max_depth=6,\n              min_child_weight=1, missing=nan, monotone_constraints='()',\n              n_estimators=100, n_jobs=6, num_parallel_tree=1, random_state=42,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n              single_precision_histogram=True, subsample=0.976171515775659,\n              tree_method='gpu_hist', use_label_encoder=False,\n              validate_parameters=1, verbosity=None)\n<\/code><\/pre>\n<p>I load the object using:<\/p>\n<pre><code>clf_model = joblib.load(model_path)\n<\/code><\/pre>\n<p>I want to use the object to predict on some data I am using Azure environment which also has xgboost=1.5.2. but it gives error:<\/p>\n<pre><code>File &quot;score.py&quot;, line 78, in score_execution\n[stderr]    clf_preds = clf_model.predict(clf_data_transformed)\n[stderr]  File &quot;\/opt\/miniconda\/lib\/python3.8\/site-packages\/xgboost\/sklearn.py&quot;, line 1284, in predict\n[stderr]    class_probs = super().predict(\n[stderr]  File &quot;\/opt\/miniconda\/lib\/python3.8\/site-packages\/xgboost\/sklearn.py&quot;, line 879, in predict\n[stderr]    if self._can_use_inplace_predict():\n[stderr]  File &quot;\/opt\/miniconda\/lib\/python3.8\/site-packages\/xgboost\/sklearn.py&quot;, line 811, in _can_use_inplace_predict\n[stderr]    predictor = self.get_params().get(&quot;predictor&quot;, None)\n[stderr]  File &quot;\/opt\/miniconda\/lib\/python3.8\/site-packages\/xgboost\/sklearn.py&quot;, line 505, in get_params\n[stderr]    params.update(cp.__class__.get_params(cp, deep))\n[stderr]  File &quot;\/opt\/miniconda\/lib\/python3.8\/site-packages\/xgboost\/sklearn.py&quot;, line 502, in get_params\n[stderr]    params = super().get_params(deep)\n[stderr]  File &quot;\/opt\/miniconda\/lib\/python3.8\/site-packages\/sklearn\/base.py&quot;, line 210, in get_params\n[stderr]    value = getattr(self, key)\n[stderr]AttributeError: 'XGBModel' object has no attribute 'enable_categorical'\n<\/code><\/pre>\n<p>We have same version in pipelines that produce\/serialize the model and in the pipeline that deserialize the model to predict on new data.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-02-19 13:36:38.680000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"python|azure|azure-machine-learning-service",
        "Question_view_count":358,
        "Owner_creation_date":"2015-09-23 14:11:04.707000 UTC",
        "Owner_last_access_date":"2022-09-23 08:54:35.743000 UTC",
        "Owner_location":"Sweden",
        "Owner_reputation":644,
        "Owner_up_votes":33,
        "Owner_down_votes":1,
        "Owner_views":126,
        "Answer_body":"<p>Here are some possible solutions :<\/p>\n<ul>\n<li>Save the model in some other way, e.g. the JSON specified here <a href=\"https:\/\/xgboost.readthedocs.io\/en\/latest\/tutorials\/saving_model.html\" rel=\"nofollow noreferrer\">https:\/\/xgboost.readthedocs.io\/en\/latest\/tutorials\/saving_model.html<\/a><\/li>\n<li>Limit the allowed range of xgboost versions to those that are known to work with our model. This could lead to issues in the future, for example if the aging version of xgboost we require is no longer supported by newer versions of Python.<\/li>\n<li>Using <code>save_model<\/code> to save in JSON is worth a shot to try.<\/li>\n<\/ul>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-02-21 07:06:52.643000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_valid_tags":[
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":58292566,
        "Question_title":"How can I use a list of files as the training set on Sagemaker with Tensorflow?",
        "Question_body":"<p>I have several million images in my training folder and want to specify a subset of them for training - the way to do this seems to be with a manifest file as described here.<\/p>\n\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/augmented-manifest.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/augmented-manifest.html<\/a><\/p>\n\n<p>But this seems to be geared towards labelled data. How can I start a sagemaker training job using sagemaker's Tensorflow <code>estimator.fit<\/code> with a list of files instead of the entire directory as input?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-10-08 19:03:51.850000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"python|tensorflow|computer-vision|amazon-sagemaker",
        "Question_view_count":809,
        "Owner_creation_date":"2014-11-17 02:56:57.423000 UTC",
        "Owner_last_access_date":"2022-09-20 17:54:23.113000 UTC",
        "Owner_location":"Gensokyo",
        "Owner_reputation":880,
        "Owner_up_votes":211,
        "Owner_down_votes":2,
        "Owner_views":111,
        "Answer_body":"<p>You can use an input type pipe parameter like so: <\/p>\n\n<pre><code>hyperparameters = {'save_checkpoints_secs':None,\n                   'save_checkpoints_steps':1000}\n\ntf_estimator = TensorFlow(entry_point='.\/my-training-file', role=role,\n                          training_steps=5100, evaluation_steps=100,\n                          train_instance_count=1, train_instance_type='ml.p3.2xlarge',\n                          input_mode = 'Pipe',\n                          train_volume_size=300, output_path = 's3:\/\/sagemaker-pocs\/test-carlsoa\/kepler\/model',\n                          framework_version = '1.12.0', hyperparameters=hyperparameters, checkpoint_path = None)\n<\/code><\/pre>\n\n<p>And create the manifest file pipe as an input:<\/p>\n\n<pre><code>train_data = sagemaker.session.s3_input('s3:\/\/sagemaker-pocs\/test-carlsoa\/manifest.json',\n                                        distribution='FullyReplicated',\n                                        content_type='image\/jpeg',\n                                        s3_data_type='ManifestFile',\n                                        attribute_names=['source-ref']) \n                                        #attribute_names=['source-ref', 'annotations']) \ndata_channels = {'train': train_data}\n<\/code><\/pre>\n\n<p>Note that you can use ManifestFile or AugmentedManifestFile depending on whether you have extra data or labels to provide. Now you can use data_channels as the input to the tf estimator:<\/p>\n\n<p><code>tf_estimator.fit(inputs=data_channels, logs=True)<\/code><\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2019-10-09 19:27:22.910000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":47946790,
        "Question_title":"AWS Sagemaker - Access Denied",
        "Question_body":"<p>I'm working through some of the example Sagemaker notebooks, and I receive the following Access Denied error when trying to run the linear_time_series_forecast example:<\/p>\n\n<p>ValueError: Error training linear-learner-2017-12-21-15-29-34-676: Failed Reason: ClientError: Data download failed:AccessDenied (403): Access Denied<\/p>\n\n<p>I can manually download and upload from my S3 bucket using the AWS command line interface, but the Jupyter notebook fails. <\/p>\n\n<p>Can someone please help me with this error?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2017-12-22 20:02:48.297000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":3,
        "Question_tags":"amazon-web-services|amazon-s3|amazon-sagemaker",
        "Question_view_count":10206,
        "Owner_creation_date":"2017-05-25 00:08:25.787000 UTC",
        "Owner_last_access_date":"2018-12-27 20:23:47.160000 UTC",
        "Owner_location":null,
        "Owner_reputation":55,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Answer_body":"<p>Thanks for using Amazon SageMaker!<\/p>\n\n<p>Looks like this question was also answered on the AWS Forums: <a href=\"https:\/\/forums.aws.amazon.com\/thread.jspa?threadID=270054&amp;tstart=0\" rel=\"nofollow noreferrer\">https:\/\/forums.aws.amazon.com\/thread.jspa?threadID=270054&amp;tstart=0<\/a><\/p>\n\n<p>The IAM Role referenced by<\/p>\n\n<blockquote>\n  <p>role = get_execution_role()<\/p>\n<\/blockquote>\n\n<p>needs to have a policy attached to it that grants S3:GetObject permission on the S3 bucket holding your training data.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2017-12-26 23:55:00.483000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":66348756,
        "Question_title":"Input of type 'Unknown' is not supported. Supported types: [azureml.data.tabular_dataset.TabularDataset, azureml.pipeline.core",
        "Question_body":"<p>According to this documentation:\n<a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-train-automl-client\/azureml.train.automl.automlconfig.automlconfig?view=azure-ml-py\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-train-automl-client\/azureml.train.automl.automlconfig.automlconfig?view=azure-ml-py<\/a><\/p>\n<p>training_data can be either a dataframe or a dataset.<\/p>\n<p>However when I use a dataframe I get this error:<\/p>\n<pre><code>\nConfigException: ConfigException:\n    Message: Input of type 'Unknown' is not supported. Supported types: [azureml.data.tabular_dataset.TabularDataset, azureml.pipeline.core.pipeline_output_dataset.PipelineOutputTabularDataset]\n    InnerException: None\n    ErrorResponse \n{\n    &quot;error&quot;: {\n        &quot;code&quot;: &quot;UserError&quot;,\n        &quot;message&quot;: &quot;Input of type 'Unknown' is not supported. Supported types: [azureml.data.tabular_dataset.TabularDataset, azureml.pipeline.core.pipeline_output_dataset.PipelineOutputTabularDataset]&quot;,\n        &quot;details_uri&quot;: &quot;https:\/\/aka.ms\/AutoMLConfig&quot;,\n        &quot;target&quot;: &quot;training_data&quot;,\n        &quot;inner_error&quot;: {\n            &quot;code&quot;: &quot;BadArgument&quot;,\n            &quot;inner_error&quot;: {\n                &quot;code&quot;: &quot;ArgumentInvalid&quot;,\n                &quot;inner_error&quot;: {\n                    &quot;code&quot;: &quot;InvalidInputDatatype&quot;\n                }\n            }\n        }\n    }\n}\n<\/code><\/pre>\n<p>My code is really simple:<\/p>\n<pre><code>\nclient = CosmosClient(HOST, MASTER_KEY)\ndatabase = client.get_database_client(database=DATABASE_ID)\ncontainer = database.get_container_client(CONTAINER_ID)\n\nitem_list = list(container.read_all_items(max_item_count=10))\ndf = pd.DataFrame(item_list)\n\nfrom azureml.core.workspace import Workspace\nws = Workspace.from_config()\n\nfrom azureml.automl.core.forecasting_parameters import ForecastingParameters\n\nforecasting_parameters = ForecastingParameters(time_column_name='EventEnqueuedUtcTime', \n                                               forecast_horizon=50,\n                                               time_series_id_column_names=[&quot;eui&quot;],\n                                               freq='H',\n                                               target_lags='auto',\n                                               target_rolling_window_size=10)\n\nfrom azureml.core.workspace import Workspace\nfrom azureml.core.experiment import Experiment\nfrom azureml.train.automl import AutoMLConfig\nfrom azureml.core.compute import ComputeTarget, AmlCompute\nimport logging\n\namlcompute_cluster_name = &quot;computecluster&quot;\ncompute_target = ComputeTarget(workspace=ws, name=amlcompute_cluster_name)\nexperiment_name = 'iot-forecast'\n\nexperiment = Experiment(ws, experiment_name)\n\nautoml_config = AutoMLConfig(task='forecasting',\n                             primary_metric='normalized_root_mean_squared_error',\n                             experiment_timeout_minutes=100,\n                             enable_early_stopping=True,\n                             training_data=df,\n                             compute_target = compute_target,\n                             label_column_name='TempC_DS',\n                             n_cross_validations=5,\n                             enable_ensembling=False,\n                             verbosity=logging.INFO,\n                             forecasting_parameters=forecasting_parameters)\n\nremote_run = experiment.submit(automl_config, show_output=True)\n<\/code><\/pre>\n<p>what am I missing here?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-02-24 10:12:24.217000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"python|python-3.x|pandas|machine-learning|azure-machine-learning-studio",
        "Question_view_count":315,
        "Owner_creation_date":"2011-04-05 19:05:03.093000 UTC",
        "Owner_last_access_date":"2022-09-16 12:42:27.473000 UTC",
        "Owner_location":"Brussels, B\u00e9lgica",
        "Owner_reputation":30340,
        "Owner_up_votes":1667,
        "Owner_down_votes":79,
        "Owner_views":2937,
        "Answer_body":"<p>Looks like you are trying to run the experiment remotely, AFAIK and as per the doc <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-configure-auto-train#data-source-and-format?WT.mc_id=AI-MVP-5003930\" rel=\"nofollow noreferrer\">here<\/a> :<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/CXYyE.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/CXYyE.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>You could refer this article to understand creating <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-register-datasets?WT.mc_id=AI-MVP-5003930\" rel=\"nofollow noreferrer\">Azure ML TabularDataset<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-03-03 05:48:37.020000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":72874937,
        "Question_title":"Is it possible set up an endpoint for a model I created in AWS SageMaker without using the SageMaker SDK",
        "Question_body":"<p>I've created my own model on a AWS SageMaker instance, with my own training and inference loops. I want to deploy it so that I can call the model for inference from AWS Lambda.<\/p>\n<p>I didn't use the SageMaker package to develop at all, but every tutorial (here is <a href=\"https:\/\/towardsdatascience.com\/using-aws-sagemaker-and-lambda-function-to-build-a-serverless-ml-platform-f14b3ec5854a%3E\" rel=\"nofollow noreferrer\">one<\/a>) I've looked at does so.<\/p>\n<p>How do I create an endpoint without using the SageMaker package.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-07-05 20:01:39.687000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|aws-lambda|amazon-sagemaker|endpoint",
        "Question_view_count":66,
        "Owner_creation_date":"2022-07-05 19:52:34.840000 UTC",
        "Owner_last_access_date":"2022-09-21 21:49:37.727000 UTC",
        "Owner_location":null,
        "Owner_reputation":15,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Answer_body":"<p>You can use the boto3 library to do this.<\/p>\n<p>Here is an example of pseudo code for this -<\/p>\n<pre><code>import boto3\nsm_client = boto3.client('sagemaker')\ncreate_model_respose = sm_client.create_model(ModelName=model_name, ExecutionRoleArn=role, Containers=[container] )\n\ncreate_endpoint_config_response = sm_client.create_endpoint_config(EndpointConfigName=endpoint_config_name)\n\ncreate_endpoint_response = sm_client.create_endpoint(EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name)\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-07-05 20:42:25.440000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":72729259,
        "Question_title":"wandb.Table raises error: AssertionError: columns argument expects a `list` object",
        "Question_body":"<p>I'm very beginner with wandb , so this is very basic question.\nI have dataframe which has my x features and y values.\nI'm tryin to follow <a href=\"https:\/\/docs.wandb.ai\/examples\" rel=\"nofollow noreferrer\">this tutorial<\/a>  to train model from my pandas dataframe . However, when I try to create wandb table from my pandas dataframe, I get an error:<\/p>\n<pre><code>\nwandb.init(project='my-xgb', config={'lr': 0.01})\n\n#the log didn't work  so I haven't run it at the moment (the log 'loss') \n#wandb.log({'loss': loss, ...})\n\n\n# Create a W&amp;B Table with your pandas dataframe\ntable = wandb.Table(df1)\n<\/code><\/pre>\n<blockquote>\n<p>AssertionError: columns argument expects a <code>list<\/code> object<\/p>\n<\/blockquote>\n<p>I have no idea why is this happen, and why it excpect a list. In the tutorial it doesn't look like the dataframe is list.<\/p>\n<p>My end goal - to be able to create wandb table.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-06-23 11:18:13.280000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|pandas|dataframe|wandb",
        "Question_view_count":63,
        "Owner_creation_date":"2019-10-28 09:51:58.027000 UTC",
        "Owner_last_access_date":"2022-09-20 12:29:28.963000 UTC",
        "Owner_location":"Israel",
        "Owner_reputation":1387,
        "Owner_up_votes":955,
        "Owner_down_votes":16,
        "Owner_views":224,
        "Answer_body":"<p><strong>Short answer<\/strong>: <code>table = wandb.Table(dataframe=my_df)<\/code>.<\/p>\n<p>The explanation of your specific case is at the bottom.<\/p>\n<hr \/>\n<p><strong>Minimal example<\/strong> of using <code>wandb.Table<\/code> with a DataFrame:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import wandb\nimport pandas as pd\n\niris_path = 'https:\/\/raw.githubusercontent.com\/mwaskom\/seaborn-data\/master\/iris.csv'\niris = pd.read_csv(iris_path)\ntable = wandb.Table(dataframe=iris)\nwandb.log({'dataframe_in_table': table})\n<\/code><\/pre>\n<p>(Here the dataset is called the Iris dataset that consists of &quot;3 different types of irises\u2019 (Setosa, Versicolour, and Virginica) petal and sepal length, stored in a 150x4 numpy.ndarray&quot;)<\/p>\n<p>There are two ways of creating W&amp;B <code>Table<\/code>s according to <a href=\"https:\/\/docs.wandb.ai\/guides\/data-vis\/log-tables#create-tables\" rel=\"nofollow noreferrer\">the official documentation<\/a>:<\/p>\n<ul>\n<li><strong>List of Rows<\/strong>: Log named columns and rows of data. For example: <code>wandb.Table(columns=[&quot;a&quot;, &quot;b&quot;, &quot;c&quot;], data=[[&quot;1a&quot;, &quot;1b&quot;, &quot;1c&quot;], [&quot;2a&quot;, &quot;2b&quot;, &quot;2c&quot;]])<\/code> generates a table with two rows and three columns.<\/li>\n<li><strong>Pandas DataFrame<\/strong>: Log a DataFrame using <code>wandb.Table(dataframe=my_df)<\/code>. Column names will be extracted from the DataFrame.<\/li>\n<\/ul>\n<hr \/>\n<p><strong>Explanation<\/strong>: Why <code>table = wandb.Table(my_df)<\/code> gives error &quot;columns argument expects a <code>list<\/code> object&quot;? Because <code>wandb.Table<\/code>'s init function looks like this:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>def __init__(\n        self,\n        columns=None,\n        data=None,\n        rows=None,\n        dataframe=None,\n        dtype=None,\n        optional=True,\n        allow_mixed_types=False,\n    ):\n<\/code><\/pre>\n<p>If one passes a DataFrame without telling it's a DataFrame, <code>wandb.Table<\/code> will assume the argument is <code>columns<\/code>.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2022-06-23 11:28:00.023000 UTC",
        "Answer_last_edit_date":"2022-06-23 12:04:19.240000 UTC",
        "Answer_score":2.0,
        "Question_valid_tags":[
            "wandb"
        ]
    },
    {
        "Question_id":62734059,
        "Question_title":"How to update pandas version in SageMaker notebook terminal?",
        "Question_body":"<p>I'm using SageMaker JupyterLab, but I found pandas is out of date, what's the process of updating it?<\/p>\n<p>I tried this:\nIn terminal:<\/p>\n<pre><code>cd SageMaker\nconda update pandas\n<\/code><\/pre>\n<p>The package has been updated to 1.0.5\nbut when I use this command in SageMaker instance:<\/p>\n<pre><code>import pandas\nprint(pandas,__version__)\n\nreturn:\n0.24.2\n<\/code><\/pre>\n<p>It didn't work at all, can someone help me? Thanks.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-07-04 20:22:39.730000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-07-04 20:38:11.990000 UTC",
        "Question_score":1,
        "Question_tags":"pandas|amazon-web-services|anaconda|amazon-sagemaker",
        "Question_view_count":1158,
        "Owner_creation_date":"2018-10-30 17:35:56.270000 UTC",
        "Owner_last_access_date":"2022-09-22 19:30:36.883000 UTC",
        "Owner_location":"United Kingdom",
        "Owner_reputation":2385,
        "Owner_up_votes":1007,
        "Owner_down_votes":16,
        "Owner_views":585,
        "Answer_body":"<p>If you want to perform any kind of upgrades or modification to the kernel of the notebook you can do this at launch by using <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/notebook-lifecycle-config.html\" rel=\"nofollow noreferrer\">lifecycle configuration<\/a>.<\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_date":"2020-07-04 20:43:32.640000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":49365900,
        "Question_title":"What's a better way to load a file using boto? (getting filename too long error)",
        "Question_body":"<p>So I'm trying to use <code>tf.contrib.learn.preprocessing.VocabularyProcessor.restore()<\/code> to restore a vocabulary file from an S3 bucket. First, I tried to get the path name to the bucket to use in <code>.restore()<\/code> and I kept getting 'object doesn't exist' error. Afterwards, upon further research, I found a method people use to load text files and JSON files and applied the same method here:<\/p>\n\n<pre><code>obj = s3.Object(BUCKET_NAME, KEY).get()['Body'].read()\nvocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor.restore(obj)\n<\/code><\/pre>\n\n<p>This worked for a while until the contents of file increased and eventually got a 'File name too long' error. Is there a better way to load and restore a file from an S3 bucket? <\/p>\n\n<p>By the way, I tested this out locally on my machine and it works perfectly fine since there it just needs to take the path to the file, not the entire contents of the file. <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2018-03-19 14:55:11.783000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"tensorflow|amazon-s3|boto|amazon-sagemaker",
        "Question_view_count":532,
        "Owner_creation_date":"2018-03-19 14:33:55.783000 UTC",
        "Owner_last_access_date":"2020-08-27 19:33:25.913000 UTC",
        "Owner_location":null,
        "Owner_reputation":210,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":12,
        "Answer_body":"<p>It looks like you\u2019re passing in the actual contents of the file as the file name?<\/p>\n\n<p>I think you\u2019ll need to download the object from S3 to a tmp file and pass the path to that file into restore.<\/p>\n\n<p>Try using the method here: <a href=\"http:\/\/boto3.readthedocs.io\/en\/latest\/reference\/services\/s3.html#S3.Object.download_file\" rel=\"nofollow noreferrer\">http:\/\/boto3.readthedocs.io\/en\/latest\/reference\/services\/s3.html#S3.Object.download_file<\/a><\/p>\n\n<p>Update:\nI went through the code here: <a href=\"https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/tensorflow\/contrib\/learn\/python\/learn\/preprocessing\/text.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/tensorflow\/contrib\/learn\/python\/learn\/preprocessing\/text.py<\/a> and it looks like this just saves a pickle so you can really easily just import pickle and call the following:<\/p>\n\n<pre><code>import pickle\nobj = s3.Object(BUCKET_NAME, KEY).get()['Body']\nvocab_processor = pickle.loads(obj.read())\n<\/code><\/pre>\n\n<p>Hopefully that works?<\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_date":"2018-03-19 18:53:49.850000 UTC",
        "Answer_last_edit_date":"2018-03-26 09:34:22.430000 UTC",
        "Answer_score":1.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":60638587,
        "Question_title":"How to get insights in exceptions and logging of AzureML endpoint deployment",
        "Question_body":"<p>Because of a faulty score.py file in my InferenceConfig, a Model.Deploy failed to Azure Machine Learning, using ACI.  I wanted to create the endpoint in the cloud, but the only state I can see in the portal is Unhealthy.  My local script to deploy the model (using ) keeps running, until it times out. (using the <code>service.wait_for_deployment(show_output=True)<\/code>statement).<\/p>\n\n<p>Is there an option to get more insights in the actual reason\/error message of the deployment turning \"Unhealthy\"?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-03-11 14:39:32.180000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-03-14 19:54:48.010000 UTC",
        "Question_score":3,
        "Question_tags":"azure-machine-learning-service",
        "Question_view_count":364,
        "Owner_creation_date":"2013-02-12 07:50:30.743000 UTC",
        "Owner_last_access_date":"2022-09-21 18:28:12.907000 UTC",
        "Owner_location":"Belgium",
        "Owner_reputation":2947,
        "Owner_up_votes":297,
        "Owner_down_votes":16,
        "Owner_views":355,
        "Answer_body":"<p>Usually the timeout is caused by an error in init() function in scoring script. You can get the detailed logs using <code>print(service.get_logs())<\/code> to find the Python error.<\/p>\n\n<p>For more comprehensive troubleshooting guide, see:<\/p>\n\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-troubleshoot-deployment\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-troubleshoot-deployment<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-03-13 21:02:44.177000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_valid_tags":[
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":66814885,
        "Question_title":"Serve online learning models with mlflow",
        "Question_body":"<p>It is not clear to me if one could use mlflow to serve a model that is evolving continuously based on its previous predictions.<\/p>\n<p>I need to be able to query a model in order to make a prediction on a sample of data which is the basic use of mlflow serve. However I also want the model to be updated internaly now that it has seen new data.<\/p>\n<p>Is it possible or does it need a FR ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-03-26 10:12:57.110000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"python-3.x|mlflow",
        "Question_view_count":360,
        "Owner_creation_date":"2018-01-05 12:55:59.093000 UTC",
        "Owner_last_access_date":"2021-09-03 19:38:29.597000 UTC",
        "Owner_location":"Fairbanks, AK, United States",
        "Owner_reputation":76,
        "Owner_up_votes":9,
        "Owner_down_votes":0,
        "Owner_views":14,
        "Answer_body":"<p>I think that you should be able to do that by implementing the custom python model or custom flavor, as it's described in the <a href=\"https:\/\/mlflow.org\/docs\/latest\/models.html#model-customization\" rel=\"nofollow noreferrer\">documentation<\/a>.  In this case you need to create a class that is inherited from <code>mlflow.pyfunc.PythonModel<\/code>, and implement the <code>predict<\/code> method, and inside that method you're free to do anything.  Here is just simple example from documentation:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>class AddN(mlflow.pyfunc.PythonModel):\n\n    def __init__(self, n):\n        self.n = n\n\n    def predict(self, context, model_input):\n        return model_input.apply(lambda column: column + self.n)\n<\/code><\/pre>\n<p>and this model is then could be saved &amp; loaded again just as normal models:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># Construct and save the model\nmodel_path = &quot;add_n_model&quot;\nadd5_model = AddN(n=5)\nmlflow.pyfunc.save_model(path=model_path, python_model=add5_model)\n\n# Load the model in `python_function` format\nloaded_model = mlflow.pyfunc.load_model(model_path)\n<\/code><\/pre>",
        "Answer_comment_count":3.0,
        "Answer_creation_date":"2021-04-11 07:04:00.630000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_valid_tags":[
            "mlflow"
        ]
    },
    {
        "Question_id":60667610,
        "Question_title":"How to deploy mlflow model with data preprocessing(text data)",
        "Question_body":"<p>I have developed keras text classification model. I have preprocessed data(tokenization). I have logged trained model successfully(mlflow.keras.log_model). I have served model using mlflow serve. Now while doing prediction on text data I need to do preprocessing using same tokenizer object used for training.\nHow to preprocess test data and get predictions from served model.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-03-13 09:08:37.703000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":3,
        "Question_tags":"mlflow",
        "Question_view_count":1996,
        "Owner_creation_date":"2017-06-26 09:55:36.987000 UTC",
        "Owner_last_access_date":"2021-03-05 03:19:48.693000 UTC",
        "Owner_location":null,
        "Owner_reputation":33,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Answer_body":"<p>You can log a custom python model: \n<a href=\"https:\/\/www.mlflow.org\/docs\/latest\/models.html#custom-python-models\" rel=\"nofollow noreferrer\">https:\/\/www.mlflow.org\/docs\/latest\/models.html#custom-python-models<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-03-18 17:25:58.667000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "mlflow"
        ]
    },
    {
        "Question_id":72760982,
        "Question_title":"How to use Tensorboard within a notebook running on Amazon SageMaker Studio Lab?",
        "Question_body":"<p>I have a Jupyter notebook running within an <code>Amazon SageMaker Studio Lab<\/code> (<a href=\"https:\/\/studiolab.sagemaker.aws\/\" rel=\"nofollow noreferrer\">https:\/\/studiolab.sagemaker.aws\/<\/a>) environment, and I want to use Tensordboard to monitor my model's performance inside the notebook.<\/p>\n<p>I have used the following commands to set up the Tensorboard:<\/p>\n<pre><code>%load_ext tensorboard\n# tb_log_dir variable holds the path to the log directory\n%tensorboard --logdir tb_log_dir\n<\/code><\/pre>\n<p>But nothing shows up in the output of the cell where I execute the commands. See:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/TwbTk.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/TwbTk.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>The two buttons shown in the picture are not responding, BTW.<\/p>\n<p>How to solve this problem? Any suggestions would be appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-06-26 10:59:50.023000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-06-26 11:36:18.470000 UTC",
        "Question_score":1,
        "Question_tags":"jupyter-notebook|amazon-sagemaker|tensorboard",
        "Question_view_count":121,
        "Owner_creation_date":"2015-01-03 12:04:10.807000 UTC",
        "Owner_last_access_date":"2022-09-22 10:10:59.487000 UTC",
        "Owner_location":"Trondheim, Norway",
        "Owner_reputation":720,
        "Owner_up_votes":129,
        "Owner_down_votes":0,
        "Owner_views":126,
        "Answer_body":"<p>I would try the canonical way to use tensorboard in AWS Sagemaker, it should be supported also by Studio Lab, it is described <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/studio-tensorboard.html\" rel=\"nofollow noreferrer\">here<\/a>. Basically install tensorboard and using the <code>EFS_PATH_LOG_DIR<\/code> launch tensorboard using the embedded console (you can do the following also from a cell):<\/p>\n<pre><code>pip install tensorboard\ntensorboard --logdir &lt;EFS_PATH_LOG_DIR&gt;\n<\/code><\/pre>\n<p>Be careful with the EFS_PATH_LOG_DIR, be sure this folder is valida path from the location you are, for example by default you are located in <code>studio-lab-user\/sagemaker-studiolab-notebooks\/<\/code> so the proper command would be <code>!tensorboard --logdir logs\/fit<\/code>.<\/p>\n<p>Then open a browser to:<\/p>\n<pre><code>https:\/\/&lt;YOUR URL&gt;\/studiolab\/default\/jupyter\/proxy\/6006\/\n<\/code><\/pre>",
        "Answer_comment_count":5.0,
        "Answer_creation_date":"2022-06-28 09:54:48.380000 UTC",
        "Answer_last_edit_date":"2022-07-07 08:05:21.617000 UTC",
        "Answer_score":2.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":65619881,
        "Question_title":"SageMaker tuning job cannot use P2 or P3 instances",
        "Question_body":"<p>I am trying to use AWS SageMaker Hyperparameter tuning job. I can use C5 instance, however, when trying to use either p2 or p3 I get this error.<\/p>\n<pre><code>{{botocore.errorfactory.ResourceLimitExceeded: An error occurred (ResourceLimitExceeded) when calling the CreateHyperParameterTuningJob operation: The account-level service limit 'ml.p3.2xlarge for training job usage' is 2 Instances, with current utilization of 0 Instances and a request delta of 5 Instances. Please contact AWS support to request an increase for this limit.\n}}\n<\/code><\/pre>\n<p>Does anybody have idea about it?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2021-01-07 21:01:17.393000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-ec2|amazon-sagemaker|auto-tuning",
        "Question_view_count":209,
        "Owner_creation_date":"2017-05-24 14:33:14.673000 UTC",
        "Owner_last_access_date":"2022-09-19 19:44:51.417000 UTC",
        "Owner_location":null,
        "Owner_reputation":95,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":19,
        "Answer_body":"<p>There is a limitation in our account so we had to request for using the instances and increasing the available resource from AWS.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-02-18 19:48:18.377000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":50872338,
        "Question_title":"Could LUIS share entity definition among multiple models?",
        "Question_body":"<p>We have serval <a href=\"http:\/\/luis.ai\" rel=\"nofollow noreferrer\">LUIS<\/a> models that work fine and returns desired <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/cognitive-services\/luis\/luis-concept-intent\" rel=\"nofollow noreferrer\">Intents<\/a> and <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/cognitive-services\/luis\/luis-concept-entity-types\" rel=\"nofollow noreferrer\">Entities<\/a>.<\/p>\n\n<p>Models are separated based on content and target business domain so we do not want to merge them.\nStill there are some <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/cognitive-services\/luis\/luis-quickstart-intents-regex-entity\" rel=\"nofollow noreferrer\">Regex entities<\/a> that are the same in each of the model.<\/p>\n\n<hr>\n\n<p>If we'd like to have one Regex definition at one place could we eventually share such definition among multiple LUIS models?<\/p>\n\n<p>Right now we proceed with Ctrl+C and Ctrl+V<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2018-06-15 09:06:40.037000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2018-06-15 10:24:20.707000 UTC",
        "Question_score":1,
        "Question_tags":"azure|nlp|azure-language-understanding|azure-machine-learning-studio",
        "Question_view_count":64,
        "Owner_creation_date":"2017-01-05 20:46:30.770000 UTC",
        "Owner_last_access_date":"2019-11-29 14:56:23.547000 UTC",
        "Owner_location":"Prague-Prague 1, Czechia",
        "Owner_reputation":507,
        "Owner_up_votes":415,
        "Owner_down_votes":9,
        "Owner_views":53,
        "Answer_body":"<p>We could share the entire app by <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/cognitive-services\/luis\/luis-how-to-manage-versions\" rel=\"noreferrer\">cloning the app<\/a>. And the only way to use specific entities would be to delete the others.<\/p>\n<blockquote>\n<p>If we'd like to have one Regex definition at one place could we eventually share such definition among multiple LUIS models?<\/p>\n<p>Right now we proceed with Ctrl+C and Ctrl+V<\/p>\n<\/blockquote>\n<p>This is the only way to do it right now.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2018-06-15 21:41:54.963000 UTC",
        "Answer_last_edit_date":"2020-06-20 09:12:55.060000 UTC",
        "Answer_score":1.0,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":51335594,
        "Question_title":"Error with \"mlflow ui\" when trying to run it on MS Windows",
        "Question_body":"<p>When I run <code>mlflow ui<\/code> the following error occurred:<\/p>\n\n<pre><code>Traceback (most recent call last):\n  File \"c:\\anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"c:\\anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Anaconda3\\Scripts\\gunicorn.exe\\__main__.py\", line 5, in &lt;module&gt;\n  File \"c:\\anaconda3\\lib\\site-packages\\gunicorn\\app\\wsgiapp.py\", line 9, in &lt;module&gt;\n    from gunicorn.app.base import Application\n  File \"c:\\anaconda3\\lib\\site-packages\\gunicorn\\app\\base.py\", line 12, in &lt;module&gt;\n    from gunicorn import util\n  File \"c:\\anaconda3\\lib\\site-packages\\gunicorn\\util.py\", line 9, in &lt;module&gt;\n    import fcntl\nModuleNotFoundError: No module named 'fcntl'\nTraceback (most recent call last):\n  File \"c:\\anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"c:\\anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Anaconda3\\Scripts\\mlflow.exe\\__main__.py\", line 9, in &lt;module&gt;\n  File \"c:\\anaconda3\\lib\\site-packages\\click\\core.py\", line 722, in __call__\n    return self.main(*args, **kwargs)\n  File \"c:\\anaconda3\\lib\\site-packages\\click\\core.py\", line 697, in main\n    rv = self.invoke(ctx)\n  File \"c:\\anaconda3\\lib\\site-packages\\click\\core.py\", line 1066, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"c:\\anaconda3\\lib\\site-packages\\click\\core.py\", line 895, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"c:\\anaconda3\\lib\\site-packages\\click\\core.py\", line 535, in invoke\n    return callback(*args, **kwargs)\n  File \"c:\\anaconda3\\lib\\site-packages\\mlflow\\cli.py\", line 131, in ui\n    mlflow.server._run_server(file_store, file_store, host, port, 1)\n  File \"c:\\anaconda3\\lib\\site-packages\\mlflow\\server\\__init__.py\", line 48, in _run_server\n    env=env_map, stream_output=True)\n  File \"c:\\anaconda3\\lib\\site-packages\\mlflow\\utils\\process.py\", line 38, in exec_cmd\n    raise ShellCommandException(\"Non-zero exitcode: %s\" % (exit_code))\nmlflow.utils.process.ShellCommandException: Non-zero exitcode: 1\n<\/code><\/pre>\n\n<p>I used anaconda + python 3.6.5 and I installed git and set path with <code>C:\\Program Files\\Git\\bin\\git.exe<\/code> and <code>C:\\Program Files\\Git\\cmd<\/code>.<\/p>\n\n<p>I installed <code>mlflow<\/code> whit <code>pip install mlflow<\/code> and its version is 0.2.1.<\/p>\n\n<p>I set a variable with name <code>GIT_PYTHON_GIT_EXECUTABLE<\/code> and value <code>C:\\Program Files\\Git\\bin\\git.exe<\/code> in Environment Variables. <\/p>\n\n<p>How can I solve this?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_date":"2018-07-14 05:34:06.273000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2018-07-17 14:18:37.033000 UTC",
        "Question_score":1,
        "Question_tags":"python|windows|fcntl|mlflow",
        "Question_view_count":4688,
        "Owner_creation_date":"2011-06-20 06:54:08.513000 UTC",
        "Owner_last_access_date":"2022-09-24 14:46:49.627000 UTC",
        "Owner_location":null,
        "Owner_reputation":1177,
        "Owner_up_votes":24,
        "Owner_down_votes":0,
        "Owner_views":144,
        "Answer_body":"<p><a href=\"https:\/\/github.com\/databricks\/mlflow\" rel=\"nofollow noreferrer\">mlflow documentation<\/a> already says that <\/p>\n\n<blockquote>\n  <p>Note 2: We <strong>do not currently support running MLflow on Windows<\/strong>.\n  Despite this, we would appreciate any contributions to make MLflow\n  work better on Windows.<\/p>\n<\/blockquote>\n\n<p>You're hitting <code>fcntl<\/code> problem: it's not available on MS Windows platform because it's a \"wrapper\" around the <a href=\"http:\/\/man7.org\/linux\/man-pages\/man2\/fcntl.2.html\" rel=\"nofollow noreferrer\">fcntl function<\/a> that's available on POSIX-compatible systems. (See <a href=\"https:\/\/stackoverflow.com\/a\/1422436\/236007\">https:\/\/stackoverflow.com\/a\/1422436\/236007<\/a> for more details.)<\/p>\n\n<p>Solving this requires modifying the source code of mlflow accordingly. <\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2018-07-17 14:17:19.907000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_valid_tags":[
            "mlflow"
        ]
    },
    {
        "Question_id":67210677,
        "Question_title":"If I close my JupyterLab from notebook instance, would my code be gone?",
        "Question_body":"<p>I'm new to AWS and I'm trying out AWS Sagemaker. I'm currently doing my project which involves quite a long time to finish and I don't think I can finish it in a day. I'm worried if I close my JupyterLab of my notebook instance in SageMaker, my code will be gone. How do I save my code and cell run progress when using Sagemaker?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-04-22 09:36:23.027000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"python|amazon-web-services|tensorflow|amazon-sagemaker",
        "Question_view_count":305,
        "Owner_creation_date":"2020-11-21 06:04:32.327000 UTC",
        "Owner_last_access_date":"2021-06-03 07:19:55.190000 UTC",
        "Owner_location":"Jakarta Selatan, South Jakarta City, Jakarta, Indonesia",
        "Owner_reputation":97,
        "Owner_up_votes":18,
        "Owner_down_votes":0,
        "Owner_views":34,
        "Answer_body":"<p>If you are training directly in the notebook the answer is yes.\nHowever the best practice is not to train directly with the notebook.\nUse instead the notebook (you can choose a very cheap instance for the notebook) to launch your training job (in the instance type you desire) adapting you code to be the entrypoint of the estimator. In that way, you can close the notebook after launching the training job and monitor the training job using cloudwatch. You can also define some regex to capture metrics from the stout and cloudwatch will automatically plot for you, which is very useful!\nAs a quick example.. in my notebook I have this cell:<\/p>\n<pre><code>import sagemaker from sagemaker.tensorflow import TensorFlow from sagemaker import get_execution_role\n\nbucket = 'mybucket'\n\ntrain_data = 's3:\/\/{}\/{}'.format(bucket,'train')\n\nvalidation_data = 's3:\/\/{}\/{}'.format(bucket,'test')\n\ns3_output_location = 's3:\/\/{}'.format(bucket)\n\nhyperparameters = {'epochs': 70, 'batch-size' : 32, 'learning-rate' :\n0.01}\n\nmetrics = [{'Name': 'Loss', 'Regex': 'loss: ([0-9\\.]+)'},\n           {'Name': 'Accuracy', 'Regex': 'acc: ([0-9\\.]+)'},\n           {'Name': 'Epoch', 'Regex': 'Epoch ([0-9\\.]+)'},\n           {'Name': 'Validation_Acc', 'Regex': 'val_acc: ([0-9\\.]+)'},\n           {'Name': 'Validation_Loss', 'Regex': 'val_loss: ([0-9\\.]+)'}]\n\ntf_estimator = TensorFlow(entry_point='training.py', \n                          role=get_execution_role(),\n                          train_instance_count=1, \n                          train_instance_type='ml.p2.xlarge',\n                          train_max_run=172800,\n                          output_path=s3_output_location,\n                          framework_version='1.12',\n                          py_version='py3',\n                          metric_definitions = metrics,\n                          hyperparameters = hyperparameters)\n\ninputs = {'train': train_data, 'test': validation_data}\n\nmyJobName = 'myname'\n\ntf_estimator.fit(inputs=inputs, job_name=myJobName)\n<\/code><\/pre>\n<p>My training script training.py is something like this:<\/p>\n<pre><code>if __name__ =='__main__':\n\n    parser = argparse.ArgumentParser()\n\n    # input data and model directories\n    parser.add_argument('--gpu-count', type=int, default=os.environ['SM_NUM_GPUS'])\n    parser.add_argument('--model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\n    parser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))\n    parser.add_argument('--learning-rate', type=float, default=0.0001)\n    parser.add_argument('--batch-size', type=int, default=32)\n    parser.add_argument('--epochs', type=int, default=1)\n....\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-04-22 09:58:14.700000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":58345935,
        "Question_title":"The amlignore file doesn't reduce the size of snapshot",
        "Question_body":"<p>To overcome <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-save-write-experiment-files#storage-limits-of-experiment-snapshots\" rel=\"nofollow noreferrer\">300MB snapshot size limit<\/a> I created an .amlignore file in the root of my repository:<\/p>\n\n<pre><code>\/*\n!\/root\n<\/code><\/pre>\n\n<p>The intention is to exclude everything except <code>\/root<\/code> directory where all python code is. The size of the <code>root<\/code> directory is less than 1MB, still I get an error of exceeding snapshot limit size of 300MB. What am I doing wrong?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-10-11 17:23:39.870000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"azure-machine-learning-service",
        "Question_view_count":522,
        "Owner_creation_date":"2018-02-15 14:47:43.680000 UTC",
        "Owner_last_access_date":"2022-08-24 16:53:36.967000 UTC",
        "Owner_location":null,
        "Owner_reputation":95,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":15,
        "Answer_body":"<p>This is fixed in version <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/azure-machine-learning-release-notes#azure-machine-learning-sdk-for-python-v1074\" rel=\"nofollow noreferrer\">1.0.74 of azureml-sdk<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2019-11-16 19:31:39.567000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_valid_tags":[
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":73677347,
        "Question_title":"Does SageMaker built-in LightGBM algorithm support distributed training?",
        "Question_body":"<p>Does  <strong>Amazon SageMaker built-in LightGBM<\/strong> algorithm support <strong>distributed training<\/strong>?<\/p>\n<p>I use Databricks for distributed training of LightGBM today. If SageMaker built-in LightGBM supports distributed training, I would consider migrating to SageMaker. It is not clear in the Amazon SageMaker's built-in <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/lightgbm.html\" rel=\"nofollow noreferrer\">LightGBM<\/a>'s documentation on whether it supports distributed training.<\/p>\n<p>Thanks very much for any suggestion or clarification on this.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2022-09-11 06:44:39.543000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker|lightgbm|distributed-training|amazon-machine-learning",
        "Question_view_count":27,
        "Owner_creation_date":"2014-01-16 15:43:59.673000 UTC",
        "Owner_last_access_date":"2022-09-25 03:22:08.463000 UTC",
        "Owner_location":"Singapore",
        "Owner_reputation":5854,
        "Owner_up_votes":155,
        "Owner_down_votes":70,
        "Owner_views":794,
        "Answer_body":"<p>I went through the LightGBM section of SageMaker documentation and there are no references that it supports distributed training. One of the example\u00a0<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/lightgbm.html\" rel=\"nofollow noreferrer\">here<\/a>\u00a0uses single instance type. Also looked at lightGBM documentation\u00a0<a href=\"https:\/\/lightgbm.readthedocs.io\/en\/latest\/Parallel-Learning-Guide.html\" rel=\"nofollow noreferrer\">here<\/a>\u00a0. Here are the parameters that you need to specify<\/p>\n<p>tree_learner=your_parallel_algorithm,<\/p>\n<p>num_machines=your_num_machines,<\/p>\n<p>Given I couldnt find any reference of above in SageMaker documentation, I assume its not supported.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-09-20 22:00:20.203000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":63305569,
        "Question_title":"How to mount an EFS volume on AWS Sagemaker Studio",
        "Question_body":"<p>I have tried to follow the normal (non-studio) documentation on mounting an EFS file system, as can be found <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/mount-an-efs-file-system-to-an-amazon-sagemaker-notebook-with-lifecycle-configurations\/\" rel=\"nofollow noreferrer\">here<\/a>, however, these steps don't work in a studio notebook. Specifically, the <code>sudo mount -t nfs ...<\/code> does not work in both the Image terminal and the system terminal.<\/p>\n<p>How do I mount an EFS file system that already exists to amazon Sagemaker, so I can access the data\/ datasets I stored in them?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-08-07 16:13:59.977000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":2331,
        "Owner_creation_date":"2017-01-02 15:26:06.803000 UTC",
        "Owner_last_access_date":"2022-09-24 21:50:13.153000 UTC",
        "Owner_location":"London, UK",
        "Owner_reputation":15819,
        "Owner_up_votes":827,
        "Owner_down_votes":21,
        "Owner_views":1395,
        "Answer_body":"<p>Update: I spoke to an AWS Solutions Architect, and he confirms that EFS is not supported on Sagemaker Studio.<\/p>\n<hr \/>\n<p><strong>Workaround:<\/strong><\/p>\n<p>Instead of mounting your old EFS, you can mount the SageMaker studio EFS onto an EC2 instance, and copy over the data manually. You would need the correct EFS storage volume id, and you'll find your newly copied data available in Sagemaker Studio. <em>I have not actually done this though.<\/em><\/p>\n<p>To find the EFS id, look at the section &quot;Manage your storage volume&quot; <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/studio-tasks.html#manage-your-storage-volume\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2020-08-07 16:13:59.977000 UTC",
        "Answer_last_edit_date":"2020-08-21 06:35:30.413000 UTC",
        "Answer_score":1.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":51933366,
        "Question_title":"Version management in SageMaker",
        "Question_body":"<p>We're currently working with 3 employees in the same notebook-instance, however, since this is a shared workspace this makes version management extra difficult. Is it possible to link aws credentials to your git account from within SageMaker? Or are there any other ways recommended for version management? <\/p>\n\n<p>Right now we're using a single git account for committing the code from within jupyter terminal. <\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2018-08-20 14:51:45.277000 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":3,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":836,
        "Owner_creation_date":"2018-08-20 14:37:26.323000 UTC",
        "Owner_last_access_date":"2020-09-25 18:34:56.833000 UTC",
        "Owner_location":null,
        "Owner_reputation":33,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":"<p>The situation has changed : Git is now <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/amazon-sagemaker-notebooks-now-support-git-integration-for-increased-persistence-collaboration-and-reproducibility\/\" rel=\"nofollow noreferrer\">available<\/a> in SageMaker<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2018-11-30 10:32:45.693000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":60816944,
        "Question_title":"AWS Sagemaker Notebook with multiple users",
        "Question_body":"<p>I am still new in AWS sagemaker. Working on a architecture where we would have an AWS sagemaker notebook. There would be multiple users, I want that students don`t see each other work. would I need to do that in terminal? or we can do that in notebook itself?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-03-23 15:50:35.243000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":1907,
        "Owner_creation_date":"2017-12-21 19:07:16.660000 UTC",
        "Owner_last_access_date":"2022-09-23 04:14:35.810000 UTC",
        "Owner_location":"Noida, Uttar Pradesh, India",
        "Owner_reputation":465,
        "Owner_up_votes":11,
        "Owner_down_votes":0,
        "Owner_views":46,
        "Answer_body":"<p>The simplest way is to create a small notebook instance for each student. This way you can have the needed isolation and also the responsibility of each student for their notebook to stop them when they are not in use.<\/p>\n\n<p>The smallest instance type <a href=\"https:\/\/aws.amazon.com\/sagemaker\/pricing\/\" rel=\"nofollow noreferrer\">costs<\/a> $0.0464 per hour. If you have it running 24\/7 it costs about $30 per month. But if the students are responsible and stop their instances when they are not using them, it can be about $1 for 20 hours of work.<\/p>\n\n<p>If you want to enable isolation to the notebooks, you can use the ability to presign the URL that is used to open the Jupyter interface. See here on the way to use the CLI to create the URL: <a href=\"https:\/\/docs.aws.amazon.com\/cli\/latest\/reference\/sagemaker\/create-presigned-notebook-instance-url.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/cli\/latest\/reference\/sagemaker\/create-presigned-notebook-instance-url.html<\/a>. It is also supported in other SDK.<\/p>\n\n<pre><code>create-presigned-notebook-instance-url\n--notebook-instance-name &lt;student-instance-name&gt;\n--session-expiration-duration-in-seconds 3600\n<\/code><\/pre>\n\n<p>You can integrate it into the internal portal that you have in your institute. <\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2020-03-24 09:11:16.743000 UTC",
        "Answer_last_edit_date":"2020-03-24 19:21:35.677000 UTC",
        "Answer_score":3.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":57436140,
        "Question_title":"How to reuse successfully built docker images in Azure ML?",
        "Question_body":"<p>In our company I use Azure ML and I have the following issue. I specify a <em>conda_requirements.yaml<\/em> file with the PyTorch estimator class, like so (... are placeholders so that I do not have to type everything out):<\/p>\n\n<pre><code>from azureml.train.dnn import PyTorch\nest = PyTorch(source_directory=\u2019.\u2019, script_params=..., compute_target=..., entry_script=..., conda_dependencies_file_path=\u2019conda_requirements.yaml\u2019, environment_variables=..., framework_version=\u20191.1\u2019)\n<\/code><\/pre>\n\n<p>The <em>conda_requirements.yaml<\/em> (shortened version of the pip part) looks like this:<\/p>\n\n<pre><code>dependencies:\n  -  conda=4.5.11\n  -  conda-package-handling=1.3.10\n  -  python=3.6.2\n  -  cython=0.29.10\n  -  scikit-learn==0.21.2\n  -  anaconda::cloudpickle==1.2.1\n  -  anaconda::cffi==1.12.3\n  -  anaconda::mxnet=1.1.0\n  -  anaconda::psutil==5.6.3\n  -  anaconda::pip=19.1.1\n  -  anaconda::six==1.12.0\n  -  anaconda::mkl==2019.4\n  -  conda-forge::openmpi=3.1.2\n  -  conda-forge::pycparser==2.19\n  -  tensorboard==1.13.1\n  -  tensorflow==1.13.1\n  -  pip:\n        - torch==1.1.0\n        - torchvision==0.2.1\n<\/code><\/pre>\n\n<p>This successfully builds on Azure. Now in order to reuse the resulting docker image in that case, I use the <code>custom_docker_image<\/code> parameter to pass to the <\/p>\n\n<pre><code>from azureml.train.estimator import Estimator\nest = Estimator(source_directory=\u2019.\u2019, script_params=..., compute_target=..., entry_script=..., custom_docker_image=\u2019&lt;container registry name&gt;.azurecr.io\/azureml\/azureml_c3a4f...\u2019, environment_variables=...)\n<\/code><\/pre>\n\n<p>But now Azure somehow seems to rebuild the image again and when I run the experiment it cannot install torch. So it seems to only install the conda dependencies and not the pip dependencies, but actually I do not want Azure to rebuild the image. Can I solve this somehow?<\/p>\n\n<p>I attempted to somehow build a docker image from my Docker file and then add to the registry. I can do az login and according to <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/container-registry\/container-registry-authentication\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/container-registry\/container-registry-authentication<\/a> I then should also be able to do an acr login and push. This does not work. \nEven using the credentials from<\/p>\n\n<pre><code>az acr credential show \u2013name &lt;container registry name&gt;\n<\/code><\/pre>\n\n<p>and then doing a <\/p>\n\n<pre><code>docker login &lt;container registry name&gt;.azurecr.io \u2013u &lt;username from credentials above&gt; -p &lt;password from credentials above&gt;\n<\/code><\/pre>\n\n<p>does not work.\nThe error message is <em>authentication required<\/em> even though I used <\/p>\n\n<pre><code>az login\n<\/code><\/pre>\n\n<p>successfully. Would also be happy if someone could explain that to me in addition to how to reuse docker images when using Azure ML.\nThank you!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-08-09 19:34:37.197000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"azure|docker|pip|conda|azure-machine-learning-service",
        "Question_view_count":604,
        "Owner_creation_date":"2018-04-25 15:32:23.370000 UTC",
        "Owner_last_access_date":"2022-09-21 14:08:46.433000 UTC",
        "Owner_location":"Z\u00fcrich, Schweiz",
        "Owner_reputation":460,
        "Owner_up_votes":668,
        "Owner_down_votes":0,
        "Owner_views":52,
        "Answer_body":"<p>AzureML should actually cache your docker image once it was created. The service will hash the base docker info and the contents of the conda.yaml file and will use that as the hash key -- unless you change any of that information, the docker should come from the ACR. <\/p>\n\n<p>As for the custom docker usage, did you set the parameter <code>user_managed=True<\/code>? Otherwise, AzureML will consider your docker to be a base image on top of which it will create the conda environment per your yaml file.<br>\nThere is an example of how to use a custom docker image in this notebook:\n<a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/4170a394edd36413edebdbab347afb0d833c94ee\/how-to-use-azureml\/training-with-deep-learning\/how-to-use-estimator\/how-to-use-estimator.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/4170a394edd36413edebdbab347afb0d833c94ee\/how-to-use-azureml\/training-with-deep-learning\/how-to-use-estimator\/how-to-use-estimator.ipynb<\/a><\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2019-08-11 05:38:27.287000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":3.0,
        "Question_valid_tags":[
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":58806807,
        "Question_title":"Create a predictor from an endpoint in a different region",
        "Question_body":"<p>I have created an endpoint on us-east-1. try to create a predictor:<\/p>\n\n<pre><code>In [106]: sagemaker.predictor.RealTimePredictor(&lt;endpoint name&gt;)\n<\/code><\/pre>\n\n<p>and get<\/p>\n\n<pre><code>ClientError: An error occurred (ValidationException) when calling the DescribeEndpoint operation: \nCould not find endpoint \"arn:aws:sagemaker:us-east-2:&lt;account number&gt;:endpoint\/&lt;endpoint name&gt;\".\n<\/code><\/pre>\n\n<p>which is perfectly correct, since the endpoint is on us-east-1.  Probably I could change some defaults, but I'd rather not - I work on us-east-2 99% of the time.<\/p>\n\n<p>So, how can I set a different region when initializing the predictor?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_date":"2019-11-11 18:31:02.720000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":478,
        "Owner_creation_date":"2019-03-28 21:25:22.940000 UTC",
        "Owner_last_access_date":"2022-07-14 13:44:34.627000 UTC",
        "Owner_location":null,
        "Owner_reputation":67,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Answer_body":"<p>The (python) <code>Predictors<\/code> <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/predictors.html\" rel=\"nofollow noreferrer\">documentation<\/a> shows that you can pass a <code>Session<\/code> object. In turn, the <code>Session<\/code> can be <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/session.html#sagemaker.session.Session\" rel=\"nofollow noreferrer\">initialized<\/a> with a <em>client<\/em> and a <em>runtime client<\/em> - the former does everything except endpoint invocations, the latter does... endpoint invocations.<\/p>\n\n<p>Those clients are tied to specific regions. It seems like you should be able to set the runtime client region to match your endpoint, by manually instantiating it, while leaving the regular client alone (disclaimer here: I haven't tried this - if you do, let me\/us know how it goes :)).<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2019-11-11 18:44:41.147000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":71593747,
        "Question_title":"GCP Vertex AI \"Enable necessary APIs\" when already enabled",
        "Question_body":"<p>I am new to GCP's Vertex AI and suspect I am running into an error from my lack of experience, but Googling the answer has brought me no fruitful information.<\/p>\n<p>I created a Jupyter Notebook in AI Platform but wanted to schedule it to run at a set period of time. So I was hoping to use Vertex AI's Execute function. At first when I tried accessing Vertex I was unable to do so because the API had not been enabled in GCP. My IT team then enabled the Vertex AI API and I can now utilize Vertex. Here is a picture showing it is enabled. <a href=\"https:\/\/i.stack.imgur.com\/pUSRO.png\" rel=\"nofollow noreferrer\">Enabled API Picture<\/a><\/p>\n<p>I uploaded my notebook to a JupyterLab instance in Vertex, and when I click on the Execute button, I get an <a href=\"https:\/\/i.stack.imgur.com\/jnUDv.png\" rel=\"nofollow noreferrer\">error message<\/a> saying I need to &quot;Enable necessary APIs&quot;, specifically for Vertex AI API. I'm not sure why this is considering it's already been enabled. I try to click Enable, but it just spins and spins, and then I can only get out of it by closing or reloading the tab.<\/p>\n<p>One other thing I want to call out in case it's a settings issue is that currently my <a href=\"https:\/\/i.stack.imgur.com\/6UxKb.png\" rel=\"nofollow noreferrer\">Managed Notebooks tab says &quot;PREVIEW&quot;<\/a> in the Workbench. I started thinking maybe this was an indicator that there was a separate feature that needed to be enabled to use Managed Notebooks (which is where I can access the Execute button from). When I click on the User-Managed Notebooks and open JupyterLab from there, I don't have the Execute button.<\/p>\n<p>The GCP account I'm using does have billing enabled.<\/p>\n<p>Can anyone point me in the right direction to getting the Execute button to work?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":8,
        "Question_creation_date":"2022-03-23 20:37:57.907000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-03-24 21:04:27.247000 UTC",
        "Question_score":3,
        "Question_tags":"google-cloud-platform|jupyter-notebook|google-cloud-vertex-ai",
        "Question_view_count":518,
        "Owner_creation_date":"2022-03-07 16:39:21.350000 UTC",
        "Owner_last_access_date":"2022-08-26 15:58:54.313000 UTC",
        "Owner_location":null,
        "Owner_reputation":31,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":"<p>Based on @JamesS comments, the issue was solved by adding necessary permissions on his individual account since it is the account configured on OP's <code>Managed Notebook Instance<\/code> in which has an access mode of <code>Single user only<\/code>.<\/p>\n<p>Based on my testing when I tried to replicate the scenario, &quot;Enable necessary APIs&quot; message box will continue to show when the user has no <em>&quot;Vertex AI User&quot;<\/em> role assigned to it. And in conclusion of my testing, below are the minimum <strong>roles<\/strong> required when trying to create a <em>Scheduled run<\/em> on a <code>Managed Notebook Instance<\/code>.<\/p>\n<ul>\n<li><strong>Notebook Admin<\/strong> - For access of the notebook instance and open it through Jupyter. User will be able to run written codes in the Notebook as well.<\/li>\n<li><strong>Vertex AI User<\/strong> - So that the user can <strong>create schedule run<\/strong> on the notebook instance since the creation of the scheduled run is under the Vertex AI API itself.<\/li>\n<li><strong>Storage Admin<\/strong> - Creation of scheduled run will require a Google Cloud Storage bucket location where the job will be saved<\/li>\n<\/ul>\n<p>Posting the answer as <em>community wiki<\/em> for the benefit of the community that might encounter this use case in the future.<\/p>\n<p>Feel free to edit this answer for additional information.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2022-03-29 09:01:16.137000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_valid_tags":[
            "google-cloud-vertex-ai"
        ]
    },
    {
        "Question_id":68823606,
        "Question_title":"Difference between tracking_uri and the backend store uri in MLFLOW",
        "Question_body":"<p>I am using Mlflow for my project hosting it in an EC2 instance. I was wondering in MlFlow what is the difference between the backend_store_uri we set when we launch the server and the trarcking_uri ?<\/p>\n<p>Thanks,<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-08-17 20:25:00.840000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"uri|tracking|mlflow",
        "Question_view_count":682,
        "Owner_creation_date":"2020-03-25 10:10:52.300000 UTC",
        "Owner_last_access_date":"2022-09-23 08:20:45.110000 UTC",
        "Owner_location":null,
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":"<p><code>tracking_uri<\/code> is the URL of the MLflow server (remote, or built-in in Databricks) that will be used to log metadata &amp; model (see <a href=\"https:\/\/mlflow.org\/docs\/latest\/quickstart.html#launch-a-tracking-server-on-a-remote-machine\" rel=\"nofollow noreferrer\">doc<\/a>).  In your case, this will be the URL pointing to your EC2 instance that should be configured in programs that will log parameters into your server.<\/p>\n<p><code>backend_store_uri<\/code> - is used by MLflow server to configure where to store this data - on filesystem, in SQL-compatible database, etc. (see <a href=\"https:\/\/mlflow.org\/docs\/latest\/cli.html#cmdoption-mlflow-server-backend-store-uri\" rel=\"nofollow noreferrer\">doc<\/a>). If you use SQL database, then you also need to provide the <code>--default-artifact-root<\/code> option to point where to store generated artifacts (images, model files, etc.)<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2021-08-18 06:58:43.973000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "mlflow"
        ]
    },
    {
        "Question_id":60067075,
        "Question_title":"SageMaker deploy custom script",
        "Question_body":"<p>I'm pretty new to SageMaker, so I'm sorry if I miss something obvious.<\/p>\n\n<p>I've trained a DL model which uses frames from a video to make a prediction. The current script, that runs in the SageMaker jupyter-notebook, takes a video URL as an input and uses an FFMPEG subprocess pipe to extract the frames and predict them afterwards. This works fine, but now I want to start that script from Lambda.<\/p>\n\n<p>As far as I understood, I could deploy my model with sagemaker and make predictions for every single frame from Lambda, unfortunately this is not an option, as ffprobe, ffmpeg and numpy are too large to fit into the limited lambda space.<\/p>\n\n<p>tl;dr: Is it possible to run my custom script (ffmpeg frame extraction + tensorflow model prediction) as an endpoint in SageMaker?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-02-04 23:36:35.073000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":386,
        "Owner_creation_date":"2016-04-12 06:57:10.317000 UTC",
        "Owner_last_access_date":"2022-09-24 00:22:01.487000 UTC",
        "Owner_location":null,
        "Owner_reputation":404,
        "Owner_up_votes":46,
        "Owner_down_votes":2,
        "Owner_views":127,
        "Answer_body":"<p>Sagemaker allows you to use a custom Docker image (<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms.html\" rel=\"nofollow noreferrer\">AWS document<\/a>)<\/p>\n\n<blockquote>\n  <p>Build your own custom container image: If there is no pre-built Amazon\n  SageMaker container image that you can use or modify for an advanced\n  scenario, you can package your own script or algorithm to use with\n  Amazon SageMaker.You can use any programming language or framework to\n  develop your container<\/p>\n<\/blockquote>\n\n<ul>\n<li>Create a docker image with your code (FFmpeg, TensorFlow)<\/li>\n<li>Testing the docker container locally<\/li>\n<li>Deploying the image on Amazon ECR (Elastic Container Repository)<\/li>\n<li>Create a SageMaker model and point to the image<\/li>\n<\/ul>\n\n<p>For details, you can learn more from <a href=\"https:\/\/towardsdatascience.com\/brewing-up-custom-ml-models-on-aws-sagemaker-e09b64627722\" rel=\"nofollow noreferrer\">this tutorial<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-02-05 00:36:40.330000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":53533434,
        "Question_title":"AWS Sagemaker - Blazingtext BatchTransform no output",
        "Question_body":"<p>I have trained a blazingText model and followed this guide.\n<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/blazingtext.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/blazingtext.html<\/a><\/p>\n\n<p>\"Sample JSON request\" The Invoke end point is working perfectly. So I switched to,\nBatch Transform Job with \"content-type: application\/jsonlines\" and created a file in S3 with the following format data:<\/p>\n\n<pre><code>{\"source\": \"source_0\"}\n<\/code><\/pre>\n\n<p>The job ran success. But the output did not sent to S3. Also In the cloud logs,<\/p>\n\n<pre><code>\" [79] [INFO] Booting worker with pid: 79\"\n<\/code><\/pre>\n\n<p>This is is the last response. Did anyone know what went wrong?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2018-11-29 06:57:05.430000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":317,
        "Owner_creation_date":"2015-06-08 11:02:24.910000 UTC",
        "Owner_last_access_date":"2022-09-24 12:48:27.623000 UTC",
        "Owner_location":"Aruppukkottai, India",
        "Owner_reputation":802,
        "Owner_up_votes":556,
        "Owner_down_votes":6,
        "Owner_views":151,
        "Answer_body":"<p>I have found the issue. The batchtransform select the folder as input and the s3 source should be S3Prefix instead of manifest.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2018-11-29 07:51:11.070000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":54521080,
        "Question_title":"aws sagemaker for detecting text in an image",
        "Question_body":"<p>I am aware that it is better to use aws Rekognition for this. However, it does not seem to work well when I tried it out with the images I have (which are sort of like small containers with labels on them). The text comes out misspelled and fragmented.<\/p>\n\n<p>I am new to ML and sagemaker. From what I have seen, the use cases seem to be for prediction and image classification. I could not find one on training a model for detecting text in an image. Is it possible to to do it with Sagemaker? I would appreciate it if someone pointed me in the right direction.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-02-04 17:11:02.230000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":4,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":2255,
        "Owner_creation_date":"2014-10-03 01:47:05.550000 UTC",
        "Owner_last_access_date":"2022-09-23 18:38:38.850000 UTC",
        "Owner_location":"West Lafayette, IN, United States",
        "Owner_reputation":463,
        "Owner_up_votes":50,
        "Owner_down_votes":0,
        "Owner_views":111,
        "Answer_body":"<p>The different services will all provide different levels of abstraction for Optical Character Recognition (OCR) depending on what parts of the pipeline you are most comfortable with working with, and what you prefer to have abstracted.<\/p>\n\n<p>Here are a few options:<\/p>\n\n<ul>\n<li><p><strong>Rekognition<\/strong> will provide out of the box OCR with the <a href=\"https:\/\/docs.aws.amazon.com\/rekognition\/latest\/dg\/text-detecting-text-procedure.html\" rel=\"nofollow noreferrer\">DetectText<\/a> feature. However, it seems you will need to perform some sort of pre-processing on your images in your current case in order to get better results. This can be done through any method of your choice (Lambda, EC2, etc).<\/p><\/li>\n<li><p><strong>SageMaker<\/strong> is a tool that will enable you to easily train and deploy your own models (of any type). You have two primary options with SageMaker:<\/p>\n\n<ol>\n<li><p><em>Do-it-yourself option<\/em>: If you're looking to go the route of labeling your own data, gathering a sizable training set, and training your own OCR model, this is possible by training and deploying your own model via SageMaker.<\/p><\/li>\n<li><p><em>Existing OCR algorithm<\/em>: There are many algorithms out there that all have different potential tradeoffs for OCR. One example would be <a href=\"https:\/\/github.com\/tesseract-ocr\/tesseract\" rel=\"nofollow noreferrer\">Tesseract<\/a>. Using this, you can more closely couple your pre-processing step to the text detection.<\/p><\/li>\n<\/ol><\/li>\n<li><p><a href=\"https:\/\/aws.amazon.com\/textract\/\" rel=\"nofollow noreferrer\"><strong>Amazon Textract<\/strong><\/a> (In preview) is a purpose-built dedicated OCR service that may offer better performance depending on what your images look like and the settings you choose. <\/p><\/li>\n<\/ul>\n\n<p>I would personally recommend looking into <a href=\"https:\/\/docparser.com\/blog\/improve-ocr-accuracy\/\" rel=\"nofollow noreferrer\">pre-processing for OCR<\/a> to see if it improves Rekognition accuracy before moving onto the other options. Even if it doesn't improve Rekognition's accuracy, it will still be valuable for most of the other options!<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2019-02-04 20:46:42.943000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":4.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":66925614,
        "Question_title":"How to access DVC-controlled files from Oracle?",
        "Question_body":"<p>I have been storing my large files in CLOBs within Oracle, but I am thinking of storing my large files in a shared drive, then having a column in Oracle contain pointers to the files. This would use DVC.<\/p>\n<p>When I do this,<\/p>\n<p>(a) are the paths in Oracle paths that point to the files in my shared drive, as in, the actual files themselves?<\/p>\n<p>(b) or do the paths in Oracle point somehow to the DVC metafile?<\/p>\n<p>Any insight would help me out!<\/p>\n<p>Thanks :)\nJustin<\/p>\n<hr \/>\n<p>EDIT to provide more clarity:<\/p>\n<p>I checked here (<a href=\"https:\/\/dvc.org\/doc\/api-reference\/open\" rel=\"nofollow noreferrer\">https:\/\/dvc.org\/doc\/api-reference\/open<\/a>), and it helped, but I'm not fully there yet ...<\/p>\n<p>I want to pull a file from a remote dvc repository using python (which I have connected to the Oracle database). So, if we can make that work, I think I will be good. But, I am confused. If I specify 'remote' below, then how do I name the file (e.g., 'activity.log') when the remote files are all encoded?<\/p>\n<pre><code>with dvc.api.open(\n        'activity.log',\n        repo='location\/of\/dvc\/project',\n        remote='my-s3-bucket'\n        ) as fd:\n    for line in fd:\n        match = re.search(r'user=(\\w+)', line)\n        # ... Process users activity log\n<\/code><\/pre>\n<p>(NOTE: For testing purposes, my &quot;remote&quot; DVC directory is just another folder on my MacBook.)<\/p>\n<p>I feel like I'm missing a key concept about getting remote files ...<\/p>\n<p>I hope that adds more clarity. Any help figuring out remote file access is appreciated! :)<\/p>\n<p>Justin<\/p>\n<hr \/>\n<p>EDIT to get insights on 'rev' parameter:<\/p>\n<p>Before my question, some background\/my setup:\n(a) I have a repo on my MacBook called 'basics'.\n(b) I copied into 'basics' a directory of 501 files (called 'surface_files') that I subsequently pushed to a remote storage folder called 'gss'. After the push, 'gss' contains 220 hash directories.<\/p>\n<p>The steps I used to get here are as follows:<\/p>\n<pre><code>&gt; cd ~\/Desktop\/Work\/basics\n&gt; git init\n&gt; dvc init\n&gt; dvc add ~\/Desktop\/Work\/basics\/surface_files\n&gt; git add .gitignore surface_files.dvc\n&gt; git commit -m &quot;Add raw data&quot;\n&gt; dvc remote add -d remote_storage ~\/Desktop\/Work\/gss\n&gt; git commit .dvc\/config -m &quot;Configure remote storage&quot;\n&gt; dvc push\n&gt; rm -rf .\/.dvc\/cache\n&gt; rm -rf .\/surface_files\n<\/code><\/pre>\n<p>Next, I ran the following Python code to take one of my surface files, named <code>surface_100141.dat<\/code>, and used <code>dvc.api.get_url()<\/code> to get the corresponding remote storage file name. I then copied this remote storage file into my desktop under the file's original name, i.e., <code>surface_100141.dat<\/code>.<\/p>\n<p>The code that does all this is as follows, but FIRST, MY QUESTION --- when I run the code as it is shown below, no problems; but when I uncomment the 'rev=' line, it fails. I am not sure why this is happening. I used <code>git log<\/code> and <code>cat .git\/refs\/heads\/master<\/code> to make sure that I was getting the right hash. WHY IS THIS FAILING? That is my question.<\/p>\n<p>(In full disclosure, my git knowledge is not too strong yet. I'm getting there, but it's still a work in progress! :))<\/p>\n<pre><code>import dvc.api\nimport os.path\nfrom os import path\nimport shutil\n\nfilename = 'surface_100141.dat' # This file name would be stored in my Oracle database\nhome_dir = os.path.expanduser('~')+'\/' # This simply expanding '~' into '\/Users\/ricej\/'\n\nresource_url = dvc.api.get_url(\n    path=f'surface_files\/{filename}', # Works when 'surface_files.dvc' exists, even when 'surface_files' directory and .dvc\/cache do not\n    repo=f'{home_dir}Desktop\/Work\/basics',\n    # rev='5c92710e68c045d75865fa24f1b56a0a486a8a45', # Commit hash, found using 'git log' or 'cat .git\/refs\/heads\/master'\n    remote='remote_storage')\nresource_url = home_dir+resource_url\nprint(f'Remote file: {resource_url}')\n\nnew_dir = f'{home_dir}Desktop\/' # Will copy fetched file to desktop, for demonstration\nnew_file = new_dir+filename\nprint(f'Remote file copy: {new_file}')\n\nif path.exists(new_file):\n    os.remove(new_file)\n    \ndest = shutil.copy(resource_url, new_file) # Check your desktop after this to see remote file copy\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2021-04-02 21:45:23.477000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-04-05 17:22:23.263000 UTC",
        "Question_score":1,
        "Question_tags":"python|oracle|dvc",
        "Question_view_count":389,
        "Owner_creation_date":"2014-08-03 18:46:34.730000 UTC",
        "Owner_last_access_date":"2022-06-08 02:07:15.420000 UTC",
        "Owner_location":null,
        "Owner_reputation":53,
        "Owner_up_votes":9,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Answer_body":"<p>I'm not 100% sure that I understand the question (it would be great to expand it a bit on the actual use case you are trying to solve with this database), but I can share a few thoughts.<\/p>\n<p>When we talk about DVC, I think you need to specify a few things to identify the file\/directory:<\/p>\n<ol>\n<li>Git commit + path (actual path like <code>data\/data\/xml<\/code>). Commit (or to be precise any Git revision) is needed to identify the version of the data file.<\/li>\n<li>Or path in the DVC storage (<code>\/mnt\/shared\/storage\/00\/198493ef2343ao<\/code> ...<code>) + actual name of this file. This way you would be saving info that <\/code>.dvc` files have.<\/li>\n<\/ol>\n<p>I would say that second way is <em>not<\/em> recommended since to some extent it's an implementation detail - how does DVC store files internally. The public interface to DVC organized data storage is its repository URL + commit + file name.<\/p>\n<p>Edit (example):<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>with dvc.api.open(\n        'activity.log',\n        repo='location\/of\/dvc\/project',\n        remote='my-s3-bucket'\n        ) as fd:\n    for line in fd:\n        match = re.search(r'user=(\\w+)', line)\n        # ... Process users activity log\n<\/code><\/pre>\n<p><code>location\/of\/dvc\/project<\/code> this path must point to an actual Git repo. This repo should have a <code>.dvc<\/code> or <code>dvc.lock<\/code> file that has <code>activity.log<\/code> name in it + its hash in the remote storage:<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>outs:\n  - md5: a304afb96060aad90176268345e10355\n    path: activity.log\n<\/code><\/pre>\n<p>By reading this Git repo and analyzing let's say <code>activity.log.dvc<\/code> DVC will be able to create the right path <code>s3:\/\/my-bucket\/storage\/a3\/04afb96060aad90176268345e10355<\/code><\/p>\n<p><code>remote='my-s3-bucket'<\/code> argument is optional. By default it will use the one that is defined in the repo itself.<\/p>\n<p>Let's take another real example:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>with dvc.api.open(\n        'get-started\/data.xml',\n        repo='https:\/\/github.com\/iterative\/dataset-registry'\n        ) as fd:\n    for line in fd:\n        match = re.search(r'user=(\\w+)', line)\n        # ... Process users activity log\n<\/code><\/pre>\n<p>In the <code>https:\/\/github.com\/iterative\/dataset-registry<\/code> you could find the <a href=\"https:\/\/github.com\/iterative\/dataset-registry\/blob\/master\/get-started\/data.xml.dvc\" rel=\"nofollow noreferrer\"><code>.dvc<\/code> file<\/a> that is enough for DVC to create a path to the file by also analyzing its <a href=\"https:\/\/github.com\/iterative\/dataset-registry\/blob\/master\/.dvc\/config\" rel=\"nofollow noreferrer\">config<\/a><\/p>\n<pre><code>https:\/\/remote.dvc.org\/dataset-registry\/a3\/04afb96060aad90176268345e10355\n<\/code><\/pre>\n<p>you could run <code>wget<\/code> on this file to download it<\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_date":"2021-04-02 23:07:02.540000 UTC",
        "Answer_last_edit_date":"2021-04-03 19:28:34.567000 UTC",
        "Answer_score":2.0,
        "Question_valid_tags":[
            "dvc"
        ]
    },
    {
        "Question_id":66887340,
        "Question_title":"How do you clear the persistent storage for a notebook instance on AWS SageMaker?",
        "Question_body":"<p>So I'm running into the following error on AWS SageMaker when trying to save:<\/p>\n<blockquote>\n<p>Unexpected error while saving file: untitled.ipynb [Errno 28] No space left on device<\/p>\n<\/blockquote>\n<p>If I remove my notebook, create a new identical one and run it, everything works fine. However, I'm suspecting the Jupyter checkpoint takes up too much space if I save the notebook while it's running and therefore I'm running out of space. Sadly, getting more storage is not an option for me, so I'm wondering if there's any command I can use to clear the storage before running my notebook?<\/p>\n<p>More specifically, clearing the persistent storage in the beginning and at the end of the training process.<\/p>\n<p>I have googled like a maniac but there is no suggestion aside from &quot;just increase the amount of storage bro&quot; and that's why I'm asking the question here.<\/p>\n<p>Thanks in advance!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-03-31 11:51:06.313000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|jupyter-notebook|storage|amazon-sagemaker",
        "Question_view_count":973,
        "Owner_creation_date":"2017-09-02 17:20:57.017000 UTC",
        "Owner_last_access_date":"2022-09-23 08:31:46.523000 UTC",
        "Owner_location":"Sweden",
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Answer_body":"<p>If you don't want your data to be persistent across multiple notebook runs, just store them in <code>\/tmp<\/code> which is not persistent. You have at least 10GB. More details <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/howitworks-create-ws.html\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_date":"2021-03-31 13:45:19.367000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":62734994,
        "Question_title":"Why package is not updated even the lifecycle script has been executed successfully in SageMaker?",
        "Question_body":"<p>I wanted to update pandas version in 'conda-python3' in SageMaker, I've followed the steps in this <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/notebook-lifecycle-config.html\" rel=\"nofollow noreferrer\">page<\/a>, and linked the new configuration to my instance, CloudWatch log shows me the script has been executed successfully, but when I restart my instance and print out the panda version, it's still showing the old version 0.24.2, I don't understand why?<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/fR82t.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/fR82t.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>This is the script in the lifecycle configuration:<\/p>\n<pre><code>#!\/bin\/bash\nsudo -u ec2-user -i &lt;&lt;'EOF'\n\npip install pandas\n\nconda update pandas\n\nsource deactivate\n\nEOF\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-07-04 22:17:16.277000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|pandas|amazon-web-services|conda|amazon-sagemaker",
        "Question_view_count":106,
        "Owner_creation_date":"2018-10-30 17:35:56.270000 UTC",
        "Owner_last_access_date":"2022-09-22 19:30:36.883000 UTC",
        "Owner_location":"United Kingdom",
        "Owner_reputation":2385,
        "Owner_up_votes":1007,
        "Owner_down_votes":16,
        "Owner_views":585,
        "Answer_body":"<p>You are not activating any conda environment such as <a href=\"https:\/\/stackoverflow.com\/questions\/60036916\/sagemaker-lifecycle-configuration-for-installing-pandas-not-working\">python3<\/a>.<\/p>\n<pre><code>#!\/bin\/bash\nsudo -u ec2-user -i &lt;&lt;'EOF'\n\n# This will affect only the Jupyter kernel called &quot;conda_python3&quot;.\nsource activate python3\n\npip install pandas\n\nconda update pandas\n\nsource deactivate\n\nEOF\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-07-10 05:34:20.763000 UTC",
        "Answer_last_edit_date":"2020-11-16 21:55:49.573000 UTC",
        "Answer_score":1.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":48267427,
        "Question_title":"Trouble in creating graphics with matplotlib in a Jupyter notebook",
        "Question_body":"<p>Following the pandas documentation for visualization (<a href=\"https:\/\/pandas.pydata.org\/pandas-docs\/stable\/visualization.html#visualization-hist\" rel=\"nofollow noreferrer\">https:\/\/pandas.pydata.org\/pandas-docs\/stable\/visualization.html#visualization-hist<\/a>) I am trying to create the following graphics:<\/p>\n\n<pre><code>import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\n## A data set in my AzureML workplace experiment \ndf = ds.to_dataframe()\nplt.figure(); \ndf.plot.hist(stacked=True, bins=20) \nplt.figure();df.boxplot()\n<\/code><\/pre>\n\n<p>However, the output is limited to <code>\"&lt;matplotlib.axes._subplots.AxesSubplot at 0x7fd12e15dc18&gt;\"<\/code> (for the histogram(=) and <code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x7fd12e0ce828&gt;\"<\/code> (to the box plot), but no image appearing. Can anyone help me to identify what I'm missing out? Thanks!<\/p>\n\n<p>I'm using Python 3 in Jupyter Notebook in AzureML. <\/p>\n\n<p>The <code>df.describe()<\/code> method works properly (there is a dataFrame)<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_date":"2018-01-15 16:51:30.403000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2018-01-15 16:55:42.393000 UTC",
        "Question_score":0,
        "Question_tags":"python|pandas|matplotlib|azure-machine-learning-studio",
        "Question_view_count":378,
        "Owner_creation_date":"2011-09-21 00:39:19.337000 UTC",
        "Owner_last_access_date":"2022-06-18 20:52:29.090000 UTC",
        "Owner_location":"Brazil",
        "Owner_reputation":2563,
        "Owner_up_votes":368,
        "Owner_down_votes":2,
        "Owner_views":503,
        "Answer_body":"<p>Have you set the backend?<\/p>\n\n<pre><code>%matplotlib inline\n<\/code><\/pre>\n\n<p>Worth reading about what this does for a notebook here too\n<a href=\"https:\/\/stackoverflow.com\/questions\/43027980\/purpose-of-matplotlib-inline\/43028034\">Purpose of &quot;%matplotlib inline&quot;<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2018-01-15 16:55:53.070000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":72538301,
        "Question_title":"Use data in AWS Athena table and real time data to make prediction through Sagemaker inference",
        "Question_body":"<p>It is like time series prediction, but not exactly. Our customer sends patient current vital information to AWS SageMaker inference endpoint to predict the patient future health condition.  The patient data is saved to AWS S3 bucket,  and we use Athena to build a table from saved data.  We did off-line training, and found the vital information change is an import feature to predict patient future health condition.\nDoes anyone know how I can query Athena table to extract a given patient historical vital information, and send both historical and current vital information to the AWS Sagemaker inference endpoint to make prediction?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-06-07 22:52:29.553000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"machine-learning|time-series|amazon-athena|amazon-sagemaker",
        "Question_view_count":108,
        "Owner_creation_date":"2020-06-15 16:45:01.487000 UTC",
        "Owner_last_access_date":"2022-09-14 03:39:54.553000 UTC",
        "Owner_location":"California, USA",
        "Owner_reputation":105,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":16,
        "Answer_body":"<p>Do you have a SageMaker real-time endpoint created? One route is using an AWS SDK (Ex: boto3 for Python) and coupling it with a Lambda function to invoke that endpoint. The Lambda function can grab data from S3 or Athena or whatever data source using the SDK and then invoke that endpoint.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2022-06-09 18:40:55.123000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":37519858,
        "Question_title":"What type should the returned scores from an R scoring script?",
        "Question_body":"<p>I am attempting to develop an Azure ML experiment that uses R to perform predictions of a continuous response variable. The initial experiment is relatively simple, incorporating only a few experiment items, including \"Create R Model\", \"Train Model\" and \"Score Model\", along with some data input.<\/p>\n\n<p>I have written a training script and a scoring script, both of which appear to execute without errors when I run the experiment within ML Studio. However, when I examine the scored dataset, the score values are all missing values. So I am concerned that my scoring script could be returning scores incorrectly. Can anyone advise what type I should be returning? Is it meant to be a single column data.frame, or something else?<\/p>\n\n<p>It is also possible that my scores are not being properly calculated within the scoring script, although I have run the training and scoring scripts within R Studio, which shows the expected results. It would also be helpful if someone could suggest how to perform debugging of my scoring script in some way, so that I could determine whereabouts the code is failing to behave as expected.<\/p>\n\n<p>Thanks, Paul<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2016-05-30 07:23:50.317000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"azure-machine-learning-studio",
        "Question_view_count":238,
        "Owner_creation_date":"2016-05-30 01:28:09.107000 UTC",
        "Owner_last_access_date":"2016-08-02 23:54:21.490000 UTC",
        "Owner_location":null,
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":"<p>Try using this sample and compare with yours - <a href=\"https:\/\/gallery.cortanaintelligence.com\/Experiment\/Compare-Sample-5-in-R-vs-Azure-ML-1\" rel=\"nofollow\">https:\/\/gallery.cortanaintelligence.com\/Experiment\/Compare-Sample-5-in-R-vs-Azure-ML-1<\/a><\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2016-05-31 06:15:35.140000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":68952727,
        "Question_title":"wandb: get a list of all artifact collections and all aliases of those artifacts",
        "Question_body":"<p>The wandb documentation doesn't seem to explain how to do this - but it should be a fairly common use case I'd imagine?<\/p>\n<p>I achieved mostly (but not completely) what I wanted like this, but it seems a bit clunky? I'd have expected to have an <code>self.aliases<\/code> property on the <code>ArtifactCollection<\/code> instances?<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>ENTITY = os.environ.get(&quot;WANDB_ENTITY&quot;)\nAPI_KEY = os.environ.get(&quot;WANDB_API_KEY&quot;)\n\ndef get_model_artifacts(key=None):\n    wandb.login(key=key if key is not None else API_KEY)\n    api = wandb.Api(overrides={&quot;entity&quot;: ENTITY})\n    model_names = [\n        i\n        for i in api.artifact_type(\n            type_name=&quot;models&quot;, project=&quot;train&quot;\n        ).collections()\n    ]\n    for model in model_names:\n        artifact = api.artifact(&quot;train\/&quot; + model.name + &quot;:latest&quot;)\n        model._attrs.update(artifact._attrs)\n        model._attrs[&quot;metadata&quot;] = json.loads(model._attrs[&quot;metadata&quot;])\n        model.aliases = [x[&quot;alias&quot;] for x in model._attrs[&quot;aliases&quot;]]\n    return model_names\n<\/code><\/pre>\n<p>I guess I could possibly look into writing a custom graph-ql query if needed or just use this clunky method.<\/p>\n<p>Am I missing something? Is there a cleaner way to do this?<\/p>\n<p>The one thing this clunky method is missing is any old aliases - it only shows the latest model and then any aliases of that (let's say &quot;latest&quot; and also &quot;v4&quot; etc.) - not sure how this would\/should be displayed but I'd have hoped to be able to get old aliases as well (i.e. aliases that point to old versions of the artifact). Although, this is less important.<\/p>\n<p><strong>EDIT<\/strong> - after a few hours looking through their sdk code, I have this (still not that happy with how clunky it is):<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>ENTITY = os.environ.get(&quot;WANDB_ENTITY&quot;)\nAPI_KEY = os.environ.get(&quot;WANDB_API_KEY&quot;)\n\ndef get_model_artifacts(key=None):\n    wandb.login(key=key if key is not None else API_KEY)\n    api = wandb.Api(overrides={&quot;entity&quot;: ENTITY})\n    model_artifacts = [\n        a\n        for a in api.artifact_type(\n            type_name=&quot;models&quot;, project=&quot;train&quot;\n        ).collections()\n    ]\n\n    def get_alias_tuple(artifact_version):\n        version = None\n        aliases = []\n        for a in artifact_version._attrs[&quot;aliases&quot;]:\n            if re.match(r&quot;^v\\d+$&quot;, a[&quot;alias&quot;]):\n                version = a[&quot;alias&quot;]\n            else:\n                aliases.append(a[&quot;alias&quot;])\n        return version, aliases\n\n    for model in model_artifacts:\n        # artifact = api.artifact(&quot;train\/&quot; + model.name + &quot;:latest&quot;)\n        # model._attrs.update(artifact._attrs)\n        # model._attrs[&quot;metadata&quot;] = json.loads(model._attrs[&quot;metadata&quot;])\n        versions = model.versions()\n        version_dict = dict(get_alias_tuple(version) for version in versions)\n        model.version_dict = version_dict\n        model.aliases = [\n            x for key, val in model.version_dict.items() for x in [key] + val\n        ]\n    return model_artifacts\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-08-27 11:34:22.593000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-08-30 21:05:43.657000 UTC",
        "Question_score":1,
        "Question_tags":"python|wandb",
        "Question_view_count":363,
        "Owner_creation_date":"2018-05-15 07:21:25.180000 UTC",
        "Owner_last_access_date":"2022-09-23 17:35:59.473000 UTC",
        "Owner_location":"Munich, Germany",
        "Owner_reputation":3944,
        "Owner_up_votes":574,
        "Owner_down_votes":47,
        "Owner_views":217,
        "Answer_body":"<p>I'm Annirudh. I'm an engineer at W&amp;B who helped build artifacts. Your solution is really close, but by using the <code>latest<\/code> alias when fetching the artifact we're only going to be considering the aliases from that one artifact instead of all the versions. You could get around that by looping over the versions:<\/p>\n<pre><code>api = wandb.Api()\ncollections = [\n    coll for coll in api.artifact_type(type_name=TYPE, project=PROJECT).collections()\n]\n\n\naliases = set()\nfor coll in collections:\n    for artifact in coll.versions():\n        aliases.update(artifact.aliases)\n\nprint(collections)\nprint(aliases)\n<\/code><\/pre>\n<p>Currently, the documentation is spare on collections but we're polishing them up in the public API and will release some docs around it shortly. These APIs aren't quite release ready yet -- so apologies for the rough edges.<\/p>\n<p>Please feel free to reach out to me directly in the future if you have any other questions regarding artifacts. Always happy to help.<\/p>",
        "Answer_comment_count":14.0,
        "Answer_creation_date":"2021-08-28 03:42:13.813000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_valid_tags":[
            "wandb"
        ]
    },
    {
        "Question_id":72381916,
        "Question_title":"Remotely execute ClearML task using local-only repo",
        "Question_body":"<p>I want to execute ClearML task remotely. According to docs there are 2 options: 1) execute single python file; 2) ClearML would identify that script is part of repo, that repo will be cloned and installed into docker and executed on the worker.<\/p>\n<p>In this second scenario it is assumed that repo has remote url and it is accessible by worker. What if it isn't the case? Is it possible to somehow pack the local repo and send it for remote execution.<\/p>\n<p>I think it is somewhat extending scenario 1, where not a single file is passed for execution but whole directory with file in it.<\/p>\n<p>PS: i understand reproducibility concerns that arise, but repo is really not accessible from worker :(<\/p>\n<p>Thanks in advance.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-05-25 17:24:40.970000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"clearml",
        "Question_view_count":25,
        "Owner_creation_date":"2022-05-25 17:13:50.777000 UTC",
        "Owner_last_access_date":"2022-09-02 10:09:50.107000 UTC",
        "Owner_location":null,
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Answer_body":"<p>Disclaimer: I'm a team members of ClearML<\/p>\n<blockquote>\n<p>In this second scenario it is assumed that repo has remote url and it is accessible by worker. What if it isn't the case? Is it possible to somehow pack the local repo and send it for remote execution.<\/p>\n<\/blockquote>\n<p>well, no :( if your code is a single script, then yes ClearML would store the entire script, then the worker will reproduce it on the remote machine. But if your code base is composed of more than a single file, then why not use git? it is free hosted by GitHub, Bitbucket, GitLab etc.<\/p>\n<p>In theory this is doable and if you feel the need, I urge you to PR this feature. Basically you would store the entire folder as an artifact (ClearML will auto zip it for you), then the agent needs to unzip the artifact and run it. The main issue would be that cloning the Task will not clone the artifact...<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-08-08 21:03:38.003000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_valid_tags":[
            "clearml"
        ]
    },
    {
        "Question_id":49130977,
        "Question_title":"Does a call to \"Deploy web service(via API key) \" re run trained Azure ML model again",
        "Question_body":"<p>I wanted to know how exactly the following works in backend<\/p>\n\n<p><strong>Scenario :<\/strong> <\/p>\n\n<blockquote>\n  <p>-> We get data from Edgex foundry in UTC format and we it store it in Azure Document DB in (CST\/CDT timezone) format<\/p>\n  \n  <p>-> We trained ML model on data(with Date in CST\/CDT timezone) and Deploy web service.<\/p>\n<\/blockquote>\n\n<p><strong>So I have few basic doubts below<\/strong><\/p>\n\n<blockquote>\n  <ol>\n  <li><p>When web job hits our predictive webservice , will the trained ML model be run again?<\/p><\/li>\n  <li><p>Do we need to convert the UTC timezone for new incoming test data( which we want to predict) into CST\/CDT timezone, as TimeStamp does\n  matter for our prediction?<\/p><\/li>\n  <li><p>What happens in backend when predictive webservice API is called?<\/p><\/li>\n  <\/ol>\n<\/blockquote>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2018-03-06 12:33:53.757000 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"azure|azure-web-app-service|azure-machine-learning-studio",
        "Question_view_count":92,
        "Owner_creation_date":"2017-09-08 10:46:44.870000 UTC",
        "Owner_last_access_date":"2019-07-02 09:54:16.807000 UTC",
        "Owner_location":"Pune, Maharashtra, India",
        "Owner_reputation":391,
        "Owner_up_votes":49,
        "Owner_down_votes":0,
        "Owner_views":125,
        "Answer_body":"<p>This is only based on my experience with Azure ML, but I think I can help with your questions.<\/p>\n\n<blockquote>\n  <p>When web job hits our predictive webservice, will the trained ML model be run again?<\/p>\n<\/blockquote>\n\n<p>Yes, in the sense that it will call the <code>predict<\/code> (or similar) method on the model on the new data. For instance, in <code>scikit-learn<\/code> you would train your model using the <code>fit<\/code> method. Once the model is in production, only the <code>predict<\/code> method would be called.<\/p>\n\n<p>It will also run the whole workflow you have set up to be deployed as the web service. As an example below is a workflow I've played around with before. Each time the web service is run with new data, this whole thing will be run. This is like creating a Pipeline in <code>scikit-learn<\/code>.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/YMFZb.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/YMFZb.png\" alt=\"Azure ML Workflow\"><\/a><\/p>\n\n<blockquote>\n  <p>Do we need to convert the UTC timezone for new incoming test data( which we want to predict) into CST\/CDT timezone, as TimeStamp does matter for our prediction?<\/p>\n<\/blockquote>\n\n<p>I would say yes, you would need to convert to the timezone that was used when training in the model. This can be done by adding a step in your workflow then when you call the web service it will do the necessary converting for you before making a prediction.<\/p>\n\n<blockquote>\n  <p>What happens in backend when predictive webservice API is called?<\/p>\n<\/blockquote>\n\n<p>I'm not sure if anyone knows for sure other than the folks at Microsoft, but for sure it will run the workflow you have set up.<\/p>\n\n<hr>\n\n<p>I know it's not much, but I hope this helps or at least gets you on the right track for what you need.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2018-03-14 11:19:15.920000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":62457880,
        "Question_title":"AML - Web service TimeoutError",
        "Question_body":"<p>We created a webservice endpoint and tested it with the following code, and also with POSTMAN.<\/p>\n\n<p>We deployed the service to an AKS in the same resource group and subscription as the AML resource.<\/p>\n\n<p><strong>UPDATE: the attached AKS had a custom networking configuration and rejected external connections.<\/strong><\/p>\n\n<pre><code>import numpy\nimport os, json, datetime, sys\nfrom operator import attrgetter\nfrom azureml.core import Workspace\nfrom azureml.core.model import Model\nfrom azureml.core.image import Image\nfrom azureml.core.webservice import Webservice\nfrom azureml.core.authentication import AzureCliAuthentication\n\ncli_auth = AzureCliAuthentication()\n# Get workspace\nws = Workspace.from_config(auth=cli_auth)\n\n# Get the AKS Details\ntry:\n    with open(\"..\/aml_config\/aks_webservice.json\") as f:\n        config = json.load(f)\nexcept:\n    print(\"No new model, thus no deployment on AKS\")\n    # raise Exception('No new model to register as production model perform better')\n    sys.exit(0)\n\nservice_name = config[\"aks_service_name\"]\n# Get the hosted web service\nservice = Webservice(workspace=ws, name=service_name)\n\n# Input for Model with all features\ninput_j = [[1, 2, 3, 4, 5, 6, 7, 8, 9, 10], [10, 9, 8, 7, 6, 5, 4, 3, 2, 1]]\nprint(input_j)\ntest_sample = json.dumps({\"data\": input_j})\ntest_sample = bytes(test_sample, encoding=\"utf8\")\ntry:\n    prediction = service.run(input_data=test_sample)\n    print(prediction)\nexcept Exception as e:\n    result = str(e)\n    print(result)\n    raise Exception(\"AKS service is not working as expected\")\n<\/code><\/pre>\n\n<p>In AML Studio, the deployment state is \"Healthy\".<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/RTB10.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/RTB10.png\" alt=\"Endpoint attributes\"><\/a><\/p>\n\n<p>We get the following error when testing:<\/p>\n\n<pre><code>Failed to establish a new connection: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond'\n<\/code><\/pre>\n\n<p><strong>Log just after deploying the AKS Webservice <a href=\"http:\/\/t.ly\/t79b\" rel=\"nofollow noreferrer\">here<\/a>.<\/strong><\/p>\n\n<p><strong>Log after running the test script <a href=\"http:\/\/t.ly\/79k5\" rel=\"nofollow noreferrer\">here<\/a>.<\/strong><\/p>\n\n<p>How can we know what is causing this problem and fix it?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_date":"2020-06-18 19:24:51.480000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-06-19 18:07:41.567000 UTC",
        "Question_score":2,
        "Question_tags":"azure-machine-learning-studio|azure-aks|azure-machine-learning-service",
        "Question_view_count":332,
        "Owner_creation_date":"2020-03-30 17:44:04.877000 UTC",
        "Owner_last_access_date":"2020-06-28 18:13:06.573000 UTC",
        "Owner_location":null,
        "Owner_reputation":55,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":12,
        "Answer_body":"<p>We checked the AKS networking configuration and realized it has an Azure CNI profile.<\/p>\n\n<p>In order to test the webservice we need to do it from inside the created virtual network.\nIt worked well!<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-06-19 18:10:02.853000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "azure-machine-learning-studio",
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":66554893,
        "Question_title":"AWS Sagemaker Workflow pIpeline use the code stored in artifact created from Codebuild",
        "Question_body":"<p>I have created a <code>sagemaker.workflow.pipeline.Pipeline<\/code> object, in which, there are couple of processing step where I am trying to reference to an s3 file path rather than a local file path, so that it won't upload files to s3 everytime the pipeline runs.<\/p>\n<p>My question is, can I modify the <code>step<\/code> or <code>scriptprocessor<\/code> or <code>pipeline<\/code> object so that I can reference a code from artifact created from AWS Codebuild?<\/p>\n<p>If not, can I use codebuild to first copy my local file to a specific S3 position (I am having permission issue so far) and then run the pipeline?<\/p>\n<p>As your reference<\/p>\n<pre><code>...\nstep_data_ingest = ProcessingStep(\n        name=&quot;DataIngestion&quot;,\n        processor=sklearn_data_ingest_processor,\n        inputs=[\n            ProcessingInput(\n                input_name=&quot;input_train_data&quot;,\n                source=input_data, \n                destination=&quot;\/opt\/ml\/processing\/input\/data\/train&quot;\n            ),\n            ProcessingInput(\n                input_name=&quot;input_test_data&quot;,\n                source=test_data, \n                destination=&quot;\/opt\/ml\/processing\/input\/data\/test&quot;\n            ),\n            ProcessingInput(\n                input_name=&quot;requirement_file&quot;,\n                source=os.path.join(code_dir, &quot;requirements.txt&quot;), \n                destination=&quot;\/opt\/ml\/processing\/input\/requirement&quot;\n            ),\n        ],\n        outputs=[\n            ProcessingOutput(\n                output_name=&quot;train&quot;, \n                source=&quot;\/opt\/ml\/processing\/output\/train&quot;,\n                destination=get_projection_s3_dir(experiment_dir, &quot;datasets\/train&quot;)\n            ),\n            ProcessingOutput(\n                output_name=&quot;validation&quot;, \n                source=&quot;\/opt\/ml\/processing\/output\/validation&quot;,\n                destination=get_projection_s3_dir(experiment_dir, &quot;datasets\/validation&quot;)\n            ),\n            ProcessingOutput(\n                output_name=&quot;test&quot;, \n                source=&quot;\/opt\/ml\/processing\/output\/test&quot;,\n                destination=get_projection_s3_dir(experiment_dir, &quot;datasets\/test&quot;)\n            ),\n            ProcessingOutput(\n                output_name=&quot;sample&quot;, \n                source=&quot;\/opt\/ml\/processing\/output\/sample&quot;,\n                destination=get_projection_s3_dir(experiment_dir, &quot;datasets\/sample&quot;)\n            ),\n        ],\n        code=os.path.join(code_dir, &quot;data_ingestion.py&quot;),\n        # something like s3:\/\/some_code_dir\/data_ingestion.py\n        job_arguments = [&quot;-c&quot;, country, \n                         &quot;-v&quot;, train_val_split_percentage],\n    )\n...\n<\/code><\/pre>\n<p>What I expect to do is something like:<\/p>\n<pre><code># in processing step or processor\nProcessingStep(\n    ...\n    code=&quot;data_ingestion.py&quot;\n    code_location=&quot;s3:\/\/some_artifact_bucket\/buildartifact\/fdskz.zip&quot;\n\n    ...\n)\n<\/code><\/pre>\n<p>OR<\/p>\n<pre><code># in processing step or processor\nProcessingStep(\n    ...\n    code=&quot;s3:\/\/some_artifact_bucket\/buildartifact\/fdsix\/data_ingestion.py&quot;\n    ...\n)\n<\/code><\/pre>\n<p>OR<\/p>\n<pre><code># in buildspec.yml for codebuild\naws s3 sync .\/code_dir\/ s3:\/\/some_code_dir\/\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-03-09 21:16:09.760000 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-s3|amazon-sagemaker|aws-codebuild",
        "Question_view_count":156,
        "Owner_creation_date":"2019-12-04 18:47:35.683000 UTC",
        "Owner_last_access_date":"2021-10-17 03:33:46.670000 UTC",
        "Owner_location":null,
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":"<p>When using the <code>ProcessingStep<\/code>, you can use an <code>S3 URI<\/code> as the code location, take a look on <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/9fc57555bba4fc1d33064478dc209a84a6726c57\/src\/sagemaker\/workflow\/steps.py#L374\" rel=\"nofollow noreferrer\">this<\/a> for reference.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-07-29 17:00:25.127000 UTC",
        "Answer_last_edit_date":"2021-08-03 21:42:47.547000 UTC",
        "Answer_score":0.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":60365473,
        "Question_title":"By how much can i approx. reduce disk volume by using dvc?",
        "Question_body":"<p>I want to classify ~1m+ documents and have a Version Control System for in- and Output of the corresponding model. <\/p>\n\n<p>The data changes over time:<\/p>\n\n<ul>\n<li>sample size increases over time<\/li>\n<li>new Features might appear<\/li>\n<li>anonymization procedure might Change over time<\/li>\n<\/ul>\n\n<p>So basically \"everything\" might change: amount of observations, Features and the values.\nWe are interested in making the ml model Building reproducible without using 10\/100+ GB \nof disk volume, because we save all updated versions of Input data. Currently the volume size of the data is ~700mb.<\/p>\n\n<p>The most promising tool i found is: <a href=\"https:\/\/github.com\/iterative\/dvc\" rel=\"noreferrer\">https:\/\/github.com\/iterative\/dvc<\/a>. Currently the data\nis stored in a database in loaded in R\/Python from there.<\/p>\n\n<p><strong>Question:<\/strong><\/p>\n\n<p>How much disk volume can be (very approx.) saved by using dvc? <\/p>\n\n<p>If one can roughly estimate that. I tried to find out if only the \"diffs\" of the data are saved. I didnt find much info by reading through: <a href=\"https:\/\/github.com\/iterative\/dvc#how-dvc-works\" rel=\"noreferrer\">https:\/\/github.com\/iterative\/dvc#how-dvc-works<\/a> or other documentation. <\/p>\n\n<p><strong>I am aware that this is a very vague question. And it will highly depend on the dataset. However, i would still be interested in getting a very approximate idea.<\/strong><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-02-23 18:31:41.247000 UTC",
        "Question_favorite_count":3.0,
        "Question_last_edit_date":"2020-02-23 19:28:48.287000 UTC",
        "Question_score":7,
        "Question_tags":"python|sql|r|git|dvc",
        "Question_view_count":689,
        "Owner_creation_date":"2017-08-30 12:46:30.907000 UTC",
        "Owner_last_access_date":"2022-03-11 18:10:58.673000 UTC",
        "Owner_location":null,
        "Owner_reputation":1365,
        "Owner_up_votes":145,
        "Owner_down_votes":3,
        "Owner_views":193,
        "Answer_body":"<p>Let me try to summarize how does DVC store data and I hope you'll be able to figure our from this how much space will be saved\/consumed in your specific scenario.<\/p>\n\n<p><strong>DVC is storing and deduplicating data on the individual <em>file level<\/em>.<\/strong> So, what does it usually mean from a practical perspective.<\/p>\n\n<p>I will use <code>dvc add<\/code> as an example, but the same logic applies to all commands that save data files or directories into DVC cache - <code>dvc add<\/code>, <code>dvc run<\/code>, etc.<\/p>\n\n<h2>Scenario 1: Modifying file<\/h2>\n\n<p>Let's imagine I have a single 1GB XML file. I start tracking it with DVC:<\/p>\n\n<pre class=\"lang-sh prettyprint-override\"><code>$ dvc add data.xml\n<\/code><\/pre>\n\n<p>On the modern file system (or if <code>hardlinks<\/code>, <code>symlinks<\/code> are enabled, see <a href=\"https:\/\/dvc.org\/doc\/user-guide\/large-dataset-optimization\" rel=\"noreferrer\">this<\/a> for more details) after this command we still consume 1GB (even though file is moved into DVC cache and is still present in the workspace).<\/p>\n\n<p>Now, let's change it a bit and save it again:<\/p>\n\n<pre class=\"lang-sh prettyprint-override\"><code>$ echo \"&lt;test\/&gt;\" &gt;&gt; data.xml\n$ dvc add data.xml\n<\/code><\/pre>\n\n<p>In this case we will have 2GB consumed. <strong>DVC does not do diff between two versions of the same file<\/strong>, neither it splits files into chunks or blocks to understand that only small portion of data has changed.<\/p>\n\n<blockquote>\n  <p>To be precise, it calculates <code>md5<\/code> of each file and save it in the content addressable key-value storage. <code>md5<\/code> of the files serves as a key (path of the file in cache) and value is the file itself:<\/p>\n  \n  <pre class=\"lang-sh prettyprint-override\"><code>(.env) [ivan@ivan ~\/Projects\/test]$ md5 data.xml\n0c12dce03223117e423606e92650192c\n\n(.env) [ivan@ivan ~\/Projects\/test]$ tree .dvc\/cache\n.dvc\/cache\n\u2514\u2500\u2500 0c\n   \u2514\u2500\u2500 12dce03223117e423606e92650192c\n\n1 directory, 1 file\n\n(.env) [ivan@ivan ~\/Projects\/test]$ ls -lh data.xml\ndata.xml ----&gt; .dvc\/cache\/0c\/12dce03223117e423606e92650192c (some type of link)\n<\/code><\/pre>\n<\/blockquote>\n\n<h2>Scenario 2: Modifying directory<\/h2>\n\n<p>Let's now imagine we have a single large 1GB directory <code>images<\/code> with a lot of files:<\/p>\n\n<pre class=\"lang-sh prettyprint-override\"><code>$ du -hs images\n1GB\n\n$ ls -l images | wc -l\n1001\n\n$ dvc add images\n<\/code><\/pre>\n\n<p>At this point we still consume 1GB. Nothing has changed. But if we modify the directory by adding more files (or removing some of them):<\/p>\n\n<pre><code>$ cp \/tmp\/new-image.png images\n\n$ ls -l images | wc -l\n1002\n\n$ dvc add images\n<\/code><\/pre>\n\n<p>In this case, after saving the new version we <strong>still close to 1GB<\/strong> consumption. <strong>DVC calculates diff on the directory level.<\/strong> It won't be saving all the files that were existing before in the directory.<\/p>\n\n<p>The same logic applies to all commands that save data files or directories into DVC cache - <code>dvc add<\/code>, <code>dvc run<\/code>, etc.<\/p>\n\n<p>Please, let me know if it's clear or we need to add more details, clarifications.<\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_date":"2020-02-23 19:57:47.857000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":12.0,
        "Question_valid_tags":[
            "dvc"
        ]
    },
    {
        "Question_id":71338160,
        "Question_title":"DVC Experiment management workflow",
        "Question_body":"<p>I'm struggling with the DVC experiment management. Suppose the following scenario:<\/p>\n<p>I have <code>params.yaml<\/code> file:<\/p>\n<pre><code>recommendations:\n  k: 66\n  q: 5\n<\/code><\/pre>\n<p>I run the experiment with <code>dvc exp run -n exp_66<\/code>, and then I do <code>dvc exp push origin exp_66<\/code>. After this, I modify <code>params.yaml<\/code> file:<\/p>\n<pre><code>recommendations:\n  k: 99\n  q: 5\n<\/code><\/pre>\n<p>and then run another experiment <code>dvc exp run -n exp_99<\/code>, after which I commit with <code>dvc exp push origin exp_99<\/code>.<\/p>\n<p>Now, when I pull the corresponding branch with Git, I try to pull <code>exp_66<\/code> from dvc by running <code>dvc exp pull origin exp_66<\/code>. This does the pull (no error messages), but the content of the <code>params.yaml<\/code> file is with <code>k: 99<\/code> (and I would expect <code>k: 66<\/code>). What am I doing wrong? Does <code>git push<\/code> have to be executed after <code>dvc push<\/code>? Apart from that, I also found <code>dvc exp apply exp_66<\/code>, but I'm not sure what it does (it is suggested that after <code>apply<\/code> one should execute <code>git add .<\/code>, then <code>git commit<\/code>?<\/p>\n<p>I would really appreciate if you could write down the workflow with committing different experiments, pushing, pulling, applying, etc.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2022-03-03 13:38:53.127000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"git|dvcs|dvc",
        "Question_view_count":152,
        "Owner_creation_date":"2022-01-28 12:42:49.633000 UTC",
        "Owner_last_access_date":"2022-09-21 13:14:46.480000 UTC",
        "Owner_location":null,
        "Owner_reputation":67,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":"<p>You did everything alright. In the end, after pulling, you can see that when using <code>dvc exp show<\/code> your experiments will be there. To restore the experiment available from your experiment list into your workspace, you simply need to run <code>dvc exp apply exp_66<\/code>. DVC will make sure that the changes corresponding to this experiment will be checked out.<\/p>\n<p>Your workflow seems correct so far. One addition: once you make sure one of the experiments is what you want to &quot;keep&quot; in git history, you can use <code>dvc exp branch {exp_id} {branch_name}<\/code> to create a separate branch for this experiment. Then you can use <code>git<\/code> commands to save the changes.<\/p>",
        "Answer_comment_count":5.0,
        "Answer_creation_date":"2022-03-03 15:05:59.073000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":3.0,
        "Question_valid_tags":[
            "dvc"
        ]
    },
    {
        "Question_id":55132599,
        "Question_title":"Difference in usecases for AWS Sagemaker vs Databricks?",
        "Question_body":"<p>I was looking at Databricks because it integrates with AWS services like Kinesis, but it looks to me like SageMaker is a direct competitor to Databricks? We are heavily using AWS, is there any reason to add DataBricks into the stack or odes SageMaker fill the same role?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2019-03-13 00:23:26.600000 UTC",
        "Question_favorite_count":3.0,
        "Question_last_edit_date":null,
        "Question_score":9,
        "Question_tags":"apache-spark|pyspark|databricks|amazon-sagemaker",
        "Question_view_count":11894,
        "Owner_creation_date":"2015-01-15 17:43:03.700000 UTC",
        "Owner_last_access_date":"2022-08-23 22:54:25.603000 UTC",
        "Owner_location":null,
        "Owner_reputation":1387,
        "Owner_up_votes":51,
        "Owner_down_votes":1,
        "Owner_views":153,
        "Answer_body":"<p>SageMaker is a great tool for deployment, it simplifies a lot of processes configuring containers, you only need to write 2-3 lines to deploy the model as an endpoint and use it.  SageMaker also provides the dev platform (Jupyter Notebook) which supports Python and Scala (sparkmagic kernal) developing, and i managed installing external scala kernel in jupyter notebook. Overall, SageMaker provides end-to-end ML services. Databricks has unbeatable Notebook environment for Spark development. <\/p>\n\n<p>Conclusion<\/p>\n\n<ol>\n<li><p>Databricks is a better platform for Big data(scala, pyspark) Developing.(unbeatable notebook environment)<\/p><\/li>\n<li><p>SageMaker is better for Deployment. and if you are not working on big data, SageMaker is a perfect choice working with (Jupyter notebook + Sklearn + Mature containers + Super easy deployment). <\/p><\/li>\n<li><p>SageMaker provides \"real time inference\", very easy to build and deploy, very impressive. you can check the official SageMaker Github.\n<a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/sagemaker-python-sdk\/scikit_learn_inference_pipeline\" rel=\"noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/sagemaker-python-sdk\/scikit_learn_inference_pipeline<\/a><\/p><\/li>\n<\/ol>",
        "Answer_comment_count":4.0,
        "Answer_creation_date":"2019-03-20 21:40:34.270000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":14.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":33091830,
        "Question_title":"How best to convert from azure blob csv format to pandas dataframe while running notebook in azure ml",
        "Question_body":"<p>I have a number of large csv (tab delimited) data stored as azure blobs, and I want to create a pandas dataframe from these. I can do this locally as follows:<\/p>\n\n<pre><code>from azure.storage.blob import BlobService\nimport pandas as pd\nimport os.path\n\nSTORAGEACCOUNTNAME= 'account_name'\nSTORAGEACCOUNTKEY= \"key\"\nLOCALFILENAME= 'path\/to.csv'        \nCONTAINERNAME= 'container_name'\nBLOBNAME= 'bloby_data\/000000_0'\n\nblob_service = BlobService(account_name=STORAGEACCOUNTNAME, account_key=STORAGEACCOUNTKEY)\n\n# Only get a local copy if haven't already got it\nif not os.path.isfile(LOCALFILENAME):\n    blob_service.get_blob_to_path(CONTAINERNAME,BLOBNAME,LOCALFILENAME)\n\ndf_customer = pd.read_csv(LOCALFILENAME, sep='\\t')\n<\/code><\/pre>\n\n<p>However, when running the notebook on azure ML notebooks, I can't 'save a local copy' and then read from csv, and so I'd like to do the conversion directly (something like pd.read_azure_blob(blob_csv) or just pd.read_csv(blob_csv) would be ideal).<\/p>\n\n<p>I can get to the desired end result (pandas dataframe for blob csv data), if I first create an azure ML workspace, and then read the datasets into that, and finally using <a href=\"https:\/\/github.com\/Azure\/Azure-MachineLearning-ClientLibrary-Python\" rel=\"noreferrer\">https:\/\/github.com\/Azure\/Azure-MachineLearning-ClientLibrary-Python<\/a> to access the dataset as a pandas dataframe, but I'd prefer to just read straight from the blob storage location.<\/p>",
        "Question_answer_count":5,
        "Question_comment_count":0,
        "Question_creation_date":"2015-10-12 23:31:58.393000 UTC",
        "Question_favorite_count":3.0,
        "Question_last_edit_date":"2015-10-14 09:20:26.277000 UTC",
        "Question_score":12,
        "Question_tags":"python|azure|pandas|azure-blob-storage|azure-machine-learning-studio",
        "Question_view_count":22789,
        "Owner_creation_date":"2012-08-30 20:36:52.420000 UTC",
        "Owner_last_access_date":"2018-06-14 21:10:57.573000 UTC",
        "Owner_location":null,
        "Owner_reputation":395,
        "Owner_up_votes":15,
        "Owner_down_votes":0,
        "Owner_views":26,
        "Answer_body":"<p>I think you want to use <code>get_blob_to_bytes<\/code>, <code>or get_blob_to_text<\/code>; these should output a string which you can use to create a dataframe as<\/p>\n\n<pre><code>from io import StringIO\nblobstring = blob_service.get_blob_to_text(CONTAINERNAME,BLOBNAME)\ndf = pd.read_csv(StringIO(blobstring))\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2015-10-12 23:38:59.743000 UTC",
        "Answer_last_edit_date":"2018-12-18 06:59:18.717000 UTC",
        "Answer_score":17.0,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":70553701,
        "Question_title":"Couldn't import cv2 in SageMaker Studio Lab",
        "Question_body":"<p>I tried importing cv2 from opencv but i got error saying<\/p>\n<blockquote>\n<p>import error: libgthread-2.0.so.0: cannot open shared object file: No such file or directory<\/p>\n<\/blockquote>\n<p>Since Sagemaker Studio Labs doesn't support installation of Ubuntu packages i couldn't use <code>apt-get<\/code> or <code>yum<\/code> to install libglib2.0-0.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2022-01-02 04:58:24.523000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-01-02 07:07:20.987000 UTC",
        "Question_score":1,
        "Question_tags":"opencv|amazon-sagemaker",
        "Question_view_count":534,
        "Owner_creation_date":"2020-05-19 16:45:19.620000 UTC",
        "Owner_last_access_date":"2022-06-10 06:36:10.120000 UTC",
        "Owner_location":null,
        "Owner_reputation":63,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Answer_body":"<p>With this line, you can install the glib dependency for Amazon Sagemaker Studio Lab. Just run it on your notebook cell.<\/p>\n<pre><code>! conda install glib=2.51.0 -y\n<\/code><\/pre>\n<p>You also can create another virtual environment for your session that contains glib:<\/p>\n<pre><code>! conda create -n glib-test -c defaults -c conda-forge python=3 glib=2.51.0` -y\n<\/code><\/pre>\n<p>After that maybe you need albumentations to import cv2:<\/p>\n<pre><code>! pip install albumentations\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-01-02 06:06:53.577000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":71284125,
        "Question_title":"GCP Vertex AI Training: Auto-packaged Custom Training Job Yields Huge Docker Image",
        "Question_body":"<p>I am trying to run a Custom Training Job in Google Cloud Platform's Vertex AI Training service.<\/p>\n<p>The job is based on <a href=\"https:\/\/cloud.google.com\/blog\/topics\/developers-practitioners\/pytorch-google-cloud-how-train-and-tune-pytorch-models-vertex-ai\" rel=\"nofollow noreferrer\">a tutorial from Google that fine-tunes a pre-trained BERT model<\/a> (from HuggingFace).<\/p>\n<p>When I use the <code>gcloud<\/code> CLI tool to auto-package my training code into a Docker image and deploy it to the Vertex AI Training service like so:<\/p>\n<pre><code>$BASE_GPU_IMAGE=&quot;us-docker.pkg.dev\/vertex-ai\/training\/pytorch-gpu.1-7:latest&quot;\n$BUCKET_NAME = &quot;my-bucket&quot;\n\ngcloud ai custom-jobs create `\n--region=us-central1 `\n--display-name=fine_tune_bert `\n--args=&quot;--job_dir=$BUCKET_NAME,--num-epochs=2,--model-name=finetuned-bert-classifier&quot; `\n--worker-pool-spec=&quot;machine-type=n1-standard-4,replica-count=1,accelerator-type=NVIDIA_TESLA_V100,executor-image-uri=$BASE_GPU_IMAGE,local-package-path=.,python-module=trainer.task&quot;\n<\/code><\/pre>\n<p>... I end up with a Docker image that is roughly 18GB (!) and takes a very long time to upload to the GCP registry.<\/p>\n<p>Granted <a href=\"https:\/\/console.cloud.google.com\/gcr\/images\/deeplearning-platform-release\/GLOBAL\/pytorch-gpu.1-7?tag=nightly-2021-03-28\" rel=\"nofollow noreferrer\">the base image is around 6.5GB<\/a> but <strong>where do the additional &gt;10GB come from and is there a way for me to avoid this &quot;image bloat&quot;?<\/strong><\/p>\n<p>Please note that my job loads the training data using the <code>datasets<\/code> Python package at run time and AFAIK does not include it in the auto-packaged docker image.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2022-02-27 10:49:52.960000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"google-cloud-platform|pytorch|google-cloud-vertex-ai",
        "Question_view_count":478,
        "Owner_creation_date":"2008-11-02 09:34:07.787000 UTC",
        "Owner_last_access_date":"2022-09-21 09:19:31.780000 UTC",
        "Owner_location":"Tel Aviv, Israel",
        "Owner_reputation":15116,
        "Owner_up_votes":890,
        "Owner_down_votes":56,
        "Owner_views":1182,
        "Answer_body":"<p>The image size shown in the UI is the virtual size of the image. It is the compressed total image size that will be downloaded over the network. Once the image is pulled, it will be extracted and the resulting size will be bigger. In this case, the <a href=\"https:\/\/console.cloud.google.com\/artifacts\/docker\/vertex-ai\/us\/training\/pytorch-gpu.1-7\/sha256:0d990ebc4fc376880fd3ee375015e43594450d11d9791ac203cf2d044871917f\" rel=\"nofollow noreferrer\">PyTorch image's virtual size<\/a> is 6.8 GB while the actual size is 17.9 GB.<\/p>\n<p>Also, when a <code>docker push<\/code> command is executed, the progress bars show the uncompressed size. The actual amount of data that\u2019s pushed will be <a href=\"https:\/\/docs.docker.com\/engine\/reference\/commandline\/push\/#extended-description\" rel=\"nofollow noreferrer\">compressed before sending<\/a>, so the uploaded size will not be reflected by the progress bar.<\/p>\n<p>To cut down the size of the docker image, custom containers can be used. Here, only the necessary components can be configured which would result in a smaller docker image. More information on custom containers <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/containers-overview\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-03-01 08:34:54.693000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "google-cloud-vertex-ai"
        ]
    },
    {
        "Question_id":48438202,
        "Question_title":"Errors while using sagemaker api to invoke endpoints",
        "Question_body":"<p>I've deployed an endpoint in sagemaker and was trying to invoke it through my python program. I had tested it using postman and it worked perfectly ok. Then I wrote the invocation code as follows<\/p>\n\n<pre><code>import boto3\nimport pandas as pd\nimport io\nimport numpy as np\n\ndef np2csv(arr):\n    csv = io.BytesIO()\n    np.savetxt(csv, arr, delimiter=',', fmt='%g')\n    return csv.getvalue().decode().rstrip()\n\n\nruntime= boto3.client('runtime.sagemaker')\npayload = np2csv(test_X)\n\nruntime.invoke_endpoint(\n    EndpointName='&lt;my-endpoint-name&gt;',\n    Body=payload,\n    ContentType='text\/csv',\n    Accept='Accept'\n)\n<\/code><\/pre>\n\n<p>Now whe I run this I get a validation error<\/p>\n\n<pre><code>ValidationError: An error occurred (ValidationError) when calling the InvokeEndpoint operation: Endpoint &lt;my-endpoint-name&gt; of account &lt;some-unknown-account-number&gt; not found.\n<\/code><\/pre>\n\n<p>While using postman i had given my access key and secret key but I'm not sure how to pass it when using sagemaker apis. I'm not able to find it in the documentation also. <\/p>\n\n<p>So my question is, how can I use sagemaker api from my local machine to invoke my endpoint?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_date":"2018-01-25 08:05:19.790000 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":"2018-01-25 10:13:29.620000 UTC",
        "Question_score":5,
        "Question_tags":"python|python-3.x|amazon-web-services|amazon-sagemaker",
        "Question_view_count":4467,
        "Owner_creation_date":"2014-09-17 16:42:55.307000 UTC",
        "Owner_last_access_date":"2022-09-22 07:49:15.917000 UTC",
        "Owner_location":null,
        "Owner_reputation":1124,
        "Owner_up_votes":156,
        "Owner_down_votes":1,
        "Owner_views":153,
        "Answer_body":"<p>When you are using any of the AWS SDK (including the one for Amazon SageMaker), you need to configure the credentials of your AWS account on the machine that you are using to run your code. If you are using your local machine, you can use the AWS CLI flow. You can find detailed instructions on the Python SDK page: <a href=\"https:\/\/aws.amazon.com\/developers\/getting-started\/python\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/developers\/getting-started\/python\/<\/a> <\/p>\n\n<p>Please note that when you are deploying the code to a different machine, you will have to make sure that you are giving the EC2, ECS, Lambda or any other target a role that will allow the call to this specific endpoint. While in your local machine it can be OK to give you admin rights or other permissive permissions, when you are deploying to a remote instance, you should restrict the permissions as much as possible. <\/p>\n\n<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"VisualEditor0\",\n            \"Effect\": \"Allow\",\n            \"Action\": \"sagemaker:InvokeEndpoint\",\n            \"Resource\": \"arn:aws:sagemaker:*:1234567890:endpoint\/&lt;my-endpoint-name&gt;\"\n        }\n    ]\n}\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2018-01-28 04:31:53.470000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":4.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":72411618,
        "Question_title":"MLFLOW and Postgres getting Bad Request error",
        "Question_body":"<p>I have been pulling my hair trying to figure out what's wrong with mlflow. Iam deploying mlflow v1.26 in google cloudRun . back end artitfactory is google storage and backend database is google cloudsql postgres v13 instance.<\/p>\n<p>here is my entrypoint using pg8000 v1.21.3 (I tried latest version as well) and psycopg2-binary v2.9.3<\/p>\n<pre><code>\nset -e\nexport ARTIFACT_URL=&quot;gs:\/\/ei-cs-dev01-ein-sb-teambucket-chaai-01\/mlflow\/&quot;\nexport DATABASE_URL=&quot;postgresql+pg8000:\/\/mlflow:change2022@10.238.139.37:5432\/mlflowps&quot; #&quot;$(python3 \/app\/get_secret.py --project=&quot;${GCP_PROJECT}&quot; --secret=mlflow_database_url)&quot;\n\nif [[ -z &quot;${PORT}&quot; ]]; then\n    export PORT=8080\nfi\n\nexec mlflow server -h 0.0.0.0 -w 4 -p ${PORT} --default-artifact-root ${ARTIFACT_URL} --backend-store-uri ${DATABASE_URL}\n<\/code><\/pre>\n<p>now when I open mlflow ui page I see this error happening:\n(<\/p>\n<blockquote>\n<p>BAD_REQUEST: (pg8000.dbapi.ProgrammingError) {'S': 'ERROR', 'V':\n'ERROR', 'C': '42883', 'M': 'operator does not exist: integer =\ncharacter varying', 'H': 'No operator matches the given name and\nargument types. You might need to add explicit type casts.', 'P':\n'382', 'F': 'parse_oper.c', 'L': '731', 'R': 'op_error'} [SQL: SELECT\nDISTINCT runs.run_uuid..<\/p>\n<\/blockquote>\n<p>)\n<a href=\"https:\/\/i.stack.imgur.com\/gjbLj.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/gjbLj.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-05-27 22:30:11.447000 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"postgresql|google-cloud-platform|mlflow|pg8000",
        "Question_view_count":185,
        "Owner_creation_date":"2022-02-11 00:55:25.377000 UTC",
        "Owner_last_access_date":"2022-09-02 21:42:33.800000 UTC",
        "Owner_location":null,
        "Owner_reputation":33,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":"<p>You should use psycopg2 instead, e.g.:<\/p>\n<p><code>postgresql+psycopg2:\/\/&lt;username&gt;:&lt;password&gt;@\/&lt;dbname&gt;?host=\/cloudsql\/&lt;my-project&gt;:&lt;us-central1&gt;:&lt;dbinstance&gt;<\/code><\/p>\n<p>It works for me, with versions:<\/p>\n<p>mlflow==1.26.1<\/p>\n<p>psycopg2-binary==2.9.3<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2022-06-17 10:05:21.537000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "mlflow"
        ]
    },
    {
        "Question_id":64286191,
        "Question_title":"How to add keyboard shortcuts to AWS Ground Truth labeler UI?",
        "Question_body":"<p>I'm using AWS Sagemaker Ground Truth for a Custom Labeling Task that involves editing bounding boxes and their labels.  Ground Truth's UI has built-in keyboard shortcuts for doing things like choosing the label for a box, but it seems to lack shortcuts for other built-in UI elements like &quot;No adjustments needed&quot; or the &quot;Submit&quot; button.<\/p>\n<p>Is there a way to add such shortcuts?  I've looked at the crowd-html-elements for customizing the appearance of the page, but can't find anything in there about keyboard shortcuts.  It doesn't even look like crowd-button or crowd-icon-button support specifying a shortcut as an attribute.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-10-09 19:28:19.933000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"amazon-sagemaker|labeling",
        "Question_view_count":412,
        "Owner_creation_date":"2010-11-14 22:01:50.723000 UTC",
        "Owner_last_access_date":"2022-09-13 00:36:55.383000 UTC",
        "Owner_location":"Mt Kisco, NY",
        "Owner_reputation":309,
        "Owner_up_votes":14,
        "Owner_down_votes":0,
        "Owner_views":40,
        "Answer_body":"<p>Could try something like:<\/p>\n<pre><code>document.addEventListener('keydown', function(event) {\n  if (event.shiftKey &amp;&amp; event.keyCode === 13) {\n    document.getElementsByTagName('crowd-bounding-box')[0].shadowRoot.getElementById('nothing-to-adjust').querySelector('label').click();\n  }\n});\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2020-11-20 00:10:50.090000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":3.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":37807158,
        "Question_title":"Train multiple models with various measures and accumulate predictions",
        "Question_body":"<p>So I have been playing around with Azure ML lately, and I got one dataset where I have multiple values I want to predict. All of them uses different algorithms and when I try to train multiple models within one experiment; it says the \u201ctrain model can only predict one value\u201d, and there are not enough input ports on the train-model to take in multiple values even if I was to use the same algorithm for each measure. I tried launching the column selector and making rules, but I get the same error as mentioned. How do I predict multiple values and later put the predicted columns together for the web service output so I don\u2019t have to have multiple API\u2019s?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2016-06-14 08:47:55.710000 UTC",
        "Question_favorite_count":0.0,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"azure|azure-machine-learning-studio",
        "Question_view_count":1763,
        "Owner_creation_date":"2016-05-12 08:21:29.043000 UTC",
        "Owner_last_access_date":"2016-06-14 14:37:43.043000 UTC",
        "Owner_location":null,
        "Owner_reputation":25,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Answer_body":"<p>What you would want to do is to train each model and save them as already trained models.\nSo create a new experiment, train your models and save them by right clicking on each model and they will show up in the left nav bar in the Studio. Now you are able to drag your models into the canvas and have them score predictions where you eventually make them end up in the same output as I have done in my example through the \u201cAdd columns\u201d module. I made this example for Ronaldo (Real Madrid CF player) on how he will perform in match after training day. You can see my demo on <a href=\"http:\/\/ronaldoinform.azurewebsites.net\" rel=\"nofollow noreferrer\">http:\/\/ronaldoinform.azurewebsites.net<\/a><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/ZwzUy.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ZwzUy.png\" alt=\"Ronaldo InForm\"><\/a><\/p>\n\n<p>For more detailed explanation on how to save the models and train multiple values; you can check out Raymond Langaeian (MSFT) answer in the comment section on this link:\n<a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-convert-training-experiment-to-scoring-experiment\/\" rel=\"nofollow noreferrer\">https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-convert-training-experiment-to-scoring-experiment\/<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2016-06-14 11:42:44.903000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":71223654,
        "Question_title":"How to get wandb to pass arguments by position?",
        "Question_body":"<p>I am trying to explore the results of different parameter settings on my python script &quot;train.py&quot;. For that, I use a wandb sweep. Each wandb agent executes the file &quot;train.py&quot; and passes some parameters to it. As per the wandb documentation (<a href=\"https:\/\/docs.wandb.ai\/guides\/sweeps\/configuration#command\" rel=\"nofollow noreferrer\">https:\/\/docs.wandb.ai\/guides\/sweeps\/configuration#command<\/a>), in case of e.g. two parameters &quot;param1&quot; and &quot;param2&quot; each agents starts the file with the command<\/p>\n<pre><code>\/usr\/bin\/env python train.py --param1=value1 --param2=value2\n<\/code><\/pre>\n<p>However, &quot;train.py&quot; expects<\/p>\n<pre><code>\/usr\/bin\/env python train.py value1 value2\n<\/code><\/pre>\n<p>and parses the parameter values by position. I did not write train.py and would like to not change it if possible. How can I get wandb to pass the values without &quot;--param1=&quot; in front?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-02-22 15:07:04.610000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|python-3.x|wandb",
        "Question_view_count":270,
        "Owner_creation_date":"2015-08-24 11:16:20.200000 UTC",
        "Owner_last_access_date":"2022-02-23 15:36:22.560000 UTC",
        "Owner_location":"Germany",
        "Owner_reputation":5,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":12,
        "Answer_body":"<p>Don't think you can get positional arguments from W&amp;B Sweeps. However, there's a little work around you can try that won't require you touching the <code>train.py<\/code> file.<\/p>\n<p>You can create an invoker file, let's call it <code>invoke.py<\/code>. Now, you can use it get rid of the keyword argument names. Something like this might work:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import sys\nimport subprocess\n\nif len(sys.argv[0]) &lt;= 1:\n  print(f&quot;{sys.argv[0]} program_name param0=&lt;param0&gt; param1=&lt;param1&gt; ...&quot;)\n  sys.exit(0)\n\nprogram = sys.argv[1]\nparams = sys.argv[2:]\n\nposparam = []\nfor param in params:\n  _, val = param.split(&quot;=&quot;)\n  posparam.append(val)\n\ncommand = [sys.executable, program, *posparam]\nprocess = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\nout, err = process.communicate()\nsys.stdout.write(out.decode())\nsys.stdout.flush()\nsys.stderr.write(err.decode())\nsys.stderr.flush()\nsys.exit(process.returncode)\n<\/code><\/pre>\n<p>This allows you to invoke your <code>train.py<\/code> file as follows:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>$ python3 invoke.py \/path\/to\/train.py param0=0.001 param1=20 ...\n<\/code><\/pre>\n<p>Now to perform W&amp;B sweeps you can create a <code>command:<\/code> section (<a href=\"https:\/\/docs.wandb.ai\/guides\/sweeps\/configuration#command\" rel=\"nofollow noreferrer\">reference<\/a>) in your <code>sweeps.yaml<\/code> file while sweeping over the parameters <code>param0<\/code> and <code>param1<\/code>. For example:<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>program: invoke.py\n...\nparameters:\n  param0:\n    distribution: uniform\n    min: 0\n    max: 1\n  param1:\n    distribution: categorical\n    values: [10, 20, 30]\ncommand:\n - ${env}\n - ${program}\n - \/path\/to\/train.py\n - ${args_no_hyphens}\n<\/code><\/pre>",
        "Answer_comment_count":3.0,
        "Answer_creation_date":"2022-02-22 16:51:46.457000 UTC",
        "Answer_last_edit_date":"2022-02-22 16:56:56.653000 UTC",
        "Answer_score":1.0,
        "Question_valid_tags":[
            "wandb"
        ]
    },
    {
        "Question_id":44932098,
        "Question_title":"Azure ML- Execute Python Script -Datatime.date not working",
        "Question_body":"<p>I am getting the following error when I try to convert a datetime variable to date.<\/p>\n\n<p><strong>My Code<\/strong><\/p>\n\n<pre><code>import datetime as dt \n\ndf['TXN_DATE_2'] = df['TXN_DATE'].dt.date\n<\/code><\/pre>\n\n<p><strong>Error<\/strong><\/p>\n\n<blockquote>\n  <p>raise NotImplementedError('Python Bridge conversion table not\n  implemented for type [{0}]'.format(value.getType()))\n  NotImplementedError: Python Bridge conversion table not implemented\n  for type [] Process returned with non-zero exit\n  code 1<\/p>\n<\/blockquote>\n\n<p>Can anyone please tell me what is going on.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2017-07-05 17:01:59.590000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2017-07-05 19:01:38.240000 UTC",
        "Question_score":0,
        "Question_tags":"python|azure|azure-machine-learning-studio",
        "Question_view_count":449,
        "Owner_creation_date":"2017-07-05 16:58:14.443000 UTC",
        "Owner_last_access_date":"2022-02-17 19:35:05.767000 UTC",
        "Owner_location":null,
        "Owner_reputation":35,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":12,
        "Answer_body":"<p>Please try to use the code below to convert as you want.<\/p>\n\n<pre><code>import pandas as pd\nimport datetime as dt\ndf['TXN_DATE_2'] = pd.to_datetime(df['TXN_DATE']).dt.date\n<\/code><\/pre>\n\n<p>Hope it helps.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2017-07-06 08:25:12.083000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":69302528,
        "Question_title":"How to pass environment variables to gcloud beta ai custom-jobs create with custom container (Vertex AI)",
        "Question_body":"<p>I'm running custom training jobs in google's Vertex AI. A simple <code>gcloud<\/code> command to execute a custom job would use something like the following syntax (complete documentation for the command can be seen <a href=\"https:\/\/cloud.google.com\/sdk\/gcloud\/reference\/beta\/ai\/custom-jobs\/create#--config\" rel=\"nofollow noreferrer\">here<\/a>):<\/p>\n<pre><code>gcloud beta ai custom-jobs create --region=us-central1 \\\n--display-name=test \\\n--config=config.yaml\n<\/code><\/pre>\n<p>In the <code>config.yaml<\/code> file, it is possible to specify the machine and accelerator (GPU) types, etc., and in my case, point to a custom container living in the Google Artifact Registry that executes the training code (specified in the <code>imageUri<\/code> part of the <code>containerSpec<\/code>). An example config file may look like this:<\/p>\n<pre><code># config.yaml\nworkerPoolSpecs:\n  machineSpec:\n    machineType: n1-highmem-2\n    acceleratorType: NVIDIA_TESLA_P100\n    acceleratorCount: 2\n  replicaCount: 1\n  containerSpec:\n    imageUri: {URI_FOR_CUSTOM_CONATINER}\n    args:\n    - {ARGS TO PASS TO CONTAINER ENTRYPOINT COMMAND}\n<\/code><\/pre>\n<p>The code we're running needs some runtime environment variables (that need to be secure) passed to the container. In the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/reference\/rest\/v1\/CustomJobSpec#containerspec\" rel=\"nofollow noreferrer\">API documentation<\/a> for the <code>containerSpec<\/code>, it says it is possible to set environment variables as follows:<\/p>\n<pre><code># config.yaml\nworkerPoolSpecs:\n  machineSpec:\n    machineType: n1-highmem-2\n    acceleratorType: NVIDIA_TESLA_P100\n    acceleratorCount: 2\n  replicaCount: 1\n  containerSpec:\n    imageUri: {URI_FOR_CUSTOM_CONATINER}\n    args:\n    - {ARGS TO PASS TO CONTAINER ENTRYPOINT COMMAND}\n    env:\n    - name: SECRET_ONE\n      value: $SECRET_ONE\n    - name: SECRET_TWO\n      value: $SECRET_TWO\n<\/code><\/pre>\n<p>When I try and add the <code>env<\/code> flag to the <code>containerSpec<\/code>, I get an error saying it's not part of the container spec:<\/p>\n<pre><code>ERROR: (gcloud.beta.ai.custom-jobs.create) INVALID_ARGUMENT: Invalid JSON payload received. Unknown name &quot;env&quot; at 'custom_job.job_spec.worker_pool_specs[0].container_spec': Cannot find field.\n- '@type': type.googleapis.com\/google.rpc.BadRequest\n  fieldViolations:\n  - description: &quot;Invalid JSON payload received. Unknown name \\&quot;env\\&quot; at 'custom_job.job_spec.worker_pool_specs[0].container_spec':\\\n      \\ Cannot find field.&quot;\n    field: custom_job.job_spec.worker_pool_specs[0].container_spec\n<\/code><\/pre>\n<p>Any idea how to securely set runtime environment variables in Vertex AI custom jobs using custom containers?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":7,
        "Question_creation_date":"2021-09-23 15:03:34.673000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-09-23 15:22:11.833000 UTC",
        "Question_score":3,
        "Question_tags":"docker|google-cloud-platform|gcloud|google-cloud-sdk|google-cloud-vertex-ai",
        "Question_view_count":1033,
        "Owner_creation_date":"2016-05-24 16:22:09.610000 UTC",
        "Owner_last_access_date":"2022-09-13 16:37:07.037000 UTC",
        "Owner_location":null,
        "Owner_reputation":457,
        "Owner_up_votes":12,
        "Owner_down_votes":0,
        "Owner_views":37,
        "Answer_body":"<p>There are two versions of the REST API - \u201c<a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/reference\/rest\/v1\/CustomJobSpec#containerspec\" rel=\"nofollow noreferrer\">v1<\/a>\u201d and \u201c<a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/reference\/rest\/v1beta1\/CustomJobSpec#containerspec\" rel=\"nofollow noreferrer\">v1beta1<\/a>\u201d where &quot;v1beta1&quot; does not have the <code>env<\/code> option in <code>ContainerSpec<\/code> but &quot;v1&quot; does. The <code>gcloud ai custom-jobs create<\/code> command without the <code>beta<\/code> parameter doesn\u2019t throw the error as it uses version \u201cv1\u201d to make the API calls.<\/p>\n<p>The environment variables from the yaml file can be passed to the custom container in the following way:<\/p>\n<p>This is the docker file of the sample custom training application I used to test the requirement. Please refer to this <a href=\"https:\/\/codelabs.developers.google.com\/vertex_custom_training_prediction\" rel=\"nofollow noreferrer\">codelab<\/a> for more information about the training application.<\/p>\n<pre class=\"lang-docker prettyprint-override\"><code>FROM gcr.io\/deeplearning-platform-release\/tf2-cpu.2-3\nWORKDIR \/root\n\nWORKDIR \/\n\n# Copies the trainer code to the docker image.\nCOPY trainer \/trainer\n\n\n# Copies the bash script to the docker image.\nCOPY commands.sh \/scripts\/commands.sh\n\n# Bash command to make the script file an executable\nRUN [&quot;chmod&quot;, &quot;+x&quot;, &quot;\/scripts\/commands.sh&quot;]\n\n\n# Command to execute the file\nENTRYPOINT [&quot;\/scripts\/commands.sh&quot;]\n\n# Sets up the entry point to invoke the trainer.\n# ENTRYPOINT &quot;python&quot; &quot;-m&quot; $SECRET_TWO \u21d2 To use the environment variable  \n# directly in the docker ENTRYPOINT. In case you are not using a bash script, \n# the trainer can be invoked directly from the docker ENTRYPOINT.\n<\/code><\/pre>\n<br \/>\n<p>Below is the <code>commands.sh<\/code> file used in the docker container to test whether the environment variables are passed to the container.<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>#!\/bin\/bash\nmkdir \/root\/.ssh\necho $SECRET_ONE\npython -m $SECRET_TWO\n<\/code><\/pre>\n<br \/>\n<p>The example <code>config.yaml<\/code> file<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code># config.yaml\nworkerPoolSpecs:\n  machineSpec:\n    machineType: n1-highmem-2\n  replicaCount: 1\n  containerSpec:\n    imageUri: gcr.io\/infosys-kabilan\/mpg:v1\n    env:\n    - name: SECRET_ONE\n      value: &quot;Passing the environment variables&quot;\n    - name: SECRET_TWO\n      value: &quot;trainer.train&quot;\n<\/code><\/pre>\n<p>As the next step, I built and pushed the container to Google Container Repository. Now, the <code>gcloud ai custom-jobs create --region=us-central1  --display-name=test --config=config.yaml<\/code> can be run to create the custom training job and the output of the <code>commands.sh<\/code> file can be seen in the job logs as shown below.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/qRHV7.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/qRHV7.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2021-09-30 07:43:34.700000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":4.0,
        "Question_valid_tags":[
            "google-cloud-vertex-ai"
        ]
    },
    {
        "Question_id":57396212,
        "Question_title":"SageMaker and TensorFlow 2.0",
        "Question_body":"<p>What is the best way to run TensorFlow 2.0 with AWS Sagemeker?<\/p>\n\n<p>As of today (Aug 7th, 2019) AWS does not provide TensorFlow 2.0 <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-container\" rel=\"noreferrer\">SageMaker containers<\/a>, so my understanding is that I need to build my own.<\/p>\n\n<p>What is the best Base image to use? Example Dockerfile?<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":1,
        "Question_creation_date":"2019-08-07 14:00:51.297000 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":"2019-08-07 14:06:30.803000 UTC",
        "Question_score":13,
        "Question_tags":"tensorflow|amazon-sagemaker|tensorflow2.0",
        "Question_view_count":4136,
        "Owner_creation_date":"2015-11-07 01:25:10.543000 UTC",
        "Owner_last_access_date":"2022-09-24 20:44:03.983000 UTC",
        "Owner_location":"Toronto, Canada",
        "Owner_reputation":3259,
        "Owner_up_votes":104,
        "Owner_down_votes":7,
        "Owner_views":233,
        "Answer_body":"<p>EDIT: <strong>Amazon SageMaker does now support TF 2.0 and higher.<\/strong><\/p>\n<ul>\n<li>SageMaker + TensorFlow docs: <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/using_tf.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/using_tf.html<\/a><\/li>\n<li>Supported Tensorflow versions (and Docker URIs): <a href=\"https:\/\/aws.amazon.com\/releasenotes\/available-deep-learning-containers-images\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/releasenotes\/available-deep-learning-containers-images<\/a><\/li>\n<\/ul>\n<hr \/>\n<p><em>Original answer<\/em><\/p>\n<p>Here is an example Dockerfile that uses <a href=\"https:\/\/github.com\/aws\/sagemaker-containers\" rel=\"nofollow noreferrer\">the underlying SageMaker Containers library<\/a> (this is what is used in the official pre-built Docker images):<\/p>\n<pre><code>FROM tensorflow\/tensorflow:2.0.0b1\n\nRUN pip install sagemaker-containers\n\n# Copies the training code inside the container\nCOPY train.py \/opt\/ml\/code\/train.py\n\n# Defines train.py as script entrypoint\nENV SAGEMAKER_PROGRAM train.py\n<\/code><\/pre>\n<p>For more information on this approach, see <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/build-container-to-train-script-get-started.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/build-container-to-train-script-get-started.html<\/a><\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_date":"2019-09-04 22:22:31.827000 UTC",
        "Answer_last_edit_date":"2020-07-17 17:54:03.227000 UTC",
        "Answer_score":10.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":68559059,
        "Question_title":"DVC connect to Min.IO to access S3",
        "Question_body":"<p>What is the proper way to connect DVC to Min.IO that is connected to some buckets on S3.<\/p>\n<pre><code>AWS-S3(My_Bucket) &gt; Min.io(MY_Bucket aliased as S3)\n<\/code><\/pre>\n<p>Right now i am accessing my bucket by using mc for example <code>mc cp s3\/my_bucket\/datasets datasets<\/code> to copy stuff from there. But I need to setup my DVC to work with min.io as a hub between AWS.S3 and DVC so i can use for example <code>&quot;DVC mc-S3 pull&quot;<\/code> and <code>&quot;DVC AWS-S3 pull&quot;<\/code>.<\/p>\n<p>How do i got for it because while googling i couldn't find anything that i could easily follow.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2021-07-28 10:52:37.323000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"amazon-s3|minio|dvc",
        "Question_view_count":375,
        "Owner_creation_date":"2020-01-09 12:58:29.920000 UTC",
        "Owner_last_access_date":"2022-03-30 10:42:37.603000 UTC",
        "Owner_location":"Poland",
        "Owner_reputation":85,
        "Owner_up_votes":7,
        "Owner_down_votes":0,
        "Owner_views":33,
        "Answer_body":"<p>It looks like you are looking for a combination of things.<\/p>\n<p>First, Jorge mentioned you can set <code>endpointurl<\/code> to access Minio the same way as you would access regular S3:<\/p>\n<pre><code>dvc remote add -d minio-remote s3:\/\/mybucket\/path\ndvc remote modify minio-remote endpointurl https:\/\/minio.example.com                          \n<\/code><\/pre>\n<p>Second, it seems you can create <em>two<\/em> remotes - one for S3, one for Minio and use <code>-r<\/code> option that is available for many data management related commands:<\/p>\n<pre><code>dvc pull -r minio-remote\ndvc pull -r s3-remote\ndvc push -r minio-remote\n...\n<\/code><\/pre>\n<p>This way you could <code>push<\/code>\/<code>pull<\/code> data to\/from a specific storage.<\/p>\n<blockquote>\n<p>But I need to setup my DVC to work with min.io as a hub between AWS.S3 and DVC<\/p>\n<\/blockquote>\n<p>There are other possible ways, I think to organize this. It indeed depends on what semantics you expect from <code>DVC mc-S3 pull<\/code>. Please let us know if <code>-r<\/code> is not enough and clarify the question- that would help us here.<\/p>",
        "Answer_comment_count":7.0,
        "Answer_creation_date":"2021-07-28 23:03:26.643000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_valid_tags":[
            "dvc"
        ]
    },
    {
        "Question_id":63770171,
        "Question_title":"\"ImportError: No module named seaborn\" in Azure ML",
        "Question_body":"<p>Created a new compute instance in Azure ML and trained a model with out any issue. I wanted to draw a pairplot using <code>seaborn<\/code> but I keep getting the error <code>&quot;ImportError: No module named seaborn&quot;<\/code><\/p>\n<p>I ran <code>!conda list<\/code> and I can see seaborn in the list<\/p>\n<pre><code># packages in environment at \/anaconda:\n#\n# Name                    Version                   Build  Channel\n_ipyw_jlab_nb_ext_conf    0.1.0                    py37_0  \nalabaster                 0.7.12                   py37_0  \nanaconda                  2018.12                  py37_0  \nanaconda-client           1.7.2                    py37_0  \nanaconda-navigator        1.9.6                    py37_0  \nanaconda-project          0.8.2                    py37_0  \napplicationinsights       0.11.9                    &lt;pip&gt;\nasn1crypto                0.24.0                   py37_0  \nastroid                   2.1.0                    py37_0  \nastropy                   3.1              py37h7b6447c_0  \natomicwrites              1.2.1                    py37_0  \nattrs                     18.2.0           py37h28b3542_0  \nbabel                     2.6.0                    py37_0  \nbackcall                  0.1.0                    py37_0  \nbackports                 1.0                      py37_1  \nbackports.os              0.1.1                    py37_0  \nbackports.shutil_get_terminal_size 1.0.0                    py37_2  \nbeautifulsoup4            4.6.3                    py37_0  \nbitarray                  0.8.3            py37h14c3975_0  \nbkcharts                  0.2                      py37_0  \nblas                      1.0                         mkl  \nblaze                     0.11.3                   py37_0  \nbleach                    3.0.2                    py37_0  \nblosc                     1.14.4               hdbcaa40_0  \nbokeh                     1.0.2                    py37_0  \nboto                      2.49.0                   py37_0  \nbottleneck                1.2.1            py37h035aef0_1  \nbzip2                     1.0.6                h14c3975_5  \nca-certificates           2020.7.22                     0    anaconda\ncairo                     1.14.12              h8948797_3  \ncertifi                   2020.6.20                py37_0    anaconda\ncffi                      1.11.5           py37he75722e_1  \nchardet                   3.0.4                    py37_1  \nclick                     7.0                      py37_0  \ncloudpickle               0.6.1                    py37_0  \nclyent                    1.2.2                    py37_1  \ncolorama                  0.4.1                    py37_0  \nconda                     4.5.12                   py37_0  \nconda-build               3.17.6                   py37_0  \nconda-env                 2.6.0                         1  \nconda-verify              3.1.1                    py37_0  \ncontextlib2               0.5.5                    py37_0  \ncryptography              2.4.2            py37h1ba5d50_0  \ncurl                      7.63.0            hbc83047_1000  \ncycler                    0.10.0                   py37_0    anaconda\ncython                    0.29.2           py37he6710b0_0  \ncytoolz                   0.9.0.1          py37h14c3975_1  \ndask                      1.0.0                    py37_0  \ndask-core                 1.0.0                    py37_0  \ndatashape                 0.5.4                    py37_1  \ndbus                      1.13.12              h746ee38_0    anaconda\ndecorator                 4.3.0                    py37_0  \ndefusedxml                0.5.0                    py37_1  \ndistributed               1.25.1                   py37_0  \ndocutils                  0.14                     py37_0  \nentrypoints               0.2.3                    py37_2  \net_xmlfile                1.0.1                    py37_0  \nexpat                     2.2.9                he6710b0_2    anaconda\nfastcache                 1.0.2            py37h14c3975_2  \nfilelock                  3.0.10                   py37_0  \nflask                     1.0.2                    py37_1  \nflask-cors                3.0.7                    py37_0  \nfontconfig                2.13.0               h9420a91_0    anaconda\nfreetype                  2.10.2               h5ab3b9f_0    anaconda\nfribidi                   1.0.5                h7b6447c_0  \nfuture                    0.17.1                   py37_0  \nget_terminal_size         1.0.0                haa9412d_0  \ngevent                    1.3.7            py37h7b6447c_1  \nglib                      2.56.2               hd408876_0    anaconda\nglob2                     0.6                      py37_1  \ngmp                       6.1.2                h6c8ec71_1  \ngmpy2                     2.0.8            py37h10f8cd9_2  \ngraphite2                 1.3.12               h23475e2_2  \ngreenlet                  0.4.15           py37h7b6447c_0  \ngst-plugins-base          1.14.0               hbbd80ab_1    anaconda\ngstreamer                 1.14.0               hb453b48_1    anaconda\nh5py                      2.8.0            py37h989c5e5_3  \nharfbuzz                  1.8.8                hffaf4a1_0  \nhdf5                      1.10.2               hba1933b_1  \nheapdict                  1.0.0                    py37_2  \nhtml5lib                  1.0.1                    py37_0  \nicu                       58.2                 he6710b0_3    anaconda\nidna                      2.8                      py37_0  \nimageio                   2.4.1                    py37_0  \nimagesize                 1.1.0                    py37_0  \nimportlib_metadata        0.6                      py37_0  \nintel-openmp              2019.1                      144  \nipykernel                 5.1.0            py37h39e3cac_0  \nipython                   7.2.0            py37h39e3cac_0  \nipython_genutils          0.2.0                    py37_0  \nipywidgets                7.4.2                    py37_0  \nisort                     4.3.4                    py37_0  \nitsdangerous              1.1.0                    py37_0  \njbig                      2.1                  hdba287a_0  \njdcal                     1.4                      py37_0  \njedi                      0.13.2                   py37_0  \njeepney                   0.4                      py37_0  \njinja2                    2.10                     py37_0  \njpeg                      9b                   habf39ab_1    anaconda\njsonschema                2.6.0                    py37_0  \njupyter                   1.0.0                    py37_7  \njupyter_client            5.2.4                    py37_0  \njupyter_console           6.0.0                    py37_0  \njupyter_core              4.4.0                    py37_0  \njupyterlab                0.35.3                   py37_0  \njupyterlab_server         0.2.0                    py37_0  \nkeyring                   17.0.0                   py37_0  \nkiwisolver                1.2.0            py37hfd86e86_0    anaconda\nkrb5                      1.16.1               h173b8e3_7  \nlazy-object-proxy         1.3.1            py37h14c3975_2  \nlcms2                     2.11                 h396b838_0    anaconda\nld_impl_linux-64          2.33.1               h53a641e_7    anaconda\nlibarchive                3.3.3                h5d8350f_5  \nlibcurl                   7.63.0            h20c2e04_1000  \nlibedit                   3.1.20191231         h14c3975_1    anaconda\nlibffi                    3.3                  he6710b0_2    anaconda\nlibgcc-ng                 9.1.0                hdf63c60_0    anaconda\nlibgfortran-ng            7.3.0                hdf63c60_0  \nliblief                   0.9.0                h7725739_1  \nlibpng                    1.6.37               hbc83047_0    anaconda\nlibsodium                 1.0.16               h1bed415_0  \nlibssh2                   1.8.0                h1ba5d50_4  \nlibstdcxx-ng              8.2.0                hdf63c60_1  \nlibtiff                   4.1.0                h2733197_1    anaconda\nlibtool                   2.4.6                h7b6447c_5  \nlibuuid                   1.0.3                h1bed415_2    anaconda\nlibxcb                    1.14                 h7b6447c_0    anaconda\nlibxml2                   2.9.10               he19cac6_1    anaconda\nlibxslt                   1.1.32               h1312cb7_0  \nllvmlite                  0.26.0           py37hd408876_0  \nlocket                    0.2.0                    py37_1  \nlxml                      4.2.5            py37hefd8a0e_0  \nlz4-c                     1.9.2                he6710b0_1    anaconda\nlzo                       2.10                 h49e0be7_2  \nmarkupsafe                1.1.0            py37h7b6447c_0  \nmatplotlib                3.3.1                         0    anaconda\nmatplotlib-base           3.3.1            py37h817c723_0    anaconda\nmccabe                    0.6.1                    py37_1  \nmistune                   0.8.4            py37h7b6447c_0  \nmkl                       2019.1                      144  \nmkl-service               1.1.2            py37he904b0f_5  \nmkl_fft                   1.0.10           py37ha843d7b_0    anaconda\nmkl_random                1.0.2            py37hd81dba3_0    anaconda\nmore-itertools            4.3.0                    py37_0  \nmpc                       1.1.0                h10f8cd9_1  \nmpfr                      4.0.1                hdf1c602_3  \nmpmath                    1.1.0                    py37_0  \nmsgpack-python            0.5.6            py37h6bb024c_1  \nmultipledispatch          0.6.0                    py37_0  \nnavigator-updater         0.2.1                    py37_0  \nnbconvert                 5.4.0                    py37_1  \nnbformat                  4.4.0                    py37_0  \nncurses                   6.2                  he6710b0_1    anaconda\nnetworkx                  2.2                      py37_1  \nnltk                      3.4                      py37_1  \nnose                      1.3.7                    py37_2  \nnotebook                  5.7.4                    py37_0  \nnumba                     0.41.0           py37h962f231_0  \nnumexpr                   2.6.8            py37h9e4a6bb_0  \nnumpy                     1.16.2           py37h7e9f1db_0    anaconda\nnumpy-base                1.16.2           py37hde5b4d6_0    anaconda\nnumpydoc                  0.8.0                    py37_0  \nodo                       0.5.1                    py37_0  \nolefile                   0.46                       py_0    anaconda\nopenpyxl                  2.5.12                   py37_0  \nopenssl                   1.1.1g               h7b6447c_0    anaconda\npackaging                 18.0                     py37_0  \npandas                    1.1.1            py37he6710b0_0    anaconda\npandoc                    1.19.2.1             hea2e7c5_1  \npandocfilters             1.4.2                    py37_1  \npango                     1.42.4               h049681c_0  \nparso                     0.3.1                    py37_0  \npartd                     0.3.9                    py37_0  \npatchelf                  0.9                  he6710b0_3  \npath.py                   11.5.0                   py37_0  \npathlib2                  2.3.3                    py37_0  \npatsy                     0.5.1                    py37_0  \npcre                      8.44                 he6710b0_0    anaconda\npep8                      1.7.1                    py37_0  \npexpect                   4.6.0                    py37_0  \npickleshare               0.7.5                    py37_0  \npillow                    7.2.0            py37hb39fc2d_0    anaconda\npip                       20.2.2                   py37_0    anaconda\npixman                    0.34.0               hceecf20_3  \npkginfo                   1.4.2                    py37_1  \npluggy                    0.8.0                    py37_0  \nply                       3.11                     py37_0  \nprometheus_client         0.5.0                    py37_0  \nprompt_toolkit            2.0.7                    py37_0  \npsutil                    5.4.8            py37h7b6447c_0  \nptyprocess                0.6.0                    py37_0  \npy                        1.7.0                    py37_0  \npy-lief                   0.9.0            py37h7725739_1  \npycodestyle               2.4.0                    py37_0  \npycosat                   0.6.3            py37h14c3975_0  \npycparser                 2.19                     py37_0  \npycrypto                  2.6.1            py37h14c3975_9  \npycurl                    7.43.0.2         py37h1ba5d50_0  \npyflakes                  2.0.0                    py37_0  \npygments                  2.3.1                    py37_0  \npylint                    2.2.2                    py37_0  \npyodbc                    4.0.25           py37he6710b0_0  \npyopenssl                 18.0.0                   py37_0  \npyparsing                 2.4.7                      py_0    anaconda\npyqt                      5.9.2            py37h22d08a2_1    anaconda\npysocks                   1.6.8                    py37_0  \npytables                  3.4.4            py37ha205bf6_0  \npytest                    4.0.2                    py37_0  \npytest-arraydiff          0.3              py37h39e3cac_0  \npytest-astropy            0.5.0                    py37_0  \npytest-doctestplus        0.2.0                    py37_0  \npytest-openfiles          0.3.1                    py37_0  \npytest-remotedata         0.3.1                    py37_0  \npython                    3.7.9                h7579374_0    anaconda\npython-dateutil           2.8.1                      py_0    anaconda\npython-libarchive-c       2.8                      py37_6  \npytz                      2020.1                     py_0    anaconda\npywavelets                1.0.1            py37hdd07704_0  \npyyaml                    3.13             py37h14c3975_0  \npyzmq                     17.1.2           py37h14c3975_0  \nqt                        5.9.7                h5867ecd_1    anaconda\nqtawesome                 0.5.3                    py37_0  \nqtconsole                 4.4.3                    py37_0  \nqtpy                      1.5.2                    py37_0  \nreadline                  8.0                  h7b6447c_0    anaconda\nrequests                  2.21.0                   py37_0  \nrope                      0.11.0                   py37_0  \nruamel_yaml               0.15.46          py37h14c3975_0  \nscikit-image              0.14.1           py37he6710b0_0  \nscikit-learn              0.20.1           py37hd81dba3_0  \nscipy                     1.2.1            py37h7c811a0_0    anaconda\nseaborn                   0.10.1                     py_0    anaconda\nsecretstorage             3.1.0                    py37_0  \nsend2trash                1.5.0                    py37_0  \nsetuptools                49.6.0                   py37_0    anaconda\nsimplegeneric             0.8.1                    py37_2  \nsingledispatch            3.4.0.3                  py37_0  \nsip                       4.19.24          py37he6710b0_0    anaconda\nsix                       1.15.0                     py_0    anaconda\nsnappy                    1.1.7                hbae5bb6_3  \nsnowballstemmer           1.2.1                    py37_0  \nsortedcollections         1.0.1                    py37_0  \nsortedcontainers          2.1.0                    py37_0  \nsphinx                    1.8.2                    py37_0  \nsphinxcontrib             1.0                      py37_1  \nsphinxcontrib-websupport  1.1.0                    py37_1  \nspyder                    3.3.2                    py37_0  \nspyder-kernels            0.3.0                    py37_0  \nsqlalchemy                1.2.15           py37h7b6447c_0  \nsqlite                    3.33.0               h62c20be_0    anaconda\nstatsmodels               0.9.0            py37h035aef0_0  \nsympy                     1.3                      py37_0  \ntblib                     1.3.2                    py37_0  \nterminado                 0.8.1                    py37_1  \ntestpath                  0.4.2                    py37_0  \ntk                        8.6.10               hbc83047_0    anaconda\ntoolz                     0.9.0                    py37_0  \ntornado                   6.0.4            py37h7b6447c_1    anaconda\ntqdm                      4.28.1           py37h28b3542_0  \ntraitlets                 4.3.2                    py37_0  \nunicodecsv                0.14.1                   py37_0  \nunixodbc                  2.3.7                h14c3975_0  \nurllib3                   1.24.1                   py37_0  \nwcwidth                   0.1.7                    py37_0  \nwebencodings              0.5.1                    py37_1  \nwerkzeug                  0.14.1                   py37_0  \nwheel                     0.35.1                     py_0    anaconda\nwidgetsnbextension        3.4.2                    py37_0  \nwrapt                     1.10.11          py37h14c3975_2  \nwurlitzer                 1.0.2                    py37_0  \nxlrd                      1.2.0                    py37_0  \nxlsxwriter                1.1.2                    py37_0  \nxlwt                      1.3.0                    py37_0  \nxz                        5.2.5                h7b6447c_0    anaconda\nyaml                      0.1.7                had09818_2  \nzeromq                    4.2.5                hf484d3e_1  \nzict                      0.1.3                    py37_0  \nzlib                      1.2.11               h7b6447c_3    anaconda\nzstd                      1.4.4                h0b5b093_3    anaconda\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-09-07 00:52:41.223000 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":"2020-10-22 16:57:23.290000 UTC",
        "Question_score":2,
        "Question_tags":"azure|azure-machine-learning-studio|azure-machine-learning-service",
        "Question_view_count":1167,
        "Owner_creation_date":"2009-06-23 03:11:55.290000 UTC",
        "Owner_last_access_date":"2022-09-25 03:45:28.337000 UTC",
        "Owner_location":"Cumming, GA",
        "Owner_reputation":77230,
        "Owner_up_votes":2724,
        "Owner_down_votes":43,
        "Owner_views":6359,
        "Answer_body":"<p>I just did the following and wasn't able to reproduce your error:<\/p>\n<ol>\n<li>make a new compute instance<\/li>\n<li>open it up using JupyterLab<\/li>\n<li>open a new terminal<\/li>\n<li><code>conda activate azureml_py36<\/code><\/li>\n<li><code>conda install seaborn -y<\/code><\/li>\n<li>open a new notebook and run <code>import seaborn as sns<\/code><\/li>\n<\/ol>\n<h3>Spitballing<\/h3>\n<ol>\n<li>Are you using the kernel, <code>Python 3.6 - AzureML<\/code> (i.e. the <code>azureml_py36<\/code> conda env)?<\/li>\n<li>Have you tried restarting the kernel and\/or creating a new compute instance?<\/li>\n<\/ol>\n<p><a href=\"https:\/\/i.stack.imgur.com\/kPbLH.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/kPbLH.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2020-09-07 04:17:19.417000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_valid_tags":[
            "azure-machine-learning-studio",
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":72732616,
        "Question_title":"Can't find scoring.py when using PythonScriptStep() in Databricks",
        "Question_body":"<p>We are defining in Databricks a PythonScriptStep(). When using PythonScriptStep() within our pipeline script we can't find the scoring.py file.<\/p>\n<pre><code>scoring_step = PythonScriptStep(\n    name=&quot;Scoring_Step&quot;,\n    source_directory=os.getenv(&quot;DATABRICKS_NOTEBOOK_PATH&quot;, &quot;\/Users\/USER_NAME\/source_directory&quot;),\n    script_name=&quot;.\/scoring.py&quot;,\n    arguments=[&quot;--input_dataset&quot;, ds_consumption],\n    compute_target=pipeline_cluster,\n    runconfig=pipeline_run_config,\n    allow_reuse=False)\n<\/code><\/pre>\n<p>We getting the following error message:<\/p>\n<pre><code>Step [Scoring_Step]: script not found at: \/databricks\/driver\/scoring.py. Make sure to specify an appropriate source_directory on the Step or default_source_directory on the Pipeline.\n<\/code><\/pre>\n<p>For some reason Databricks is searching for the file in '\/databricks\/driver\/' instead of the folder we entered.<\/p>\n<p>There is also the way to use DatabricksStep() instead of PythonScriptStep(), but because of specific reasons we need to use the PythonSriptStep() class.<\/p>\n<p>Could anybody help us with this specific problem?<\/p>\n<p>Thank you very much for any help!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-06-23 15:17:01.433000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-06-24 03:05:42.703000 UTC",
        "Question_score":0,
        "Question_tags":"python|pipeline|azure-databricks|azure-machine-learning-studio",
        "Question_view_count":68,
        "Owner_creation_date":"2018-12-12 07:16:09.960000 UTC",
        "Owner_last_access_date":"2022-09-22 06:31:00.857000 UTC",
        "Owner_location":"Germany",
        "Owner_reputation":37,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":21,
        "Answer_body":"<pre><code>scoring_step = PythonScriptStep(\n    name=&quot;Scoring_Step&quot;,\n    source_directory=os.getenv(&quot;DATABRICKS_NOTEBOOK_PATH&quot;, &quot;\/Users\/USER_NAME\/source_directory&quot;),\n    script_name=&quot;.\/scoring.py&quot;,\n    arguments=[&quot;--input_dataset&quot;, ds_consumption],\n    compute_target=pipeline_cluster,\n    runconfig=pipeline_run_config,\n    allow_reuse=False)\n<\/code><\/pre>\n<p>Change the above code block with below code block. It will resolve the error<\/p>\n<pre><code>data_ref = OutputFileDatasetConfig(\n    name='data_ref',\n    destination=(ds, '\/data')\n).as_upload()\n\n\ndata_prep_step = PythonScriptStep(\n    name='data_prep',\n    script_name='pipeline_steps\/data_prep.py',\n    source_directory='\/.',\n    arguments=[\n        '--main_path', main_ref,\n        '--data_ref_folder', data_ref\n                ],\n    inputs=[main_ref, data_ref],\n    outputs=[data_ref],\n    runconfig=arbitrary_run_config,\n    allow_reuse=False\n)\n<\/code><\/pre>\n<p>Reference link for the <a href=\"https:\/\/scoring_step%20=%20PythonScriptStep(%20%20%20%20%20name=%22Scoring_Step%22,%20%20%20%20%20source_directory=os.getenv(%22DATABRICKS_NOTEBOOK_PATH%22,%20%22\/Users\/USER_NAME\/source_directory%22),%20%20%20%20%20script_name=%22.\/scoring.py%22,%20%20%20%20%20arguments=%5B%22--input_dataset%22,%20ds_consumption%5D,%20%20%20%20%20compute_target=pipeline_cluster,%20%20%20%20%20runconfig=pipeline_run_config,%20%20%20%20%20allow_reuse=False)\" rel=\"nofollow noreferrer\">documentation<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-06-27 10:12:54.633000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":67744934,
        "Question_title":"Is it possible to check that the version of a file tracked by a DVC metadata file exists in remote storage without pulling the file?",
        "Question_body":"<p>My team has a set up wherein we track datasets and models in DVC, and have a GitLab repository for tracking our code and DVC metadata files. We have a job in our dev GitLab pipeline (run on each push to a merge request) that has the goal of checking to be sure that the developer remembered to run <code>dvc push<\/code> to keep DVC remote storage up-to-date. Right now, the way we do this is by running <code>dvc pull<\/code> on the GitLab runner, which will fail with errors telling you which files (new files or latest versions of existing files) were not found.<\/p>\n<p>The downside to this approach is that we are loading the entirety of our data stored in DVC onto a GitLab runner, and we've run into out-of-memory issues, not to mention lengthy run time to download all that data. Since the path and md5 hash of the objects are stored in the DVC metadata files, I would think that's all the information that DVC would need to be able to answer the question &quot;is the remote storage system up-to-date&quot;.<\/p>\n<p>It seems like <code>dvc status<\/code> is similar to what I'm asking for, but compares the cache or workspace and remote storage. In other words, it requires the files to actually be present on whatever filesystem is making the call.<\/p>\n<p>Is there some way to achieve the goal I laid out above (&quot;inform the developer that they need to run <code>dvc push<\/code>&quot;) without pulling everything from DVC?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-05-28 20:10:29.793000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-05-29 03:04:51.983000 UTC",
        "Question_score":5,
        "Question_tags":"git|gitlab|continuous-integration|dvc",
        "Question_view_count":488,
        "Owner_creation_date":"2021-04-12 19:17:42.697000 UTC",
        "Owner_last_access_date":"2022-02-22 18:42:26.683000 UTC",
        "Owner_location":null,
        "Owner_reputation":75,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":"<blockquote>\n<p>It seems like dvc status is similar to what I'm asking for<\/p>\n<\/blockquote>\n<p><code>dvc status --cloud<\/code> will give you a list of &quot;new&quot; files if they that haven't been pushed to the (default) remote. It won't error out though, so your CI script should fail depending on the stdout message.<\/p>\n<p>More info: <a href=\"https:\/\/dvc.org\/doc\/command-reference\/status#options\" rel=\"nofollow noreferrer\">https:\/\/dvc.org\/doc\/command-reference\/status#options<\/a><\/p>\n<p>I'd also ask everyone to run <code>dvc install<\/code>, which will setup some Git hooks, including automatic <code>dvc push<\/code> with <code>git push<\/code>.<\/p>\n<p>See <a href=\"https:\/\/dvc.org\/doc\/command-reference\/install\" rel=\"nofollow noreferrer\">https:\/\/dvc.org\/doc\/command-reference\/install<\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2021-05-29 03:09:19.210000 UTC",
        "Answer_last_edit_date":"2021-05-31 23:24:13.297000 UTC",
        "Answer_score":3.0,
        "Question_valid_tags":[
            "dvc"
        ]
    },
    {
        "Question_id":53423061,
        "Question_title":"How do I make this IAM role error in aws sagemaker go away?",
        "Question_body":"<p>I suspect this has to more to do with IAM roles than Sagemaker.<\/p>\n\n<p>I'm following the example <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/README.rst\" rel=\"noreferrer\">here<\/a><\/p>\n\n<p>Specifically, when it makes this call<\/p>\n\n<pre><code>tf_estimator.fit('s3:\/\/bucket\/path\/to\/training\/data')\n<\/code><\/pre>\n\n<p>I get this error<\/p>\n\n<pre><code>ClientError: An error occurred (AccessDenied) when calling the GetRole operation: User: arn:aws:sts::013772784144:assumed-role\/AmazonSageMaker-ExecutionRole-20181022T195630\/SageMaker is not authorized to perform: iam:GetRole on resource: role SageMakerRole\n<\/code><\/pre>\n\n<p>My notebook instance has an IAM role attached to it.\nThat role has the <code>AmazonSageMakerFullAccess<\/code> policy. It also has a custom policy that looks like this<\/p>\n\n<pre><code>{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n    {\n        \"Effect\": \"Allow\",\n        \"Action\": [\n            \"s3:GetObject\",\n            \"s3:PutObject\",\n            \"s3:DeleteObject\",\n            \"s3:ListBucket\"\n        ],\n        \"Resource\": [\n            \"arn:aws:s3:::*\"\n        ]\n    }\n]\n<\/code><\/pre>\n\n<p>}<\/p>\n\n<p>My input files and .py script is in an s3 bucket with the phrase <code>sagemaker<\/code> in it.<\/p>\n\n<p>What else am I missing?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_date":"2018-11-22 02:27:05.410000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":6,
        "Question_tags":"amazon-web-services|amazon-iam|amazon-sagemaker",
        "Question_view_count":8160,
        "Owner_creation_date":"2011-10-21 21:58:08.810000 UTC",
        "Owner_last_access_date":"2022-09-17 00:51:12.053000 UTC",
        "Owner_location":null,
        "Owner_reputation":4966,
        "Owner_up_votes":744,
        "Owner_down_votes":11,
        "Owner_views":304,
        "Answer_body":"<p>If you're running the example code on a SageMaker notebook instance, you can use the execution_role which has the <code>AmazonSageMakerFullAccess<\/code> attached.<\/p>\n<pre><code>from sagemaker import get_execution_role\nsagemaker_session = sagemaker.Session()\nrole = get_execution_role()\n<\/code><\/pre>\n<p>And you can pass this role when initializing <code>tf_estimator<\/code>.\nYou can check out the example <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/automatic-model-tuning-ex-role.html\" rel=\"nofollow noreferrer\">here<\/a> for using <code>execution_role<\/code> with S3 on notebook instance.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2018-11-23 22:08:24.777000 UTC",
        "Answer_last_edit_date":"2021-05-06 09:35:48.347000 UTC",
        "Answer_score":8.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":57716459,
        "Question_title":"Copying models between workspaces",
        "Question_body":"<p>I am setting up deployment pipelines for our models and I wanted to support this scenario:<\/p>\n\n<ol>\n<li>User registers model in <code>test<\/code> AML workspace in test subscription, checks in deployment code\/configs that references the model version (there is a <code>requirements.txt<\/code>-like file that specifies the model ID - name and version)<\/li>\n<li>Azure DevOps CI is triggered after code checkin to run <code>az ml model deploy<\/code> to a test environment.<\/li>\n<li>User decides after that endpoint works well, wants to deploy to prod. In Azure DevOps, manually invokes a prod pipeline that will use the same checked-in code\/configs (with the same referenced model):\n\n<ul>\n<li>copy the model from the <code>test<\/code> AML workspace to a new registered model in the <code>prod<\/code> AML workspace in a different subscription, <em>with the same version<\/em><\/li>\n<li>run <code>az ml model deploy<\/code> with different variables corresponding to the <code>prod<\/code> env, but using the same checked-in AML code\/configs<\/li>\n<\/ul><\/li>\n<\/ol>\n\n<p>I've looked at the MLOps references but can't seem to figure out how to support step 3 in the above scenario. <\/p>\n\n<p>I thought I could do an <code>az ml model download<\/code> to download the model from the <code>test<\/code> env and register it in the <code>prod<\/code> env. The registration process automatically sets the version number so, e.g. the config that references <code>myModel:12<\/code> is no longer valid since in <code>prod<\/code> the ID is <code>myModel:1<\/code><\/p>\n\n<p>How can I copy the model from one workspace in one subscription to another and preserve the ID?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-08-29 19:06:50.797000 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"azure-machine-learning-service",
        "Question_view_count":515,
        "Owner_creation_date":"2009-09-30 03:04:37.887000 UTC",
        "Owner_last_access_date":"2022-09-23 16:14:18.720000 UTC",
        "Owner_location":"Seattle, WA",
        "Owner_reputation":153,
        "Owner_up_votes":101,
        "Owner_down_votes":1,
        "Owner_views":111,
        "Answer_body":"<p>You could use model tags to set up your own identifiers that are shared across workspace, and query models with specific tags:<\/p>\n\n<pre><code>az ml model update --add-tag\naz ml model list --tag\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2019-08-30 18:33:30.650000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":65937623,
        "Question_title":"Unable to serve an mlflow model locally",
        "Question_body":"<p>I have created an mlflow model with custom pyfunc. It shows the results when I send input to the loaded model in Jupyter notebook.\nHowever if I am trying to serve it to a local port<\/p>\n<pre><code>!mlflow models serve -m Home\/miniconda3\/envs\/mlruns\/0\/baa40963927a49258c845421e3175c06\/artifacts\/model -p 8001\n<\/code><\/pre>\n<p>I am getting this error<\/p>\n<pre><code> Traceback (most recent call last):\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/bin\/mlflow&quot;, line 10, in &lt;module&gt;\n    sys.exit(cli())\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/click\/core.py&quot;, line 829, in __call__\n    return self.main(*args, **kwargs)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/click\/core.py&quot;, line 782, in main\n    rv = self.invoke(ctx)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/click\/core.py&quot;, line 1259, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/click\/core.py&quot;, line 1259, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/click\/core.py&quot;, line 1066, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/click\/core.py&quot;, line 610, in invoke\n    return callback(*args, **kwargs)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/mlflow\/models\/cli.py&quot;, line 56, in serve\n    install_mlflow=install_mlflow).serve(model_uri=model_uri, port=port,\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/mlflow\/models\/cli.py&quot;, line 163, in _get_flavor_backend\n    append_to_uri_path(underlying_model_uri, &quot;MLmodel&quot;), output_path=tmp.path())\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/mlflow\/tracking\/artifact_utils.py&quot;, line 76, in _download_artifact_from_uri\n    artifact_path=artifact_path, dst_path=output_path)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/mlflow\/store\/artifact\/local_artifact_repo.py&quot;, line 67, in download_artifacts\n    return super(LocalArtifactRepository, self).download_artifacts(artifact_path, dst_path)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/mlflow\/store\/artifact\/artifact_repo.py&quot;, line 140, in download_artifacts\n    return download_file(artifact_path)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/mlflow\/store\/artifact\/artifact_repo.py&quot;, line 105, in download_file\n    self._download_file(remote_file_path=fullpath, local_path=local_file_path)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/mlflow\/store\/artifact\/local_artifact_repo.py&quot;, line 95, in _download_file\n    shutil.copyfile(remote_file_path, local_path)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/shutil.py&quot;, line 120, in copyfile\n    with open(src, 'rb') as fsrc:\nFileNotFoundError: [Errno 2] No such file or directory: 'Home\/miniconda3\/envs\/mlruns\/0\/baa40963927a49258c845421e3175c06\/artifacts\/model\/MLmodel'\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-01-28 13:01:27.777000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"deployment|localhost|mlflow",
        "Question_view_count":1277,
        "Owner_creation_date":"2019-11-14 13:58:10.560000 UTC",
        "Owner_last_access_date":"2022-09-23 08:37:32.563000 UTC",
        "Owner_location":null,
        "Owner_reputation":115,
        "Owner_up_votes":16,
        "Owner_down_votes":0,
        "Owner_views":25,
        "Answer_body":"<p>From your error traceback, the model artifact can't be located. In your code, you are executing the 'mlflow' command from within a Jupyter Notebook. I would suggest trying the following:<\/p>\n<ol>\n<li>Check if your models artifacts are on the path you are using Home\/miniconda3\/envs\/mlruns\/0\/baa40963927a49258c845421e3175c06\/artifacts\/model<\/li>\n<li>Try opening a terminal, then <code>cd \/Home\/miniconda3\/envs<\/code> and  execute <code>mlflow models serve -m .\/mlruns\/0\/baa40963927a49258c845421e3175c06\/artifacts\/model -p 8001<\/code><\/li>\n<li>MLFlow offers different solutions to serve a model, you can try to register your model and refer to it as &quot;models:\/{model_name}\/{stage}&quot; as mentioned in the Model Registry <a href=\"https:\/\/mlflow.org\/docs\/latest\/model-registry.html#serving-an-mlflow-model-from-model-registry\" rel=\"nofollow noreferrer\">docs<\/a><\/li>\n<\/ol>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2021-01-28 13:30:03.100000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_valid_tags":[
            "mlflow"
        ]
    },
    {
        "Question_id":71136057,
        "Question_title":"Identifying user from AWS Sagemaker Studio generated EFS storage",
        "Question_body":"<p>When a sagemaker studio domain is created. An EFS storage is associated with the domain. As the assigned users log into Sagemaker studio, a corresponding home directory is created.<\/p>\n<p>Using a separate EC2 instance, I mounted the EFS storage that was created to try to see whether is it possible to look at each of the individual home domains. I noticed that each of these home directories are shown in terms of numbers (e.g 200000, 200005). Is there a specific rule on how this folders are named? Is it possible to trace the folders back to a particular user or whether this is done by design?<\/p>\n<p>(currently doing exploration on my personal aws account)<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-02-16 03:29:37.253000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker|amazon-efs",
        "Question_view_count":302,
        "Owner_creation_date":"2022-02-16 03:15:56.940000 UTC",
        "Owner_last_access_date":"2022-09-23 15:06:17.550000 UTC",
        "Owner_location":null,
        "Owner_reputation":53,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":2,
        "Answer_body":"<p>Yes, if you <a href=\"https:\/\/docs.aws.amazon.com\/cli\/latest\/reference\/sagemaker\/list-user-profiles.html\" rel=\"nofollow noreferrer\">list<\/a> and <a href=\"https:\/\/docs.aws.amazon.com\/cli\/latest\/reference\/sagemaker\/describe-user-profile.html\" rel=\"nofollow noreferrer\">describe<\/a> the domain users, you'll get back the user's <code>HomeEfsFileSystemUid<\/code> value.<br \/>\nHere's a CLI example:<\/p>\n<pre><code>aws sagemaker describe-user-profile --domain-id d-lcn1vbt47yku --user-profile-name default-1588670743757\n{\n    ...\n    &quot;UserProfileName&quot;: &quot;default-1588670743757&quot;,\n    &quot;HomeEfsFileSystemUid&quot;: &quot;200005&quot;,\n    ...\n}\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-02-17 21:39:04.567000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":70855730,
        "Question_title":"Is it possible from a gcp vertex job to hit an http endpoint in another gcp pod in the same project?",
        "Question_body":"<p>I have a custom container vertex endpoint that is passed a url as input so that the job can call it to get a particular frame of data needed for the job. (gcs:\/\/ buckets do work) but I want to specifically use an http request to a server in the same gcp project.<\/p>\n<p>I have tried setting the endpoint up as private using the --networks param on the endpoint but then get the message:<\/p>\n<pre><code>{\n  &quot;error&quot;: {\n    &quot;code&quot;: 400,\n    &quot;message&quot;: &quot;Making request from public OnePlatform API is not allowed on a private Endpoint peered with network (projects\/11111111111\/global\/networks\/some-dev-project-vpc).&quot;,\n    &quot;status&quot;: &quot;FAILED_PRECONDITION&quot;\n  }\n}\n<\/code><\/pre>\n<p>when I try to hit that private vertex endpoint.  I've tried curling it from within a running pod in the same project, but that didn't work either.<\/p>\n<p>Is there a way to do this?\nThanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2022-01-25 21:11:36.180000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-01-26 05:34:22.993000 UTC",
        "Question_score":1,
        "Question_tags":"google-cloud-platform|google-cloud-vertex-ai",
        "Question_view_count":274,
        "Owner_creation_date":"2016-05-07 00:35:30.170000 UTC",
        "Owner_last_access_date":"2022-09-23 23:45:09.807000 UTC",
        "Owner_location":"Berkeley, CA, United States",
        "Owner_reputation":329,
        "Owner_up_votes":101,
        "Owner_down_votes":0,
        "Owner_views":65,
        "Answer_body":"<p>The error states that your request is to a public API, which may because you are using the public url schema to make your prediction. The structure of vertex endpoints differ between private and public, so double check that you are using the private endpoint url for your requests.<\/p>\n<p><strong>Public<\/strong><\/p>\n<pre><code>https:\/\/{REGION}-aiplatform.googleapis.com\/v1\/projects\/{PROJECT}\/locations\/{REGION}\/endpoints\/{ENDPOINT_ID}:predict\n<\/code><\/pre>\n<p><strong>Private<\/strong><\/p>\n<pre><code>http:\/\/{ENDPOINT_ID}.aiplatform.googleapis.com\/v1\/models\/{DEPLOYED_MODEL_ID}:predict\n<\/code><\/pre>\n<p>You can generate a private endpoint url using the following gcloud command:<\/p>\n<pre><code>gcloud beta ai endpoints describe {ENDPOINT_ID} \\\n  --region=us-central1 \\\n  --format=&quot;value(deployedModels.privateEndpoints.predictHttpUri)&quot;\n<\/code><\/pre>\n<p>More documentation on private endpoints can be found <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/using-private-endpoints#private-predict-uri-format\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-01-28 00:50:21.110000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":3.0,
        "Question_valid_tags":[
            "google-cloud-vertex-ai"
        ]
    },
    {
        "Question_id":71292240,
        "Question_title":"How to replace missing datapoints with prior in MS Azure?",
        "Question_body":"<p>When using the ML-pipeline designer in MS Azure it is possible to clean missing data, namely by replacing them by means or constant values.<\/p>\n<p>In my dataset I have gaps, when the measured value did not change enough, thus I should want to replace the missing data with the last existing entry.\nSo from<\/p>\n<pre><code>VALUE A\n2\nNONE\nNONE\nNONE\n3\nNONE\nNONE\n<\/code><\/pre>\n<p>I would like to get<\/p>\n<pre><code>VALUE A\n2\n2\n2\n2\n3\n3\n3\n<\/code><\/pre>\n<p>This option is not available in the pipeline designer as far as I know. Can I manipulate the dataset somehow else within Azure, before training?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-02-28 08:23:08.140000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"azure|azure-machine-learning-service",
        "Question_view_count":13,
        "Owner_creation_date":"2020-11-26 11:40:24.850000 UTC",
        "Owner_last_access_date":"2022-09-23 09:09:31.537000 UTC",
        "Owner_location":null,
        "Owner_reputation":11,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":"<p>I figured it out, by using the Notebooks (do not work in Firefox for me, only on Chrome).\nThere it is possible to handle the dataset in python, transform it to pandas, manipulate it and save it to the datastore.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-03-14 08:58:23.807000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_valid_tags":[
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":56861525,
        "Question_title":"Interpretation of output of bounding box annotation job",
        "Question_body":"<p>The output folder of an annotation job contains the following file structure: <\/p>\n\n<ul>\n<li><p>active learning<\/p><\/li>\n<li><p>annotation-tools<\/p><\/li>\n<li><p>annotations<\/p><\/li>\n<li><p>intermediate<\/p><\/li>\n<li><p>manifests<\/p><\/li>\n<\/ul>\n\n<p>Each line of the manifests\/output\/output.manifest file is a dictionary, where the key 'jobname' contains information about the annotations, and the key 'jobname-metadata' contains confidence score and other information about each of the bounding box annotations. There is also another folder called annotations which contain json files which contain information about annotations and associated worker ids. How are the two annotation informations related to each other? Is there any blogs\/tutorials which discuss how to interpret the data received from amazon sagemaker ground-truth service? Thanks in advance. <\/p>\n\n<p>Links I referred to: \n1. <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-data-output.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-data-output.html<\/a>\n2. <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/ground_truth_labeling_jobs\/ground_truth_object_detection_tutorial\/object_detection_tutorial.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/ground_truth_labeling_jobs\/ground_truth_object_detection_tutorial\/object_detection_tutorial.ipynb<\/a> <\/p>\n\n<p>I have displayed the annotations received using the code available in the link 2 <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/ground_truth_labeling_jobs\/ground_truth_object_detection_tutorial\/object_detection_tutorial.ipynb\" rel=\"nofollow noreferrer\">here<\/a>, which treats consolidated annotations and worker response separately.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-07-03 00:37:14.297000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"amazon-web-services|bounding-box|amazon-sagemaker|labeling",
        "Question_view_count":199,
        "Owner_creation_date":"2019-07-02 23:47:49.163000 UTC",
        "Owner_last_access_date":"2019-08-22 20:40:42.113000 UTC",
        "Owner_location":null,
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":"<p>Thank you for your question. I\u2019m the product manager for Amazon SageMaker Ground Truth and am happy to answer your question here.<\/p>\n\n<p>We have a feature called annotation consolidation that takes the responses from multiple workers for a single image and then consolidates those responses into a single set of bounding boxes for the image. The bounding boxes referenced in the manifest file are the consolidated responses whereas what you see in the annotations folders are the raw annotations (which is why you have the respective worker IDs). <\/p>\n\n<p>You can find out more about the annotation consolidation feature here: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-annotation-consolidation.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-annotation-consolidation.html<\/a><\/p>\n\n<p>Please let us know if you have any further questions.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2019-08-19 17:25:46.553000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":48158545,
        "Question_title":"How to execute multiple rows in web service Azure Machine Learning Studio",
        "Question_body":"<p>I create a model in Azure ML studio. \nI deployed the web service.<\/p>\n\n<p>Now, I know how to check one record at a time, but how can I load a csv file and made the algorithm go through all records ?<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/1tHuM.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/1tHuM.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>If I click on Batch Execution - it will ask me to create an account for Azure storage. <\/p>\n\n<p>Is any way to execute multiple records from csv file without creating any other accounts?<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/90zP7.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/90zP7.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2018-01-08 21:47:45.933000 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"azure|azure-machine-learning-studio|ml-studio",
        "Question_view_count":204,
        "Owner_creation_date":"2016-03-10 08:00:45.393000 UTC",
        "Owner_last_access_date":"2022-09-23 23:59:58.457000 UTC",
        "Owner_location":"San Diego, CA, United States",
        "Owner_reputation":4046,
        "Owner_up_votes":505,
        "Owner_down_votes":7,
        "Owner_views":825,
        "Answer_body":"<p>Yes, there is a way and it is simple. What you need is an excel add-in. You need not create any other account.<\/p>\n\n<p>You can either read <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio\/excel-add-in-for-web-services\" rel=\"nofollow noreferrer\">Excel Add-in for Azure Machine Learning web services doc<\/a> or you can watch <a href=\"https:\/\/www.youtube.com\/watch?v=ju1CzDjiOMQ\" rel=\"nofollow noreferrer\">Azure ML Excel Add-in video<\/a>. <\/p>\n\n<p>If you search for <a href=\"https:\/\/www.google.co.in\/search?q=excel%20add%20in%20for%20azure%20ml&amp;client=firefox-b-ab&amp;dcr=0&amp;source=lnms&amp;tbm=vid&amp;sa=X&amp;ved=0ahUKEwinqP3a_67ZAhXBr48KHdiYAXUQ_AUICigB&amp;biw=1280&amp;bih=616\" rel=\"nofollow noreferrer\">videos on excel add in for azure ml<\/a>, you get other useful videos too. <\/p>\n\n<p>I hope this is the solution you are looking for.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2018-02-18 08:10:29.767000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":71860839,
        "Question_title":"Slow ResNet50 training time using AWS Sagemaker GPU instance",
        "Question_body":"<p>I am trying to train a ResNet50 model using keras with tensorflow backend. I'm using a sagemaker GPU instance <strong>ml.p3.2xlarge<\/strong> but my training time is extremely long. I am using conda_tensorflow_p36 kernel and I have verified that I have tensorflow-gpu installed.<\/p>\n<p>When inspecting the output of nvidia-smi I see the process is on the GPU, but the utilization is never above <strong>0%<\/strong>.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/CDSkC.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/CDSkC.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Tensorflow also recognizes the GPU.\n<a href=\"https:\/\/i.stack.imgur.com\/wRHBC.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/wRHBC.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Screenshot of training time.\n<a href=\"https:\/\/i.stack.imgur.com\/Yh73K.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Yh73K.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Is sagemaker in fact using the GPU even though the usage is <strong>0%?<\/strong>\nCould the long epoch training time be caused by another issue?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-04-13 16:27:11.580000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-04-14 01:15:08.327000 UTC",
        "Question_score":0,
        "Question_tags":"python|tensorflow|deep-learning|computer-vision|amazon-sagemaker",
        "Question_view_count":120,
        "Owner_creation_date":"2018-10-29 03:02:23.880000 UTC",
        "Owner_last_access_date":"2022-09-20 01:24:14.397000 UTC",
        "Owner_location":null,
        "Owner_reputation":31,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Answer_body":"<p>Looks like you've completed 8 steps and it just takes very long. What's your step time?<br \/>\nIt might be due to data loading. Where ia data stored? Try to take data loading out of the picture by caching and feeding a single image to the DNN repeatedly and see if that helps.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2022-04-15 07:34:10.343000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":51517103,
        "Question_title":"Deploy my own tensorflow model on a virtual machine with AWS",
        "Question_body":"<p>I have a Tensorflow model which is working perfectly fine on my laptop (Tf 1.8 on OS HighSierra). However, I wanted to scale my operations up and use Amazon's Virtual Machine to run predictions faster. What is the best way to use my saved model and classify images in jpeg format which are stored locally? Thank you! <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2018-07-25 10:50:40.557000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"tensorflow|amazon-ec2|tensorflow-serving|amazon-sagemaker",
        "Question_view_count":196,
        "Owner_creation_date":"2018-07-04 21:18:30.150000 UTC",
        "Owner_last_access_date":"2022-04-06 10:24:32.773000 UTC",
        "Owner_location":null,
        "Owner_reputation":391,
        "Owner_up_votes":15,
        "Owner_down_votes":0,
        "Owner_views":57,
        "Answer_body":"<p>you have two options:<\/p>\n\n<p>1) Start a virtual machine on AWS (known as an Amazon EC2 instance). You can pick from many different instance types, including GPU instances. You'll have full administrative access on this machine, meaning that you can copy you TF model to it and predict just like you would on your own machine. <\/p>\n\n<p>More details on getting started with EC2 here: <a href=\"https:\/\/aws.amazon.com\/ec2\/getting-started\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/ec2\/getting-started\/<\/a> <\/p>\n\n<p>I would also recommend using the Deep Learning Amazon Machine Image, which bundles all the popular ML\/DL tools as well as the NVIDIA environment for GPU training\/prediction : <a href=\"https:\/\/aws.amazon.com\/machine-learning\/amis\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/machine-learning\/amis\/<\/a><\/p>\n\n<p>2) If you don't want to manage virtual machines, I'd recommend looking at Amazon SageMaker. You'll be able to import your TF model and to deploy it on fully-managed infrastructure for prediction. <\/p>\n\n<p>Here's a sample notebook showing you how to bring your own TF model to SageMaker: <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/tensorflow_iris_byom\/tensorflow_BYOM_iris.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/tensorflow_iris_byom\/tensorflow_BYOM_iris.ipynb<\/a><\/p>\n\n<p>Hope this helps.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2018-07-25 12:27:51.907000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":45558337,
        "Question_title":"What is wrong with my experiment (Trying to predict car sales)?",
        "Question_body":"<p>I have the dataset like this (just a sample of it):<\/p>\n\n<pre><code>DATE_REF,MONTH,YEAR,DAY_OF_YEAR,DAY_OF_MONTH,WEEK_DAY,WEEK_DAY_1,WEEK_DAY_2,WEEK_DAY_3,WEEK_DAY_4,WEEK_DAY_5,WEEK_DAY_6,WEEK_DAY_7,WEEK_NUMBER_IN_MONTH,WEEKEND,WORK_DAY,AMOUNT_SOLD\n20100101,1,2010,1,1,6,0,0,0,0,0,1,0,1,0,0,0\n20100102,1,2010,2,2,7,0,0,0,0,0,0,1,1,1,0,2\n20100103,1,2010,3,3,1,1,0,0,0,0,0,0,2,1,0,0\n20100104,1,2010,4,4,2,0,1,0,0,0,0,0,2,0,1,12830\n20100105,1,2010,5,5,3,0,0,1,0,0,0,0,2,0,1,19200\n20100106,1,2010,6,6,4,0,0,0,1,0,0,0,2,0,1,22930\n20100107,1,2010,7,7,5,0,0,0,0,1,0,0,2,0,1,23495\n20100108,1,2010,8,8,6,0,0,0,0,0,1,0,2,0,1,23215\n20100109,1,2010,9,9,7,0,0,0,0,0,0,1,2,1,0,172\n20100110,1,2010,10,10,1,1,0,0,0,0,0,0,3,1,0,0\n20100111,1,2010,11,11,2,0,1,0,0,0,0,0,3,0,1,18815\n20100112,1,2010,12,12,3,0,0,1,0,0,0,0,3,0,1,25415\n20100113,1,2010,13,13,4,0,0,0,1,0,0,0,3,0,1,25262\n20100114,1,2010,14,14,5,0,0,0,0,1,0,0,3,0,1,27967\n20100115,1,2010,15,15,6,0,0,0,0,0,1,0,3,0,1,26352\n20100116,1,2010,16,16,7,0,0,0,0,0,0,1,3,1,0,202\n20100117,1,2010,17,17,1,1,0,0,0,0,0,0,4,1,0,10\n20100118,1,2010,18,18,2,0,1,0,0,0,0,0,4,0,1,20295\n20100119,1,2010,19,19,3,0,0,1,0,0,0,0,4,0,1,25982\n20100120,1,2010,20,20,4,0,0,0,1,0,0,0,4,0,1,24745\n20100121,1,2010,21,21,5,0,0,0,0,1,0,0,4,0,1,28087\n20100122,1,2010,22,22,6,0,0,0,0,0,1,0,4,0,1,28417\n20100123,1,2010,23,23,7,0,0,0,0,0,0,1,4,1,0,115\n20100124,1,2010,24,24,1,1,0,0,0,0,0,0,5,1,0,5\n20100125,1,2010,25,25,2,0,1,0,0,0,0,0,5,0,1,20185\n20100126,1,2010,26,26,3,0,0,1,0,0,0,0,5,0,1,25932\n20100127,1,2010,27,27,4,0,0,0,1,0,0,0,5,0,1,31710\n20100128,1,2010,28,28,5,0,0,0,0,1,0,0,5,0,1,21020\n20100129,1,2010,29,29,6,0,0,0,0,0,1,0,5,0,1,51460\n20100130,1,2010,30,30,7,0,0,0,0,0,0,1,5,1,0,670\n20100131,1,2010,31,31,1,1,0,0,0,0,0,0,6,1,0,17\n<\/code><\/pre>\n\n<p>I'm trying to predict the <code>AMOUNT_SOLD<\/code> for new dates (<code>DATE_REF<\/code>) using the following experiment on Azure ML:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/7Mfhs.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/7Mfhs.png\" alt=\"Azure ML Experiment\"><\/a><\/p>\n\n<p>Then I deployed the Web Service and tested the prediction, but all I got was zero for the <code>AMOUNT_SOLD<\/code> column.<\/p>\n\n<p>What may I be missing? <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2017-08-08 02:23:35.170000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"machine-learning|regression|azure-machine-learning-studio",
        "Question_view_count":178,
        "Owner_creation_date":"2012-02-02 09:18:41.837000 UTC",
        "Owner_last_access_date":"2022-08-05 19:38:30.850000 UTC",
        "Owner_location":"Belo Horizonte - MG, Brasil",
        "Owner_reputation":7753,
        "Owner_up_votes":98,
        "Owner_down_votes":30,
        "Owner_views":743,
        "Answer_body":"<p>As much as I want to replicate your Azure ML experiment, I do not have enough data. But what I've done are as follows:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/XNaeg.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/XNaeg.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>I copied your sample data, and then multiplied it by 4 times (<strong>Add Rows x 2<\/strong>).\nThen <strong>Split Data<\/strong> (70%\/30%), random seed 7 (for reproducible results).\nThe <strong>Boosted Decision Tree Regression<\/strong> has default parameters.\nOn <strong>Tune Model Hyperparameters<\/strong>, I selected <strong><em>AMOUNT_SOLD<\/em><\/strong> as the label column.\nThen <strong>Score Model<\/strong> and <strong>Evaluate Model<\/strong>.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/aIJlk.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/aIJlk.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Accuracy \/ Coefficient of Determination was pretty good.<\/p>\n\n<p>After that, to deploy this as a web service, you must setup first a Predictive Experiment from your Training Experiment. <code>Setup Web Service &gt; Predictive Experiment<\/code> You experiment will move like magic.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/gTEOl.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/gTEOl.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>The <strong>Web Service Input<\/strong> module will be placed by default at the top of the experiment. I <strong>moved it and connected at the right side of Score Model<\/strong>, so that when you are inputting the parameters of your web service, it <em>will be predicted using your Trained Model<\/em>.<\/p>\n\n<p>After the Score Model module, I placed a <strong>Select Columns in Dataset<\/strong> module and selected only the column named <strong>Scored Labels<\/strong>. This column contains the model's predictions. Then I used <strong>Edit Metadata<\/strong> module to rename the Scored Labels column, before passing it to the <strong>Web Service Output<\/strong> module.<\/p>\n\n<p>Your experiment is now ready to deploy as a web service.<\/p>\n\n<p>To predict new values, I tested the web service using the current date details as input. (<strong>Although the DATE_REF input must be 20170818<\/strong> :D )<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/fPm65.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/fPm65.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>And then the output looks like this:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/R6N4B.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/R6N4B.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Your web service can now predict new values.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2017-08-18 06:06:36.010000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":58802366,
        "Question_title":"Deploying the sagemaker endpoint created as a service",
        "Question_body":"<p>I have trained a credit-fraud data set on AWS Sagemaker and created an endpoint of the model. Suppose I want to provide it as a service to my friend. He has some credit data and wanted to know whether the transaction is fraud or not. He wishes to use my endpoint. How do I share it?<\/p>\n\n<ol>\n<li>Should I share my ARN for endpoint? I don't think its the right way. without a common account he won't be able to use it.<\/li>\n<li>Or is there another way<\/li>\n<\/ol>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-11-11 13:31:06.647000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"amazon-web-services|endpoint|amazon-sagemaker",
        "Question_view_count":226,
        "Owner_creation_date":"2019-09-12 20:07:41.627000 UTC",
        "Owner_last_access_date":"2022-09-21 14:35:39.230000 UTC",
        "Owner_location":"Hyderabad, Telangana, India",
        "Owner_reputation":486,
        "Owner_up_votes":28,
        "Owner_down_votes":2,
        "Owner_views":75,
        "Answer_body":"<p>To share your model as an endpoint, you should use lambda and API Gateway to create your API.<\/p>\n\n<ol>\n<li>Create an API gateway that triggers a Lambda with the HTTP POST method;<\/li>\n<li>your lambda should instantiate the SageMaker endpoint, get the requested parameter in the event, call the SageMaker endpoint and return the predicted value. you can also create a DynamoDB to store commonly requested parameters with their answers;<\/li>\n<li>Send the API Gateway Endpoint to your friend.<\/li>\n<\/ol>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/qLss4.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/qLss4.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2019-11-13 14:00:26.700000 UTC",
        "Answer_last_edit_date":"2019-11-13 14:11:08.900000 UTC",
        "Answer_score":6.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":39652384,
        "Question_title":"How can I import into Azure machine learning studio from azure table storage with ODATA query?",
        "Question_body":"<p>The Import Data module for Azure Table documention can be found here: <a href=\"https:\/\/msdn.microsoft.com\/en-us\/library\/azure\/mt674699\" rel=\"nofollow\">https:\/\/msdn.microsoft.com\/en-us\/library\/azure\/mt674699<\/a><\/p>\n\n<p>In there it mentions that:<\/p>\n\n<blockquote>\n  <p>The Import Data module does not support filtering as data is being read. The exception is reading from data feeds, which sometimes allow you to specify a filter condition as part of the feed URL.<\/p>\n<\/blockquote>\n\n<p>There is a large amount of data in our table storage and it is not feasible to re-download the entire data set each time we run the experiment. I'm aware that there is the option to cache the data, however there is new data constantly being inserted and we would like to be able to use the new data whenever the experiment is run.<\/p>\n\n<p>Is there an alternative to the Import Data module that we could use to get table storage data with an ODATA query instead?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2016-09-23 03:59:43.637000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"azure-table-storage|azure-machine-learning-studio",
        "Question_view_count":324,
        "Owner_creation_date":"2015-12-20 22:57:46.693000 UTC",
        "Owner_last_access_date":"2020-12-29 19:40:03.983000 UTC",
        "Owner_location":"Brisbane, Australia",
        "Owner_reputation":802,
        "Owner_up_votes":30,
        "Owner_down_votes":1,
        "Owner_views":152,
        "Answer_body":"<p>There is no generic way to incrementally update a dataset. <\/p>\n\n<p>However, depending on what you want to do with the data, there are different options for adding new data:<\/p>\n\n<p>The Add Rows module effectively concatenates two datasets. So you could use the old, cached dataset on the left-hand input and add the new data on the right-hand input. That way you only have to read in the new data.\nHowever, you would have to create some complex logic for figuring out which rows were new and old, and then maintain that outside Azure ML.<\/p>\n\n<p>You could create an OData feed based on table storage, to enable filtering and get the new data that way. Just be aware that right now only public feeds are supported. And you would have to use Join or Add Rows to recombine the old and new data as described above. <\/p>\n\n<p>You might also look into ways of using the <a href=\"https:\/\/blog.maartenballiauw.be\/post\/2012\/10\/08\/what-partitionkey-and-rowkey-are-for-in-windows-azure-table-storage.html\" rel=\"nofollow\">table names<\/a>, partitions, and rowkeys to chunk your data. <\/p>\n\n<p>If you are retraining a model and you want to update your feature statistics, the <a href=\"https:\/\/msdn.microsoft.com\/library\/dn913056.aspx\" rel=\"nofollow\">Learning with Counts<\/a> modules support incremental updates of count-based features. <\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2016-09-24 01:03:09.117000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":70337958,
        "Question_title":"Do I need garbage collector when I delete object from branch by API?",
        "Question_body":"<p>Do I need a garbage collector in LakeFS when I delete an object from a branch by API?\nUsing appropriate method of course.\nDo I understand right that the garbage collector is used only for objects that are deleted by a commit. And this objects are soft deleted (by the commit). And if I use the delete API method than the object is hard deleted and I don\u2019t need to invoke the garbage collector?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-12-13 16:37:42.713000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":4,
        "Question_tags":"lakefs",
        "Question_view_count":67,
        "Owner_creation_date":"2020-03-03 05:43:31.067000 UTC",
        "Owner_last_access_date":"2022-09-24 15:23:44.207000 UTC",
        "Owner_location":null,
        "Owner_reputation":73,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":14,
        "Answer_body":"<p>lakeFS manages versions of your data.  So deletions only affect successive versions.  The object itself remains, and can be accessed by accessing an older version.<\/p>\n<p>Garbage collection removes the underlying files.  Once the file is gone, its key is still <em>visible<\/em> in older versions, but if you try to access the file itself you will receive HTTP status code <code>410 Gone<\/code>.<\/p>\n<p>For full information, please see the <a href=\"https:\/\/docs.lakefs.io\/reference\/garbage-collection.html\" rel=\"nofollow noreferrer\">Garbage collection<\/a> docs.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-12-14 07:58:15.777000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":4.0,
        "Question_valid_tags":[
            "lakefs"
        ]
    },
    {
        "Question_id":67015185,
        "Question_title":"How can I match my local azure automl python sdk version to the remote version?",
        "Question_body":"<p>I'm using the azure automl python sdk to download and save a model then reload it. I get the following error:<\/p>\n<pre><code>anaconda3\\envs\\automl_21\\lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator Pipeline from version 0.22.1 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n  UserWarning)\n<\/code><\/pre>\n<p>How can I ensure that the versions match?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-04-09 04:47:03.520000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|azure-machine-learning-service",
        "Question_view_count":62,
        "Owner_creation_date":"2011-12-20 03:17:46.393000 UTC",
        "Owner_last_access_date":"2022-09-15 01:17:57.817000 UTC",
        "Owner_location":"Christchurch, New Zealand",
        "Owner_reputation":1306,
        "Owner_up_votes":49,
        "Owner_down_votes":5,
        "Owner_views":81,
        "Answer_body":"<p>My Microsoft contact says -<\/p>\n<p>&quot;For this, their best  bet is probably to see what the training env was pinned to and install those same pins. They can get that env by running child_run.get_environment() and then pip install all the pkgs listed in there with the pins listed there.&quot;<\/p>\n<p>A useful code snippet.<\/p>\n<pre><code>for run in experiment.get_runs():\n    tags_dictionary = run.get_tags()\n    best_run = AutoMLRun(experiment, tags_dictionary['automl_best_child_run_id'])\n    env = best_run.get_environment()\n    print(env.python.conda_dependencies.serialize_to_string())\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-04-11 21:26:49.273000 UTC",
        "Answer_last_edit_date":"2021-04-11 21:56:22.617000 UTC",
        "Answer_score":0.0,
        "Question_valid_tags":[
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":64849557,
        "Question_title":"SageMaker Batch Transform fails with ID Column",
        "Question_body":"<p>I am using SageMaker pipeline to do inference on test data. The Pipeline uses a SKLearn perprocessor and a XGBoost model. The pipeline works fine on data without an ID column. However, when I try to include an ID column to track the predictions, it fails. I have given the code snippets below.<\/p>\n<pre><code>import sagemaker\nfrom sagemaker.predictor import json_serializer, csv_serializer, json_deserializer\n\ninput_data_path = 's3:\/\/batch-transform\/input-data\/validation_data.csv'\noutput_data_path = 's3:\/\/batch-transform\/predictions\/'\n\ntransform_job = sagemaker.transformer.Transformer(\n    model_name = model_name,\n    instance_count = 1,\n    instance_type = 'ml.m4.xlarge',\n    strategy = 'MultiRecord',\n    assemble_with = 'Line',\n    output_path = output_data_path,\n    base_transform_job_name='pipeline_with_id',\n    sagemaker_session=sagemaker.Session(),\n    accept = 'text\/csv')\n\ntransform_job.transform(data = input_data_path,\n                        content_type = 'text\/csv', \n                        split_type = 'Line',\n                        input_filter='$[1:]', \n                        join_source='Input')\n                        output_filter='$[0,-1]')\n<\/code><\/pre>\n<p>This results in the following error:<\/p>\n<pre><code>Fail to join data: mismatched line count between the input and the output\n<\/code><\/pre>\n<p>I am following the example given in this page:<\/p>\n<p><a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/associating-prediction-results-with-input-data-using-amazon-sagemaker-batch-transform\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/associating-prediction-results-with-input-data-using-amazon-sagemaker-batch-transform\/<\/a><\/p>\n<p>Can someone provide pointers to what is causing the error? Thank you<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_date":"2020-11-15 20:41:50.920000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"python|amazon-web-services|scikit-learn|amazon-sagemaker",
        "Question_view_count":1080,
        "Owner_creation_date":"2011-10-19 10:12:30.600000 UTC",
        "Owner_last_access_date":"2022-09-09 09:12:03.143000 UTC",
        "Owner_location":null,
        "Owner_reputation":3073,
        "Owner_up_votes":238,
        "Owner_down_votes":5,
        "Owner_views":341,
        "Answer_body":"<p>Came across the same issue.<\/p>\n<p>Check the number of rows returned after prediction in your serving code. In my case, my prediction output didn't have a column header.<\/p>\n<p>e.g. As a text\/csv response, using batch transform with join will post join the input &amp; output.<\/p>\n<p>A single input record would be [[&quot;feature_1&quot;, &quot;feature_2&quot;],[0, 1]], while my model predicted output returned [1].<\/p>\n<p>add column name to predicted output like this [&quot;result&quot;, 1] then returning the csv result will yield [[&quot;result&quot;],[1]] matching input.<\/p>\n<p>P.S. you may need to find a scalable way of doing this for multi-row  batch. Not sure.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-01-15 09:37:28.710000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":71872506,
        "Question_title":"Unable To Run AzureML Experiment with SDK - Failed to Build Wheel for pynacl \/ Exit status:1",
        "Question_body":"<p>I am trying to run a AzureML Experiment using sdk (following a Udemy course). When I try to use the Experiment.submit function the experiment prepares and then fails with the following error messages:<\/p>\n<pre><code>ERROR: Command errored out with exit status 1 \n\nERROR: Failed building wheel for pynacl\nERROR: Could not build wheels for pynacl which use PEP 517 and cannot be installed directly\n<\/code><\/pre>\n<p>The Azure env as created within my anaconda navigator for a short period of time and then gets removed.<\/p>\n<p>Does anyone know how I can get around this? Any help would be really appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-04-14 13:32:38.360000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-04-19 08:37:31.940000 UTC",
        "Question_score":0,
        "Question_tags":"python|azure|anaconda|azure-machine-learning-service|pynacl",
        "Question_view_count":73,
        "Owner_creation_date":"2020-10-24 10:49:09.850000 UTC",
        "Owner_last_access_date":"2022-09-21 12:00:40.910000 UTC",
        "Owner_location":null,
        "Owner_reputation":51,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Answer_body":"<p>To resolve <code>ERROR: Could not build wheels for pynacl which use PEP 517 and cannot be installed directly<\/code> this error, try either of the following ways:<\/p>\n<ol>\n<li><p>Install missing dependencies:<\/p>\n<pre><code>sudo apt install libpython3-dev build-essential\n<\/code><\/pre>\n<\/li>\n<li><p>Upgrade pip:<\/p>\n<pre><code>pip3 install --upgrade pip\n<\/code><\/pre>\n<\/li>\n<li><p>Upgrade pip with setuptools wheel:<\/p>\n<pre><code>pip3 install --upgrade pip setuptools wheel\n<\/code><\/pre>\n<\/li>\n<li><p>Reinstall PEP517:<\/p>\n<pre><code>pip3 install p5py\npip3 install PEP517\n<\/code><\/pre>\n<\/li>\n<\/ol>\n<p>You can refer to  <a href=\"https:\/\/stackoverflow.com\/questions\/61365790\/error-could-not-build-wheels-for-scipy-which-use-pep-517-and-cannot-be-installe\">ERROR: Could not build wheels for scipy which use PEP 517 and cannot be installed directly<\/a>, <a href=\"https:\/\/stackoverflow.com\/questions\/64038673\/could-not-build-wheels-for-which-use-pep-517-and-cannot-be-installed-directly\">Could not build wheels for _ which use PEP 517 and cannot be installed directly - Easy Solution<\/a> and <a href=\"https:\/\/github.com\/martomi\/chiadog\/issues\/44\" rel=\"nofollow noreferrer\">failed building wheel for pynacl<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-04-18 04:47:54.673000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_valid_tags":[
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":71783228,
        "Question_title":"How do I specify nodeSelector while deploying an Azure ML model to an AKS Cluster?",
        "Question_body":"<p>I am currently deploying a model trained using AzureML to an AKS cluster as follows:<\/p>\n<pre><code>deployment_config_aks = AksWebservice.deploy_configuration(\n    cpu_cores = 1, \n    memory_gb = 1)\n\nservice = Model.deploy(ws, &quot;test&quot;, [model], inference_config, deployment_config_aks, aks_target)\n\n<\/code><\/pre>\n<p>I would like this service to be scheduled on a specific nodepool. With normal Kubernetes deployment, I can specify a <code>nodeSelector<\/code> like:<\/p>\n<pre><code>spec:\n  nodeSelector:\n    myNodeName: alpha\n<\/code><\/pre>\n<p>How do I specify a <code>nodeSelector<\/code> while deploying an Azure ML model to an AKS Cluster? Or in general, is there a way to merge my custom pod spec with the one generated by Azure ML library?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-04-07 13:40:37.060000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"azure|azure-aks|azure-machine-learning-service",
        "Question_view_count":289,
        "Owner_creation_date":"2012-06-06 09:14:53.053000 UTC",
        "Owner_last_access_date":"2022-09-23 10:11:20.003000 UTC",
        "Owner_location":"Beijing, China",
        "Owner_reputation":1830,
        "Owner_up_votes":413,
        "Owner_down_votes":3,
        "Owner_views":155,
        "Answer_body":"<blockquote>\n<p>How do I specify a nodeSelector while deploying an Azure ML model to an AKS Cluster? Or in general, is there a way to merge my custom pod spec with the one generated by Azure ML library?<\/p>\n<\/blockquote>\n<p>As per <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-attach-arc-kubernetes?tabs=studio\" rel=\"nofollow noreferrer\">Configure Kubernetes clusters for machine learning<\/a>:<\/p>\n<p><code>nodeSelector<\/code> : Set the node selector so the extension components and the training\/inference workloads will only be deployed to the nodes with all specified selectors.<\/p>\n<p>For example:<\/p>\n<p><code>nodeSelector.key=value<\/code> , <code>nodeSelector.node-purpose=worker<\/code> and <code>nodeSelector.node-region=eastus<\/code><\/p>\n<p>You can refer to <a href=\"https:\/\/kubernetes.io\/docs\/concepts\/scheduling-eviction\/assign-pod-node\/#built-in-node-labels\" rel=\"nofollow noreferrer\">Assigning Pods to Nodes<\/a> and <a href=\"https:\/\/github.com\/Azure\/AKS\/issues\/2866\" rel=\"nofollow noreferrer\">Cannot create nodepool with node-restriction.kubernetes.io\/ prefix label<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-04-21 06:21:15.520000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":63691515,
        "Question_title":"Azure Machine Learning Studio: cannot create Datastore from Azure SQL Database",
        "Question_body":"<p>I am trying to connect to an Azure SQL Database from inside Azure Machine Learning Studio. Based on <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.datastore.datastore?view=azure-ml-py\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.datastore.datastore?view=azure-ml-py<\/a>, it seems that the recommended pattern is to create a Datastore using the Datastore.register_azure_sql_database method as follows:<\/p>\n<pre><code>import os\nfrom azureml.core import Workspace, Datastore\n\nws = Workspace.from_config() # asks for interactive authentication the first time\n\nsql_datastore_name  = &quot;datastore_test_01&quot; # any name should be fine\nserver_name         = os.getenv(&quot;SQL_SERVERNAME&quot;    , &quot;{SQL_SERVERNAME}&quot;) # Name of the Azure SQL server\ndatabase_name       = os.getenv(&quot;SQL_DATABASENAME&quot;  , &quot;{SQL_DATABASENAME}&quot;) # Name of the Azure SQL database\nusername            = os.getenv(&quot;SQL_USER_NAME&quot;     , &quot;{SQL_USER_NAME}&quot;) # The username of the database user.\npassword            = os.getenv(&quot;SQL_USER_PASSWORD&quot; , &quot;{SQL_USER_PASSWORD}&quot;) # The password of the database user.\n\nsql_datastore = Datastore.register_azure_sql_database(workspace      = ws,\n                                                      datastore_name = sql_datastore_name,\n                                                      server_name    = server_name,\n                                                      database_name  = database_name,\n                                                      username       = username,\n                                                      password       = password)\n<\/code><\/pre>\n<p>I am pretty sure I have set all parameters right, having copied them from the ADO.NET connection string at my SQL Database resource --&gt; Settings --&gt; Connection strings:<\/p>\n<pre><code>Server=tcp:{SQL_SERVERNAME}.database.windows.net,1433;Initial Catalog={SQL_DATABASENAME};Persist Security Info=False;User ID={SQL_USER_NAME};Password={SQL_USER_PASSWORD};MultipleActiveResultSets=False;Encrypt=True;TrustServerCertificate=False;Connection Timeout=30;\n<\/code><\/pre>\n<p>However, I get the following error:<\/p>\n<pre><code>Registering datastore failed with a 400 error code and error message 'Azure SQL Database Error -2146232060: Please check the correctness of the datastore information.'\n<\/code><\/pre>\n<p>Am I missing something? E.g., a firewall rule? I have also tried adding the Azure ML compute resource's public IP address to the list of allowed IP addresses in my SQL Database resource, but still no success.<\/p>\n<hr \/>\n<p><strong>UPDATE<\/strong>: adding <code>skip_validation = True<\/code> to <code>Datastore.register_azure_sql_database<\/code> solves the issue. I can then query the data with<\/p>\n<pre><code>from azureml.core import Dataset\nfrom azureml.data.datapath import DataPath\n\nquery   = DataPath(sql_datastore, 'SELECT * FROM my_table')\ntabular = Dataset.Tabular.from_sql_query(query, query_timeout = 10)\ndf = tabular.to_pandas_dataframe()\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-09-01 16:13:13.243000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-09-02 07:46:58.923000 UTC",
        "Question_score":1,
        "Question_tags":"azure|azure-sql-database|azure-machine-learning-studio|azure-machine-learning-service|azure-sdk-python",
        "Question_view_count":793,
        "Owner_creation_date":"2017-08-11 12:35:17.960000 UTC",
        "Owner_last_access_date":"2021-01-27 09:52:25.200000 UTC",
        "Owner_location":"Milano, MI, Italia",
        "Owner_reputation":132,
        "Owner_up_votes":10,
        "Owner_down_votes":0,
        "Owner_views":41,
        "Answer_body":"<p>is the datastore behind vnet? where are you running the registration code above? On a compute instance behind the same vnet?\nhere is the doc that describe what you need to do to connect to data behind vnet:\n<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-enable-virtual-network#use-datastores-and-datasets\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-enable-virtual-network#use-datastores-and-datasets<\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2020-09-01 16:53:06.260000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_valid_tags":[
            "azure-machine-learning-studio",
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":50514817,
        "Question_title":"Azure Machine Learning Studio SelectColumnsTransform - how to patch or set web service input parameter?",
        "Question_body":"<p>The sentiment analysis sample at <a href=\"https:\/\/gallery.azure.ai\/Collection\/Twitter-Sentiment-Analysis-Collection-1\" rel=\"nofollow noreferrer\">https:\/\/gallery.azure.ai\/Collection\/Twitter-Sentiment-Analysis-Collection-1<\/a> shows use of Filter Based Feature Selection in the training experiment, which is used to generate a SelectColumnsTransform to be saved and used in the predictive experiment, alongside the trained model. The article at <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio\/create-models-and-endpoints-with-powershell\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio\/create-models-and-endpoints-with-powershell<\/a> explains how you can programmatically train multiple models on different datasets, save those models and create then patch multiple new endpoints, so that each can be used for scoring using a different model. The same technique can also be used to create and save multiple SelectColumnsTransform outputs, for feature selection specific to a given set of training data. However, the Patch-AmlWebServiceEndpoint does not appear to allow a SelectColumnsTransform in a scoring web service to be amended to use the relevant itransform saved during training. An 'EditableResourcesNotAvailable' message is returned, along with a list of resources that can be edited which includes models but not transformations. In addition, unlike (say) ImportData, a SelectColumnsTransform does not offer any parameters that can be exposed as web service parameters. <\/p>\n\n<p>So, how is it possible to create multiple web service endpoints programmatically that each use different SelectColumnsTransform itransform blobs, such as for a document classification service where each endpoint is based on a different set of training data?<\/p>\n\n<p>Any information much appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2018-05-24 17:14:19.560000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"azure-machine-learning-studio",
        "Question_view_count":74,
        "Owner_creation_date":"2018-04-25 12:15:35.683000 UTC",
        "Owner_last_access_date":"2020-10-23 11:00:26.277000 UTC",
        "Owner_location":null,
        "Owner_reputation":31,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":3,
        "Answer_body":"<p>Never mind. I got rid of the SelectColumnsTransform altogether (departing from the example experiment), instead using a R script in the training experiment to save the names of the columns selected, then another R script in the predictive experiment to load those names and remove any other feature columns.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2018-06-01 15:11:30.430000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":49168673,
        "Question_title":"How to load a training set in AWS SageMaker to build a model?",
        "Question_body":"<p>I am very new to SageMaker. Upon my first interaction, it looks like the AWS SageMaker requires you to start from its Notebook. I have a training set which is ready. Is there a way to bypass setting the Notebook and just to start by upload the training set? Or it should be done through the Notebook. If anyone knows some example fitting my need above, that will be great. <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2018-03-08 08:33:40.543000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":700,
        "Owner_creation_date":"2011-05-13 06:51:53.437000 UTC",
        "Owner_last_access_date":"2022-09-25 04:56:44.010000 UTC",
        "Owner_location":null,
        "Owner_reputation":10317,
        "Owner_up_votes":268,
        "Owner_down_votes":1,
        "Owner_views":595,
        "Answer_body":"<p>Amazon SageMaker is a combination of multiple services that each is independent of the others. You can use the notebook instances if you want to develop your models in the familiar Jupyter environment. But if just need to train a model, you can use the training jobs without opening a notebook instance. <\/p>\n\n<p>There a few ways to launch a training job:<\/p>\n\n<ul>\n<li>Use the high-level SDK for Python that is similar to the way that you start a training step in your python code<\/li>\n<\/ul>\n\n<p><code>kmeans.fit(kmeans.record_set(train_set[0]))<\/code><\/p>\n\n<p>Here is the link to the python library: <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk<\/a><\/p>\n\n<ul>\n<li>Use the low-level API to Create-Training-Job, and you can do that using various SDK (Java, Python, JavaScript, C#...) or the CLI. <\/li>\n<\/ul>\n\n<p><code>sagemaker = boto3.client('sagemaker')\n sagemaker.create_training_job(**create_training_params)<\/code><\/p>\n\n<p>Here is a link to the documentation on these options: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-train-model-create-training-job.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-train-model-create-training-job.html<\/a> <\/p>\n\n<ul>\n<li>Use Spark interface to launch it using a similar interface to creating an MLLib training job<\/li>\n<\/ul>\n\n<p><code>val estimator = new KMeansSageMakerEstimator(\n  sagemakerRole = IAMRole(roleArn),\n  trainingInstanceType = \"ml.p2.xlarge\",\n  trainingInstanceCount = 1,\n  endpointInstanceType = \"ml.c4.xlarge\",\n  endpointInitialInstanceCount = 1)\n  .setK(10).setFeatureDim(784)<\/code><\/p>\n\n<p><code>val model = estimator.fit(trainingData)<\/code><\/p>\n\n<p>Here is a link to the spark-sagemaker library: <a href=\"https:\/\/github.com\/aws\/sagemaker-spark\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-spark<\/a><\/p>\n\n<ul>\n<li>Create a training job in the Amazon SageMaker console using the wizard there: <a href=\"https:\/\/console.aws.amazon.com\/sagemaker\/home?region=us-east-1#\/jobs\" rel=\"nofollow noreferrer\">https:\/\/console.aws.amazon.com\/sagemaker\/home?region=us-east-1#\/jobs<\/a><\/li>\n<\/ul>\n\n<p>Please note that there a few options also to train models, either using the built-in algorithms such as K-Means, Linear Learner or XGBoost (see here for the complete list: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algos.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algos.html<\/a>). But you can also bring your own models for pre-baked Docker images such as TensorFlow (<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/tf.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/tf.html<\/a>) or MXNet (<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/mxnet.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/mxnet.html<\/a>), your own Docker image (<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo.html<\/a>).  <\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2018-03-09 21:36:14.253000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":60712225,
        "Question_title":"How to retrain a sagemaker model on new data, after a training job is created",
        "Question_body":"<p>I have a sagemaker model that is trained on a specific dataset, and training job is created. Now i have a new dataset that the model has to be trained on, how do I retrain the model on new data from the already existing model ? Can we have the model checkpoints saved ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2020-03-16 19:37:49.987000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":3,
        "Question_tags":"amazon-sagemaker|pre-trained-model",
        "Question_view_count":660,
        "Owner_creation_date":"2017-07-19 21:52:51.967000 UTC",
        "Owner_last_access_date":"2020-06-02 23:45:58.220000 UTC",
        "Owner_location":"New York, NY, United States",
        "Owner_reputation":33,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":8,
        "Answer_body":"<blockquote>\n  <p>Only three built-in algorithms currently support incremental training: Object Detection Algorithm, Image Classification Algorithm, and Semantic Segmentation Algorithm.<\/p>\n<\/blockquote>\n\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/incremental-training.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/incremental-training.html<\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2020-03-17 16:31:44.473000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":64211755,
        "Question_title":"How to upload packages to an instance in a Processing step in Sagemaker?",
        "Question_body":"<p>I have to do large scale feature engineering on some data. My current approach is to spin up an instance using <code>SKLearnProcessor<\/code> and then scale the job by choosing a larger instance size or increasing the number of instances. I require using some packages that are not installed on Sagemaker instances by default and so I want to install the packages using .whl files.<\/p>\n<p>Another hurdle is that the Sagemaker role does not have internet access.<\/p>\n<pre><code>import boto3\nimport sagemaker\nfrom sagemaker import get_execution_role\nfrom sagemaker.sklearn.processing import SKLearnProcessor\n\nsess = sagemaker.Session()\nsess.default_bucket()        \n\nregion = boto3.session.Session().region_name\n\nrole = get_execution_role()\nsklearn_processor = SKLearnProcessor(framework_version='0.20.0',\n                                     role=role,\n                                     sagemaker_session = sess,\n                                     instance_type=&quot;ml.t3.medium&quot;,\n                                     instance_count=1)\n\nsklearn_processor.run(code='script.py')\n<\/code><\/pre>\n<p><strong>Attempted resolutions:<\/strong><\/p>\n<ol>\n<li>Upload the packages to a CodeCommit repository and clone the repo into the SKLearnProcessor instances. Failed with error <code>fatal: could not read Username for 'https:\/\/git-codecommit.eu-west-1.amazonaws.com': No such device or address<\/code>. I tried cloning the repo into a sagemaker notebook instance and it works, so its not a problem with my script.<\/li>\n<li>Use a bash script to copy the packages from s3 using the CLI. The bash script I used is based off <a href=\"https:\/\/medium.com\/@shadidc\/installing-custom-python-package-to-sagemaker-notebook-b7b897f4f655\" rel=\"nofollow noreferrer\">this post<\/a>. But the packages never get copied, and an error message is not thrown.<\/li>\n<li>Also looked into using the package <code>s3fs<\/code> but it didn't seem suitable to copy the wheel files.<\/li>\n<\/ol>\n<p><strong>Alternatives<\/strong><\/p>\n<p>My client is hesitant to spin up containers from custom docker images. Any alternatives?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-10-05 15:33:40.397000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|amazon-s3|package|amazon-sagemaker",
        "Question_view_count":1071,
        "Owner_creation_date":"2018-08-29 14:33:10.827000 UTC",
        "Owner_last_access_date":"2022-09-23 13:44:45.730000 UTC",
        "Owner_location":"Johannesburg",
        "Owner_reputation":438,
        "Owner_up_votes":180,
        "Owner_down_votes":4,
        "Owner_views":67,
        "Answer_body":"<p><code>2. Use a bash script to copy the packages from s3 using the CLI. The bash script I used is based off this post. But the packages never get copied, and an error message is not thrown.<\/code><\/p>\n<p>This approach seems sound.<\/p>\n<p>You may be better off overriding the <code>command<\/code> field on the <code>SKLearnProcessor<\/code> to <code>\/bin\/bash<\/code>, run a bash script like <code>install_and_run_my_python_code.sh<\/code> that installs the wheel containing your python dependencies, then runs your main python entry point script.<\/p>\n<p>Additionally, instead of using AWS S3 calls to download your code in a script, you could use a <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/processing.html#sagemaker.processing.ProcessingInput\" rel=\"nofollow noreferrer\">ProcessingInput<\/a> to download your code rather than doing this with AWS CLI calls in a bash script, which is what the <code>SKLearnProcessor<\/code> does to download your entry point <code>script.py<\/code> code across all the instances.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2020-10-29 21:22:55.953000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":44053028,
        "Question_title":"Setting job tags for Neptune",
        "Question_body":"<p>Recently I started using Neptune (via <a href=\"https:\/\/go.neptune.deepsense.io\/\" rel=\"nofollow noreferrer\">Neptune Go<\/a>) and want to have a well-organised history of experiments. How to set tags to a given experiment? (Do I do it before running it, or after?)<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2017-05-18 16:17:28.987000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"tags|neptune",
        "Question_view_count":57,
        "Owner_creation_date":"2011-08-23 11:04:24.770000 UTC",
        "Owner_last_access_date":"2022-09-19 15:31:46.563000 UTC",
        "Owner_location":"Warsaw, Poland",
        "Owner_reputation":11056,
        "Owner_up_votes":1563,
        "Owner_down_votes":3,
        "Owner_views":544,
        "Answer_body":"<p>There are four ways to set tags to your experiment:<\/p>\n\n<ol>\n<li>In the <code>run\/enqueue\/exec<\/code> command, i.e:<\/li>\n<\/ol>\n\n<p><code>neptune run --tags tag1 tag2 tag3 tag4<\/code><\/p>\n\n<ol start=\"2\">\n<li>In the configuration file:<\/li>\n<\/ol>\n\n<p><code>tags: [tag1, tag2, tag3, tag4]<\/code><\/p>\n\n<ol start=\"3\">\n<li>In your code:<\/li>\n<\/ol>\n\n<p><code>ctx.job.tags.append('new-tag')<\/code><\/p>\n\n<ol start=\"4\">\n<li>In the Web UI. In the experiment dashboard you have to click on \"Job Properties\" in the top left corner of the screen. Side panel will appear where you can modify job properties.<\/li>\n<\/ol>\n\n<p>So you can change tags of your experiment in every phase of your experiment execution.<\/p>\n\n<p>Sources: <\/p>\n\n<ul>\n<li><p><a href=\"http:\/\/neptune.deepsense.io\/versions\/latest\/reference-guides\/cli.html#tags\" rel=\"nofollow noreferrer\">http:\/\/neptune.deepsense.io\/versions\/latest\/reference-guides\/cli.html#tags<\/a><\/p><\/li>\n<li><p><a href=\"http:\/\/neptune.deepsense.io\/versions\/latest\/reference-guides\/job-and-experiment.html#tags\" rel=\"nofollow noreferrer\">http:\/\/neptune.deepsense.io\/versions\/latest\/reference-guides\/job-and-experiment.html#tags<\/a> <\/p><\/li>\n<\/ul>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2017-05-19 06:41:46.327000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":3.0,
        "Question_valid_tags":[
            "neptune"
        ]
    },
    {
        "Question_id":56497428,
        "Question_title":"Use images in s3 with SageMaker without .lst files",
        "Question_body":"<p>I am trying to create (what I thought was) a simple image classification pipeline between s3 and SageMaker.<\/p>\n\n<p>Images are stored in an s3 bucket with their class labels in their file names currently, e.g.<\/p>\n\n<p><strong>My-s3-bucket-dir<\/strong><\/p>\n\n<pre><code>cat-1.jpg\ndog-1.jpg\ncat-2.jpg\n..\n<\/code><\/pre>\n\n<p>I've been trying to leverage several related example .py scripts, but most seem to be download data sets already in .rec format or containing special manifest or annotation files I don't have.<\/p>\n\n<p>All I want is to pass the images from s3 to the SageMaker image classification algorithm that's located in the same region, IAM account, etc. I suppose this means I need a <code>.lst<\/code> file<\/p>\n\n<p>When I try to manually create the <code>.lst<\/code> it doesn't seem to like it and it also takes too long doing manual work to be a good practice.<\/p>\n\n<p>How can I automatically generate the <code>.lst<\/code> file (or otherwise send the images\/classes for training)? <\/p>\n\n<p>Things I read made it sound like <code>im2rec.py<\/code> was a solution, but I don't see how. The example I'm working with now is <\/p>\n\n<p><code>Image-classification-fulltraining-highlevel.ipynb<\/code><\/p>\n\n<p>but it seems to download the data as <code>.rec<\/code>, <\/p>\n\n<pre><code>download('http:\/\/data.mxnet.io\/data\/caltech-256\/caltech-256-60-train.rec')\ndownload('http:\/\/data.mxnet.io\/data\/caltech-256\/caltech-256-60-val.rec')\n<\/code><\/pre>\n\n<p>which just skips working with the .jpeg files. I found another that converts them to <code>.rec<\/code> but again it has essentially the <code>.lst<\/code> already as <code>.json<\/code> and just converts it.<\/p>\n\n<p>I have mostly been working in a Python Jupyter notebook within the AWS console (in my browser) but I have also tried using their GUI. <\/p>\n\n<p>How can I simply and automatically generate the <code>.lst<\/code> or otherwise get the data\/class info into SageMaker without manually creating a <code>.lst<\/code> file?<\/p>\n\n<p><strong><em>Update<\/em><\/strong><\/p>\n\n<p>It looks like im2py can't be run against s3. You'd have to completely download everything from all s3 buckets into the notebook's storage...<\/p>\n\n<blockquote>\n  <p>Please note that [...] im2rec.py is running locally,\n  therefore cannot take input from the S3 bucket. To generate the list\n  file, you need to download the data and then use the im2rec tool. - AWS SageMaker Team<\/p>\n<\/blockquote>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-06-07 15:37:31.153000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2019-06-10 15:50:49.817000 UTC",
        "Question_score":2,
        "Question_tags":"python-3.x|amazon-s3|computer-vision|amazon-sagemaker",
        "Question_view_count":937,
        "Owner_creation_date":"2014-05-05 14:48:58.467000 UTC",
        "Owner_last_access_date":"2022-09-24 18:32:51.257000 UTC",
        "Owner_location":"Columbia, MD, USA",
        "Owner_reputation":21413,
        "Owner_up_votes":2163,
        "Owner_down_votes":516,
        "Owner_views":6465,
        "Answer_body":"<p>There are 3 options to provide annotated data to the Image Classification algo: (1) packing labels in recordIO files, (2) storing labels in a JSON manifest file (\"augmented manifest\" option), (3) storing labels in a list file. All options are documented here: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/image-classification.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/image-classification.html<\/a>.<\/p>\n\n<p>Augmented Manifest and .lst files option are quick to do since they just require you to create an annotation file with a usually quick <code>for<\/code> loop for example. RecordIO requires you to use <code>im2rec.py<\/code> tool, which is a little more work.<\/p>\n\n<p>Using .lst files is <strong>another option<\/strong> that is reasonably easy: you just need to create annotation them with a quick for loop, like this:<\/p>\n\n<pre><code># assuming train_index, train_class, train_pics store the pic index, class and path\n\nwith open('train.lst', 'a') as file:\n    for index, cl, pic in zip(train_index, train_class, train_pics):\n        file.write(str(index) + '\\t' + str(cl) + '\\t' + pic + '\\n')\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2019-06-07 22:58:18.333000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":65887231,
        "Question_title":"Use mlflow to serve a custom python model for scoring",
        "Question_body":"<p>I am using Python code generated from an ml software with mlflow to read a dataframe, perform some table operations and output a dataframe. I am able to run the code successfully and save the new dataframe as an artifact. However I am unable to log the model using log_model because it is not a lr or classifier model where we train and fit. I want to log a model for this so that it can be served with new data and deployed with a rest API<\/p>\n<pre><code>df = pd.read_csv(r&quot;\/home\/xxxx.csv&quot;)\n\n\nwith mlflow.start_run():\n    def getPrediction(row):\n        \n        perform_some_python_operaions \n\n        return [Status_prediction, Status_0_probability, Status_1_probability]\n    columnValues = []\n    for column in columns:\n        columnValues.append([])\n\n    for index, row in df.iterrows():\n        results = getPrediction(row)\n        for n in range(len(results)):\n            columnValues[n].append(results[n])\n\n    for n in range(len(columns)):\n        df[columns[n]] = columnValues[n]\n\n    df.to_csv('dataset_statistics.csv')\n    mlflow.log_artifact('dataset_statistics.csv')\n   \n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-01-25 15:00:24.463000 UTC",
        "Question_favorite_count":3.0,
        "Question_last_edit_date":null,
        "Question_score":4,
        "Question_tags":"python|deployment|mlflow|mlops",
        "Question_view_count":3026,
        "Owner_creation_date":"2019-11-14 13:58:10.560000 UTC",
        "Owner_last_access_date":"2022-09-23 08:37:32.563000 UTC",
        "Owner_location":null,
        "Owner_reputation":115,
        "Owner_up_votes":16,
        "Owner_down_votes":0,
        "Owner_views":25,
        "Answer_body":"<p>MLflow supports <a href=\"https:\/\/mlflow.org\/docs\/latest\/models.html#custom-python-models\" rel=\"nofollow noreferrer\">custom models<\/a> of mlflow.pyfunc flavor.  You can create a custom  class  inherited from the <code>mlflow.pyfunc.PythonModel<\/code>, that needs to provide function <code>predict<\/code> for performing predictions, and optional <code>load_context<\/code> to load the necessary artifacts, like this (adopted from the docs):<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>class MyModel(mlflow.pyfunc.PythonModel):\n\n    def load_context(self, context):\n        # load your artifacts\n\n    def predict(self, context, model_input):\n        return my_predict(model_input.values)\n<\/code><\/pre>\n<p>You can log to MLflow whatever artifacts you need for your models, define Conda environment if necessary, etc.<br \/>\nThen you can use <code>save_model<\/code> with your class to save your implementation, that could be loaded with <code>load_model<\/code> and do the <code>predict<\/code> using your model:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>mlflow.pyfunc.save_model(\n        path=mlflow_pyfunc_model_path, \n        python_model=MyModel(), \n        artifacts=artifacts)\n\n# Load the model in `python_function` format\nloaded_model = mlflow.pyfunc.load_model(mlflow_pyfunc_model_path)\n<\/code><\/pre>",
        "Answer_comment_count":3.0,
        "Answer_creation_date":"2021-01-25 16:41:54.947000 UTC",
        "Answer_last_edit_date":"2021-10-14 05:05:40.523000 UTC",
        "Answer_score":9.0,
        "Question_valid_tags":[
            "mlflow"
        ]
    },
    {
        "Question_id":49586005,
        "Question_title":"Azure ML workbench failed installing on Windows 10 Enterprise",
        "Question_body":"<p>Error installing component: azure_cli_ml_cliextension.windows \"The action failed catastrophically with <\/p>\n\n<p>Microsoft.MachineLearning.Installer.Engine.Actions.RegisteredActions.AzureCliException: Unable to get list of currently installed Azure CLI extensions<\/p>\n\n<p>at <\/p>\n\n<p>Microsoft.MachineLearning.Installer.Engine.Actions.RegisteredActions.InstallAzureCliExtensionAction.d__23.MoveNext() in C:\\swarm\\workspace\\Installer-1.2\\Installer.Engine\\Actions\\RegisteredActions\\InstallAzureCliExtensionAction.cs:line 97<\/p>\n\n<p>from there everything is stops....<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2018-03-31 08:51:36.150000 UTC",
        "Question_favorite_count":2.0,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"windows|azure-machine-learning-studio|azure-machine-learning-workbench",
        "Question_view_count":296,
        "Owner_creation_date":"2017-09-26 14:24:54.640000 UTC",
        "Owner_last_access_date":"2022-09-25 04:59:24.040000 UTC",
        "Owner_location":"Southeast Asia",
        "Owner_reputation":1922,
        "Owner_up_votes":1232,
        "Owner_down_votes":72,
        "Owner_views":404,
        "Answer_body":"<p>Workbench is a preview product and issues may occur. Please try and get a newer exe and try again. It also seems like you have azure powershell issues here which I would have expected to be taken care of by the installer, but perhaps you can try and install azure powershell first. <\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_date":"2018-05-06 18:04:52.870000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_valid_tags":[
            "azure-machine-learning-studio",
            "azure-machine-learning-workbench"
        ]
    },
    {
        "Question_id":71578582,
        "Question_title":"connect vertex ai endpoint through .Net",
        "Question_body":"<p>Is there any way to connected google cloud platform service vertex ai endpoint through .Net code ? I am new to gcp vertex. any help is really apricated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-03-22 20:32:35.950000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|nlp|google-cloud-vertex-ai",
        "Question_view_count":128,
        "Owner_creation_date":"2016-05-05 17:32:36.837000 UTC",
        "Owner_last_access_date":"2022-09-22 20:14:36.583000 UTC",
        "Owner_location":null,
        "Owner_reputation":509,
        "Owner_up_votes":71,
        "Owner_down_votes":0,
        "Owner_views":65,
        "Answer_body":"<p>You could refer to <a href=\"https:\/\/cloud.google.com\/dotnet\/docs\/reference\/Google.Cloud.AIPlatform.V1\/latest\" rel=\"nofollow noreferrer\">Google.Cloud.AIPlatform.V1 documentation<\/a> for reference for dot net. You can start by:<\/p>\n<ol>\n<li>Installing the package from <a href=\"https:\/\/www.nuget.org\/packages\/Google.Cloud.AIPlatform.V1\/\" rel=\"nofollow noreferrer\">Google AI Platform nuget<\/a><\/li>\n<li>If you don't have an endpoint yet you can check out <a href=\"https:\/\/cloud.google.com\/dotnet\/docs\/reference\/Google.Cloud.AIPlatform.V1\/latest\/Google.Cloud.AIPlatform.V1.EndpointServiceClient\" rel=\"nofollow noreferrer\">Google.Cloud.AIPlatform.V1.EndpointServiceClient<\/a>. You can use this class to manage endpoints like create endpoint, delete endpoint, deploy endpoint, etc.\n<ul>\n<li>Check this <a href=\"https:\/\/github.com\/googleapis\/google-cloud-dotnet\/blob\/main\/apis\/Google.Cloud.AIPlatform.V1\/Google.Cloud.AIPlatform.V1.Snippets\/IndexEndpointServiceClientSnippets.g.cs\" rel=\"nofollow noreferrer\">EndpointServiceClient code sample<\/a> for usage.<\/li>\n<\/ul>\n<\/li>\n<li>If you have an endpoint and you want to run predictions using it, you can check out <a href=\"https:\/\/cloud.google.com\/dotnet\/docs\/reference\/Google.Cloud.AIPlatform.V1\/latest\/Google.Cloud.AIPlatform.V1.PredictionServiceClient\" rel=\"nofollow noreferrer\">Google.Cloud.AIPlatform.V1.PredictionServiceClient<\/a>. You can use this class to perform prediction using your endpoint.\n<ul>\n<li>Specifically <a href=\"https:\/\/cloud.google.com\/dotnet\/docs\/reference\/Google.Cloud.AIPlatform.V1\/latest\/Google.Cloud.AIPlatform.V1.PredictionServiceClient#Google_Cloud_AIPlatform_V1_PredictionServiceClient_Predict_Google_Cloud_AIPlatform_V1_EndpointName_System_Collections_Generic_IEnumerable_Google_Protobuf_WellKnownTypes_Value__Google_Protobuf_WellKnownTypes_Value_Google_Api_Gax_Grpc_CallSettings_\" rel=\"nofollow noreferrer\">Predict(EndpointName, IEnumerable, Value, CallSettings)<\/a> method where it accepts an endpoint as parameter.<\/li>\n<li>Check this <a href=\"https:\/\/github.com\/googleapis\/google-cloud-dotnet\/blob\/main\/apis\/Google.Cloud.AIPlatform.V1\/Google.Cloud.AIPlatform.V1.Snippets\/PredictionServiceClientSnippets.g.cs\" rel=\"nofollow noreferrer\">PredictionServiceClient code sample<\/a> for usage.<\/li>\n<\/ul>\n<\/li>\n<\/ol>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-03-23 02:54:51.847000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "google-cloud-vertex-ai"
        ]
    },
    {
        "Question_id":70279636,
        "Question_title":"Azure Auto ML JobConfigurationMaxSizeExceeded error when using a cluster",
        "Question_body":"<p>I am running into the following error when I try to run Automated ML through the studio on a GPU compute cluster:<\/p>\n<p><img src=\"https:\/\/i.stack.imgur.com\/uLyxr.png\" alt=\"Azure ML error message\" \/><\/p>\n<blockquote>\n<p>Error: AzureMLCompute job failed. JobConfigurationMaxSizeExceeded: The\nspecified job configuration exceeds the max allowed size of 32768\ncharacters. Please reduce the size of the job's command line arguments\nand environment settings<\/p>\n<\/blockquote>\n<p>The attempted run is on a registered tabulated dataset in filestore and is a simple regression case. Strangely, it works just fine with the CPU compute instance I use for my other pipelines. I have been able to run it a few times using that and wanted to upgrade to a cluster only to be hit by this error. I found online that it could be a case of having the following setting: AZUREML_COMPUTE_USE_COMMON_RUNTIME:false; but I am not sure where to put this in when just running from the web studio.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2021-12-08 17:57:54.657000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-01-03 10:09:48.637000 UTC",
        "Question_score":5,
        "Question_tags":"azure-machine-learning-studio|azure-machine-learning-service",
        "Question_view_count":171,
        "Owner_creation_date":"2020-06-17 16:49:15.903000 UTC",
        "Owner_last_access_date":"2022-09-22 15:56:36.937000 UTC",
        "Owner_location":null,
        "Owner_reputation":51,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":0,
        "Answer_body":"<p>It looks like the bug was fixed. I just ran it on a cluster without changing any of the parameters. Thank you Yutong for the help!<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-12-15 16:19:23.673000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_valid_tags":[
            "azure-machine-learning-studio",
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":52632388,
        "Question_title":"Is it possible to deploy a already trained Keras to Sagemaker?",
        "Question_body":"<p>I was playing with the AWS instances and trying to deploy some locally trained Keras models, but I find no documentation on that. Has anyone already been able to do it? <\/p>\n\n<p>I tried to use a similar approach to <a href=\"https:\/\/aws.amazon.com\/pt\/blogs\/machine-learning\/bring-your-own-pre-trained-mxnet-or-tensorflow-models-into-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/pt\/blogs\/machine-learning\/bring-your-own-pre-trained-mxnet-or-tensorflow-models-into-amazon-sagemaker\/<\/a>, but I had no success. I also found some examples for training keras models in the cloud, but I was not able to get the entry_point + artifacts right. <\/p>\n\n<p>Thanks for your time!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2018-10-03 17:14:39.113000 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"tensorflow|amazon-ec2|keras|amazon-sagemaker",
        "Question_view_count":1291,
        "Owner_creation_date":"2015-11-20 14:31:08.997000 UTC",
        "Owner_last_access_date":"2022-06-22 10:23:49.857000 UTC",
        "Owner_location":"Portugal",
        "Owner_reputation":260,
        "Owner_up_votes":65,
        "Owner_down_votes":0,
        "Owner_views":52,
        "Answer_body":"<p>Yes, it is possible, and yes, the official documentation is not much of help.\nHowever, I wrote an <a href=\"https:\/\/gnomezgrave.com\/2018\/07\/05\/using-a-custom-model-for-ml-inference-with-amazon-sagemaker\" rel=\"nofollow noreferrer\">article on that<\/a>, and I hope it will help you.<\/p>\n\n<p>Let me know if you need more details. Cheers!<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2018-10-04 09:22:37.793000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":3.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":69944447,
        "Question_title":"How to change the directory of mlflow logs?",
        "Question_body":"<p>I am using MLflow to log the metrics but I want to change the default saving logs directory. So, instead of writing log files besides my main file, I want to store them to <code>\/path\/outputs\/lg <\/code>. I don't know how to change it. I use it without in the <code>Model<\/code>.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import os\nfrom time import time\n\nimport mlflow\nimport numpy as np\nimport torch\nimport tqdm\n\n# from segmentation_models_pytorch.utils import metrics\nfrom AICore.emergency_landing.metrics import IoU, F1\nfrom AICore.emergency_landing.utils import AverageMeter\nfrom AICore.emergency_landing.utils import TBLogger\n\n\nclass Model:\n    def __init__(self, model, num_classes=5, ignore_index=0, optimizer=None, scheduler=None, criterion=None,\n                 device=None, epochs=30, train_loader=None, val_loader=None, tb_logger: TBLogger = None,\n                 logger=None,\n                 best_model_path=None,\n                 model_check_point_path=None,\n                 load_from_best_model=None,\n                 load_from_model_checkpoint=None,\n                 early_stopping=None,\n                 debug=False):\n\n        self.debug = debug\n\n        self.early_stopping = {\n            'init': early_stopping,\n            'changed': 0\n        }\n        self.optimizer = optimizer\n        self.scheduler = scheduler\n        self.criterion = criterion\n        self.device = device\n        self.epochs = epochs\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n\n        self.model = model.to(device)\n\n        self.tb_logger = tb_logger\n        self.logger = logger\n\n        self.best_loss = np.Inf\n\n        if not os.path.exists(best_model_path):\n            os.makedirs(best_model_path)\n        self.best_model_path = best_model_path\n\n        if not os.path.exists(model_check_point_path):\n            os.makedirs(model_check_point_path)\n        self.model_check_point_path = model_check_point_path\n\n        self.load_from_best_model = load_from_best_model\n        self.load_from_model_checkpoint = load_from_model_checkpoint\n\n        if self.load_from_best_model is not None:\n            self.load_model(path=self.load_from_best_model)\n        if self.load_from_model_checkpoint is not None:\n            self.load_model_checkpoint(path=self.load_from_model_checkpoint)\n\n        self.train_iou = IoU(num_classes=num_classes, ignore_index=ignore_index)\n        self.val_iou = IoU(num_classes=num_classes, ignore_index=ignore_index)\n        self.test_iou = IoU(num_classes=num_classes, ignore_index=ignore_index)\n\n        self.train_f1 = F1(num_classes=num_classes, ignore_index=ignore_index, mdmc_average='samplewise')\n        self.val_f1 = F1(num_classes=num_classes, ignore_index=ignore_index, mdmc_average='samplewise')\n        self.test_f1 = F1(num_classes=num_classes, ignore_index=ignore_index, mdmc_average='samplewise')\n\n    def metrics(self, is_train=True):\n        if is_train:\n            train_losses = AverageMeter('Training Loss', ':.4e')\n            train_iou = AverageMeter('Training iou', ':6.2f')\n            train_f_score = AverageMeter('Training F_score', ':6.2f')\n\n            return train_losses, train_iou, train_f_score\n        else:\n            val_losses = AverageMeter('Validation Loss', ':.4e')\n            val_iou = AverageMeter('Validation mean iou', ':6.2f')\n            val_f_score = AverageMeter('Validation F_score', ':6.2f')\n\n            return val_losses, val_iou, val_f_score\n\n    def fit(self):\n\n        self.logger.info(&quot;\\nStart training\\n\\n&quot;)\n        start_training_time = time()\n\n        with mlflow.start_run():\n            for e in range(self.epochs):\n                start_training_epoch_time = time()\n                self.model.train()\n                train_losses_avg, train_iou_avg, train_f_score_avg = self.metrics(is_train=True)\n                with tqdm.tqdm(self.train_loader, unit=&quot;batch&quot;) as tepoch:\n                    tepoch.set_description(f&quot;Epoch {e}&quot;)\n                    for image, target in tepoch:\n                        # Transfer Data to GPU if available\n                        image = image.to(self.device)\n                        target = target.to(self.device)\n                        # Clear the gradients\n                        self.optimizer.zero_grad()\n                        # Forward Pass\n                        # out = self.model(image)['out']\n                        # if unet == true =&gt; remove ['out']\n                        out = self.model(image)\n                        # Find the Loss\n                        loss = self.criterion(out, target)\n                        # Calculate Loss\n                        train_losses_avg.update(loss.item(), image.size(0))\n                        # Calculate gradients\n                        loss.backward()\n                        # Update Weights\n                        self.optimizer.step()\n\n                        iou = self.train_iou(out.cpu(), target.cpu()).item()\n                        train_iou_avg.update(iou)\n\n                        f1_score = self.train_f1(out.cpu(), target.cpu()).item()\n                        train_f_score_avg.update(f1_score)\n\n                        tepoch.set_postfix(loss=train_losses_avg.avg,\n                                           iou=train_iou_avg.avg,\n                                           f_score=train_f_score_avg.avg)\n                        if self.debug:\n                            break\n\n                self.tb_logger.log(log_type='criterion\/training', value=train_losses_avg.avg, epoch=e)\n                self.tb_logger.log(log_type='iou\/training', value=train_iou_avg.avg, epoch=e)\n                self.tb_logger.log(log_type='f_score\/training', value=train_f_score_avg.avg, epoch=e)\n\n                mlflow.log_metric('criterion\/training', train_losses_avg.avg, step=e)\n                mlflow.log_metric('iou\/training', train_iou_avg.avg, step=e)\n                mlflow.log_metric('f_score\/training', train_f_score_avg.avg, step=e)\n\n                end_training_epoch_time = time() - start_training_epoch_time\n                print('\\n')\n                self.logger.info(\n                    f'Training Results - [{end_training_epoch_time:.3f}s] Epoch: {e}:'\n                    f' f_score: {train_f_score_avg.avg:.3f},'\n                    f' IoU: {train_iou_avg.avg:.3f},'\n                    f' Loss: {train_losses_avg.avg:.3f}')\n\n                # validation step\n                val_loss = self.evaluation(e)\n                # apply scheduler\n                if self.scheduler:\n                    self.scheduler.step()\n                # early stopping\n                if self.early_stopping['init'] &gt;= self.early_stopping['changed']:\n                    self._early_stopping_model(val_loss=val_loss)\n                else:\n                    print(f'The model can not learn more, Early Stopping at epoch[{e}]')\n                    break\n\n                # save best model\n                if self.best_model_path is not None:\n                    self._best_model(val_loss=val_loss, path=self.best_model_path)\n\n                # model check points\n                if self.model_check_point_path is not None:\n                    self.save_model_check_points(path=self.model_check_point_path, epoch=e, net=self.model,\n                                                 optimizer=self.optimizer, loss=self.criterion,\n                                                 avg_loss=train_losses_avg.avg)\n\n                # log mlflow\n                if self.scheduler:\n                    mlflow.log_param(&quot;get_last_lr&quot;, self.scheduler.get_last_lr())\n                    mlflow.log_param(&quot;scheduler&quot;, self.scheduler.state_dict())\n\n                self.tb_logger.flush()\n                if self.debug:\n                    break\n\n            end_training_time = time() - start_training_time\n            print(f'Finished Training after {end_training_time:.3f}s')\n            self.tb_logger.close()\n\n    def evaluation(self, epoch):\n        print('Validating...')\n        start_validation_epoch_time = time()\n        self.model.eval()  # Optional when not using Model Specific layer\n        with torch.no_grad():\n            val_losses_avg, val_iou_avg, val_f_score_avg = self.metrics(is_train=False)\n            with tqdm.tqdm(self.val_loader, unit=&quot;batch&quot;) as tepoch:\n                for image, target in tepoch:\n                    # Transfer Data to GPU if available\n                    image = image.to(self.device)\n                    target = target.to(self.device)\n                    # out = self.model(image)['out']\n                    # if unet == true =&gt; remove ['out']\n                    out = self.model(image)\n                    # Find the Loss\n                    loss = self.criterion(out, target)\n                    # Calculate Loss\n                    val_losses_avg.update(loss.item(), image.size(0))\n\n                    iou = self.val_iou(out.cpu(), target.cpu()).item()\n                    val_iou_avg.update(iou)\n\n                    f1_score = self.val_f1(out.cpu(), target.cpu()).item()\n                    val_f_score_avg.update(f1_score)\n\n                    tepoch.set_postfix(loss=val_losses_avg.avg,\n                                       iou=val_iou_avg.avg,\n                                       f_score=val_f_score_avg.avg)\n                    if self.debug:\n                        break\n            print('\\n')\n            self.tb_logger.log(log_type='criterion\/validation', value=val_losses_avg.avg, epoch=epoch)\n            self.tb_logger.log(log_type='iou\/validation', value=val_iou_avg.avg, epoch=epoch)\n            self.tb_logger.log(log_type='f_score\/validation', value=val_f_score_avg.avg, epoch=epoch)\n\n            mlflow.log_metric('criterion\/validation', val_losses_avg.avg, step=epoch)\n            mlflow.log_metric('iou\/validation', val_iou_avg.avg, step=epoch)\n            mlflow.log_metric('f_score\/validation', val_f_score_avg.avg, step=epoch)\n\n            end_validation_epoch_time = time() - start_validation_epoch_time\n            self.logger.info(\n                f'validation Results - [{end_validation_epoch_time:.3f}s] Epoch: {epoch}:'\n                f' f_score: {val_f_score_avg.avg:.3f},'\n                f' IoU: {val_iou_avg.avg:.3f},'\n                f' Loss: {val_losses_avg.avg:.3f}')\n            print('\\n')\n            return val_losses_avg.avg\n\n    def _save_model(self, name, path, params):\n        torch.save(params, path)\n\n    def _early_stopping_model(self, val_loss):\n        if self.best_loss &lt; val_loss:\n            self.early_stopping['changed'] += 1\n        else:\n            self.early_stopping['changed'] = 0\n\n    def _best_model(self, val_loss, path):\n        if self.best_loss &gt; val_loss:\n            self.best_loss = val_loss\n            name = f'\/best_model_loss_{self.best_loss:.2f}'.replace('.', '_')\n            self._save_model(name, path=f'{path}\/{name}.pt', params={\n                'model_state_dict': self.model.state_dict(),\n            })\n\n            print(f'The best model is saved with criterion: {self.best_loss:.2f}')\n\n    def save_model_check_points(self, path, epoch, net, optimizer, loss, avg_loss):\n        name = f'\/model_epoch_{epoch}_loss_{avg_loss:.2f}'.replace('.', '_')\n        self._save_model(name, path=f'{path}\/{name}.pt', params={\n            'epoch': epoch,\n            'model_state_dict': net.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'criterion': loss,\n        })\n        print(f'model checkpoint is saved at model_epoch_{epoch}_loss_{avg_loss:.2f}')\n\n    def load_model_checkpoint(self, path):\n        checkpoint = torch.load(path)\n        self.model.load_state_dict(checkpoint['model_state_dict'])\n        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        epoch = checkpoint['epoch']\n        self.criterion = checkpoint['criterion']\n\n        return epoch\n\n    def load_model(self, path):\n        best_model = torch.load(path)\n        self.model.load_state_dict(best_model['model_state_dict'])\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2021-11-12 14:25:19.830000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-11-12 22:16:19.243000 UTC",
        "Question_score":0,
        "Question_tags":"machine-learning|deep-learning|pytorch|mlflow",
        "Question_view_count":436,
        "Owner_creation_date":"2014-09-10 07:11:45.327000 UTC",
        "Owner_last_access_date":"2022-09-06 18:46:08.593000 UTC",
        "Owner_location":"Turin, Metropolitan City of Turin, Italy",
        "Owner_reputation":477,
        "Owner_up_votes":133,
        "Owner_down_votes":0,
        "Owner_views":130,
        "Answer_body":"<p>The solution is:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>mlflow.set_tracking_uri(uri=f'file:\/\/{hydra.utils.to_absolute_path(&quot;..\/output\/mlruns&quot;)}')\nexp = mlflow.get_experiment_by_name(name='Emegency_landing')\nif not exp:\n    experiment_id = mlflow.create_experiment(name='Emegency_landing',\n                                                 artifact_location=f'file:\/\/{hydra.utils.to_absolute_path(&quot;..\/output\/mlruns&quot;)}')\nelse:\n    experiment_id = exp.experiment_id\n<\/code><\/pre>\n<p>And then you should pass the experiment Id to:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>with mlflow.start_run(experiment_id=experiment_id):\n     pass \n<\/code><\/pre>\n<p>If you don't mention the <code>\/path\/mlruns<\/code>, when you run the command of <code>mlflow ui<\/code>, it will create another folder automatically named <code>mlruns<\/code>. so, pay attention to this point to have the same name as <code>mlruns<\/code>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-11-12 15:50:35.590000 UTC",
        "Answer_last_edit_date":"2021-11-12 22:19:00.677000 UTC",
        "Answer_score":0.0,
        "Question_valid_tags":[
            "mlflow"
        ]
    },
    {
        "Question_id":50669991,
        "Question_title":"AWS SageMaker is not authorized to perform: ecr:CreateRepository on resource: *",
        "Question_body":"<p>I am creating my own Docker image so that I can use my own models in AWS SageMaker. I sucessfully created a Docker image using command line inside the Jupyter Notebook in SageMaker ml.t2.medium instance using a customized Dockerfile:<\/p>\n\n<pre><code>REPOSITORY            TAG                 IMAGE ID            CREATED             SIZE\nsklearn               latest              01234212345        6 minutes ago       1.23GB\n<\/code><\/pre>\n\n<p>But when I run in Jupyter:<\/p>\n\n<pre><code>! aws ecr create-repository --repository-name sklearn\n<\/code><\/pre>\n\n<p>I get the following error:<\/p>\n\n<pre><code>An error occurred (AccessDeniedException) when calling the CreateRepository operation: User: arn:aws:sts::1234567:assumed-role\/AmazonSageMaker-ExecutionRole-12345\/SageMaker is not authorized to perform: ecr:CreateRepository on resource: *\n<\/code><\/pre>\n\n<p>I already set up SageMaker, EC2, EC2ContainerService permissions and the following policy for EC2Container but I still get the same error.<\/p>\n\n<pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"sagemaker:*\",\n        \"ec2:*\"\n      ],\n      \"Resource\": \"*\"\n    }\n  ]\n}\n<\/code><\/pre>\n\n<p>Any idea on how I can solve this issue?<\/p>\n\n<p>Thanks in advance.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_date":"2018-06-03 19:06:16.997000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2018-06-03 19:46:45.113000 UTC",
        "Question_score":1,
        "Question_tags":"amazon-web-services|docker|scikit-learn|dockerfile|amazon-sagemaker",
        "Question_view_count":4509,
        "Owner_creation_date":"2016-09-29 20:35:09.097000 UTC",
        "Owner_last_access_date":"2022-09-25 02:50:38.173000 UTC",
        "Owner_location":"Brazil",
        "Owner_reputation":4242,
        "Owner_up_votes":735,
        "Owner_down_votes":15,
        "Owner_views":421,
        "Answer_body":"<p>I solved the problem. We must set a permission at SageMaker Execution Role as following:<\/p>\n\n<pre><code>{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n    {\n        \"Effect\": \"Allow\",\n        \"Action\": [\n            \"ecr:*\"            ],\n        \"Resource\": \"*\"\n    }\n]}\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2018-06-04 15:31:01.857000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":5.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":60331084,
        "Question_title":"Where does Azure Machine Learning Service cache data?",
        "Question_body":"<p>I am looking to use Azure Machine Learning Services (the one with the new drag and drop feature; still in preview) in a new data science project. <\/p>\n\n<p>I have realised that I can preview the data when I connect a data set; I am able to do this using the option 'Dataset output' which is available as part of the dataset.<\/p>\n\n<p>To be able to see this data, the data needs to be cached some where. <\/p>\n\n<p>Can someone advise where this is cached? <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-02-21 01:53:42.010000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"azure|azure-machine-learning-service|data-caching",
        "Question_view_count":397,
        "Owner_creation_date":"2016-02-26 11:19:26.000000 UTC",
        "Owner_last_access_date":"2022-09-23 07:19:03.243000 UTC",
        "Owner_location":"Canberra, Australian Capital Territory, Australia",
        "Owner_reputation":214,
        "Owner_up_votes":21,
        "Owner_down_votes":2,
        "Owner_views":54,
        "Answer_body":"<p>Data is cached by default in a storage account that is created along with the the ML service workspace. It has the same name as the workspace plus some numbers. Inside the account there is a blobstore called <code>azureml-blobstore-{GUID}<\/code> Inside of that container your data is cached,  organized by runs.<\/p>\n\n<p>This data is made available to ML service as a <code>Datastore<\/code> that you can navigate to in the UI by clicking \"Datastores\" in the blade on the left-hand of the Studio.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/YVwPl.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/YVwPl.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-02-21 03:38:14.053000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":3.0,
        "Question_valid_tags":[
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":70877982,
        "Question_title":"How to format parameter of data type json in a aws cloudformation yaml template?",
        "Question_body":"<p>The yaml template <a href=\"https:\/\/docs.aws.amazon.com\/AWSCloudFormation\/latest\/UserGuide\/aws-properties-sagemaker-model-containerdefinition.html\" rel=\"nofollow noreferrer\">documentation<\/a> for the AWS Cloudformation AWS::SageMaker::Model ContainerDefinition specifies that &quot;Environment&quot; is of type Json. I can't work out how to submit json in my yaml template that does not cause a &quot;CREATE_FAILED    Internal Failure&quot; after running a deploy with the below command.<\/p>\n<pre><code>aws cloudformation deploy --stack-name test1 --template-file test-template-export.yml\n<\/code><\/pre>\n<p>test-template-export.yml<\/p>\n<pre><code>Description: Example yaml\n\nResources:\n  Model:\n    Type: AWS::SageMaker::Model\n    Properties:\n      Containers:\n      - ModelPackageName: arn:aws:sagemaker:us-east-1:123456789123:model-package\/name\/25\n      - Environment: '{&quot;SAGEMAKER_CONTAINER_LOG_LEVEL&quot;: &quot;20&quot;}'\n      ExecutionRoleArn: arn:aws:iam::123456789123:role\/service-role\/AmazonSageMakerServiceCatalogProductsUseRole\n<\/code><\/pre>\n<p>I have also tried the below formats as well and still no luck.<\/p>\n<pre><code>Containers:\n- ModelPackageName: arn:aws:sagemaker:us-east-1:123456789123:model-package\/name\/25\n  Environment: '{&quot;SAGEMAKER_CONTAINER_LOG_LEVEL&quot;: &quot;20&quot;}'\n<\/code><\/pre>\n<p>--<\/p>\n<pre><code>Containers:\n- ModelPackageName: arn:aws:sagemaker:us-east-1:123456789123:model-package\/name\/25\n- Environment: | \n         {\n            &quot;SAGEMAKER_CONTAINER_LOG_LEVEL&quot;: &quot;20&quot;\n          }\n<\/code><\/pre>\n<p>--<\/p>\n<pre><code>Containers:\n- ModelPackageName: arn:aws:sagemaker:us-east-1:123456789123:model-package\/name\/25\n- Environment:\n  - SAGEMAKER_CONTAINER_LOG_LEVEL: &quot;20&quot;\n<\/code><\/pre>\n<p>Running without Environment deploys fine.<\/p>\n<p>I have tried everything in <a href=\"https:\/\/stackoverflow.com\/questions\/39041209\/how-to-specify-json-formatted-string-in-cloudformation\">this answer.<\/a>\nHow do I format this Environment argument?<\/p>\n<p>My version of aws cli is &quot;aws-cli\/2.4.10 Python\/3.8.8&quot;<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-01-27 11:42:22.143000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"json|amazon-web-services|yaml|amazon-cloudformation|amazon-sagemaker",
        "Question_view_count":391,
        "Owner_creation_date":"2016-01-16 21:50:20.537000 UTC",
        "Owner_last_access_date":"2022-09-23 14:52:42.610000 UTC",
        "Owner_location":"UK",
        "Owner_reputation":311,
        "Owner_up_votes":67,
        "Owner_down_votes":0,
        "Owner_views":37,
        "Answer_body":"<p>Hi when you see json format think more dict.\nSo write it like this:<\/p>\n<pre><code>Containers:\n- ModelPackageName: arn:aws:sagemaker:us-east-1:123456789123:model-package\/name\/25\n  Environment:\n     SAGEMAKER_CONTAINER_LOG_LEVEL: 20\n<\/code><\/pre>\n<p>For IAM Policies the PolicyDocument is json type and this is how AWS do it in their exempel:<\/p>\n<pre><code>Type: 'AWS::IAM::Policy'\nProperties:\n  PolicyName: CFNUsers\n  PolicyDocument:\n    Version: &quot;2012-10-17&quot;\n    Statement:\n      - Effect: Allow\n        Action:\n          - 'cloudformation:Describe*'\n          - 'cloudformation:List*'\n          - 'cloudformation:Get*'\n        Resource: '*'\n  Groups:\n    - !Ref CFNUserGroup\n<\/code><\/pre>",
        "Answer_comment_count":3.0,
        "Answer_creation_date":"2022-01-27 11:53:22.370000 UTC",
        "Answer_last_edit_date":"2022-01-27 15:24:53.190000 UTC",
        "Answer_score":2.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":57978333,
        "Question_title":"What should I do when H2O AutoML returns \"H2OFrame is empty\"?",
        "Question_body":"<p>While training a job on a SageMaker instance using H2o AutoML a message \"This H2OFrame is empty\" has come up after running the code, what should I do to fix the problem?<\/p>\n\n<pre><code>\/opt\/ml\/input\/config\/hyperparameters.json\nAll Parameters:\n{'nfolds': '5', 'training': \"{'classification': 'true', 'target': 'y'}\", 'max_runtime_secs': '3600'}\n\/opt\/ml\/input\/config\/resourceconfig.json\nAll Resources:\n{'current_host': 'algo-1', 'hosts': ['algo-1'], 'network_interface_name': 'eth0'}\nWaiting until DNS resolves: 1\n10.0.182.83\nStarting up H2O-3\nCreating Connection to H2O-3\nAttempt 0: H2O-3 not running yet...\nConnecting to H2O server at http:\/\/127.0.0.1:54321... successful.\n-------------------------- ----------------------------------------\n\n-------------------------- ----------------------------------------\nBeginning Model Training\nParse progress: |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100%\nClassification - If you want to do a regression instead, set \"classification\":\"false\" in \"training\" params, inhyperparamters.json\nConverting specified columns to categorical values:\n[]\nAutoML progress: |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100%\nThis H2OFrame is empty.\nException during training: Argument `model` should be a ModelBase, got NoneType None\nTraceback (most recent call last):\nFile \"\/opt\/program\/train\", line 138, in _train_model\nh2o.save_model(aml.leader, path=model_path)\nFile \"\/root\/.local\/lib\/python3.7\/site-packages\/h2o\/h2o.py\", line 969, in save_model\nassert_is_type(model, ModelBase)\nFile \"\/root\/.local\/lib\/python3.7\/site-packages\/h2o\/utils\/typechecks.py\", line 457, in assert_is_type\nskip_frames=skip_frames)\nh2o.exceptions.H2OTypeError: Argument `model` should be a ModelBase, got NoneType None\nH2O session _sid_8aba closed.\n<\/code><\/pre>\n\n<p>I'm wondering if it's a problem because of the max_runtime_secs, my data has around 500 rows and 250000 columns.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_date":"2019-09-17 16:21:12.183000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2019-09-17 16:44:04.663000 UTC",
        "Question_score":0,
        "Question_tags":"h2o|amazon-sagemaker|automl",
        "Question_view_count":517,
        "Owner_creation_date":"2017-10-26 10:07:59.113000 UTC",
        "Owner_last_access_date":"2021-09-21 19:53:06.417000 UTC",
        "Owner_location":"Belo Horizonte, MG, Brasil",
        "Owner_reputation":97,
        "Owner_up_votes":15,
        "Owner_down_votes":0,
        "Owner_views":25,
        "Answer_body":"<p>thanks @Marcel Mendes Reis for following up on your solution in the comments. I will repost here for others to easily find:<\/p>\n\n<p><em>I realized the issue was due to the max_runtime. When I trained the model with more time I didn't have the problem.<\/em> <\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2019-09-19 15:16:28.310000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":62778020,
        "Question_title":"Embarrassingly parallel hyperparameter search via Azure + DataBricks + MLFlow",
        "Question_body":"<p>Conceptual question.  My company is pushing Azure + DataBricks.  I am trying to understand where this can take us.<\/p>\n<p>I am porting some work I've done locally to the Azure + Databricks platform.  I want to run an experiment with a large number of hyperparameter combinations using Azure + Databricks + MLfLow.  I am using PyTorch to implement my models.<\/p>\n<p>I have a cluster with 8 nodes.  I want to kick off the parameter search across all of the nodes in an embarrassingly parallel manner (one run per node, running independently).  Is this as simple as creating a MLflow project and then using the mlflow.projects.run command for each hyperparameter combination and Databricks + MLflow will take care of the rest?<\/p>\n<p>Is this technology capable of this?  I'm looking for some references I could use to make this happen.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2020-07-07 14:52:06.057000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-07-07 15:12:05.200000 UTC",
        "Question_score":0,
        "Question_tags":"databricks|azure-databricks|mlflow",
        "Question_view_count":262,
        "Owner_creation_date":"2012-02-09 20:11:42.350000 UTC",
        "Owner_last_access_date":"2022-09-16 14:37:11.437000 UTC",
        "Owner_location":"Sioux City, IA",
        "Owner_reputation":325,
        "Owner_up_votes":254,
        "Owner_down_votes":4,
        "Owner_views":51,
        "Answer_body":"<p>The short answer is yes, it's possible, but won't be exactly as easy as running a single mlflow command. You can paralelize single-node workflows using spark Python UDFs, a good example of this is this <a href=\"https:\/\/pages.databricks.com\/rs\/094-YMS-629\/images\/Fine-Grained-Time-Series-Forecasting.html?_ga=2.64430959.1760852900.1593769579-972789996.1561118598\" rel=\"nofollow noreferrer\">notebook<\/a><\/p>\n<p>I'm not sure if this will work with pytorch, but there is hyperopt library that lets you parallelize search across parameters using Spark - it's integrated with mlflow and available in databricks ML runtime. I've been using it only with scikit-learn, but it may be <a href=\"https:\/\/docs.databricks.com\/applications\/machine-learning\/automl\/hyperopt\/hyperopt-model-selection.html\" rel=\"nofollow noreferrer\">worth checking out<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-07-17 11:52:54.773000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "mlflow"
        ]
    },
    {
        "Question_id":63024900,
        "Question_title":"How to train and deploy model in script mode on Sagemaker without using jupyter notebook instance (serverless)?",
        "Question_body":"<p>I have been using a jupyter notebook instance to spin up a training job (on separate instance) and deploy the endpoint (on another instance). I am using sagemaker tensorflow APIs for this as shown below:<\/p>\n<pre><code># create Tensorflow object and provide and entry point script\ntf_estimator = TensorFlow(entry_point='tf-train.py', role='SageMakerRole',\n                      train_instance_count=1, train_instance_type='ml.p2.xlarge',\n                      framework_version='1.12', py_version='py3')\n\n# train model on data on s3 and save model artifacts to s3\ntf_estimator.fit('s3:\/\/bucket\/path\/to\/training\/data')\n\n# deploy model on another instance using checkpoints saved on S3\npredictor = estimator.deploy(initial_instance_count=1,\n                         instance_type='ml.c5.xlarge',\n                         endpoint_type='tensorflow-serving')\n<\/code><\/pre>\n<p>I have been doing all of these steps through a jupyter notebook instance. What AWS services I can use to get rid off the dependency of jupyter notebook instance and automate these tasks of training and deploying the model in serverless fashion?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-07-22 00:19:39.513000 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":3,
        "Question_tags":"amazon-web-services|tensorflow|jupyter-notebook|amazon-sagemaker",
        "Question_view_count":758,
        "Owner_creation_date":"2013-09-04 04:27:22.847000 UTC",
        "Owner_last_access_date":"2022-09-22 00:59:12.557000 UTC",
        "Owner_location":"Pune, India",
        "Owner_reputation":4616,
        "Owner_up_votes":751,
        "Owner_down_votes":5,
        "Owner_views":592,
        "Answer_body":"<p>I recommend <code>AWS Step Functions<\/code>.  Been using it to schedule <code>SageMaker Batch Transform<\/code> and preprocessing jobs since it integrates with <code>CloudWatch<\/code> event rules.  It can also train models, perform hpo tuning, and integrates with <code>lambda<\/code>.  There is a SageMaker\/Step Functions SDK as well as you can use Step Functions directly by creating state machines. Some examples and documentation:<\/p>\n<p><a href=\"https:\/\/aws.amazon.com\/about-aws\/whats-new\/2019\/11\/introducing-aws-step-functions-data-science-sdk-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/about-aws\/whats-new\/2019\/11\/introducing-aws-step-functions-data-science-sdk-amazon-sagemaker\/<\/a><\/p>\n<p><a href=\"https:\/\/docs.aws.amazon.com\/step-functions\/latest\/dg\/connect-sagemaker.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/step-functions\/latest\/dg\/connect-sagemaker.html<\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2020-07-24 21:06:01.857000 UTC",
        "Answer_last_edit_date":"2020-07-24 21:17:13.193000 UTC",
        "Answer_score":2.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":57854136,
        "Question_title":"Registering and downloading a fastText .bin model fails with Azure Machine Learning Service",
        "Question_body":"<p>I have a simple RegisterModel.py script that uses the Azure ML Service SDK to register a fastText .bin model. This completes successfully and I can see the model in the Azure Portal UI (I cannot see what model files are in it). I then want to download the model (DownloadModel.py) and use it (for testing purposes), however it throws an error on the <strong>model.download<\/strong> method (<em>tarfile.ReadError: file could not be opened successfully<\/em>) and makes a 0 byte rjtestmodel8.tar.gz file.<\/p>\n\n<p>I then use the Azure Portal and Add Model and select the same bin model file and it uploads fine. Downloading it with the download.py script below works fine, so I am assuming something is not correct with the Register script.<\/p>\n\n<p>Here are the 2 scripts and the stacktrace - let me know if you can see anything wrong:<\/p>\n\n<p><strong>RegisterModel.py<\/strong><\/p>\n\n<pre><code>import azureml.core\nfrom azureml.core import Workspace, Model\nws = Workspace.from_config()\nmodel = Model.register(workspace=ws,\n                       model_name='rjSDKmodel10',\n                       model_path='riskModel.bin')\n<\/code><\/pre>\n\n<p><strong>DownloadModel.py<\/strong><\/p>\n\n<pre><code># Works when downloading the UI Uploaded .bin file, but not the SDK registered .bin file\nimport os\nimport azureml.core\nfrom azureml.core import Workspace, Model\n\nws = Workspace.from_config()\nmodel = Model(workspace=ws, name='rjSDKmodel10')\nmodel.download(target_dir=os.getcwd(), exist_ok=True)\n<\/code><\/pre>\n\n<p><strong>Stacktrace<\/strong><\/p>\n\n<pre><code>Traceback (most recent call last):\n  File \"...\\.vscode\\extensions\\ms-python.python-2019.9.34474\\pythonFiles\\ptvsd_launcher.py\", line 43, in &lt;module&gt;\n    main(ptvsdArgs)\n  File \"...\\.vscode\\extensions\\ms-python.python-2019.9.34474\\pythonFiles\\lib\\python\\ptvsd\\__main__.py\", line 432, in main\n    run()\n  File \"...\\.vscode\\extensions\\ms-python.python-2019.9.34474\\pythonFiles\\lib\\python\\ptvsd\\__main__.py\", line 316, in run_file\n    runpy.run_path(target, run_name='__main__')\n  File \"...\\.conda\\envs\\DoC\\lib\\runpy.py\", line 263, in run_path\n    pkg_name=pkg_name, script_name=fname)\n  File \"...\\.conda\\envs\\DoC\\lib\\runpy.py\", line 96, in _run_module_code\n    mod_name, mod_spec, pkg_name, script_name)\n  File \"...\\.conda\\envs\\DoC\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"...\\\\DownloadModel.py\", line 21, in &lt;module&gt;\n    model.download(target_dir=os.getcwd(), exist_ok=True)\n  File \"...\\.conda\\envs\\DoC\\lib\\site-packages\\azureml\\core\\model.py\", line 712, in download\n    file_paths = self._download_model_files(sas_to_relative_download_path, target_dir, exist_ok)\n  File \"...\\.conda\\envs\\DoC\\lib\\site-packages\\azureml\\core\\model.py\", line 658, in _download_model_files\n    file_paths = self._handle_packed_model_file(tar_path, target_dir, exist_ok)\n  File \"...\\.conda\\envs\\DoC\\lib\\site-packages\\azureml\\core\\model.py\", line 670, in _handle_packed_model_file\n    with tarfile.open(tar_path) as tar:\n  File \"...\\.conda\\envs\\DoC\\lib\\tarfile.py\", line 1578, in open\n    raise ReadError(\"file could not be opened successfully\")\ntarfile.ReadError: file could not be opened successfully\n<\/code><\/pre>\n\n<p><strong>Environment<\/strong><\/p>\n\n<ul>\n<li>riskModel.bin is 6 megs<\/li>\n<li>AMLS 1.0.60<\/li>\n<li>Python 3.7<\/li>\n<li>Working locally with Visual Code<\/li>\n<\/ul>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2019-09-09 12:31:46.497000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2019-09-09 20:28:30.630000 UTC",
        "Question_score":0,
        "Question_tags":"python-3.x|fasttext|azure-machine-learning-service",
        "Question_view_count":281,
        "Owner_creation_date":"2009-10-21 01:51:25.500000 UTC",
        "Owner_last_access_date":"2022-09-13 05:24:36.847000 UTC",
        "Owner_location":"Sydney, Australia",
        "Owner_reputation":4947,
        "Owner_up_votes":277,
        "Owner_down_votes":8,
        "Owner_views":531,
        "Answer_body":"<p>The Azure Machine Learning service SDK has a bug with how it interacts with Azure Storage, which causes it to upload corrupted files if it has to retry uploading. <\/p>\n\n<p>A couple workarounds:<\/p>\n\n<ol>\n<li>The bug was introduced in 1.0.60 release. If you downgrade to AzureML-SDK 1.0.55, the code should fail when there are issue uploading instead of silently corrupting data.<\/li>\n<li>It's possible that the retry is being triggered by the low timeout values that the AzureML-SDK defaults to. You could investigate changing the timeout in <code>site-packages\/azureml\/_restclient\/artifacts_client.py<\/code><\/li>\n<\/ol>\n\n<p>This bug should be fixed in the next release of the AzureML-SDK.<\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_date":"2019-09-09 18:37:20.467000 UTC",
        "Answer_last_edit_date":"2019-09-09 18:45:11.300000 UTC",
        "Answer_score":2.0,
        "Question_valid_tags":[
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":51533650,
        "Question_title":"No space left on device in Sagemaker model training",
        "Question_body":"<p>I'm using custom algorithm running shipped with Docker image on p2 instance with AWS Sagemaker (a bit similar to <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/scikit_bring_your_own\/scikit_bring_your_own.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/scikit_bring_your_own\/scikit_bring_your_own.ipynb<\/a>)<\/p>\n\n<p>At the end of training process, I try to write down my model to output directory, that is mounted via Sagemaker (like in tutorial), like this:<\/p>\n\n<pre><code>model_path = \"\/opt\/ml\/model\"\nmodel.save(os.path.join(model_path, 'model.h5'))\n<\/code><\/pre>\n\n<p>Unluckily, apparently the model gets too big with time and I get the\nfollowing error:<\/p>\n\n<blockquote>\n  <p>RuntimeError: Problems closing file (file write failed: time = Thu Jul\n  26 00:24:48 2018<\/p>\n  \n  <p>00:24:49 , filename = 'model.h5', file descriptor = 22, errno = 28,\n  error message = 'No space left on device', buf = 0x1a41d7d0, total\n  write[...]<\/p>\n<\/blockquote>\n\n<p>So all my hours of GPU time are wasted. How can I prevent this from happening again? Does anyone know what is the size limit for model that I store on Sagemaker\/mounted directories?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_date":"2018-07-26 07:51:03.817000 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"amazon-web-services|keras|amazon-sagemaker",
        "Question_view_count":2721,
        "Owner_creation_date":"2013-04-19 21:52:19.740000 UTC",
        "Owner_last_access_date":"2022-09-21 08:47:03.060000 UTC",
        "Owner_location":null,
        "Owner_reputation":8057,
        "Owner_up_votes":1123,
        "Owner_down_votes":72,
        "Owner_views":491,
        "Answer_body":"<p>When you train a model with <code>Estimators<\/code>, it <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/latest\/estimators.html\" rel=\"nofollow noreferrer\">defaults to 30 GB of storage<\/a>, which may not be enough. You can use the <code>train_volume_size<\/code> param on the constructor to increase this value. Try with a large-ish number (like 100GB) and see how big your model is. In subsequent jobs, you can tune down the value to something closer to what you actually need.<\/p>\n\n<p>Storage costs <a href=\"https:\/\/aws.amazon.com\/sagemaker\/pricing\/\" rel=\"nofollow noreferrer\">$0.14 per GB-month of provisioned storage<\/a>. Partial usage is prorated, so giving yourself some extra room is a cheap insurance policy against running out of storage.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2018-10-15 19:27:34.853000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":64139290,
        "Question_title":"How can I mark an Azure Dataset as a time series dataset reading from a parquet folder with date partitions?",
        "Question_body":"<p>I would like to create a Time series dataset from a folder that contains parquet files this way:<\/p>\n<ul>\n<li>timestamp=2018-01-06<\/li>\n<li>timestamp=2018-01-07<\/li>\n<\/ul>\n<p>How can I make Azure Dataset, through the GUI, recognises the timestamp partition as a date and mark my dataset as a time series dataset?<\/p>\n<p>It is supposed to be automatic, but it doesn't work.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2020-09-30 14:04:18.457000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"parquet|azure-machine-learning-studio",
        "Question_view_count":109,
        "Owner_creation_date":"2015-02-11 07:34:40.283000 UTC",
        "Owner_last_access_date":"2022-09-23 14:32:37.963000 UTC",
        "Owner_location":"Lyon, France",
        "Owner_reputation":457,
        "Owner_up_votes":19,
        "Owner_down_votes":2,
        "Owner_views":125,
        "Answer_body":"<p>Thanks for reaching out to us.<\/p>\n<p>In Azure Machine Learning Studio, you would need to setup partition format similar to python SDK, as follows, assuming your data path is &quot;timeseries\/timestamp=2020-01-01\/data.parquet&quot;:\n<a href=\"https:\/\/i.stack.imgur.com\/HwYfF.png\" rel=\"nofollow noreferrer\">Set up partition format when creating time series dataset<\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2020-10-05 17:58:05.457000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":3.0,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":57010184,
        "Question_title":"XGBoost Model in AWS-Sagemaker Fails with no error message",
        "Question_body":"<p>I'm trying to get a model using the XGBoost classifier in AWS-Sagemaker.  I'm following the abalone example, but when I run it to build the training job it states InProgress 3 times and then just states Failed.  Where do I go to find why it failed?  <\/p>\n\n<p>I've double checked the parameters and made sure the input and output files and directories in S3 were correct.  I know there is permission to read and write because when setting up the data for train\/validate\/test I read and write to S3 with no problems.<\/p>\n\n<pre><code>print(status)\nwhile status !='Completed' and status!='Failed':\n    time.sleep(60)\n    status = client.describe_training_job(TrainingJobName=job_name)['TrainingJobStatus']\n    print(status)\n<\/code><\/pre>\n\n<p>That is the code where the print statements come from.  Is there something I can add to receive a better error message?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_date":"2019-07-12 15:44:33.023000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"xgboost|amazon-sagemaker",
        "Question_view_count":204,
        "Owner_creation_date":"2015-05-29 03:50:33.557000 UTC",
        "Owner_last_access_date":"2019-12-30 21:07:55.380000 UTC",
        "Owner_location":"Abilene, TX, USA",
        "Owner_reputation":1,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Answer_body":"<p>The problem occurred was that the file sent for predictions was csv but the XGBoost settings were set to receive libsvm.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2019-07-30 13:44:54.317000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":66978458,
        "Question_title":"In Azure ML timeseries forecasting, Model Explanations, how do I upload actual values?",
        "Question_body":"<p>Using Azure ML through the web UI. I'm doing a timeseries forecasting automl training job. In the explanations tab for a model, how can I upload the actual data for the forecast period to compare. See the red circled box in the image below.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/hBhzR.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/hBhzR.png\" alt=\"Example Explanation GUI\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-04-07 01:36:26.410000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"azure-machine-learning-service",
        "Question_view_count":53,
        "Owner_creation_date":"2011-12-20 03:17:46.393000 UTC",
        "Owner_last_access_date":"2022-09-15 01:17:57.817000 UTC",
        "Owner_location":"Christchurch, New Zealand",
        "Owner_reputation":1306,
        "Owner_up_votes":49,
        "Owner_down_votes":5,
        "Owner_views":81,
        "Answer_body":"<p>We are currently developing test-set ingestion in the UI. However, currently there is no way to upload test data through the UI to populate these graphs. This experience can only be accessed by kicking off an explanation through the SDK with the test data. We refer to this as &quot;Interpretability at inference time&quot; and have some documentation on how to do this here: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-machine-learning-interpretability-aml#interpretability-at-inference-time\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-machine-learning-interpretability-aml#interpretability-at-inference-time<\/a><\/p>\n<p>Test-set ingestion is scoped to land for private preview before end of June. Let's keep in touch to ensure you get early access here.<\/p>\n<p>Thanks,\nSabina<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-04-07 22:54:26.050000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":54629890,
        "Question_title":"Invoke aws sagemaker endpoint",
        "Question_body":"<p>I have some data in S3 and I want to create a lambda function to predict the output with my deployed aws sagemaker endpoint then I put the outputs in S3 again. Is it necessary in this case to create an api gateway like decribed in this <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda\/\" rel=\"noreferrer\">link<\/a> ? and in the lambda function what I have to put. I expect to put (where to find the data, how to invoke the endpoint, where to put the data) <\/p>\n\n<pre><code>import boto3\nimport io\nimport json\nimport csv\nimport os\n\n\nclient = boto3.client('s3') #low-level functional API\n\nresource = boto3.resource('s3') #high-level object-oriented API\nmy_bucket = resource.Bucket('demo-scikit-byo-iris') #subsitute this for your s3 bucket name. \n\nobj = client.get_object(Bucket='demo-scikit-byo-iris', Key='foo.csv')\nlines= obj['Body'].read().decode('utf-8').splitlines()\nreader = csv.reader(lines)\n\nimport io\nfile = io.StringIO(lines)\n\n# grab environment variables\nruntime= boto3.client('runtime.sagemaker')\n\nresponse = runtime.invoke_endpoint(\n    EndpointName= 'nilm2',\n    Body = file.getvalue(),\n    ContentType='*\/*',\n    Accept = 'Accept')\n\noutput = response['Body'].read().decode('utf-8')\n<\/code><\/pre>\n\n<p>my data is a csv file of 2 columns of floats with no headers, the problem is that lines return a list of strings(each row is an element of this list:['11.55,65.23', '55.68,69.56'...]) the invoke work well but the response is also a string: output = '65.23\\n,65.23\\n,22.56\\n,...' <\/p>\n\n<p>So how to save this output to S3 as a csv file <\/p>\n\n<p>Thanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2019-02-11 11:44:59.183000 UTC",
        "Question_favorite_count":4.0,
        "Question_last_edit_date":null,
        "Question_score":5,
        "Question_tags":"python|amazon-s3|amazon-sagemaker",
        "Question_view_count":3146,
        "Owner_creation_date":"2017-08-06 09:14:59.810000 UTC",
        "Owner_last_access_date":"2019-11-15 11:59:17.507000 UTC",
        "Owner_location":"Tunis, Tunisia",
        "Owner_reputation":109,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":14,
        "Answer_body":"<p>If your Lambda function is scheduled, then you won't need an API Gateway. But if the predict action will be triggered by a user, by an application, for example, you will need.<\/p>\n\n<p>When you call the invoke endpoint, actually you are calling a SageMaker endpoint, which is not the same as an API Gateway endpoint. <\/p>\n\n<p>A common architecture with SageMaker is:<\/p>\n\n<ol>\n<li>API Gateway with receives a request then calls an authorizer, then\ninvoke your Lambda; <\/li>\n<li>A Lambda with does some parsing in your input data, then calls your SageMaker prediction endpoint, then, handles the result and returns to your application.<\/li>\n<\/ol>\n\n<p>By the situation you describe, I can't say if your task is some academic stuff or a production one.<\/p>\n\n<p>So, how you can save the data as a CSV file from your Lambda? <\/p>\n\n<p>I believe you can just parse the output, then just upload the file to S3. Here you will parse manually or with a lib, with boto3 you can upload the file. The output of your model depends on your implementation on SageMaker image. So, if you need the response data in another format, maybe you will need to use a <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/scikit_bring_your_own\" rel=\"nofollow noreferrer\">custom image<\/a>. I normally use a custom image, which I can define how I want to handle my data on requests\/responses.<\/p>\n\n<p>In terms of a production task, I certainly recommend you check Batch transform jobs from SageMaker. You can provide an input file (the S3 path) and also a destination file (another S3 path). The SageMaker will run the batch predictions and will persist a file with the results. Also, you won't need to deploy your model to an endpoint, when this job run, will create an instance of your endpoint, download your data to predict, do the predictions, upload the output, and shut down the instance. You only need a trained model.<\/p>\n\n<p>Here some info about Batch transform jobs:<\/p>\n\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/how-it-works-batch.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/how-it-works-batch.html<\/a><\/p>\n\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-batch-transform.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-batch-transform.html<\/a><\/p>\n\n<p>I hope it helps, let me know if need more info.<\/p>\n\n<p>Regards.<\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_date":"2019-02-13 01:44:56.023000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":4.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":65882686,
        "Question_title":"AWS Sagemaker processing Job automatically created?",
        "Question_body":"<p>I haven't used sagemaker for a while and today I started a training job (with the same old settings I always used before), but this time I noticed that a processing job has been automatically created and it's running while my training job run (I presume for debugging purpose).\nI'm sure that this is the first time that it happens.. Is that a new feature introduced by sagemaker? I didn't find any related in documentation, but it's important to know because I don't want extra costs..<\/p>\n<p>This is the image used by the processing job, with a instance type of <code>ml.m5.2xlarge<\/code> which I didn't set anywhere..<\/p>\n<blockquote>\n<p>929884845733.dkr.ecr.eu-west-1.amazonaws.com\/sagemaker-debugger-rules:latest<\/p>\n<\/blockquote>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-01-25 10:11:34.177000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-01-25 10:39:11.463000 UTC",
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":301,
        "Owner_creation_date":"2014-11-18 21:32:30.293000 UTC",
        "Owner_last_access_date":"2022-09-24 17:06:59.437000 UTC",
        "Owner_location":"Jesi, Italy",
        "Owner_reputation":2302,
        "Owner_up_votes":51,
        "Owner_down_votes":4,
        "Owner_views":227,
        "Answer_body":"<p>I can answer my question.. it seems to be a new feature as highlighted <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/use-debugger-built-in-rules.html\" rel=\"nofollow noreferrer\">here<\/a>. You can turn it off as suggested in the doc:<\/p>\n<pre><code>To disable both monitoring and profiling, include the disable_profiler parameter to your estimator and set it to True. \n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-01-25 11:12:07.773000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":70268372,
        "Question_title":"How to adjust feature importance in Azure AutoML",
        "Question_body":"<p>I am hoping to have some <strong>low code model<\/strong> using Azure AutoML, which is really just going to the AutoML tab, running a classification experiment with my dataset, after it's done, I deploy the best selected model.<\/p>\n<p>The model kinda works (meaning, I publish the endpoint and then I do some manual validation, seems accurate), however, I am not confident enough, because when I am looking at the explanation, I can see something like this:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/qM51x.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/qM51x.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>4 top features are not really closely important. The most &quot;important&quot; one is really not the one I prefer it to use. I am hoping it will use the <code>Title<\/code> feature more.<\/p>\n<p>Is there such a thing I can adjust the importance of individual features, like ranking all features before it starts the experiment?<\/p>\n<p>I would love to do more reading, but I only found this:<\/p>\n<p><a href=\"https:\/\/stackoverflow.com\/questions\/52484233\/increase-feature-importance\">Increase feature importance<\/a><\/p>\n<p>The only answer seems to be about how to measure if a feature is important.<\/p>\n<p>Hence, does it mean, if I want to customize the experiment, such as selecting which features to &quot;focus&quot;, I should learn how to use the &quot;designer&quot; part in Azure ML? Or is it something I can't do, even with the designer. I guess my confusion is, with ML being such a big topic, I am looking for a direction of learning, in this case of what I am having, so I can improve my current model.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-12-07 23:55:26.880000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-12-08 00:57:18.330000 UTC",
        "Question_score":2,
        "Question_tags":"machine-learning|azure-machine-learning-studio|azure-machine-learning-service|azure-auto-ml",
        "Question_view_count":119,
        "Owner_creation_date":"2011-02-01 14:45:49.840000 UTC",
        "Owner_last_access_date":"2022-09-22 01:11:43.810000 UTC",
        "Owner_location":"Seattle, WA",
        "Owner_reputation":1021,
        "Owner_up_votes":79,
        "Owner_down_votes":7,
        "Owner_views":138,
        "Answer_body":"<p>Here is <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-automated-ml#customize-featurization\" rel=\"nofollow noreferrer\">link<\/a> to the document for feature customization.<\/p>\n<p>Using the SDK you can specify &quot;feauturization&quot;: 'auto' \/ 'off' \/ 'FeaturizationConfig' in your <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-train-automl-client\/azureml.train.automl.automlconfig.automlconfig?view=azure-ml-py\" rel=\"nofollow noreferrer\">AutoMLConfig<\/a> object. Learn more about <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-configure-auto-features\" rel=\"nofollow noreferrer\">enabling featurization<\/a>.<\/p>\n<p>Automated ML tries out different ML models that have different settings which control for overfitting.  Automated ML will pick which overfitting parameter configuration is best based on the best score (e.g. accuracy) it gets from hold-out data.  The kind of overfitting settings these models has includes:<\/p>\n<ul>\n<li>Explicitly penalizing overly-complex models in the loss function that the ML model is optimizing<\/li>\n<li>Limiting model complexity before training, for example by limiting the size of trees in an ensemble tree learning model (e.g. gradient boosting trees or random forest)<\/li>\n<\/ul>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-manage-ml-pitfalls\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-manage-ml-pitfalls<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-01-03 11:55:36.730000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "azure-machine-learning-studio",
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":71611419,
        "Question_title":"Mixing shell variables and python variables in IPython '!command'",
        "Question_body":"<p>Trying to figure out whether this behaviour on IPython (v7.12.0, on Amazon SageMaker) is a bug or I'm missing some proper way \/ documented constraint...<\/p>\n<p>Say I have some Python variables like:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>NODE_VER = &quot;v16.14.2&quot;\nNODE_DISTRO = &quot;linux-x64&quot;\n<\/code><\/pre>\n<p>These commands both work fine in a notebook:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>!echo $PATH\n# Shows **contents of system path**\n!echo \/usr\/local\/lib\/nodejs\/node-{NODE_VER}-{NODE_DISTRO}\/bin:\n# Shows \/usr\/local\/lib\/nodejs\/node-v16.14.2-linux-x64\/bin\n<\/code><\/pre>\n<p>...But this does not:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>!echo \/usr\/local\/lib\/nodejs\/node-{NODE_VER}-{NODE_DISTRO}\/bin:$PATH\n# Shows:\n# \/usr\/local\/lib\/nodejs\/node-{NODE_VER}-{NODE_DISTRO}\/bin:**contents of system path**\n<\/code><\/pre>\n<p>I've tried a couple of combinations of e.g. using <code>$NODE_VER<\/code> syntax instead (which produces <code>node--\/<\/code> instead of <code>node-{NODE_VER}-{NODE_DISTRO}\/<\/code>, but seems like any combination using both shell variables (PATH) and Python variables (NODE_VER\/NODE_DISTRO) fails.<\/p>\n<p>Can anybody help me understand why and how to work around it?<\/p>\n<p>My end goal, as you might have guessed already, is to actually add this folder to the PATH rather than just echoing it - something like:<\/p>\n<pre><code>!export PATH=\/usr\/local\/lib\/nodejs\/node-{NODE_VER}-{NODE_DISTRO}\/bin:$PATH\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2022-03-25 02:12:04.543000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"jupyter-notebook|ipython|jupyter-lab|amazon-sagemaker",
        "Question_view_count":160,
        "Owner_creation_date":"2020-04-19 07:33:10.603000 UTC",
        "Owner_last_access_date":"2022-09-20 07:39:49.537000 UTC",
        "Owner_location":null,
        "Owner_reputation":473,
        "Owner_up_votes":26,
        "Owner_down_votes":0,
        "Owner_views":37,
        "Answer_body":"<p><a href=\"https:\/\/stackoverflow.com\/questions\/69194172\/how-to-reference-both-a-python-and-environment-variable-in-jupyter-bash-magic\">How to reference both a python and environment variable in jupyter bash magic?<\/a><\/p>\n<p>Try<\/p>\n<pre><code>!echo \/usr\/local\/lib\/nodejs\/node-{NODE_VER}-{NODE_DISTRO}\/bin:$$PATH\n<\/code><\/pre>\n<p><code>$$PATH<\/code> forces it to use the system variable rather than try to find a Python\/local one.<\/p>\n<p>Various examples:<\/p>\n<pre><code>In [130]: foo = 'foo*.txt'\nIn [131]: HOME = 'myvar'\nIn [132]: !echo $foo\nfoo1.txt foobar0.txt foobar2.txt foobar3.txt foo.txt\nIn [133]: !echo $foo $HOME\nfoo1.txt foobar0.txt foobar2.txt foobar3.txt foo.txt myvar\nIn [134]: !echo $foo $$HOME\nfoo1.txt foobar0.txt foobar2.txt foobar3.txt foo.txt \/home\/paul\nIn [135]: !echo $foo $PWD\n\/home\/paul\/mypy\nIn [136]: !echo $foo $$PWD\nfoo1.txt foobar0.txt foobar2.txt foobar3.txt foo.txt \/home\/paul\/mypy\nIn [137]: !echo {foo} $PWD\n{foo} \/home\/paul\/mypy\nIn [138]: !echo {foo} $$PWD\nfoo1.txt foobar0.txt foobar2.txt foobar3.txt foo.txt \/home\/paul\/mypy\n<\/code><\/pre>\n<p>Any variable not locally defined forces the behavior you see:<\/p>\n<pre><code>In [139]: !echo $abc\n\nIn [140]: !echo {foo} $abc\n{foo}\n<\/code><\/pre>\n<p>It may put the substitution in a <code>try\/except<\/code> block, and &quot;give up&quot; if there's any <code>NameError<\/code>.<\/p>\n<p>This substitution can occur in most of the magics, not just <code>!<\/code>.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2022-03-25 04:55:37.557000 UTC",
        "Answer_last_edit_date":"2022-03-25 07:08:28.500000 UTC",
        "Answer_score":3.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":57243583,
        "Question_title":"Sagemaker KMeans Built-In - List of files csv as input",
        "Question_body":"<p>I Want to use <strong>Sagemaker KMeans BuilIn Algorithm<\/strong> in one of my applications. I have a large CSV file in S3 (raw data) that I split into several parts to be easy to clean. Before I had cleaned, I tried to use it as the input of Kmeans to perform the training job but It doesn't work.<\/p>\n\n<p>My manifest file:<\/p>\n\n<pre><code>[\n    {\"prefix\": \"s3:\/\/&lt;BUCKET_NAME&gt;\/kmeans_data\/KMeans-2019-28-07-13-40-00-001\/\"}, \n    \"file1.csv\", \n    \"file2.csv\"\n]\n<\/code><\/pre>\n\n<p>The error I've got:<\/p>\n\n<pre><code>Failure reason: ClientError: Unable to read data channel 'train'. Requested content-type is 'application\/x-recordio-protobuf'. Please verify the data matches the requested content-type. (caused by MXNetError) Caused by: [16:47:31] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsCppLibs\/AIAlgorithmsCppLibs-2.0.1620.0\/AL2012\/generic-flavor\/src\/src\/aialgs\/io\/iterator_base.cpp:100: (Input Error) The header of the MXNet RecordIO record at position 0 in the dataset does not start with a valid magic number. Stack trace returned 10 entries: [bt] (0) \/opt\/amazon\/lib\/libaialgs.so(+0xb1f0) [0x7fb5674c31f0] [bt] (1) \/opt\/amazon\/lib\/libaialgs.so(+0xb54a) [0x7fb5674c354a] [bt] (2) \/opt\/amazon\/lib\/libaialgs.so(aialgs::iterator_base::Next()+0x4a6) [0x7fb5674cc436] [bt] (3) \/opt\/amazon\/lib\/libmxnet.so(MXDataIterNext+0x21) [0x7fb54ecbcdb1] [bt] (4) \/opt\/amazon\/python2.7\/lib\/python2.7\/lib-dynload\/_ctypes.so(ffi_call_unix64+0x4c) [0x7fb567a1e858] [bt] (5) \/opt\/amazon\/python2.7\/lib\/python2.7\/lib-dynload\/_ctypes.so(ffi_call+0x15f) [0x7fb567a1d95f\n<\/code><\/pre>\n\n<p>My question is: It's possible to use multiple CSV files as input in Sagemaker KMeans BuilIn Algorithm only in GUI? If it's possible, How should I format my manifest?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-07-28 17:55:14.917000 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":"2019-07-28 19:36:44.717000 UTC",
        "Question_score":3,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":522,
        "Owner_creation_date":"2016-05-27 23:31:32.937000 UTC",
        "Owner_last_access_date":"2022-07-18 14:07:45.653000 UTC",
        "Owner_location":"Rio de Janeiro, State of Rio de Janeiro, Brazil",
        "Owner_reputation":2243,
        "Owner_up_votes":497,
        "Owner_down_votes":32,
        "Owner_views":148,
        "Answer_body":"<p>the manifest looks fine, but based on the error message, it looks like you haven't set the right data format for you S3 data. It's expecting protobuf, which is the default format :)<\/p>\n\n<p>You have to set the CSV data format explicitly. See <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/session.html#sagemaker.session.s3_input\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/session.html#sagemaker.session.s3_input<\/a>. <\/p>\n\n<p>It should look something like this:<\/p>\n\n<pre><code>s3_input_train = sagemaker.s3_input(\n  s3_data='s3:\/\/{}\/{}\/train\/manifest_file'.format(bucket, prefix),    \n  s3_data_type='ManifestFile',\n  content_type='csv')\n\n...\n\nkmeans_estimator = sagemaker.estimator.Estimator(kmeans_image, ...)\nkmeans_estimator.set_hyperparameters(...)\n\ns3_data = {'train': s3_input_train}\nkmeans_estimator.fit(s3_data)\n<\/code><\/pre>\n\n<p>Please note the KMeans estimator in the SDK only supports protobuf, see <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/kmeans.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/kmeans.html<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2019-07-28 18:55:23.623000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":51853249,
        "Question_title":"Error Tracking in Amazon SageMaker",
        "Question_body":"<p>I am trying to create a custom Image Classifier in Amazon SageMaker. It is giving me the following error:<\/p>\n\n<p><code>\"ClientError: Data download failed:NoSuchKey (404): The specified key does not exist.\"<\/code><\/p>\n\n<p>I'm assuming this means one of the pictures in my <code>.lst<\/code> file is missing from the directory. Is there some way to find out which <code>.lst<\/code> listing it is specifically having trouble with?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2018-08-15 05:08:01.237000 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":"2018-08-15 08:48:50.320000 UTC",
        "Question_score":2,
        "Question_tags":"amazon-web-services|amazon-sagemaker",
        "Question_view_count":305,
        "Owner_creation_date":"2013-03-15 12:54:59.923000 UTC",
        "Owner_last_access_date":"2022-09-23 16:05:44.973000 UTC",
        "Owner_location":"Washington, DC, USA",
        "Owner_reputation":828,
        "Owner_up_votes":516,
        "Owner_down_votes":1172,
        "Owner_views":530,
        "Answer_body":"<p>Upon further examination (of the log files), it appears the issue does not lie with the .lst file itself, but with the image files it was referencing (which now leaves me wondering why AWS doesn't just say that instead of saying the .lst file is corrupt). I'm going through the image files one-by-one to verify they are correct, hopefully that will solve the problem.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2018-12-17 16:20:24.710000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":59517355,
        "Question_title":"Permission denied: '.\\NTUSER.DAT' when trying to run an Azure ML Pipeline",
        "Question_body":"<p>The short story is, when I try to submit an azure ML pipeline run (an <em>azure ML pipeline<\/em>, not an <em>Azure pipeline<\/em>) from a jupyter notebook, I get PermissionError: [Errno 13] Permission denied: '.\\NTUSER.DAT'.  More details:<\/p>\n\n<p>Relevant code:<\/p>\n\n<pre><code>from azureml.train.automl import AutoMLConfig\nfrom azureml.train.automl.runtime import AutoMLStep\nautoml_settings = {\n    \"iteration_timeout_minutes\": 20,\n    \"experiment_timeout_minutes\": 30,\n    \"n_cross_validations\": 3,\n    \"primary_metric\": 'r2_score',\n    \"preprocess\": True,\n    \"max_concurrent_iterations\": 3,\n    \"max_cores_per_iteration\": -1,\n    \"verbosity\": logging.INFO,\n    \"enable_early_stopping\": True,\n    'time_column_name': \"DateTime\"\n}\n\nautoml_config = AutoMLConfig(task = 'forecasting',\n                             debug_log = 'automl_errors.log',\n                             path = \".\",\n                             compute_target=compute_target,\n                             run_configuration=conda_run_config,                               \n                             training_data = financeforecast_dataset,\n                             label_column_name = 'TotalUSD',\n                             **automl_settings\n                            )\n\nautoml_step = AutoMLStep(\n    name='automl_module',\n    automl_config=automl_config,\n    allow_reuse=False)\n\ntraining_pipeline = Pipeline(\n    description=\"training_pipeline\",\n    workspace=ws,    \n    steps=[automl_step])\n\ntraining_pipeline_run = Experiment(ws, 'test').submit(training_pipeline)\n<\/code><\/pre>\n\n<p>The training_pipeline step runs for apx 20 seconds, and then I get a long trace, ending in:<\/p>\n\n<pre><code>~\\AppData\\Local\\Continuum\\anaconda2\\envs\\forecasting\\lib\\site- \npackages\\azureml\\pipeline\\core\\_module_builder.py in _hash_from_file_paths(hash_src)\n    100             hasher = hashlib.md5()\n    101             for f in hash_src:\n--&gt; 102                 with open(str(f), 'rb') as afile:\n    103                     buf = afile.read()\n    104                     hasher.update(buf)\n\nPermissionError: [Errno 13] Permission denied: '.\\\\NTUSER.DAT'\n<\/code><\/pre>\n\n<p>According to <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-your-first-pipeline\" rel=\"nofollow noreferrer\">Azure's docs on this topic<\/a>, submitting a pipeline uploads a \"snapshot\" of the \"source directory\" you specified.  Initially, I hadn't specified a source directory, so, to test that out, I added: <\/p>\n\n<pre><code>default_source_directory=\"testing\",\n<\/code><\/pre>\n\n<p>as a parameter for the training_pipeline object, but saw the same behavior when I then tried to run it.  Not sure if that is the same source directory the documentation is referring to.  The docs also say that if no source directory is specified, the \"current local directory\" is uploaded.  I used print (os.getcwd()) to get the working directory and gave \"Everyone\" full control permissions on the directory (working in a windows env).<\/p>\n\n<p>All the preceding code works fine, and I can submit an experiment if I use a ScriptRunConfig and run it on attached compute rather than using a pipeline\/training cluster.  <\/p>\n\n<p>Any ideas?  Thanks in advance to anyone who tries to help.  P.S. There is no \"azure-machine-learning-pipelines\" tag, and I can't add one because I don't have enough reputation points.  Someone else could though!  <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-ml-pipelines\" rel=\"nofollow noreferrer\">General<\/a> info on what they are.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-12-29 06:39:40.777000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|azure|azure-machine-learning-studio|automl",
        "Question_view_count":409,
        "Owner_creation_date":"2018-11-09 22:24:53.200000 UTC",
        "Owner_last_access_date":"2022-09-21 15:21:46.450000 UTC",
        "Owner_location":null,
        "Owner_reputation":163,
        "Owner_up_votes":53,
        "Owner_down_votes":0,
        "Owner_views":10,
        "Answer_body":"<p>I resolved this answer by setting the path and the data_script variables in the AutoMLConfig task object, like this (relevant code indicated by -->):<\/p>\n\n<pre><code>automl_config = AutoMLConfig(task = 'forecasting',\n                             debug_log = 'automl_errors.log',\n                             compute_target=compute_target,\n                             run_configuration=conda_run_config,\n                             --&gt;path = \"c:\\\\users\\\\me\",\n                             data_script =\"script.py\",&lt;--\n                             **automl_settings\n                            )\n<\/code><\/pre>\n\n<p>Setting the data_script variable to include the full path, as shown below, did not work.<\/p>\n\n<pre><code>automl_config = AutoMLConfig(task = 'forecasting',\n                             debug_log = 'automl_errors.log',\n                             path = \".\",\n                             --&gt;data_script = \"c:\\\\users\\\\me\\\\script.py\"&lt;--\n                             compute_target=compute_target,\n                             run_configuration=conda_run_config, \n                             **automl_settings\n                            )\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-01-02 19:50:37.207000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":54034172,
        "Question_title":"AWS Sagemaker | Why multiple instances training taking time multiplied to instance number",
        "Question_body":"<p>I am using AWS Sagemaker for model training and deployment, this is sample example for model training <\/p>\n\n<pre><code>from sagemaker.estimator import Estimator\nhyperparameters = {'train-steps': 10}\ninstance_type = 'ml.m4.xlarge'\n\nestimator = Estimator(role=role,\n                      train_instance_count=1,\n                      train_instance_type=instance_type,\n                      image_name=ecr_image,\n                      hyperparameters=hyperparameters)\n\nestimator.fit(data_location)\n<\/code><\/pre>\n\n<p>The docker image mentioned here is a tensorflow system. <\/p>\n\n<p>Suppose it will take 1000 seconds to train the model, now I will increase the instance count to 5 then the training time will increase 5 times i.e. 5000 seconds. As per my understanding the training job will be distributed to 5 machines so ideally it will take 200 seconds per machine but seems its doing separate training on each machine. Can someone please let me know its working over distributed system in general or with Tensorflow.<\/p>\n\n<p>I tried to find out the answer on this documentation <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-dg.pdf\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-dg.pdf<\/a> but seems the way of working on distributed machines is not mentioned here.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-01-04 06:43:36.850000 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|tensorflow|amazon-sagemaker",
        "Question_view_count":1386,
        "Owner_creation_date":"2017-07-30 08:26:08.107000 UTC",
        "Owner_last_access_date":"2022-09-20 14:45:30.870000 UTC",
        "Owner_location":"Delhi, India",
        "Owner_reputation":1370,
        "Owner_up_votes":94,
        "Owner_down_votes":1,
        "Owner_views":125,
        "Answer_body":"<p>Are you using <a href=\"https:\/\/www.tensorflow.org\/guide\/estimators\" rel=\"nofollow noreferrer\">TensorFlow estimator APIs<\/a> in your script? If yes, I think you should run the script by wrapping it in <code>sagemaker.tensorflow.TensorFlow<\/code> class as described <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/README.rst#training-with-tensorflow-estimator\" rel=\"nofollow noreferrer\">in the documentation here<\/a>. If you run training that way, parallelization and communication between instances should work out-of-the-box.<\/p>\n\n<p>But note that scaling will not be linear when you increase the number of instances. Communicating between instances takes time and there could be non-parallelizable bottlenecks in your script like loading data to memory.<\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_date":"2019-01-04 06:59:03.583000 UTC",
        "Answer_last_edit_date":"2019-01-04 07:08:16.710000 UTC",
        "Answer_score":1.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":52244963,
        "Question_title":"Impossible to invoke endpoint with sagemaker",
        "Question_body":"<p>I am using aws sagemaker to invoke the endpoint : <\/p>\n\n<pre><code>payload = pd.read_csv('payload.csv', header=None)\n\n&gt;&gt; payload\n\n\n    0   1   2   3   4\n0   setosa  5.1     3.5     1.4     0.2\n1   setosa  5.1     3.5     1.4     0.2\n<\/code><\/pre>\n\n<p>with this code :<\/p>\n\n<pre><code>response = runtime.invoke_endpoint(EndpointName=r_endpoint,\n                                   ContentType='text\/csv',\n                                   Body=payload)\n<\/code><\/pre>\n\n<p>But I got this problem : <\/p>\n\n<pre><code>ParamValidationError                      Traceback (most recent call last)\n&lt;ipython-input-304-f79f5cf7e0e0&gt; in &lt;module&gt;()\n      1 response = runtime.invoke_endpoint(EndpointName=r_endpoint,\n      2                                    ContentType='text\/csv',\n----&gt; 3                                    Body=payload)\n      4 \n      5 result = json.loads(response['Body'].read().decode())\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/client.py in _api_call(self, *args, **kwargs)\n    312                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n    313             # The \"self\" in this scope is referring to the BaseClient.\n--&gt; 314             return self._make_api_call(operation_name, kwargs)\n    315 \n    316         _api_call.__name__ = str(py_operation_name)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/client.py in _make_api_call(self, operation_name, api_params)\n    584         }\n    585         request_dict = self._convert_to_request_dict(\n--&gt; 586             api_params, operation_model, context=request_context)\n    587 \n    588         handler, event_response = self.meta.events.emit_until_response(\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/client.py in _convert_to_request_dict(self, api_params, operation_model, context)\n    619             api_params, operation_model, context)\n    620         request_dict = self._serializer.serialize_to_request(\n--&gt; 621             api_params, operation_model)\n    622         prepare_request_dict(request_dict, endpoint_url=self._endpoint.host,\n    623                              user_agent=self._client_config.user_agent,\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/validate.py in serialize_to_request(self, parameters, operation_model)\n    289                                                     operation_model.input_shape)\n    290             if report.has_errors():\n--&gt; 291                 raise ParamValidationError(report=report.generate_report())\n    292         return self._serializer.serialize_to_request(parameters,\n    293                                                      operation_model)\n\nParamValidationError: Parameter validation failed:\nInvalid type for parameter Body, value:         0    1    2    3    4\n0  setosa  5.1  3.5  1.4  0.2\n1  setosa  5.1  3.5  1.4  0.2, type: &lt;class 'pandas.core.frame.DataFrame'&gt;, valid types: &lt;class 'bytes'&gt;, &lt;class 'bytearray'&gt;, file-like object\n<\/code><\/pre>\n\n<p>I am just using the same code\/step like in the aws tutorial .  <\/p>\n\n<p>Can you help me to resolve this problem please?<\/p>\n\n<p>thank you<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2018-09-09 13:24:52.700000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":3,
        "Question_tags":"amazon-sagemaker",
        "Question_view_count":4668,
        "Owner_creation_date":"2018-02-14 14:17:32.857000 UTC",
        "Owner_last_access_date":"2019-10-30 10:34:04.507000 UTC",
        "Owner_location":null,
        "Owner_reputation":495,
        "Owner_up_votes":8,
        "Owner_down_votes":0,
        "Owner_views":81,
        "Answer_body":"<p>The payload variable is a Pandas' DataFrame, while <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker-runtime.html#SageMakerRuntime.Client.invoke_endpoint\" rel=\"nofollow noreferrer\">invoke_endpoint()<\/a> expects  <code>Body=b'bytes'|file<\/code>.<\/p>\n\n<p>Try something like this (coding blind):<\/p>\n\n<pre><code>response = runtime.invoke_endpoint(EndpointName=r_endpoint,\n                                   ContentType='text\/csv',\n                                   Body=open('payload.csv'))\n<\/code><\/pre>\n\n<p>More on the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/cdf-inference.html\" rel=\"nofollow noreferrer\">expected formats here<\/a>. \nMake sure the file doesn't include a header.<\/p>\n\n<p>Alternatively, convert your DataFrame to bytes, <a href=\"https:\/\/stackoverflow.com\/questions\/34666860\/converting-pandas-dataframe-to-bytes\">like in this example<\/a>, and pass those bytes instead of passing a DataFrame.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2018-09-11 19:13:14.577000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":4.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":73041737,
        "Question_title":"Print SageMaker instance type",
        "Question_body":"<p>Is there a function I can use to get the instance type of my SageMaker instance.<\/p>\n<p>I basically want to do something like this<\/p>\n<pre><code>region = boto3.Session().region_name\n<\/code><\/pre>\n<p>but for the instance type.<\/p>\n<p>I know I can find it manually, but I want to automate it so that my script can work on any instance.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-07-19 18:14:36.363000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|amazon-sagemaker",
        "Question_view_count":40,
        "Owner_creation_date":"2022-07-05 19:52:34.840000 UTC",
        "Owner_last_access_date":"2022-09-21 21:49:37.727000 UTC",
        "Owner_location":null,
        "Owner_reputation":15,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Answer_body":"<p>You can use <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_DescribeNotebookInstance.html\" rel=\"nofollow noreferrer\">DescribeNotebookInstance<\/a> API to get the instance size.<\/p>\n<pre><code>sm_client = boto3.client(&quot;sagemaker&quot;)\nsm.describe_notebook_instance(\n    NotebookInstanceName=&lt;nb-name&gt;\n)['InstanceType']\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2022-07-19 22:52:10.893000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":65721061,
        "Question_title":"How to parse stepfunction executionId to SageMaker batch transform job name?",
        "Question_body":"<p>I have created a stepfunction, the definition for this statemachine below (<code>step-function.json<\/code>) is used in terraform (using the syntax in this page:<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateTransformJob.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateTransformJob.html<\/a>)<\/p>\n<p>The first time if I execute this statemachine, it will create a SageMaker batch transform job named <code>example-jobname<\/code>, but I need to exeucute this statemachine everyday, then it will give me error <code>&quot;error&quot;: &quot;SageMaker.ResourceInUseException&quot;, &quot;cause&quot;: &quot;Job name must be unique within an AWS account and region, and a job with this name already exists <\/code>.<\/p>\n<p>The cause is because the job name is hard-coded as <code>example-jobname<\/code> so if the state machine gets executed after the first time, since the job name needs to be unique, the task will fail, just wondering how I can add a string (something like ExecutionId at the end of the job name). Here's what I have tried:<\/p>\n<ol>\n<li><p>I added <code>&quot;executionId.$&quot;: &quot;States.Format('somestring {}', $$.Execution.Id)&quot;<\/code> in the <code>Parameters<\/code> section in the json file, but when I execute the task I got error <code> &quot;error&quot;: &quot;States.Runtime&quot;, &quot;cause&quot;: &quot;An error occurred while executing the state 'SageMaker CreateTransformJob' (entered at the event id #2). The Parameters '{\\&quot;BatchStrategy\\&quot;:\\&quot;SingleRecord\\&quot;,..............\\&quot;executionId\\&quot;:\\&quot;somestring arn:aws:states:us-east-1:xxxxx:execution:xxxxx-state-machine:xxxxxxxx72950\\&quot;}' could not be used to start the Task: [The field \\&quot;executionId\\&quot; is not supported by Step Functions]&quot;}<\/code><\/p>\n<\/li>\n<li><p>I modified the jobname in the json file to  <code>&quot;TransformJobName&quot;: &quot;example-jobname-States.Format('somestring {}', $$.Execution.Id)&quot;,<\/code>, when I execute the statemachine, it gave me error: <code>&quot;error&quot;: &quot;SageMaker.AmazonSageMakerException&quot;, &quot;cause&quot;: &quot;2 validation errors detected: Value 'example-jobname-States.Format('somestring {}', $$.Execution.Id)' at 'transformJobName' failed to satisfy constraint: Member must satisfy regular expression pattern: ^[a-zA-Z0-9](-*[a-zA-Z0-9]){0,62}; Value 'example-jobname-States.Format('somestring {}', $$.Execution.Id)' at 'transformJobName' failed to satisfy constraint: Member must have length less than or equal to 63<\/code><\/p>\n<\/li>\n<\/ol>\n<p>I really run out of ideas, can someone help please? Many thanks.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2021-01-14 14:39:27.150000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-01-19 15:57:17.133000 UTC",
        "Question_score":2,
        "Question_tags":"amazon-web-services|terraform|state-machine|amazon-sagemaker|aws-step-functions",
        "Question_view_count":1209,
        "Owner_creation_date":"2018-10-30 17:35:56.270000 UTC",
        "Owner_last_access_date":"2022-09-22 19:30:36.883000 UTC",
        "Owner_location":"United Kingdom",
        "Owner_reputation":2385,
        "Owner_up_votes":1007,
        "Owner_down_votes":16,
        "Owner_views":585,
        "Answer_body":"<p>So as per the <a href=\"https:\/\/docs.aws.amazon.com\/step-functions\/latest\/dg\/sample-train-model.html#sample-train-model-code-examples\" rel=\"nofollow noreferrer\">documentation<\/a>, we should be passing the parameters in the following format<\/p>\n<pre><code>        &quot;Parameters&quot;: {\n            &quot;ModelName.$&quot;: &quot;$$.Execution.Name&quot;,  \n            ....\n        },\n<\/code><\/pre>\n<p>If you take a close look this is something missing from your definition, So your step function definition should be something like below:<\/p>\n<p>either<\/p>\n<pre><code>      &quot;TransformJobName.$&quot;: &quot;$$.Execution.Id&quot;,\n<\/code><\/pre>\n<p>OR<\/p>\n<pre><code>      &quot;TransformJobName.$: &quot;States.Format('mytransformjob{}', $$.Execution.Id)&quot;\n<\/code><\/pre>\n<p>full State machine definition:<\/p>\n<pre><code>    {\n        &quot;Comment&quot;: &quot;Defines the statemachine.&quot;,\n        &quot;StartAt&quot;: &quot;Generate Random String&quot;,\n        &quot;States&quot;: {\n            &quot;Generate Random String&quot;: {\n                &quot;Type&quot;: &quot;Task&quot;,\n                &quot;Resource&quot;: &quot;arn:aws:lambda:eu-central-1:1234567890:function:randomstring&quot;,\n                &quot;ResultPath&quot;: &quot;$.executionid&quot;,\n                &quot;Parameters&quot;: {\n                &quot;executionId.$&quot;: &quot;$$.Execution.Id&quot;\n                },\n                &quot;Next&quot;: &quot;SageMaker CreateTransformJob&quot;\n            },\n        &quot;SageMaker CreateTransformJob&quot;: {\n            &quot;Type&quot;: &quot;Task&quot;,\n            &quot;Resource&quot;: &quot;arn:aws:states:::sagemaker:createTransformJob.sync&quot;,\n            &quot;Parameters&quot;: {\n            &quot;BatchStrategy&quot;: &quot;SingleRecord&quot;,\n            &quot;DataProcessing&quot;: {\n                &quot;InputFilter&quot;: &quot;$&quot;,\n                &quot;JoinSource&quot;: &quot;Input&quot;,\n                &quot;OutputFilter&quot;: &quot;xxx&quot;\n            },\n            &quot;Environment&quot;: {\n                &quot;SAGEMAKER_MODEL_SERVER_TIMEOUT&quot;: &quot;300&quot;\n            },\n            &quot;MaxConcurrentTransforms&quot;: 100,\n            &quot;MaxPayloadInMB&quot;: 1,\n            &quot;ModelName&quot;: &quot;${model_name}&quot;,\n            &quot;TransformInput&quot;: {\n                &quot;DataSource&quot;: {\n                    &quot;S3DataSource&quot;: {\n                        &quot;S3DataType&quot;: &quot;S3Prefix&quot;,\n                        &quot;S3Uri&quot;: &quot;${s3_input_path}&quot;\n                    }\n                },\n                &quot;ContentType&quot;: &quot;application\/jsonlines&quot;,\n                &quot;CompressionType&quot;: &quot;Gzip&quot;,\n                &quot;SplitType&quot;: &quot;Line&quot;\n            },\n            &quot;TransformJobName.$&quot;: &quot;$.executionid&quot;,\n            &quot;TransformOutput&quot;: {\n                &quot;S3OutputPath&quot;: &quot;${s3_output_path}&quot;,\n                &quot;Accept&quot;: &quot;application\/jsonlines&quot;,\n                &quot;AssembleWith&quot;: &quot;Line&quot;\n            },    \n            &quot;TransformResources&quot;: {\n                &quot;InstanceType&quot;: &quot;xxx&quot;,\n                &quot;InstanceCount&quot;: 1\n            }\n        },\n            &quot;End&quot;: true\n        }\n        }\n    }\n<\/code><\/pre>\n<p>In the above definition the lambda could be a function which parses the execution id arn which I am passing via the parameters section:<\/p>\n<pre><code> def lambda_handler(event, context):\n    return(event.get('executionId').split(':')[-1])\n<\/code><\/pre>\n<p>Or if you dont wanna pass the execution id , it can simply return the random string like<\/p>\n<pre><code> import string\n def lambda_handler(event, context):\n    return(string.ascii_uppercase + string.digits)\n<\/code><\/pre>\n<p>you can generate all kinds of random string or do generate anything in the lambda and pass that to the transform job name.<\/p>",
        "Answer_comment_count":23.0,
        "Answer_creation_date":"2021-01-14 15:13:35.397000 UTC",
        "Answer_last_edit_date":"2021-01-14 16:25:19.770000 UTC",
        "Answer_score":3.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":34320449,
        "Question_title":"Use Azure ML methods like an API",
        "Question_body":"<p>Is that possible to use machine learning methods from Microsoft Azure Machine Learning  as an API from my own code (without ML Studio) with possibility to calculate everything on their side?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2015-12-16 19:26:56.510000 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"frameworks|rapidminer|azure-machine-learning-studio",
        "Question_view_count":180,
        "Owner_creation_date":"2012-05-05 14:23:44.220000 UTC",
        "Owner_last_access_date":"2022-09-22 08:49:42.560000 UTC",
        "Owner_location":null,
        "Owner_reputation":834,
        "Owner_up_votes":159,
        "Owner_down_votes":2,
        "Owner_views":122,
        "Answer_body":"<p>You can <a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-publish-a-machine-learning-web-service\/\" rel=\"nofollow\">publish<\/a> an experiment (machine learning functions you hooked together in Azure ML Studio) as an API. When you call that API in your custom code you give it your data and all the computation runs in the cloud in Azure ML. <\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_date":"2015-12-17 02:00:13.533000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":69751254,
        "Question_title":"Submitting multiple runs to the same node on AzureML",
        "Question_body":"<p>I want to perform hyperparameter search using AzureML. My models are small (around 1GB) thus I would like to run multiple models on the same GPU\/node to save costs but I do not know how to achieve this.<\/p>\n<p>The way I currently submit jobs is the following (resulting in one training run per GPU\/node):<\/p>\n<pre><code>experiment = Experiment(workspace, experiment_name)\nconfig = ScriptRunConfig(source_directory=&quot;.\/src&quot;,\n                         script=&quot;train.py&quot;,\n                         compute_target=&quot;gpu_cluster&quot;,\n                         environment=&quot;env_name&quot;,\n                         arguments=[&quot;--args args&quot;])\nrun = experiment.submit(config)\n<\/code><\/pre>\n<p><code>ScriptRunConfig<\/code> can be provided with a <code>distributed_job_config<\/code>. I tried to use <code>MpiConfiguration<\/code> there but if this is done the run fails due to an MPI error that reads as if the cluster is configured to only allow one run per node:<\/p>\n<blockquote>\n<pre><code>Open RTE detected a bad parameter in hostfile: [...]\nThe max_slots parameter is less than the slots parameter:\nslots = 3\nmax_slots = 1\n[...] ORTE_ERROR_LOG: Bad Parameter in file util\/hostfile\/hostfile.c at line 407\n<\/code><\/pre>\n<\/blockquote>\n<p>Using <code>HyperDriveConfig<\/code> also defaults to submitting one run to one GPU and additionally providing a <code>MpiConfiguration<\/code> leads to the same error as shown above.<\/p>\n<p>I guess I could always rewrite my train script to train multiple models in parallel, s.t. each <code>run<\/code> wraps multiple trainings. I would like to avoid this option though, because then logging and checkpoint writes become increasingly messy and it would require a large refactor of the train pipeline. Also this functionality seems so basic that I hope there is a way to do this gracefully. Any ideas?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-10-28 09:09:02.523000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":3,
        "Question_tags":"azure|mpi|cluster-computing|azure-machine-learning-service",
        "Question_view_count":364,
        "Owner_creation_date":"2014-04-04 10:29:38.877000 UTC",
        "Owner_last_access_date":"2022-07-08 15:24:09.527000 UTC",
        "Owner_location":null,
        "Owner_reputation":107,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":17,
        "Answer_body":"<p>Use Run.create_children method which will start child runs that are \u201clocal\u201d to the parent run, and don\u2019t need authentication.<\/p>\n<p>For AMLcompute max_concurrent_runs map to maximum number of nodes that will be used to run  a hyperparameter tuning run.\nSo there would be 1 execution per node.<\/p>\n<p>single service deployed but you can load multiple model versions in the init then the score function, depending on the request\u2019s param, uses particular model version to score.\nor with the new ML Endpoints (Preview).\n<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-endpoints\" rel=\"nofollow noreferrer\">What are endpoints (preview) - Azure Machine Learning | Microsoft Docs<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-10-29 12:53:19.763000 UTC",
        "Answer_last_edit_date":"2021-10-29 13:08:00.997000 UTC",
        "Answer_score":1.0,
        "Question_valid_tags":[
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":70692270,
        "Question_title":"Azure ML Online Endpoint deployment DriverFileNotFound Error",
        "Question_body":"<p>When running the Azure ML Online endpoint commands, it works locally. But when I try to deploy it to Azure I get this error.\nCommand - <code>az ml online-deployment create --name blue --endpoint &quot;unique-name&quot; -f endpoints\/online\/managed\/sample\/blue-deployment.yml --all-traffic<\/code><\/p>\n<pre><code>{\n    &quot;status&quot;: &quot;Failed&quot;,\n    &quot;error&quot;: {\n        &quot;code&quot;: &quot;DriverFileNotFound&quot;,\n        &quot;message&quot;: &quot;Driver file with name score.py not found in provided dependencies. Please check the name of your file.&quot;,\n        &quot;details&quot;: [\n            {\n                &quot;code&quot;: &quot;DriverFileNotFound&quot;,\n                &quot;message&quot;: &quot;Driver file with name score.py not found in provided dependencies. Please check the name of your file.\\nThe build log is available in the workspace blob store \\&quot;coloraiamlsa\\&quot; under the path \\&quot;\/azureml\/ImageLogs\/1673692e-e30b-4306-ab81-2eed9dfd4020\/build.log\\&quot;&quot;,\n                &quot;details&quot;: [],\n                &quot;additionalInfo&quot;: []\n            }\n        ],\n        \n<\/code><\/pre>\n<p>This is the deployment YAML taken straight from <a href=\"https:\/\/github.com\/Azure\/azureml-examples\" rel=\"nofollow noreferrer\">azureml-examples<\/a> repo<\/p>\n<pre><code>$schema: https:\/\/azuremlschemas.azureedge.net\/latest\/managedOnlineDeployment.schema.json\nname: blue\nendpoint_name: my-endpoint\nmodel:\n  local_path: ..\/..\/model-1\/model\/sklearn_regression_model.pkl\ncode_configuration:\n  code: \n    local_path: ..\/..\/model-1\/onlinescoring\/\n  scoring_script: score.py\nenvironment: \n  conda_file: ..\/..\/model-1\/environment\/conda.yml\n  image: mcr.microsoft.com\/azureml\/openmpi3.1.2-ubuntu18.04:20210727.v1\ninstance_type: Standard_F2s_v2\ninstance_count: 1\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-01-13 06:25:50.693000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":2,
        "Question_tags":"endpoint|azure-machine-learning-service",
        "Question_view_count":130,
        "Owner_creation_date":"2010-05-24 09:36:42.127000 UTC",
        "Owner_last_access_date":"2022-09-23 09:14:38.260000 UTC",
        "Owner_location":"Hyderabad, India",
        "Owner_reputation":1561,
        "Owner_up_votes":61,
        "Owner_down_votes":1,
        "Owner_views":243,
        "Answer_body":"<p>Finally after lot of head banging, I have been able to consistently repro this bug in another Azure ML Workspace.<\/p>\n<p>I tried deploying the same sample in a brand new Azure ML workspace created and it went smoothly.<\/p>\n<p>At this point I remembered that I had upgraded the Storage Account of my previous AML Workspace to DataLake Gen2.<\/p>\n<p>So I did the same upgrade in this new workspace\u2019s storage account. After the upgrade, when I try to deploy the same endpoint, I get the same <code>DriverFileNotFoundError<\/code>!<\/p>\n<p>It seems Azure ML does not support Storage Account with DataLake Gen2 capabilities although the support page says otherwise. <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-data#supported-data-storage-service-types\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-data#supported-data-storage-service-types<\/a>.<\/p>\n<p>At this point my only option is to recreate a new workspace and deploy my code there. Hope Azure team fixes this soon.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-01-17 04:15:13.590000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_valid_tags":[
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":67937857,
        "Question_title":"Pachyderm deploy GCP - no such image",
        "Question_body":"<p>I'm deploying Pachyderm on GKE but when I deploy the pipeline (following the <a href=\"https:\/\/docs.pachyderm.com\/latest\/getting_started\/beginner_tutorial\/\" rel=\"nofollow noreferrer\">https:\/\/docs.pachyderm.com\/latest\/getting_started\/beginner_tutorial\/<\/a>) the Pod fails in ImagePullCrashLoopBack giving this error &quot;no such image&quot;.<\/p>\n<p>Here, the output of the command &quot;kubectl get pods&quot;:\n<a href=\"https:\/\/i.stack.imgur.com\/lpvj7.png\" rel=\"nofollow noreferrer\">screenshot<\/a><\/p>\n<p>How can I fix the deployment procedure?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2021-06-11 13:26:09.943000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"kubernetes|google-cloud-platform|pachyderm",
        "Question_view_count":48,
        "Owner_creation_date":"2014-11-06 09:41:52.943000 UTC",
        "Owner_last_access_date":"2022-09-24 19:13:38.297000 UTC",
        "Owner_location":"Milano, Metropolitan City of Milan, Italy",
        "Owner_reputation":107,
        "Owner_up_votes":27,
        "Owner_down_votes":0,
        "Owner_views":24,
        "Answer_body":"<p>As mentioned in the Slack channel of Pachyderm community, adding the flag <code>--no-expose-docker-socket<\/code> to the deploy call should solve the issue.<\/p>\n<p><code>pachctl deploy google ${BUCKET_NAME} ${STORAGE_SIZE} --dynamic-etcd-nodes=1 --no-expose-docker-socket<\/code><\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2021-06-11 13:29:00.677000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "pachyderm"
        ]
    },
    {
        "Question_id":45753090,
        "Question_title":"Azure ML endpoint 404 error",
        "Question_body":"<p>An Azure Data Factory pipeline for updating a trained ML model returns this error:<\/p>\n\n<pre><code>HTTP 404. The resource you are looking for (or one of its dependencies) could have been removed, had its name changed, or is temporarily unavailable. Please review the following URL and make sure that it is spelled correctly.\nRequested URL: \/workspaces\/xxxx\/webservices\/xxxx\/endpoints\/update\n\nDiagnostic details: job ID xxxx. Endpoint https:\/\/services.azureml.net\/workspaces\/xxxx\/webservices\/xxxx\/endpoints\/update.\n<\/code><\/pre>\n\n<p>I don't even want to think about why it returned a HTML document...\nI am 100% sure that the endpoint exists and the key provided is correct.\nSo what is my mistake?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2017-08-18 09:23:15.263000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2017-08-20 12:44:20.163000 UTC",
        "Question_score":0,
        "Question_tags":"azure|azure-data-factory|azure-machine-learning-studio",
        "Question_view_count":494,
        "Owner_creation_date":"2016-10-18 16:00:55.803000 UTC",
        "Owner_last_access_date":"2022-08-31 11:34:34.687000 UTC",
        "Owner_location":"Holzkirchen, Deutschland",
        "Owner_reputation":3068,
        "Owner_up_votes":186,
        "Owner_down_votes":66,
        "Owner_views":386,
        "Answer_body":"<p>Deleting and creating the endpoint again fixed it.<\/p>\n\n<p>Microsoft...<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2017-10-25 14:42:24.573000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":68479297,
        "Question_title":"AWS send image to Sagemaker from Lambda: how to set content handling?",
        "Question_body":"<p>similar question to\n<a href=\"https:\/\/stackoverflow.com\/a\/66683538\/6896705\">AWS Lambda send image file to Amazon Sagemaker<\/a><\/p>\n<p>I try to make simple-mnist work (the model was built by referring to <a href=\"https:\/\/sagemaker-immersionday.workshop.aws\/en\/lab3\/option1.html\" rel=\"nofollow noreferrer\">aws tutorial<\/a>)<\/p>\n<p>Then I am using API gateway (REST API w\/ proxy integration) to post image data to lambda, and would like to send it to sagemaker endpoint and make an inference.<\/p>\n<p>In lambda function, I wrote the code(.py) like this.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>runtime = boto3.Session().client('sagemaker-runtime')\n\nendpoint_name = 'tensorflow-training-YYYY-mm-dd-...'\nres = runtime.invoke_endpoint(EndpointName=endpoint_name,\n                              Body=Image,\n                              ContentType='image\/jpeg',\n                              Accept='image\/jpeg')\n<\/code><\/pre>\n<p>However, when I send image to lambda via API gateway, this error occurs.<\/p>\n<blockquote>\n<p>[ERROR] ModelError: An error occurred (ModelError) when calling the\nInvokeEndpoint operation: Received client error (415) from model with\nmessage &quot; {\n&quot;error&quot;: &quot;Unsupported Media Type: image\/jpeg&quot; }<\/p>\n<\/blockquote>\n<p>I think I need to do something referring to <a href=\"https:\/\/docs.aws.amazon.com\/apigateway\/latest\/developerguide\/api-gateway-payload-encodings.html\" rel=\"nofollow noreferrer\">Working with binary media types for REST APIs\n<\/a><\/p>\n<p>But since I am very new, I have no idea about the appropriate thing to do, on which page (maybe API Gateway page?) or how...<\/p>\n<p>I need some clues to solve this problem. Thank you in advance.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-07-22 04:43:38.517000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-07-22 08:09:35.923000 UTC",
        "Question_score":1,
        "Question_tags":"amazon-web-services|aws-lambda|amazon-sagemaker",
        "Question_view_count":390,
        "Owner_creation_date":"2016-09-29 00:32:31.950000 UTC",
        "Owner_last_access_date":"2022-05-17 13:21:20.043000 UTC",
        "Owner_location":null,
        "Owner_reputation":43,
        "Owner_up_votes":4,
        "Owner_down_votes":0,
        "Owner_views":18,
        "Answer_body":"<p>Looking <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/adapt-inference-container.html\" rel=\"nofollow noreferrer\">here<\/a> you can see that only some specific content types are supported by default, and images are not in this list. I think you have to either implement your <code>input_fn<\/code> function or adapt your data to one of the supported content types.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2021-07-22 15:17:02.753000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":69522401,
        "Question_title":"Compare String with list of strings in bash",
        "Question_body":"<p>I am trying to compare the service with a list of available service names, if service is found in the list then do update the service otherwise deploy the service.\nBut below condition only deploying new service even when service available in list variable?<\/p>\n<pre><code>SERVNAME=ner\nSERVICE=$(az ml service list -g $(ml_rg) --workspace-name $(ml_ws) --model-name $(model_name) --query &quot;[].name&quot;)\n\nif [[ &quot;$SERVNAME&quot; == &quot;$SERVICE&quot; ]];\nthen\n   echo &quot;Service Found: $(SERVNAME) and updating the service&quot;\n   az ml service update --name $(AKS_DEPLOYMENT_NAME) \\\n          --model '$(MODEL_NAME):$(MODEL_VERSION)' \\\n          --dc aksDeploymentConfig.json \\\n          --ic inferenceConfig.json \\\n          -e $(ml_env_name) --ev $(ml_env_version) \\\n          -g $(ml_rg) --workspace-name $(ml_ws) -v ;\nelse\n   echo &quot;Service Not found and starting deploying new service&quot;\n   az ml model deploy --name $(AKS_DEPLOYMENT_NAME) --model \\\n   '$(MODEL_NAME):$(MODEL_VERSION)' \\\n          --compute-target $(ml_aks_name) \\\n          --ic inferenceConfig.json \\\n          -e $(ml_env_name) --ev $(ml_env_version) \\\n          --dc aksDeploymentConfig.json \\\n          -g $(ml_rg) --workspace-name $(ml_ws) \\\n          --overwrite -v ;\nfi\n<\/code><\/pre>\n<p>Example list<\/p>\n<pre><code>SERVNAME=&quot;ner&quot;\nSERVICE=[ &quot;ner&quot;, &quot;aks-gpu-ner-0306210907&quot;, &quot;aks-gpu-ner-30012231&quot;, &quot;aks-gpu-ner-1305211336&quot;]\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_date":"2021-10-11 07:19:51.707000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-10-11 12:02:12.907000 UTC",
        "Question_score":1,
        "Question_tags":"linux|bash|azure-devops|azure-machine-learning-studio",
        "Question_view_count":1153,
        "Owner_creation_date":"2018-05-23 09:09:49.807000 UTC",
        "Owner_last_access_date":"2022-02-14 05:15:05.353000 UTC",
        "Owner_location":null,
        "Owner_reputation":35,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Answer_body":"<p>Assuming the <code>az ml<\/code> command returns a json array string and you want to\ncheck if the array includes the value of variable <code>SERVNAME<\/code>, would you\nplease try:<\/p>\n<pre><code>SERVNAME=&quot;ner&quot;\nSERVICE='[ &quot;ner&quot;, &quot;aks-gpu-ner-0306210907&quot;, &quot;aks-gpu-ner-30012231&quot;, &quot;aks-gpu-ner-1305211336&quot;]'\n\nif [[ $SERVICE =~ &quot;\\&quot;$SERVNAME\\&quot;&quot; ]]; then\n    echo &quot;Service Found&quot;\n    # put your command here to update the service\nelse\n    echo &quot;Service Not Found&quot;\n    # put your command here to deploy new service\nfi\n<\/code><\/pre>\n<p>The regex operator <code>$SERVICE =~ &quot;\\&quot;$SERVNAME\\&quot;&quot;<\/code> matches if the string <code>$SERVICE<\/code>\ncontains the substring <code>$SERVNAME<\/code> enclosed with double quotes.<\/p>\n<p>If <code>jq<\/code> is available, you could also say:<\/p>\n<pre><code>result=$(echo &quot;$SERVICE&quot; | jq --arg var &quot;$SERVNAME&quot; '. | index($var)')\nif [[ $result != &quot;null&quot; ]]; then\n    echo &quot;Service Found&quot;\nelse\n    echo &quot;Service Not Found&quot;\nfi\n<\/code><\/pre>",
        "Answer_comment_count":6.0,
        "Answer_creation_date":"2021-10-12 00:57:13.753000 UTC",
        "Answer_last_edit_date":"2021-10-15 06:20:32.727000 UTC",
        "Answer_score":2.0,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":56848293,
        "Question_title":"What is Random seed in Azure Machine Learning?",
        "Question_body":"<p>I am learning Azure Machine Learning. I am frequently encountering the <strong>Random Seed<\/strong> in some of the steps like,<\/p>\n\n<ol>\n<li>Split Data<\/li>\n<li>Untrained algorithm models as Two Class Regression, Multi-class regression, Tree, Forest,..<\/li>\n<\/ol>\n\n<p>In the tutorial, they choose Random Seed as '123'; trained model has high accuracy but when I try to choose other random integers like 245, 256, 12, 321,.. it did not do well.<\/p>\n\n<hr>\n\n<p><strong>Questions<\/strong><\/p>\n\n<ul>\n<li>What is a Random Seed Integer?<\/li>\n<li>How to carefully choose a Random Seed from range of integer values? What is the key or strategy to choose it?<\/li>\n<li>Why does Random Seed significantly affect the ML Scoring, Prediction and Quality of the trained model?<\/li>\n<\/ul>\n\n<hr>\n\n<p><strong>Pretext<\/strong><\/p>\n\n<ol>\n<li>I have <a href=\"https:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/iris\/iris.data\" rel=\"nofollow noreferrer\">Iris-Sepal-Petal-Dataset<\/a> with Sepal (<em>Length &amp; Width<\/em>) and Petal (<em>Length &amp; Width<\/em>)<\/li>\n<li>Last column in data-set is 'Binomial ClassName'<\/li>\n<li>I am training the data-set with Multiclass Decision Forest Algorithm and splitting the data with different random seeds 321, 123 and 12345 in order<\/li>\n<li>It affects the final quality of trained model. Random seed#123 being best of Prediction probability score: 1.<\/li>\n<\/ol>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/12OyD.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/12OyD.png\" alt=\"ML Studio Snap\"><\/a><\/p>\n\n<hr>\n\n<p><strong>Observations<\/strong><\/p>\n\n<p><strong>1. Random seed: 321<\/strong><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/YcsiS.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/YcsiS.png\" alt=\"Random-seed-321\"><\/a><\/p>\n\n<p><strong>2. Random seed: 123<\/strong><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/Qrk4G.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Qrk4G.png\" alt=\"Random-seed-123\"><\/a><\/p>\n\n<p><strong>3. Random seed: 12345<\/strong><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/D1Rki.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/D1Rki.png\" alt=\"Random-seed-12345\"><\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":9,
        "Question_creation_date":"2019-07-02 08:27:18.867000 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":"2019-07-02 11:29:03.383000 UTC",
        "Question_score":0,
        "Question_tags":"machine-learning|azure-machine-learning-studio|random-seed|iris-dataset",
        "Question_view_count":2046,
        "Owner_creation_date":"2016-10-05 18:11:48.437000 UTC",
        "Owner_last_access_date":"2022-07-24 08:26:07.790000 UTC",
        "Owner_location":"India",
        "Owner_reputation":1849,
        "Owner_up_votes":366,
        "Owner_down_votes":21,
        "Owner_views":253,
        "Answer_body":"<blockquote>\n  <p>What is a Random Seed Integer?<\/p>\n<\/blockquote>\n\n<p>Will not go into any details regarding what a random seed is in general; there is plenty of material available by a simple web search (see for example <a href=\"https:\/\/stackoverflow.com\/questions\/22639587\/random-seed-what-does-it-do\">this SO thread<\/a>).<\/p>\n\n<p>Random seed serves just to initialize the (pseudo)random number generator, mainly in order to make ML examples reproducible.<\/p>\n\n<blockquote>\n  <p>How to carefully choose a Random Seed from range of integer values? What is the key or strategy to choose it?<\/p>\n<\/blockquote>\n\n<p>Arguably this is already answered implicitly above: you are simply not supposed to choose any particular random seed, and your results should be roughly the same across different random seeds.<\/p>\n\n<blockquote>\n  <p>Why does Random Seed significantly affect the ML Scoring, Prediction and Quality of the trained model?<\/p>\n<\/blockquote>\n\n<p>Now, to the heart of your question. The answer <em>here<\/em> (i.e. with the iris dataset) is the <strong>small-sample effects<\/strong>...<\/p>\n\n<p>To start with, your reported results across different random seeds are not <em>that<\/em> different. Nevertheless, I agree that, at first sight, a difference in macro-average precision of 0.9 and 0.94 might <em>seem<\/em> large; but looking more closely it is revealed that the difference is really not an issue. Why?<\/p>\n\n<p>Using the 20% of your (only) 150-samples dataset leaves you with only 30 samples in your test set (where the evaluation is performed); this is stratified, i.e. about 10 samples from each class. Now, for datasets of <em>that<\/em> small size, it is not difficult to imagine that a difference in the correct classification of <strong>only 1-2<\/strong> samples can have this apparent difference in the performance metrics reported.<\/p>\n\n<p>Let's try to verify this in scikit-learn using a decision tree classifier (the essence of the issue does not depend on the specific framework or the ML algorithm used):<\/p>\n\n<pre class=\"lang-python prettyprint-override\"><code>from sklearn.datasets import load_iris\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.model_selection import train_test_split\n\nX, y = load_iris(return_X_y=True)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=321, stratify=y)\ndt = DecisionTreeClassifier()\ndt.fit(X_train, y_train)\ny_pred = dt.predict(X_test)\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\n<\/code><\/pre>\n\n<p>Result:<\/p>\n\n<pre><code>[[10  0  0]\n [ 0  9  1]\n [ 0  0 10]]\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        10\n           1       1.00      0.90      0.95        10\n           2       0.91      1.00      0.95        10\n\n   micro avg       0.97      0.97      0.97        30\n   macro avg       0.97      0.97      0.97        30\nweighted avg       0.97      0.97      0.97        30\n<\/code><\/pre>\n\n<p>Let's repeat the code above, changing only the <code>random_state<\/code> argument in <code>train_test_split<\/code>; for <code>random_state=123<\/code> we get:<\/p>\n\n<pre><code>[[10  0  0]\n [ 0  7  3]\n [ 0  2  8]]\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        10\n           1       0.78      0.70      0.74        10\n           2       0.73      0.80      0.76        10\n\n   micro avg       0.83      0.83      0.83        30\n   macro avg       0.84      0.83      0.83        30\nweighted avg       0.84      0.83      0.83        30\n<\/code><\/pre>\n\n<p>while for <code>random_state=12345<\/code> we get:<\/p>\n\n<pre><code>[[10  0  0]\n [ 0  8  2]\n [ 0  0 10]]\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        10\n           1       1.00      0.80      0.89        10\n           2       0.83      1.00      0.91        10\n\n   micro avg       0.93      0.93      0.93        30\n   macro avg       0.94      0.93      0.93        30\nweighted avg       0.94      0.93      0.93        30\n<\/code><\/pre>\n\n<p>Looking at the <em>absolute numbers<\/em> of the 3 confusion matrices (in <em>small samples<\/em>, percentages can be <strong>misleading<\/strong>), you should be able to convince yourself that the differences are not that big, and they can be arguably justified by the random element inherent in the whole procedure (here the exact split of the dataset into training and test).<\/p>\n\n<p>Should your test set be significantly bigger, these discrepancies would be practically negligible... <\/p>\n\n<p>A last notice; I have used the exact same seed numbers as you, but this does not actually mean anything, as in general the random number generators <em>across<\/em> platforms &amp; languages are not the same, hence the corresponding seeds are not actually compatible. See own answer in <a href=\"https:\/\/stackoverflow.com\/questions\/52293899\/are-random-seeds-compatible-between-systems\">Are random seeds compatible between systems?<\/a> for a demonstration.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2019-07-02 12:17:30.057000 UTC",
        "Answer_last_edit_date":"2019-07-02 12:22:31.170000 UTC",
        "Answer_score":2.0,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":54003052,
        "Question_title":"How do I implement a PyTorch Dataset for use with AWS SageMaker?",
        "Question_body":"<p>I have implemented a PyTorch <code>Dataset<\/code> that works locally (on my own desktop), but when executed on AWS SageMaker, it breaks. My <code>Dataset<\/code> implementation is as follows.<\/p>\n\n<pre><code>class ImageDataset(Dataset):\n    def __init__(self, path='.\/images', transform=None):\n        self.path = path\n        self.files = [join(path, f) for f in listdir(path) if isfile(join(path, f)) and f.endswith('.jpg')]\n        self.transform = transform\n        if transform is None:\n            self.transform = transforms.Compose([\n                transforms.Resize((128, 128)),\n                transforms.ToTensor(),\n                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n            ])\n\n    def __len__(self):\n        return len(files)\n\n    def __getitem__(self, idx):\n        img_name = self.files[idx]\n\n        # we may infer the label from the filename\n        dash_idx = img_name.rfind('-')\n        dot_idx = img_name.rfind('.')\n        label = int(img_name[dash_idx + 1:dot_idx])\n\n        image = Image.open(img_name)\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label\n<\/code><\/pre>\n\n<p>I am following this <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/tree\/master\/src\/sagemaker\/pytorch\" rel=\"noreferrer\">example<\/a> and this <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/pytorch_cnn_cifar10\/pytorch_local_mode_cifar10.ipynb\" rel=\"noreferrer\">one too<\/a>, and I run the <code>estimator<\/code> as follows.<\/p>\n\n<pre><code>inputs = {\n 'train': 'file:\/\/images',\n 'eval': 'file:\/\/images'\n}\nestimator = PyTorch(entry_point='pytorch-train.py',\n                            role=role,\n                            framework_version='1.0.0',\n                            train_instance_count=1,\n                            train_instance_type=instance_type)\nestimator.fit(inputs)\n<\/code><\/pre>\n\n<p>I get the following error.<\/p>\n\n<blockquote>\n  <p>FileNotFoundError: [Errno 2] No such file or directory: '.\/images'<\/p>\n<\/blockquote>\n\n<p>In the example that I am following, they upload the CFAIR dataset (which is downloaded locally) to S3.<\/p>\n\n<pre><code>inputs = sagemaker_session.upload_data(path='data', bucket=bucket, key_prefix='data\/cifar10')\n<\/code><\/pre>\n\n<p>If I take a peek at <code>inputs<\/code>, it is just a string literal <code>s3:\/\/sagemaker-us-east-3-184838577132\/data\/cifar10<\/code>. The code to create a <code>Dataset<\/code> and a <code>DataLoader<\/code> is shown <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/pytorch_mnist\/mnist.py#L41\" rel=\"noreferrer\">here<\/a>, which does not help unless I track down the source and step through the logic.<\/p>\n\n<p>I think what needs to happen inside my <code>ImageDataset<\/code> is to supply the <code>S3<\/code> path and use the <code>AWS CLI<\/code> or something to query the files and acquire their content. I do not think the <code>AWS CLI<\/code> is the right approach as this relies on the console and I will have to execute some sub-process commands and then parse through. <\/p>\n\n<p>There must be a recipe or something to create a custom <code>Dataset<\/code> backed by <code>S3<\/code> files, right?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2019-01-02 08:02:40.377000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":8,
        "Question_tags":"python|amazon-s3|pytorch|amazon-sagemaker",
        "Question_view_count":4597,
        "Owner_creation_date":"2013-03-15 17:49:38.320000 UTC",
        "Owner_last_access_date":"2022-09-22 03:51:42.617000 UTC",
        "Owner_location":null,
        "Owner_reputation":7643,
        "Owner_up_votes":416,
        "Owner_down_votes":10,
        "Owner_views":515,
        "Answer_body":"<p>I was able to create a PyTorch <code>Dataset<\/code> backed by S3 data using <code>boto3<\/code>. Here's the snippet if anyone is interested.<\/p>\n\n<pre><code>class ImageDataset(Dataset):\n    def __init__(self, path='.\/images', transform=None):\n        self.path = path\n        self.s3 = boto3.resource('s3')\n        self.bucket = self.s3.Bucket(path)\n        self.files = [obj.key for obj in self.bucket.objects.all()]\n        self.transform = transform\n        if transform is None:\n            self.transform = transforms.Compose([\n                transforms.Resize((128, 128)),\n                transforms.ToTensor(),\n                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n            ])\n\n    def __len__(self):\n        return len(files)\n\n    def __getitem__(self, idx):\n        img_name = self.files[idx]\n\n        # we may infer the label from the filename\n        dash_idx = img_name.rfind('-')\n        dot_idx = img_name.rfind('.')\n        label = int(img_name[dash_idx + 1:dot_idx])\n\n        # we need to download the file from S3 to a temporary file locally\n        # we need to create the local file name\n        obj = self.bucket.Object(img_name)\n        tmp = tempfile.NamedTemporaryFile()\n        tmp_name = '{}.jpg'.format(tmp.name)\n\n        # now we can actually download from S3 to a local place\n        with open(tmp_name, 'wb') as f:\n            obj.download_fileobj(f)\n            f.flush()\n            f.close()\n            image = Image.open(tmp_name)\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2019-01-08 16:41:37.560000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":12.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":61481147,
        "Question_title":"MLFlow Registry high availability",
        "Question_body":"<p>I am running the mlflow registry using <code>mlflow server<\/code> (<a href=\"https:\/\/mlflow.org\/docs\/latest\/model-registry.html\" rel=\"nofollow noreferrer\">https:\/\/mlflow.org\/docs\/latest\/model-registry.html<\/a>). The server runs fine. If the server crashes for any reason it restart automatically. But for the time of restart the server is not available.<\/p>\n\n<p>Is it possible to run multiple isntances in parallel behind a load balancer? Is this safe or could it be possible that there are any inconsistencies?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2020-04-28 13:18:31.073000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"mlflow|mlmodel",
        "Question_view_count":501,
        "Owner_creation_date":"2013-09-07 14:14:09.260000 UTC",
        "Owner_last_access_date":"2022-07-27 10:56:50.330000 UTC",
        "Owner_location":null,
        "Owner_reputation":470,
        "Owner_up_votes":128,
        "Owner_down_votes":7,
        "Owner_views":45,
        "Answer_body":"<p>Yes, it's possible to have multiple instances of MLflow Tracker Service running behind a load balancer.<\/p>\n\n<p>Because the Tracking server is stateless, you could have multiple instances log to a replicated primary DB as a store. A second hot standby can take over if the primary fails.<\/p>\n\n<p>As for the documentation in how to set up replicated instances of your backend store will vary on which one you elect to use, we cannot definitely document all different scenarios and their configurations.<\/p>\n\n<p>I would check the respective documentation of your backend DB and load balancer for how to federate requests to multiple instances of an MLflow tracking server, how to failover to a hot standby or replicated DB, or how to configure a hot-standby replicated DB instance.<\/p>\n\n<p>The short of it: MLflow tracking server is stateless.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-05-02 01:27:08.087000 UTC",
        "Answer_last_edit_date":"2020-05-02 04:42:51.917000 UTC",
        "Answer_score":0.0,
        "Question_valid_tags":[
            "mlflow"
        ]
    },
    {
        "Question_id":61091659,
        "Question_title":"Should I run forecast predictive model with AWS lambda or sagemaker?",
        "Question_body":"<p>I've been reading some articles regarding this topic and have preliminary thoughts as what I should do with it, but still want to see if anyone can share comments if you have more experience with running machine learning on AWS. I was doing a project for a professor at school, and we decided to use AWS. I need to find a cost-effective and efficient way to deploy a forecasting model on it. <\/p>\n\n<p>What we want to achieve is:<\/p>\n\n<ul>\n<li>read the data from S3 bucket monthly (there will be new data coming in every month), <\/li>\n<li>run a few python files (.py) for custom-built packages and install dependencies (including the files, no more than 30kb), <\/li>\n<li>produce predicted results into a file back in S3 (JSON or CSV works), or push to other endpoints (most likely to be some BI tools - tableau etc.) - but really this step can be flexible (not web for sure) <\/li>\n<\/ul>\n\n<p><strong>First thought I have is AWS sagemaker<\/strong>. However, we'll be using \"fb prophet\" model to predict the results, and we built a customized package to use in the model, therefore, I don't think the notebook instance is gonna help us. (Please correct me if I'm wrong) My understanding is that sagemaker is a environment to build and train the model, but we already built and trained the model. Plus, we won't be using AWS pre-built models anyways.<\/p>\n\n<p>Another thing is if we want to use custom-built package, we will need to create container image, and I've never done that before, not sure about the efforts to do that.<\/p>\n\n<p><strong>2nd option is to create multiple lambda functions<\/strong><\/p>\n\n<ul>\n<li><p>one that triggers to run the python scripts from S3 bucket (2-3 .py files) every time a new file is imported into S3 bucket, which will happen monthly.<\/p><\/li>\n<li><p>one that trigger after the python scripts are done running and produce results and save into S3 bucket.<\/p><\/li>\n<\/ul>\n\n<p>3rd option will combine both options:\n - Use lambda function to trigger the implementation on the python scripts in S3 bucket when the new file comes in.\n - Push the result using sagemaker endpoint, which means we host the model on sagemaker and deploy from there.<\/p>\n\n<p>I am still not entirely sure how to put pre-built model and python scripts onto sagemaker instance and host from there.<\/p>\n\n<p>I'm hoping whoever has more experience with AWS service can help give me some guidance, in terms of more cost-effective and efficient way to run model.<\/p>\n\n<p>Thank you!! <\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2020-04-08 00:49:02.937000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"amazon-web-services|amazon-s3|aws-lambda|amazon-sagemaker|facebook-prophet",
        "Question_view_count":2675,
        "Owner_creation_date":"2020-01-16 15:21:31.243000 UTC",
        "Owner_last_access_date":"2022-08-25 04:44:57.580000 UTC",
        "Owner_location":null,
        "Owner_reputation":23,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":7,
        "Answer_body":"<p>I would say it all depends on how heavy your model is \/ how much data you're running through it. You're right to identify that Lambda will likely be less work. It's quite easy to get a lambda up and running to do the things that you need, and <a href=\"https:\/\/aws.amazon.com\/lambda\/pricing\/\" rel=\"nofollow noreferrer\">Lambda has a very generous free tier<\/a>. The problem is:<\/p>\n\n<ol>\n<li><p>Lambda functions are fundamentally limited in their processing capacity (they timeout after <em>max<\/em> 15 minutes).<\/p><\/li>\n<li><p>Your model might be expensive to load.<\/p><\/li>\n<\/ol>\n\n<p>If you have a lot of data to run through your model, you will need multiple lambdas. Multiple lambdas means you have to load your model multiple times, and that's wasted work. If you're working with \"big data\" this will get expensive once you get through the free tier.<\/p>\n\n<p>If you don't have much data, Lambda will work just fine. I would eyeball it as follows: assuming your data processing step is dominated by your model step, and if all your model interactions (loading the model + evaluating all your data) take less than 15min, you're definitely fine. If they take more, you'll need to do a back-of-the-envelope calculation to figure out whether you'd leave the Lambda free tier.<\/p>\n\n<p>Regarding Lambda: You can literally copy-paste code in to setup a prototype. If your execution takes more than 15min for all your data, you'll need a method of splitting your data up between multiple Lambdas. Consider <a href=\"https:\/\/aws.amazon.com\/step-functions\/\" rel=\"nofollow noreferrer\">Step Functions<\/a> for this.<\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_date":"2020-04-08 03:43:18.727000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":67786052,
        "Question_title":"Log Pickle files as a part of Mlflow run",
        "Question_body":"<p>I am running an MLflow experiment as a part of it I would like to log a few artifacts as a python pickle.<\/p>\n<p>Ex: Trying out different categorical encoders, so wanted to log the encoder objects as a pickle file.<\/p>\n<p>Is there a way to achieve this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-06-01 09:15:22.663000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":3,
        "Question_tags":"python|databricks|azure-databricks|mlflow",
        "Question_view_count":1843,
        "Owner_creation_date":"2014-09-22 04:46:57.027000 UTC",
        "Owner_last_access_date":"2022-09-03 08:12:58.187000 UTC",
        "Owner_location":"Bengaluru, Karnataka, India",
        "Owner_reputation":569,
        "Owner_up_votes":41,
        "Owner_down_votes":3,
        "Owner_views":123,
        "Answer_body":"<p>There are two functions for there:<\/p>\n<ol>\n<li><a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.log_artifact\" rel=\"nofollow noreferrer\">log_artifact<\/a> - to log a local file or directory as an artifact<\/li>\n<li><a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.log_artifacts\" rel=\"nofollow noreferrer\">log_artifacts<\/a> - to log a contents of a local directory<\/li>\n<\/ol>\n<p>so it would be as simple as:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>with mlflow.start_run():\n    mlflow.log_artifact(&quot;encoder.pickle&quot;)\n<\/code><\/pre>\n<p>And you will need to use the <a href=\"https:\/\/mlflow.org\/docs\/latest\/models.html#model-customization\" rel=\"nofollow noreferrer\">custom MLflow model<\/a> to use that pickled file, something like this:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import mlflow.pyfunc\n\nclass my_model(mlflow.pyfunc.PythonModel):\n    def __init__(self, encoders):\n        self.encoders = encoders\n\n    def predict(self, context, model_input):\n        _X = ...# do encoding using self.encoders.\n        return str(self.ctx.predict([_X])[0])\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-06-01 10:16:03.553000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_valid_tags":[
            "mlflow"
        ]
    },
    {
        "Question_id":65819978,
        "Question_title":"Sagemaker multi-model endpoints with unsupported built-in algorithms",
        "Question_body":"<p>I am aware that Sagemaker <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/multi-model-endpoints.html#multi-model-support\" rel=\"nofollow noreferrer\">does not support multi-model endpoints for their built-in image classification algorithm<\/a>. However, in the documentation they hint at building a custom container to use &quot;any other framework or algorithm&quot; with the multi-model endpoint functionality:<\/p>\n<blockquote>\n<p>To use any other framework or algorithm, use the SageMaker inference toolkit to build a container that supports multi-model endpoints. For information, see <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/build-multi-model-build-container.html\" rel=\"nofollow noreferrer\">Build Your Own Container with Multi Model Server<\/a>.<\/p>\n<\/blockquote>\n<p>Ideally, I would like to deploy many (20+) image classification models I have already trained to a single endpoint to save on costs. However, after reading the &quot;Build Your Own Container&quot; guide it is still not exactly clear to me how to build a custom inference container for the models produced by a non-custom algorithm. Most of the tutorials and example notebooks refer to using Pytorch or Sklearn. It is not clear to me that I could make inferences using these libraries on the models I've created with the built-in image classification algorithm.<\/p>\n<p><em>Is<\/em> it possible to create a container to support multi-model endpoints for unsupported built-in Sagemaker algorithms? If so, would somebody be able to hint at how this might be done?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-01-21 01:31:39.733000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|amazon-sagemaker",
        "Question_view_count":524,
        "Owner_creation_date":"2021-01-21 01:08:49.713000 UTC",
        "Owner_last_access_date":"2021-08-04 15:49:41.623000 UTC",
        "Owner_location":null,
        "Owner_reputation":3,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":4,
        "Answer_body":"<p>yes, it is possible to deploy the built in image classification models as a SageMaker multi model endpoint. The key is that the image classification uses <a href=\"https:\/\/mxnet.apache.org\/versions\/1.7.0\/\" rel=\"nofollow noreferrer\">Apache MXNet<\/a>. You can extract the model artifacts (SageMaker stores them in a zip file named model.tar.gz in S3), then load them in to MXNet. The SageMaker MXNet container supports multi model endpoints, so you can use that to deploy the model.<\/p>\n<p>If you unzip the model.tar.gz from this algorithm, you'll find three files:<\/p>\n<p>image-classification-****.params<\/p>\n<p>image-classification-symbol.json<\/p>\n<p>model-shapes.json<\/p>\n<p>The MxNet container expects these files to be named <strong>image-classification-0000.params, model-symbol.json, and model-shapes.json<\/strong>. So I unzipped the zip file, renamed the files and rezipped them. For more information on the MXNet container check out the <a href=\"https:\/\/github.com\/aws\/sagemaker-mxnet-inference-toolkit\" rel=\"nofollow noreferrer\">GitHub repository<\/a>.<\/p>\n<p>After that you can deploy the model as a single MXNet endpoint using the SageMaker SDK with the following code:<\/p>\n<pre><code>from sagemaker import get_execution_role\nfrom sagemaker.mxnet.model import MXNetModel\n\nrole = get_execution_role()\n\nmxnet_model = MXNetModel(model_data=s3_model, role=role, \n                         entry_point='built_in_image_classifier.py', \n                         framework_version='1.4.1',\n                         py_version='py3')\n\npredictor = mxnet_model.deploy(instance_type='ml.c4.xlarge', initial_instance_count=1)\n<\/code><\/pre>\n<p>The entry point Python script can be an empty Python file for now. We will be using the default inference handling provided by the MXNet container.<\/p>\n<p>The default MXNet container only accepts JSON, CSV, and Numpy arrays as valid input. So you will have to format your input in to one of these three formats. The code below demonstrates how I did it with Numpy arrays:<\/p>\n<pre><code>import cv2\nimport io\n\nnp_array = cv2.imread(filename=img_filename)\nnp_array = np_array.transpose((2,0,1))\nnp_array = np.expand_dims(np_array, axis=0)\n\nbuffer = io.BytesIO()\nnp.save(buffer, np_array)\n\nresponse = sm.invoke_endpoint(EndpointName='Your_Endpoint_name', Body=buffer.getvalue(), ContentType='application\/x-npy')\n<\/code><\/pre>\n<p>Once you have a single endpoint working with MXNet container, you should be able to get it running in multi model endpoint using the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/multi_data_model.html\" rel=\"nofollow noreferrer\">SageMaker MultiDataModel constructor<\/a>.<\/p>\n<p>If you want to use a different input data type so you don't have to do the preprocessing in your application code, you can overwrite the input_fn method in the MxNet container by providing it in the entry_point script. <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/mxnet\/using_mxnet.html\" rel=\"nofollow noreferrer\">See here<\/a> for more information. If you do this, you could pass the image bytes directly to SageMaker, without formatting the numpy arrays.<\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_date":"2021-02-03 19:01:31.027000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":46900593,
        "Question_title":"Azure ML - Import Hive Query Failing - Hive over ADLS",
        "Question_body":"<p>We are working on Azure ML and ADLS combination. Since HDInsight Cluster is working over ADLS, we are trying to use Hive Query and HDFS route and running into problems. \nRequest your help in solving the problem of reading data from hive query and writing to HDFS. Below is the error URL for reference:<\/p>\n\n<p><a href=\"https:\/\/studioapi.azureml.net\/api\/sharedaccess?workspaceId=025ba20578874d7086e6c495cc49a3f2&amp;signature=ZMUCNMwRjlrksrrmsrx5SaGedSgwMmO%2FfSHvq190%2F1I%3D&amp;sharedAccessUri=https%3A%2F%2Fesprodussouth001.blob.core.windows.net%2Fexperimentoutput%2Fccf9a206-730d-4773-b44e-a2dd8c6e87b9%2Fccf9a206-730d-4773-b44e-a2dd8c6e87b9.txt%3Fsv%3D2015-02-21%26sr%3Db%26sig%3DHkuFm8B2Ba1kEWWIwanqlv%2FcQPWVz0XYveSsZnEa0Wg%3D%26st%3D2017-10-16T18%3A31%3A06Z%26se%3D2017-10-17T18%3A36%3A06Z%26sp%3Dr\" rel=\"nofollow noreferrer\">https:\/\/studioapi.azureml.net\/api\/sharedaccess?workspaceId=025ba20578874d7086e6c495cc49a3f2&amp;signature=ZMUCNMwRjlrksrrmsrx5SaGedSgwMmO%2FfSHvq190%2F1I%3D&amp;sharedAccessUri=https%3A%2F%2Fesprodussouth001.blob.core.windows.net%2Fexperimentoutput%2Fccf9a206-730d-4773-b44e-a2dd8c6e87b9%2Fccf9a206-730d-4773-b44e-a2dd8c6e87b9.txt%3Fsv%3D2015-02-21%26sr%3Db%26sig%3DHkuFm8B2Ba1kEWWIwanqlv%2FcQPWVz0XYveSsZnEa0Wg%3D%26st%3D2017-10-16T18%3A31%3A06Z%26se%3D2017-10-17T18%3A36%3A06Z%26sp%3Dr<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2017-10-24 00:55:05.617000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2017-11-12 07:05:14.877000 UTC",
        "Question_score":0,
        "Question_tags":"azure|azure-hdinsight|azure-machine-learning-studio",
        "Question_view_count":64,
        "Owner_creation_date":"2014-12-23 06:43:26.220000 UTC",
        "Owner_last_access_date":"2018-09-15 12:23:34.433000 UTC",
        "Owner_location":null,
        "Owner_reputation":97,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":6,
        "Answer_body":"<p>Azure Machine Learning supports Hive but not over ADLS. <\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2017-10-25 15:22:20.720000 UTC",
        "Answer_last_edit_date":"2017-10-27 19:57:05.963000 UTC",
        "Answer_score":0.0,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":70141714,
        "Question_title":"How to hard delete objects older than n-days in LakeFS?",
        "Question_body":"<p>How to find and hard delete objects older than n-days in LakeFS? Later it'll be a scheduled job.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-11-28 08:23:22.253000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2021-11-28 11:02:06.610000 UTC",
        "Question_score":1,
        "Question_tags":"object-storage|lakefs",
        "Question_view_count":114,
        "Owner_creation_date":"2020-03-03 05:43:31.067000 UTC",
        "Owner_last_access_date":"2022-09-24 15:23:44.207000 UTC",
        "Owner_location":null,
        "Owner_reputation":73,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":14,
        "Answer_body":"<p>To do that you should use the <a href=\"https:\/\/docs.lakefs.io\/reference\/garbage-collection.html\" rel=\"nofollow noreferrer\">Garbage Collection<\/a> (GC) feature in lakeFS.<\/p>\n<p><strong>Note:<\/strong> This feature cleans objects from the storage only after they are deleted from your branches in lakeFS.<\/p>\n<p>You will need to:<\/p>\n<ol>\n<li><p>Define GC rules to set your desired retention period.<\/p>\n<p>From the lakeFS UI, go to the repository you would like to hard delete objects from -&gt; Settings -&gt; Retention, and define the GC rule for each branch under the repository. For example -<\/p>\n<pre><code>{\n    &quot;default_retention_days&quot;: 21,\n    &quot;branches&quot;: [\n        {&quot;branch_id&quot;: &quot;main&quot;, &quot;retention_days&quot;: 28},\n        {&quot;branch_id&quot;: &quot;dev&quot;, &quot;retention_days&quot;: 7}\n    ]\n}\n<\/code><\/pre>\n<\/li>\n<li><p>Run the GC Spark job that does the actual cleanup, with -<\/p>\n<pre><code>spark-submit --class io.treeverse.clients.GarbageCollector \\\n  -c spark.hadoop.lakefs.api.url=https:\/\/lakefs.example.com:8000\/api\/v1  \\\n  -c spark.hadoop.lakefs.api.access_key=&lt;LAKEFS_ACCESS_KEY&gt; \\\n  -c spark.hadoop.lakefs.api.secret_key=&lt;LAKEFS_SECRET_KEY&gt; \\\n  -c spark.hadoop.fs.s3a.access.key=&lt;S3_ACCESS_KEY&gt; \\\n  -c spark.hadoop.fs.s3a.secret.key=&lt;S3_SECRET_KEY&gt; \\\n  --packages io.lakefs:lakefs-spark-client-301_2.12:0.5.0 \\\n  example-repo us-east-1\n<\/code><\/pre>\n<\/li>\n<\/ol>",
        "Answer_comment_count":3.0,
        "Answer_creation_date":"2021-11-28 09:37:29.080000 UTC",
        "Answer_last_edit_date":"2021-11-28 10:34:21.197000 UTC",
        "Answer_score":3.0,
        "Question_valid_tags":[
            "lakefs"
        ]
    },
    {
        "Question_id":60306240,
        "Question_title":"export azure ml studio designer project as jupyter notebook?",
        "Question_body":"<p>I hope I am not missing something obvious here. I am using the new azure ml studio designer. I am able to use to create datasets, train models and use them just fine.<\/p>\n\n<p>azure ml studio allows creation of Jupyter notebooks (also) and use them to do machine learning. I am able to do that too. <\/p>\n\n<p>So, now, I am wondering, can I build my ML pipeline\/experiment in ML studio designer, and once it is in good shape, export it as a python and jupyter notebook? then, use it in the same designer provided notebook option or may be use it locally?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":4,
        "Question_creation_date":"2020-02-19 17:40:46.887000 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":5,
        "Question_tags":"azure|jupyter-notebook|azure-machine-learning-studio|azure-machine-learning-service",
        "Question_view_count":789,
        "Owner_creation_date":"2015-09-15 16:27:17.953000 UTC",
        "Owner_last_access_date":"2022-09-24 06:49:58.907000 UTC",
        "Owner_location":"Bangalore, Karnataka, India",
        "Owner_reputation":2272,
        "Owner_up_votes":1340,
        "Owner_down_votes":67,
        "Owner_views":516,
        "Answer_body":"<p>This is not currently supported, but I am 80% sure it is in the roadmap.\nAn alternative would be to use the SDK to create the same pipeline using <code>ModuleStep<\/code> where  I <em>believe<\/em> you can reference a Designer Module by its name to use it like a <code>PythonScriptStep<\/code><\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2020-02-19 19:12:54.710000 UTC",
        "Answer_last_edit_date":"2020-02-20 23:29:48.597000 UTC",
        "Answer_score":6.0,
        "Question_valid_tags":[
            "azure-machine-learning-studio",
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":69666500,
        "Question_title":"Training Job is Stopping in Sagemaker",
        "Question_body":"<p>Recently, I have changed account on AWS and faced with weird error in Sagemaker.<\/p>\n<p>Basically, I'm just checking <code>xgboost<\/code> algo with some toy dataset in this manner:<\/p>\n<pre><code>from sagemaker import image_uris\n\nxgb_image_uri = image_uris.retrieve(&quot;xgboost&quot;, boto3.Session().region_name, &quot;1&quot;)\n\nclf = sagemaker.estimator.Estimator(xgb_image_uri,\n                   role, 1, 'ml.c4.2xlarge',\n                   output_path=&quot;s3:\/\/{}\/output&quot;.format(session.default_bucket()),\n                   sagemaker_session=session)\n\nclf.fit(location_data)\n<\/code><\/pre>\n<p>Then the training job is starting to be executed but for some reason, on downloading data step it stops the training job and displays the following message:<\/p>\n<pre><code>2021-10-21 17:33:27 Downloading - Downloading input data\n2021-10-21 17:33:27 Stopping - Stopping the training job\n2021-10-21 17:33:27 Stopped - Training job stopped\nProfilerReport-1634837444: Stopping\n..\nJob ended with status 'Stopped' rather than 'Completed'. This could mean the job timed out or stopped early for some other reason: Consider checking whether it completed as you expect.\n<\/code><\/pre>\n<p>Also, when I'm trying to go back to training jobs section and check for logs in cloudwatch there is nothing to be displayed. Is it common issue and who had faced with that? Are there any workarounds?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-10-21 17:40:48.463000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|xgboost|amazon-sagemaker",
        "Question_view_count":361,
        "Owner_creation_date":"2013-12-08 08:33:34.717000 UTC",
        "Owner_last_access_date":"2022-09-21 18:08:28.293000 UTC",
        "Owner_location":null,
        "Owner_reputation":2778,
        "Owner_up_votes":138,
        "Owner_down_votes":1,
        "Owner_views":352,
        "Answer_body":"<p>The problem was most likely with templates for sagemaker that was runned before creating the instance.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-10-21 19:24:58.240000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":56604989,
        "Question_title":"How to install mlflow using pip install",
        "Question_body":"<p>I'm working on a Window 10 machine and trying to pip install mlflow but I'm getting the following error message.<\/p>\n\n<pre><code>THESE PACKAGES DO NOT MATCH THE HASHES FROM THE REQUIREMENTS FILE. If you have updated the package versions, please update the hashes. Otherwise, examine the package contents carefully; someone may have tampered with them.\nmlflow from https:\/\/files.pythonhosted.org\/packages\/01\/ec\/8c9448968d4662e8354b9c3a62e635f8929ed507a45af3d9fdb84be51270\/mlflow-1.0.0-py3-none-any.whl#sha256=0f2f116a377b9da538642eaf688caa0a7166ee1ede30c8734830eb9e789574b4:\n    Expected sha256 0f2f116a377b9da538642eaf688caa0a7166ee1ede30c8734830eb9e789574b4\n         Got        eb34ea16ecfe02d474ce50fd1f88aba82d56dcce9e8fdd30193ab39edf32ac9e\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_date":"2019-06-14 20:52:53.400000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2019-06-17 19:29:45.057000 UTC",
        "Question_score":1,
        "Question_tags":"pip|mlflow",
        "Question_view_count":365,
        "Owner_creation_date":"2012-12-12 20:12:11.933000 UTC",
        "Owner_last_access_date":"2022-04-05 02:26:00.750000 UTC",
        "Owner_location":null,
        "Owner_reputation":5655,
        "Owner_up_votes":73,
        "Owner_down_votes":3,
        "Owner_views":629,
        "Answer_body":"<p>It is trying to check cache for packages. They were likely compiled in linux or some other OS and you are trying to install them in Windows.<\/p>\n\n<p>This should fix your issue:<\/p>\n\n<pre><code>pip install --no-cache-dir mlflow\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2019-06-14 20:57:49.140000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "mlflow"
        ]
    },
    {
        "Question_id":69676225,
        "Question_title":"GCP AI Platform API - Object Detection Metrics at Class Level (Python)",
        "Question_body":"<p>I have trained a AutoML Object Detection model in Vertex AI (a service under AI Platform in GCP). I am trying to access model evaluation metrics for each label (precision, recall, accuracy etc.) for varying Confidence Score Threshold and IoU Threshold.<\/p>\n<p>However, I am stuck at step one, even to get model's aggerate performance metric much less to the performance metric at granular levels. I have followed <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/evaluating-automl-models?authuser=1#aggregate\" rel=\"nofollow noreferrer\">this instruction<\/a> But I cannot seem to figure out what is <code>evaluation_id<\/code> (also see the official sample code snippet <a href=\"https:\/\/github.com\/googleapis\/python-aiplatform\/blob\/main\/samples\/snippets\/model_service\/get_model_evaluation_image_object_detection_sample.py\" rel=\"nofollow noreferrer\">here<\/a>), which is:<\/p>\n<pre><code>def get_model_evaluation_image_object_detection_sample(\n    project: str,\n    model_id: str,\n    evaluation_id: str,\n    location: str = &quot;us-central1&quot;,\n    api_endpoint: str = &quot;us-central1-aiplatform.googleapis.com&quot;,\n):\n    # The AI Platform services require regional API endpoints.\n    client_options = {&quot;api_endpoint&quot;: api_endpoint}\n    # Initialize client that will be used to create and send requests.\n    # This client only needs to be created once, and can be reused for multiple requests.\n    client = aiplatform.gapic.ModelServiceClient(client_options=client_options)\n    name = client.model_evaluation_path(\n        project=project, location=location, model=model_id, evaluation=evaluation_id\n    )\n    response = client.get_model_evaluation(name=name)\n    print(&quot;response:&quot;, response)\n<\/code><\/pre>\n<p>After sometime I have figured out that for model trained in EU,  and <code>api_endpoint<\/code> shall be passed as:<\/p>\n<pre><code>location: str = &quot;europe-west4&quot;\napi_endpoint: str = &quot;europe-west4-aiplatform.googleapis.com&quot;\n<\/code><\/pre>\n<p>But whatever I try for <code>evaluation_id<\/code> leads to the following errors:<\/p>\n<pre><code>InvalidArgument: 400 List of found errors:  1.Field: name; Message: Invalid ModelEvaluation resource name.\n<\/code><\/pre>\n<p>There in the documentation it says (which is seems it contains what I need):<\/p>\n<blockquote>\n<p>For the bounding box metric, Vertex AI returns an array of metric\nvalues at different IoU threshold values (between 0 and 1) and\nconfidence threshold values (between 0 and 1). For example, you can\nnarrow in on evaluation metrics at an IoU threshold of 0.85 and a\nconfidence threshold of 0.8228. By viewing these different threshold\nvalues, you can see how they affect other metrics such as precision\nand recall.<\/p>\n<\/blockquote>\n<p>Without knowing that is contained in the output array, how would that work for each class? Basically I need for each class the model metrics for varying IoU threshold values and confidence threshold.<\/p>\n<p>Also I have tried to query from AutoML API instead, like:<\/p>\n<pre><code>client_options = {'api_endpoint': 'eu-automl.googleapis.com:443'}\n\nclient = automl.AutoMlClient(client_options=client_options)\n# Get the full path of the model.\nmodel_full_id = client.model_path(project_id, &quot;europe-west4&quot;, model_id)\n\nprint(&quot;List of model evaluations:&quot;)\nfor evaluation in client.list_model_evaluations(parent=model_full_id, filter=&quot;&quot;):\n    print(&quot;Model evaluation name: {}&quot;.format(evaluation.name))\n    print(&quot;Model annotation spec id: {}&quot;.format(evaluation.annotation_spec_id))\n    print(&quot;Create Time: {}&quot;.format(evaluation.create_time))\n    print(&quot;Evaluation example count: {}&quot;.format(evaluation.evaluated_example_count))\n    print(\n        &quot;Classification model evaluation metrics: {}&quot;.format(\n            evaluation.classification_evaluation_metrics\n        )\n    )\n<\/code><\/pre>\n<p>No surprise, also this doesn't work, and leads to:<\/p>\n<pre><code>InvalidArgument: 400 List of found errors:  1.Field: parent; Message: The provided location ID doesn't match the endpoint. For automl.googleapis.com, the valid location ID is `us-central1`. For eu-automl.googleapis.com, the valid location ID is `eu`.\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-10-22 11:37:21.630000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"google-cloud-platform|google-cloud-automl|google-cloud-vertex-ai",
        "Question_view_count":298,
        "Owner_creation_date":"2017-03-29 13:05:41.113000 UTC",
        "Owner_last_access_date":"2022-09-23 09:40:26.750000 UTC",
        "Owner_location":"Berlin, Germany",
        "Owner_reputation":423,
        "Owner_up_votes":897,
        "Owner_down_votes":4,
        "Owner_views":84,
        "Answer_body":"<p>I was able to get the response of the model evaluation using <a href=\"https:\/\/googleapis.dev\/python\/aiplatform\/latest\/aiplatform.html\" rel=\"nofollow noreferrer\">aiplatform_v1<\/a> which is well documented and this is the reference linked from the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/start\/client-libraries?authuser=1#client_libraries\" rel=\"nofollow noreferrer\">Vertex AI reference page<\/a>.<\/p>\n<p>On this script I ran <a href=\"https:\/\/googleapis.dev\/python\/aiplatform\/latest\/aiplatform_v1\/model_service.html#google.cloud.aiplatform_v1.services.model_service.ModelServiceClient.list_model_evaluations\" rel=\"nofollow noreferrer\">list_model_evaluations()<\/a> to get the evaluation name and used it as input for <a href=\"https:\/\/googleapis.dev\/python\/aiplatform\/latest\/aiplatform_v1\/model_service.html#google.cloud.aiplatform_v1.services.model_service.ModelServiceClient.get_model_evaluation\" rel=\"nofollow noreferrer\">get_model_evaluation()<\/a> which will return the evaluation details for Confidence Score Threshold, IoU Threshold, etc.<\/p>\n<p>NOTE: I don't have a trained model in <code>europe-west4<\/code> so I used <code>us-central1<\/code> instead. But if you have trained in <code>europe-west4<\/code> you should use <code>https:\/\/europe-west4-aiplatform.googleapis.com<\/code> as <code>api_endpoint<\/code> as per <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/general\/locations#specifying_the_location_using_the\" rel=\"nofollow noreferrer\">location document<\/a>.<\/p>\n<pre><code>from google.cloud import aiplatform_v1 as aiplatform\n\napi_endpoint = 'us-central1-aiplatform.googleapis.com'\nclient_options = {&quot;api_endpoint&quot;: api_endpoint}\nclient_model = aiplatform.services.model_service.ModelServiceClient(client_options=client_options)\nproject_id = 'your-project-id'\nlocation = 'us-central1'\nmodel_id = '999999999999'\n\nmodel_name = f'projects\/{project_id}\/locations\/{location}\/models\/{model_id}'\nlist_eval_request = aiplatform.types.ListModelEvaluationsRequest(parent=model_name)\nlist_eval = client_model.list_model_evaluations(request=list_eval_request)\neval_name=''\nfor val in list_eval:\n    eval_name = val.name\n\nget_eval_request = aiplatform.types.GetModelEvaluationRequest(name=eval_name)\nget_eval = client_model.get_model_evaluation(request=get_eval_request)\nprint(get_eval)\n<\/code><\/pre>\n<p>See response snippet:<\/p>\n<pre><code>name: &quot;projects\/xxxxxxxxx\/locations\/us-central1\/models\/999999999999\/evaluations\/1234567890&quot;\nmetrics_schema_uri: &quot;gs:\/\/google-cloud-aiplatform\/schema\/modelevaluation\/image_object_detection_metrics_1.0.0.yaml&quot;\nmetrics {\n  struct_value {\n    fields {\n      key: &quot;boundingBoxMeanAveragePrecision&quot;\n      value {\n        number_value: 0.20201288\n      }\n    }\n    fields {\n      key: &quot;boundingBoxMetrics&quot;\n      value {\n        list_value {\n          values {\n            struct_value {\n              fields {\n                key: &quot;confidenceMetrics&quot;\n                value {\n                  list_value {\n                    values {\n                      struct_value {\n                        fields {\n                          key: &quot;confidenceThreshold&quot;\n                          value {\n                            number_value: 0.06579724\n                          }\n                        }\n                        fields {\n                          key: &quot;f1Score&quot;\n                          value {\n                            number_value: 0.15670435\n                          }\n                        }\n                        fields {\n                          key: &quot;precision&quot;\n                          value {\n                            number_value: 0.09326923\n                          }\n                        }\n                        fields {\n                          key: &quot;recall&quot;\n                          value {\n                            number_value: 0.48989898\n                          }\n                        }\n                      }\n                    }\n                    values {\n                      struct_value {\n....\n<\/code><\/pre>\n<p><strong>EDIT 1: Get response per class<\/strong><\/p>\n<p>To get metrics per class, you can use <a href=\"https:\/\/googleapis.dev\/python\/aiplatform\/latest\/aiplatform_v1\/model_service.html#google.cloud.aiplatform_v1.services.model_service.ModelServiceClient.list_model_evaluation_slices\" rel=\"nofollow noreferrer\">list_model_evaluation_slices()<\/a> to get the name for each class, then use the name to <a href=\"https:\/\/googleapis.dev\/python\/aiplatform\/latest\/aiplatform_v1\/model_service.html#google.cloud.aiplatform_v1.services.model_service.ModelServiceClient.get_model_evaluation_slice\" rel=\"nofollow noreferrer\">get_model_evaluation_slice()<\/a>. In this code I pushed the names to a list since I have multiple classes. Then just use the values stored in the array to get the metric per class.<\/p>\n<p>In my code I used <code>label[0]<\/code> to get a single response from this class.<\/p>\n<pre><code>from google.cloud import aiplatform_v1 as aiplatform\n\napi_endpoint = 'us-central1-aiplatform.googleapis.com'\nclient_options = {&quot;api_endpoint&quot;: api_endpoint}\nclient_model = aiplatform.services.model_service.ModelServiceClient(client_options=client_options)\nproject_id = 'your-project-id'\nlocation = 'us-central1'\nmodel_id = '999999999999'\n\nmodel_name = f'projects\/{project_id}\/locations\/{location}\/models\/{model_id}'\nlist_eval_request = aiplatform.types.ListModelEvaluationsRequest(parent=model_name)\nlist_eval = client_model.list_model_evaluations(request=list_eval_request)\neval_name=''\nfor val in list_eval:\n    eval_name = val.name\n\nlabel=[]\nslice_eval_request = aiplatform.types.ListModelEvaluationSlicesRequest(parent=eval_name)\nslice_eval = client_model.list_model_evaluation_slices(request=slice_eval_request)\nfor data in slice_eval:\n    label.append(data.name)\n\nget_eval_slice_request = aiplatform.types.GetModelEvaluationSliceRequest(name=label[0])\nget_eval_slice = client_model.get_model_evaluation_slice(request=get_eval_slice_request)\nprint(get_eval_slice)\n<\/code><\/pre>\n<p>Print all classes:\n<a href=\"https:\/\/i.stack.imgur.com\/pinWU.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/pinWU.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Classes in UI:\n<a href=\"https:\/\/i.stack.imgur.com\/mXlIW.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/mXlIW.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Response snippet for a class:<\/p>\n<pre><code>name: &quot;projects\/xxxxxxxxx\/locations\/us-central1\/models\/999999999\/evaluations\/0000000000\/slices\/777777777&quot;\nslice_ {\n  dimension: &quot;annotationSpec&quot;\n  value: &quot;Cheese&quot;\n}\nmetrics_schema_uri: &quot;gs:\/\/google-cloud-aiplatform\/schema\/modelevaluation\/image_object_detection_metrics_1.0.0.yaml&quot;\nmetrics {\n  struct_value {\n    fields {\n      key: &quot;boundingBoxMeanAveragePrecision&quot;\n      value {\n        number_value: 0.14256561\n      }\n    }\n    fields {\n      key: &quot;boundingBoxMetrics&quot;\n      value {\n        list_value {\n          values {\n            struct_value {\n              fields {\n                key: &quot;confidenceMetrics&quot;\n                value {\n                  list_value {\n                    values {\n                      struct_value {\n                        fields {\n                          key: &quot;confidenceThreshold&quot;\n                          value {\n                            number_value: 0.06579724\n                          }\n                        }\n                        fields {\n                          key: &quot;f1Score&quot;\n                          value {\n                            number_value: 0.10344828\n                          }\n                        }\n                        fields {\n                          key: &quot;precision&quot;\n                          value {\n                            number_value: 0.06198347\n                          }\n                        }\n....\n<\/code><\/pre>",
        "Answer_comment_count":3.0,
        "Answer_creation_date":"2021-10-25 02:40:16.533000 UTC",
        "Answer_last_edit_date":"2021-10-25 08:38:55.017000 UTC",
        "Answer_score":2.0,
        "Question_valid_tags":[
            "google-cloud-vertex-ai"
        ]
    },
    {
        "Question_id":52370381,
        "Question_title":"Azure ML ModelManagement web service update",
        "Question_body":"<p><strong>NOTICE: Azure Machine Learning Workbench (Preview) is deprecated. The workflow for deploying models, images and services has been updated since this question was posted.<\/strong><\/p>\n\n<p>I have been developing a Machine Learning model for Azure Machine Learning Services using Azure Machine Learning Workbench (Preview). I successfully managed to deploy the model as a web service, as instructed in <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/desktop-workbench\/model-management-service-deploy\" rel=\"nofollow noreferrer\">Azure Machine Learning Documentation (Preview)<\/a>. I have managed to get the service up and running, and the model, manifest and images are all configured correctly. So far so good.<\/p>\n\n<p>But now I have come to the phase where I want to be able to update the service with new configurations. And this is where I find myself with more questions than answers. <\/p>\n\n<p>I have figured out that I can<\/p>\n\n<ol>\n<li>configure a new model<\/li>\n<li>configure a new manifest pointing to that model<\/li>\n<li>configure a new image pointing to that manifest<\/li>\n<li>update an existing (or create a new) service to point to the new image<\/li>\n<\/ol>\n\n<p>This seems reasonable enough. But what If I just need to update the manifest, would it be possible to skip the configuration of a new model (1), and just begin the update from (2) above, and let it point to an existing model instead of a new one?<\/p>\n\n<p>I have of course tried this by calling the following from the CLI, and I get stuck with the following output:<\/p>\n\n<pre><code>&gt;&gt; az ml manifest create --manifest-name manifestname -f score.py -r python -c aml_config\/conda_dependencies.yml -s outputs\/schema.json -i [existing-model-id]\nCreating new driver at \/var\/folders\/tmp\/tmp.py\nSuccessfully created manifest\nId: [manifest-id]\n&gt;&gt; az ml image create -n imagename --manifest-id [manifest-id-from-above]\nCreating image............................................Done.\nImage ID: [image-id]\n&gt;&gt; az ml service update realtime -i [existing-service-id] --image-id [image-id-from-above] -v\nUpdating service..................................Failed\nFound default kubeconfig in \/Users\/username\/.kube\/config using it\nUsing kubeconfig file: \/Users\/username\/.kube\/config\nKubectl exists in default location, adding it to PATH\nloading kubeconfig file\nGetting Replica sets from default namespace\nGot hash ####\n{\n    \"Azure-cli-ml Version\": null,\n    \"Error\": \"Error occurred\",\n    \"Response Content\": {\n        \"CreatedTime\": \"2018-09-17T13:31:22.4230543Z\",\n        \"EndTime\": \"2018-09-17T13:34:18.0774994Z\",\n        \"Error\": {\n            \"Code\": \"KubernetesDeploymentFailed\",\n            \"Details\": [\n                {\n                    \"Code\": \"CrashLoopBackOff\",\n                    \"Message\": \"Back-off 40s restarting failed container=### pod=###\"\n                }\n            ],\n            \"Message\": \"Kubernetes Deployment failed\",\n            \"StatusCode\": 400\n        },\n        \"Id\": \"###\",\n        \"OperationType\": \"Service\",\n        \"ResourceLocation\": \"###\",\n        \"State\": \"Failed\"\n    },\n    \"Response Headers\": {\n        \"Connection\": \"keep-alive\",\n        \"Content-Encoding\": \"gzip\",\n        \"Content-Type\": \"application\/json; charset=utf-8\",\n        \"Date\": \"Mon, 17 Sep 2018 13:34:22 GMT\",\n        \"Strict-Transport-Security\": \"max-age=15724800; includeSubDomains; preload\",\n        \"Transfer-Encoding\": \"chunked\",\n        \"X-Content-Type-Options\": \"nosniff\",\n        \"X-Frame-Options\": \"SAMEORIGIN\",\n        \"api-supported-versions\": \"2017-09-01-preview, 2018-04-01-preview\",\n        \"x-ms-client-request-id\": \"###\",\n        \"x-ms-client-session-id\": \"\"\n    }\n}\n<\/code><\/pre>\n\n<p>If I try to rollback to the previous manifest, there is no error message, and everything works just fine. This makes me assume there is something wrong with my new manifest and\/or image. There is no warning or error when creating them, however.<\/p>\n\n<p>I have tried searching for the error messages but I find nothing.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2018-09-17 14:47:32.417000 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":"2018-10-03 08:11:29.270000 UTC",
        "Question_score":0,
        "Question_tags":"azure|azure-machine-learning-studio|azure-machine-learning-workbench",
        "Question_view_count":537,
        "Owner_creation_date":"2016-05-20 15:01:49.237000 UTC",
        "Owner_last_access_date":"2022-09-05 14:52:13.070000 UTC",
        "Owner_location":"Uppsala, Sverige",
        "Owner_reputation":400,
        "Owner_up_votes":146,
        "Owner_down_votes":0,
        "Owner_views":43,
        "Answer_body":"<p>CrashLoopBackOff error normally means that the init() function of your score.py file has a problem, for example, finding or loading the model. It could also mean you are using a library that hasn't been imported.\nAzure ML just announced an update to the preview with an updated Python SDK (<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/quickstart-get-started\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/quickstart-get-started<\/a>). \nThere are tutorials and notebooks that show the process in more details with examples. I would start there.<\/p>\n\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/tutorial-deploy-models-with-aml\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/tutorial-deploy-models-with-aml<\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2018-09-25 22:32:47.653000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_valid_tags":[
            "azure-machine-learning-studio",
            "azure-machine-learning-workbench"
        ]
    },
    {
        "Question_id":72096297,
        "Question_title":"Hyperparameter tuning job In Sagemaker with cross valdiation",
        "Question_body":"<p>I managed to get something <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/automatic-model-tuning-ex-tuning-job.html\" rel=\"nofollow noreferrer\">along those lines<\/a> to work. This is great but to be more on the save side (i.e. not rely too much on the train validation split) one should really use cross validation. I am curious, if this can also be achieved via Sagemaker hyperparameter tuning jobs? I googled extensively ...<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_date":"2022-05-03 07:52:01.897000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|cross-validation|amazon-sagemaker|hyperparameters",
        "Question_view_count":70,
        "Owner_creation_date":"2010-03-01 10:53:04.443000 UTC",
        "Owner_last_access_date":"2022-09-24 18:56:19.313000 UTC",
        "Owner_location":"Somewhere",
        "Owner_reputation":15705,
        "Owner_up_votes":2171,
        "Owner_down_votes":91,
        "Owner_views":2150,
        "Answer_body":"<p>It is not possible through HPO.<\/p>\n<p>You need to add additional step in your workflow to achieve cross-validation.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2022-05-04 02:05:55.790000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":58018893,
        "Question_title":"How do S3 file download and estimator.fit() work in this blog post?",
        "Question_body":"<h1>Difficulty in understanding<\/h1>\n<p>Q2) How to download a file from S3?<\/p>\n<p><strong>From<\/strong>  <a href=\"https:\/\/medium.com\/akeneo-labs\/machine-learning-workflow-with-sagemaker-b83b293337ff\" rel=\"nofollow noreferrer\">The Machine Learning Workflow with SageMaker<\/a><\/p>\n<p>And also why are we using this piece of code?<\/p>\n<p><code>estimator.fit(train_data_location)<\/code><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-09-19 21:03:01.497000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-06-20 09:12:55.060000 UTC",
        "Question_score":1,
        "Question_tags":"python|amazon-web-services|machine-learning|amazon-s3|amazon-sagemaker",
        "Question_view_count":220,
        "Owner_creation_date":"2019-04-28 11:46:27.417000 UTC",
        "Owner_last_access_date":"2022-07-28 06:01:52.983000 UTC",
        "Owner_location":"India",
        "Owner_reputation":1309,
        "Owner_up_votes":84,
        "Owner_down_votes":20,
        "Owner_views":288,
        "Answer_body":"<h2>Downloading a file from S3:<\/h2>\n\n<p>This code block in the Q2 section defines the function that downloads a file from S3. The user instantiates an S3 client, and then passes the S3 URL along to the <code>s3.Bucket.download_file()<\/code> method.<\/p>\n\n<pre><code>def download_from_s3(url):\n    \"\"\"ex: url = s3:\/\/sagemakerbucketname\/data\/validation.tfrecords\"\"\"\n    url_parts = url.split(\"\/\")  # =&gt; ['s3:', '', 'sagemakerbucketname', 'data', ...\n    bucket_name = url_parts[2]\n    key = os.path.join(*url_parts[3:])\n    filename = url_parts[-1]\n    if not os.path.exists(filename):\n        try:\n            # Create an S3 client\n            s3 = boto3.resource('s3')\n            print('Downloading {} to {}'.format(url, filename))\n            s3.Bucket(bucket_name).download_file(key, filename)\n        except botocore.exceptions.ClientError as e:\n            if e.response['Error']['Code'] == \"404\":\n                print('The object {} does not exist in bucket {}'.format(\n                    key, bucket_name))\n            else:\n                raise\n<\/code><\/pre>\n\n<h2>Estimator.fit() explanation:<\/h2>\n\n<p>The <code>estimator.fit(train_data_location)<\/code> line is what initiates the training process with SageMaker. When run, SageMaker will provision the necessary infrastructure, fetch the data from the location the user designated (here, <code>train_data_location<\/code> which is a path to Amazon S3) and distribute it amongst the training cluster, carry out the training process, return the resulting model, and tear down the training infrastructure. <\/p>\n\n<p>You can find the result of this training job in the SageMaker console.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2019-09-19 21:28:42.077000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":48650152,
        "Question_title":"AWS uploading file into wrong bucket",
        "Question_body":"<p>I am using AWS Sagemaker and trying to upload a data folder into S3 from Sagemaker. I am trying to do is to upload my data into the s3_train_data directory (the directory exists in S3). However, it wouldn't upload it in that bucket, but in a default Bucket that has been created, and in turn creates a new folder directory with the S3_train_data variables.<\/p>\n\n<p>code to input in directory<\/p>\n\n<pre><code>import os\nimport sagemaker\nfrom sagemaker import get_execution_role\n\nsagemaker_session = sagemaker.Session()\nrole = get_execution_role()\n\nbucket = &lt;bucket name&gt;\nprefix = &lt;folders1\/folders2&gt;\nkey = &lt;input&gt;\n\n\ns3_train_data = 's3:\/\/{}\/{}\/{}\/'.format(bucket, prefix, key)\n\n\n#path 'data' is the folder in the Jupyter Instance, contains all the training data\ninputs = sagemaker_session.upload_data(path= 'data', key_prefix= s3_train_data)\n<\/code><\/pre>\n\n<p>Is the problem in the code or more in how I created the notebook?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2018-02-06 19:03:37.340000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2018-05-06 15:50:37.033000 UTC",
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|amazon-sagemaker",
        "Question_view_count":722,
        "Owner_creation_date":"2017-10-30 19:41:59.217000 UTC",
        "Owner_last_access_date":"2019-03-16 23:32:36.337000 UTC",
        "Owner_location":null,
        "Owner_reputation":33,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":5,
        "Answer_body":"<p>You could look at the Sample notebooks, how to upload the data S3 bucket \nThere have many ways. I am just giving you hints to answer. \nAnd you forgot create a boto3 session to access the S3 bucket <\/p>\n\n<p><strong>It is one of the ways to do it.<\/strong> <\/p>\n\n<pre><code>import os \nimport urllib.request\nimport boto3\n\ndef download(url):\n    filename = url.split(\"\/\")[-1]\n    if not os.path.exists(filename):\n        urllib.request.urlretrieve(url, filename)\n\n\ndef upload_to_s3(channel, file):\n    s3 = boto3.resource('s3')\n    data = open(file, \"rb\")\n    key = channel + '\/' + file\n    s3.Bucket(bucket).put_object(Key=key, Body=data)\n\n\n# caltech-256\ndownload('http:\/\/data.mxnet.io\/data\/caltech-256\/caltech-256-60-train.rec')\nupload_to_s3('train', 'caltech-256-60-train.rec')\ndownload('http:\/\/data.mxnet.io\/data\/caltech-256\/caltech-256-60-val.rec')\nupload_to_s3('validation', 'caltech-256-60-val.rec')\n<\/code><\/pre>\n\n<p>link : <a href=\"https:\/\/buildcustom.notebook.us-east-2.sagemaker.aws\/notebooks\/sample-notebooks\/introduction_to_amazon_algorithms\/imageclassification_caltech\/Image-classification-fulltraining.ipynb\" rel=\"nofollow noreferrer\">https:\/\/buildcustom.notebook.us-east-2.sagemaker.aws\/notebooks\/sample-notebooks\/introduction_to_amazon_algorithms\/imageclassification_caltech\/Image-classification-fulltraining.ipynb<\/a><\/p>\n\n<p><strong>Another way to do it.<\/strong> <\/p>\n\n<pre><code>bucket = '&lt;your_s3_bucket_name_here&gt;'# enter your s3 bucket where you will copy data and model artifacts\nprefix = 'sagemaker\/breast_cancer_prediction' # place to upload training files within the bucket\n# do some processing then prepare to push the data. \n\nf = io.BytesIO()\nsmac.write_numpy_to_dense_tensor(f, train_X.astype('float32'), train_y.astype('float32'))\nf.seek(0)\n\nboto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train', train_file)).upload_fileobj(f)\n<\/code><\/pre>\n\n<p>Link : <a href=\"https:\/\/buildcustom.notebook.us-east-2.sagemaker.aws\/notebooks\/sample-notebooks\/introduction_to_applying_machine_learning\/breast_cancer_prediction\/Breast%20Cancer%20Prediction.ipynb\" rel=\"nofollow noreferrer\">https:\/\/buildcustom.notebook.us-east-2.sagemaker.aws\/notebooks\/sample-notebooks\/introduction_to_applying_machine_learning\/breast_cancer_prediction\/Breast%20Cancer%20Prediction.ipynb<\/a><\/p>\n\n<p>Youtube link : <a href=\"https:\/\/www.youtube.com\/watch?v=-YiHPIGyFGo\" rel=\"nofollow noreferrer\">https:\/\/www.youtube.com\/watch?v=-YiHPIGyFGo<\/a> - how to pull the data in S3 bucket.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2018-02-08 13:46:57.247000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":44416344,
        "Question_title":"Azure Machine Learning Studio Custom Module Upload Error 0114 : An item with the same key has already been added",
        "Question_body":"<p>When trying to upload a custom R module to Azure Machine Learning Studio what causes the following error.<\/p>\n\n<blockquote>\n  <p>[ModuleOutput]<\/p>\n<\/blockquote>\n\n<pre><code>\"ErrorId\":\"BuildCustomModuleFailed\",\"ErrorCode\":\"0114\",\"ExceptionType\":\"ModuleException\",\"Message\":\"Error 0114: Custom module build failed with error(s): An item with the same key has already been added.\"}} [ModuleOutput] Error: Error 0114: Custom module build failed with error(s): An item with the same key has already been added. \n<\/code><\/pre>\n\n<p>I have tried renaming the module so a name that does not exists.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2017-06-07 15:03:41.797000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"azure-machine-learning-studio",
        "Question_view_count":75,
        "Owner_creation_date":"2011-09-26 15:52:22.823000 UTC",
        "Owner_last_access_date":"2022-09-22 19:57:35.203000 UTC",
        "Owner_location":"Franklin, TN",
        "Owner_reputation":8183,
        "Owner_up_votes":598,
        "Owner_down_votes":2,
        "Owner_views":727,
        "Answer_body":"<p>The duplicate key exception is a red herring. <a href=\"https:\/\/msdn.microsoft.com\/en-us\/library\/azure\/dn962112.aspx\" rel=\"nofollow noreferrer\" title=\"MSDN Module Error Code 0114\">Build error 0114<\/a> is a general error that occurs if there is a system exception while building the custom module. The real issue my module was compressed using the built in compress folder option in the Mac Finder. To fix this compress the file using the command line interface for <code>zip<\/code> in Terminal in the following very specific manner.<\/p>\n\n<blockquote>\n  <p>The following example:<\/p>\n<\/blockquote>\n\n<pre><code>cd ScoredDatasetMetadata\/\nzip ScoredDatasetMetadata *\nmv ScoredDatasetMetadata.zip ..\/\n<\/code><\/pre>\n\n<p>Builds a zip file with the correct file structure.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2017-06-07 15:03:41.797000 UTC",
        "Answer_last_edit_date":"2017-06-07 16:01:12.470000 UTC",
        "Answer_score":0.0,
        "Question_valid_tags":[
            "azure-machine-learning-studio"
        ]
    },
    {
        "Question_id":55752427,
        "Question_title":"How to filter and list all objects in s3 folder by a certain size using python",
        "Question_body":"<p>I am trying to get all the files that are a specified size within a folder of an s3 bucket. How do I go about iterating through the bucket and filtering the files by the specified size? I also want to return the file names of those with the correct size.<\/p>\n\n<pre><code>s3 = boto3.client('s3')\ns3.list_objects_v2(Bucket = 'my-images')\n<\/code><\/pre>\n\n<p>A sample output is <\/p>\n\n<pre><code> u'Key': u'detail\/01018535.jpg',\n   u'LastModified': datetime.datetime(2019, 1, 23, 0, 48, 41, tzinfo=tzlocal()),\n   u'Size': 13535,\n   u'StorageClass': 'STANDARD'},\n  {u'ETag': '\"cd65991a1c6f118e8b036208a30028a7\"',\n   u'Key': u'detail\/0119AF2.jpg',\n   u'LastModified': datetime.datetime(2019, 1, 10, 17, 17, tzinfo=tzlocal()),\n   u'Size': 12984,\n   u'StorageClass': 'STANDARD'}\n<\/code><\/pre>\n\n<p>for instance lets say that I would want a search for a size of 12984.\nThen it would return the 'Key'<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2019-04-18 19:12:59.330000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"python|amazon-web-services|amazon-s3|amazon-sagemaker",
        "Question_view_count":4330,
        "Owner_creation_date":"2019-02-08 14:45:30.877000 UTC",
        "Owner_last_access_date":"2020-02-14 01:16:21.643000 UTC",
        "Owner_location":"El Paso, TX, USA",
        "Owner_reputation":17,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":10,
        "Answer_body":"<p>If you are looking to use boto3, I use this function to find zero byte objects. You can tweak it to your needs by filtering on specific size<\/p>\n\n<pre><code>import boto3\n\ndef get_empty_objects(bucket_name, prefixes):\n    \"\"\"\n    get list of objects from a given s3 prefix recursively\n    \"\"\"\n    results = []\n    for prefix in prefixes:\n        s3client = boto3.client('s3')\n        paginator = s3client.get_paginator(\"list_objects_v2\")\n        paginator_result = paginator.paginate(\n            Bucket=bucket_name, Prefix=prefix)\n        try:\n            for object in paginator_result.search('Contents'):\n                if object['Size'] == 0:\n                    results.append(\"s3:\/\/\" + bucket_name + \"\/\" + object['Key'])\n        except Exception as err:\n            print(\"&gt;&gt;&gt; Error processing objects of [s3:\/\/\" + bucket_name +\n                  \"\/\" + prefix + \"] - \" + str(err))\n        print(\"&gt;&gt;&gt; Returning \" + str(len(results)) + \" objects for [s3:\/\/\" + bucket_name + \"\/\" + prefix + \"]\")\n    return results\n<\/code><\/pre>\n\n<p>Usage:<\/p>\n\n<pre><code>get_empty_objects(\"mybucket\", [\"prefix1\/\", \"prefix2\/\"])\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2019-04-19 19:25:34.873000 UTC",
        "Answer_last_edit_date":"2019-04-19 19:40:03.203000 UTC",
        "Answer_score":2.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":59833019,
        "Question_title":"memory error while writing a large dataframe to S3 AWS",
        "Question_body":"<p>I have created a dataframe with the following shape using amazon sagemaker.<\/p>\n\n<pre><code>10612611 rows \u00d7 4 columns\n<\/code><\/pre>\n\n<p>All are numeric values.\n When I am trying to write this dataframe into my S3 bucket as follows, I get memory error.<\/p>\n\n<pre><code>bytes_to_write = df.to_csv(None).encode()\nwith s3.open('aws-athena-query-results-xxxxxxx\/query_result\/xx.csv','wb') as f:\n    f.write(bytes_to_write)\n<\/code><\/pre>\n\n<blockquote>\n  <p>MemoryError:<\/p>\n<\/blockquote>\n\n<p>I am using <strong>ml.t2.medium<\/strong> for sagemaker instance.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-01-21 01:28:52.453000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|amazon-s3|amazon-sagemaker",
        "Question_view_count":414,
        "Owner_creation_date":"2017-06-15 00:55:05.307000 UTC",
        "Owner_last_access_date":"2022-09-23 03:07:45.110000 UTC",
        "Owner_location":null,
        "Owner_reputation":1237,
        "Owner_up_votes":214,
        "Owner_down_votes":5,
        "Owner_views":376,
        "Answer_body":"<p>I solved this issue by changing the instance type from <strong>ml.t2.medium<\/strong> to <strong>ml.t2.2xlarge<\/strong> and it worked perfectly.<\/p>\n\n<p>The original issue was with the RAM of the instance type and not with S3.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-01-21 03:29:39.867000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":71953876,
        "Question_title":"How to create Sagemaker studio project using aws cdk",
        "Question_body":"<p>I am trying to create sagemaker studio project using aws cdk following below steps:<\/p>\n<p>create domain (<a href=\"https:\/\/github.com\/aws-samples\/aws-cdk-sagemaker-studio.git\" rel=\"nofollow noreferrer\">using this example<\/a>)\ncreate user (<a href=\"https:\/\/github.com\/aws-samples\/aws-cdk-sagemaker-studio.git\" rel=\"nofollow noreferrer\">using this example<\/a>)\ncreate jupyter app\ncreate project<\/p>\n<p>Code for creating jupyter app:<\/p>\n<pre><code>\ndef __init__(self, scope: Construct,\n             construct_id: str, *,\n             app_name: str,\n             app_type: str,\n             domain_id: str,\n             user_profile_name: str,\n             depends_on=None, **kwargs) -&gt; None:\n    super().__init__(scope, construct_id)\n\n    sagemaker_jupyter_app = sg.CfnApp(self, construct_id,\n                                      app_name=app_name,\n                                      app_type=app_type,\n                                      domain_id=domain_id,\n                                      user_profile_name=user_profile_name\n                                      )\n    sagemaker_jupyter_app.add_depends_on(depends_on_user_creation)\n<\/code><\/pre>\n<p>Code for creating project:<\/p>\n<pre><code>\ndef __init__(self, scope: Construct,\n             construct_id: str, *,\n             project_name: str,\n             project_description: str,\n             product_id: str,\n             depends_on=None,\n             **kwargs) -&gt; None:\n    super().__init__(scope, construct_id)\n\n    sagemaker_studio_project = sg.CfnProject(self, construct_id,\n                                             project_name=project_name,\n                                             service_catalog_provisioning_details={\n                                                 &quot;ProductId&quot;: &quot;prod-7tjedn5dz4jrw&quot;\n                                             },\n                                             project_description=project_description\n                                             )\n<\/code><\/pre>\n<p>Domain, user, jupyter app all gets created successfully. The problem comes in with project.\nBelow is the error :<\/p>\n<blockquote>\n<p>Resource handler returned message: &quot;Product prod-7tjedn5dz4jrw does\nnot exist or access was denied (Service: SageMaker, Status Code: 400,\nRequest ID: 768116aa-e77b-4691-a972-38b83093fdc4)&quot; (RequestToken:\n45ca2a0c-3f03-e3e0-f29d-d9443ff4dfc1, HandlerErrorCode:\nGeneralServiceException)<\/p>\n<\/blockquote>\n<p>I am running this code from an ec2 instance that has SagemakerFullAccess\nI also tried attaching SagemakerFullAccess execution role with project...but got the same error.\nI have also attached below policy to my domain:<\/p>\n<ul>\n<li>AmazonSageMakerAdmin-ServiceCatalogProductsServiceRolePolicy<\/li>\n<\/ul>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_date":"2022-04-21 11:42:51.883000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"amazon-web-services|aws-cdk|amazon-sagemaker",
        "Question_view_count":491,
        "Owner_creation_date":"2018-08-12 07:18:11.093000 UTC",
        "Owner_last_access_date":"2022-09-14 09:15:34.510000 UTC",
        "Owner_location":null,
        "Owner_reputation":116,
        "Owner_up_votes":20,
        "Owner_down_votes":0,
        "Owner_views":33,
        "Answer_body":"<p>Basically this was an issue related to IAM.\nRunning cdk program requires bootstrapping it using the command <code>cdk bootstrap<\/code>\nAfter running this command cdk was creating a bunch of roles out of which one role will be related to cloudformation's execution role. Something like<\/p>\n<blockquote>\n<p>cdk-serialnumber-cfn-exec-role-Id-region<\/p>\n<\/blockquote>\n<p>Now this role was used by cloudformation to run the stack.<\/p>\n<p>Using sagemaker from console automatically adds the role associated with domain\/user at<\/p>\n<blockquote>\n<p>ServiceCatalog -&gt; Portfolios -&gt; Imported -&gt; Amazon SageMaker Solutions and ML Ops products -&gt; Groups, roles, and users<\/p>\n<\/blockquote>\n<p>Thats was the reason why product id was accessible from console.<\/p>\n<p>After adding the role created by cdk bootsrap to the above path I was able to run my stack.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-04-23 19:07:50.210000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":53165953,
        "Question_title":"ValueError: Tensor is not an element of this graph, when hosting a model in Sagemaker with Gunicorn and Flask and Keras",
        "Question_body":"<p>I created a time series predictor with Keras and  Dockerized the model with with Flash and Gunicorn as per AWS docs. I am loading the serialized model with this code.<\/p>\n\n<pre><code>@classmethod\ndef get_model(cls):\n    if cls.model == None:\n        cls.model = load_model('\/opt\/ml\/bitcoin_model.h5')\n    return cls.model\n<\/code><\/pre>\n\n<p>Then I used the predict method to produce the results , the dockerized container is working perfectly in the local environment , but when I try to host the model in sagemaker it produces this error.<\/p>\n\n<pre><code>ValueError: Tensor Tensor(\"dense_1\/BiasAdd:0\", shape=(?, 1), dtype=float32) is not an element of this graph.\n<\/code><\/pre>\n\n<p>So how can I resolve this issue ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2018-11-06 04:57:19.330000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2018-11-06 05:07:40.787000 UTC",
        "Question_score":1,
        "Question_tags":"amazon-web-services|keras|amazon-sagemaker",
        "Question_view_count":223,
        "Owner_creation_date":"2016-04-06 04:30:54.450000 UTC",
        "Owner_last_access_date":"2022-09-25 05:42:06.587000 UTC",
        "Owner_location":"Frankfurt, Germany",
        "Owner_reputation":9168,
        "Owner_up_votes":1086,
        "Owner_down_votes":5,
        "Owner_views":675,
        "Answer_body":"<p>The issue was resolved by calling _make_predict_function() method in the model load phase.<\/p>\n\n<pre><code>@classmethod\ndef get_model(cls):\n    if cls.model == None:\n        cls.model = load_model('\/opt\/ml\/bitcoin_model.h5')\n        cls.model._make_predict_function()\n    return cls.model\n<\/code><\/pre>\n\n<p>Bug Reference : <a href=\"https:\/\/github.com\/keras-team\/keras\/issues\/6462\" rel=\"nofollow noreferrer\">https:\/\/github.com\/keras-team\/keras\/issues\/6462<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2018-11-06 04:57:19.330000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":51549048,
        "Question_title":"How to convert jupyter notebook (ipython) to slideshow NOT using command line",
        "Question_body":"<p>I am new with <code>Jupyter<\/code>, and I use Amazon SageMaker so that everything is cloud based and not local.  I cannot use any resources locally, nor can I install <code>Jupyter<\/code> on this local computer that I want to do this on, so I cannot use the command line to put :<\/p>\n\n<pre><code>jupyter nbconvert Jupyter\\ Slides.ipynb --to slides --post serve\n<\/code><\/pre>\n\n<p>So, I am struggling to find a way to convert my notebook to a slideshow NOT using command line. Thanks in advance!<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_date":"2018-07-27 00:06:12.710000 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":"2018-07-27 07:03:59.560000 UTC",
        "Question_score":1,
        "Question_tags":"ipython|jupyter-notebook|amazon-sagemaker",
        "Question_view_count":4610,
        "Owner_creation_date":"2018-06-09 18:02:16.707000 UTC",
        "Owner_last_access_date":"2022-05-11 20:45:04.530000 UTC",
        "Owner_location":"Kelowna, BC, Canada",
        "Owner_reputation":165,
        "Owner_up_votes":13,
        "Owner_down_votes":0,
        "Owner_views":16,
        "Answer_body":"<p>You can follow below steps to convert your notebook to slides on AWS Sagemaker (tried on sagemaker notebook instance) without installing any extensions.<\/p>\n\n<p><strong>Step 1:<\/strong> Follow this <a href=\"https:\/\/medium.com\/@mjspeck\/presenting-code-using-jupyter-notebook-slides-a8a3c3b59d67\" rel=\"nofollow noreferrer\">article<\/a> to chose which cells in your notebook can be presented or skipped.\n  - Go to View \u2192 Cell Toolbar \u2192 Slideshow\n  - A light gray bar will appear above each cell with a scroll down window on the top right\n  - Select type of slide each cell should be - regular slide, sub-slide, skip, notes<\/p>\n\n<p><strong>Step 2:<\/strong> Go to Sagemaker notebook home page and open terminal<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/kDl3d.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/kDl3d.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><strong>Step 3:<\/strong> Change directory in the instance where your notebook exists<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/rA1lZ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/rA1lZ.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><strong>Step 4:<\/strong> Clone <code>reveal.js<\/code> in the directory where notebook exists from <a href=\"https:\/\/github.com\/hakimel\/reveal.js\" rel=\"nofollow noreferrer\">github<\/a>. <code>reveal.js<\/code> is used for rendering HTML file as presentation.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/dillF.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/dillF.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><strong>Step 5:<\/strong> Run the below command (same as in your question) to convert the notebook to slides without serving them (since there is no browser on the Sagemaker instance). This will just convert notebook to slides html.<\/p>\n\n<pre><code>jupyter nbconvert Image-classification-fulltraining.ipynb --to slides\n[NbConvertApp] Converting notebook Image-classification-fulltraining.ipynb to slides\n[NbConvertApp] Writing 346423 bytes to Image-classification-fulltraining.slides.html\n<\/code><\/pre>\n\n<p><strong>Step 6:<\/strong> Now open the html file from Sagemaker notebook file browser <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/fykyl.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/fykyl.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Now you can see the notebook rendered as slides based on how setup each cell in your notebook in Step 1<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/9PLUA.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/9PLUA.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Hope it helps.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2018-07-27 22:56:32.903000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":72909085,
        "Question_title":"Sagemaker creates output folders but no model.tar.gz after successful completion of the Training Job",
        "Question_body":"<p>I am running a <em>Training Job<\/em> using the Sagemaker API. The code for configuring the estimator looks as follows (I shrinked the full path names a bit):<\/p>\n<pre><code>s3_input = &quot;s3:\/\/sagemaker-studio-****\/training-inputs&quot;.format(bucket)\ns3_images = &quot;s3:\/\/sagemaker-studio-****\/dataset&quot;\ns3_labels = &quot;s3:\/\/sagemaker-studio-****\/labels&quot;\ns3_output = 's3:\/\/sagemaker-studio-****\/output'.format(bucket)\n\ncfg='{}\/input\/models\/'.format(s3_input)\nweights='{}\/input\/data\/weights\/'.format(s3_input)\noutpath='{}\/'.format(s3_output)\nimages='{}\/'.format(s3_images)\nlabels='{}\/'.format(s3_labels)\n\nhyperparameters = {\n    &quot;epochs&quot;: 1,\n    &quot;batch-size&quot;: 2\n}\n\ninputs = {\n    &quot;cfg&quot;: TrainingInput(cfg),\n    &quot;images&quot;: TrainingInput(images),\n    &quot;weights&quot;: TrainingInput(weights),\n    &quot;labels&quot;: TrainingInput(labels)\n}\n\nestimator = PyTorch(\n    entry_point='train.py',\n    source_dir='s3:\/\/sagemaker-studio-****\/input\/input.tar.gz',\n    image_uri=container,\n    role=get_execution_role(),\n    instance_count=1,\n    instance_type='ml.g4dn.xlarge',\n    input_mode='File',\n    output_path=outpath,\n    train_output=outpath,\n    base_job_name='visualsearch',\n    hyperparameters=hyperparameters,\n    framework_version='1.9',\n    py_version='py38'\n)\n\nestimator.fit(inputs)\n<\/code><\/pre>\n<p>Everything runs fine and I get the success message:<\/p>\n<pre><code>Results saved to #033[1mruns\/train\/exp#033[0m\n2022-07-08 08:38:35,766 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\n2022-07-08 08:38:35,766 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\n2022-07-08 08:38:35,767 sagemaker-training-toolkit INFO     Reporting training SUCCESS\n\n2022-07-08 08:39:08 Uploading - Uploading generated training model\n2022-07-08 08:39:08 Completed - Training job completed\nProfilerReport-1657268881: IssuesFound\nTraining seconds: 558\nBillable seconds: 558\nCPU times: user 1.34 s, sys: 146 ms, total: 1.48 s\nWall time: 11min 20s\n<\/code><\/pre>\n<p>When I call <code>estimator.model_data<\/code> I get a path poiting to a model.tar.gz file <code>s3:\/\/sagemaker-studio-****\/output\/...\/model.tar.gz<\/code><\/p>\n<p>Sagemaker generated subfoldes into the output folder (which in turn contain a lot of json files and other artifacts):<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/WymlH.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/WymlH.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>But the file <code>model.tar.gz<\/code> is missing. This file is nowhere to be found. Is there anything I need to change or to add, in order to obtain my model?<\/p>\n<p>Any help is much appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-07-08 09:03:14.073000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-s3|pytorch|amazon-sagemaker",
        "Question_view_count":94,
        "Owner_creation_date":"2021-12-27 13:35:09.920000 UTC",
        "Owner_last_access_date":"2022-09-21 09:31:13.630000 UTC",
        "Owner_location":"Zedtwitz, Germany",
        "Owner_reputation":215,
        "Owner_up_votes":40,
        "Owner_down_votes":0,
        "Owner_views":46,
        "Answer_body":"<p>you need to make sure to store your model output to the right location inside the training container. Sagemaker will upload everything that is stored in the MODEL_DIR directory. You can find the location in the ENV of the training job:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>model_dir = os.environ.get(&quot;SM_MODEL_DIR&quot;)\n<\/code><\/pre>\n<p>Normally it is set to <code>opt\/ml\/model<\/code><\/p>\n<p>Ref:<\/p>\n<ul>\n<li><a href=\"https:\/\/github.com\/aws\/sagemaker-training-toolkit\/blob\/master\/ENVIRONMENT_VARIABLES.md#sm_model_dir\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-training-toolkit\/blob\/master\/ENVIRONMENT_VARIABLES.md#sm_model_dir<\/a><\/li>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-output.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-output.html<\/a><\/li>\n<\/ul>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-07-08 09:39:48.600000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":60641337,
        "Question_title":"mlflow R installation MLFLOW_PYTHON_BIN",
        "Question_body":"<p>I am trying to install mlflow in R and im getting this error message saying <\/p>\n\n<blockquote>\n  <p>mlflow::install_mlflow()\n  Error in mlflow_conda_bin() :\n    Unable to find conda binary. Is Anaconda installed?\n    If you are not using conda, you can set the environment variable MLFLOW_PYTHON_BIN to the path of yourpython executable.<\/p>\n<\/blockquote>\n\n<p>I have tried the following<\/p>\n\n<pre><code>export MLFLOW_PYTHON_BIN=\"\/usr\/bin\/python\" \nsource ~\/.bashrc\necho $MLFLOW_PYTHON_BIN  -&gt; this prints the \/usr\/bin\/python.\n<\/code><\/pre>\n\n<p>or in R,<\/p>\n\n<pre><code>sys.setenv(MLFLOW_PYTHON_BIN=\"\/usr\/bin\/python\")\nsys.getenv() -&gt; prints MLFLOW_PYTHON_BIN is set to \/usr\/bin\/python.\n<\/code><\/pre>\n\n<p>however, it still does not work<\/p>\n\n<p>I do not want to use conda environment.<\/p>\n\n<p>how to I get past this error?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_date":"2020-03-11 17:17:32.940000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2020-03-17 00:07:46.973000 UTC",
        "Question_score":5,
        "Question_tags":"r|mlflow|system-variable",
        "Question_view_count":1141,
        "Owner_creation_date":"2018-10-10 22:41:41.843000 UTC",
        "Owner_last_access_date":"2022-09-24 01:18:25.137000 UTC",
        "Owner_location":null,
        "Owner_reputation":117,
        "Owner_up_votes":3,
        "Owner_down_votes":0,
        "Owner_views":10,
        "Answer_body":"<p>The install_mlflow command only works with conda right now, sorry about the confusing message. You can either:<\/p>\n<ul>\n<li>install conda - this is the recommended way of installing and using mlflow<\/li>\n<\/ul>\n<p>or<\/p>\n<ul>\n<li>install mlflow python package yourself via pip<\/li>\n<\/ul>\n<p>To install mlflow yourself, pip install correct (matching the the R package) python version of mlflow and set the MLFLOW_PYTHON_BIN environment variable as well as MLFLOW_BIN evn variable: e.g.<\/p>\n<pre><code>library(mlflow)\nsystem(paste(&quot;pip install -U mlflow==&quot;, mlflow:::mlflow_version(), sep=&quot;&quot;))\nSys.setenv(MLFLOW_BIN=system(&quot;which mlflow&quot;))\nSys.setenv(MLFLOW_PYTHON_BIN=system(&quot;which python&quot;))\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2020-03-18 18:03:05.177000 UTC",
        "Answer_last_edit_date":"2021-06-20 15:16:15.903000 UTC",
        "Answer_score":3.0,
        "Question_valid_tags":[
            "mlflow"
        ]
    },
    {
        "Question_id":59412213,
        "Question_title":"Error: \"Make sure to create your workspace using a client which support MSI\" when deploying Azure ARM template for Machine Learning Services Workpsace",
        "Question_body":"<p>I'm currently trying to script our Azure Machine Learning infrastructure, using ARM templates and running through Terraform.  In order to ensure that the template works, I'm first running it from a file using the Az CLI.<\/p>\n\n<p>I'm running this on Ubuntu, with the below version of the Az CLI:-<\/p>\n\n<pre><code>azure-cli                         2.0.78\n\ncommand-modules-nspkg              2.0.3\ncore                              2.0.78\nnspkg                              3.0.4\ntelemetry                          1.0.4\n\nPython location '\/opt\/az\/bin\/python3'\nExtensions directory '\/home\/blah\/.azure\/cliextensions'\n\nPython (Linux) 3.6.5 (default, Dec 12 2019, 11:11:33) \n[GCC 8.3.0]\n<\/code><\/pre>\n\n<p>I have already created the Storage Account, App Insights and Key Vault using terraform.<\/p>\n\n<p>When trying to run the template using the Az CLI with the command:-<\/p>\n\n<pre><code>az group deployment create --name MachineLearning --resource-group data-science --template-file ML_ARM.json --parameters appInsightsName=machine-learning-dev storageAccountName=machinelearningdev keyVaultName=data-science-dev mlApiVersion=2018-11-19 mlWorkspaceName=machine-learning-dev location=uksouth\n<\/code><\/pre>\n\n<p>I receive the following error:-<\/p>\n\n<p><code>Make sure to create your workspace using a client which support MSI<\/code><\/p>\n\n<p>The ARM template is below:-<\/p>\n\n<pre><code>{\n    \"$schema\": \"https:\/\/schema.management.azure.com\/schemas\/2015-01-01\/deploymentTemplate.json#\",\n    \"contentVersion\": \"1.0.0.0\",\n    \"parameters\": {\n        \"storageAccountName\": {\n            \"type\": \"string\",\n            \"metadata\": {\n                \"description\": \"The name of the storage account\"\n            }\n        },\n        \"appInsightsName\" : {\n            \"type\": \"string\",\n            \"metadata\": {\n                \"description\": \"The name of the app insights account\"\n            }\n        },\n        \"keyVaultName\": {\n            \"type\": \"string\",\n            \"metadata\": {\n                \"description\": \"The name of the keyvault resource\"\n            }\n        },\n        \"mlApiVersion\": {\n            \"type\": \"string\",\n            \"metadata\": {\n                \"description\": \"The api version of the ML workspace\"\n            }\n        },\n        \"mlWorkspaceName\": {\n            \"type\": \"string\",\n            \"metadata\": {\n                \"description\": \"The name of the Machine Learning Workspace\"\n            }\n        },\n        \"location\": {\n            \"type\": \"string\",\n            \"metadata\": {\n                \"description\": \"Resource location\"\n            }\n        }\n    },\n  \"resources\": [\n        {\n            \"apiVersion\": \"[parameters('mlApiVersion')]\",\n            \"type\": \"Microsoft.MachineLearningServices\/workspaces\",\n            \"name\": \"[parameters('mlWorkspaceName')]\",\n            \"location\": \"[parameters('location')]\",\n            \"sku\": {\n              \"tier\": \"enterprise\",\n              \"name\": \"enterprise\"\n            },\n            \"properties\": {\n                \"storageAccount\": \"[resourceId('Microsoft.Storage\/storageAccounts',parameters('storageAccountName'))]\",\n                \"applicationInsights\": \"[resourceId('Microsoft.Insights\/components',parameters('appInsightsName'))]\",\n                \"keyVault\": \"[resourceId('Microsoft.KeyVault\/vaults',parameters('keyVaultName'))]\"\n            }\n        }\n    ]\n}\n<\/code><\/pre>\n\n<p>Some rudimentary googling hasn't really been enlightening into what the issue might be with this; the documentation and guide templates for the Machine Learning Service are linked below:-<\/p>\n\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-create-workspace-template\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-create-workspace-template<\/a>\n<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/templates\/microsoft.machinelearningservices\/2019-11-01\/workspaces\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/templates\/microsoft.machinelearningservices\/2019-11-01\/workspaces<\/a><\/p>\n\n<p>Any idea what the issue might be?  Thanks in advance for any pointers!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2019-12-19 14:58:08.223000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"azure|terraform|azure-resource-manager|terraform-provider-azure|azure-machine-learning-service",
        "Question_view_count":422,
        "Owner_creation_date":"2019-07-09 20:45:35.403000 UTC",
        "Owner_last_access_date":"2022-02-24 07:03:27.060000 UTC",
        "Owner_location":null,
        "Owner_reputation":327,
        "Owner_up_votes":7,
        "Owner_down_votes":0,
        "Owner_views":38,
        "Answer_body":"<p>I am not familar with Terraform or that robust on ML Services; however, the error you provided lends itself to needing to have MSI authentication configured which is configured in the link you provided.<\/p>\n\n<p>Try updating your ARM to include the identity section like this:<\/p>\n\n<pre><code>   ...  },\n\"identity\": {\n        \"type\": \"systemAssigned\"\n      },\n                \"properties\": {\n                    \"storageAccount\": \"[resourceId('Microsoft.Storage\/storageAccounts',parameters('storageAccountName'))]\",\n                    \"applicationInsights\": \"[resourceId('Microsoft.Insights\/components',parameters('appInsightsName'))]\",\n                    \"keyVault\": \"[resourceId('Microsoft.KeyVault\/vaults',parameters('keyVaultName'))]\"\n                }\n<\/code><\/pre>\n\n<p>This will create the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/active-directory\/managed-identities-azure-resources\/overview\" rel=\"nofollow noreferrer\">Managed Service Identity<\/a>.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2019-12-19 18:19:55.737000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":70648776,
        "Question_title":"Python click incorrectly parses arguments when called in Vertex AI Pipeline",
        "Question_body":"<p>I'm trying to run a simple Ada-boosted Decision Tree regressor on GCP Vertex AI. To parse hyperparams and other arguments I use Click for Python, a very simple CLI library. Here's the setup for my task function:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>@click.command()\n@click.argument(&quot;input_path&quot;, type=str)\n@click.option(&quot;--output-path&quot;, type=str, envvar='AIP_MODEL_DIR')\n@click.option('--gcloud', is_flag=True, help='Run as if in Google Cloud Vertex AI Pipeline')\n@click.option('--grid', is_flag=True, help='Perform a grid search instead of a single run. Ignored with --gcloud')\n@click.option(&quot;--max_depth&quot;, type=int, default=4, help='Max depth of decision tree', show_default=True)\n@click.option(&quot;--n_estimators&quot;, type=int, default=50, help='Number of AdaBoost boosts', show_default=True)\ndef click_main(input_path, output_path, gcloud, grid, max_depth, n_estimators):\n    train_model(input_path, output_path, gcloud, grid, max_depth, n_estimators)\n\n\ndef train_model(input_path, output_path, gcloud, grid, max_depth, n_estimators):\n    print(input_path, output_path, gcloud)\n    logger = logging.getLogger(__name__)\n    logger.info(&quot;training models from processed data&quot;)\n    ...\n<\/code><\/pre>\n<p>When I run it locally like below, Click correctly grabs the params both from console and environment and proceeds with model training (<code>AIP_MODEL_DIR<\/code> is <code>gs:\/\/(BUCKET_NAME)\/models<\/code>)<\/p>\n<pre><code>\u276f python3 -m src.models.train_model gs:\/\/(BUCKET_NAME)\/data\/processed --gcloud\n\ngs:\/\/(BUCKET_NAME)\/data\/processed gs:\/\/(BUCKET_NAME)\/models True\n\n<\/code><\/pre>\n<p>However, when I put this code on the Vertex AI Pipeline, it throws an error, namely<\/p>\n<pre><code>FileNotFoundError: b\/(BUCKET_NAME)\/o\/data%2Fprocessed%20%20--gcloud%2Fprocessed_features.csv\n<\/code><\/pre>\n<p>As it is clearly seen, Click grabs both the parameter and the <code>--gcloud<\/code> option and assigns it to <code>input_path<\/code>. The print statement before that confirms it, both by having one too many spaces and <code>--gcloud<\/code> being parsed as false.<\/p>\n<pre><code>gs:\/\/(BUCKET_NAME)\/data\/processed  --gcloud gs:\/\/(BUCKET_NAME)\/models\/1\/model\/ False\n<\/code><\/pre>\n<p>Has anyone here encountered this issue or have any idea how to solve it?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-01-10 06:58:21.177000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-01-11 10:27:33.597000 UTC",
        "Question_score":1,
        "Question_tags":"python|google-cloud-platform|google-cloud-storage|python-click|google-cloud-vertex-ai",
        "Question_view_count":129,
        "Owner_creation_date":"2020-01-23 17:50:31.103000 UTC",
        "Owner_last_access_date":"2022-09-15 02:08:52.260000 UTC",
        "Owner_location":"Tempe, AZ, USA",
        "Owner_reputation":71,
        "Owner_up_votes":5,
        "Owner_down_votes":0,
        "Owner_views":30,
        "Answer_body":"<p>I think is due the nature of <a href=\"https:\/\/click.palletsprojects.com\/en\/7.x\/arguments\/?highlight=arguments\" rel=\"nofollow noreferrer\">arguments<\/a> and <a href=\"https:\/\/click.palletsprojects.com\/en\/7.x\/options\/?highlight=options\" rel=\"nofollow noreferrer\">options<\/a>, you are mixing arguments and options although is not implicit stated in the documentation but argument will eat up the options that follow. If nargs is not allocated it will default to 1 considering everything after it follows as string which it looks like this is the case.<\/p>\n<blockquote>\n<p>nargs \u2013 the number of arguments to match. If not 1 the return value is a tuple instead of single value. The default for nargs is 1 (except if the type is a tuple, then it\u2019s the arity of the tuple).<\/p>\n<\/blockquote>\n<p>I think you should first use options followed by the argument as display on the <a href=\"https:\/\/click.palletsprojects.com\/en\/7.x\/documentation\/?highlight=arguments\" rel=\"nofollow noreferrer\">documentation page<\/a>. Other approach is to group it under a command as show on this <a href=\"https:\/\/click.palletsprojects.com\/en\/7.x\/commands\/\" rel=\"nofollow noreferrer\">link<\/a>.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2022-01-10 10:36:28.040000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "google-cloud-vertex-ai"
        ]
    },
    {
        "Question_id":61464960,
        "Question_title":"SageMaker multimodel and RandomCutForest",
        "Question_body":"<p>I am trying to invoke a MultiModel Endpoint with a RandomCutForest Model. I receive error though, 'Error loading model'. I can invoke the endpoint with models given from the examples.\nAm I missing something e.g. limitations on what models I can use? <\/p>\n\n<p>For MultiModel inspiration I am using below:<\/p>\n\n<blockquote>\n  <p><a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/multi_model_xgboost_home_value\/xgboost_multi_model_endpoint_home_value.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/multi_model_xgboost_home_value\/xgboost_multi_model_endpoint_home_value.ipynb<\/a><\/p>\n  \n  <p><a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/save-on-inference-costs-by-using-amazon-sagemaker-multi-model-endpoints\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/save-on-inference-costs-by-using-amazon-sagemaker-multi-model-endpoints\/<\/a><\/p>\n<\/blockquote>\n\n<p>I am trying to deploy the outputted 'model.tar.gz' from below RCF example in the MultiModel endpoint:<\/p>\n\n<blockquote>\n  <p><a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/random_cut_forest\/random_cut_forest.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/random_cut_forest\/random_cut_forest.ipynb<\/a><\/p>\n<\/blockquote>\n\n<pre><code>model_name = 'model'\nfull_model_name = '{}.tar.gz'.format(model_name)\nfeatures = data\n\nbody = ','.join(map(str, features)) + '\\n'\nresponse = runtime_sm_client.invoke_endpoint(\n                    EndpointName=endpoint_name,\n                    ContentType='text\/csv',\n                    TargetModel=full_model_name,\n                    Body=body)\nprint(response)\n<\/code><\/pre>\n\n<p><strong>Cloudwatch log Error:<\/strong><\/p>\n\n<pre><code>&gt; 2020-04-27 17:28:59,005 [INFO ]\n&gt; W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Error loading model: Unable\n&gt; to load model: invalid load key, '{'. [17:28:59]\n&gt; \/workspace\/src\/learner.cc:334: Check failed: fi-&gt;Read(&amp;mparam_,\n&gt; sizeof(mparam_)) == sizeof(mparam_) (25 vs. 136) : BoostLearner: wrong\n&gt; model format 2020-04-27 17:28:59,005 [INFO ]\n&gt; W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Stack trace: 2020-04-27\n&gt; 17:28:59,005 [INFO ] W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   [bt] (0)\n&gt; \/miniconda3\/xgboost\/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x24)\n&gt; [0x7f37ce1cacb4] 2020-04-27 17:28:59,005 [INFO ]\n&gt; W-9003-b39b888fb4a3fa6cf83bb34a9 com.amazonaws.ml.mms.wlm.WorkerThread\n&gt; - Backend response time: 0 2020-04-27 17:28:59,005 [INFO ] W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   [bt] (1)\n&gt; \/miniconda3\/xgboost\/libxgboost.so(xgboost::LearnerImpl::Load(dmlc::Stream*)+0x4b5)\n&gt; [0x7f37ce266985] 2020-04-27 17:28:59,005 [INFO ]\n&gt; W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   [bt] (2)\n&gt; \/miniconda3\/xgboost\/libxgboost.so(XGBoosterLoadModel+0x37)\n&gt; [0x7f37ce1bf417] 2020-04-27 17:28:59,005 [INFO ]\n&gt; W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   [bt] (3)\n&gt; \/miniconda3\/lib\/python3.7\/lib-dynload\/..\/..\/libffi.so.6(ffi_call_unix64+0x4c)\n&gt; [0x7f37ee993ec0] 2020-04-27 17:28:59,005 [INFO ]\n&gt; W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   [bt] (4)\n&gt; \/miniconda3\/lib\/python3.7\/lib-dynload\/..\/..\/libffi.so.6(ffi_call+0x22d)\n&gt; [0x7f37ee99387d] 2020-04-27 17:28:59,005 [INFO ]\n&gt; W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   [bt] (5)\n&gt; \/miniconda3\/lib\/python3.7\/lib-dynload\/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce)\n&gt; [0x7f37eeba91de] 2020-04-27 17:28:59,005 [INFO ]\n&gt; W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   [bt] (6)\n&gt; \/miniconda3\/lib\/python3.7\/lib-dynload\/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12c14)\n&gt; [0x7f37eeba9c14] 2020-04-27 17:28:59,005 [INFO ]\n&gt; W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   [bt] (7)\n&gt; \/miniconda3\/bin\/python(_PyObject_FastCallKeywords+0x48b)\n&gt; [0x563d71b4218b] 2020-04-27 17:28:59,005 [INFO ]\n&gt; W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   [bt] (8)\n&gt; \/miniconda3\/bin\/python(_PyEval_EvalFrameDefault+0x52cf)\n&gt; [0x563d71b91e8f] 2020-04-27 17:28:59,005 [INFO ]\n&gt; W-9003-b39b888fb4a3fa6cf83bb34a9-stdout\n&gt; com.amazonaws.ml.mms.wlm.WorkerLifeCycle -  2020-04-27 17:28:59,005\n&gt; [WARN ] W-9003-b39b888fb4a3fa6cf83bb34a9\n&gt; com.amazonaws.ml.mms.wlm.WorkerThread - Backend worker thread\n&gt; exception. java.lang.IllegalArgumentException: reasonPhrase contains\n&gt; one of the following prohibited characters: \\r\\n: Unable to load\n&gt; model: Unable to load model: invalid load key, '{'. [17:28:59]\n&gt; \/workspace\/src\/learner.cc:334: Check failed: fi-&gt;Read(&amp;mparam_,\n&gt; sizeof(mparam_)) == sizeof(mparam_) (25 vs. 136) : BoostLearner: wrong\n&gt; model format Stack trace:   [bt] (0)\n&gt; \/miniconda3\/xgboost\/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x24)\n&gt; [0x7f37ce1cacb4]   [bt] (1)\n&gt; \/miniconda3\/xgboost\/libxgboost.so(xgboost::LearnerImpl::Load(dmlc::Stream*)+0x4b5)\n&gt; [0x7f37ce266985]   [bt] (2)\n&gt; \/miniconda3\/xgboost\/libxgboost.so(XGBoosterLoadModel+0x37)\n&gt; [0x7f37ce1bf417]   [bt] (3)\n&gt; \/miniconda3\/lib\/python3.7\/lib-dynload\/..\/..\/libffi.so.6(ffi_call_unix64+0x4c)\n&gt; [0x7f37ee993ec0]   [bt] (4)\n&gt; \/miniconda3\/lib\/python3.7\/lib-dynload\/..\/..\/libffi.so.6(ffi_call+0x22d)\n&gt; [0x7f37ee99387d]   [bt] (5)\n&gt; \/miniconda3\/lib\/python3.7\/lib-dynload\/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce)\n&gt; [0x7f37eeba91de]   [bt] (6)\n&gt; \/miniconda3\/lib\/python3.7\/lib-dynload\/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12c14)\n&gt; [0x7f37eeba9c14]   [bt] (7)\n&gt; \/miniconda3\/bin\/python(_PyObject_FastCallKeywords+0x48b)\n&gt; [0x563d71b4218b]   [bt] (8)\n&gt; \/miniconda3\/bin\/python(_PyEval_EvalFrameDefault+0x52cf)\n&gt; [0x563d71b91e8f]\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-04-27 17:39:45.817000 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":null,
        "Question_score":0,
        "Question_tags":"amazon-web-services|machine-learning|xgboost|amazon-sagemaker",
        "Question_view_count":324,
        "Owner_creation_date":"2015-03-16 08:02:10.157000 UTC",
        "Owner_last_access_date":"2022-09-14 09:01:01.523000 UTC",
        "Owner_location":null,
        "Owner_reputation":398,
        "Owner_up_votes":37,
        "Owner_down_votes":1,
        "Owner_views":84,
        "Answer_body":"<p>SageMaker Random Cut Forest is part of the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algos.html\" rel=\"nofollow noreferrer\">built-in algorithm library<\/a> and cannot be deployed in multi-model endpoint (MME). Built-in algorithms currently cannot be deployed to MME. XGboost is an exception, since it has an open-source container <a href=\"https:\/\/github.com\/aws\/sagemaker-xgboost-container\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-xgboost-container<\/a>.<\/p>\n\n<p>If you really need to deploy a RCF to a multi-model endpoint, one option is to find a reasonably similar open-source implementation (for example <a href=\"https:\/\/github.com\/kLabUM\/rrcf\" rel=\"nofollow noreferrer\"><code>rrcf<\/code><\/a> looks reasonably serious: based <a href=\"http:\/\/proceedings.mlr.press\/v48\/guha16.pdf\" rel=\"nofollow noreferrer\">on the same paper from Guha et al<\/a> and with 170+ github stars) and create a custom MME docker container. The <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/build-multi-model-build-container.html\" rel=\"nofollow noreferrer\">documentation is here<\/a> and there is <a href=\"https:\/\/github.com\/giuseppeporcelli\/sagemaker-custom-serving-containers\/blob\/master\/multi-model-server-container\/notebook\/multi-model-server-container.ipynb\" rel=\"nofollow noreferrer\">an excellent tuto here<\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_date":"2020-04-28 08:13:01.360000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":56285351,
        "Question_title":"Updating tracked dir in DVC",
        "Question_body":"<p>According to <a href=\"https:\/\/dvc.org\/doc\/user-guide\/update-tracked-file\" rel=\"nofollow noreferrer\">this tutorial<\/a> when I update file I should remove file from under DVC control first (i.e. execute <code>dvc unprotect &lt;myfile&gt;.dvc<\/code> or <code>dvc remove &lt;myfile&gt;.dvc<\/code>) and then add it again via <code>dvc add &lt;mifile&gt;<\/code>. However It's not clear if I should apply the same workflow for the directories.<\/p>\n\n<p>I have the directory under DVC control with the following structure:<\/p>\n\n<pre><code>data\/\n    1.jpg\n    2.jpg\n<\/code><\/pre>\n\n<p>Should I run <code>dvc unprotect data<\/code> every time the directory content is updated?<\/p>\n\n<p>More specifically I'm interested if I should run <code>dvc unprotect data<\/code> in the following use cases:<\/p>\n\n<ul>\n<li><strong>New file is added.<\/strong> For example if I put <code>3.jpg<\/code> image in the data dir<\/li>\n<li><strong>File is deleted.<\/strong> For example if I delete <code>2.jpg<\/code> image in the <code>data<\/code> dir<\/li>\n<li><strong>File is updated.<\/strong> For example if I edit <code>1.jpg<\/code> image via graphic editor.<\/li>\n<li>A combination of the previous use cases (i.e. some files are updated, other deleted and new files are added)<\/li>\n<\/ul>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2019-05-24 03:10:22.490000 UTC",
        "Question_favorite_count":1.0,
        "Question_last_edit_date":"2019-05-24 14:39:32.617000 UTC",
        "Question_score":4,
        "Question_tags":"dvc",
        "Question_view_count":995,
        "Owner_creation_date":"2018-03-28 16:31:38.710000 UTC",
        "Owner_last_access_date":"2022-09-23 10:08:33.687000 UTC",
        "Owner_location":"Russia",
        "Owner_reputation":784,
        "Owner_up_votes":32,
        "Owner_down_votes":0,
        "Owner_views":77,
        "Answer_body":"<p>Only when file is updated - i.e. edit <code>1.jpg<\/code> with your editor <strong>AND<\/strong> only if hadrlink or symlink cache type is enabled.<\/p>\n\n<p>Please, check this <a href=\"https:\/\/dvc.org\/doc\/user-guide\/update-tracked-file\" rel=\"nofollow noreferrer\">link<\/a>:<\/p>\n\n<blockquote>\n  <p>updating tracked files has to be carried out with caution to avoid data corruption when the DVC config option cache.type is set to hardlink or\/and symlink<\/p>\n<\/blockquote>\n\n<p>I would strongly recommend reading this document: <a href=\"https:\/\/dvc.org\/doc\/user-guide\/cache-file-linking\" rel=\"nofollow noreferrer\">Performance Optimization for Large Files<\/a> it explains benefits of using hardlinks\/symlinks.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2019-05-24 05:04:26.680000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":2.0,
        "Question_valid_tags":[
            "dvc"
        ]
    },
    {
        "Question_id":58816515,
        "Question_title":"Databricks UDF calling an external web service cannot be serialised (PicklingError)",
        "Question_body":"<p>I am using Databricks and have a column in a dataframe that I need to update for every record with an external web service call. In this case it is using the Azure Machine Learning Service SDK and does a service call. This code works fine when not run as a UDF in spark (ie. just python) however it throws a serialization error when I try to call it as a UDF. The same happens if I use a lambda and a map with an rdd.<\/p>\n\n<p>The model uses fastText and can be invoked fine from Postman or python via a normal http call or using the WebService SDK from AMLS - it's just when it is a UDF that it fails with this message:<\/p>\n\n<p>TypeError: can't pickle _thread._local objects<\/p>\n\n<p>The only workaround I can think of is to loop through each record in the dataframe sequentially and update the record with a call, however this is not very efficient. I don't know if this is a spark error or because the service is loading a fasttext model. When I use the UDF and mock a return value it works though.<\/p>\n\n<p>Error at bottom...<\/p>\n\n<pre><code>from azureml.core.webservice import Webservice, AciWebservice\nfrom azureml.core import Workspace\n\ndef predictModelValue2(summary, modelName, modelLabel):  \n    raw_data = '[{\"label\": \"' + modelLabel + '\", \"model\": \"' + modelName + '\", \"as_full_account\": \"' + summary + '\"}]'\n    prediction = service.run(raw_data)\n    return prediction\n\nfrom pyspark.sql.types import FloatType\nfrom pyspark.sql.functions import udf\n\npredictModelValueUDF = udf(predictModelValue2)\n\nDVIRCRAMFItemsDFScored1 = DVIRCRAMFItemsDF.withColumn(\"Result\", predictModelValueUDF(\"Summary\", \"ModelName\", \"ModelLabel\"))\n<\/code><\/pre>\n\n<blockquote>\n  <p>TypeError: can't pickle _thread._local objects<\/p>\n  \n  <p>During handling of the above exception, another exception occurred:<\/p>\n  \n  <p>PicklingError                             Traceback (most recent call\n  last)  in \n  ----> 2 x = df.withColumn(\"Result\", predictModelValueUDF(\"Summary\",\n  \"ModelName\", \"ModelLabel\"))<\/p>\n  \n  <p>\/databricks\/spark\/python\/pyspark\/sql\/udf.py in wrapper(*args)\n      194         @functools.wraps(self.func, assigned=assignments)\n      195         def wrapper(*args):\n  --> 196             return self(*args)\n      197 \n      198         wrapper.<strong>name<\/strong> = self._name<\/p>\n  \n  <p>\/databricks\/spark\/python\/pyspark\/sql\/udf.py in <strong>call<\/strong>(self, *cols)\n      172 \n      173     def <strong>call<\/strong>(self, *cols):\n  --> 174         judf = self._judf\n      175         sc = SparkContext._active_spark_context\n      176         return Column(judf.apply(_to_seq(sc, cols, _to_java_column)))<\/p>\n  \n  <p>\/databricks\/spark\/python\/pyspark\/sql\/udf.py in _judf(self)\n      156         # and should have a minimal performance impact.\n      157         if self._judf_placeholder is None:\n  --> 158             self._judf_placeholder = self._create_judf()\n      159         return self._judf_placeholder\n      160 <\/p>\n  \n  <p>\/databricks\/spark\/python\/pyspark\/sql\/udf.py in _create_judf(self)\n      165         sc = spark.sparkContext\n      166 \n  --> 167         wrapped_func = _wrap_function(sc, self.func, self.returnType)\n      168         jdt = spark._jsparkSession.parseDataType(self.returnType.json())\n      169         judf = sc._jvm.org.apache.spark.sql.execution.python.UserDefinedPythonFunction(<\/p>\n  \n  <p>\/databricks\/spark\/python\/pyspark\/sql\/udf.py in _wrap_function(sc,\n  func, returnType)\n       33 def _wrap_function(sc, func, returnType):\n       34     command = (func, returnType)\n  ---> 35     pickled_command, broadcast_vars, env, includes = _prepare_for_python_RDD(sc, command)\n       36     return sc._jvm.PythonFunction(bytearray(pickled_command), env, includes, sc.pythonExec,\n       37                                   sc.pythonVer, broadcast_vars, sc._javaAccumulator)<\/p>\n  \n  <p>\/databricks\/spark\/python\/pyspark\/rdd.py in _prepare_for_python_RDD(sc,\n  command)    2461     # the serialized command will be compressed by\n  broadcast    2462     ser = CloudPickleSerializer()\n  -> 2463     pickled_command = ser.dumps(command)    2464     if len(pickled_command) >\n  sc._jvm.PythonUtils.getBroadcastThreshold(sc._jsc):  # Default 1M<br>\n  2465         # The broadcast will have same life cycle as created\n  PythonRDD<\/p>\n  \n  <p>\/databricks\/spark\/python\/pyspark\/serializers.py in dumps(self, obj)\n      709                 msg = \"Could not serialize object: %s: %s\" % (e.<strong>class<\/strong>.<strong>name<\/strong>, emsg)\n      710             cloudpickle.print_exec(sys.stderr)\n  --> 711             raise pickle.PicklingError(msg)\n      712 \n      713 <\/p>\n  \n  <p>PicklingError: Could not serialize object: TypeError: can't pickle\n  _thread._local objects<\/p>\n<\/blockquote>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_date":"2019-11-12 10:18:14.003000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"pyspark|user-defined-functions|pickle|azure-databricks|azure-machine-learning-service",
        "Question_view_count":931,
        "Owner_creation_date":"2009-10-21 01:51:25.500000 UTC",
        "Owner_last_access_date":"2022-09-13 05:24:36.847000 UTC",
        "Owner_location":"Sydney, Australia",
        "Owner_reputation":4947,
        "Owner_up_votes":277,
        "Owner_down_votes":8,
        "Owner_views":531,
        "Answer_body":"<p>I am not expert in DataBricks or Spark, but pickling functions from the local notebook context is always problematic when you are touching complex objects like the <code>service<\/code> object. In this particular case, I would recommend removing the dependency on the azureML <code>service<\/code> object and just use <code>requests<\/code> to call the service. <\/p>\n\n<p>Pull the key from the service:<\/p>\n\n<pre><code># retrieve the API keys. two keys were generated.\nkey1, key2 = service.get_keys()\nscoring_uri = service.scoring_uri\n<\/code><\/pre>\n\n<p>You should be able to use these strings in the UDF directly without pickling issues -- <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/9233ce089afb81d466076e36e7e61c3ce4cfafec\/how-to-use-azureml\/ml-frameworks\/chainer\/deployment\/train-hyperparameter-tune-deploy-with-chainer\/train-hyperparameter-tune-deploy-with-chainer.ipynb\" rel=\"nofollow noreferrer\">here is an example<\/a> of  how you would call the service with just requests. Below applied to your UDF:<\/p>\n\n<pre><code>import requests, json\ndef predictModelValue2(summary, modelName, modelLabel):  \n  input_data = json.dumps({\"summary\": summary, \"modelName\":, ....})\n\n  headers = {'Content-Type':'application\/json', 'Authorization': 'Bearer ' + key1}\n\n  # call the service for scoring\n  resp = requests.post(scoring_uri, input_data, headers=headers)\n\n  return resp.text[1]\n\n<\/code><\/pre>\n\n<p>On a side node, though: your UDF will be called for each row in your data frame and each time it will make a network call -- that will be very slow. I would recommend looking for ways to batch the execution. As you can see from your constructed json <code>service.run<\/code> will accept an array of items, so you should call it in batches of 100s or so.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2019-11-15 18:06:07.917000 UTC",
        "Answer_last_edit_date":"2019-11-15 19:06:25.320000 UTC",
        "Answer_score":1.0,
        "Question_valid_tags":[
            "azure-machine-learning-service"
        ]
    },
    {
        "Question_id":61181955,
        "Question_title":"Is there any limits of saving result on S3 from sagemaker Processing?",
        "Question_body":"<p>\u203b I used google translation, if you have any question, let me know!<\/p>\n\n<p>I am trying to run python script with huge 4 data, using sagemaker processing. And my current situation are as follows:<\/p>\n\n<ul>\n<li>can run this script with 3 data<\/li>\n<li>can't run the script with only 1 data (the biggest, the same structure with others)<\/li>\n<li>as for all of 4 data, the script has finished (so, I suspected this error in S3, ie. when copying sagemaker result to S3)<\/li>\n<\/ul>\n\n<p>The error I got is this InternalServerError.<\/p>\n\n<pre><code>Traceback (most recent call last):\n  File \"sagemaker_train_and_predict.py\", line 56, in &lt;module&gt;\n    outputs=outputs\n  File \"{xxx}\/sagemaker_constructor.py\", line 39, in run\n    outputs=outputs\n  File \"{masked}\/.pyenv\/versions\/3.6.8\/lib\/python3.6\/site-packages\/sagemaker\/processing.py\", line 408, in run\n    self.latest_job.wait(logs=logs)\n  File \"{masked}\/.pyenv\/versions\/3.6.8\/lib\/python3.6\/site-packages\/sagemaker\/processing.py\", line 723, in wait\n    self.sagemaker_session.logs_for_processing_job(self.job_name, wait=True)\n  File \"{masked}\/.pyenv\/versions\/3.6.8\/lib\/python3.6\/site-packages\/sagemaker\/session.py\", line 3111, in logs_for_processing_job\n    self._check_job_status(job_name, description, \"ProcessingJobStatus\")\n  File \"{masked}\/.pyenv\/versions\/3.6.8\/lib\/python3.6\/site-packages\/sagemaker\/session.py\", line 2615, in _check_job_status\n    actual_status=status,\nsagemaker.exceptions.UnexpectedStatusException: Error for Processing job sagemaker-vm-train-and-predict-2020-04-12-04-15-40-655: Failed. Reason: InternalServerError: We encountered an internal error.  Please try again.\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-04-13 05:22:28.657000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"python|amazon-s3|amazon-sagemaker",
        "Question_view_count":95,
        "Owner_creation_date":"2020-04-13 05:07:12.800000 UTC",
        "Owner_last_access_date":"2020-06-30 10:24:46.190000 UTC",
        "Owner_location":null,
        "Owner_reputation":13,
        "Owner_up_votes":0,
        "Owner_down_votes":0,
        "Owner_views":1,
        "Answer_body":"<p>There may be some issue transferring the output data to S3 if the output is generated at a high rate and size is too large. <\/p>\n\n<p>You can 1) try to slow down writing the output a bit or 2) call S3 from your algorithm container to upload the output directly using boto client (<a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/s3.html\" rel=\"nofollow noreferrer\">https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/s3.html<\/a>).<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2020-04-14 17:50:13.193000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":0.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":70560288,
        "Question_title":"DVC Shared Windows Directory Setup",
        "Question_body":"<p>I have one Linux machine and one Windows machine for developments. For data sharing, we have set up a shared Windows directory in another Windows machine, which both my Linux and Windows can access.<\/p>\n<p>I am now using <a href=\"https:\/\/dvc.org\/\" rel=\"nofollow noreferrer\">DVC<\/a> for version control of the shared data. To make it easy, I mount the shared Windows folder both in Windows and in Linux development machine. In Windows, it looks like<\/p>\n<pre><code> [core]\n    analytics = false\n    remote = remote_storage\n['remote &quot;remote_storage&quot;']\n    url = \\\\my_shared_storage\\project_dir\n<\/code><\/pre>\n<p>In Linux, it looks like:<\/p>\n<pre><code>[core]\n    analytics = false\n    remote = remote_storage\n['remote &quot;remote_storage&quot;']\n    url = \/mnt\/mount_point\/project_dir\n<\/code><\/pre>\n<p>As you can see, Windows and Linux have different mounting points. So my question is: is there a way to make that both Windows and Linux have the same <code>\u00f9rl<\/code> in the DVC configuration file?<\/p>\n<p>If this is impossible, is there another alternative solution for DVC keeps data in remote shared Windows folder? Thanks.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2022-01-02 22:41:03.100000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2022-01-03 08:44:24.667000 UTC",
        "Question_score":1,
        "Question_tags":"linux|dvc",
        "Question_view_count":128,
        "Owner_creation_date":"2012-03-12 11:50:57.367000 UTC",
        "Owner_last_access_date":"2022-09-23 13:23:56.043000 UTC",
        "Owner_location":null,
        "Owner_reputation":10643,
        "Owner_up_votes":1174,
        "Owner_down_votes":7,
        "Owner_views":504,
        "Answer_body":"<p>If you are using a local remote this way, you won't be able to have to the same <code>url<\/code> on both platforms since the mount points are different (as you already realized).<\/p>\n<p>The simplest way to configure this would be to pick one (Linux or Windows) <code>url<\/code> to use as your default case that gets git-committed into <code>.dvc\/config<\/code>. On the other platform you (or your users) can override that <code>url<\/code> in the local configuration file: <code>.dvc\/config.local<\/code>.<\/p>\n<p>(Note that <code>.dvc\/config.local<\/code> is a git-ignored file and will not be included in any commits)<\/p>\n<p>So if you wanted Windows to be the default case, in <code>.dvc\/config<\/code> you would have:<\/p>\n<pre><code> [core]\n    analytics = false\n    remote = remote_storage\n['remote &quot;remote_storage&quot;']\n    url = \\\\my_shared_storage\\project_dir\n<\/code><\/pre>\n<p>and on your Linux machine you would add the file <code>.dvc\/config.local<\/code> containing:<\/p>\n<pre><code>['remote &quot;remote_storage&quot;']\n    url = \/mnt\/mount_point\/project_dir\n<\/code><\/pre>\n<p>See the DVC docs for <code>dvc config --local<\/code> and <code>dvc remote modify --local<\/code> for more details:<\/p>\n<ul>\n<li><a href=\"https:\/\/dvc.org\/doc\/command-reference\/config#description\" rel=\"nofollow noreferrer\">https:\/\/dvc.org\/doc\/command-reference\/config#description<\/a><\/li>\n<li><a href=\"https:\/\/dvc.org\/doc\/command-reference\/remote\/modify#command-options-flags\" rel=\"nofollow noreferrer\">https:\/\/dvc.org\/doc\/command-reference\/remote\/modify#command-options-flags<\/a><\/li>\n<\/ul>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2022-01-03 03:08:55.767000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":3.0,
        "Question_valid_tags":[
            "dvc"
        ]
    },
    {
        "Question_id":61245284,
        "Question_title":"Is it necessary to commit DVC files from our CI pipelines?",
        "Question_body":"<p>DVC uses git commits to save the experiments and navigate between experiments.<\/p>\n<p>Is it possible to avoid making auto-commits in CI\/CD (to save data artifacts after <code>dvc repro<\/code> in CI\/CD side).<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2020-04-16 07:56:22.283000 UTC",
        "Question_favorite_count":0.0,
        "Question_last_edit_date":"2020-07-13 11:33:41.920000 UTC",
        "Question_score":6,
        "Question_tags":"git|machine-learning|continuous-integration|dvc|mlops",
        "Question_view_count":1047,
        "Owner_creation_date":"2019-05-22 12:54:44.193000 UTC",
        "Owner_last_access_date":"2022-01-15 10:43:18.997000 UTC",
        "Owner_location":"Bangalore, Karnataka, India",
        "Owner_reputation":79,
        "Owner_up_votes":107,
        "Owner_down_votes":0,
        "Owner_views":14,
        "Answer_body":"<blockquote>\n  <p>will you make it part of CI pipeline<\/p>\n<\/blockquote>\n\n<p>DVC often serves as a part of MLOps infrastructure. There is a popular <a href=\"https:\/\/martinfowler.com\/articles\/cd4ml.html\" rel=\"noreferrer\">blog post about CI\/CD for ML<\/a> where DVC is used under the hood. <a href=\"https:\/\/blog.codecentric.de\/en\/2020\/01\/remote-training-gitlab-ci-dvc\/\" rel=\"noreferrer\">Another example<\/a> but with GitLab CI\/CD.<\/p>\n\n<blockquote>\n  <p>scenario where you will integrate dvc commit command with CI\n  pipelines?<\/p>\n<\/blockquote>\n\n<p>If you mean <code>git commit<\/code> of DVC files (not <code>dvc commit<\/code>) then yes, you need to commit dvc-files into Git during CI\/CD process. Auto-commit is not the best practice.<\/p>\n\n<p>How to avoid Git commit in CI\/CD:<\/p>\n\n<ol>\n<li>After ML model training in CI\/CD, save changed dvc-files in external storage (for example GitLab artifact\/releases), then get the files to a developer machine and commit there. Users usually write scripts to automate it.<\/li>\n<li>Wait for DVC 1.0 release when <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/1234\" rel=\"noreferrer\">run-cache (like build-cache)<\/a> will be implemented. Run-cache makes dvc-files ephemeral and no additional Git commits will be required. Technically, run-cache is an associative storage <code>repo state --&gt; run results<\/code> outside of Git repo (in data remote).<\/li>\n<\/ol>\n\n<p>Disclaimer: I'm one of the creators of DVC.<\/p>",
        "Answer_comment_count":5.0,
        "Answer_creation_date":"2020-04-16 09:36:01.967000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":6.0,
        "Question_valid_tags":[
            "dvc"
        ]
    },
    {
        "Question_id":51183169,
        "Question_title":"AWS Sagemaker Multiple Object Detection in Image Recognition \/ Classification",
        "Question_body":"<p>Does anyone know if any SageMaker built-in algorithm supports multiple object detection in image recognition? I am thinking something like multi-label image training and detection \/ inference. <\/p>\n\n<p>Thus, can we: <\/p>\n\n<p><strong>a) train using multi-label images<\/strong> <\/p>\n\n<p>and\/or <\/p>\n\n<p><strong>b) infer multiple objects from images (sort of like AWS Rekognition but with custom labels and training \/ transfer learning).<\/strong><\/p>\n\n<p>Also, I know that the doc for SageMaker Image Classification Algorithm says \"takes an image as input and classifies it into <strong>one<\/strong> of multiple output categories\". <\/p>\n\n<p>Any recommendations are also welcome.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_date":"2018-07-05 03:54:14.690000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":"2018-08-30 09:14:29.950000 UTC",
        "Question_score":0,
        "Question_tags":"amazon-web-services|machine-learning|computer-vision|amazon-rekognition|amazon-sagemaker",
        "Question_view_count":1764,
        "Owner_creation_date":"2011-03-24 04:39:38.957000 UTC",
        "Owner_last_access_date":"2022-03-14 04:59:19.877000 UTC",
        "Owner_location":"San Francisco Bay Area",
        "Owner_reputation":645,
        "Owner_up_votes":46,
        "Owner_down_votes":2,
        "Owner_views":75,
        "Answer_body":"<p>There is a new built-in algorithm released with Amazon Sagemaker today for object detection. Based on the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/object-detection.html\" rel=\"nofollow noreferrer\">documentation<\/a>, Amazon SageMaker Object Detection uses the Single Shot multibox Detector (SSD) algorithm. The response from the inference contains an array consists of a predicted class label of the object detected, associated confidence score and bounding box co-ordinates.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_date":"2018-07-13 04:30:53.520000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "amazon-sagemaker"
        ]
    },
    {
        "Question_id":69245175,
        "Question_title":"Error in azureml \"Non numeric value(s) were encountered in the target column.\"",
        "Question_body":"<p>I am using Automated ML to run a time series forecasting pipeline.<\/p>\n<p>When the AutoMLStep gets triggered, I get this error: <code>Non numeric value(s) were encountered in the target column<\/code>.<\/p>\n<p>The data to this step is passed through an OutputTabularDatasetConfig, after applying the read_delimited_files() on an OutputFileDatasetConfig. I've inspected the prior step, and the data is comprised of a 'Date' column and a numeric column called 'Place' with +80 observations in monthly frequencies.<\/p>\n<p>Nothing seems to be wrong with the column type or the data. I've also applied a number of techniques on the data prep side e.g. pd.to_numeric(), astype(float) to ensure it is numeric.<\/p>\n<p>I've also tried forcing this through the FeaturizationConfig() <code>add_column_purpose('Place','Numeric')<\/code> but in this case, I get another error: <code>Expected column(s) Place in featurization config's column purpose not found in X.<\/code><\/p>\n<p>Any thoughts on how to solve?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-09-19 16:41:20.553000 UTC",
        "Question_favorite_count":null,
        "Question_last_edit_date":null,
        "Question_score":1,
        "Question_tags":"automl|azure-machine-learning-service",
        "Question_view_count":127,
        "Owner_creation_date":"2014-06-11 05:02:56.833000 UTC",
        "Owner_last_access_date":"2022-09-24 23:36:40.860000 UTC",
        "Owner_location":"Redmond, WA, United States",
        "Owner_reputation":525,
        "Owner_up_votes":70,
        "Owner_down_votes":0,
        "Owner_views":52,
        "Answer_body":"<p>So a few learnings on this interacting with the stellar Azure Machine Learning engineering team.<\/p>\n<ol>\n<li>When calling the <code>read_delimited_files()<\/code> method, ensure that the output folder does not have many inputs or files. For example, if all intermediate outputs are saved to a common folder, it may read all the prior inputs into this folder, and depending upon the shape of the data, borrow the schema from the first file, or confuse all of them together. This can lead to inconsistencies and errors. In my case, I was dumping many files to the same location, hence this was causing confusion for this method. The fix is either to distinctly mark the output folder (e.g. with a UUID) or give different paths.<\/li>\n<li>The dataframe from <code>read_delimiter_files()<\/code> may treat all columns as object type which can lead to a data type check failure (i.e. label_column needs to be numeric). To mitigate, explictly state the type. For example:<\/li>\n<\/ol>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.data import DataType\nprepped_data = prepped_data.read_delimited_files(set_column_types={&quot;Place&quot;:DataType.to_float()})\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-09-30 21:36:16.347000 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":1.0,
        "Question_valid_tags":[
            "azure-machine-learning-service"
        ]
    }
]