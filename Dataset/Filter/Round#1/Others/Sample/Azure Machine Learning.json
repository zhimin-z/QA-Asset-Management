[
	{
		"Question_title": "Endpoint Deletion",
		"Question_creation_date": "2021-08-11T04:45:45.543Z",
		"Question_link": "https://learn.microsoft.com/answers/questions/509079/endpoint-deletion.html",
		"Question_tag": [
			"azure-machine-learning",
			"azure-container-instances"
		],
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_comment_count": 3,
		"Question_follower_count": 16,
		"Question_upvote_count": 0,
		"Question_body": "Hi , I Have tried to deploy a time series model which was created using Auto-Ml,and i tried to deplot that model as a Azure Container instance service ,but it got failed,in the model section it shows deploy status as Failed but in the Endpoints Section it shows that the deployment is in transitioning state,So I'm unable to delete the Endpoint ,can you help me resolve this?\n\nNiranjan",
		"Answers": []
	},
	{
		"Question_title": "Connection Error 403",
		"Question_creation_date": "2022-09-19T18:13:07.617Z",
		"Question_link": "https://learn.microsoft.com/answers/questions/1014310/connction-error-403.html",
		"Question_tag": [
			"azure-machine-learning",
			"azure-storage-accounts",
			"azure-blob-storage",
			"azure-event-hubs",
			"azure-stream-analytics"
		],
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_comment_count": 2,
		"Question_follower_count": 36,
		"Question_upvote_count": 0,
		"Question_body": "Hi, I keep on getting this error while trying to access the Twitter API using the TwitterClient. The Client builds successfully but then it shows this error when I run: dotnet run",
		"Answers": [
			{
				"Answer_creation_date": "2022-09-19T20:27:36.457Z",
				"Answer_upvote_count": 1,
				"Answer_body": "This is not an Azure error, right?\n\nThis is the developer forum https://developer.microsoft.com/en-us/community/\nAnd this is the .NET forum https://techcommunity.microsoft.com/t5/net/ct-p/dotnet",
				"Answer_comment_count": 1,
				"Answer_has_accepted": false
			}
		]
	},
	{
		"Question_title": "(Beginner/New to Azure ML)Unable to execute Jupyter Notebook code in Azure ML",
		"Question_creation_date": "2020-10-25T15:30:45.287Z",
		"Question_link": "https://learn.microsoft.com/answers/questions/138151/beginnernew-to-azure-mlunable-to-execute-jupyter-n.html",
		"Question_tag": [
			"azure-machine-learning"
		],
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_comment_count": 2,
		"Question_follower_count": 4,
		"Question_upvote_count": 0,
		"Question_body": "Hello all,\nWhile trying to follow official guide for MOC as described here:\nhttps://docs.microsoft.com/en-in/learn/modules/classify-images-custom-vision/3-create-image-classifier\n\nI am stuck at step 3 under 'Download the exercise files':\n\nWhen the new notebook has been created, ensure that the compute instance you created previously is selected in the Compute box, and that it has a status of Running. Then, in the rectangular cell that has been created in the notebook, paste the following code:\n\n!git clone https://github.com/MicrosoftDocs/ai-fundamentals\n\nAnd same issue running jupyter notebook here (step 11 running the code to test deployment):\n\nhttps://docs.microsoft.com/en-in/learn/modules/create-regression-model-azure-machine-learning-designer/deploy-service\n\n\n\n\nWhat could be missing,why am I unable to see the output of the execution, it was working fine 2 days back with old Compute Instance(even tried creating new instance but unable to see any output or import of github folder).\n\n\n\n\nRegards.\nAditya Garg",
		"Answers": [
			{
				"Answer_creation_date": "2020-10-26T22:31:36.267Z",
				"Answer_upvote_count": 0,
				"Answer_body": "Hi, thanks for reaching out. Here's a screenshot of how the output should look like. Have you tried clicking on the refresh button?",
				"Answer_comment_count": 2,
				"Answer_has_accepted": false
			}
		]
	},
	{
		"Question_title": "is there any way I can serialize the R model which trained using caret or some other libraries instead of RevoScaleR?",
		"Question_creation_date": "2021-05-18T01:54:12.49Z",
		"Question_link": "https://learn.microsoft.com/answers/questions/398544/is-there-any-way-i-can-serialize-the-r-model-which.html",
		"Question_tag": [
			"sql-server-general",
			"azure-machine-learning"
		],
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_comment_count": 1,
		"Question_follower_count": 16,
		"Question_upvote_count": 0,
		"Question_body": "I am using SQL Server Machine Learning services to Run a R model. I am able to serialize it using RevoScaleR and rxSerializeModel but I want to use caret instead of RevoScaleR here is my code which is giving an error when asks to serialiaze the trained model\n\nStep1 train model using SP\n\nDROP PROCEDURE IF EXISTS generate_PCL_R_native_model;\ngo\nCREATE PROCEDURE generate_PCL_R_native_model (@model_type varchar(30), @trained_model varbinary(max) OUTPUT)\nAS\nBEGIN\n\n EXECUTE sp_execute_external_script\n   @language = N'R'                                  -- Spesify langauge and R code \n , @script = N'                                      \n\nrequire(\"RevoScaleR\")\nrequire(\"caret\")\nrequire(\"ranger\")\nlibrary(caret)\nlibrary(ranger)\n\nfitControl <- trainControl(method = \"cv\",\nnumber = 5,\n\n                        savePredictions = TRUE\n                       ,\n                           \n                        classProbs = T, \n                       verboseIter = F\n                        )\n                        rf_grid <- expand.grid(mtry = 2,\n                    splitrule = c(\"gini\", \"extratrees\"),\n                    min.node.size = c(1, 3, 5));\n\n\n\nif(model_type == \"dtree\") {\nmodel_linmod <-\ntrain(pclitemspct10r_new ~\n\n + d1\n + d2\n + d3\n + d4\n + d5\n + d6\n + d7\n + e1\n + e2\n + e3\n + e4\n + e5\n + e6\n + marriedd1\n + marriedd2\n    \n    \n    \n , data = PCL_train_data, \n             method = \"ranger\",\n             trControl = fitControl,\n             #tuneGrid = rf_grid\n             )\n    \n #serialize the model\n    \n trained_model <- as.raw(serialize((model_linmod, NULL));\n }\n\n'\n\n , @input_data_1 = N'select  * from [dbo].[training_IOP_data_new]'\n , @input_data_1_name = N'PCL_train_data'; \n\nSTEP 2 - Setup model table for storing the model\n\nDROP TABLE IF EXISTS PCL_models;\nGO\nCREATE TABLE PCL_models (\nmodel_name VARCHAR(30) NOT NULL DEFAULT('default model'),\nlang VARCHAR(30),\nmodel VARBINARY(MAX),\nnative_model VARBINARY(MAX),\nPRIMARY KEY (model_name, lang)\n\n);\nGO\nStep 3 Save the model in table format\n\nDECLARE @model2 VARBINARY(MAX);\nEXEC generate_PCL_R_native_model \"dtree\", @model2 OUTPUT;\nINSERT INTO PCL_models (model_name, native_model, lang) VALUES('dtree_model', @model2, 'R');\n\nis there any way I can serialize the R model which trained using caret or some other libraries instead of RevoScaleR.",
		"Answers": []
	},
	{
		"Question_title": "Code: AuthorizationFailed",
		"Question_creation_date": "2022-05-07T09:50:05.187Z",
		"Question_link": "https://learn.microsoft.com/answers/questions/840311/code-authorizationfailed.html",
		"Question_tag": [
			"azure-virtual-machines",
			"azure-machine-learning"
		],
		"Question_has_accepted_answer": true,
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_follower_count": 10,
		"Question_upvote_count": 0,
		"Question_body": "Unit 4 of 7\nExercise - Back up an Azure virtual machine\nCreate a backup for Azure virtual machines\n\nI am unable to run the following command in cloud shell to set up the environment:\nRGROUP=$(az group create --name vmbackups --location westus2 --output tsv --query name)\n\nFollowing error pop up:\nERROR: (AuthorizationFailed) The client 'live.com#...... does not have authorization to perform action 'Microsoft.Resources/subscriptions/resourcegroups/write' over scope '/subscriptions/......./resourcegroups/vmbackups' or the scope is invalid. If access was recently granted, please refresh your credentials.\nCode: AuthorizationFailed\nMessage: The client 'live.com#l...... with object id '.......' does not have authorization to perform action 'Microsoft.Resources/subscriptions/resourcegroups/write' over scope '/subscriptions/...../resourcegroups/vmbackups' or the scope is invalid. If access was recently granted, please refresh your credentials.\n\nWhen I do refresh, sign out or sign in do not helps. Anybody has any idea what to do?\nThank you",
		"Answers": [
			{
				"Answer_creation_date": "2022-05-07T10:07:44.643Z",
				"Answer_upvote_count": 1,
				"Answer_body": "Hello @Krisztian-8931\n\nWelcome to Microsoft Q&A community.\n\nHave you tried to do this first?\n\n\n\n\n\n\n\n\n\nCheers,\n\nPlease \"Accept the answer\" if the information helped you. This will help us and others in the community as well.",
				"Answer_comment_count": 2,
				"Answer_has_accepted": true
			}
		]
	},
	{
		"Question_title": "Advantage of clustering algorithm",
		"Question_creation_date": "2020-12-23T10:37:02.847Z",
		"Question_link": "https://learn.microsoft.com/answers/questions/208687/advantage-of-clustering-algorithm.html",
		"Question_tag": [
			"not-supported-azure",
			"azure-machine-learning"
		],
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_comment_count": 1,
		"Question_follower_count": 5,
		"Question_upvote_count": 0,
		"Question_body": "We don't need the name of attributes in clustering, so, if I do not know my attribute names how can I understand that which data should I enter and also if I do not know the name of the attributes how can I give the axis name of the plotted graph? If it is the advantage of clustering algorithm, then why it is not the advantage of other algorithms, because if we do not know the name of the attributes we can create our models because if we have attribute values that is enough, but why we need attribute names too in other algorithms?",
		"Answers": [
			{
				"Answer_creation_date": "2020-12-23T16:35:59.897Z",
				"Answer_upvote_count": 0,
				"Answer_body": "@SanniddhaChakrabarti-9451 Thanks, If the products data is not already labeled and ready for training, you can start with a clustering problem.\nOnce you identify those clusters, a Domain Expert can review those clusters and try to set a name for each (the Categories/classes).\nThen, all the data can be labeled according to those new categories/classes and finally train a model.\n\nFrom there, with a trained model, you could \u201cpredict\u201d what category/class a product should be assigned based on its product\u2019s name and description.\n\n\u2022 For the clustering problem you need to directly use a framework such as Scikit-Learn (or even ML.NET in C#).\n\u2022 Defining a category/class name for each identified cluster needs to be done manually by a Domain Expert.\n\u2022 Labeling the data could be semi-automated with a custom program, based on the multiple clusters defined, it would label each row with a new class-caterogy-name defined for each of the identified clusters.\n\u2022 For the multi-class classification model training, you can use Azure Automated ML as the easiest approach.",
				"Answer_comment_count": 3,
				"Answer_has_accepted": false
			}
		]
	},
	{
		"Question_title": "Token authentication failed: 'utf-8' codec can't decode byte 0xe4 in position 0: invalid continuation byte",
		"Question_creation_date": "2021-12-22T18:48:18.043Z",
		"Question_link": "https://learn.microsoft.com/answers/questions/672951/token-authentication-failed-39utf-839-codec-can39t.html",
		"Question_tag": [
			"azure-machine-learning",
			"azure-databricks",
			"azure-event-hubs"
		],
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_comment_count": 2,
		"Question_follower_count": 16,
		"Question_upvote_count": 0,
		"Question_body": "I'm trying to send the json data from azure ml to eventhub using codes below\n\nimport json\nd = result.to_dict(orient='records')\ndata = json.dumps(d,ensure_ascii=False)\n\nimport asyncio\nfrom azure.eventhub.aio import EventHubProducerClient\nfrom azure.eventhub import EventData\nimport time\nconn_sting = \"Endpoint=***\"\nasync def run():\nproducer = EventHubProducerClient.from_connection_string(conn_str=conn_string)\nasync with producer:\n\n event_data_batch = await producer.create_batch(partition_id='0')\n event_data_batch.add(EventData(data))\n   \n await producer.send_batch(event_data_batch)\n\nnest_asyncio.apply()\nloop = asyncio.get_event_loop()\nloop.run_until_complete(run())\nprint(\"sent to eventhub\")\n\nand getting follwing error..\n\nToken authentication failed: 'utf-8' codec can't decode byte 0xe4 in\nposition 0: invalid continuation byte\nToken authentication failed: 'utf-8' codec can't decode byte 0xe4 in\nposition 0: invalid continuation byte\n\nanyone could help debug the error? thanks",
		"Answers": []
	},
	{
		"Question_title": "Can I set a cost limit?",
		"Question_creation_date": "2021-10-09T17:50:37.46Z",
		"Question_link": "https://learn.microsoft.com/answers/questions/584194/can-i-set-a-cost-limit.html",
		"Question_tag": [
			"azure-machine-learning"
		],
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_comment_count": 3,
		"Question_follower_count": 9,
		"Question_upvote_count": 0,
		"Question_body": "Hi all;\n\nWhen I run a compute to train a model, is there a way to set a maximum charge for that run? And if it hits that number, it stops?\n\nI have this nightmare that I set a training model to run and when done, it's a $12,000.00 charge.\n\nthanks - dave",
		"Answers": []
	},
	{
		"Question_title": "Is there a way to get all the machine learning algorigthms tried by azure automl for a machine learning job in python sdk?",
		"Question_creation_date": "2022-09-05T02:37:49.883Z",
		"Question_link": "https://learn.microsoft.com/answers/questions/993641/is-there-a-way-to-get-all-the-machine-learning-alg.html",
		"Question_tag": [
			"azure-machine-learning"
		],
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_follower_count": 11,
		"Question_upvote_count": 0,
		"Question_body": "I am trying azure automl through python sdk. What I need is to get all the algorithm names along with the best algorithm azure tried for that particular job using python sdk.\nI have been exploring documentation but didn't get much.",
		"Answers": [
			{
				"Answer_creation_date": "2022-09-05T12:46:12.697Z",
				"Answer_upvote_count": 0,
				"Answer_body": "@Nuron-5007 I believe this can be achieved by getting the best and fitted model from your automl run's get_output and then printing the details of the algorithms used by this model.\n\n automl_config = AutoMLConfig(\u2026)\n automl_run = experiment.submit(automl_config \u2026)\n best_run, fitted_model = automl_run.get_output()\n\n\n\n\nUse fitted_model.steps to print the selected algorithm with its hyperparameter values\nThis is documented here from the Azure ML documentation.\n\nI hope this helps!!\n\n\n\n\nIf an answer is helpful, please click on  or upvote  which might help other community members reading this thread.",
				"Answer_comment_count": 0,
				"Answer_has_accepted": false
			}
		]
	},
	{
		"Question_title": "Wrong display of .ilearner file",
		"Question_creation_date": "2021-05-12T16:01:29.703Z",
		"Question_link": "https://learn.microsoft.com/answers/questions/392953/wrong-display-of-ilearner-file.html",
		"Question_tag": [
			"azure-machine-learning"
		],
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_comment_count": 2,
		"Question_follower_count": 9,
		"Question_upvote_count": 0,
		"Question_body": "Can anyone help me with this issue please ?\nI get the file (in .ilearner format) cannot be correctly displayed cuz it contains an unknown extension when I try to open it.",
		"Answers": [
			{
				"Answer_creation_date": "2021-05-25T06:55:57.853Z",
				"Answer_upvote_count": 0,
				"Answer_body": "@Salah-1213 Hello Salah,\n\nPer my research, the ilearn interface is supported on Azure Machine Learning Studio(class), but I can not find any official document indicate this is supported on Azure Machine Learning Designer as well. Sorry for the experience. https://docs.microsoft.com/en-us/azure/machine-learning/studio-module-reference/ilearner-interface\n\nIf there is any document you are using has any clue please let me know.\n\n\n\n\nRegards,\nYutong",
				"Answer_comment_count": 0,
				"Answer_has_accepted": false
			}
		]
	},
	{
		"Question_title": "Azure Machine Learning - Data Labeling - Refresh",
		"Question_creation_date": "2021-11-08T15:31:50.357Z",
		"Question_link": "https://learn.microsoft.com/answers/questions/619198/azure-machine-learning-data-labeling-refresh.html",
		"Question_tag": [
			"azure-machine-learning"
		],
		"Question_has_accepted_answer": true,
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_follower_count": 9,
		"Question_upvote_count": 0,
		"Question_body": "Hello,\n\nI've started a new Data labeling project in Azure Machine Learning and I configured the incremental refresh.\n\nHow often is the data refreshed? Is it possible to force a refresh manually? Is it possible to execute this command via SDK (Python or PowerShell)?\n\nThanks.\n\nG",
		"Answers": [
			{
				"Answer_creation_date": "2021-11-09T02:24:49.197Z",
				"Answer_upvote_count": 0,
				"Answer_body": "Hi, data is refreshed within 24hrs. Currently, incremental refresh is only enabled using the portal and there's no option to trigger refresh manually.\n\n\n\n\n--- Kindly Accept Answer if the information helps. Thanks.",
				"Answer_comment_count": 1,
				"Answer_has_accepted": true
			}
		]
	},
	{
		"Question_title": "Notebook files have disaperred",
		"Question_creation_date": "2022-05-18T12:06:00.357Z",
		"Question_link": "https://learn.microsoft.com/answers/questions/854288/notebook-files-have-disaperred.html",
		"Question_tag": [
			"azure-machine-learning"
		],
		"Question_has_accepted_answer": true,
		"Question_answer_count": 1,
		"Question_comment_count": 1,
		"Question_follower_count": 11,
		"Question_upvote_count": 0,
		"Question_body": "Hello,\n\nIn my MLStudio my notebook files window has disappeared so I can not access any of my data (as seen on the image) and I do not know what to do.\n\nPlease your help to solve this as soon as poosible.\n\nThank you.",
		"Answers": [
			{
				"Answer_creation_date": "2022-05-19T02:46:25.6Z",
				"Answer_upvote_count": 0,
				"Answer_body": "Hello,\n\nThanks for reaching out to us. Could you please check the access of Storage? https://docs.microsoft.com/en-us/azure/storage/blobs/assign-azure-role-data-access?tabs=portal#assign-an-azure-role\n\nTo access these storage services, you must have at least Storage Blob Data Reader access to the storage account. Only storage account owners can change your access level via the Azure portal.\n\nOr, your admin put the data storage behind V-Net and you can not get access to it- https://docs.microsoft.com/en-us/azure/machine-learning/how-to-identity-based-data-access#work-with-virtual-networks\nIn this situation, you need to ask permission from your admin.\n\nCould you please share which situation you are in?\n\n\n\n\nRegards,\nYutong",
				"Answer_comment_count": 2,
				"Answer_has_accepted": true
			}
		]
	},
	{
		"Question_title": "The request failed with status code: 502 error while consuming the model deployed",
		"Question_creation_date": "2021-03-17T11:56:57.267Z",
		"Question_link": "https://learn.microsoft.com/answers/questions/318618/the-request-failed-with-status-code-502-error-whil.html",
		"Question_tag": [
			"azure-machine-learning"
		],
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_comment_count": 2,
		"Question_follower_count": 7,
		"Question_upvote_count": 0,
		"Question_body": "I have deployed an ML model trained with combining CSVs through dataset. When I try to consume the model REST endpoint through python. I get 502 error. In score.py, I predicted by using the dataset in the workspace as I need to use Count vectorizer. Below is my score.py code for your reference.\n\n def init():\n     global model\n     model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), \"prediction-model.pickle\")\n     model = joblib.load(model_path)\n    \n    \n def run(data):\n     try:\n         data = json.loads(data)['data']\n         print(data)\n         workspace = Workspace.get(name=\"xxx\", subscription_id='xxx',\n                                   resource_group='xxx')\n         dataset_name = 'prediction_ds'\n         prediction_ds = Dataset.get_by_name(workspace=workspace, name=dataset_name)\n         df = prediction_ds.to_pandas_dataframe()\n         df = df[pd.notnull(df['DESCRIPTION'])]\n         df = df[pd.notnull(df['CUSTOMERCODE'])]\n         col = ['CUSTOMERCODE', 'DESCRIPTION']\n         df = df[col]\n         df.columns = ['CUSTOMERCODE', 'DESCRIPTION']\n         df['category_id'] = df['DESCRIPTION'].factorize()[0]\n         df = df.applymap(str)\n         X_train, X_test, y_train, y_test = train_test_split(df['CUSTOMERCODE'], df['DESCRIPTION'], random_state=0)\n         count_vect = CountVectorizer()\n         count_vect.fit_transform(X_train)\n         predicted_result = model.predict(count_vect.transform(data))\n         return predicted_result.tolist()\n     except Exception as e:\n         print(\"exception occured\")\n         error = str(e)\n         print(error)\n         logging.info(error)\n         return error",
		"Answers": []
	},
	{
		"Question_title": "In multi step pipeline execution, how to maintain the data type of the columns when pass the dataset to next step",
		"Question_creation_date": "2022-02-25T16:08:24.397Z",
		"Question_link": "https://learn.microsoft.com/answers/questions/751127/in-multi-step-pipeline-execution-how-to-maintain-t.html",
		"Question_tag": [
			"azure-machine-learning"
		],
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_follower_count": 9,
		"Question_upvote_count": 0,
		"Question_body": "i am building a pipeline with multiple steps.\n\nStep 1 - Read the data from tabular dataset(with proper data types) , apply transformation and create an output dataset which will be passed as input to the step 2. However when i opened this dataset from the pipeline run log, the datatype all become string instead of maintaining the original data types of the input tabular data set\n\n\nStep 2 - use the output dataset of step 1 as input and apply some more transformations. However i have some logic based on data types which doesn't work because intermediate data set does not maintain the same data structure\n\nis there anyway we can maintain the original data types/schema structure in the intermediate datasets?\n\nHere is some snippets on my code :\n\nfeature_work = (\nOutputFileDatasetConfig(\nname=\"data_enhanced_add_global_variables\",\ndestination=(def_blob_store, \"data/processed/output/1\"),\n)\n.read_delimited_files()\n.as_upload(overwrite=True)\n\n\n\n\nfeature_engineering_step_1 = PythonScriptStep(name = \"1_feature_engineering\",\n#source_directory = experiment_folder,\nscript_name = \"1_feature_engineering.py\",\narguments = ['--input-data', data_aggregate_DS.as_named_input('raw_data'),\n'--prepped-data', feature_work],\n#outputs=[prepped_data_folder],\noutputs=[feature_work],\ncompute_target = compute_name,\nrunconfig = pipeline_run_config,\nallow_reuse = True)\n# Step 2\nfeature_engineering_step_2 = PythonScriptStep(name = \"2_feature_engineering\",\n#source_directory = experiment_folder,\nscript_name = \"2_feature_engineering.py\",\narguments = ['--input-data', feature_work.as_input(name='raw_data'),\n'--prepped-data', feature_work1],\noutputs=[feature_work1],\ncompute_target = compute_name,\nrunconfig = pipeline_run_config,\nallow_reuse = True)",
		"Answers": [
			{
				"Answer_creation_date": "2022-02-28T12:43:31.087Z",
				"Answer_upvote_count": 0,
				"Answer_body": "@VinothKumarK-8698 Thanks for the question. Can you please share the sample notebook that you are trying.\nHere is the notebook and doc that can help.\nOutputFileDatasetConfig as Tutorial: ML pipelines for batch scoring - Azure Machine Learning | Microsoft Docs as a means to pass data between pipeline steps.",
				"Answer_comment_count": 2,
				"Answer_has_accepted": false
			}
		]
	},
	{
		"Question_title": "Azure On-Demand ML cluster from a search in the data catalog",
		"Question_creation_date": "2021-04-23T20:28:46.297Z",
		"Question_link": "https://learn.microsoft.com/answers/questions/369952/azure-on-demand-ml-cluster-from-a-search-in-the-da.html",
		"Question_tag": [
			"azure-machine-learning",
			"azure-data-catalog"
		],
		"Question_has_accepted_answer": true,
		"Question_answer_count": 2,
		"Question_comment_count": 1,
		"Question_follower_count": 9,
		"Question_upvote_count": 1,
		"Question_body": "I'm trying to implement a self-service solution in Azure so users can run a Jupyter or PySpark notebook on-Demand/automatically with the dataset they found a search in the Azure Data Catalog. I visualize, once the user finds the data in a search, there will be a link that will take him/her to a Notebook and the dataset can be used for analysis. Any suggestion would be very much appreciated!",
		"Answers": [
			{
				"Answer_creation_date": "2021-04-26T10:22:10.247Z",
				"Answer_upvote_count": 0,
				"Answer_body": "@JairoMelo-1657 Thanks for the question.Azure Purview can find, understand, and consume data sources. Please follow the Azure Purview documentation: https://docs.microsoft.com/en-us/azure/purview/\n\nand We have Azure Open Datasets where you can download a Notebook for AML, Databricks or Synapse that explores the data: Azure Open Datasets Catalog | Microsoft Azure. What are open datasets? Curated public datasets - Azure Open Datasets | Microsoft Docs.",
				"Answer_comment_count": 0,
				"Answer_has_accepted": true
			},
			{
				"Answer_creation_date": "2021-04-28T11:51:31.093Z",
				"Answer_upvote_count": 1,
				"Answer_body": "Thank you very much for your response. I'll look into Azure Purview. Best! J.",
				"Answer_comment_count": 0,
				"Answer_has_accepted": false
			}
		]
	},
	{
		"Question_title": "Load the most recent data from Date partitioned folder",
		"Question_creation_date": "2022-03-08T08:09:20.863Z",
		"Question_link": "https://learn.microsoft.com/answers/questions/763313/load-the-most-recent-data-from-date-partitioned-fo.html",
		"Question_tag": [
			"azure-machine-learning"
		],
		"Question_has_accepted_answer": true,
		"Question_answer_count": 1,
		"Question_comment_count": 2,
		"Question_follower_count": 16,
		"Question_upvote_count": 0,
		"Question_body": "Hello,\n\nI have set up a pipeline with Azure Data Factory in order to move data from my on-premises Oracle DB to parquet files in an Azure blob container every 10 days.\nThe folder structure of my blob container is as follows:\n\nonpremises/2022/02/18/file1.parquet\nonpremises/2022/02/18/file2.parquet\nonpremises/2022/02/18/file3.parquet\n\n\nonpremises/2022/02/28/file1.parquet\nonpremises/2022/02/28/file2.parquet\nonpremises/2022/02/28/file3.parquet\n\n\nonpremises/2022/03/08/file1.parquet\nonpremises/2022/03/08/file2.parquet\nonpremises/2022/03/08/file3.parquet\n\n\n...\n\nNow I'm trying to set up a pipeline in Azure ML which will run every time new data is coming into this container.\nIn my script below, I start by getting a reference to my container before calling the function 'from_parquet_files' to read from Parquet files.\nProblem: the script reads all files from every folder and adds a data column to the dataset (I believe it is because of the parameter 'partition_format').\n\n from azureml.core import Workspace, Datastore\n    \n # Get a reference to the workspace\n ws = Workspace.from_config()\n    \n # Reference to the datastore 'onpremises' from which we will contruct our dataset\n data_store = Datastore(ws, \"onpremises\")\n    \n from azureml.core import Dataset\n # Create a dataset from the data stored in datastore 'onpremises' at the specified path\n specs_dataset = Dataset.Tabular.from_parquet_files(path=(data_store, ''), partition_format='/{PartitionDate:yyyy/MM/dd}/')\n    \n # Register the dataset to the workspace. Increments the version if dataset already exists.\n specs_dataset.register(workspace=ws, name=\"specs\", description=\"Specs data from on-premises\", create_new_version=True)\n\n\n\nWhat I would like to do is to read only the most recent set of files (in my case, files listed under 'onpremises/2022/03/08/').\nAs the pipeline will run automatically, it should detect what is the most recent data among the folder structure.\nIs there a simple way to achieve this programmatically?\n\nThanks in advance.",
		"Answers": [
			{
				"Answer_creation_date": "2022-03-09T08:13:14.767Z",
				"Answer_upvote_count": 1,
				"Answer_body": "@ThierryL-3166 You could only pass the required files by getting the year, month and day from the timestamp or date output. If your pipeline runs on schedule, you could list all the paths for all the days since the last run and load the files. I think something like below should work.\n\n # create tabular dataset from multiple paths\n from datetime import date\n from datetime import timedelta\n    \n today = date.today()\n    \n yesterday = today - timedelta(1)\n        \n d1 = today.strftime(\"%Y/%m/%d\")\n d2=yesterday.strftime(\"%Y/%m/%d\") \n    \n    \n path1 = 'onpremises/'+ d1 + '/*.parquet'\n path2 = 'onpremises/'+ d2 + '/*.parquet'\n    \n data_paths = [(datastore, path1),(datastore, path2)]\n tabular_dataset = Dataset.Tabular.from_parquet_files(path=data_paths)\n\n\n\n\n\n\n\n\nIf an answer is helpful, please click on  or upvote  which might help other community members reading this thread.",
				"Answer_comment_count": 1,
				"Answer_has_accepted": true
			}
		]
	},
	{
		"Question_title": "Project in Data Labeling not working, getting only \"Loading project details\"",
		"Question_creation_date": "2022-03-19T07:51:45.203Z",
		"Question_link": "https://learn.microsoft.com/answers/questions/778679/project-in-data-labeling-not-working-getting-only.html",
		"Question_tag": [
			"azure-machine-learning"
		],
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_follower_count": 9,
		"Question_upvote_count": 0,
		"Question_body": "Project in Azure Machine Learning Studio in Data Labeling was working, we did label it every day. One day we just could not open it, it showed only Loading project details.. for hours.\nSAS token is working, it also work for our admin account, but not for Labellers.\nDo you have any idea?",
		"Answers": [
			{
				"Answer_creation_date": "2022-03-21T16:36:47.9Z",
				"Answer_upvote_count": 0,
				"Answer_body": "@ika-8686 Thanks for the question. Please share details of your experiment and issue from the ml.azure.com portal for a service engineer to lookup the issue from the back-end? This option is available from the top right hand corner of the portal by clicking the smiley face, Please select the option Microsoft can email you about the feedback along with a screen shot so our service team can lookup and advise through email.",
				"Answer_comment_count": 1,
				"Answer_has_accepted": false
			}
		]
	},
	{
		"Question_title": "Auto ML processor never utilized, utilization below 10%",
		"Question_creation_date": "2021-10-04T18:53:37.237Z",
		"Question_link": "https://learn.microsoft.com/answers/questions/576887/auto-ml-processor-never-utilized-utilization-below.html",
		"Question_tag": [
			"azure-machine-learning"
		],
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_follower_count": 8,
		"Question_upvote_count": 0,
		"Question_body": "I'm trying to run an Automated ML run with a 5Gb .csv file as the dataset. I've selected the Standard_D8_v3 (8 cores, 32 GB RAM, 200 GB disk) for my compute cluster and it allows 2 nodes.\n\nWhen it starts out initially the CPU Utilization spikes to 80%, but then stays below 1% for as long as I'll let it run, which has been over a day. I see this utilization measure under the \"Monitoring (preview)\" tab of the run, and I've also setup configured Metrics at the Workspace level, which reflect the same thing. The CPUMemoryUtilization never raises.\n\nNo models have ever appeared in the \"models\" tab.\n\nThe first run continually indicates \"Setting up the run\", but the one child run indicates \"running\". I think I've got a managed ID issue.\n\nI suspect the training never starts. Is there an error I can look for in the logs? I'm not even sure which log would have it. I'll post them here if needed.",
		"Answers": [
			{
				"Answer_creation_date": "2021-10-05T03:34:15.603Z",
				"Answer_upvote_count": 1,
				"Answer_body": "Hi, perhaps you cancel the run and try again? Here's information on how to view log information for a run (including description of log files).",
				"Answer_comment_count": 0,
				"Answer_has_accepted": false
			}
		]
	},
	{
		"Question_title": "D365 Demand Forecasting - Can we connect to new Azure ML Service instead of a classic studio service?",
		"Question_creation_date": "2020-12-03T01:40:42.257Z",
		"Question_link": "https://learn.microsoft.com/answers/questions/183990/d365-demand-forecasting-can-we-connect-to-new-azur.html",
		"Question_tag": [
			"azure-machine-learning",
			"azure-machine-learning-inference"
		],
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_comment_count": 3,
		"Question_follower_count": 8,
		"Question_upvote_count": 0,
		"Question_body": "There is a D365 forecasting option that allows to connect to a azure ml classic studio service. Can we connect from D365 to the new Azure ML Service? I couldnt find any documentation about this, any pointers please.Thanks.",
		"Answers": [
			{
				"Answer_creation_date": "2020-12-04T08:31:11.253Z",
				"Answer_upvote_count": 0,
				"Answer_body": "@SriramNarayanan-6939 you can deploy a real-time endpoint in Azure Machine Learning designer and get the REST endpoint/token by following this doc: https://docs.microsoft.com/en-us/azure/machine-learning/tutorial-designer-automobile-price-deploy",
				"Answer_comment_count": 1,
				"Answer_has_accepted": false
			}
		]
	},
	{
		"Question_title": "Can't find the run button",
		"Question_creation_date": "2021-04-08T03:31:02.423Z",
		"Question_link": "https://learn.microsoft.com/answers/questions/348777/can39t-find-the-run-button.html",
		"Question_tag": [
			"azure-machine-learning"
		],
		"Question_has_accepted_answer": true,
		"Question_answer_count": 2,
		"Question_comment_count": 0,
		"Question_follower_count": 10,
		"Question_upvote_count": 1,
		"Question_body": "Do I need to add the .ipynb extension manually to my notebook file ?\nI can't find the run button.",
		"Answers": [
			{
				"Answer_creation_date": "2021-04-08T09:06:51.417Z",
				"Answer_upvote_count": 2,
				"Answer_body": "@paulgureghian-9874 You can re-name your file with .ipynb extension which should help to display the cells and the available options like run button for cell. Usually while creating new files in your workspace the UI prompts to select the extension of the file. If you can create a new file with .ipynb extension and copy these cells individually that should also work. Thanks.",
				"Answer_comment_count": 2,
				"Answer_has_accepted": true
			},
			{
				"Answer_creation_date": "2021-04-08T19:02:09.323Z",
				"Answer_upvote_count": 0,
				"Answer_body": "Thanks. will add the .ipynb extension.",
				"Answer_comment_count": 0,
				"Answer_has_accepted": false
			}
		]
	},
	{
		"Question_title": "Is there any kinect version that helps in speech therapy?",
		"Question_creation_date": "2020-11-23T09:06:57.867Z",
		"Question_link": "https://learn.microsoft.com/answers/questions/171998/is-there-any-kinect-version-that-helps-in-speech-t.html",
		"Question_tag": [
			"azure-machine-learning",
			"azure-kinect-dk"
		],
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_follower_count": 2,
		"Question_upvote_count": 1,
		"Question_body": "Hi,\nI am an educator/researcher who is working on a project or a thesis for my degree.\nI am thinking of using Kinect to recognize the lips/ mouth movements to train speech delay kids. My idea is to show students audio-visual 3d mouth, tongue, and throat movements on the Kinect to train them to speak a letter or word by interacting with the camera and evaluating their sounds and movements by the Kinect. At the same time, the Kinect camera will configure if the student's sound and mouth movement are correct.\n\nI am perplexed about which tag I should use for my question to be best answered.\n\nI really appreciated it if there are studies or experiments on this issue to let me know.\n\nRegards",
		"Answers": [
			{
				"Answer_creation_date": "2020-11-24T14:29:09.493Z",
				"Answer_upvote_count": 0,
				"Answer_body": "@LaylaObeid-3644 Thanks, Please have a look at https://docs.microsoft.com/en-us/azure/cognitive-services/speech-service/how-to-select-audio-input-devices for more info on selecting audio devices.\nWe also have a Robot Operating System(ROS) package for the Azure Kinect. The Kinect camera is a widely adopted sensor in the field of robotics.\nhttps://github.com/microsoft/azure_kinect_ros_driver\n\nAlso Have a look at https://github.com/microsoft/Azure-Kinect-Samples",
				"Answer_comment_count": 1,
				"Answer_has_accepted": false
			}
		]
	},
	{
		"Question_title": "allow_reuse in Azure ml sdk v2",
		"Question_creation_date": "2022-06-28T17:54:17.84Z",
		"Question_link": "https://learn.microsoft.com/answers/questions/906911/allow-reuse-in-azure-ml-sdk-v2.html",
		"Question_tag": [
			"azure-machine-learning"
		],
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_follower_count": 12,
		"Question_upvote_count": 1,
		"Question_body": "I am running a training pipeline on azure ml python sdk v2. It is running fine but when I am reinitiating it, it takes only 1 second to finish which I assume is taking previous step from last ran pipeline. In v1 sdk we have allow_reuse parameter to stop this behaviour. But how can we do that in ml python sdk v2.",
		"Answers": [
			{
				"Answer_creation_date": "2022-06-29T05:54:50.887Z",
				"Answer_upvote_count": 1,
				"Answer_body": "Hello @rjtshrm ,\n\nThank you for posting query in Microsoft Q&A Platform.\n\nWe don't have allow_reuse flag in the v2. However, I would like to understand use case little more in details.\nDo you mind sharing why you want to rerun the step when everything is not changed (meaning the output would be exactly the same even if you rerun)?\nThere is a way to disable reuse behavior by setting \"is_deterministic: False\" for your component.\n\nRegards,\nPritee",
				"Answer_comment_count": 5,
				"Answer_has_accepted": false
			}
		]
	},
	{
		"Question_title": "Why the storage account assosiated to azure machine learning has differenent region compared ML workspace?",
		"Question_creation_date": "2021-02-12T08:54:56.11Z",
		"Question_link": "https://learn.microsoft.com/answers/questions/270693/why-the-storage-account-assosiated-to-azure-machin.html",
		"Question_tag": [
			"azure-machine-learning",
			"azure-storage-accounts"
		],
		"Question_has_accepted_answer": true,
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_follower_count": 10,
		"Question_upvote_count": 0,
		"Question_body": "I have created a machine learning workspace in West Europe region. But the storage account, key vault and application insights got created in East US region. All these got created by default with creation on ML workspace.\nSo I want to know the reason for different region and also want to move the storage account to West Europe region.",
		"Answers": [
			{
				"Answer_creation_date": "2021-02-12T22:09:04.633Z",
				"Answer_upvote_count": 0,
				"Answer_body": "Hi, this is unusual. There's no way to move your default AML storage account to a different region. I recommend creating a new workspace or contacting Azure Support to investigate further.",
				"Answer_comment_count": 0,
				"Answer_has_accepted": true
			}
		]
	},
	{
		"Question_title": "Custom Argument pass to Docker Container Azure ML inference",
		"Question_creation_date": "2021-09-28T19:07:00.243Z",
		"Question_link": "https://learn.microsoft.com/answers/questions/569816/custom-argument-pass-to-docker-container-azure-ml.html",
		"Question_tag": [
			"azure-machine-learning",
			"azure-machine-learning-inference"
		],
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_follower_count": 9,
		"Question_upvote_count": 0,
		"Question_body": "Hello Team,\n\nI'm trying to pass the arguments to Azure ML docker. I have created an environment like this.\n\n env = Environment.from_conda_specification(name='pytorch-1.6-gpu', file_path='curated_env/conda_dependencies.yml' )\n\n\n\nAm I passing the arguments correct?\n\n DOCKER_ARGUMENTS = [\"--shm-size\",\"32G\"]  # increase shared memory\n env.docker.arguments = DOCKER_ARGUMENTS\n\n\n\n\nThe main goal of this project is to deploy a model on the AKS inference cluster. I have successfully deployed the model. When I try to get predictions from the model I got this error\n\nIt is possible that data loaders workers are out of shared memory. Please try to raise your shared memory limit\n\nHow can I do that if that's not the correct way to pass arguments?",
		"Answers": [
			{
				"Answer_creation_date": "2021-09-29T06:31:54.6Z",
				"Answer_upvote_count": 0,
				"Answer_body": "@khubaibRaza-8970 To pass the argument for increasing the default \"shm_size\" you would have to use the DockerConfiguration object. Here is a sample to achieve this:\n\n from azureml.core import Environment\n from azureml.core import ScriptRunConfig\n from azureml.core.runconfig import DockerConfiguration\n    \n    \n # Specify VM and Python environment:\n my_env = Environment.from_conda_specification(name='my-test-env', file_path=PATH_TO_YAML_FILE)\n my_env.docker.base_image = 'mcr.microsoft.com/azureml/openmpi3.1.2-cuda10.2-cudnn7-ubuntu18.04'\n    \n docker_config = DockerConfiguration(use_docker=True,shm_size='32g')\n    \n # Finally, use the environment in the ScriptRunConfig:\n src = ScriptRunConfig(source_directory=DEPLOY_CONTAINER_FOLDER_PATH,\n                       script=SCRIPT_FILE_TO_EXECUTE,\n                       arguments=EXECUTE_ARGUMENTS,\n                       compute_target=compute_target,\n                       environment=my_env,\n                       docker_runtime_config=docker_config)\n\n\n\n\nIf an answer is helpful, please click on  or upvote  which might help other community members reading this thread.",
				"Answer_comment_count": 2,
				"Answer_has_accepted": false
			}
		]
	},
	{
		"Question_title": "Round columns in Azure ML",
		"Question_creation_date": "2021-01-12T12:33:27.547Z",
		"Question_link": "https://learn.microsoft.com/answers/questions/227626/round-columns-in-azure-ml.html",
		"Question_tag": [
			"azure-machine-learning"
		],
		"Question_has_accepted_answer": true,
		"Question_answer_count": 2,
		"Question_comment_count": 0,
		"Question_follower_count": 5,
		"Question_upvote_count": 1,
		"Question_body": "Hello,\n\nI have made an experiment in Azure ML with a coefficient of detemination reaching 86% (regression). I would like to improve it using the rounding of several features (columns). I would like to round some columns to \"xx-ten\" example: 1854 => 1850 (up and down if possible)\nI have used ceil functions before to avoid decimal numbers but here this is another case. I cannot see how to do this.\n\nCan anyone help in this please?\n\nKind Regards,\n\nMohamed.",
		"Answers": [
			{
				"Answer_creation_date": "2021-01-13T21:48:43.547Z",
				"Answer_upvote_count": 1,
				"Answer_body": "Hello Dear,\n\nThank you for your advice. I have found in the rounding area the \"tomultiple\" function where I can decide to round a number to tens, hundreds... (example: tomultiple(2583,10) => 2580).\n\nThank you :)\n\nMohamed.",
				"Answer_comment_count": 0,
				"Answer_has_accepted": true
			},
			{
				"Answer_creation_date": "2021-01-12T20:22:24.783Z",
				"Answer_upvote_count": 0,
				"Answer_body": "Hi, the Apply Math Operation module has a rounding operator that can be used to round up/down on a given column set.",
				"Answer_comment_count": 0,
				"Answer_has_accepted": false
			}
		]
	},
	{
		"Question_title": "Unable to change input data of \"Web Service Output\" component in Designer in Azure ML Studio",
		"Question_creation_date": "2022-05-31T23:40:27.4Z",
		"Question_link": "https://learn.microsoft.com/answers/questions/872177/unable-to-change-input-data-of-34web-service-outpu.html",
		"Question_tag": [
			"azure-machine-learning"
		],
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_comment_count": 3,
		"Question_follower_count": 12,
		"Question_upvote_count": 0,
		"Question_body": "I am trying to complete the following unit in a MS Learn module:\nhttps://docs.microsoft.com/en-us/learn/modules/create-regression-model-azure-machine-learning-designer/7-inference-pipeline\n\nI added a \"Execute Python Script\" between \"Score Model\" and \"Web Service Output\" as the following capture and saved the model in Designer.\n(Previously \"Score Model\" was directly connected to \"Web Service Output\".)\n\n\nAnd then I revisited the model, the connection between \"Execute Python Script\" and \"Web Service Output\" was disabled, and again \"Score Model\" was directory connected to \"Web Service Output\".\n\n\nDoes anyone tell me how to keep the connection between \"Execute Python Script\" and \"Web Service Output\".\n\nBest regards,",
		"Answers": [
			{
				"Answer_creation_date": "2022-06-07T15:00:01.877Z",
				"Answer_upvote_count": 1,
				"Answer_body": "@keijimscom @williamsandres-0381 I have confirmation that this issue has been fixed in all regions by development. Could you retry and let us know if it is successful?\nIf it still fails additional information might be required related to your workspace and runs. We will let you know details on sharing this information if required. Thanks!!\n\nIf an answer is helpful, please click on  or upvote  which might help other community members reading this thread.",
				"Answer_comment_count": 1,
				"Answer_has_accepted": false
			}
		]
	},
	{
		"Question_title": "ModuleExceptionMessage:ColumnUniqueValuesExceeded: Number of unique values in column: ds is greater than allowed.",
		"Question_creation_date": "2020-08-12T17:42:15.8Z",
		"Question_link": "https://learn.microsoft.com/answers/questions/65476/moduleexceptionmessagecolumnuniquevaluesexceeded-n.html",
		"Question_tag": [
			"azure-machine-learning"
		],
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_comment_count": 2,
		"Question_follower_count": 1,
		"Question_upvote_count": 0,
		"Question_body": "We are trying to train a model using the Boosted Tree Regression model and running into the \"ModuleExceptionMessage:ColumnUniqueValuesExceeded: Number of unique values in column: \"ds\" is greater than allowed.\" error.\n\nThis is a time series data and it is going to have unique values in the timestamp - how are we supposed to get around the issue.\n\nPS: When using the exact same dataset in the classic studio it runs fine and does not throw this error.\n\nThe big picture: We are happy with studio based results on the prediction however, there isn't a way to fully end to end automate, meaning retrain the model with latest data and use the API to download the results in classic version. We are looking to use the new Azure ML pipelines and endpoints etc but quite frankly all of this is turning out be quite complex than needed for some reason. And the documentation to retrain the models shows a mixture of classic and new azure ml steps which is quite frustrating.",
		"Answers": []
	},
	{
		"Question_title": "How to use blob storage file as input to azure ml endpoint",
		"Question_creation_date": "2022-06-16T10:09:39.99Z",
		"Question_link": "https://learn.microsoft.com/answers/questions/891865/how-to-use-blob-storage-file-as-input-to-azure-ml.html",
		"Question_tag": [
			"azure-machine-learning",
			"azure-blob-storage"
		],
		"Question_has_accepted_answer": true,
		"Question_answer_count": 2,
		"Question_comment_count": 1,
		"Question_follower_count": 21,
		"Question_upvote_count": 1,
		"Question_body": "I have big data that I need to pass it to already trained Machine Learning model on Azure and has been deployed as online endpoint, I realize that batch endpoints supports adding a reference to blob file as input, my question is: how to do the same for online enpdoints ?\n\nSo far all examples I see are passing the payload as json (Even in the test tab of the online enpoints) but i don't know how to simply pass a blob storage file's uri as the payload.",
		"Answers": [
			{
				"Answer_creation_date": "2022-06-24T06:20:12.08Z",
				"Answer_upvote_count": 1,
				"Answer_body": "@MostafaMansour-4203 Thanks for the question. I have checked internally with the product team, Currently Online endpoints are for Realtime synchronous requests.\n\n\n\n\nIf an answer is helpful, please click on  or upvote  which might help other community members reading this thread.",
				"Answer_comment_count": 0,
				"Answer_has_accepted": true
			},
			{
				"Answer_creation_date": "2022-06-24T07:11:52.483Z",
				"Answer_upvote_count": 1,
				"Answer_body": "Answering my own question: real time endpoints doesn't support blob storage file/folder as input, will have to use batch endpoints",
				"Answer_comment_count": 0,
				"Answer_has_accepted": false
			}
		]
	},
	{
		"Question_title": "how to export trained azure ml model to production environment",
		"Question_creation_date": "2022-06-26T13:25:28.277Z",
		"Question_link": "https://learn.microsoft.com/answers/questions/903600/how-to-export-trained-azure-ml-model-to-production.html",
		"Question_tag": [
			"azure-machine-learning"
		],
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_follower_count": 11,
		"Question_upvote_count": 0,
		"Question_body": "How we can copying trained azure ml model from dev environment to production. Its possible to use trained model from one resource group to another resource group with same trained data.",
		"Answers": [
			{
				"Answer_creation_date": "2022-06-27T11:04:35.717Z",
				"Answer_upvote_count": 1,
				"Answer_body": "@Dhineshkumar-1686 Currently, Azure ML supports mlflow for model management which can be used to register and query models using the mlflow client. Stages are assigned to a model's version (instead of models) which means that a given model can have multiple versions on different stages. You can use this documentation to refer the capabilities of the mlflow client. However, the following is also a current limitation.\n\nStages can only be accessed using the MLflow SDK. They don't show up in the Azure ML Studio portal and can't be retrieved using neither Azure ML SDK, Azure ML CLI, or Azure ML REST API. Creating deployment from a given model's stage is not supported by the moment.\n\n\n\n\nMoving of Azure ML workspace from one resource group to another is currently not supported.\n\n\n\n\nIf an answer is helpful, please click on  or upvote  which might help other community members reading this thread.",
				"Answer_comment_count": 0,
				"Answer_has_accepted": false
			}
		]
	},
	{
		"Question_title": "Azure ML real time endpoints stuck in 'Transitioning' state",
		"Question_creation_date": "2020-11-29T17:50:54.767Z",
		"Question_link": "https://learn.microsoft.com/answers/questions/179215/azure-ml-real-time-endpoints-stuck-in-39transition.html",
		"Question_tag": [
			"azure-machine-learning",
			"azure-kubernetes-service",
			"azure-machine-learning-studio-classic"
		],
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_comment_count": 3,
		"Question_follower_count": 9,
		"Question_upvote_count": 0,
		"Question_body": "I am trying to deploy Azure ML models as webservice endpoints using an AKS cluster. I have deployed the real time inference pipeline using the Azure ML Studio interface.\nThe endpoints deploy successfully and quickly reach a \"Healthy\" state, however occasionally the deployments are stuck in the \"Transitioning\" state for indefinite time. This disables the testing for the endpoints on the portal and we are unable to consume the webservice for that period of time.\nAny idea why this might be happening, or what I can do to fix this?\nIs there a limit to the number of endpoints available on an inference cluster ?",
		"Answers": []
	},
	{
		"Question_title": "Retrain a model programmatically with python",
		"Question_creation_date": "2021-02-16T14:05:58.493Z",
		"Question_link": "https://learn.microsoft.com/answers/questions/275122/retrain-a-model-programmatically-with-python.html",
		"Question_tag": [
			"azure-machine-learning"
		],
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_follower_count": 7,
		"Question_upvote_count": 0,
		"Question_body": "I have created a Scikit-learn model with python and I have ran the experiment. Now , I need to retrain the model with new data through python but its mentioned in the documentation that we need to create a batch interference pipeline to retrain a model. If I create a batch interference pipeline, will I be able to deploy it? Any help will be appreciated. Thanks.",
		"Answers": [
			{
				"Answer_creation_date": "2021-02-16T21:45:05.22Z",
				"Answer_upvote_count": 0,
				"Answer_body": "Batch Inference Pipeline is useful for making predictions on large data and you can publish the pipeline to your workspace. To retrain your model programmatically, you can simply run your experiment again, then save and register the model to use at the deployment stage. Please review Azure ML Pipelines for more details on how to optimize your workflow.",
				"Answer_comment_count": 0,
				"Answer_has_accepted": false
			}
		]
	},
	{
		"Question_title": "Getting OSError: [Errno 30] Read-only file system",
		"Question_creation_date": "2021-06-28T15:29:47.85Z",
		"Question_link": "https://learn.microsoft.com/answers/questions/454901/getting-oserror-errno-30-read-only-file-system.html",
		"Question_tag": [
			"azure-machine-learning"
		],
		"Question_has_accepted_answer": false,
		"Question_answer_count": 3,
		"Question_comment_count": 1,
		"Question_follower_count": 10,
		"Question_upvote_count": 0,
		"Question_body": "I am new to AzureML, I am trying to run the pipeline using parallelRunSteps and pipeline is getting submitted successfully but while running the pipeline it is throwing an above error not sure what would be the root cause of it.\n\nThe step I am following is\n\nCreating the workspace if does not exists\n\n\nFetching the datastore by specifying the storage account and other details\n\n\nUsing the from file dataset\n\n\nRegistering the dataset\n\n\nAfter registering fetching the dataset\n\n\nFetching/Initialising Experiment\n\n\nFetching/Initialising Environment\n\n\nAdding Private wheel file to pip package\n\n\nRegistering the packages to conda dependencies\n\n\nRegistering the Environment\n\n\nFetching/Initialising the Compute Target\n\n\nInitialising the ParallelRunConfig\n\n\nInitialising the PipelineData as output data\n\n\nInitialising the ParallelRunStep\n\n\nFetching/Initialising the Pipeline\n\n\nSubmitting the Pipeline\n\nThe above same technique I tried with different PythonScriptSteps instead of ParallelRunStep method.\n\nCreating the workspace if does not exists\n\n\nFetching the datastore by specifying the storage account and other details\n\n\nTabular Dataset\n\n\nsetting dataset name input\n\n\nFetching the Experiment\n\n\nFetching/Initialising the Experiment\n\n\nFetching/Initialising the Environment\n\n\nAdding Private wheel file to pip package\n\n\nRegistering the packages to conda dependencies\n\n\nRegistering the Environment\n\n\nFetching the ComputeTarget\n\n\nInitialising the PythonStepScript\n\n\nInitialising the Pipeline\n\n\nSubmitting the Pipeline\n\nWith PythonStepScripts it is working fine. Not able to understand what mistake I am doing while running ParallelRunStep method.",
		"Answers": [
			{
				"Answer_creation_date": "2021-07-06T08:12:22.683Z",
				"Answer_upvote_count": 0,
				"Answer_body": "Hello,\n\nHope your issue has been solved. We haven\u2019t heard from you on the last response and was just checking back to see if you have a resolution yet.\n\nThe workaround I have seen for the similar issue is to add \"tmp\" to the file path like filepath = '/tmp/' + key\n\nIn case if you have any resolution please do share that same with the community as it can be helpful to others . Please do let us know if you still have issue for it.\n\n\n\n\nRegards,\nYutong",
				"Answer_comment_count": 0,
				"Answer_has_accepted": false
			},
			{
				"Answer_creation_date": "2021-07-13T06:48:36.4Z",
				"Answer_upvote_count": 0,
				"Answer_body": "Hi Yutong,\n\nSorry for the late reply was on leave.\n\nSharing the error message below\n\nTraceback (most recent call last):\nFile \"driver/amlbi_main.py\", line 48, in <module>\nmain()\nFile \"driver/amlbi_main.py\", line 44, in main\nJobStarter().start_job()\nFile \"/mnt/batch/tasks/shared/LS_root/jobs/gmail/azureml/68b3ef53-65a6-4d2f-a3ba-07af48d1081e/wd/azureml/68b3ef53-65a6-4d2f-a3ba-07af48d1081e/driver/job_starter.py\", line 50, in start_job\nself.setup(is_master=True)\nFile \"/mnt/batch/tasks/shared/LS_root/jobs/gmail/azureml/68b3ef53-65a6-4d2f-a3ba-07af48d1081e/wd/azureml/68b3ef53-65a6-4d2f-a3ba-07af48d1081e/driver/job_starter.py\", line 44, in setup\nLogConfig().config(args.logging_level, is_master=is_master)\nFile \"/mnt/batch/tasks/shared/LS_root/jobs/gmail/azureml/68b3ef53-65a6-4d2f-a3ba-07af48d1081e/wd/azureml/68b3ef53-65a6-4d2f-a3ba-07af48d1081e/driver/singleton_meta.py\", line 18, in call\ncls.instances[cls] = super(SingletonMeta, cls).call(args, *kwargs)\nFile \"/mnt/batch/tasks/shared/LS_root/jobs/gmail/azureml/68b3ef53-65a6-4d2f-a3ba-07af48d1081e/wd/azureml/68b3ef53-65a6-4d2f-a3ba-07af48d1081e/driver/log_config.py\", line 39, in init_\nself.log_dir = self.get_log_dir()\nFile \"/mnt/batch/tasks/shared/LS_root/jobs/gmail/azureml/68b3ef53-65a6-4d2f-a3ba-07af48d1081e/wd/azureml/68b3ef53-65a6-4d2f-a3ba-07af48d1081e/driver/log_config.py\", line 48, in get_log_dir\nworking_dir = RunContextFactory.get_context().working_dir\nFile \"/mnt/batch/tasks/shared/LS_root/jobs/gmail/azureml/68b3ef53-65a6-4d2f-a3ba-07af48d1081e/wd/azureml/68b3ef53-65a6-4d2f-a3ba-07af48d1081e/driver/run_context.py\", line 64, in working_dir\npth.mkdir(parents=True, exist_ok=True)\nFile \"/azureml-envs/azureml_91e342c44c0de9bc46808411bb1fed8e/lib/python3.6/pathlib.py\", line 1226, in mkdir\nself._accessor.mkdir(self, mode)\nFile \"/azureml-envs/azureml_91e342c44c0de9bc46808411bb1fed8e/lib/python3.6/pathlib.py\", line 387, in wrapped\nreturn strfunc(str(pathobj), *args)\nOSError: [Errno 30] Read-only file system: '/mnt/batch/tasks/shared/LS_root/jobs/gmail/azureml/68b3ef53-65a6-4d2f-a3ba-07af48d1081e/mounts/workspaceblobstore/azureml/68b3ef53-65a6-4d2f-a3ba-07af48d1081e'\n\nSorry not getting where to add file path like filepath = '/tmp/' + key can you some reference or example",
				"Answer_comment_count": 0,
				"Answer_has_accepted": false
			},
			{
				"Answer_creation_date": "2021-10-19T21:21:49.037Z",
				"Answer_upvote_count": 0,
				"Answer_body": "I am having the same issue\n\nI am creating a compute cluster and then mounting a Jupyter Lab mounted at the Workspace default datastore at this location:\n\n/mnt/batch/tasks/shared/LS_root/jobs/{workspace_name}/azureml/{run_id.lower()}/mounts/\n\nI also uploaded Jupyter Notebooks to\n\n/mnt/batch/tasks/shared/LS_root/jobs/{workspace_name}/azureml/{run_id.lower()}/mounts/workspaceblobstore/\n\nI used to be able to run the Jupyter Notebooks and save the results on the mount, or being able to upload content using the Jupyter Lab, or duplicating or saving changes to the notebooks\n\nBut not anymore I am getting this error:\n\nUnexpected error while saving file: workspaceblobstore/tao/bpnet/bpnet-Copy1.ipynb [Errno 30] Read-only file system: '/mnt/batch/tasks/shared/LS_root/jobs/ngc_aml_toolkit_ws_test2/azureml/tao-mrg-exp34_1634671669_282ea162/mounts/workspaceblobstore/tao/bpnet/bpnet-Copy1.ipynb\n\nWhile trying to duplicate notebook bpnet.ipynb",
				"Answer_comment_count": 0,
				"Answer_has_accepted": false
			}
		]
	},
	{
		"Question_title": "how to get started about analysing time series with Azure?",
		"Question_creation_date": "2021-01-22T16:52:54.593Z",
		"Question_link": "https://learn.microsoft.com/answers/questions/241953/how-to-get-started-about-analysing-time-series-wit.html",
		"Question_tag": [
			"azure-machine-learning"
		],
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_comment_count": 1,
		"Question_follower_count": 5,
		"Question_upvote_count": 0,
		"Question_body": "We have a large amount of time series that get updated each month or week and would like to use some Azure based machine learning to QC this.\n\nHaving no practical experience of machine learning or azure, could you point me where/how I could get started, please?\n\nOur data itself is located on the cloud in a Snowflake database.\n\n\n\n\nMany Thanks\n\nEric",
		"Answers": [
			{
				"Answer_creation_date": "2021-01-23T10:35:18.64Z",
				"Answer_upvote_count": 0,
				"Answer_body": "Hello,\n\nNot sure which way you want to do but we have two sample for time-series training:\n\nFor Machine Learning SDK:\nhttps://docs.microsoft.com/en-us/azure/machine-learning/how-to-auto-train-forecast\n\nFor Machine Learning Studio:\nhttps://docs.microsoft.com/en-us/azure/machine-learning/studio-module-reference/time-series\n\nThose two should be a good start.\n\n\n\n\nRegards,\nYutong",
				"Answer_comment_count": 0,
				"Answer_has_accepted": false
			}
		]
	},
	{
		"Question_title": "Error while accessing the dataset from a datastore",
		"Question_creation_date": "2021-04-29T12:16:39.813Z",
		"Question_link": "https://learn.microsoft.com/answers/questions/377203/error-while-accessing-the-dataset-from-a-datastore.html",
		"Question_tag": [
			"azure-machine-learning",
			"azure-machine-learning-studio-classic"
		],
		"Question_has_accepted_answer": true,
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_follower_count": 7,
		"Question_upvote_count": 1,
		"Question_body": "I have tried to read the dataset from datastore. Also tried to create the dataset also.\n\nThe code for reading the dataset is below\n\n from azureml.core import Workspace\n ws = Workspace.from_config()\n datastore = Datastore.get(ws, 'qdataset')\n\n\n\nIt works fine still now.\n\n from azureml.core.dataset import Dataset\n six_dataset = Dataset.get_by_name(workspace=ws, name='combined_classifier')\n\n\n\nAlso i have tried from azureml.core import Dataset\n\nIt shows the following error:\n\n2021-04-29 11:56:47.284077 | ActivityCompleted: Activity=_dataflow, HowEnded=Failure, Duration=0.0 [ms], Info = {'activity_id': 'xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx', 'activity_name': '_dataflow', 'activity_type': 'InternalCall', 'app_name': 'dataset', 'source': 'azureml.dataset', 'version': '1.27.0', 'dataprepVersion': '2.14.2', 'subscription': '', 'run_id': '', 'resource_group': '', 'workspace_name': '', 'experiment_id': '', 'location': '', 'completionStatus': 'Failure', 'durationMs': 962.01}, Exception=AttributeError; module 'azureml.dataprep' has no attribute 'api'\n\n\n\n\nAttributeError Traceback (most recent call last)\n<ipython-input-34-ac7a8d35da4d> in <module>\n1 from azureml.core.dataset import Dataset\n----> 2 six_dataset = Dataset.get_by_name(workspace=ws, name='combined_classifier')\n\n~\\AppData\\Roaming\\Python\\Python38\\site-packages\\azureml\\data_loggerfactory.py in wrapper(args, *kwargs)\n127 with LoggerFactory.track_activity(logger, func.name_, activity_type, custom_dimensions) as al:\n128 try:\n--> 129 return func(args, *kwargs)\n130 except Exception as e:\n131 if hasattr(al, 'activity_info') and hasattr(e, 'error_code'):\n\n~\\AppData\\Roaming\\Python\\Python38\\site-packages\\azureml\\data\\abstract_dataset.py in get_by_name(workspace, name, version)\n87 :rtype: typing.Union[azureml.data.TabularDataset, azureml.data.FileDataset]\n88 \"\"\"\n---> 89 dataset = AbstractDataset._get_by_name(workspace, name, version)\n90 AbstractDataset._track_lineage([dataset])\n91 return dataset\n\n~\\AppData\\Roaming\\Python\\Python38\\site-packages\\azureml\\data\\abstract_dataset.py in _get_by_name(workspace, name, version)\n652 if not success:\n653 raise result\n--> 654 dataset = _dto_to_dataset(workspace, result)\n655 warn_deprecated_blocks(dataset)\n656 return dataset\n\n~\\AppData\\Roaming\\Python\\Python38\\site-packages\\azureml\\data_dataset_rest_helper.py in _dto_to_dataset(workspace, dto)\n93 registration=registration)\n94 if dto.dataset_type == _DATASET_TYPE_FILE:\n---> 95 return FileDataset._create(\n96 definition=dataflow_json,\n97 properties=dto.latest.properties,\n\n~\\AppData\\Roaming\\Python\\Python38\\site-packages\\azureml\\data_loggerfactory.py in wrapper(args, *kwargs)\n127 with LoggerFactory.track_activity(logger, func.name_, activity_type, custom_dimensions) as al:\n128 try:\n--> 129 return func(args, *kwargs)\n130 except Exception as e:\n131 if hasattr(al, 'activity_info') and hasattr(e, 'error_code'):\n\n~\\AppData\\Roaming\\Python\\Python38\\site-packages\\azureml\\data\\abstract_dataset.py in _create(cls, definition, properties, registration, telemetry_info)\n555 from azureml.data._partition_format import parse_partition_format\n556\n--> 557 steps = dataset._dataflow._get_steps()\n558 partition_keys = []\n559 for step in steps:\n\n~\\AppData\\Roaming\\Python\\Python38\\site-packages\\azureml\\data_loggerfactory.py in wrapper(args, *kwargs)\n127 with LoggerFactory.track_activity(logger, func.name_, activity_type, custom_dimensions) as al:\n128 try:\n--> 129 return func(args, *kwargs)\n130 except Exception as e:\n131 if hasattr(al, 'activity_info') and hasattr(e, 'error_code'):\n\n~\\AppData\\Roaming\\Python\\Python38\\site-packages\\azureml\\data\\abstract_dataset.py in _dataflow(self)\n215 raise UserErrorException('Dataset definition is missing. Please check how the dataset is created.')\n216 if self._registration and self._registration.workspace:\n--> 217 dataprep().api._datastore_helper._set_auth_type(self._registration.workspace)\n218 if not isinstance(self._definition, dataprep().Dataflow):\n219 try:\n\nAttributeError: module 'azureml.dataprep' has no attribute 'api'\n\n\n\n\n\nPlease give a solution to solve this",
		"Answers": [
			{
				"Answer_creation_date": "2021-04-29T13:22:51.567Z",
				"Answer_upvote_count": 1,
				"Answer_body": "It now worked..\nWe need to install azure-ml-api-sdk using this command\n\npip install azure-ml-api-sdk",
				"Answer_comment_count": 0,
				"Answer_has_accepted": true
			}
		]
	},
	{
		"Question_title": "Why can't I access the Dataset class using azureml.core?",
		"Question_creation_date": "2021-10-12T17:37:54.827Z",
		"Question_link": "https://learn.microsoft.com/answers/questions/587772/why-can39t-i-access-the-dataset-class-using-azurem.html",
		"Question_tag": [
			"azure-machine-learning"
		],
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_follower_count": 8,
		"Question_upvote_count": 0,
		"Question_body": "Hi, I am unable to use Dataset class in azureml.core with new version of azureml.core==1.35.0\n\nImportError: cannot import name 'Dataset' from 'azureml.core'\n\nAlso, I am unable to downgrade or reinstall azureml.core to a lower version (1.32.0). Please find the snapshot below of pip install azureml.core==1.32.0\n\nCan someone please guide me to a possible alternative. I need to use the Dataset methods -\n\nThanks",
		"Answers": [
			{
				"Answer_creation_date": "2021-10-13T00:43:10.873Z",
				"Answer_upvote_count": 0,
				"Answer_body": "Hi, I'm not able to reproduce this issue. I was able to import Dataset using version 1.35.0 as shown below. Perhaps try uninstalling and reinstalling the package?\n\n\n\n\n\n\n\n--- Kindly Accept Answer if the information helps. Thanks.",
				"Answer_comment_count": 0,
				"Answer_has_accepted": false
			}
		]
	},
	{
		"Question_title": "Can I use compressed data on TabularDataset?",
		"Question_creation_date": "2021-11-30T01:02:30.553Z",
		"Question_link": "https://learn.microsoft.com/answers/questions/645118/can-i-use-compressed-data-on-tabulardataset.html",
		"Question_tag": [
			"azure-machine-learning",
			"azure-data-lake-storage"
		],
		"Question_has_accepted_answer": true,
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_follower_count": 13,
		"Question_upvote_count": 0,
		"Question_body": "I have a question about the source of TabularDataset on Azure Machine Learnigng.\n\nCan I use compressed data saved Azure Data Lake Storage Gen2 like below on TablarDataset without expansion?\n\ncsv with bzip2(.bz2)\n\n\nparquet with gzip(gz)\n\n\nparquet with snappy",
		"Answers": [
			{
				"Answer_creation_date": "2021-11-30T02:15:20.327Z",
				"Answer_upvote_count": 0,
				"Answer_body": "Hi, tabular dataset does not support compressed files. You'll need to extract the data as shown here for example before creating a tabular dataset. However, file dataset supports any format.\n\n\n\n\n--- Kindly Accept Answer if the information helps. Thanks.",
				"Answer_comment_count": 1,
				"Answer_has_accepted": true
			}
		]
	},
	{
		"Question_title": "How do we create learning virtual machine AI assistants with smart home control and self-driving vehicle funtionality?",
		"Question_creation_date": "2020-06-04T10:33:30.177Z",
		"Question_link": "https://learn.microsoft.com/answers/questions/31993/how-do-we-create-learning-virtual-machine-ai-assis.html",
		"Question_tag": [
			"azure-machine-learning"
		],
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_follower_count": 19,
		"Question_upvote_count": 0,
		"Question_body": "I am developing Conscious Quantum Coding Living AI Virtual Assistants to help with everything.\n\nJodi, The AI Motor Home\nJodi will be an integrative, quantum coded, learning/self-improving, online/cloud, virtual machine, life conscious Living AI assistant who fully controls, and self drives, an RV/Motor home\n\nHow would you create a Living AI assistant for a motor home?",
		"Answers": [
			{
				"Answer_creation_date": "2020-06-04T23:06:32.373Z",
				"Answer_upvote_count": 0,
				"Answer_body": "Thanks for reaching out. The Azure Bot Service may be useful for your scenario. I encourage you to check out our documentation on Virtual Assistant and Template Outline for best practices. There are also videos available to help you get started. Feel free to followup with any particular questions or concerns for the community to chime in. Thanks.",
				"Answer_comment_count": 0,
				"Answer_has_accepted": false
			}
		]
	},
	{
		"Question_title": "Azure OpenAI service capabilities",
		"Question_creation_date": "2022-09-24T03:15:41.807Z",
		"Question_link": "https://learn.microsoft.com/answers/questions/1021561/azure-openai-service-capabilities.html",
		"Question_tag": [
			"azure-machine-learning"
		],
		"Question_has_accepted_answer": true,
		"Question_answer_count": 1,
		"Question_comment_count": 1,
		"Question_follower_count": 11,
		"Question_upvote_count": 1,
		"Question_body": "How do I get access to the Azure OpenAI service to evaluate it's capabilities?",
		"Answers": [
			{
				"Answer_creation_date": "2022-09-24T06:32:47.767Z",
				"Answer_upvote_count": 0,
				"Answer_body": "@Srin-4824 Thanks for the question. It is a Limited Access service so you have to apply for it https://aka.ms/oai/access",
				"Answer_comment_count": 0,
				"Answer_has_accepted": true
			}
		]
	},
	{
		"Question_title": "Azure ML designer: force pipeline to execute with only an underlying data change",
		"Question_creation_date": "2021-05-07T18:00:22.613Z",
		"Question_link": "https://learn.microsoft.com/answers/questions/387140/azure-ml-designer-force-pipeline-to-execute-with-o.html",
		"Question_tag": [
			"azure-machine-learning"
		],
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_comment_count": 1,
		"Question_follower_count": 8,
		"Question_upvote_count": 0,
		"Question_body": "We are deploying an ML model through the Azure ML designer. Over time the underlying data changes and so the model needs to be regularly retrained. The actual designer pipeline and the dataset definition (a query on a SQL database) are not changed, only the underlying data in the Azure SQL database.\n\nRight now, the pipeline API can be triggered, but it does not execute (as expected). This is equivalent to the default allow_reuse = True in the Azure ML SDK. Is there a way to disable this setting (or set in to False) in the designer so that when the API is triggered we can force it to re-execute the pipeline every time we want to do a retraining (eg once a week) as new data comes in, so that a new model version is generated every time.\n\nTo be clear, the training takes around 20 minutes, and the compute cluster it runs on has a 120 second scale-down time, so cost considerations etc (ie the reason for this feature being enabled by default) are not a concern.\n\nThanks in advance for any help.",
		"Answers": [
			{
				"Answer_creation_date": "2021-05-08T18:43:30.677Z",
				"Answer_upvote_count": 0,
				"Answer_body": "Hello,\n\nUse the following steps to update a module pipeline parameter:\n\nAt the top of the canvas, select the gear icon.\nIn the Pipeline parameters section, you can view and update the name and default value for all of your pipeline parameter.\n\nHope this helps. Thanks.\n\nRegards,\nYutong",
				"Answer_comment_count": 2,
				"Answer_has_accepted": false
			}
		]
	},
	{
		"Question_title": "Pre-existing Compute Resource necessary for running a scheduled ML pipeline?",
		"Question_creation_date": "2021-05-12T07:14:16.847Z",
		"Question_link": "https://learn.microsoft.com/answers/questions/392165/pre-existing-compute-resource-necessary-for-runnin.html",
		"Question_tag": [
			"azure-machine-learning"
		],
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_follower_count": 8,
		"Question_upvote_count": 0,
		"Question_body": "Hi fellows,\nI have been exploring Azure ML Pipeline. I am referring to this notebook for the below code:\n\nHere is a small snippet from a MS Repo:\ntrain_step = PythonScriptStep(name = \"Prepare Data\",\nsource_directory = experiment_folder,\nscript_name = \"prep_diabetes.py\",\narguments = ['--input-data', diabetes_ds.as_named_input('raw_data'),\n'--prepped-data', prepped_data_folder],\noutputs=[prepped_data_folder],\ncompute_target = pipeline_cluster,\nrunconfig = pipeline_run_config,\nallow_reuse = True)\n\nThis suggests that while defining a pipeline, we must provide it a compute resource. This obviously makes sense, since specific compute might be required for a specific step.\n\nBut do we need to have this compute resource up and running always, so that whenever a pipeline is triggered, it can find the compute resource?\n\nAlso, i figured we can probably keep a cluster with Zero minimum nodes, in which cases cluster is resized whenever pipeline is triggered. But i think there is a minimal cost incurrent in probably container registry regularly in such a setup. Is this the recommended way to deploy ML pipelines or some more efficient approach is possible?",
		"Answers": [
			{
				"Answer_creation_date": "2021-05-13T10:02:30.86Z",
				"Answer_upvote_count": 0,
				"Answer_body": "@HimanshuGautam-0592 Thanks for the question. AmlCompute clusters are designed to scale dynamically based on your workload. The cluster can be scaled up to the maximum number of nodes you configure. As each run completes, the cluster will release nodes and scale to your configured minimum node count.\n\nTo avoid charges when no jobs are running, set the minimum nodes to 0. This setting allows Azure Machine Learning to de-allocate the nodes when they aren't in use. Any value larger than 0 will keep that number of nodes running, even if they are not in use.\n\n\n\n\nAnother way to save money on compute resources is Azure Reserved VM Instance. amlcompute supports reserved instances out of the box. These reservations can be used across azure compute resources (vmss/batch/vm) and AzureML compute.\n\nCheck out this article to learn more about planning and managing costs : https://docs.microsoft.com/en-us/azure/machine-learning/concept-plan-manage-cost",
				"Answer_comment_count": 0,
				"Answer_has_accepted": false
			}
		]
	},
	{
		"Question_title": "Can't run a basic model in Azure ML",
		"Question_creation_date": "2020-12-09T22:39:08.183Z",
		"Question_link": "https://learn.microsoft.com/answers/questions/192735/can39t-run-a-basic-model-in-azure-ml.html",
		"Question_tag": [
			"azure-machine-learning"
		],
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_follower_count": 6,
		"Question_upvote_count": 0,
		"Question_body": "Hello,\n\nI'm new to Azure Machine Learning, so I'm trying some tutorials on a free trial account.\n\nNone of the tutorials I've followed is running on my account, not even the most basic ones.\n\nFor example, the \"Flight Delay Prediction\" is running for several hours and then frustratingly it fails....\n\nThis is how the designer looks like:\n\n\n\n\n\n\nDoes this make sense?\nCan anyone help?\n\nThanks in advance!",
		"Answers": [
			{
				"Answer_creation_date": "2020-12-10T07:45:59.39Z",
				"Answer_upvote_count": 0,
				"Answer_body": "@nunonogueira From the screen shot it looks like your experiment is running with some modules run queued. Depending on the compute resources you have chosen the run time might vary and with a free trial account a lower compute power will slow down the run time. I have tried the same experiment with a STANDARD_DS2_V2 type with scaling option from 0-4 resources in the cluster and the run completed in around 23 minutes where most the time went to setup the compute since I do not have any running compute to run the experiment directly. You can try a similar setup with a higher compute and check if it runs faster.",
				"Answer_comment_count": 3,
				"Answer_has_accepted": false
			}
		]
	},
	{
		"Question_title": "how many models can be deployed in single node in azure kubernetes service?",
		"Question_creation_date": "2021-09-04T09:07:52.547Z",
		"Question_link": "https://learn.microsoft.com/answers/questions/540001/how-many-models-can-be-deployed-in-single-node-in.html",
		"Question_tag": [
			"azure-machine-learning",
			"azure-kubernetes-service"
		],
		"Question_has_accepted_answer": true,
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_follower_count": 14,
		"Question_upvote_count": 0,
		"Question_body": "Working on deployment of 170 ml models using ML studio and azure Kubernetes service which is referred on the below doc link \"https://github.com/MicrosoftDocs/azure-docs/blob/master/articles/machine-learning/how-to-deploy-azure-kubernetes-service.md\".\n\nWe are training the model using python script with the custom environment and we are registering the ml model on the Azure ML services. Once we register the mode we are deploying it on the AKS by using the container images.\n\nWhile deploying the ML model we are able to deploy up 10 to 11 models per pods for each Node in AKS. When we try to deploy the model on the same node we are getting deployment timeout error and we are getting the below error message.",
		"Answers": [
			{
				"Answer_creation_date": "2021-09-04T12:32:05.4Z",
				"Answer_upvote_count": 0,
				"Answer_body": "Hi @suvedharan-5910\n\nThe number of models to be deployed is limited to 1,000 models per deployment (per container).\n\nAutoscaling for Azure ML model deployments is azureml-fe, which is a smart request router. Since all inference requests go through it, it has the necessary data to automatically scale the deployed model(s).\nmore details\n\n\n\n\n\nIf the Answer is helpful, please click Accept Answer and up-vote, so that it can help others in the community looking for help on similar topics.",
				"Answer_comment_count": 2,
				"Answer_has_accepted": true
			}
		]
	},
	{
		"Question_title": "Azure ML Compute Instance Times Out During File Upload",
		"Question_creation_date": "2022-11-15T17:39:54.507Z",
		"Question_link": "https://learn.microsoft.com/answers/questions/1090211/azure-ml-compute-instance-times-out-during-file-up.html",
		"Question_tag": [
			"azure-machine-learning",
			"azure-machine-learning-inference"
		],
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_follower_count": 13,
		"Question_upvote_count": 0,
		"Question_body": "I have deployed a gpu-enabled AZ ML Compute instance and am running a custom docker container on top of it. This custom docker container uses bentoml to receive, batch, and manage inference requests, with various endpoints for doing so. I have exposed the necessary ports so that the endpoints are available to send requests to. There are 2 major types of request endpoints involve: first, an endpoint where a path to a file that the compute/container have access to through a volume mount/storage mounting is sent as the data, and the container endpoint preforms inference on the file that path points to. This works just fine. Second is where the actual file is uploaded to the endpoint. The files in question are quite large image files, ~300-500mb. This second method has an issue: the file never reaches the endpoint. I attached a remote debugger and found the entrypoint of the data, and it is never reached in the case of the image upload, while it is in the case of the path upload. I then replicated the container on a local compute, and repeated the same scenario, in which the container was able to receive and handle the image upload with no issue. Additionally, when the image is more than 500mb, when attempting to upload to the container on the ml compute instance, I get an \"entity too large\" error, that I do not get when doing the same thing on a local compute instance.\n\nI have been unable to find any definite documentation of the limitations on file upload size/speed for ML compute instances, all that I have been able to find pertains to azure apps and to ml online endpoints (which could be an alternative option), but not to ml compute instances. Is there such a limit? If so, what is it?",
		"Answers": [
			{
				"Answer_creation_date": "2022-11-15T23:35:12.483Z",
				"Answer_upvote_count": 0,
				"Answer_body": "Hello @JasonLunder-5242\n\nThanks for using Microsoft Q&A and sorry to hear your experience is not smooth.\n\nUploading file directly to Azure ML compute instance is acutualy not a suggested way due to security limitation from Ngnix (Ngnix is one of our environment here ) - Restricting file upload size is useful to prevent some types of denial-of-service (DOS) attacks and many other related issues. https://www.tecmint.com/limit-file-upload-size-in-nginx/\n\nThe official limitation of upload suggests from Ngnix is only 1MB, for Azure the limitation is 512MB.\n\nWe are obsessed in MS with security challenges, the workaround for myself for your reference is not to upload files, but to create a custom docker image, and on that docker image, between the docker commands to download the needed file with wget.\n\nI hope my answer help you solve your issue and I also provide my suggestion to product team for make this point clear.\n\nRegards,\nYutong\n\n-Please kindly accept the answer if you feel helpful to support the community, thanks a lot.",
				"Answer_comment_count": 2,
				"Answer_has_accepted": false
			}
		]
	},
	{
		"Question_title": "Group Categorical Data module missing in Designer, how to reduce number of levels?",
		"Question_creation_date": "2021-05-05T02:54:20.377Z",
		"Question_link": "https://learn.microsoft.com/answers/questions/383026/group-categorical-data-module-missing-in-designer.html",
		"Question_tag": [
			"azure-machine-learning"
		],
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_comment_count": 2,
		"Question_follower_count": 7,
		"Question_upvote_count": 0,
		"Question_body": "Hi there,\n\nI'm working through a tutorial on reducing the number of levels of a categorical variable before using the \"Convert to Indicator Values\" module. In the tutorial, the presenter is using the classic studio which has a module called \"Group Categorical Data\". Unfortunately, I'm using ML Designer and it doesn't have that module.\n\nIs there an easy workaround to reduce the number of categorical levels in Designer before using the Convert to Indicator Values module?\n\nThanks kindly,",
		"Answers": [
			{
				"Answer_creation_date": "2021-05-06T03:47:55.663Z",
				"Answer_upvote_count": 0,
				"Answer_body": "Hi, thanks for your feedback. Based on further exploration and confirmation from the product team, it seems Designer does not currently support 'Group Categorical Data' module. The recommendation is to use Execute Python/R Script module to perform custom data transformations. Sorry for any inconvenience.",
				"Answer_comment_count": 0,
				"Answer_has_accepted": false
			}
		]
	},
	{
		"Question_title": "Data input format (call the service) for Azure ML time series forecast model deployed as a web service (Python)",
		"Question_creation_date": "2021-01-04T04:08:30.997Z",
		"Question_link": "https://learn.microsoft.com/answers/questions/217305/data-input-format-call-the-service-for-azure-ml-ti.html",
		"Question_tag": [
			"azure-machine-learning",
			"azure-machine-learning-inference"
		],
		"Question_has_accepted_answer": true,
		"Question_answer_count": 1,
		"Question_comment_count": 1,
		"Question_follower_count": 6,
		"Question_upvote_count": 1,
		"Question_body": "Sorry in advance for the lengthy question as I wanted to explain it as detailed as possible. I used the Azure AutoML to train a model and deployed it as a web service. Now I can access (call) it over the REST endpoint.\n\nI have the following data types for attributes: date (timestamp), number, number, number, number, integer. I trained the model with the following parametres:\n\nTimestaps interval: 15 min\nForecast Horizon: 4 (I need the forecast every hour for the next hour)\nTarget rolling window size: 96 (the forecast must ba based on the last 24 hours of data)\n\nI have two questions.\n\nAs I understand, based on the above, I have to provide last 4 entries to the model for a correct prediction. Otherwise, it will consider a time gap. Am I right? In this case, how I could input 4 instances at a time for a single prediction? The following example is wrong as it asks for 4 predictions for each instance:\n\nimport requests\nimport json\n\nURL for the web service\n\nscoring_uri = 'http://xxxxx-xxxxxxx-xxxxxx-xxxxxxx.xxxxx.azurecontainer.io/score'\n\ndata = {\"data\":\n[\n[\n2020-10-04 19:30:00,1.29281,1.29334,1.29334,1.29334,1\n],\n[\n2020-10-04 19:45:00,1.29334,1.29294,1.29294,1.29294,1\n],\n[\n2020-10-04 21:00:00,1.29294,1.29217,1.29334,1.29163,34\n],\n[\n2020-10-04 21:15:00,1.29217,1.29257,1.29301,1.29115,195]\n]\n}\n# Convert to JSON string\ninput_data = json.dumps(data)\n\nSet the content type\n\nheaders = {'Content-Type': 'application/json'}\n\nMake the request and display the response\n\nresp = requests.post(scoring_uri, input_data, headers=headers)\nprint(resp.text)\n\nThe above code is based on the provided Microsoft example https://docs.microsoft.com/en-us/azure/machine-learning/how-to-consume-web-service?tabs=python#call-the-service-python.\n\nI am unable to replicate the provided example with my data. I have an error \"SyntaxError: leading zeros in decimal integer literals are not permitted; use an 0o prefix for octal integers\" pointing to the date. I assume, I need to specify the data type but could not find how.\nI tried to load a line from a csv file but I have an error (SyntaxError: invalid syntax) pointing to \"with\" with the following:\n\ndata = {\"data\":\n[with open('file', \"r\") as f:\nfor line in f: pass\nprint(line)]\n}\n\nI tested getting the last line from a csv file intependetly and it works but not inside the full script.\n\nI very appreciate any help or direction. Thank you.",
		"Answers": [
			{
				"Answer_creation_date": "2021-01-04T21:41:48.693Z",
				"Answer_upvote_count": 1,
				"Answer_body": "@AlexeyPisakov-8757\nPlease try the solution mentioned below.\n\nThe service takes data in form of deserialized pandas data frame. In the example below, it will look like:\nimport json\n\nX_test = pd.DataFrame([\n\n ['2020-10-04 19:30:00', 1.29281, 1.29334, 1.29334, 1.29334, 1],\n ['2020-10-04 19:45:00', 1.29334, 1.29294, 1.29294, 1.29294, 1],\n ['2020-10-04 21:00:00', 1.29294, 1.29217, 1.29334, 1.29163, 34],\n ['2020-10-04 21:15:00', 1.29217, 1.29257, 1.29301, 1.29115, 195]],\n columns=['date', 'number_1', 'number_2', 'number_3', 'number_4', 'integer']\n\n\n\n)\n\ntest_sample = json.dumps({'data': X_test.to_dict(orient='records')})\n\ntest_sample\n\n\n\n\nWhich will result in JSON string as:\n\n{\"data\": [{\"date\": \"2020-10-04 19:30:00\", \"number_1\": 1.29281, \"number_2\": 1.29334, \"number_3\": 1.29334, \"number_4\": 1.29334, \"integer\": 1}, {\"date\": \"2020-10-04 19:45:00\", \"number_1\": 1.29334, \"number_2\": 1.29294, \"number_3\": 1.29294, \"number_4\": 1.29294, \"integer\": 1}, {\"date\": \"2020-10-04 21:00:00\", \"number_1\": 1.29294, \"number_2\": 1.29217, \"number_3\": 1.29334, \"number_4\": 1.29163, \"integer\": 34}, {\"date\": \"2020-10-04 21:15:00\", \"number_1\": 1.29217, \"number_2\": 1.29257, \"number_3\": 1.29301, \"number_4\": 1.29115, \"integer\": 195}]}\n\n\n\n\nPlease rename the columns to the corresponding columns from the training data set.",
				"Answer_comment_count": 1,
				"Answer_has_accepted": true
			}
		]
	},
	{
		"Question_title": "Azure ML BFSMountError: Unable to mount blob fuse file system",
		"Question_creation_date": "2022-08-11T06:48:47.397Z",
		"Question_link": "https://learn.microsoft.com/answers/questions/963683/azure-ml-bfsmounterror-unable-to-mount-blob-fuse-f.html",
		"Question_tag": [
			"azure-machine-learning"
		],
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_comment_count": 3,
		"Question_follower_count": 12,
		"Question_upvote_count": 0,
		"Question_body": "Training yolov5 using Azure ML SDK and encountered error below. I have a gpu cluster created (no vnet) and encountered this error when i ScriptRunConfig and experiment.submit. My Azure ML Storage account is enabled from all network (no vnet). Thank you very much\n\nsrc = ScriptRunConfig(.......)\nrun = experiment.submit(src)\n\nAzureMLCompute job failed.\nBFSMountError: Unable to mount blob fuse file system\nInfo: Could not mount Azure Blob Container azureml-blobstore-xxxxxxxx at workspaceblobstore: <nil>. Unable to start blobfuse due to a lack of credentials. Please check the readme for valid auth setups.\nUnmounting blobfuse.\nUnmounted blobfuse successfully.\n\n Info: Failed to setup runtime for job execution: Job environment preparation failed on 10.0.0.5 with err exit status 1.",
		"Answers": []
	},
	{
		"Question_title": "Enabling Model Data Collector for ACI through az ml deploy",
		"Question_creation_date": "2022-03-31T14:09:39.63Z",
		"Question_link": "https://learn.microsoft.com/answers/questions/795453/enabling-model-data-collector-for-aci-through-az-m.html",
		"Question_tag": [
			"azure-machine-learning",
			"azure-container-instances",
			"azure-machine-learning-inference"
		],
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_comment_count": 2,
		"Question_follower_count": 15,
		"Question_upvote_count": 0,
		"Question_body": "Hello,\n\nI want to create a release pipeline in DevOps that deploys a model to ACI, I have a PowerShell script that deploys using the az ml package. However even though I set the model data collector flag in the deploy command, the endpoint does not collect the inputs. I have tried deploying the same model, using the exact same score.py, but deployed programmatically through azureml.core.webservice and it worked just fine with collection data, so the problem is elsewhere.\n\nI have deployed my model using the following command in a PowerShell script (paraphrasing some of the arguments):\n\naz ml model deploy -n $name --model $model -g $resource-group -w $workspace --es $entry_script_path --cf $conda_file_path --dc $deployment_config_path --md True --overwrite -v\n\nNotice that I have set the --md argument to True, which is the model data collector flag.\n\nAnd in my score.py, I have imported the ModelDataCollector, initialized it and I call collect. I have something like this:\n\nfrom azureml.monitoring import ModelDataCollector\n\n\ndef init():\nglobal model, scaler, input_name, label_name, inputs_dc, prediction_dc\n\n\n  # variables to monitor model input and output data\n  inputs_dc = ModelDataCollector(\"model\", designation=\"inputs\")\n\ninput_sample = pd.DataFrame(sample})\n@input_schema('data',PandasParameterType(input_sample))\n@output_schema(NumpyParameterType(np.array([0])))\ndef run(data):\ntry:\ninputs_dc.collect(data)\n# model inference\nresult = model.predict(data)\nreturn {\"result\": result.tolist()}\nexcept Exception as e:\nresult = e\nreturn {\"result\": result}\n\nHowever I deploy my model using the PowerShell script, and everything goes fine. The endpoint is callable, healthy and outputs correct results. But the data does not get saved in the storage account or in application insights (which I have enabled, and can see is enabled on the endpoint as well.\n\nIn my deployment config I see the following:\n\nData collection is not enabled. Set environment variable ML_MODEL_DC_STORAGE_ENABLED to 'true' to enable.\n\nHow do I go about that? I have enabled Data collection in my deployment command, why does it not work, and how can I set the environmental variable?\nI tried adding it to my conda_env.yml which is part of the deployment command \"--cf $conda_file_path\" like so:\n\nname: my_env\ndependencies:\n- python=3.6.2\n- pip:\n- numpy\n- onnxruntime\n- joblib\n- azureml-core~=1.10.0\n- azureml-defaults~=1.10.0\n- scikit-learn==0.22.2.post1\n- inference-schema\n- inference-schema[numpy-support]\n- azureml-monitoring\nchannels:\n- anaconda\n- conda-forge\nvariables:\nML_MODEL_DC_STORAGE_ENABLED = true\n\nBut that just produces another error. How do I solve this problem?",
		"Answers": []
	},
	{
		"Question_title": "ML Model data output",
		"Question_creation_date": "2021-03-11T18:41:49.277Z",
		"Question_link": "https://learn.microsoft.com/answers/questions/310400/ml-model-data-output.html",
		"Question_tag": [
			"azure-machine-learning",
			"azure-blob-storage"
		],
		"Question_has_accepted_answer": false,
		"Question_answer_count": 1,
		"Question_comment_count": 0,
		"Question_follower_count": 9,
		"Question_upvote_count": 0,
		"Question_body": "Hi,\n\nI was running several time series model using Azure automated machine learning, I didn't write any code. After the running was completed, there are some datasets stored in Azure Blob Storage. But I don't know if these files include the prediction results or not because I can't find a right software to open it . I don't need to deploy the model. I just need a plain spreadsheet which contains the result. Why it is so hard? The attachment is the screenshot of the fiels stored in Blob of the model I ran? What do those files mean?\nAnd I just check the running outcome, it shows there is no output dataset. I was so confused! Do I need to change something when I set the model running?\n][1]",
		"Answers": [
			{
				"Answer_creation_date": "2021-03-12T08:00:46.453Z",
				"Answer_upvote_count": 0,
				"Answer_body": "@MarcusGonzalez-1907 I think you might be interested to check your models explainability which tells more about the results of your run and performance of your model. Since you are not planning to deploy your model as a service running explain for all the models will help you choose the best model that can be later used for deployment. The steps to run explain are mentioned in this document.\n\nThe files you might be referring to are files that might have been used for processing your data based on the input settings or configuration of your automl run. The files might be different between child runs as these child runs are basically run against an algorithm and different algorithms produce different set of files. Usually, the end user need not refer these files as automl gets the best model for you from all the child runs which is deployed as a web service on ACI or AKS.",
				"Answer_comment_count": 3,
				"Answer_has_accepted": false
			}
		]
	},
	{
		"Question_title": "Azure ML kernel",
		"Question_creation_date": "2021-11-17T09:59:19.873Z",
		"Question_link": "https://learn.microsoft.com/answers/questions/630550/azure-ml-kernel.html",
		"Question_tag": [
			"azure-machine-learning"
		],
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_comment_count": 1,
		"Question_follower_count": 10,
		"Question_upvote_count": 0,
		"Question_body": "Hi,\n\nI installed custom env and added it to jupyter on my compute instance.\nYet, when I run the code error says that it occurs under base env (azureml_py38)\n\nsee the pic. Can you help me?",
		"Answers": []
	},
	{
		"Question_title": "Automated ML endpoint questions (performance, multiple return values, scoring)",
		"Question_creation_date": "2022-09-07T22:43:22.493Z",
		"Question_link": "https://learn.microsoft.com/answers/questions/998394/automated-ml-endpoint-questions-performance-multip.html",
		"Question_tag": [
			"azure-machine-learning"
		],
		"Question_has_accepted_answer": false,
		"Question_answer_count": 0,
		"Question_comment_count": 3,
		"Question_follower_count": 11,
		"Question_upvote_count": 0,
		"Question_body": "Hi....\n\nI used Automated ML to train a model on a set of grouping ids and titles, very simple use case, just two columns...predict the group id from the title. There were about 32000 rows in the input split into a training set of 90% and a validation set of 10%. Best model was 'MaxAbsScaler, LogisticRegression'.\n\nI deployed the endpoint using the 'realtimeendpoint' method. But each request takes 10 seconds to return a response. I took the default ML compute type VM which isn't a wimpy machine. Is it normal to be so slow? Will better hardware get the response time into the sub-one-second time-frame I need it to be? Are there other options to improve performance?\n\n\nI only ever get back one value in the response. Is it possible to get multiple predicted values?\n\n\nI don't see a 'confidence' score in the response, or see a way to request one. Is that possible?\n\nThank you.",
		"Answers": []
	}
]