[
	{
		"Questiont_title": "How to register a multi container model to a model registry?",
		"Question_creation_time": "2022-06-23T13:19:40.836Z",
		"Question_link": "https://repost.aws/questions/QUwYjH1ZG2SNqPAB9VGmXT1A/how-to-register-a-multi-container-model-to-a-model-registry",
		"Question_topic": [
			"Developer Tools",
			"Machine Learning & AI"
		],
		"Question_tag": [
			"AWS CodePipeline",
			"Amazon SageMaker",
			"Machine Learning & AI",
			"Amazon SageMaker Pipelines",
			"Amazon SageMaker Model Building"
		],
		"Question_upvote_count": 0,
		"Question_view_count": 157,
		"Question_answer_count": 1,
		"Question_has_accepted_answer": false,
		"Question_body": "I have created a multi-container model in SageMaker notebook and deployed it through an endpoint. But while attempting to do the same through a SageMaker Studio Project (build, train and deploy model template), I need to register the multi-container model through a 'sagemaker.workflow.step_collections.RegisterModel' step, which I am unable to do. What I understand till now is multi-container model is created through boto3 api call. I haven't found a way to create it using 'sagemaker.model.Model' and hence not being able to register it. Please help.",
		"Answers": [
			{
				"Answer_creation_date": "2022-06-23T13:42:23.974Z",
				"Answer_upvote_count": 0,
				"Answer_body": "Hi,\n\nin order to register a multi-container model you need to use the PipelineModel class instead of the Model one you are trying to do currently.\n\nAn example of this can be found in this notebook: https://github.com/aws/amazon-sagemaker-examples/blob/main/sagemaker-pipelines/tabular/train-register-deploy-pipeline-model/train%20register%20and%20deploy%20a%20pipeline%20model.ipynb\n\nIn this example a pipeline model is created with 2 containers, first one is an sklearn one doing some preprocessing and the second an XGBoost one for ML inference. However, in your case, the number or containers, type, invocation order and more, may differ, but you should be able to draw parallels based on this example.\n\nHope this helps,",
				"Answer_has_accepted": false
			}
		]
	},
	{
		"Questiont_title": "Inconsistent keras model.summary() output shapes on AWS SageMaker and EC2",
		"Question_creation_time": "2022-06-17T19:28:27.206Z",
		"Question_link": "https://repost.aws/questions/QU8UWsqxW8RbejgzLHYIFduA/inconsistent-keras-model-summary-output-shapes-on-aws-sage-maker-and-ec-2",
		"Question_topic": [
			"Machine Learning & AI",
			"Compute"
		],
		"Question_tag": [
			"Amazon SageMaker",
			"Amazon EC2",
			"Machine Learning & AI"
		],
		"Question_upvote_count": 0,
		"Question_view_count": 36,
		"Question_answer_count": 1,
		"Question_has_accepted_answer": false,
		"Question_body": "I have the following model in a jupyter notebook:\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import layers\n\n\n\nphysical_devices = tf.config.list_physical_devices('GPU')\ntf.config.experimental.set_memory_growth(physical_devices[0], True)\n\nSIZE = (549, 549)\nSHUFFLE = False \nBATCH = 32\nEPOCHS = 20\n\ntrain_datagen =  DataGenerator(train_files, batch_size=BATCH, dim=SIZE, n_channels=1, shuffle=SHUFFLE)\ntest_datagen =  DataGenerator(test_files, batch_size=BATCH, dim=SIZE, n_channels=1, shuffle=SHUFFLE)\n\n\ninp = layers.Input(shape=(*SIZE, 1))\n\nx = layers.Conv2D(filters=549, kernel_size=(5,5), padding=\"same\", activation=\"relu\")(inp)\nx = layers.BatchNormalization()(x)\n\n\nx = layers.Conv2D(filters=549, kernel_size=(3, 3), padding=\"same\", activation=\"relu\")(x)\nx = layers.BatchNormalization()(x)\n\n\nx = layers.Conv2D(filters=549, kernel_size=(1, 1), padding=\"same\", activation=\"relu\")(x)\nx = layers.BatchNormalization()(x)\n\nx = layers.Conv2D(filters=549, kernel_size=(3, 3), padding=\"same\", activation=\"sigmoid\")(x)\n\nmodel = Model(inp, x)\n\nmodel.compile(loss=tf.keras.losses.binary_crossentropy, optimizer=Adam())\n\nmodel.summary()\n\nSagemaker and EC2 are running tensorflow 2.7.1. The EC2 instance is p3.2xlarge with Deep Learning AMI GPU TensorFlow 2.7.0 (Amazon Linux 2) 20220607. The SageMaker notebook is using ml.p3.2xlarge and I am using the conda_tensorflow2_p38 kernel. The notebook is in an FSx Lustre file system that is mounted to both SageMaker and EC2 so it is definitely the same code running on both machines.\n\nnvidia-smi output on SageMaker:\n\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla V100-SXM2...  On   | 00000000:00:1E.0 Off |                    0 |\n| N/A   37C    P0    24W / 300W |      0MiB / 16384MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n\n\nnvidia-smi output on EC2:\n\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla V100-SXM2...  On   | 00000000:00:1E.0 Off |                    0 |\n| N/A   42C    P0    51W / 300W |   2460MiB / 16384MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|    0   N/A  N/A     11802      C   /bin/python3.8                    537MiB |\n|    0   N/A  N/A     26391      C   python3.8                        1921MiB |\n+-----------------------------------------------------------------------------+\n\n\nThe model.summary() output on SageMaker is:\n\nModel: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, 549, 549, 1)]     0         \n                                                                 \n conv2d (Conv2D)             (None, 549, 549, 1)       7535574   \n                                                                 \n batch_normalization (BatchN  (None, 549, 549, 1)      4         \n ormalization)                                                   \n                                                                 \n conv2d_1 (Conv2D)           (None, 549, 549, 1)       2713158   \n                                                                 \n batch_normalization_1 (Batc  (None, 549, 549, 1)      4         \n hNormalization)                                                 \n                                                                 \n conv2d_2 (Conv2D)           (None, 549, 549, 1)       301950    \n                                                                 \n batch_normalization_2 (Batc  (None, 549, 549, 1)      4         \n hNormalization)                                                 \n                                                                 \n conv2d_3 (Conv2D)           (None, 549, 549, 1)       2713158   \n                                                                 \n=================================================================\nTotal params: 13,263,852\nTrainable params: 13,263,846\nNon-trainable params: 6\n\n\nThe model.summary() output on EC2 is (notice the shape change):\n\n\nModel: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, 549, 549, 1)]     0         \n                                                                 \n conv2d (Conv2D)             (None, 549, 549, 549)     14274     \n                                                                 \n batch_normalization (BatchN  (None, 549, 549, 549)    2196      \n ormalization)                                                   \n                                                                 \n conv2d_1 (Conv2D)           (None, 549, 549, 549)     2713158   \n                                                                 \n batch_normalization_1 (Batc  (None, 549, 549, 549)    2196      \n hNormalization)                                                 \n                                                                 \n conv2d_2 (Conv2D)           (None, 549, 549, 549)     301950    \n                                                                 \n batch_normalization_2 (Batc  (None, 549, 549, 549)    2196      \n hNormalization)                                                 \n                                                                 \n conv2d_3 (Conv2D)           (None, 549, 549, 549)     2713158   \n                                                                 \n=================================================================\nTotal params: 5,749,128\nTrainable params: 5,745,834\nNon-trainable params: 3,294\n_________________________________________________________________\n\nOne other thing that is interesting, if I change my model on the EC2 instance to:\n\ninp = layers.Input(shape=(*SIZE, 1))\n\nx = layers.Conv2D(filters=1, kernel_size=(5,5), padding=\"same\", activation=\"relu\")(inp)\nx = layers.BatchNormalization()(x)\n\n\nx = layers.Conv2D(filters=1, kernel_size=(3, 3), padding=\"same\", activation=\"relu\")(x)\nx = layers.BatchNormalization()(x)\n\n\nx = layers.Conv2D(filters=1, kernel_size=(1, 1), padding=\"same\", activation=\"relu\")(x)\nx = layers.BatchNormalization()(x)\n\nx = layers.Conv2D(filters=1, kernel_size=(3, 3), padding=\"same\", activation=\"sigmoid\")(x)\n\nmodel = Model(inp, x)\n\nmodel.compile(loss=tf.keras.losses.binary_crossentropy, optimizer=Adam())\n\nMy model.summary() output becomes:\n\nModel: \"model_2\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_3 (InputLayer)        [(None, 549, 549, 1)]     0         \n                                                                 \n conv2d_8 (Conv2D)           (None, 549, 549, 1)       26        \n                                                                 \n batch_normalization_6 (Batc  (None, 549, 549, 1)      4         \n hNormalization)                                                 \n                                                                 \n conv2d_9 (Conv2D)           (None, 549, 549, 1)       10        \n                                                                 \n batch_normalization_7 (Batc  (None, 549, 549, 1)      4         \n hNormalization)                                                 \n                                                                 \n conv2d_10 (Conv2D)          (None, 549, 549, 1)       2         \n                                                                 \n batch_normalization_8 (Batc  (None, 549, 549, 1)      4         \n hNormalization)                                                 \n                                                                 \n conv2d_11 (Conv2D)          (None, 549, 549, 1)       10        \n                                                                 \n=================================================================\nTotal params: 60\nTrainable params: 54\nNon-trainable params: 6\n_________________________________________________________________\n\nIn the last model the shape is similar to SageMaker but the trainable parameters are very low.\n\nAny ideas as to why the output shape is different and why this is happening with the filters? When I run this model on my personal computer, the shape is the same as EC2. I think there might be an issue with SageMaker.",
		"Answers": [
			{
				"Answer_creation_date": "2022-07-20T13:27:09.004Z",
				"Answer_upvote_count": 0,
				"Answer_body": "Hello, I am checking the versions you mentioned and in my notebook instance , using conda_tensorflow2_p38 I get Tensorflow version 2.5. Is it the same for you or have you upgraded it to tensorflow 2.7?: import tensorflow as tf print(tf.version) 2.5.0",
				"Answer_has_accepted": false
			}
		]
	},
	{
		"Questiont_title": "Amazon SageMaker Local Mode raised boto3.exceptions.RetriesExceededError: Max Retries Exceeded",
		"Question_creation_time": "2021-12-27T14:37:30.316Z",
		"Question_link": "https://repost.aws/questions/QUVLhj-0JzTUCLcs7yH5Kznw/amazon-sage-maker-local-mode-raised-boto-3-exceptions-retries-exceeded-error-max-retries-exceeded",
		"Question_topic": [
			"Machine Learning & AI",
			"Containers"
		],
		"Question_tag": [
			"Amazon SageMaker",
			"Machine Learning & AI",
			"Amazon Elastic Container Registry (ECR)"
		],
		"Question_upvote_count": 0,
		"Question_view_count": 83,
		"Question_answer_count": 1,
		"Question_has_accepted_answer": false,
		"Question_body": "I try to run the SageMaker local mode example without any modification at https://github.com/aws-samples/amazon-sagemaker-local-mode/tree/main/pytorch_nlp_script_mode_local_model_inference on my local machine.\n\nHowever I encountered the **boto3.exceptions.RetriesExceededError: Max Retries Exceeded ** exception when the example tries to deploy the inference endpoint to 'local' instance type.\n\nI checked with\n\ndocker images -a \n\n\ncommand and it does not pull the expected pre-built SageMaker deep learning container image from ECR. The code example is using a dummy role for the local SageMaker session. I need help as I am blocked at this point as the exception error message is not helpful to pinpoint the actual root cause of this issue. Thanks in advance.\n\nBelow are my configurations:\n\nUbuntu: 20.04.3 LTS\nAWS CLI version: 2.4.7\nPython: 3.8.12\nDocker: 20.10.12\nDocker Compose: 1.29.2\nboto3: 1.20.26\nsagemaker: 2.72.1",
		"Answers": [
			{
				"Answer_creation_date": "2021-12-27T15:10:36.523Z",
				"Answer_upvote_count": 0,
				"Answer_body": "Hi, if you aren't able to pull the expected pre-built SageMaker deep learning container image, I would check your network settings. From the local mode sample documentation, \"you'll need to be able to access a public Amazon ECR repository from your local environment.\"\n\nFor more detailed information from boto3, you can enable logging and debug mode as well, which could help pinpoint the exact cause of the error. More information on Boto3 Retries: https://boto3.amazonaws.com/v1/documentation/api/latest/guide/retries.html.",
				"Answer_has_accepted": false
			}
		]
	},
	{
		"Questiont_title": "Submit EMR serverless jobs from SageMaker notebook",
		"Question_creation_time": "2022-10-05T04:08:41.237Z",
		"Question_link": "https://repost.aws/questions/QUaaYMax_hTIi0UNn-7wGPIQ/submit-emr-serverless-jobs-from-sage-maker-notebook",
		"Question_topic": [
			"Machine Learning & AI",
			"Analytics"
		],
		"Question_tag": [
			"Amazon SageMaker",
			"Amazon EMR",
			"Amazon EMR Serverless"
		],
		"Question_upvote_count": 0,
		"Question_view_count": 45,
		"Question_answer_count": 2,
		"Question_has_accepted_answer": false,
		"Question_body": "I am processing a dataset and need to submit a job to EMR serverless for the dataset to be processed in a distributed way. I have created an application in EMR studio. I would like to submit jobs to that application. I found the command to submit jobs\n\naws emr-serverless start-job-run \\\n    --application-id application-id \\\n    --execution-role-arn job-role-arn \\\n    --job-driver '{\n        \"sparkSubmit\": {\n            \"entryPoint\": \"s3://us-east-1.elasticmapreduce/emr-containers/samples/wordcount/scripts/wordcount.py\",\n            \"entryPointArguments\": [\"s3://DOC-EXAMPLE-BUCKET-OUTPUT/wordcount_output\"],\n            \"sparkSubmitParameters\": \"--conf spark.executor.cores=1 --conf spark.executor.memory=4g --conf spark.driver.cores=1 --conf spark.driver.memory=4g --conf spark.executor.instances=1\"\n        }\n    }'\n\n\nBut how can I run the above command from a Python 3 Data Science Notebook in SageMaker studio. Basically what endpoint do I need to use to submit the job.",
		"Answers": [
			{
				"Answer_creation_date": "2022-10-05T18:25:33.441Z",
				"Answer_upvote_count": 0,
				"Answer_body": "Hello,\n\nInstead of using the CLI to submit your job, have you tried using the boto3 Python library? https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/emr-serverless.html . All of the configuration parameters you've shared can be passed in EMRServerless boto3.",
				"Answer_has_accepted": false
			},
			{
				"Answer_creation_date": "2022-10-06T07:59:43.539Z",
				"Answer_upvote_count": 0,
				"Answer_body": "Hello,\n\nYou can use below method to submit job for EMR serverless.\n\n=>Running jobs from the EMR Studio console\n\n=>Running jobs from the AWS CLI\n\nhttps://docs.aws.amazon.com/emr/latest/EMR-Serverless-UserGuide/jobs.html\n\nSubmittion of EMR serverless jobs from SageMaker notebook is not supported yet.",
				"Answer_has_accepted": false
			}
		]
	},
	{
		"Questiont_title": "how to use custom_attribute in sagemaker api?",
		"Question_creation_time": "2022-03-21T21:26:28.473Z",
		"Question_link": "https://repost.aws/questions/QUyzl7vjQGSVGdbnkxv8HKxA/how-to-use-custom-attribute-in-sagemaker-api",
		"Question_topic": [
			"Machine Learning & AI"
		],
		"Question_tag": [
			"Amazon SageMaker",
			"Machine Learning & AI"
		],
		"Question_upvote_count": 1,
		"Question_view_count": 231,
		"Question_answer_count": 1,
		"Question_has_accepted_answer": false,
		"Question_body": "based on documentation provided here -> https://docs.aws.amazon.com/cli/latest/reference/sagemaker-runtime/invoke-endpoint-async.html I am passing a custom attributes parameter when calling the invoke-endpoint-async function.\n\n  invoke-endpoint-async\n--endpoint-name <value>\n[--custom-attributes <value>]\n[--inference-id <value>]\n--input-location <value>\n\n\n\nare there any sample/example on how can i read this in my code before/after invoking my model for inference.\n\nI am creating a preprocessing file and have input_fn and predict_fn function, can value passed in custom_attributes during api call be read or written to the response here? also, if it can be written to the response like documentation says, where can i see it . as the async endpoint only gives the output location when invoked and later process the request, i don't see the response, i just see the dumped out file in the specified output location. how can i see the full response?\n\nsagemaker_model = TensorFlowModel(model_data = 'model_data_path',\nrole = execution_role,\nframework_version = '1.12',\nsource_dir ='src',\nentry_point = 'preprocessing.py', \n...)\n\n\nfile: preprocessing.py\n\ndef input_fn(request_body, content_type):\n    //read the custom attribute here",
		"Answers": [
			{
				"Answer_creation_date": "2022-03-30T18:45:29.679Z",
				"Answer_upvote_count": 0,
				"Answer_body": "Additionally to AWS CLI, you can pass CustomAttributes parameter during invocation of endpoint with boto3-client or SM SDK:\n\nIn case of boto3: runtime_client.invoke_endpoint(CustomAttributes=json.dumps({key1:val1, key2:val2, ...}))\nIn case of SM SDK: predictor.predict(payload, initial_args={'CustomAttributes': json.dumps({key1:val1, key2:val2, ...})})\n\nAnd then you could parse the context-arg in the input-handlers of preprocessing.py, as for example:\n\ndef handler(data, context):\n    processed_input = _process_input(data, context)\n    custom_attrs = json.loads(context.custom_attributes)\n    # logic to parse / enact custom attrs ...\n    response = requests.post(context.rest_uri, data=processed_input)\n    return _process_output(response, context)\n\n\nSee also this post: https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-runtime-now-supports-the-customattribute-header/",
				"Answer_has_accepted": false
			}
		]
	},
	{
		"Questiont_title": "can a sagemaker endpoint be made public?",
		"Question_creation_time": "2022-09-27T00:52:09.738Z",
		"Question_link": "https://repost.aws/questions/QU96EIw32SSxmx3plPtUUcYA/can-a-sagemaker-endpoint-be-made-public",
		"Question_topic": [
			"Machine Learning & AI"
		],
		"Question_tag": [
			"Amazon SageMaker"
		],
		"Question_upvote_count": 1,
		"Question_view_count": 40,
		"Question_answer_count": 2,
		"Question_has_accepted_answer": false,
		"Question_body": "is there a way to make a sagemaker endpoint be accessible publicly ?",
		"Answers": [
			{
				"Answer_creation_date": "2022-09-27T09:46:53.193Z",
				"Answer_upvote_count": 3,
				"Answer_body": "I believe the way to make a sagemaker inference endpoint public is to use api-gw infront of it. Check out this solution https://docs.aws.amazon.com/solutions/latest/constructs/aws-apigateway-sagemakerendpoint.html",
				"Answer_has_accepted": false
			},
			{
				"Answer_creation_date": "2022-09-27T02:32:12.501Z",
				"Answer_upvote_count": 2,
				"Answer_body": "Hello. In order to make an Amazon SageMaker Real-Time Endpoint public, you can create and manage APIs through an API Gateway. This is an official blog that is showing you a possible solution:\n\nhttps://aws.amazon.com/blogs/machine-learning/creating-a-machine-learning-powered-rest-api-with-amazon-api-gateway-mapping-templates-and-amazon-sagemaker/",
				"Answer_has_accepted": false
			}
		]
	},
	{
		"Questiont_title": "Error in Creating Project in SageMaker Studio",
		"Question_creation_time": "2022-05-19T00:27:34.013Z",
		"Question_link": "https://repost.aws/questions/QUsvzAjgcJTzGrh0wfKRDvjQ/error-in-creating-project-in-sage-maker-studio",
		"Question_topic": [
			"Machine Learning & AI",
			"Security, Identity, & Compliance"
		],
		"Question_tag": [
			"Amazon SageMaker",
			"IAM Policies"
		],
		"Question_upvote_count": 0,
		"Question_view_count": 463,
		"Question_answer_count": 1,
		"Question_has_accepted_answer": false,
		"Question_body": "I encountered following erorr when I tried to create a new project in SageMaker Studio. I tried to add AdminAccess to role/service-role/AmazonSageMakerServiceCatalogProductsLaunchRole, but still doesn't help. What could be the problem?\n\nError getting the details of Service Catalog Provisioning Parameters. Error message: ValidationException: Access denied while assuming the role arn:aws:iam::XXXXXXXXX:role/service-role/AmazonSageMakerServiceCatalogProductsLaunchRole. Args: {\"productId\":\"prod-xxxxxxxx\",\"provisioningArtifactId\":\"pa-xxxxxxxxx\",\"pathId\":\"lpv2-xxxxxxxxxx\"}\n\n\nprintscreen",
		"Answers": [
			{
				"Answer_creation_date": "2022-05-19T16:32:30.379Z",
				"Answer_upvote_count": 1,
				"Answer_body": "SageMaker assumes the AmazonSageMakerServiceCatalogProductsLaunchRole role to access and launch Projects. If you aren't using the AmazonSageMakerFullAccess policy for your Studio or user profile's execution role, make sure your Studio execution role has enough permissions to assume the AmazonSageMakerServiceCatalogProductsLaunchRole.\n\nYou don't need to add Admin access to the launch role, it has the minimum required permissions to launch a service catalog product, including assuming the AmazonSageMakerServiceCatalogProductsUseRole for running the project successfully.",
				"Answer_has_accepted": false
			}
		]
	},
	{
		"Questiont_title": "HELP!!!! Amazon SageMaker not writing best optimal route based on Genetic Algorithm to 2nd Output Dynamo Database(am stucked here&incurring dollar charges with no progress -Error Screenshot available)",
		"Question_creation_time": "2022-10-24T11:33:45.424Z",
		"Question_link": "https://repost.aws/questions/QUDsciSjamQ5WZR_FHC1u0vQ/help-amazon-sage-maker-not-writing-best-optimal-route-based-on-genetic-algorithm-to-2nd-output-dynamo-database-am-stucked-here-incurring-dollar-charges-with-no-progress-error-screenshot-available",
		"Question_topic": [
			"Machine Learning & AI",
			"Database",
			"Containers",
			"DevOps"
		],
		"Question_tag": [
			"Amazon SageMaker",
			"Amazon DynamoDB",
			"Containers",
			"DevOps"
		],
		"Question_upvote_count": 0,
		"Question_view_count": 26,
		"Question_answer_count": 0,
		"Question_has_accepted_answer": false,
		"Question_body": "My Challenges is this: I used CloudFormation template to deploy 2 Dynamo DB (Input and Output) and 1 IAM role to use AWS-managed Lamba Function for Genetic Algorithm, so Amazon SageMaker (using Jupyter Notebook on AWS) is meant to write the locations (X and Y coordinates into the input Dynamo DB (Successful), while the Docker file to Docker Image to Docker Container is also to be run by Amazon SageMaker to write the best Optimal Route based on Genetic Algorithm (Mutation, genomes and generation transfer mode of operation) to the 2nd Dynamo DB (Unsuccessful) and this is where I am stucked, have read a lot of materials and research a lot and even reached out to some Amazon AWS Community but they could not resolve it, Please will be glad if repost.aws can help please (Error Screenshot Available)",
		"Answers": []
	},
	{
		"Questiont_title": "Deploying multiple Comprehend Custom Classifiers (multi-label mode)",
		"Question_creation_time": "2022-09-26T18:09:55.645Z",
		"Question_link": "https://repost.aws/questions/QUEQiFVzOhR5q1XYhjlltO7w/deploying-multiple-comprehend-custom-classifiers-multi-label-mode",
		"Question_topic": [
			"Machine Learning & AI",
			"DevOps"
		],
		"Question_tag": [
			"Amazon SageMaker",
			"Machine Learning & AI",
			"DevOps",
			"Amazon Comprehend",
			"Amazon SageMaker Deployment"
		],
		"Question_upvote_count": 0,
		"Question_view_count": 36,
		"Question_answer_count": 1,
		"Question_has_accepted_answer": true,
		"Question_body": "I want to train and deploy multiple comprehend custom classifiers (for example 50 models). I want to be able to classify my documents in near real-time (a couple of seconds are fine) 24/7. The problem is that deploying one end-point for each classifier is very expensive, especially that one or two IU would be enough for all my models combined (I am expecting to process around 10 document a minute total/length of one document is around 1000 characters ). Is there a way where I can deploy multiple models behind the same endpoint (similar to the multi-model endpoint in SageMaker)? Or maybe do an asynchronous approach and somewho make sure I get the response within seconds?",
		"Answers": [
			{
				"Answer_creation_date": "2022-10-24T19:34:11.311Z",
				"Answer_upvote_count": 0,
				"Answer_body": "No , Comprehend don't support hosting multiple models with the same endpoint right now. Thanks for your suggestions . We will take them into consideration .",
				"Answer_has_accepted": true
			}
		]
	},
	{
		"Questiont_title": "OSError: [Errno 28] No space left on device -- PyTorch, CNN, estimator",
		"Question_creation_time": "2022-07-11T15:56:51.984Z",
		"Question_link": "https://repost.aws/questions/QUuRZhu6ZpTPSmVO-4P2GDmQ/os-error-errno-28-no-space-left-on-device-py-torch-cnn-estimator",
		"Question_topic": [
			"Machine Learning & AI",
			"Storage"
		],
		"Question_tag": [
			"Amazon SageMaker",
			"Storage"
		],
		"Question_upvote_count": 0,
		"Question_view_count": 404,
		"Question_answer_count": 1,
		"Question_has_accepted_answer": false,
		"Question_body": "hey there, I'm training a convolutional neural network (CNN) on a large dataset (10k images - 50 GB) stored on S3 bucket using estimator(sagemaker infrastructure) . everything works well when I work with 2000 images which has the total size of almost 5-10 GB. however, I get an error when I increase number of images to 3000 or more. the error indicates that there is no space left on device. I also attached the estimator setup to this message, as you can see I am using ml.g4dn.12xlarge instance which has 192 GB of memory!! I also increased the volume size to 900 GB. I still don't know why I am getting space/storage error!! I know that error is related the function \"_get_train_data_loader\" in which it is trying to download the images!! I was reading somewhere that EFS (elastic file system) might help with this issue, if so, I don't know how to specify it in the estimator. estimator = PyTorch( entry_point=\"pbdl_sm.py\", role=role, framework_version=\"1.4.0\", py_version=\"py3\", instance_count=1, instance_type=\"ml.g4dn.12xlarge\", volume_size = 900, hyperparameters={\"epochs\": 6, \"backend\": \"gloo\",\"lr\": 0.001,\"train_size\":2900,\"n_realz\":3000}, )",
		"Answers": [
			{
				"Answer_creation_date": "2022-07-13T19:49:21.485Z",
				"Answer_upvote_count": 0,
				"Answer_body": "Hi\n\nThank you for reaching out to us.\n\nIn general the no space left on device error occurs when there is a high disk utilization, Requesting you to review the instance metrics and cloudwatch metrics/logs of the training job for more detailed information.\n\nIt also depends on the various factors like learning rate, number epochs and configuration of the training job and the estimator, However I would recommend you to try distributed training with the pytorch using the smdistributed [1] on multiple instances , Currently, the following are supported: distributed training with parameter servers, SageMaker Distributed (SMD) Data and Model Parallelism, and MPI. SMD Model Parallelism can only be used with MPI.\n\nTo enable the SageMaker distributed data parallelism: { \"smdistributed\": { \"dataparallel\": { \"enabled\": True } } }\n\nI would also recommend to try the PIPE mode, With Pipe input mode, your dataset is streamed directly to your training instances instead of being downloaded first. This means that your training jobs start sooner, finish quicker, and need less disk space.\n\nIf you are facing any issues and require further investigation on the issue, I would encourage you to open a case with the premium support along with the training job ARN and the cloudwatch logs of the job.Due to security reason, this post is not suitable for sharing customer's resource.\n\nReference:\n\n[1] https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/sagemaker.pytorch.html\n\n[2] https://aws.amazon.com/blogs/machine-learning/using-pipe-input-mode-for-amazon-sagemaker-algorithms/\n\n[3] https://github.com/aws/amazon-sagemaker-examples/blob/80df7d61a4bf14a11f0442020e2003a7c1f78115/advanced_functionality/pipe_bring_your_own/train.py\n\n[4] https://sagemaker-examples.readthedocs.io/en/latest/training/distributed_training/pytorch/data_parallel/maskrcnn/pytorch_smdataparallel_maskrcnn_demo.html",
				"Answer_has_accepted": false
			}
		]
	},
	{
		"Questiont_title": "how to access/set up a model registry ?",
		"Question_creation_time": "2022-11-15T02:03:19.381Z",
		"Question_link": "https://repost.aws/questions/QUW-r2Vv8-RC-qtY34c3WobQ/how-to-access-set-up-a-model-registry",
		"Question_topic": [
			"Machine Learning & AI"
		],
		"Question_tag": [
			"Build & Train ML Models",
			"Amazon SageMaker",
			"Machine Learning & AI",
			"ML Ops with Amazon SageMaker and Kubernetes"
		],
		"Question_upvote_count": 0,
		"Question_view_count": 25,
		"Question_answer_count": 1,
		"Question_has_accepted_answer": false,
		"Question_body": "based on aws docs/examples (https://docs.aws.amazon.com/sagemaker/latest/dg/model-registry-version.html), one can create/register model that is generated by your training pipeline. first we need to create a model package group ( sample code below). is model registry already set up by default in our account or do we have to create it as well? can you have more than one model registry in an account. I don't see , in the code below, where we are passing the model registry info. I am assuming , we simply create multiple training jobs and save the models in its own model group. is there a sample code/repo, where there are multiple training steps that calls same model registry with a different group name ( i can use it as a sample). also, once the model is registered, how do i set up such that, a model can be accepted or declined, before promoting. can we set this up via code , please point me to any examples.\n\nimport time\ngroup_name = \"mygroup\"\ninput_dict = { \"ModelPackageGroupName\" : group_name}\n\nresponse =  client.create_model_package_group(**input_dict)\nprint('result :  {}'.format(response['ModelPackageGroupArn']))",
		"Answers": [
			{
				"Answer_creation_date": "2022-11-15T12:54:41.012Z",
				"Answer_upvote_count": 0,
				"Answer_body": "Hello,\n\nModel package group and model registry are 2 different things. Model registry is already in your account but you have to create a model package group where you will have model packages (different versions of the model). see this notebook about using model registry. Your code above is basically creating a model group/model package group in model registry, and in that model group you will have the different versions of your model.\n\nI don't have an example in mind, but you can easily have a code where you create different models and add them in the model groups accordingly. Following the notebook shared above , you would just add the same code but adapt it to another model group assuming that you have to two trainings happening each with a different purpose.\n\nTo summarize: model registry = N model groups and each model group = N model versions (model packages). Suppose you are doing a sentiment analysis task where you have different models per customer. You could create a Model Package Group per customer and in each group register Model Package Versions for each model created.\n\nYou can automate the testing of your model before registering into the model group by for example checking the loss metric. You can use SageMaker projects for an example that includes a pipeline. Check this example. You could basically have a conditional step in your pipeline.\n\nHope this helps and if I answered your question, please accept it.\n\nThank you",
				"Answer_has_accepted": false
			}
		]
	},
	{
		"Questiont_title": "Sage Maker Notebook - overcommit_memory settings - permission denied",
		"Question_creation_time": "2021-05-24T17:45:51.000Z",
		"Question_link": "https://repost.aws/questions/QUPG-W9iG_RteLMwkPbpvEZg/sage-maker-notebook-overcommit-memory-settings-permission-denied",
		"Question_topic": [
			"Machine Learning & AI"
		],
		"Question_tag": [
			"Amazon SageMaker"
		],
		"Question_upvote_count": 0,
		"Question_view_count": 273,
		"Question_answer_count": 1,
		"Question_has_accepted_answer": false,
		"Question_body": "I am doing some NLP work using spaCy in a ml.p2.xlarge notebook instance in sagemaker.\n\nHowever, I get the following error:\n\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 3.10 GiB for an array with shape (2889884, 288) and data type float32\n\nTo fix the issue, I have tried the following commands:\n\n!echo 1 | sudo tee /proc/sys/vm/overcommit_memory\n!echo 1 > /proc/sys/vm/overcommit_memory\n\nNeither of them work and they both return:\n/bin/sh: /proc/sys/vm/overcommit_memory: Permission denied\n\nAny suggestions? I even added all policies with \"admin\" in them to the notebook IAM with no luck.",
		"Answers": [
			{
				"Answer_creation_date": "2021-05-24T18:57:06.000Z",
				"Answer_upvote_count": 0,
				"Answer_body": "Okay, it seems that I needed to restart my notebook after adding the admin policies to it. It's working now and I can overcommit the memory!",
				"Answer_has_accepted": false
			}
		]
	},
	{
		"Questiont_title": "Bad RMSE when predicting Price with Linear Regression",
		"Question_creation_time": "2022-03-17T14:22:40.709Z",
		"Question_link": "https://repost.aws/questions/QUly8ruHWYTCy_MEaBQfz4ZA/bad-rmse-when-predicting-price-with-linear-regression",
		"Question_topic": [
			"Machine Learning & AI"
		],
		"Question_tag": [
			"Amazon SageMaker",
			"Machine Learning & AI",
			"Amazon SageMaker Autopilot"
		],
		"Question_upvote_count": 0,
		"Question_view_count": 30,
		"Question_answer_count": 3,
		"Question_has_accepted_answer": false,
		"Question_body": "Hi. I have a dataset of price data. It looks like this\n\nPrice\tBranch\tItemCode\tDiscount\tDateTimeOfPrice\n10\t002\t52345436\t0.33\t2022-03-24 14:00\n\nThe dataset has about 1M records\n\nI feature engineered it in the following way\n\nPrice\tDiscount\tItemCode\tYear\tMonth\tDay\tHour\tBranch1\tBranch2\tBranch3\n10\t0.33\t52345436\t2022\t03\t24\t14\t0\t1\t0\n\nEach component of the DateTimeOfPrice got a separate column We have 3 branches. To avoid the situation when algorithm may think that \"branch\" column is some kind of priority column, I created 3 new column (we have 3 branches). If the item belongs to branch2, the column will get the value 1, if not - it will be 0\n\nI run Linear Learner, XGBoost build-in algorithms and also SageMaker AutoPilot. In all cases I run , the best RMSE was 60 and prediction/ validation gives sometimes a result which is far from the actual value. I tried also to run XGBoost from the notebook with the following parameters\n\nhyperparams = {\n    \"max_depth\": \"7\",\n    \"eta\": \"0.2\",\n    \"gamma\": \"4\",\n    \"min_child_weight\": \"6\",\n    \"subsample\": \"0.7\",\n    \"objective\": \"reg:squarederror\",\n    \"num_round\": \"100\",\n    \"eval_metric\":\"rmse\",\n    \"verbosity\": \"2\",\n}\n\n\nStill, the RMSE is arround 60.\n\nPlease advice what can be done to improve the mertic and predication",
		"Answers": [
			{
				"Answer_creation_date": "2022-03-23T08:07:24.157Z",
				"Answer_upvote_count": 0,
				"Answer_body": "Since I see you have a timestamp field in your data, would it be fair to assume your use case is mainly aimed at forecasting future prices - rather than estimating missing historical prices at different points in time?\n\nIf so, plain tabular regression (Autopilot regression task type) is probably not a good way to tackle this problem as forecasting techniques would work better instead. You could instead explore:\n\nSageMaker Canvas, which offers a forecasting model (see the docs here to make sure your input timestamp is recognised so that Canvas shows you the forecasting option)\nAmazon Forecast, a dedicated managed forecasting service separate from SageMaker",
				"Answer_has_accepted": false
			},
			{
				"Answer_creation_date": "2022-03-20T13:16:02.354Z",
				"Answer_upvote_count": 0,
				"Answer_body": "I followed you suggestion and used Sagemaker Canvas\n\nI modified the data structure in the following way\n\nItemPrice\tBranch\tDiscount\tItemCode\tPriceDate\nData\tData\tData\tData\tData\nData\tData\tData\tData\tData\n\nI choose ItemCode as \"id\" and \"grouped\" by \"branch\". However the score of the prediction is very poor score 22%\n\nAccording to the analisys the reason is because of the Discount column. So I removed it and run the process again. And the score was even lower :(",
				"Answer_has_accepted": false
			},
			{
				"Answer_creation_date": "2022-03-18T07:48:45.405Z",
				"Answer_upvote_count": 0,
				"Answer_body": "I suggest before you start to build your algorithm, do a data exploration. Does your data have a seasonality? Some items are just not seasonal.",
				"Answer_has_accepted": false
			}
		]
	},
	{
		"Questiont_title": "AWS sagemaker abalone example pipeline endpoint json rejected",
		"Question_creation_time": "2022-06-30T16:51:23.243Z",
		"Question_link": "https://repost.aws/questions/QUJRDfNUpdR-WQgjsw_UOi5g/aws-sagemaker-abalone-example-pipeline-endpoint-json-rejected",
		"Question_topic": [
			"Machine Learning & AI",
			"DevOps"
		],
		"Question_tag": [
			"Amazon SageMaker",
			"Machine Learning & AI",
			"DevOps"
		],
		"Question_upvote_count": 0,
		"Question_view_count": 145,
		"Question_answer_count": 1,
		"Question_has_accepted_answer": false,
		"Question_body": "We've just created a train/build/deploy template in AWS SageMaker which provides a deployment of an Abalone model. We're trying to test it via the Test Inference endpoint, but the JSON there is rejected with the following message:\n\nError invoking endpoint: Received client error (415) from model with message \"application/json is not an accepted ContentType: csv, libsvm, parquet, recordio-protobuf, text/csv, text/libsvm, text/x-libsvm, application/x-parquet, application/x-recordio-protobuf.\". See https://eu-west-1.console.aws.amazon.com/cloudwatch/home?region=eu-west-1#logEventViewer:group=/aws/sagemaker/Endpoints/USEngProbOfConversion-staging in account 607522716587 for more information.\n\nHowever the Test Inference endpoint only allows us to hit the endpoint with JSON - what can we do? Here's a screenshot (if this dropbox embed works):\n\nbut that's not working so here's the request dump:\n\n{\n  \"body\": {\n    \"s-x\": \"M\",\n    \"length\": 3,\n    \"diameter\": 5,\n    \"height\": 7,\n    \"whole_weight\": 45,\n    \"shucked_weight\": 34,\n    \"viscera_weight\": 23,\n    \"shell_weight\": 76\n  },\n  \"contentType\": \"application/json\",\n  \"endpointName\": \"USEngProbOfConversion-staging\",\n  \"customURL\": \"\",\n  \"customHeaders\": [\n    {\n      \"Key\": \"sm_endpoint_name\",\n      \"Value\": \"USEngProbOfConversion-staging\"\n    },\n    {\n      \"Key\": \"\",\n      \"Value\": \"\"\n    }\n  ]\n}",
		"Answers": [
			{
				"Answer_creation_date": "2022-07-01T03:03:27.263Z",
				"Answer_upvote_count": 0,
				"Answer_body": "So as you saw already, the SageMaker Studio \"Test inference\" UI currently only supports JSON format... But this is a constraint of the UI, not your endpoint.\n\nIf you want to test your deployed endpoint with non-JSON data, you can do this from code (e.g. from a notebook):\n\nUsing the sagemaker Python SDK, create a Predictor specifying your endpoint name and the relevant de/serializers (from sagemaker.(de)serializers - for example sagemaker.serializers.CSVSerializer). Then call predictor.predict(data).\nUsing a boto3.client(\"sagemaker-runtime\"), serialize your data to required format yourself (e.g. \"M,3,5,7,45,34,23,76\") and then call invoke_endpoint().\n\nThis would be necessary if you're using a pre-built algorithm that doesn't support JSON as a request/response format. Since you're using the Abalone pipeline example, I guess it's likely you're using XGBoost as a pre-built SageMaker algorithm?\n\nAlternatively, if you're building a custom algorithm with your own training script OR would be interested in using XGBoost as a script-mode framework - more information in the SageMaker SDK doc), you may like to extend your algorithm to accept application/json requests and return JSON responses.\n\nThe process for this will vary a little by framework, but should be documented here for XGBoost. Essentially, you'll want to provide a script file e.g. inference.py which defines special functions input_fn() and output_fn(). You can provide implementations of these functions that accept application/json content types and de/serialize appropriately.\n\nThat way you could make your deployed endpoint support JSON format and therefore be able to use the test UI in SageMaker Studio.",
				"Answer_has_accepted": false
			}
		]
	},
	{
		"Questiont_title": "invoke_endpoint error in Lambda: StreamingBody is not JSON serializable",
		"Question_creation_time": "2019-02-20T06:36:44.000Z",
		"Question_link": "https://repost.aws/questions/QU6wwr5n1XTuWtQjkbCGbCOA/invoke-endpoint-error-in-lambda-streaming-body-is-not-json-serializable",
		"Question_topic": [
			"Machine Learning & AI"
		],
		"Question_tag": [
			"Amazon SageMaker"
		],
		"Question_upvote_count": 0,
		"Question_view_count": 763,
		"Question_answer_count": 5,
		"Question_has_accepted_answer": false,
		"Question_body": "I'm writing a Lambda function that invokes an endpoint:\n\nruntime= boto3.Session().client('runtime.sagemaker')\r\npayload = {\"data\": [\"McDonalds\"]}\r\nresponse = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME,\r\n                                       ContentType='application/json',\r\n                                       Body=json.dumps(payload))\n\n\nIt returns this error\n\nAn error occurred during JSON serialization of response: <botocore.response.StreamingBody object at 0x7f59e40acc50> is not JSON serializable\n\n\nI tried this exact function in SageMaker notebook and it works but it doesn't work in Lambda. Can someone please help me?\n\nEdited by: aurelius on Feb 19, 2019 10:38 PM",
		"Answers": [
			{
				"Answer_creation_date": "2019-12-20T09:29:36.000Z",
				"Answer_upvote_count": 0,
				"Answer_body": "Was this because I didn't attach the necessary SageMaker policies to the IAM role? I only added this policy:\n\n{\r\n    \"Version\": \"2012-10-17\",\r\n    \"Statement\": [\r\n        {\r\n            \"Sid\": \"VisualEditor0\",\r\n            \"Effect\": \"Allow\",\r\n            \"Action\": \"sagemaker:InvokeEndpoint\",\r\n            \"Resource\": \"*\"\r\n        }\r\n    ]\r\n}",
				"Answer_has_accepted": false
			},
			{
				"Answer_creation_date": "2019-12-20T11:03:09.000Z",
				"Answer_upvote_count": 0,
				"Answer_body": "Hi aurelius,\nDo you have further code after the response = runtime.invoke_endpoint(..) line? the error message says problem with serialization of the response object.\n\nYour role permission looks fine.\n\nThank you,\nArun",
				"Answer_has_accepted": false
			},
			{
				"Answer_creation_date": "2019-12-18T12:10:27.000Z",
				"Answer_upvote_count": 0,
				"Answer_body": "I am facing similar issue, in sagemaker jupyter notebook instance the endpoint is invoked successfully and I am able to get back the inference results.\n\nBelow is my Lambda function that i used to invoke the endpoint, but I am facing the following error,\n\nimport json \r\nimport io\r\nimport boto3 \r\n\r\nclient = boto3.client('runtime.sagemaker')\r\n\r\ndef lambda_handler(event, context):\r\n    print(\"Received event: \" + json.dumps(event, indent=2))\r\n    \r\n    #data = json.loads(json.dumps(event))\r\n    #payload = data['data']\r\n    print(json.dumps(event))\r\n    \r\n    response = client.invoke_endpoint(EndpointName='linear-learner-2019-12-12-16-20-56-788',\r\n                                  ContentType='application/json',\r\n                                  Body=(json.dumps(event)))\r\n    return response\n\n\nOutput:\n\nFunction Logs:\r\nSTART RequestId: 7f4c7589-b70f-4af8-834c-89a1a1fbe5e5 Version: $LATEST\r\nReceived event: {\r\n  \"instances\": [\r\n    {\r\n      \"features\": [\r\n        0.1,\r\n        0.2\r\n      ]\r\n    }\r\n  ]\r\n}\r\n{\"instances\": [{\"features\": [0.1, 0.2]}]}\r\nAn error occurred during JSON serialization of response: <botocore.response.StreamingBody object at 0x7f53918e2828> is not JSON serializable\n\n\nPlease help me resolve the issue, you can see the JSON input passed to the function. Not sure what is going wrong here. I even checked the cloudwatch logs not able to identify the origin of the issue.\n\nThanks in advance,\nArun\n\nEdited by: NMAK on Dec 18, 2019 5:54 AM",
				"Answer_has_accepted": false
			},
			{
				"Answer_creation_date": "2019-02-22T01:07:19.000Z",
				"Answer_upvote_count": 0,
				"Answer_body": "I call Sagemaker from Lambda using a slightly different approach in terms of data structures:\n\nfinal_data = ','.join(ordered_data.iloc[0].astype(str).values.tolist())\r\nruntime = boto3.client('runtime.sagemaker')\r\nresponse = runtime.invoke_endpoint(EndpointName='whatever-endpoint', \r\n                                           ContentType='text/csv',\r\n                                           Body=final_data)\r\nresult = json.loads(response['Body'].read().decode())\n\n\nI start with a dataframe of one row containing all the data.",
				"Answer_has_accepted": false
			},
			{
				"Answer_creation_date": "2019-02-20T22:36:48.000Z",
				"Answer_upvote_count": 0,
				"Answer_body": "hello Javierlopez,\n\nThanks for your response, I really appreciate it.\n\nI tried the below code and still it gives me same error\n\n    payload = ','.join(str(item) for item in data['instances'][0]['data']['features'])\r\n    #payload=bytearray(payload)\r\n    print(payload)\r\n    \r\n    response = client.invoke_endpoint(EndpointName='linear-learner-2019-12-12-16-20-56-788',\r\n                                  ContentType='text/csv',\r\n                                  Body=payload)\r\n    return response\n\n\nbelow is output/error for the above code.\n\nFunction Logs:\r\nSTART RequestId: 73daa03e-dec2-4d37-b779-72c1f70a7142 Version: $LATEST\r\nReceived event: {\r\n  \"instances\": [\r\n    {\r\n      \"data\": {\r\n        \"features\": [\r\n          0.1,\r\n          0.2\r\n        ]\r\n      }\r\n    }\r\n  ]\r\n}\r\n0.1,0.2 # this is the payload sent for inference.\r\nAn error occurred during JSON serialization of response: <botocore.response.StreamingBody object at 0x7f77555e59e8> is not JSON serializable\n\n\nI really don't understand the logic behind this on what format the function expects the input, my colleague ran a different model with same JSON format I used and it works fine. I believe you used this is inference format for XGBoost. I am not able to find any documentation on linear-learner sample inference requests using lambda.\n\nCould you please point me in the right direction.\n\nRegards,\nArun\n\nEdited by: NMAK on Dec 20, 2019 3:04 AM",
				"Answer_has_accepted": false
			}
		]
	},
	{
		"Questiont_title": "Ground Truth Label Jobs - Same document repeating in the labeling job",
		"Question_creation_time": "2022-11-10T09:40:21.576Z",
		"Question_link": "https://repost.aws/questions/QUu20cb3GSTPKBsnJsRDjbYg/ground-truth-label-jobs-same-document-repeating-in-the-labeling-job",
		"Question_topic": [
			"Machine Learning & AI"
		],
		"Question_tag": [
			"Amazon SageMaker",
			"Machine Learning & AI",
			"Amazon SageMaker Ground Truth"
		],
		"Question_upvote_count": 0,
		"Question_view_count": 21,
		"Question_answer_count": 0,
		"Question_has_accepted_answer": false,
		"Question_body": "We want to extract the entities from pdf documents. We are exploring labeling the existing document using Safemaker GroundTruth. The documents are keep repeating for some reasons which we are not able to identify the reason ? When it repeats some pages coming as read only and other pages will us to annotate. When it repeats the previously read only pages are annotable .\n\nCan some expert from this help us to understand why it is repeating ?",
		"Answers": []
	},
	{
		"Questiont_title": "SageMaker Model Spend",
		"Question_creation_time": "2020-06-16T13:03:59.000Z",
		"Question_link": "https://repost.aws/questions/QUlNS8ujYmQqePwWS-mgso3Q/sage-maker-model-spend",
		"Question_topic": [
			"Machine Learning & AI"
		],
		"Question_tag": [
			"Amazon SageMaker"
		],
		"Question_upvote_count": 0,
		"Question_view_count": 71,
		"Question_answer_count": 1,
		"Question_has_accepted_answer": true,
		"Question_body": "If I deploy a SageMaker model, am I incurring hosting charges even while no one is accessing my model?",
		"Answers": [
			{
				"Answer_creation_date": "2020-06-16T13:24:24.000Z",
				"Answer_upvote_count": 0,
				"Answer_body": "When you deploy a SageMaker model, it deploys it behind a SageMaker endpoint for real-time inference. You are charged by the second for on-demand ML hosting. Check the model deployment section of each region on the SageMaker Pricing page. In some use cases, you can save on inference cost by hosting several models behind the same endpoint (check this blog post).",
				"Answer_has_accepted": true
			}
		]
	},
	{
		"Questiont_title": "What is SageMaker Autopilot doing when in state \"InProgress - AnalyzingData\" ?",
		"Question_creation_time": "2020-07-29T15:33:07.000Z",
		"Question_link": "https://repost.aws/questions/QU8QUiTTSMQ2W2uOgHXC7lqA/what-is-sage-maker-autopilot-doing-when-in-state-in-progress-analyzing-data",
		"Question_topic": [
			"Machine Learning & AI"
		],
		"Question_tag": [
			"Amazon SageMaker",
			"Machine Learning & AI"
		],
		"Question_upvote_count": 0,
		"Question_view_count": 24,
		"Question_answer_count": 1,
		"Question_has_accepted_answer": true,
		"Question_body": "Hi,\n\nI'm trying this nice SageMaker Autopilot demo https://github.com/awslabs/amazon-sagemaker-examples/blob/master/autopilot/autopilot_customer_churn_high_level_with_evaluation.ipynb\n\nAt the beginning of the job, the status is \"InProgress - AnalyzingData\" for several minutes. This is long enough that I'd like to know more about it: what is Autopilot doing when at that status?",
		"Answers": [
			{
				"Answer_creation_date": "2020-08-20T01:25:08.000Z",
				"Answer_upvote_count": 0,
				"Answer_body": "There are some metrics begin collected in this stage. To understanding what is doing is the same as what happens when you're using tensorflow autoML. There's a deep explanation what is does in our Science page https://www.amazon.science/publications/amazon-sagemaker-autopilot-a-white-box-automl-solution-at-scale",
				"Answer_has_accepted": true
			}
		]
	},
	{
		"Questiont_title": "Does Sagemaker support private GitLab repos?",
		"Question_creation_time": "2022-04-26T17:52:54.723Z",
		"Question_link": "https://repost.aws/questions/QUI8X0yZHTQ1uiS-TxOJe4ww/does-sagemaker-support-private-git-lab-repos",
		"Question_topic": [
			"Machine Learning & AI"
		],
		"Question_tag": [
			"Amazon SageMaker",
			"Machine Learning & AI"
		],
		"Question_upvote_count": 1,
		"Question_view_count": 283,
		"Question_answer_count": 1,
		"Question_has_accepted_answer": false,
		"Question_body": "I have a private GitLab and would like to connect it to Sagemaker Studio and/or Notebooks. Is that supported? What is it is through a VPC? If so, how can it be done?",
		"Answers": [
			{
				"Answer_creation_date": "2022-04-27T07:23:12.161Z",
				"Answer_upvote_count": 1,
				"Answer_body": "Yes, it is possible to connect a private GitLab to SageMaker Studio and Notebooks. You will find the detailed steps that you can use in this blog post.\n\nThe standard documentation on Git repository associations can be found here.\n\nIf you are interested in how to build MLOps workflows with Amazon SageMaker projects, GitLab, and GitLab pipelines this recent blog provides valuable information.\n\nYou can also do the same configuration in a VPC, just ensure you have a route to your private GitLab repository.",
				"Answer_has_accepted": false
			}
		]
	},
	{
		"Questiont_title": "AutoPilot for Forecasting",
		"Question_creation_time": "2020-07-19T10:26:56.000Z",
		"Question_link": "https://repost.aws/questions/QUu_IodOxiQ6eTu010AYb8pQ/auto-pilot-for-forecasting",
		"Question_topic": [
			"Machine Learning & AI"
		],
		"Question_tag": [
			"Amazon SageMaker",
			"Machine Learning & AI"
		],
		"Question_upvote_count": 0,
		"Question_view_count": 40,
		"Question_answer_count": 1,
		"Question_has_accepted_answer": true,
		"Question_body": "Hi there,\n\nIHAC who asked a question regarding the possible use of AutoPilot for solving Forecasting problems. They don't have the knowledge to play with DeepAR and they are running tests in parallel with Amazon Forecast. Their questions are:\n\nIs it possible to use AutoPilot for Forecasting problems? (my answer would be yes, since regression problems can be solved by XGBoost, which also won a bunch of competitions on Forecasting)\nWhich kind of pre-processing should the customer do and which pre-processing is done by AutoPilot which could simplify transformation of data for solving forecasting? In particular: are there any transformation to be done on the timestamp column? Should we introduce lagged entries - or is it done by AutoPilot?\n\nThanks to those taking the time to answer these questions :)\n\nBest, Davide Gallitelli",
		"Answers": [
			{
				"Answer_creation_date": "2020-07-20T08:24:30.000Z",
				"Answer_upvote_count": 0,
				"Answer_body": "Although it is possible to model the forecasting as a regression problem in Autopilot, there is no time-series capability built in Autopilot. So, you need to do the preprocessing tasks such as time-series windowing, lag differencing, etc. in order to generate the training/test datasets for the autopilot experiment.\n\nAdditionally, time series forecasting usually requires a model which can detect the pattern in a sequence of features. So, services such as DeepAR or Amazon forecast provide better capabilities to address this challenge.",
				"Answer_has_accepted": true
			}
		]
	},
	{
		"Questiont_title": "sagemakee endpoint failing with \"\"An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (413) from primary and could not load the entire response body\"\"",
		"Question_creation_time": "2022-11-25T20:10:59.790Z",
		"Question_link": "https://repost.aws/questions/QUbzR_PqclRoK5dmfEzR53wg/sagemakee-endpoint-failing-with-an-error-occurred-model-error-when-calling-the-invoke-endpoint-operation-received-client-error-413-from-primary-and-could-not-load-the-entire-response-body",
		"Question_topic": [
			"Serverless",
			"Machine Learning & AI"
		],
		"Question_tag": [
			"Serverless",
			"Amazon SageMaker",
			"Machine Learning & AI"
		],
		"Question_upvote_count": 0,
		"Question_view_count": 64,
		"Question_answer_count": 1,
		"Question_has_accepted_answer": false,
		"Question_body": "Hello, I have created sagemaker endpoint by following https://github.com/huggingface/notebooks/blob/main/sagemaker/20_automatic_speech_recognition_inference/sagemaker-notebook.ipynb and this is failing with error \"\"An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (413) from primary and could not load the entire response body\"\".\n\nThe predict function returning me following error but CW log does not have any error details for the endpoint.\n\nModelError Traceback (most recent call last)\n/tmp/ipykernel_16248/2846183179.py in\n2 # audio_path = \"s3://ml-backend-sales-call-audio/sales-call-audio/1279881599154831602.playback.mp3\"\n3 audio_path = \"/home/ec2-user/SageMaker/finetune-deploy-bert-with-amazon-sagemaker-for-hugging-face/1279881599154831602.playback.mp3\" ## AS OF NOW have stored locally in notebook instance\n----> 4 res = predictor.predict(data=audio_path)\n5 print(res)\n\n~/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages/sagemaker/predictor.py in predict(self, data, initial_args, target_model, target_variant, inference_id)\n159 data, initial_args, target_model, target_variant, inference_id\n160 )\n--> 161 response = self.sagemaker_session.sagemaker_runtime_client.invoke_endpoint(**request_args)\n162 return self._handle_response(response)\n163\n\n~/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages/botocore/client.py in _api_call(self, *args, **kwargs)\n493 )\n494 # The \"self\" in this scope is referring to the BaseClient.\n--> 495 return self._make_api_call(operation_name, kwargs)\n496\n497 _api_call.name = str(py_operation_name)\n\n~/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages/botocore/client.py in _make_api_call(self, operation_name, api_params)\n912 error_code = parsed_response.get(\"Error\", {}).get(\"Code\")\n913 error_class = self.exceptions.from_code(error_code)\n--> 914 raise error_class(parsed_response, operation_name)\n915 else:\n916 return parsed_response\n\nModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (413) from primary and could not load the entire response body. See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/asr-facebook-wav2vec2-base-960h-2022-11-25-19-27-19 in account xxxx for more information.\n\n`",
		"Answers": [
			{
				"Answer_creation_date": "2022-11-26T15:49:44.270Z",
				"Answer_upvote_count": 0,
				"Answer_body": "Hello AmitKayal,\n\nI understand that you have successfully created an Endpoint. However, when you try to invoke this Endpoint you get the following error:\n\nModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (413) from primary and could not load the entire response body. See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/asr-facebook-wav2vec2-base-960h-2022-11-25-19-27-19 in account xxxx for more information.\n\n\n\nAnd when you looked at your CloudWatch logs there was nothing related to this error. Let me know if I have misunderstood anything.\n\nThe ClientError 413 usually occurs when the payload size for the endpoint invocation exceeds the limit of 6 MB [1,2], this could be the reason why your Endpoint is throwing the error 413.\n\nIf your payload size is more than 6MB you can work around this by using either Batch Transform [3] or Asynchronous Inference [4]. Batch Transform can be used if you would like to process the request to your model in batches. With Batch Transform you have an option to define your own maximum payload size [5]. Otherwise, if you want to receive inference for each request to your model you can use Asynchronous Inference which takes up to 1 GB of payload size with the runtime of about 15 minutes [6]. Asynchronous Inference queues requests to your model and processes them asynchronously, this is ideal for payloads that are greater than 6MB but not more than 1GB.\n\nShould the suggested work around not work, I recommend that you open a Support case with AWS Technical Support [7]. The Technical Support team will be able to help you to further troubleshoot this issue.\n\nI trust this information finds you well. Should you have any further questions, please feel free to reach out.\n\nReferences:\n\nhttps://github.com/aws/amazon-sagemaker-examples/issues/245\nhttps://docs.aws.amazon.com/general/latest/gr/sagemaker.html#limits_sagemaker\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/inference-pipeline-batch.html\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/async-inference.html\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/hosting-faqs.html#hosting-faqs-general\nhttps://support.console.aws.amazon.com/support/home?region=us-east-1#/case/create",
				"Answer_has_accepted": false
			}
		]
	},
	{
		"Questiont_title": "How to access file system in Sagemaker notebook instance from outside of that instance (ie via Python Sagemaker Estimator training call)",
		"Question_creation_time": "2021-12-07T21:58:13.851Z",
		"Question_link": "https://repost.aws/questions/QU3yXAL7d7Sl--kKO3TTZf1g/how-to-access-file-system-in-sagemaker-notebook-instance-from-outside-of-that-instance-ie-via-python-sagemaker-estimator-training-call",
		"Question_topic": [
			"Machine Learning & AI",
			"Storage"
		],
		"Question_tag": [
			"Build & Train ML Models",
			"Amazon SageMaker",
			"Storage"
		],
		"Question_upvote_count": 0,
		"Question_view_count": 690,
		"Question_answer_count": 1,
		"Question_has_accepted_answer": true,
		"Question_body": "Hi,\n\nI have large image dataset stored in a Sagemaker notebook instance, in the file system. I was hoping to learn how I could access this data from outside of that particular notebook instance. I have done quite a bit of researching but can't seem to find much - I am relatively new to this.\n\nI want to be able to access the data in that notebook in a fast manner as I will be using the data to train an AI model. Is there any recommended way to do this?\n\nI originally uploaded the data within that notebook instance to train a model within that instance in exactly the same file system. Note that it is a reasonably large dataset which I had to do some preprocessing on within Sagemaker.\n\nWhat is the best way to store data when using the Sagemaker estimators from training AI models?\n\nMany thanks\n\nTim",
		"Answers": [
			{
				"Answer_creation_date": "2021-12-07T22:53:56.668Z",
				"Answer_upvote_count": 2,
				"Answer_body": "Hi Tim, when you create a sagemaker training job using the estimator, the general best practice is to store your data on S3 and the training job will launch instances as requested by the training job configuration. As now we support fast file mode, which allows faster training job start compared to the file mode (which downloads the data from s3 to the training instance). But when you say you used sagemaker notebook instance to train the model, I assume you were not using SageMaker Training jobs but rather running the notebook (.ipynb) on the SageMaker notebook instance. Please note that as SageMaker is a fully managed service, the notebook instance (also training instances, hosting instances etc.) are launched in the service account, so you will not have directly access to those instance. The SageMaker notebook instance use EBS to store data and the EBS volume is mounted to the /home/ec2-user/SageMaker. Please note that the EBS volume used by a SageMaker notebook instance can only be increased but not decrease. If you want to reduce the EBS volume, you need to create a new notebook instance with a smaller volume and move your data from the previous instance via s3. You will not be able to access that EBS volume from outside of the SageMaker notebook instance. The general best practice is to store large dataset on s3 and only use sample data on the SageMaker notebook instance (reduce the storage). Then use that small amount of sample data to test/build your code. Then when you are ready to train on the whole dataset, you can launch a SageMaker training job and use the whole dataset stored on s3. Note that, running the training on the whole dataset on a SageMaker notebook instance will require you to use a big instance with enough computing power and also will not be able to perform distributed training with multiple instances. Comparatively, if you run the training job use SageMaker training instances, it gives you more flexibility of choosing the instance type and allow you to run on multiple instances for distributed training. Lastly, once the SageMaker training job is done, all the resources will be terminated which will save cost compared to continue using the big instance with a SageMaker notebook instance. Hope this has helped answer your question",
				"Answer_has_accepted": true
			}
		]
	},
	{
		"Questiont_title": "Is it possible to create Parallel Pipelines in Sagemaker",
		"Question_creation_time": "2022-04-11T08:34:43.053Z",
		"Question_link": "https://repost.aws/questions/QUZtsbNf1GTAOnaTCrif-WJg/is-it-possible-to-create-parallel-pipelines-in-sagemaker",
		"Question_topic": [
			"Machine Learning & AI"
		],
		"Question_tag": [
			"Amazon SageMaker",
			"Amazon SageMaker Pipelines"
		],
		"Question_upvote_count": 0,
		"Question_view_count": 373,
		"Question_answer_count": 1,
		"Question_has_accepted_answer": false,
		"Question_body": "I want to bind processing pipeline to multiple training pipeline. I just want to compare algorithm accuracy. Same dataset will be trained by multiple algorithms and will be predicted by them. My goal for the future is consolidate predict results of different algorithms and generate combined/consolidated resulst. Is is possible to do in SageMaker.\n\nExample Schema:\n\n            - Train_Algo1      \n Process    - Train_Algo2    - Predict Result\n            - Train_AlgoN",
		"Answers": [
			{
				"Answer_creation_date": "2022-04-11T09:48:58.637Z",
				"Answer_upvote_count": 0,
				"Answer_body": "I'd recommend checking out SageMaker Pipelines for this - especially if you're able to use SageMaker Studio for the graphical pipeline management UIs.\n\nYou can build your pipeline definition through the SageMaker Python SDK, just like you might normally define Training and Processing jobs. In fact pipeline steps (like TrainingStep) typically just wrap around the standalone constructs (like Estimator) that you might be using already.\n\nPipeline steps are executed in paralllel by default, unless there is an implicit (properties data) or explicit (depends_on) dependency between them.\n\nSM Pipelines can take parameters, so you could expose necessary training hyperparameters or pre-processing parameters up to the pipeline level, and use the pipeline to kick off multiple end-to-end runs with different configurations.\n\nBy turning on step caching, you could prevent your pre-processing from being re-run if the input parameters are unchanged (however, note that caching doesn't look at ongoing executions: So better to trigger one pipeline execution first and wait a bit for the processing step to complete, rather than triggering ~20 all at once so none of them see a cached processing result and all re-run the job).\n\n...And Pipelines automatically tag SageMaker Experiments config (Pipeline = Experiment; Execution = Trial; Step = Trial Component) which you can then use to plot and compare multiple training jobs in the SM Studio UI. So for example your pipeline might just be Pre-process > Train > Evaluate > RegisterModel. If you right click your pipeline's \"Experiment\" in SMStudio Experiments and Trials view, you can open a list of the executed training jobs and select multiple to scatter-plot the final loss/accuracy vs the hyperparameters.\n\nIf you run your evaluation as a SageMaker Processing Job which outputs a JSON in model quality metrics format, you can even have your pipeline load the model into SM Model Registry tagged with this data. This way, you'd be able to see and compare the metrics between model versions (and even charts e.g. ROC curves in classification case) through the SMStudio Model Registry UI.\n\nSome relevant code samples:\n\nhttps://github.com/aws/amazon-sagemaker-examples/tree/main/sagemaker-pipeline-compare-model-versions\nhttps://github.com/aws/amazon-sagemaker-examples/tree/main/sagemaker-pipeline-parameterization",
				"Answer_has_accepted": false
			}
		]
	},
	{
		"Questiont_title": "Why my sagemaker training job slower than notebook from studiolab.sagemaker.aws?",
		"Question_creation_time": "2022-05-04T01:18:54.522Z",
		"Question_link": "https://repost.aws/questions/QUsEj-8jnJRTK3a3gXcH8dRw/why-my-sagemaker-training-job-slower-than-notebook-from-studiolab-sagemaker-aws",
		"Question_topic": [
			"Compute",
			"Machine Learning & AI"
		],
		"Question_tag": [
			"High Performance Compute",
			"Amazon SageMaker",
			"Machine Learning & AI",
			"Amazon SageMaker Studio Lab",
			"Amazon SageMaker Model Training"
		],
		"Question_upvote_count": 0,
		"Question_view_count": 84,
		"Question_answer_count": 1,
		"Question_has_accepted_answer": false,
		"Question_body": "I run neural network tensorflow train on studiolab. and I got:\n\nEpoch 145/4000\n1941/1941 - 10s - ... - 10s/epoch - 5ms/step\n\n\nthen I try to make a train job with script_mode with ml.c5.xlarge\n\nestimator = TensorFlow(entry_point='untitled.py',\n                       source_dir='./training/',\n                       instance_type='ml.c5.xlarge',\n                       instance_count=1,\n                       output_path=\"s3://sagemaker-[skip]\",\n                       role=sagemaker.get_execution_role(),\n                       framework_version='2.8.0',\n                       py_version='py39',\n                       hyperparameters={...},\n                       metric_definitions=[...],\n                       script_mode=True)\n\n\nand its got:\n\nEpoch 19/4000\n1941/1941 - 49s - ... - 49s/epoch - 25ms/step\n\n\nWhy is it 5 times slower than studiolab notebook? Is it because instance type?",
		"Answers": [
			{
				"Answer_creation_date": "2022-05-05T23:51:05.130Z",
				"Answer_upvote_count": 0,
				"Answer_body": "May I know which instance type you are using for training locally on your notebook instance. Including factors that influence training performance, hardware spec of the training node is very critical. You might be either getting bottlenecked on CPU, Storage or Memory. See here for more details",
				"Answer_has_accepted": false
			}
		]
	},
	{
		"Questiont_title": "Reason why errors occur when starting SageMaker Studio",
		"Question_creation_time": "2022-08-31T04:39:24.293Z",
		"Question_link": "https://repost.aws/questions/QUfTu0DsSxTf-B8n4srTzG9A/reason-why-errors-occur-when-starting-sage-maker-studio",
		"Question_topic": [
			"Machine Learning & AI"
		],
		"Question_tag": [
			"Amazon SageMaker"
		],
		"Question_upvote_count": 0,
		"Question_view_count": 152,
		"Question_answer_count": 1,
		"Question_has_accepted_answer": false,
		"Question_body": "Hello! I have a question about errors found when starting SageMaker Studio (below).\n\nAccessDeniedException User: X is not auth**orized to perform: sagemaker:CreateDomain on resource: arn:aws:sagemaker:ap-northeast-1:XX because no identity-based policy allows the sagemaker:CreateDomain action\n\nValidationException Access denied in getting/deleting the portfolio shared by SageMaker. Please call withservicecatalog:ListAcceptedPortfolioShares permission.\n\nAccessDeniedException User: X is not authorized to perform: sagemaker:CreateUserProfile on resource: arn:aws:sagemaker:ap-northeast-1:XX because no identity-based policy allows the sagemaker:CreateUserProfile action\n\nI resolved the errors by adding some inline policies, but I cannot understand the reason why the errors occur on my user with S3 Full Access and SageMaker Full Access policies.\n\nI'd happy to tell me any information about the errors. Thank you!",
		"Answers": [
			{
				"Answer_creation_date": "2022-08-31T12:25:56.766Z",
				"Answer_upvote_count": 1,
				"Answer_body": "As far as I know, according the aws docs the passRole action should be granted to the SageMake execution role for some cases such as creating images. So your s3:* and sagemaker:* is not enough, but still need to add iam:passRole to policies",
				"Answer_has_accepted": false
			}
		]
	},
	{
		"Questiont_title": "How to serve a pretrained model from hugging face in sagemaker without custom script?",
		"Question_creation_time": "2022-09-07T01:32:20.477Z",
		"Question_link": "https://repost.aws/questions/QUFjO6dWOKQW2RdY8XyeP0-A/how-to-serve-a-pretrained-model-from-hugging-face-in-sagemaker-without-custom-script",
		"Question_topic": [
			"Machine Learning & AI"
		],
		"Question_tag": [
			"Amazon SageMaker",
			"Machine Learning & AI"
		],
		"Question_upvote_count": 0,
		"Question_view_count": 31,
		"Question_answer_count": 1,
		"Question_has_accepted_answer": false,
		"Question_body": "I have been working with an example , where I write my own custom script ( sample below) , where i am overriding the predict_fn and other functions. I have tested my model without the custom script or inference.py. in the event when we don't provide our custom script, how is the model called? what does the default code for predict_fn look like, when I don't override it?\n\ninference.py\n\n\nimport os\nimport json\nimport torch\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef model_fn(model_dir):\n   model_dir = './pytorch_model.bin'\n\n    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n    model = AutoModelForSeq2SeqLM.from_pretrained(model_dir).to(device).eval()\n    \n    model_dict = {'model':model, 'tokenizer':tokenizer}\n    \n    return model_dict\n        \n\ndef predict_fn(input_data, model_dict):\n \n    input = input_data.pop('inputs')\n   \n    \n    tokenizer = model_dict['tokenizer']\n    model = model_dict['model']\n\n    input_ids = tokenizer(input, truncation=True, return_tensors=\"pt\").input_ids.to(device)\n     ....\n    \n    \ndef input_fn(request_body, request_content_type):\n    return  json.loads(request_body)",
		"Answers": [
			{
				"Answer_creation_date": "2022-09-07T19:46:58.567Z",
				"Answer_upvote_count": 0,
				"Answer_body": "Usually you have to write your own inference code. Default predict_fn function is just returning a model. You may want to check the documentation here: https://docs.aws.amazon.com/sagemaker/latest/dg/adapt-inference-container.html",
				"Answer_has_accepted": false
			}
		]
	},
	{
		"Questiont_title": "How to prevent disassociating SageMaker LifecycleConfig unintentionally",
		"Question_creation_time": "2022-03-04T03:15:23.581Z",
		"Question_link": "https://repost.aws/questions/QUTvkDhX_yQXyW7WpFinO_vA/how-to-prevent-disassociating-sage-maker-lifecycle-config-unintentionally",
		"Question_topic": [
			"Machine Learning & AI"
		],
		"Question_tag": [
			"Amazon SageMaker"
		],
		"Question_upvote_count": 0,
		"Question_view_count": 28,
		"Question_answer_count": 0,
		"Question_has_accepted_answer": false,
		"Question_body": "When you go to SageMaker Notebook Instance edit screen in AWS Web Console (to change the Instance Type for example), it is sometimes the case that Lifecycle configuration is popped up as No Configuration even though the configuration is actually set earlier. This results in an unintentional disassociation of the LifecycleConfig because it's easy to save the instance change without noticing the change in Lifecycle Config. This is a serious problem for us. I was able to reproduce this issue in Chrome and Firefox (but you need to try several times to repro the issue).\n\nI am in the position of provisioning different cloud resources for the end users and I need a way to systematically prevent this disassociation to happen. I considered applying an IAM policy that denies the update operation containing the change in the LifecycleConfig of notebooks, but there seems no condition key for LifecycleConfig which makes me think this approach isn't feasible.\n\nWhat can I do?\n\nThanks.",
		"Answers": []
	},
	{
		"Questiont_title": "Using Hyperparameter Tuning Jobs over Training and Preprocessing",
		"Question_creation_time": "2021-01-14T21:01:14.000Z",
		"Question_link": "https://repost.aws/questions/QU3xcWKDPHR8ylWSaX83lNKQ/using-hyperparameter-tuning-jobs-over-training-and-preprocessing",
		"Question_topic": [
			"Machine Learning & AI"
		],
		"Question_tag": [
			"Amazon SageMaker",
			"Machine Learning & AI"
		],
		"Question_upvote_count": 0,
		"Question_view_count": 65,
		"Question_answer_count": 1,
		"Question_has_accepted_answer": true,
		"Question_body": "Some data science teams want to tune the hyperparameters of their preprocessing jobs alongside ML model training jobs.\n\nDoes AWS have a recommended approach to establish this using Sagemaker Hyperparameter tuning?",
		"Answers": [
			{
				"Answer_creation_date": "2021-01-15T20:17:42.000Z",
				"Answer_upvote_count": 0,
				"Answer_body": "It depends on the dataset and the question for ML to answer.\n\nYes, it is feasible to do HPO with preprocessing. However, to run a HPO job, it is required to define to a specific target to achieve, e.g. maximize/minimize certain values during the whole HPO process. Thus, it is important to understand what is the target during preprocessing. If the answer is yes, they should be able to leverage Hyperparameter Tuning Jobs.\n\nHere is how HPO works in SageMaker. Firstly, we define each training Job with output in a container and specify the hyperparameters in /opt/ml/input/config/hyperparameters.json. When we run the pipeline using HyperparameterTuner in SageMaker, the initial Job can pass the hyperparameters to the Pipeline for HPO, and return the model with highest score.\n\nOption 1, if there is a clear defined target for preprocessing to achieve, we can also do HPO separately for data preprocessing through defining the function and outputs in a container and use HyperparameterTuner fit to tune the preprocessing.\n\nOption 2. include the preprocessing + training code in the whole SageMaker Training Job. But then you can't use separate infrastructure for training and preprocessing.\n\nSo it depends on what exactly they are looking for, but they can likely use SageMaker HPO.",
				"Answer_has_accepted": true
			}
		]
	},
	{
		"Questiont_title": "not authorized to perform: sagemaker:CreateModel on resource",
		"Question_creation_time": "2022-08-09T14:56:26.989Z",
		"Question_link": "https://repost.aws/questions/QU1sGemgvLQQS-w46eoBoo6w/not-authorized-to-perform-sagemaker-create-model-on-resource",
		"Question_topic": [
			"Machine Learning & AI",
			"DevOps",
			"Security, Identity, & Compliance"
		],
		"Question_tag": [
			"Amazon SageMaker",
			"Machine Learning & AI",
			"DevOps",
			"IAM Policies"
		],
		"Question_upvote_count": 0,
		"Question_view_count": 62,
		"Question_answer_count": 1,
		"Question_has_accepted_answer": false,
		"Question_body": "I have been given AmazonSagemakerFullAccess by my companie's AWS admin. No one at our company can figure out why I can't get this line to run to launch the model.\n\n***** CODE PRODUCING ERROR *****\n\nlang_id = sagemaker.Model( image_uri=container, model_data=model_location, role=role, sagemaker_session=sess ) lang_id.deploy(initial_instance_count=1, instance_type=\"ml.t2.medium\")\n\n***** ERROR MESSAGE *****\n\nClientError Traceback (most recent call last) <ipython-input-5-4c80ec284a4b> in <module> 2 image_uri=container, model_data=model_location, role=role, sagemaker_session=sess 3 ) ----> 4 lang_id.deploy(initial_instance_count=1, instance_type=\"ml.t2.medium\") 5 6 from sagemaker.deserializers import JSONDeserializer\n\n~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/sagemaker/model.py in deploy(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, tags, kms_key, wait, data_capture_config, async_inference_config, serverless_inference_config, **kwargs) 1132 1133 self._create_sagemaker_model( -> 1134 instance_type, accelerator_type, tags, serverless_inference_config 1135 ) 1136\n\n~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/sagemaker/model.py in _create_sagemaker_model(self, instance_type, accelerator_type, tags, serverless_inference_config) 671 tags=tags, 672 ) --> 673 self.sagemaker_session.create_model(**create_model_args) 674 675 def _ensure_base_name_if_needed(self, image_uri, script_uri, model_uri):\n\n~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/sagemaker/session.py in create_model(self, name, role, container_defs, vpc_config, enable_network_isolation, primary_container, tags) 2715 raise 2716 -> 2717 self._intercept_create_request(create_model_request, submit, self.create_model.name) 2718 return name 2719\n\n~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/sagemaker/session.py in _intercept_create_request(self, request, create, func_name) 4294 func_name (str): the name of the function needed intercepting 4295 \"\"\" -> 4296 return create(request) 4297 4298\n\n~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/sagemaker/session.py in submit(request) 2703 LOGGER.debug(\"CreateModel request: %s\", json.dumps(request, indent=4)) 2704 try: -> 2705 self.sagemaker_client.create_model(**request) 2706 except ClientError as e: 2707 error_code = e.response[\"Error\"][\"Code\"]\n\n~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/botocore/client.py in _api_call(self, *args, **kwargs) 506 ) 507 # The \"self\" in this scope is referring to the BaseClient. --> 508 return self._make_api_call(operation_name, kwargs) 509 510 _api_call.name = str(py_operation_name)\n\n~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/botocore/client.py in _make_api_call(self, operation_name, api_params) 909 error_code = parsed_response.get(\"Error\", {}).get(\"Code\") 910 error_class = self.exceptions.from_code(error_code) --> 911 raise error_class(parsed_response, operation_name) 912 else: 913 return parsed_response\n\nClientError: An error occurred (AccessDeniedException) when calling the CreateModel operation: User: arn:aws:sts::XXXXXXXXXX:assumed-role/sagemakeraccesstoservices/SageMaker is not authorized to perform: sagemaker:CreateModel on resource: arn:aws:sagemaker:us-east-2:XXXXXXXXXX:model/blazingtext-2022-08-09-13-58-21-739 because no identity-based policy allows the sagemaker:CreateModel action",
		"Answers": [
			{
				"Answer_creation_date": "2022-08-10T08:36:27.079Z",
				"Answer_upvote_count": 0,
				"Answer_body": "Based on your description I understand that you are trying to create a Model using the Amazon SageMaker Python SDK with an assumed IAM Role with the AmazonSagemakerFullAccess policy, which should allow sagemaker:CreateModel. It's difficult to identify what the underlying issue is since multiple IAM mechanisms (like SCPs or explicit Denies) can prevent the sagemaker:CreateModel permission.\n\nTo work towards a solution I would recommend:\n\nSimulate the sagemaker:CreateModel action using the IAM Policy Simulator with your User/Role. This may identify the root cause for your issue and how it can be fixed.\nOpen an AWS Support ticket describing the issue. AWS Support engineers are highly trained, experienced and well equipped to provide you with timely assistance.",
				"Answer_has_accepted": false
			}
		]
	},
	{
		"Questiont_title": "Please validate: SageMaker Endpoint URL Authentication/Authorization",
		"Question_creation_time": "2020-10-08T14:57:45.000Z",
		"Question_link": "https://repost.aws/questions/QUFlHNZ7JxTFGIkPHQ75u44w/please-validate-sage-maker-endpoint-url-authentication-authorization",
		"Question_topic": [
			"Machine Learning & AI"
		],
		"Question_tag": [
			"Amazon SageMaker"
		],
		"Question_upvote_count": 0,
		"Question_view_count": 282,
		"Question_answer_count": 1,
		"Question_has_accepted_answer": true,
		"Question_body": "Need validation:\n\nOnce the SageMaker endpoint is deployed. It can be invoked with the Sagemaker Runtime API InvokeEndpoint OR it can be invoked using the endpoint URL+HTTP AZ headers (below).\n\nSuccessful deployment also exposes a URL (on the console) that has the format:\n\nhttps://runtime.sagemaker.us-east-1.amazonaws.com/endpoints/ENDPOINT-NAME/invocations\n\nWhat is the purpose of this URL (shown on console)?\n\nIn my understanding this URL Cannot be invoked w/o appropriate headers as then there will be a need to have globally unique endpoint name!! THAT IS to invoke this URL it needs to have the \"HTTP Authorization headers\" (refer: https://docs.aws.amazon.com/AmazonS3/latest/API/sig-v4-authenticating-requests.html)\n\nI have a customer who is concerned that anyone can invoke the URL even from the internet. Tried to do it and received the <MissingTokenException> so I know it can't be done but just want to ensure I have the right explanation. (Test with HTTP/AZ headers pending)",
		"Answers": [
			{
				"Answer_creation_date": "2020-10-08T15:38:33.000Z",
				"Answer_upvote_count": 0,
				"Answer_body": "Your understanding is correct. From the docs:\n\nAmazon SageMaker strips all POST headers except those supported by the API. Amazon SageMaker might add additional headers. You should not rely on the behavior of headers outside those enumerated in the request syntax.\n\nCalls to InvokeEndpoint are authenticated by using AWS Signature Version 4.",
				"Answer_has_accepted": true
			}
		]
	},
	{
		"Questiont_title": "How to get batch transform with jsonl data?",
		"Question_creation_time": "2022-08-28T16:15:17.094Z",
		"Question_link": "https://repost.aws/questions/QUkP-cRiP3QiCAIqnwyirz1A/how-to-get-batch-transform-with-jsonl-data",
		"Question_topic": [
			"Machine Learning & AI",
			"Management & Governance"
		],
		"Question_tag": [
			"Amazon SageMaker",
			"Machine Learning & AI",
			"Amazon CloudWatch Logs"
		],
		"Question_upvote_count": 0,
		"Question_view_count": 34,
		"Question_answer_count": 1,
		"Question_has_accepted_answer": false,
		"Question_body": "I am using my own inference.py file as a entry point for inference. I have tested this pytorch model, served as a real time endpoint in amaon sagemaker. but when i try to create a batch job and use multiple json object in my input file (jsonl format) . i get the following error at the input_fn function on this line data = json.loads(request_body), in cloudwatch logs ==>\n\ndata = json.loads(request_body) raise JSONDecodeError(\"Extra data\", s, end) json.decoder.JSONDecodeError: Extra data : line 2 column 1 (Char ..)\n\nI am not sure why am i getting extra data on line 2 error, because this is supposed to be batch job with multiple json input and each line.\n\ninference.py\n\ndef model_fn(model_dir):\n   //load the model\n\n\n\n\ndef input_fn(request_body, request_content_type):\n    input_data= json.loads(request_body)\n    return data\n\ndef predict_fn(input_data, model):\n    return model.predict(input_data)\n\n\nset up batch job\n\nresponse = client.create_transform_job(\n    TransformJobName='some-job',\n    ModelName='mypytorchmodel',\n    ModelClientConfig={\n        'InvocationsTimeoutInSeconds': 3600,\n        'InvocationsMaxRetries': 1\n    },\n    BatchStrategy='MultiRecord',\n    TransformInput={\n        'DataSource': {\n            'S3DataSource': {\n                'S3DataType': 'S3Prefix',\n                'S3Uri': 's3://inputpath'\n            }\n        },\n        'ContentType': 'application/json',\n        'SplitType': 'Line'\n    },\n    TransformOutput={\n        'S3OutputPath': 's3://outputpath',\n        'Accept': 'application/json',\n        'AssembleWith': 'Line',\n    },\n    TransformResources={\n        'InstanceType': 'ml.g4dn.xlarge'\n        'InstanceCount': 1\n    }\n)\n\n\ninput file\n\n{\"input\" : \"some text here\"}\n{\"input\" : \"another\"}\n...",
		"Answers": [
			{
				"Answer_creation_date": "2022-08-30T16:00:00.274Z",
				"Answer_upvote_count": 1,
				"Answer_body": "You're seeing this because of your MultiRecord batch strategy: SageMaker is aware of how to split your source data files into individual records (because you configured SplitType), but is composing batches with multiple records and trying to send those through to your model/endpoint. It seems like your inference input handler is not capable of interpreting JSONLines chunks, only single JSON objects.\n\nOne way of fixing this would be to switch to SingleRecord batch strategy, which would result in each record triggering a separate inference request to your model.\n\nIf you're concerned about the HTTP overhead of request-per-record limiting your job performance, you could alternatively stick with MultiRecord but edit your input_fn to handle JSONLines data. I'd probably suggest to set a different ContentType to explicitly signal your container when to expect JSONLines vs single-record JSON. Your input_fn can detect that different request_content_type (e.g. application/x-jsonlines) and use a different parsing logic.\n\nI'm not 100% sure whether the request_body supports iterating through lines like a file would ([json.loads(l) for l in request_body]), whether you could treat it like a string ([json.loads(l) for l in request_body.split(\"\\n\")]), or perhaps it's a binary string you'd need to decode first e.g. request_body.decode(\"utf-8\").split(\"\\n\")... Would need to check - but something along these lines should allow you to first split your body by newlines, then parse each line as a valid JSON object.",
				"Answer_has_accepted": false
			}
		]
	},
	{
		"Questiont_title": "How to create (Serverless) SageMaker Endpoint using exiting tensorflow pb (frozen model) file?",
		"Question_creation_time": "2022-04-25T20:42:27.687Z",
		"Question_link": "https://repost.aws/questions/QUZ3v2_JXIRCSbBHVBDJUWgQ/how-to-create-serverless-sage-maker-endpoint-using-exiting-tensorflow-pb-frozen-model-file",
		"Question_topic": [
			"Serverless",
			"Machine Learning & AI"
		],
		"Question_tag": [
			"Serverless",
			"Amazon SageMaker",
			"Machine Learning & AI",
			"Amazon SageMaker Deployment"
		],
		"Question_upvote_count": 0,
		"Question_view_count": 74,
		"Question_answer_count": 1,
		"Question_has_accepted_answer": false,
		"Question_body": "Note: I am a senior developer, but am very new to the topic of machine learning.\n\nI have two frozen TensorFlow model weight files: weights_face_v1.0.0.pb and weights_plate_v1.0.0.pb. I also have some python code using Tensorflow 2, that loads the model and handles basic inference. The models detect respectively faces and license plates, and the surrounding code converts an input image to a numpy array, and applies blurring to the images in areas that had detections.\n\nI want to get a SageMaker endpoint so that I can run inference on the model. I initially tried using a regular Lambda function (container based), but that is too slow for our use case. A SageMaker endpoint should give us GPU inference, which should be much faster.\n\nI am struggling to find out how to do this. From what I can tell reading the documentation and watching some YouTube video's, I need to create my own docker container. As a start, I can use for example 763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-inference:2.8.0-gpu-py39-cu112-ubuntu20.04-sagemaker.\n\nHowever, I can't find any solid documentation on how I would implement my other code. How do I send an image to SageMaker? Who tells it to convert the image to numpy array? How does it know the tensor names? How do I install additional requirements? How can I use the detections to apply blurring on the image, and how can I return the result image?\n\nCan someone here please point me in the right direction? I searched a lot but can't find any example code or blogs that explain this process. Thank you in advance! Your help is much appreciated.",
		"Answers": [
			{
				"Answer_creation_date": "2022-04-26T13:06:11.302Z",
				"Answer_upvote_count": 0,
				"Answer_body": "You should package the model files into model.tar.gz file and use TensorFlowModel object to deploy the mode in SageMaker Endpoint.\n\nYou can see and example here.\n\nThis is and example for the same, but with PyTorch.",
				"Answer_has_accepted": false
			}
		]
	},
	{
		"Questiont_title": "XGBoost Reports Not Generated",
		"Question_creation_time": "2021-12-02T20:06:10.231Z",
		"Question_link": "https://repost.aws/questions/QUx_M71_2nQJSDp-I1mgbjDg/xg-boost-reports-not-generated",
		"Question_topic": [
			"Machine Learning & AI",
			"AWS Well-Architected Framework"
		],
		"Question_tag": [
			"Amazon SageMaker",
			"Machine Learning & AI",
			"Performance Efficiency"
		],
		"Question_upvote_count": 0,
		"Question_view_count": 169,
		"Question_answer_count": 3,
		"Question_has_accepted_answer": false,
		"Question_body": "Hi!\n\nI have been trying to create a model using XGBoost, and was able to successfully run/train the model. However, I have not been able to generate the training reports. I have included the rules parameter as follows: \"rules=[Rule.sagemaker(rule_configs.create_xgboost_report())]\".\n\nI am following this tutorial, but I am using objective: \"multi:softmax\" instead of the \"binary:logistic\" used in the example.\n\nWhen I run the model everything is fine but only the Profiler Report gets generated and I do not see the XGBoostReport under the rule-output folder. According to the tutorial it should be under the same file path.\n\nHere is my code for the model if it helps any:\n\ns3_output_location='s3://{}/{}/{}'.format(bucket, prefix, 'xgboost_model')\ncontainer = sagemaker.image_uris.retrieve(\"xgboost\", boto3.Session().region_name, \"latest\")\n\ntrain_input = TrainingInput(\n    \"s3://{}/{}/{}\".format(bucket, prefix, \"data/train.csv\"), content_type=\"csv\"\n)\nvalidation_input = TrainingInput(\n    \"s3://{}/{}/{}\".format(bucket, prefix, \"data/validation.csv\"), content_type=\"csv\"\n)\n\nrules=[\n    Rule.sagemaker(rule_configs.create_xgboost_report())\n]\n\nxgb = sagemaker.estimator.Estimator(\n    image_uri=container,\n    role=sagemaker.get_execution_role(),\n    instance_count=1,\n    instance_type=\"ml.c5.2xlarge\",\n    volume_size=5,\n    output_path=s3_output_location,\n    sagemaker_session=sagemaker.Session(),\n    rules=rules\n)\n\nxgb.set_hyperparameters(\n    max_depth=6,\n    objective='multi:softmax',\n    num_class=num_classes,\n    gamma=800,\n    num_round=250\n)\n\n\nAny help is appreciated! Thanks!",
		"Answers": [
			{
				"Answer_creation_date": "2021-12-03T14:44:54.672Z",
				"Answer_upvote_count": 0,
				"Answer_body": "I believe the issue here is the rules parameter is receiving a URL, not an array of RuleBase objects, as required by the Estimator documentation. Try re-writing your estimator accordingly, as suggested by the SageMaker Developer Guide:\n\nxgb = sagemaker.estimator.Estimator(\n   image_uri=container, \n   role=sagemaker.get_execution_role(), \n   instance_count=1, \n   instance_type=\"ml.c5.2xlarge\", \n   volume_size=5, \n   output_path=s3_output_location, \n   sagemaker_session=sagemaker.Session(), \n   rules=[\n      Rule.sagemaker(\n         rule_configs.create_xgboost_report()\n      )  \n   ]\n)\n\nFor more information on using Rules, check out the SageMaker Debugger documentation.",
				"Answer_has_accepted": false
			},
			{
				"Answer_creation_date": "2022-11-14T11:17:17.194Z",
				"Answer_upvote_count": 0,
				"Answer_body": "Hello jughead, Has your problem of your code been resolve? Let us know. Also, remember to click on the \"Accept\" button when an answer provided in the community helped you. This allows other community members to also benefit from it. Thank you for your participation.",
				"Answer_has_accepted": false
			},
			{
				"Answer_creation_date": "2022-02-13T22:12:23.421Z",
				"Answer_upvote_count": 0,
				"Answer_body": "Hi,\n\nI had the same problem. To be able to generate the xgboost report, make sure that you use xgboost version 1.2-1 in your image_uri and sklearn version 1.0-1 as your estimator_cls in FrameworkProcessor. Furthermore, set header to False in train, validation and test files in your preprocessing script so that you exclude them before you run estimator.fit(). This worked for me.",
				"Answer_has_accepted": false
			}
		]
	},
	{
		"Questiont_title": "Cant generate XGBoost training report in sagemaker, only profiler_report.",
		"Question_creation_time": "2022-07-07T19:35:31.986Z",
		"Question_link": "https://repost.aws/questions/QUskI-0YIvQ_2GRSkzyGiD2A/cant-generate-xg-boost-training-report-in-sagemaker-only-profiler-report",
		"Question_topic": [
			"Storage",
			"Machine Learning & AI"
		],
		"Question_tag": [
			"Amazon S3 Glacier",
			"Amazon SageMaker",
			"Machine Learning & AI",
			"Amazon SageMaker Model Training"
		],
		"Question_upvote_count": 0,
		"Question_view_count": 57,
		"Question_answer_count": 0,
		"Question_has_accepted_answer": false,
		"Question_body": "I am trying to generate the XGBoost training report to see feature importances however the following code only generates the profiler report.\n\nimport boto3, re, sys, math, json, os, sagemaker, urllib.request\nfrom sagemaker import get_execution_role\nimport numpy as np\nimport pandas as pd\nfrom sagemaker.predictor import csv_serializer\nfrom sagemaker.debugger import Rule, rule_configs\n\n# Define IAM role\nrules=[\n    Rule.sagemaker(rule_configs.create_xgboost_report())\n]\nrole = get_execution_role()\nprefix = 'sagemaker/models'\nmy_region = boto3.session.Session().region_name \n\n# this line automatically looks for the XGBoost image URI and builds an XGBoost container.\nxgboost_container = sagemaker.image_uris.retrieve(\"xgboost\", my_region, \"latest\")\n\n\n\nbucket_name = 'binary-base' \ns3 = boto3.resource('s3')\ntry:\n    if  my_region == 'us-east-1':\n      s3.create_bucket(Bucket=bucket_name)\n    else: \n      s3.create_bucket(Bucket=bucket_name, CreateBucketConfiguration={ 'LocationConstraint': my_region })\n    print('S3 bucket created successfully')\nexcept Exception as e:\n    print('S3 error: ',e)\n\nboto3.Session().resource('s3').Bucket(bucket_name).Object(os.path.join(prefix, 'train/train.csv')).upload_file('../Data/Base_Model_Data_No_Labels/train.csv')\nboto3.Session().resource('s3').Bucket(bucket_name).Object(os.path.join(prefix, 'validation/val.csv')).upload_file('../Data/Base_Model_Data_No_Labels/val.csv')\nboto3.Session().resource('s3').Bucket(bucket_name).Object(os.path.join(prefix, 'test/test.csv')).upload_file('../Data/Base_Model_Data/test.csv'\n\n\nsess = sagemaker.Session()\nxgb = sagemaker.estimator.Estimator(xgboost_container,\n                                    role, \n                                    volume_size =5,\n                                    instance_count=1, \n                                    instance_type='ml.m4.xlarge',\n                                    output_path='s3://{}/{}/output'.format(bucket_name, prefix, 'xgboost_model'),\n                                    sagemaker_session=sess, \n                                    rules=rules)\n\nxgb.set_hyperparameters(objective='binary:logistic',\n                        num_round=100, \n                        scale_pos_weight=8.5)\n\nxgb.fit({'train': s3_input_train, \"validation\": s3_input_val}, wait=True)\n\n\nWhen Checking the output path via:\n\nrule_output_path = xgb.output_path + \"/\" + xgb.latest_training_job.job_name + \"/rule-output\"\n! aws s3 ls {rule_output_path} --recursive\n\n\nWhich Outputs:\n\n2022-07-07 18:40:27     329715 sagemaker/models/output/xgboost-2022-07-07-18-35-55-436/rule-output/ProfilerReport-1657218955/profiler-output/profiler-report.html\n2022-07-07 18:40:26     171087 sagemaker/models/output/xgboost-2022-07-07-18-35-55-436/rule-output/ProfilerReport-1657218955/profiler-output/profiler-report.ipynb\n2022-07-07 18:40:23        191 sagemaker/models/output/xgboost-2022-07-07-18-35-55-436/rule-output/ProfilerReport-1657218955/profiler-output/profiler-reports/BatchSize.json\n2022-07-07 18:40:23        199 sagemaker/models/output/xgboost-2022-07-07-18-35-55-436/rule-output/ProfilerReport-1657218955/profiler-output/profiler-reports/CPUBottleneck.json\n2022-07-07 18:40:23        126 sagemaker/models/output/xgboost-2022-07-07-18-35-55-436/rule-output/ProfilerReport-1657218955/profiler-output/profiler-reports/Dataloader.json\n2022-07-07 18:40:23        127 sagemaker/models/output/xgboost-2022-07-07-18-35-55-436/rule-output/ProfilerReport-1657218955/profiler-output/profiler-reports/GPUMemoryIncrease.json\n2022-07-07 18:40:23        198 sagemaker/models/output/xgboost-2022-07-07-18-35-55-436/rule-output/ProfilerReport-1657218955/profiler-output/profiler-reports/IOBottleneck.json\n2022-07-07 18:40:23        119 sagemaker/models/output/xgboost-2022-07-07-18-35-55-436/rule-output/ProfilerReport-1657218955/profiler-output/profiler-reports/LoadBalancing.json\n2022-07-07 18:40:23        151 sagemaker/models/output/xgboost-2022-07-07-18-35-55-436/rule-output/ProfilerReport-1657218955/profiler-output/profiler-reports/LowGPUUtilization.json\n2022-07-07 18:40:23        179 sagemaker/models/output/xgboost-2022-07-07-18-35-55-436/rule-output/ProfilerReport-1657218955/profiler-output/profiler-reports/MaxInitializationTime.json\n2022-07-07 18:40:23        133 sagemaker/models/output/xgboost-2022-07-07-18-35-55-436/rule-output/ProfilerReport-1657218955/profiler-output/profiler-reports/OverallFrameworkMetrics.json\n2022-07-07 18:40:23        465 sagemaker/models/output/xgboost-2022-07-07-18-35-55-436/rule-output/ProfilerReport-1657218955/profiler-output/profiler-reports/OverallSystemUsage.json\n2022-07-07 18:40:23        156 sagemaker/models/output/xgboost-2022-07-07-18-35-55-436/rule-output/ProfilerReport-1657218955/profiler-output/profiler-reports/StepOutlier.json\n\n\nAs you can see only the profiler report in created which does not interest me. Why isn't there a CreateXGBoostReport folder generated with the training report? How do I generate this/what am I missing?",
		"Answers": []
	},
	{
		"Questiont_title": "Processing Job automatically created when I start a training job",
		"Question_creation_time": "2021-01-25T10:25:09.000Z",
		"Question_link": "https://repost.aws/questions/QUmy7drPKBRH6hs3Pw_yj3tw/processing-job-automatically-created-when-i-start-a-training-job",
		"Question_topic": [
			"Machine Learning & AI"
		],
		"Question_tag": [
			"Amazon SageMaker"
		],
		"Question_upvote_count": 0,
		"Question_view_count": 38,
		"Question_answer_count": 1,
		"Question_has_accepted_answer": false,
		"Question_body": "Hi,\nI haven't used sagemaker for a while and today I started a training job (with the same old settings I always used before), but this time I noticed that a processing job has been automatically created and it's running while my training job runs (I don't even know what a processing job is). I also checked in the dashboard to be sure, this was not happening before, it's the second time (first time was in December) but I've been using sagemaker for the last two years..\nIs this a wanted behaviour? I didn't find anything related in the documentation, but it's important to know because I don't want extra costs..\nThis is the image used by the processing job, with a instance type of ml.m5.2xlarge which I didn't set anywhere..\n\n929884845733.dkr.ecr.eu-west-1.amazonaws.com/sagemaker-debugger-rules:latest  \n\n\nAnd this is how I launch my training job (the entrypoint script is basically Keras code for a MobileNetV3)\n\nimport sagemaker\nfrom sagemaker.tensorflow import TensorFlow\nfrom sagemaker import get_execution_role\n\nbucket = 'mybucket'\n\ntrain_data = 's3://{}/{}'.format(bucket,'train')\n\nvalidation_data = 's3://{}/{}'.format(bucket,'test')\n\ns3_output_location = 's3://{}'.format(bucket)\n\nhyperparameters = {'epochs': 130, 'batch-size' : 512, 'learning-rate' : 0.0002}\n\nmetrics = .. some regex here\n\ntf_estimator = TensorFlow(entry_point='train.py',\nrole=get_execution_role(),\ntrain_instance_count=1,\ntrain_instance_type='ml.p2.xlarge',\ntrain_max_run=172800,\noutput_path=s3_output_location,\nframework_version='2.3.0',\npy_version='py37',\nmetric_definitions = metrics,\nhyperparameters = hyperparameters,\nsource_dir=\"data\")\n\ninputs = {'train': train_data, 'test': validation_data}\nmyJobName = 'myname'\ntf_estimator.fit(inputs=inputs, job_name=myJobName)\n\nEdited by: rokk07 on Jan 25, 2021 2:55 AM",
		"Answers": [
			{
				"Answer_creation_date": "2021-01-25T11:14:01.000Z",
				"Answer_upvote_count": 0,
				"Answer_body": "I can answer myself. It's described https://docs.aws.amazon.com/sagemaker/latest/dg/use-debugger-built-in-rules.html , must be a recent feature. The documentation explain also how to disable the debugger.",
				"Answer_has_accepted": false
			}
		]
	},
	{
		"Questiont_title": "Ground Truth - Label Consolidation, NN Models, and Accuracy Levels",
		"Question_creation_time": "2019-04-08T13:53:57.000Z",
		"Question_link": "https://repost.aws/questions/QUVR_6Xq0sQKaRrK65qA2VOg/ground-truth-label-consolidation-nn-models-and-accuracy-levels",
		"Question_topic": [
			"Machine Learning & AI"
		],
		"Question_tag": [
			"Amazon SageMaker"
		],
		"Question_upvote_count": 0,
		"Question_view_count": 36,
		"Question_answer_count": 2,
		"Question_has_accepted_answer": false,
		"Question_body": "Hello,\n\n(1)\nIn https://docs.aws.amazon.com/sagemaker/latest/dg/sms-annotation-consolidation.html, it says: \"Multi-class annotation consolidation for image and text classification uses a variant of the Expectation Maximization approach to annotations. It estimates parameters for each worker and uses Bayesian inference to estimate the true class based on the class annotations from individual workers.\" Is there anywhere I can get more information and detail on this consolidation process? For now, I am trying to understand what this looks like when I have only 2 labelers per object, but I would also want to get a deeper understanding of how this works in general.\n\n(2)\nIs there anywhere I can get more information on the neural networks used in the automatic labeling component of Ground Truth? Is there a way to access and customize them? Also, is there a way to export/pickle the models and use them outside of Ground Truth?\n\n(3)\nLastly, in https://docs.aws.amazon.com/sagemaker/latest/dg/sms-automated-labeling.html, it says: \"The potential benefit of automated data labeling also depends on the accuracy that you require. Higher accuracy levels generally reduce the number of data objects that are automatically labeled.\" Where is this accuracy level set?\n\nThank you!",
		"Answers": [
			{
				"Answer_creation_date": "2019-04-10T12:58:31.000Z",
				"Answer_upvote_count": 0,
				"Answer_body": "Hi naquent,\n\nI'm an engineer at AWS, and I wanted to offer a response to your three questions.\n\nWe have a blog post that describes the active learning process and annotation consolidation methodology in a little bit more detail, which might be helpful in understanding the approach - https://aws.amazon.com/blogs/machine-learning/annotate-data-for-less-with-amazon-sagemaker-ground-truth-and-automated-data-labeling/. The exact nature of the annotation consolidation algorithm used in the service is proprietary. However, there is a large body of work that describes this type of method that might give you a better sense for what is involved [1].\n\nI should add that we typically recommend at least three annotators for all scenarios for exactly the reason indicated in the question. That is, when you only have two annotators, it is difficult to determine the preference that should be given to any individual worker in the presence of disagreements.\n\nThe neural networks used by Ground Truth are the same as those available elsewhere on the SageMaker platform. When you run an image classification labeling job, you are leveraging a SageMaker image classification model. There are many writeups of these algorithms in various locations, but a useful starting point would be https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html.\nYou can definitely access and customize these models! As I mentioned above, when you run Ground Truth labeling jobs, machine learning models will be trained over the course of the labeling job. You will find these in your SageMaker console, e.g., the \"Training Jobs\" console will show you the training jobs that have been run, and the \"Models\" console will list the model artifacts that have been created. You can use these to run inference, or retrain / fine tune them as desired depending on your circumstances. Please see one of our sample notebooks for a demonstration of how you can use this model for further training, inference, and hosting - https://github.com/awslabs/amazon-sagemaker-examples/blob/master/ground_truth_labeling_jobs/from_unlabeled_data_to_deployed_machine_learning_model_ground_truth_demo_image_classification/from_unlabeled_data_to_deployed_machine_learning_model_ground_truth_demo_image_classification.ipynb.\n\nThis accuracy level is set by the service. You can always assess the labeled data after a labeling job in the console. Every label provided by Ground Truth is associated with a \"confidence score\" which can be helpful in filtering the output of a labeling job. The sample notebook linked above illustrates a couple of workflows in this regard.\n\nThanks for being a valued AWS customer. Please don't hesitate to reach back out to us.\n\n, e.g., A. P. Dawid and A. M. Skene. 1979. Maximum Likelihood Estimation of Observer Error-Rates Using the EM Algorithm. J. Royal Stat. Soc. Series C 28, 1 (1979), 20\u201328. http://www.jstor.org/stable/2346806",
				"Answer_has_accepted": false
			},
			{
				"Answer_creation_date": "2019-04-10T00:10:54.000Z",
				"Answer_upvote_count": 0,
				"Answer_body": "Thank you, Jonathan! These resources look to be very helpful. I will look through them and follow up if I have any further questions.",
				"Answer_has_accepted": false
			}
		]
	},
	{
		"Questiont_title": "Unable to compile model to Neuron: no error message, no output",
		"Question_creation_time": "2022-09-14T14:43:57.969Z",
		"Question_link": "https://repost.aws/questions/QUA_oVwSPQReCt96QyX4cz-g/unable-to-compile-model-to-neuron-no-error-message-no-output",
		"Question_topic": [
			"Machine Learning & AI",
			"Compute"
		],
		"Question_tag": [
			"Amazon SageMaker",
			"Machine Learning & AI",
			"AWS Neuron"
		],
		"Question_upvote_count": 0,
		"Question_view_count": 71,
		"Question_answer_count": 2,
		"Question_has_accepted_answer": true,
		"Question_body": "Hi. We are trying to convert all our in-house pytorch models to aws-neuron on inferentia. We successfully converted one, but the second model we tried did not compile. Unfortunately, compilation did not generate any error message nor log of any kind, so we are stuck. The model is rather simple, but large, U-Net, with partial convolutions instead of regular ones, but otherwise no fancy operators. Conversion of this model to torchscript is ok on the same instance. Could it be a memory problem ?",
		"Answers": [
			{
				"Answer_creation_date": "2022-09-15T09:25:42.678Z",
				"Answer_upvote_count": 2,
				"Answer_body": "Hi, in order to see more information about the error, you can enable debugging during tracing by passing 'verbose' to the tracing command like this:\n\nimport torch\nimport torch.neuron\ntorch.neuron.trace(\n    model,\n    example_inputs=inp,\n    verbose=\"debug\",\n    compiler_workdir=\"logs\" # dir where debugging logs will be saved\n)\n\n\nYou'll see the error messages in the console and they will also be saved to the \"logs\" dir.\n\nIt is always good to run the NeuronSDK analyzer first to make sure the model is: 1/ torch.jit traceable; 2/ supported by the compiler\n\nimport torch\nimport torch.neuron\ntorch.neuron.analyze_model(model, example_inputs=inp)\n\n\nYou can also see a sample that shows how to compile an U-net Pytorch (3rd party implementation) to Inf1 instances here: https://github.com/samir-souza/laboratory/blob/master/05_Inferentia/03_UnetPytorch/03_UnetPytorch.ipynb\n\nRef: https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/neuron-frameworks/pytorch-neuron/api-compilation-python-api.html\n\nIf everything fails, try to look for something like this in the logs:\n\nINFO:Neuron:Compile command returned: -11\nWARNING:Neuron:torch.neuron.trace failed on _NeuronGraph$647; falling back to native python function call\nERROR:Neuron:neuron-cc failed with the following command line call:\n\n\nAnd paste here, please. With the \"Compile command returned:\" code it is possible to identify the error. You are suspecting that there is some issue related to memory, maybe Out of Memory. Normally when that is the case, you'll find the code: -9 in this part of the error.",
				"Answer_has_accepted": true
			},
			{
				"Answer_creation_date": "2022-09-21T07:33:00.671Z",
				"Answer_upvote_count": 0,
				"Answer_body": "Following your answer we were able to check the log and got\n\nINFO:Neuron:Compile command returned: -9\n\nwhich is apparently an out of memory error. Switching to a 6x instance solved the problem",
				"Answer_has_accepted": false
			}
		]
	},
	{
		"Questiont_title": "How to search for Amazon SageMaker Models using Tags?",
		"Question_creation_time": "2022-07-04T17:40:45.714Z",
		"Question_link": "https://repost.aws/questions/QUfe51Pe1zRKqts-P7SZVuSA/how-to-search-for-amazon-sage-maker-models-using-tags",
		"Question_topic": [
			"Developer Tools",
			"Machine Learning & AI"
		],
		"Question_tag": [
			"AWS CodePipeline",
			"Build & Train ML Models",
			"Amazon SageMaker",
			"Machine Learning & AI",
			"Amazon SageMaker Model Building"
		],
		"Question_upvote_count": 0,
		"Question_view_count": 65,
		"Question_answer_count": 2,
		"Question_has_accepted_answer": false,
		"Question_body": "When I open the Model details in the Amazon SageMaker console, the details clearly show Tags that have been added to the model during it's creation.\n\n| Tags | | Key | | sagemaker:project-name | | aws:cloudformation:stack-name| | sagemaker:deployment-stage| | sagemaker:deployment-stage|\n\nBut the Search API provided by Amazon SageMaker, https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_Search.html\n\nmentions only the following resources can be searched for using Tags: Valid Values: TrainingJob | Experiment | ExperimentTrial | ExperimentTrialComponent | Endpoint | ModelPackage | ModelPackageGroup | Pipeline | PipelineExecution | FeatureGroup | Project | FeatureMetadata\n\nI wish to obtain Model details, not the ModelPackageGroup/ModelPackage details, using Tags so if there is a way to do that, please share. Also if there is no way to obtain it using Tags, like the Search API Documentation suggests, what is the purpose of the Tags still present in the Model details?",
		"Answers": [
			{
				"Answer_creation_date": "2022-07-17T18:27:41.176Z",
				"Answer_upvote_count": 0,
				"Answer_body": "DescribeModel will provide information about model details, the tags in the model was created as a part of the CloudFormation deployment. It identify each resources by CF.\n\nTo search by tags, you could add custom tags for your purpose.",
				"Answer_has_accepted": false
			},
			{
				"Answer_creation_date": "2022-07-04T18:23:35.438Z",
				"Answer_upvote_count": 0,
				"Answer_body": "To use custom tag, you will need to search by GetResources from AWS Resource Groups: https://docs.aws.amazon.com/ARG/latest/userguide/find-resources-to-tag.html",
				"Answer_has_accepted": false
			}
		]
	},
	{
		"Questiont_title": "What classifier is used in SageMakers BlazingText algorithm?",
		"Question_creation_time": "2022-10-27T14:03:13.628Z",
		"Question_link": "https://repost.aws/questions/QUhUSE7oIGTt2ZUt8bQS-SdQ/what-classifier-is-used-in-sage-makers-blazing-text-algorithm",
		"Question_topic": [
			"Machine Learning & AI"
		],
		"Question_tag": [
			"Text processing & Analytics",
			"Amazon SageMaker",
			"Machine Learning & AI"
		],
		"Question_upvote_count": 0,
		"Question_view_count": 16,
		"Question_answer_count": 1,
		"Question_has_accepted_answer": false,
		"Question_body": "I would like to use the BlazingText algorithm in SageMaker for text classification: As I understand it we first represent the text with a word2vec algorithm to get the word embeddings, and then use these embeddings as input to a classifier. But is it known what classifier is being used and is it possible to modify it?\n\nThanks!",
		"Answers": [
			{
				"Answer_creation_date": "2022-10-28T08:31:03.340Z",
				"Answer_upvote_count": 0,
				"Answer_body": "The BlazingText algorithm uses fasttext classifiers under the file model.bin and thus I believe by modifying/replacing that file with your own fasttext classifier you should be able to modify the classifier used by BlazingText.\n\nRegards NN",
				"Answer_has_accepted": false
			}
		]
	},
	{
		"Questiont_title": "What is the difference between SageMaker Pipelines and SageMaker Step Function SDK?",
		"Question_creation_time": "2020-12-03T11:15:00.000Z",
		"Question_link": "https://repost.aws/questions/QU2iheeTzhSTmWw4aqVEeqOQ/what-is-the-difference-between-sage-maker-pipelines-and-sage-maker-step-function-sdk",
		"Question_topic": [
			"Serverless",
			"Application Integration",
			"Machine Learning & AI"
		],
		"Question_tag": [
			"AWS Step Functions",
			"Amazon SageMaker"
		],
		"Question_upvote_count": 0,
		"Question_view_count": 673,
		"Question_answer_count": 2,
		"Question_has_accepted_answer": true,
		"Question_body": "What is the difference between SageMaker Pipelines and SageMaker Step Function SDK?\n\nThe official Pipelines notebook is basically only doing a workflow - pretty much a copy cat of what step functions has been doing for years. In the nice video from Julien Simon I see CICD capacities mentioned, where are those? any demos?",
		"Answers": [
			{
				"Answer_creation_date": "2020-12-03T17:04:25.000Z",
				"Answer_upvote_count": 0,
				"Answer_body": "Hey, that demo is missing the project part of Pipelines and therefore the SM provided project templates. Go to SM studio and on the Studio summary hit edit settings and then enable access and provisioning of Service Catalog Portfolio of products in SM Studio. Then check your service catalog portfolios. Haven't tried it out yet though.",
				"Answer_has_accepted": true
			},
			{
				"Answer_creation_date": "2022-08-29T14:23:35.610Z",
				"Answer_upvote_count": 0,
				"Answer_body": "both StepFunctions and SageMaker pipeline can be used to build a pipeline for ML. In the end, we need a SageMaker DAG StepFunction can integrate with different services, and good when one already used it. SageMaker pipeline is natively integrated with SageMaker system.\n\nThis is my example using both to build a ML Pipeline GitHub",
				"Answer_has_accepted": false
			}
		]
	},
	{
		"Questiont_title": "AWS Sagemaker model endpoint data capture",
		"Question_creation_time": "2022-11-10T18:30:22.579Z",
		"Question_link": "https://repost.aws/questions/QUJfA77iAoT2-EkVff7sc7LA/aws-sagemaker-model-endpoint-data-capture",
		"Question_topic": [
			"Machine Learning & AI"
		],
		"Question_tag": [
			"Amazon SageMaker"
		],
		"Question_upvote_count": 0,
		"Question_view_count": 35,
		"Question_answer_count": 1,
		"Question_has_accepted_answer": false,
		"Question_body": "I have deployed a sagemaker endpoint, with data capture enabled with a 100% sampling rate. But after running it multiple times, it is not storing all the inputs to the model,\n\nFor instance, after running 3 times, it saves only one time.",
		"Answers": [
			{
				"Answer_creation_date": "2022-11-14T12:08:34.106Z",
				"Answer_upvote_count": 0,
				"Answer_body": "CLOSED,\n\nSolved something else was going wrong",
				"Answer_has_accepted": false
			}
		]
	},
	{
		"Questiont_title": "Experiment tracking with Sagemaker Pipelines",
		"Question_creation_time": "2022-11-02T09:00:30.510Z",
		"Question_link": "https://repost.aws/questions/QUttCrJyfVQYyAE-AO0vg-EA/experiment-tracking-with-sagemaker-pipelines",
		"Question_topic": [
			"Machine Learning & AI"
		],
		"Question_tag": [
			"Amazon SageMaker",
			"Amazon SageMaker Pipelines"
		],
		"Question_upvote_count": 0,
		"Question_view_count": 21,
		"Question_answer_count": 1,
		"Question_has_accepted_answer": false,
		"Question_body": "Is it possible to track only TrainingSteps in a Sagemaker Pipeline that contains multiple Processing & other steps? I don't really see big benefit of creating Trial Components for Processing Jobs or Model Repacking jobs into the experiments as they just overflow the UI.\n\nBasically could the pipeline_experiment_config parameter be used for defining which steps of the Pipeline should be tracked or should I disable automatic experiment creation and just try to create a manual experiment tracker during the Training Job.",
		"Answers": [
			{
				"Answer_creation_date": "2022-11-02T13:40:54.790Z",
				"Answer_upvote_count": 0,
				"Answer_body": "Sure, if you don't want an experiment and trial created for the pipeline, you could easily achieve this by setting the pipeline_experiment_config to None.",
				"Answer_has_accepted": false
			}
		]
	},
	{
		"Questiont_title": "Unauthorized AWS account racked up charges on stolen credit card.",
		"Question_creation_time": "2022-04-09T17:16:34.036Z",
		"Question_link": "https://repost.aws/questions/QUhV-lkkYyS1qaYFvsoPYiWg/unauthorized-aws-account-racked-up-charges-on-stolen-credit-card",
		"Question_topic": [
			"Compute",
			"Machine Learning & AI",
			"AWS Well-Architected Framework",
			"Security, Identity, & Compliance"
		],
		"Question_tag": [
			"Amazon Lightsail",
			"Amazon SageMaker",
			"Security",
			"Support Case",
			"Shared Responsibility Model"
		],
		"Question_upvote_count": 0,
		"Question_view_count": 280,
		"Question_answer_count": 1,
		"Question_has_accepted_answer": false,
		"Question_body": "My mother was automatically signed up for an AWS account or someone used her credentials to sign up. She did not know that she had been signed up, and it sat unused for 3 years. Last month, she got an email from AWS for \"unusual activity\" and she asked me to help her look into it. Someone racked up $800+ in charges in 10 days for AWS services she has never heard of, let alone used (SageMaker, LightSail were among the services). The card on the AWS account is a credit card that was stolen years ago and has since been cancelled. So when AWS tried to charge the card, it didn't go through.\n\nMy experience with AWS customer service has been unhelpful so far. Mom changed her AWS password in time so we could get into the account and contact support. I deleted the instances so that the services incurring charges are now stopped. But now AWS is telling me to put in a \"valid payment method\" or else they will not review the fraudulent bill. They also said that I have to set up additional AWS services (Cost Management, Amazon Cloud Watch, Cloud Trail, WAF, security services) before they'll review the bill. I have clearly explained to them that this entire account is unauthorized and we want to close it ASAP, so adding further services and a payment method doesn't make sense.\n\nWhy am I being told to use more AWS services when my goal is to use zero? Why do I have to set up \"preventative services\" when the issue I'm trying to resolve is a PAST issue of fraud? They also asked me to write back and confirm that we have read and understood the AWS Customer Agreement and shared responsibility model.\" Of course we haven't, because we didn't even know the account existed!\n\nAny advice or input into this situation? It's extremely frustrating to be told that AWS won't even look into the issue unless I set up these additional AWS services and give them a payment method. This is a clear case of identity fraud. We want this account shut down.\n\nSupport Case # is xxxxxxxxxx.\n\nEdit- removed case ID -Ann D",
		"Answers": [
			{
				"Answer_creation_date": "2022-04-10T21:14:09.639Z",
				"Answer_upvote_count": 0,
				"Answer_body": "Hello,\n\nFirst, I want to apologize for the wait and the frustration this situation has caused.\n\nWhile we can\u2019t discuss account or case details, via this platform, I can assure you that the issue is currently under investigation. Our team takes these matters seriously and will update you with any action needed/taken.\n\nPlease continue to address any further concerns or questions, with our teams, via your case.\n\nThank you,\n\nRandi S.",
				"Answer_has_accepted": false
			}
		]
	},
	{
		"Questiont_title": "Processing environmental data with netCDF files",
		"Question_creation_time": "2022-02-02T10:09:16.447Z",
		"Question_link": "https://repost.aws/questions/QU9sduyLViQxepK_RcYuhPwQ/processing-environmental-data-with-net-cdf-files",
		"Question_topic": [
			"Storage",
			"Machine Learning & AI",
			"Analytics"
		],
		"Question_tag": [
			"Amazon Simple Storage Service",
			"Amazon SageMaker",
			"AWS Glue"
		],
		"Question_upvote_count": 0,
		"Question_view_count": 42,
		"Question_answer_count": 0,
		"Question_has_accepted_answer": false,
		"Question_body": "I'm looking for some experience, reference architecture, best practices about the processing of environmental data stored in netCDF files.",
		"Answers": []
	},
	{
		"Questiont_title": "I am not able to find this solution in sagemaker jumpstart",
		"Question_creation_time": "2022-01-19T03:24:15.390Z",
		"Question_link": "https://repost.aws/questions/QULbPt5pRtT1q8sV-CK5wPhA/i-am-not-able-to-find-this-solution-in-sagemaker-jumpstart",
		"Question_topic": [
			"Machine Learning & AI"
		],
		"Question_tag": [
			"Build & Train ML Models",
			"Amazon SageMaker",
			"Machine Learning & AI",
			"Amazon SageMaker Ground Truth"
		],
		"Question_upvote_count": 0,
		"Question_view_count": 111,
		"Question_answer_count": 1,
		"Question_has_accepted_answer": false,
		"Question_body": "Link for the post: https://aws.amazon.com/blogs/machine-learning/build-custom-amazon-sagemaker-pytorch-models-for-real-time-handwriting-text-recognition/ I am unable to find this in sagemaker jumpstart. Please guide me through this.",
		"Answers": [
			{
				"Answer_creation_date": "2022-03-15T10:51:30.251Z",
				"Answer_upvote_count": 0,
				"Answer_body": "I can confirm that the solution is available in JumpStart. Can I ask what steps you have undertaken to find the solution?",
				"Answer_has_accepted": false
			}
		]
	},
	{
		"Questiont_title": "Visualizing custom Model Monitor metrics in Sagemaker Studio",
		"Question_creation_time": "2022-04-15T16:57:24.554Z",
		"Question_link": "https://repost.aws/questions/QUjbz0DSjCSoujr8vBAJvOOA/visualizing-custom-model-monitor-metrics-in-sagemaker-studio",
		"Question_topic": [
			"Machine Learning & AI",
			"Management & Governance",
			"Containers"
		],
		"Question_tag": [
			"Amazon SageMaker",
			"Machine Learning & AI",
			"Amazon CloudWatch",
			"Containers",
			"Monitoring"
		],
		"Question_upvote_count": 1,
		"Question_view_count": 300,
		"Question_answer_count": 2,
		"Question_has_accepted_answer": false,
		"Question_body": "We are developing a custom model monitoring container to be used to interact with Sagemaker's model monitoring API, as we require additional custom metrics. One of our requirements is for these metrics to populate and visualize appropriately in Studio visualization tabs, e.g. Data Quality, Model Quality, and Explainability. None of the built-in containers are an option for us, and we are trying to interop with the Monitoring APIs as seamlessly as possible.\n\nThe docs specify the container contracts for output, specifically statistics.json, constraints.json, and constraint_violations.json. However, the docs are not clear on what JSON files to emit for specifically custom Model Quality metrics. There does not appear to be a provided schema.\n\nThings I have tried:\n\nEmitting CloudWatch metrics to a different namespace, sagemaker/Endpoint/model-quality-metrics\nAttempting to retrofit the statistics.json file to also include Model Quality metrics.\n\nIs there a separate JSON file I should write to /opt/ml/processing/resultdata for our custom model quality metrics to populate the Studio visualization tab?\n\nSpecifically, where do the base containers look for this JSON report: https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-model-quality-metrics.html",
		"Answers": [
			{
				"Answer_creation_date": "2022-04-22T15:03:22.611Z",
				"Answer_upvote_count": 1,
				"Answer_body": "Hi Michael,\n\nIf I understand your question correctly, you are trying to build a BYOC Model Monitor container in order to bring in custom metrics to monitor your model. Firstly, when we take that route, the metrics that are monitored by default, lets say for Model Quality, metrics such as mae, r1 for instance does not come out of the box, you have to incorporate logic to calculate the respective metrics listed here. In short, you cannot \"add-on\" a new custom metric to the existing metric OOTB until the SageMaker team exposes the container which does this logic publicly.\n\nSecondly, there is no \"strict\" formatting(json files) to writing your custom metrics per say, it is at the discretion of the customer to implement the the json file as required as long as they can read that in the BYOC container to calculate violations and so on. However, we encourage customers to follow the KLL fashion as show in the example here.\n\nLinking a few samples of BYOC implementations of model monitor for your reference below -\n\nNLP data drift BYOC model monitor - https://github.com/aws-samples/detecting-data-drift-in-nlp-using-amazon-sagemaker-custom-model-monitor\n\nCV BYOC model monitor - https://aws.amazon.com/blogs/machine-learning/detecting-and-analyzing-incorrect-model-predictions-with-amazon-sagemaker-model-monitor-and-debugger/",
				"Answer_has_accepted": false
			},
			{
				"Answer_creation_date": "2022-04-15T20:28:11.741Z",
				"Answer_upvote_count": 0,
				"Answer_body": "Apologies for the delayed response Michael, to answer your question s\n\n1/ on where and what name the metrics need to be stored - The constraints and violations files need to be written to /opt/ml/processing https://github.com/aws-samples/detecting-data-drift-in-nlp-using-amazon-sagemaker-custom-model-monitor/blob/main/docker/evaluation.py#L156 and the filename is constraint_violations.json\n\n2/ I have ran a default ModelQualityMonitor job. I see that this job also produces the 3-set of (constraints.json, constraint_violations.json, statistics.json). How are these differentiated between monitoring job types?\n\nAnswer: Even though the files generated by each job is the same no matter the type of Model monitoring you choose(Data/Quality), if you closely observe, the model data monitoring for Statistics file will have -\n\n\"mean\" : 0.13082980736646624, \"sum\" : 54.94851909391582, \"std_dev\" : 0.2511377559440087, \"min\" : 0.006144209299236536, \"max\" : 0.989563524723053, \"distribution\" : {\n\nwhereas, statistics for Model Quality will have ( for Binary classification problem)\n\n\"binary_classification_metrics\" : { \"confusion_matrix\" : { \"0\" : { \"0\" : 173, \"1\" : 0 }, \"1\" : { \"0\" : 12, \"1\" : 16 }\n\nTo sum it up, yes if you provide these 3 files i.e statistics.json, constraints.json and constraint_violations.json you should be all set.",
				"Answer_has_accepted": false
			}
		]
	},
	{
		"Questiont_title": "How to get logs or print statements from SageMaker PyTorch deployed endpoint?",
		"Question_creation_time": "2022-08-26T21:34:00.227Z",
		"Question_link": "https://repost.aws/questions/QU74MThjkyRVCtySw-DEozrQ/how-to-get-logs-or-print-statements-from-sage-maker-py-torch-deployed-endpoint",
		"Question_topic": [
			"Machine Learning & AI",
			"Developer Tools",
			"Management & Governance"
		],
		"Question_tag": [
			"Amazon SageMaker",
			"Machine Learning & AI",
			"Monitoring & Logging",
			"Amazon CloudWatch Logs",
			"Amazon SageMaker Deployment"
		],
		"Question_upvote_count": 0,
		"Question_view_count": 48,
		"Question_answer_count": 2,
		"Question_has_accepted_answer": false,
		"Question_body": "I've deployed an extended Pytorch model as an endpoint and I'm trying to make inference requests to it. Problem is, the responses from the endpoint get timed out and CloudWatch logs show nothing beyond:\n\ntimestamp\tmessage\n1661544743589\tWARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\n1661544749569\tModel server started.\n\nNow in my inference.py file, which I provided as the entry point I've set logging as follows:\n\nimport logging\nimport sys\n\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.DEBUG)\nlogger.addHandler(logging.StreamHandler(sys.stdout))\nlogger.info(\"Loading file.\")\nprint(\"Loading file.\")\n\n\nI wish to see those logs/prints. How can I accomplish that?",
		"Answers": [
			{
				"Answer_creation_date": "2022-08-30T16:47:09.082Z",
				"Answer_upvote_count": 0,
				"Answer_body": "I've come across similar issues in the past of log messages not making it through to CloudWatch, and can suggest setting environment variable PYTHONUNBUFFERED=1 (discussed further here on StackOverflow wrt containerized Python in general).\n\nThe procedure for this may vary a little depending how you're creating your model, endpoint config & endpoint (e.g. direct boto3/API calls, SageMaker SDK Estimator.deploy() or PyTorchModel). PyTorchModel should accept an env={\"PYTHONUNBUFFERED\": \"1\"} constructor argument for example.\n\nIf you are using the SageMaker Python SDK, do watch out that some methods (especially shortcuts like Estimator.deploy()) may re-use existing models & endpoint configs rather than re-creating each time they're run. Check you see the environment variable set in the SageMaker > Inference > Models > {Your Model Name} details page in AWS Console, and run DeleteModel first if needed to force an update!",
				"Answer_has_accepted": false
			},
			{
				"Answer_creation_date": "2022-08-30T16:30:29.746Z",
				"Answer_upvote_count": 0,
				"Answer_body": "Hello,\n\nPlease note that you can view the logs under CloudWatch logs. There would be a generated log stream under your pytorch-inference tab when it is inService (Navigate to inference tab under SageMaker console > Endpoints > click on the endpoint name > locate \"view logs\"). The previous steps will take you to CloudWatch logs console. You then click on log groups and locate /aws/sagemaker/Endpoints/pytorch-inference-YYYY-MM-DD-HH-MM-SS-sss > AllTraffic/i-instanceId. For example, for the code snippet you shared, If you add the lines of code below for logging purpose within the inference.py script):\n\nimport logging\nimport sys\n\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.DEBUG)\nlogger.addHandler(logging.StreamHandler(sys.stdout))\nlogger.info(\"Loading file.\")\nprint(\"Loading file. --> from print statement\")\n\n# rest of the inference script from here\n\n\nThe above will show up under AllTraffic/instance-id (once the endpoint is inService) as the following:\n\n2022-08-30 15:48:42,183 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Loading file.\n2022-08-30 15:48:42,936 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Loading file. --> from print statement\n\n\nFrom above, you can see that print statements will show up as INFO level log.\n\nPlease see the link [1] for more details on how the log stream is captured for inference jobs. If you have logging statements within your input_fn or say within your predict_fn function, those statements will show up when there is a prediction/scoring made. I hope the shared information is helpful.\n\nReference: [1] https://docs.aws.amazon.com/sagemaker/latest/dg/inference-pipeline-logs-metrics.html#inference-pipeline-logs\n\nplease see @Alex_T answer for PYTHONUNBUFFERED=1 which forces print/logging to stdout. If you are bringing your own container you can define this parameter in the Dockerfile.",
				"Answer_has_accepted": false
			}
		]
	},
	{
		"Questiont_title": "SageMaker endpoint creation fails for Multi Model",
		"Question_creation_time": "2022-04-06T14:54:35.061Z",
		"Question_link": "https://repost.aws/questions/QU2bUNsPi3Rgautb0ZycrziA/sage-maker-endpoint-creation-fails-for-multi-model",
		"Question_topic": [
			"Machine Learning & AI"
		],
		"Question_tag": [
			"Amazon SageMaker"
		],
		"Question_upvote_count": 0,
		"Question_view_count": 119,
		"Question_answer_count": 1,
		"Question_has_accepted_answer": false,
		"Question_body": "When using scikit to create multi model, it throws an exception, but when in single model it works.\n\nComplains about model_fn implementation or ping issues, any tips on how to fix this?\n\ne.g container={\\n\", \" 'Image' : image_uri,\", \" 'Mode': 'MultiModel',\", \" 'ModelDataUrl': 's3://somepatch/with/all/models/,\", \" 'Environment': {'SAGEMAKER_SUBMIT_DIRECTORY': mme_artifacts_path,\", \" 'SAGEMAKER_PROGRAM': 'inference.py'} \"\n\nFile \"/miniconda3/bin/serve\", line 8, in <module> sys.exit(serving_entrypoint()) File \"/miniconda3/lib/python3.7/site-packages/sagemaker_sklearn_container/serving.py\", line 144, in serving_entrypoint start_model_server() File \"/miniconda3/lib/python3.7/site-packages/sagemaker_sklearn_container/serving_mms.py\", line 124, in start_model_server modules.import_module(serving_env.module_dir, serving_env.module_name) File \"/miniconda3/lib/python3.7/site-packages/sagemaker_containers/_modules.py\", line 263, in import_module six.reraise(_errors.ImportModuleError, _errors.ImportModuleError(e), sys.exc_info()[2]) File \"/miniconda3/lib/python3.7/site-packages/six.py\", line 702, in reraise raise value.with_traceback(tb) File \"/miniconda3/lib/python3.7/site-packages/sagemaker_containers/_modules.py\", line 258, in import_module module = importlib.import_module(name) File \"/miniconda3/lib/python3.7/importlib/init.py\", line 118, in import_module if name.startswith('.'):",
		"Answers": [
			{
				"Answer_creation_date": "2022-04-08T09:24:37.439Z",
				"Answer_upvote_count": 0,
				"Answer_body": "Hi,\n\nWe would need a bit more information to help you, including a bit more detail on how you create the model (code wise) as well as more details on the errors you receive.\n\nI would also suggest first trying out one of the available examples for multi model endpoints, like this one: Amazon SageMaker Multi-Model Endpoints using Scikit Learn and from there modify to your own needs.",
				"Answer_has_accepted": false
			}
		]
	},
	{
		"Questiont_title": "how can I use sagemaker_sklearn_extension in Sagemaker job?",
		"Question_creation_time": "2022-06-26T06:07:17.583Z",
		"Question_link": "https://repost.aws/questions/QUMUk4WTgJRD68w1PvBudSow/how-can-i-use-sagemaker-sklearn-extension-in-sagemaker-job",
		"Question_topic": [
			"Machine Learning & AI"
		],
		"Question_tag": [
			"Amazon SageMaker",
			"Machine Learning & AI"
		],
		"Question_upvote_count": 0,
		"Question_view_count": 60,
		"Question_answer_count": 1,
		"Question_has_accepted_answer": false,
		"Question_body": "I'm creating a data processing job in sagemaker notebook:\n\nfrom sagemaker.sklearn.processing import SKLearnProcessor\n\nsklearn_processor = SKLearnProcessor(role=role,\n                                     base_job_name='end-to-end-ml-sm-proc',\n                                     instance_type='ml.m5.large',\n                                     instance_count=1,\n                                     framework_version='0.23-1')\n\n\nmy processing script uses :\n\nfrom sagemaker_sklearn_extension.decomposition import RobustPCA\n\n\nand I get an error during the job exectution:\n\nTraceback (most recent call last):\n  File \"/opt/ml/processing/input/code/preprocessor.py\", line 14, in <module>\n    from sagemaker_sklearn_extension.decomposition import RobustPCA\nModuleNotFoundError: No module named 'sagemaker_sklearn_extension'\n\n\nas far as I understrand : framework_version='0.23-1' should make sagemaker create docker image based on image from that repo: https://github.com/aws/sagemaker-scikit-learn-container and the 0.23-1 branch handles extensions installation (if extenssion/Dockerfile.cpu file is executed), but I don't see how I can make Sagemaker run that script when creating the job.\n\nhow can I use sagemaker_sklearn_extension in Sagemaker job?",
		"Answers": [
			{
				"Answer_creation_date": "2022-07-01T13:55:05.505Z",
				"Answer_upvote_count": 0,
				"Answer_body": "There is a way to install the packages that you need via subprocess on the entry_point.py:\n\nimport subprocess\n\nlets pip install the custom package\n\nsubprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"sagemaker-scikit-learn-extension==(your version)\"])",
				"Answer_has_accepted": false
			}
		]
	},
	{
		"Questiont_title": "Sagemaker g4 and g5 instances do not have working nvidia-drivers",
		"Question_creation_time": "2022-11-22T01:56:28.275Z",
		"Question_link": "https://repost.aws/questions/QUBqYWuFr7SyC6P6Uae9LOww/sagemaker-g-4-and-g-5-instances-do-not-have-working-nvidia-drivers",
		"Question_topic": [
			"Machine Learning & AI",
			"Compute"
		],
		"Question_tag": [
			"Amazon SageMaker",
			"Machine Learning & AI",
			"GPU Development"
		],
		"Question_upvote_count": 3,
		"Question_view_count": 80,
		"Question_answer_count": 1,
		"Question_has_accepted_answer": false,
		"Question_body": "I am a heavy user of g4 and g5 instances on Sagemaker (notebook instances). Today when I tried to use the same instances as I always do I was met with the following when running nvidia-smi\n\nNVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n\nThese are all the exact same instances and workloads I have used before. The same message was found when trying to run on ec2 natively as well.",
		"Answers": [
			{
				"Answer_creation_date": "2022-11-23T04:32:10.418Z",
				"Answer_upvote_count": 0,
				"Answer_body": "Hi,\n\nThis is a NVIDIA driver issue which affect all NVIDA functions.\n\nCould you please try using the following cmds to unblock\n\nsudo dkms remove nvidia/510.47.03 --all\n\nsudo dkms install nvidia/510.47.03 -k $(uname -r)\n\n\nPlease let me know if this would work.",
				"Answer_has_accepted": false
			}
		]
	}
]