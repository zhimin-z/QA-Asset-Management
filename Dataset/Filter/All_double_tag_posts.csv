Question_title,Question_body,Question_answer_count,Question_comment_count,Question_creation_date,Question_favorite_count,Question_last_edit_date,Question_score,Question_tags,Question_view_count,Owner_creation_date,Owner_last_access_date,Owner_location,Owner_reputation,Owner_up_votes,Owner_down_votes,Owner_views,Answer_body,Answer_comment_count,Answer_creation_date,Answer_last_edit_date,Answer_score,Question_valid_tags
Difference in usecases for AWS Sagemaker vs Databricks?,"<p>I was looking at Databricks because it integrates with AWS services like Kinesis, but it looks to me like SageMaker is a direct competitor to Databricks? We are heavily using AWS, is there any reason to add DataBricks into the stack or odes SageMaker fill the same role?</p>",2,0,2019-03-13 00:23:26.6 UTC,3.0,,9,apache-spark|pyspark|databricks|amazon-sagemaker,11894,2015-01-15 17:43:03.7 UTC,2022-08-23 22:54:25.603 UTC,,1387,51,1,153,"<p>SageMaker is a great tool for deployment, it simplifies a lot of processes configuring containers, you only need to write 2-3 lines to deploy the model as an endpoint and use it.  SageMaker also provides the dev platform (Jupyter Notebook) which supports Python and Scala (sparkmagic kernal) developing, and i managed installing external scala kernel in jupyter notebook. Overall, SageMaker provides end-to-end ML services. Databricks has unbeatable Notebook environment for Spark development. </p>

<p>Conclusion</p>

<ol>
<li><p>Databricks is a better platform for Big data(scala, pyspark) Developing.(unbeatable notebook environment)</p></li>
<li><p>SageMaker is better for Deployment. and if you are not working on big data, SageMaker is a perfect choice working with (Jupyter notebook + Sklearn + Mature containers + Super easy deployment). </p></li>
<li><p>SageMaker provides ""real time inference"", very easy to build and deploy, very impressive. you can check the official SageMaker Github.
<a href=""https://github.com/awslabs/amazon-sagemaker-examples/tree/master/sagemaker-python-sdk/scikit_learn_inference_pipeline"" rel=""noreferrer"">https://github.com/awslabs/amazon-sagemaker-examples/tree/master/sagemaker-python-sdk/scikit_learn_inference_pipeline</a></p></li>
</ol>",4.0,2019-03-20 21:40:34.27 UTC,,14.0,"['amazon-sagemaker', 'databricks']"
Accessing Delta Lake Table in Databricks via Spark in MLflow project,"<p>I am currently accessing deltalake table from databricks notebook using spark. However now I need to access delta tables from MLflow project. MLflow spark api only allows logging and loading of SparkML models. Any idea on how can I accomplish this?</p>
<p>Currently I am trying to access spark via this code in MLflow project:</p>
<pre class=""lang-py prettyprint-override""><code>
spark = pyspark.sql.SparkSession._instantiatedSession
if spark is None:
  # NB: If there is no existing Spark context, create a new local one.
  # NB: We're disabling caching on the new context since we do not need it and we want to
  # avoid overwriting cache of underlying Spark cluster when executed on a Spark Worker
  # (e.g. as part of spark_udf).
  spark = ( pyspark.sql.SparkSession.builder \
   .config(&quot;spark.python.worker.reuse&quot;, True)
   .config(&quot;spark.databricks.io.cache.enabled&quot;, False)
   # In Spark 3.1 and above, we need to set this conf explicitly to enable creating
   # a SparkSession on the workers
   .config(&quot;spark.executor.allowSparkContext&quot;, &quot;true&quot;)
   .master(&quot;local[*]&quot;)
   .appName(&quot;MLflow Project&quot;)
   .getOrCreate()
  )
</code></pre>
<p>But I am getting this error:</p>
<pre><code>py4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.
</code></pre>",2,2,2022-02-05 20:21:43 UTC,,2022-02-06 17:51:53.797 UTC,1,apache-spark|pyspark|databricks|delta-lake|mlflow,282,2016-10-23 15:09:44.12 UTC,2022-09-23 01:20:21.6 UTC,,71,1,0,9,,,,,,"['databricks', 'mlflow']"
InvalidMountException while loading model from mlflow-registry in databricks,"<p>I am trying to load my spark model that I have registered using mlflow in databricks ( referring  this documentation : <a href=""https://docs.microsoft.com/en-us/azure/databricks/applications/mlflow/model-registry-example#load-versions-of-the-registered-model-using-the-api"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/databricks/applications/mlflow/model-registry-example#load-versions-of-the-registered-model-using-the-api</a>)</p>
<p>My Code is :</p>
<pre class=""lang-py prettyprint-override""><code>model_uri = &quot;models:/{model_name}/{stage_name}&quot;.format(
   model_name=model_name,stage_name=stage_name)

model = mlflow.spark.load_model(model_uri )
</code></pre>
<p>But this gives me following error :</p>
<pre><code>: com.databricks.backend.daemon.data.common.InvalidMountException: 
  Error while using path /databricks/mlflow-registry/run-id/models/spark-model/sparkml 
  for resolving path '/run-id/models/spark-model/sparkml' 
  within mount at '/databricks/mlflow-registry'.
</code></pre>
<p>(Note: <code>artifact_path</code> passed by me while registering model was 'spark-model')</p>
<p>How can this error be resolved?</p>",0,1,2020-10-26 17:09:21.65 UTC,,2020-11-01 17:49:03.517 UTC,2,apache-spark|databricks|mlflow,324,2020-10-13 08:19:36.863 UTC,2022-09-22 10:59:26.177 UTC,,31,0,0,1,,,,,,"['databricks', 'mlflow']"
Assume IAM Role to store MLFlow Artifact on S3 bucket in another account,"<p>I have to save my MLFlow artifacts (using Databricks Unified Analytics) to a S3 bucket, with service-side encrpytion using a KMS key.</p>

<p>My instances are into an AWS account A, my S3 bucket and my KMS key into an account B. I can't have my KMS Key into my account A.</p>

<p>I don't want to use DBFS to mount S3 buckets, for security reasons (buckets can contains sensitive data and I don't want to share this between users).</p>

<p>I have to assume an IAM role in order to access the bucket, as I did to access it through s3a (with <code>spark.hadoop.fs.s3a.credentialsType</code> and <code>spark.hadoop.fs.s3a.stsAssumeRole.arn</code> parameters).</p>

<p>When I create an experiment with s3 and try to log a model like this : </p>

<pre class=""lang-py prettyprint-override""><code>import mlflow
import mlflow.sklearn
id_exp = mlflow.create_experiment(""/Users/first.last@company.org/Experiment"",'s3://s3-bucket-name/')
with mlflow.start_run(experiment_id=id_exp):
  clf_mlf = tree.DecisionTreeClassifier()
  clf_mlf = clf_mlf.fit(X_train, y_train)
  y_pred = clf_mlf.predict(X_test)
  mlflow.sklearn.log_model(clf_mlf, ""model"", serialization_format='pickle')
</code></pre>

<p>I have this error : </p>

<pre><code>S3UploadFailedError: Failed to upload /tmp/tmp2yl2olhi/model/conda.yaml to s3-bucket-name//05c17a33a33d46a5ad3cc811a9faf35a/artifacts/model/conda.yaml: An error occurred (KMS.NotFoundException) when calling the PutObject operation: Key 'arn:aws:kms:eu-central-1:account_a_id:key/key_id' does not exist
</code></pre>

<p>How can I told MLFlow to assume a role before accessing to S3 ?</p>",0,0,2019-07-24 09:27:00.5 UTC,,,3,amazon-s3|amazon-iam|databricks|mlflow,693,2018-03-13 17:58:54.917 UTC,2020-11-03 18:13:09.453 UTC,,31,0,0,4,,,,,,"['databricks', 'mlflow']"
MLFlow Webhook calling Azure DevOps pipeline - retrieve body,"<p>I am using the MLFlow Webhooks , mentioned <a href=""https://docs.microsoft.com/en-us/azure/databricks/applications/mlflow/model-registry-webhooks"" rel=""nofollow noreferrer"">here</a>. I am using that to queue an Azure Devops Pipeline.</p>
<p>However, I can't seem to to find a way to retrieve the payload variables inside my pipeline.</p>
<p>E.g. during transition of models, according to the document, such a payload is passed</p>
<pre><code>POST
/your/endpoint/for/event/model-versions/stage-transition
--data {
  &quot;event&quot;: &quot;MODEL_VERSION_TRANSITIONED_STAGE&quot;,
  &quot;webhook_id&quot;: &quot;c5596721253c4b429368cf6f4341b88a&quot;,
  &quot;event_timestamp&quot;: 1589859029343,
  &quot;model_name&quot;: &quot;Airline_Delay_SparkML&quot;,
  &quot;version&quot;: &quot;8&quot;,
  &quot;to_stage&quot;: &quot;Production&quot;,
  &quot;from_stage&quot;: &quot;None&quot;,
  &quot;text&quot;: &quot;Registered model 'someModel' version 8 transitioned from None to Production.&quot;
}
</code></pre>
<p>My webhook is created like this:</p>
<pre><code>mlflow_webhook_triggerDevOps={
  &quot;events&quot;: [&quot;TRANSITION_REQUEST_CREATED&quot;, &quot;REGISTERED_MODEL_CREATED&quot;],
  &quot;description&quot;: &quot;Integration with Azure DevOps&quot;,
  &quot;status&quot;: &quot;ACTIVE&quot;,
  &quot;http_url_spec&quot;: {
                    &quot;url&quot;: &quot;https://dev.azure.com/orgname/ProjectName/_apis/build/builds?definitionId=742&amp;api-version=6.0&quot;,
                    &quot;authorization&quot;: &quot;Basic &quot; + base64_message
                    }
 }

mlflow_createwebhook=requests.post('https://databricksurl/api/2.0/mlflow/registry-webhooks/create', headers=header, proxies=proxies, json=mlflow_webhook_body)
</code></pre>
<p>How do I then retrieve the payload variable e.g. model_name, inside my pipeline definition in Azure Devops?.</p>
<p>I looked at <a href=""https://stackoverflow.com/questions/50838651/vsts-use-api-to-set-build-parameters-at-queue-time"">this post</a>, but I can't seem to see any payload information (like mentioned above) under the Network-payload tab (or I am not using properly).</p>
<p>Right now, I can trigger the pipeline, but can't seem to find a way to retrieve the payload.</p>
<p>Is it possible? Am I missing something?</p>",0,3,2022-05-14 09:55:21.283 UTC,,,1,azure-devops|databricks|webhooks|azure-databricks|mlflow,125,2015-04-10 08:31:54.763 UTC,2022-09-24 09:37:37.383 UTC,,596,53,1,80,,,,,,"['databricks', 'mlflow']"
Difference in performance between Mlflow run and manual training,"<p>I'm working in Databricks trying to train a <code>XGBClassifier</code> model in an sklearn <code>Pipeline</code> using both manual code and within an Mlflow run. I have a training set <code>X_train</code>, a validation set <code>X_val</code>, and a test set <code>X_test</code>. When I run the model on my test set, the performance metrics for each method are different, with the Mlflow metrics being much better. Below is my code for the manual fitting and testing (preprocessor is a standard scaler and one-hot encoder):</p>
<pre><code>model = Pipeline([
    (&quot;column_selector&quot;, col_selector),
    (&quot;preprocessor&quot;, preprocessor),
    (&quot;classifier&quot;, xgb_model),
])

model.fit(X_train, y_train, classifier__early_stopping_rounds=5,
classifier__eval_set=[(X_val_processed,y_val)], classifier__verbose=False)

y_pred = model.predict(X_test)
y_probs = model.predict_proba(X_test)

from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, log_loss

test_acc = accuracy_score(y_test, y_pred)
test_f1 = f1_score(y_test, y_pred)
test_prec = precision_score(y_test, y_pred)
test_rec = recall_score(y_test, y_pred)
test_logloss = log_loss(y_test, y_pred)
test_rocauc = roc_auc_score(y_test, y_probs[:, 1])
metric_cols = ['accuracy', 'f1_score', 'precision', 'recall', 'log_loss', 'roc_auc']
pd.DataFrame([[test_acc, test_f1, test_prec, test_rec, test_logloss, test_rocauc]], columns=metric_cols, index=['test'])
</code></pre>
<p>Which gets me the following:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th></th>
<th>accuracy</th>
<th>f1_score</th>
<th>precision</th>
<th>recall</th>
<th>log_loss</th>
<th>roc_auc</th>
</tr>
</thead>
<tbody>
<tr>
<td>test</td>
<td>0.865784</td>
<td>0.326917</td>
<td>0.753635</td>
<td>0.208731</td>
<td>4.635675</td>
<td>0.738647</td>
</tr>
</tbody>
</table>
</div>
<p>Here is my code for the mlflow run:</p>
<pre><code>import mlflow
import databricks.automl_runtime

mlflow.sklearn.autolog(disable=True)

mlflow.sklearn.autolog(log_input_examples=True, silent=True)

with mlflow.start_run(experiment_id=&quot;363669966797473&quot;, run_name=&quot;xgb-mod-3-run&quot;) as mlflow_run:
    model.fit(X_train, y_train, classifier__early_stopping_rounds=5, classifier__eval_set=[(X_val_processed,y_val)], classifier__verbose=False)

    xgbc_test_metrics = mlflow.sklearn.eval_and_log_metrics(model, X_test, y_test, prefix=&quot;test_&quot;)

    xgbc_test_metrics = {k.replace(&quot;test_&quot;, &quot;&quot;): v for k, v in xgbc_test_metrics.items()}

    display(pd.DataFrame([xgbc_test_metrics], index=[&quot;test&quot;]))
</code></pre>
<p>After which I get much better results for precision, recall, f1_score, and log_loss:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th></th>
<th>accuracy</th>
<th>f1_score</th>
<th>precision</th>
<th>recall</th>
<th>log_loss</th>
<th>roc_auc</th>
</tr>
</thead>
<tbody>
<tr>
<td>test</td>
<td>0.865784</td>
<td>0.83134</td>
<td>0.85255</td>
<td>0.86578</td>
<td>0.36501</td>
<td>0.738647</td>
</tr>
</tbody>
</table>
</div>
<p>While accuracy and ROC AUC remain unchanged.</p>
<p>What is causing this difference in performance? Is Mlflow performing any sort of tuning/optimization under the hood? Is the experiment run actually fitting multiple models other than the <code>xgb_model</code> I instantiated? Let me know if I need to provide more code.</p>",0,1,2022-06-23 15:18:29.287 UTC,,,0,python|machine-learning|databricks|mlflow|xgbclassifier,65,2021-06-15 19:55:08.787 UTC,2022-07-06 21:04:46.54 UTC,,23,3,0,2,,,,,,"['databricks', 'mlflow']"
Does MLflow allow to log artifacts from remote locations like S3?,"<h2>My setting</h2>
<p>I have developed an environment for ML experiments that looks like the following: training happens in the AWS cloud with SageMaker Training Jobs. The trained model is stored in the <code>/opt/ml/model</code> directory, <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo-output.html"" rel=""nofollow noreferrer"">which is reserved by SageMaker to pack models</a> as a <code>.tar.gz</code> in SageMaker's own S3 bucket. Several evaluation metrics are computed during training and testing, and recorded to an MLflow infrastructure consisting of an S3-based artifact store (see <a href=""https://www.mlflow.org/docs/latest/tracking.html#scenario-4-mlflow-with-remote-tracking-server-backend-and-artifact-stores"" rel=""nofollow noreferrer"">Scenario 4</a>). Note that this is a different S3 bucket than SageMaker's.</p>
<p>A very useful feature from MLflow is that any model artifacts can be logged to a training run, so data scientists have access to both metrics and more complex outputs through the UI. These outputs include (but are not limited to) the trained model itself.</p>
<p>A limitation is that, as I understand it, the <a href=""https://www.mlflow.org/docs/latest/python_api/mlflow.html#mlflow.log_artifact"" rel=""nofollow noreferrer"">MLflow API for logging artifacts</a> only accepts as input a local path to the artifact itself, and will always upload it to its artifact store. This is suboptimal when the artifacts are stored somewhere outside MLflow, as you have to store them twice. A transformer model may weigh more than 1GB.</p>
<h2>My questions</h2>
<ul>
<li>Is there a way to pass an S3 path to MLflow and make it count as an artifact, without having to download it locally first?</li>
<li>Is there a way to avoid pushing a copy of an artifact to the artifact store? If my artifacts already reside in another remote location, it would be ideal to just have a link to such location in MLflow and not a copy in MLflow storage.</li>
</ul>",1,0,2022-01-12 10:49:12.913 UTC,,,0,python|amazon-s3|amazon-sagemaker|mlflow|mlops,533,2016-01-08 20:55:48.08 UTC,2022-09-23 10:18:38.03 UTC,"Madrid, Spain",118,28,0,19,,,,,,"['amazon-sagemaker', 'mlflow']"
How to call a model's artifacts (pickeled vectorizer) when the model is on Production in databricks?,"<p>I am using databrick, machine learning view. I have successfully created and saved my model and also logged my pickled vectorizer as artifacts to it. I would like to load it in a different notebook; the model and the artifacts belong to the model which is currently in production.</p>
<pre><code>   import mlflow.pyfunc

model_name = &quot;Sentiment&quot;
stage = 'Production'

model = mlflow.pyfunc.load_model(
    model_uri=f&quot;models:/{model_name}/{stage}&quot;
)
</code></pre>
<p>So this code seems to be working but it does not load the artifacts or if it does, i do not know how to display them.</p>
<p>I have found this code but not sure what to do with it to only get the artifacts from the model which is in Production.</p>
<pre><code>  from mlflow.store.artifact.models_artifact_repo import ModelsArtifactRepository
version =1
model_uri = MlflowClient.get_model_version_download_uri(name=model_name, version=version)
ModelsArtifactRepository(model_uri).download_artifacts(artifact_path=&quot;&quot;)
</code></pre>
<p>even when i run this i get an error :</p>
<p>TypeError: get_model_version_download_uri() missing 1 required positional argument: 'self'</p>",0,0,2022-09-16 12:56:25.133 UTC,,2022-09-16 13:15:02.563 UTC,0,model|databricks|mlflow|artifacts,10,2022-06-06 08:24:57.99 UTC,2022-09-19 13:50:16.62 UTC,,47,0,0,6,,,,,,"['mlflow', 'databricks']"
cmd: mlflow sagemaker build-and-push-container gives FileNotFoundError:,"<p>I don't understand what type of files I missing</p>
<blockquote>
<p>and I used this code to connect AWS ECR</p>
</blockquote>
<pre><code>setx AWS_ACCESS_KEY_ID AKIAIOSFODNN7EXAMPLE
setx AWS_SECRET_ACCESS_KEY wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY
setx AWS_DEFAULT_REGION us-west-2
</code></pre>
<blockquote>
<p>at mlflow artifact directory shown below and here python isn't a python.exe it's just a file name</p>
</blockquote>
<pre><code>(deploy_ml) D:\****\Python\mlruns\1\2877b0a860934a179723fd11ed946589\artifacts\random-forest-model&gt;


(deploy_ml) D:\***\Python\mlruns\1\2877b0a86093***723fd11ed946589\artifacts\random-forest-model&gt;mlflow sagemaker  build-and-push-container
2021/09/14 22:22:13 INFO mlflow.models.docker_utils: Building docker image with name mlflow-pyfunc
FIND: Parameter format not correct
Traceback (most recent call last):
  File &quot;c:\users\s***\anaconda3\envs\deploy_ml\lib\runpy.py&quot;, line 193, in _run_module_as_main
    &quot;__main__&quot;, mod_spec)
  File &quot;c:\users\s***\anaconda3\envs\deploy_ml\lib\runpy.py&quot;, line 85, in _run_code
    exec(code, run_globals)
  File &quot;C:\Users\s***\Anaconda3\envs\deploy_ml\Scripts\mlflow.exe\__main__.py&quot;, line 7, in &lt;module&gt;
  File &quot;c:\users\s****\anaconda3\envs\deploy_ml\lib\site-packages\click\core.py&quot;, line 1137, in __call__
    return self.main(*args, **kwargs)
  File &quot;c:\users\s****\anaconda3\envs\deploy_ml\lib\site-packages\click\core.py&quot;, line 1062, in main
    rv = self.invoke(ctx)
  File &quot;c:\users\s****\anaconda3\envs\deploy_ml\lib\site-packages\click\core.py&quot;, line 1668, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File &quot;c:\users\s****\anaconda3\envs\deploy_ml\lib\site-packages\click\core.py&quot;, line 1668, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File &quot;c:\users\***\anaconda3\envs\deploy_ml\lib\site-packages\click\core.py&quot;, line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File &quot;c:\users\***\anaconda3\envs\deploy_ml\lib\site-packages\click\core.py&quot;, line 763, in invoke
    return __callback(*args, **kwargs)
  File &quot;c:\users\s****\anaconda3\envs\deploy_ml\lib\site-packages\mlflow\sagemaker\cli.py&quot;, line 280, in build_and_push_container
    custom_setup_steps_hook=setup_container,
  File &quot;c:\users\s***\anaconda3\envs\deploy_ml\lib\site-packages\mlflow\models\docker_utils.py&quot;, line 114, in _build_image
    universal_newlines=True,
  File &quot;c:\users\s***\anaconda3\envs\deploy_ml\lib\subprocess.py&quot;, line 729, in __init__
    restore_signals, start_new_session)
  File &quot;c:\users\s****\anaconda3\envs\deploy_ml\lib\subprocess.py&quot;, line 1017, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] The system cannot find the file specified
</code></pre>",0,0,2021-09-14 17:16:10.42 UTC,,,1,amazon-web-services|docker|amazon-sagemaker|amazon-ecr|mlflow,173,2021-08-03 07:46:47.183 UTC,2021-12-03 06:36:36.86 UTC,"Bangalore, Karnataka, India",11,0,0,3,,,,,,"['amazon-sagemaker', 'mlflow']"
"How to add coefficients, p-values and relevant variable name in mlflow?","<p>I am running a linear regression model and I would like to add the coefficients and P-values of each variable and the variable name in to the metrics of the mlflow output. I am new to using mlflow and not very familiar in doing this. Below is an example of part of my code</p>
<pre><code>with mlflow.start_run(run_name=p_key + '_' + str(o_key)):
    
    lr = LinearRegression(
      featuresCol = 'features',
      labelCol = target_var,
      maxIter = 10,
      regParam = 0.0,
      elasticNetParam = 0.0,
      solver=&quot;normal&quot;
        )
    
    lr_model_item = lr.fit(train_model_data)
    lr_coefficients_item = lr_model_item.coefficients
    lr_coefficients_intercept = lr_model_item.intercept
    
    lr_predictions_item = lr_model_item.transform(train_model_data)
    lr_predictions_item_oos = lr_model_item.transform(test_model_data)
    
    rsquared = lr_model_item.summary.r2
    
    # Log mlflow attributes for mlflow UI
    mlflow.log_metric(&quot;rsquared&quot;, rsquared)
    mlflow.log_metric(&quot;intercept&quot;, lr_coefficients_intercept)
    for i in lr_coefficients_item:
      mlflow.log_metric('coefficients', lr_coefficients_item[i])
</code></pre>
<p>Would like to know whether this is possible? In the final output I should have the intercept, coefficients, p-values and the relevant variable name.</p>",1,0,2020-08-21 01:46:29.493 UTC,,,1,databricks|mlflow,242,2019-05-08 02:16:04.547 UTC,2021-05-19 04:34:38.877 UTC,,129,3,0,29,,,,,,"['databricks', 'mlflow']"
Databricks does not display the grid search info with verbose mode when n_jobs > 1,"<p>When I run the machine learning algorithm using Grid Search with verbose as 10 and n_jobs as -1, the info for each model is not getting displayed in databricks. If I use n_jobs as 1, then each param grid combination output is getting displayed. Any values greater than 1 results in no info on the notebook rather it just displays the completed message without any verbose info of each model fit.</p>
<p>Can you please help me on this.</p>",0,0,2022-08-14 17:40:33.153 UTC,,,1,databricks|grid-search|gridsearchcv|databricks-ml,15,2020-01-21 09:48:39.807 UTC,2022-08-30 03:17:09.94 UTC,,85,1,0,32,,,,,,"['databricks-ml', 'databricks']"
MLFlow Model Registry ENDPOINT_NOT_FOUND: No API found for ERROR,"<p>I'm currently using MLFlow in Azure Databricks and trying to load a model from the Model Registry. Currently referencing the version, but will want to reference the stage 'Production' (I get the same error when referencing the stage as well)</p>
<p>I keep encountering an error:</p>
<pre><code>ENDPOINT_NOT_FOUND: No API found for 'POST /mlflow/model-versions/get-download-uri'
</code></pre>
<p>My artifacts are stored in the dbfs filestore.</p>
<p>I have not been able to identify why this is happening.</p>
<p>Code:</p>
<pre><code>from mlflow.tracking.client import MlflowClient
from mlflow.entities.model_registry.model_version_status import ModelVersionStatus

import mlflow.pyfunc

model_name = &quot;model_name&quot;

model_version_uri = &quot;models:/{model_name}/4&quot;.format(model_name=model_name)

print(&quot;Loading registered model version from URI: '{model_uri}'&quot;.format(model_uri=model_version_uri))
model_version_4 = mlflow.pyfunc.load_model(model_version_uri)

model_production_uri = &quot;models:/{model_name}/production&quot;.format(model_name=model_name)

print(&quot;Loading registered model version from URI: '{model_uri}'&quot;.format(model_uri=model_production_uri))
model_production = mlflow.pyfunc.load_model(model_production_uri)
</code></pre>",0,2,2021-01-15 22:43:29.033 UTC,,2021-01-18 10:02:12.683 UTC,1,azure|databricks|mlflow,387,2020-02-09 23:48:58.33 UTC,2021-02-26 02:01:08.413 UTC,,11,0,0,6,,,,,,"['databricks', 'mlflow']"
How to direct DLT target table to a Unity Catalog Metastore,"<p>This question is pretty straightforward. Seems like in DLT, you can define the output table name like below:</p>
<pre><code>@dlt.table(name=&quot;my_table_name&quot;)
def my_pipeline():
  ...
</code></pre>
<p>This writes to the hive_metastore catalog, but how do I customize it for a different catalog?</p>",1,0,2022-08-12 20:58:01.967 UTC,,2022-08-13 08:00:22.087 UTC,1,pyspark|databricks|delta-live-tables|databricks-unity-catalog,143,2021-08-03 19:33:19.5 UTC,2022-09-23 03:10:16.28 UTC,,39,1,0,2,,,,,,"['databricks', 'databricks-unity-catalog']"
Sagemaker Train Job can't connect to ec2 instance,"<p>I have MLFlow server running on ec2 instance, port 5000.</p>
<p>This ec2 instance has security group with opened TCP connection on port 5000 to another security group designated for SageMaker.</p>
<p>ec2 instance inbound rules:
<a href=""https://i.stack.imgur.com/VXwid.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VXwid.png"" alt=""enter image description here"" /></a></p>
<p>SageMaker outbound rules:
<a href=""https://i.stack.imgur.com/ZUzek.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZUzek.png"" alt=""enter image description here"" /></a></p>
<p>These 2 security groups are in the same VPC</p>
<p>Now, I try to run SageMaker training job with designated security group, so that the training script will log metrics to ec2 server via internal IP address. (As answered <a href=""https://stackoverflow.com/questions/45416882/aws-security-group-include-another-security-group"">here</a>), but connection fails</p>
<p>SageMaker job init:</p>
<pre><code>   role = &quot;ml_sagemaker&quot;
   security_group_ids = ['sg-04868acca16e81183']
   bucket = sagemaker_session.default_bucket()  
   out_path = f&quot;s3://{bucket}/{project_name}&quot;

   estimator = PyTorch(entry_point='run_train.py',
                       source_dir='.',
                       sagemaker_session=sagemaker_session,
                       instance_type=instance_type,
                       instance_count=1,
                       framework_version='1.5.0',
                       py_version='py3',
                       role=role,
                       security_group_ids=security_group_ids,
                       hyperparameters={},
                       )
   ....

</code></pre>
<p>Inside <code>run_train.py</code>:</p>
<pre><code>import mlflow
tracking_uri = &quot;http://172.31.77.137:5000&quot;  # &lt;- this is internal ec2 IP
mlflow.set_tracking_uri(tracking_uri)
mlflow.log_param(&quot;test_param&quot;, 3)
</code></pre>
<p>Error:</p>
<pre><code>File &quot;/opt/conda/lib/python3.6/site-packages/urllib3/util/connection.py&quot;, line 74, in create_connection
    sock.connect(sa)
TimeoutError: [Errno 110] Connection timed out
</code></pre>
<p><strong>However</strong>, when when I create SageMaker Notebook instance with the same security group and the same IAM role, I am able to successfully connect to ec2 and log metrics from within the Notebook.</p>
<p><a href=""https://i.stack.imgur.com/YYHlO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YYHlO.png"" alt=""enter image description here"" /></a></p>
<p>Here is SageMaker Notebook configurations:</p>
<img src=""https://i.stack.imgur.com/bslu8.png"" width=""300"" />
<p>How can I connect to ec2 instance from SageMaker Training Job?</p>",1,0,2021-02-19 17:18:49.957 UTC,1.0,,1,amazon-ec2|amazon-vpc|amazon-sagemaker|aws-security-group|mlflow,608,2015-07-28 15:46:05.74 UTC,2022-09-22 13:24:29.297 UTC,,653,216,1,76,,,,,,"['amazon-sagemaker', 'mlflow']"
"Not finding Unity Catalog ""Create Metastore"" in Azure Databricks","<p>I am trying to set up Unity Catalog in Azure Databricks following this documentation <a href=""https://docs.microsoft.com/en-us/azure/databricks/data-governance/unity-catalog/azure-managed-identities#config-managed-id"" rel=""nofollow noreferrer"">Unity Catalog</a>, but I do not find &quot;Create Metastore&quot; on Databricks. What do I need to do?</p>",0,5,2022-09-05 09:48:23.867 UTC,,,3,azure|databricks|databricks-unity-catalog,123,2021-05-03 12:26:56.793 UTC,2022-09-15 09:45:21.097 UTC,,31,0,0,2,,,,,,"['databricks', 'databricks-unity-catalog']"
mlflow is not working and not generating UI on browser on windows 7,"<p>Hi I am trying to learn mlflow with python, but when I am trying to run the same set of instructions with my laptop running windows 7 home basic OS ,</p>

<p>I am following the instructions as per the official website ""<a href=""https://mlflow.org/docs/latest/quickstart.html"" rel=""nofollow noreferrer"">https://mlflow.org/docs/latest/quickstart.html</a>""</p>

<p>I am running the scrit ""mlflow_tracking.py"" located in the examples folder of the repo. The script is working fine, output is getting generated at the same time mlruns folder along with all the internal content is also getting generated but On running the <code>mlflow ui</code> command on the command prompt it runs without any output or message, and the tracking ui on typing ""<a href=""http://localhost:5000"" rel=""nofollow noreferrer"">http://localhost:5000</a>"" gives me an error:</p>

<p><code>This site can’t be reached
localhost refused to connect.
</code></p>

<p>while, when I am running the same on the windows 10 laptop it is working fine , is there a dependency of windows version on mlflow? 
Does Mlflow not works on windows 7, and if yes how to get it running. </p>",1,2,2020-04-12 05:40:45.1 UTC,,2020-04-12 07:25:02.69 UTC,1,python|machine-learning|databricks|azure-databricks|mlflow,1655,2017-06-08 10:07:18.38 UTC,2021-03-09 15:57:56.98 UTC,,522,0,1,37,,,,,,"['databricks', 'mlflow']"
Azure : Ansible role for deploying ML model integrated over databricks,"<p>I have developed ML predictive model on historical data in Azure Databricks using python notebook.
Which means i have done data extraction, preparation, feature engineering and model training everything done in Databricks using python notebook.
I have almost completed development part of it, now we want to deploy ML model into production using ansible roles.</p>",1,2,2021-08-10 07:13:27.8 UTC,,2021-08-11 06:34:36.903 UTC,1,deployment|databricks|mlflow|mlmodel,128,2020-11-16 06:35:10.493 UTC,2021-11-29 05:21:57.803 UTC,,11,0,0,0,,,,,,"['databricks', 'mlflow']"
Cannot find '/dbfs/databricks-datasets' in my notebook,"<p>I am using the Databricks community edition and working through the <a href=""https://databricks.com/discover/introduction-to-data-analysis-workshop-series/ml-scikit-learn"" rel=""nofollow noreferrer"">ML intro</a> tutorial.</p>
<p>I am able to <code>%fs ls databricks-datasets/COVID/covid-19-data/us-states.csv</code>, but not able to read it through pandas</p>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; df = pd.read_csv(&quot;/dbfs/databricks-datasets/COVID/covid-19-data/us-states.csv&quot;)

FileNotFoundError: [Errno 2] File /dbfs/databricks-datasets/COVID/covid-19-data/us-states.csv does not exist: '/dbfs/databricks-datasets/COVID/covid-19-data/us-states.csv'

</code></pre>
<p>Directly <code>open</code> the <code>README.md</code> file in <a href=""https://docs.databricks.com/data/databricks-datasets.html"" rel=""nofollow noreferrer"">databricks dataset</a> also failed</p>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; f = open(&quot;/dbfs/databricks-datasets/README.md&quot;, &quot;r&quot;)

FileNotFoundError: [Errno 2] No such file or directory: '/dbfs/databricks-datasets/README.md'
</code></pre>
<p>Any thoughts or suggestions?</p>
<p><a href=""https://i.stack.imgur.com/YoMVX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YoMVX.png"" alt=""enter image description here"" /></a></p>",0,4,2021-09-04 05:01:13.607 UTC,,2021-09-04 15:16:08.45 UTC,0,databricks|databricks-community-edition|databricks-ml,23,2014-10-29 19:07:27.193 UTC,2022-09-24 21:41:03.507 UTC,"Bay Area, CA, USA",4278,587,3,346,,,,,,"['databricks-ml', 'databricks']"
mlflow model serving not working properly,"<p>I am trying to serve my model after model registry. The model is successfully being registered in mlflow but when I am going to serve it it is showing the following error.</p>
<p>Error: Got unexpected extra arguments (Class Classification New log)
Thu May 19 06:52:51 UTC 2022
Waiting for another process to start...
Usage: mlflow artifacts download [OPTIONS]
Try 'mlflow artifacts download --help' for help.</p>",0,0,2022-05-19 06:55:41.443 UTC,,,0,databricks|mlflow,45,2022-02-27 20:42:11.227 UTC,2022-09-21 06:56:42.217 UTC,,1,0,0,2,,,,,,"['databricks', 'mlflow']"
how to save mlflow metrics and paramters to an s3 bucket without a server?,"<p>I want to save the parameters and metrics gotten from mlflow into an s3 bucket. Usually I get these from setting the <code>tracking_uri</code> in mlflow and that saves it on a server but I can't have a server in this case(was told no) and just want to store my parameters and metrics on the s3 bucket in the same manner as it would using the <code>tracking_uri</code>.</p>
<p>I can store the artifacts on the s3 bucket without issue but not the params/metrics.</p>
<p>Here is some code:</p>
<pre class=""lang-py prettyprint-override""><code>def mlflow_testing():
    
    tracking_uri =  &quot;s3://bucket_name/mlflow/&quot;,
    experiment_name = &quot;test&quot;,
    artifact_uri= &quot;s3://bucket_name/mlflow/&quot;
    
    mlflow.set_tracking_uri(tracking_uri)
    mlflow.create_experiment(experiment_name, artifact_uri)
    mlflow.set_experiment(experiment_name)
    
    with mlflow.start_run() as run:
        mlflow.log_param(&quot;test1&quot;, 0)
        mlflow.log_metric(&quot;test2&quot;, 1)
    
        with open(&quot;test.txt&quot;, &quot;w&quot;) as f:
            f.write(&quot;this is an artifact&quot;)
    
        mlflow.log_artifact(&quot;test.txt&quot;)
        mlflow.end_run()
</code></pre>
<p>This is capable of storing the artifact text file on the s3 bucket(so long as I make the uri a local path like <code>local_data/mlflow</code> instead of the s3 bucket).</p>
<p>Setting the s3 bucket for the <code>tracking_uri</code> results in this error:</p>
<pre><code>mlflow.tracking.registry.UnsupportedModelRegistryStoreURIException:
Model registry functionality is unavailable; got unsupported URI
's3://bucket_location/mlflow/' for model registry data storage.
Supported URI schemes are: ['', 'file', 'databricks', 'http', 'https',
'postgresql', 'mysql', 'sqlite', 'mssql']. See
https://www.mlflow.org/docs/latest/tracking.html#storage for how to
run an MLflow server against one of the supported backend storage
locations.
</code></pre>
<p>Does anyone have advice on getting around this without setting up a server? I just want those metrics and params.</p>",1,1,2022-05-14 08:41:29.357 UTC,1.0,2022-05-14 14:14:43.397 UTC,2,python|amazon-s3|amazon-sagemaker|mlflow,818,2021-03-17 15:21:32.347 UTC,2022-07-14 10:53:41.117 UTC,,21,0,0,12,,,,,,"['amazon-sagemaker', 'mlflow']"
Pyspark: How to save and apply IndexToString to convert labels back to original values in a new predicted dataset,"<p>I am using pyspark.ml.RandomForestClassifier and one of the steps here involves <strong>StringIndexer</strong> on the training data target variable to convert it into labels.</p>
<pre><code>indexer = StringIndexer(inputCol = target_variable_name, outputCol = 'label').fit(df)
df = indexer.transform(df)
</code></pre>
<p>After fitting the final model I am saving it using mlflow.spark.log_model(). So, when applying the model on a new dataset in future, I just load the model again and apply to the new data:</p>
<pre><code>model = mlflow.sklearn.load_model(&quot;models:/RandomForest_model/None&quot;)
predictions = rfModel.transform(new_data)
</code></pre>
<p>In the new_data the prediction will come as <strong>labels</strong> and not in original value. So, if I have to get the original values I have to use <strong>IndexToString</strong></p>
<pre><code>labelConverter = IndexToString(inputCol=&quot;prediction&quot;, outputCol=&quot;predictedLabel&quot;,labels=indexer.labels)
predictions = labelConverter.transform(predictions)
</code></pre>
<p>So, the question is, my model doesn't save the <strong>indexer.labels</strong> as only the model gets saved. How do, I save and use the indexer.labels from my training dataset on any new dataset. Can this be saved and retrived in mlflow ?</p>
<p>Apologies, if Iam sounding naïve here . But, getting back the original values in the new dataset is really getting me confused.</p>",0,0,2021-10-07 16:29:57.133 UTC,,2021-10-08 07:06:50.887 UTC,1,pyspark|databricks|random-forest|apache-spark-mllib|mlflow,115,2017-07-27 12:59:26.927 UTC,2022-09-21 07:04:54.843 UTC,,459,100,0,61,,,,,,"['databricks', 'mlflow']"
Serving models from mlflow registry to sagemaker,"<p>I have an mlflow server running locally and being exposed at port 80. I also have a model in the mlflow registry and I want to deploy it using the <code>mlflow sagemaker run-local</code> because after testing this locally, I am going to deploy everything to AWS and Sagemaker. My problem is that when I run:</p>
<pre><code>export MODEL_PATH=models:/churn-lgb-test/2
export LOCAL_PORT=8000
mlflow sagemaker run-local -m $MODEL_PATH -p $LOCAL_PORT -f python_function -i splicemachine/mlflow-pyfunc:1.6.0
</code></pre>
<p>it starts the container and I immediately get this error:</p>
<pre><code>2020-07-27 13:02:13 +0000] [827] [ERROR] Exception in worker process
Traceback (most recent call last):
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/arbiter.py&quot;, line 583, in spawn_worker
    worker.init_process()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/workers/ggevent.py&quot;, line 162, in init_process
    super().init_process()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/workers/base.py&quot;, line 119, in init_process
    self.load_wsgi()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/workers/base.py&quot;, line 144, in load_wsgi
    self.wsgi = self.app.wsgi()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/app/base.py&quot;, line 67, in wsgi
    self.callable = self.load()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/app/wsgiapp.py&quot;, line 49, in load
    return self.load_wsgiapp()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/app/wsgiapp.py&quot;, line 39, in load_wsgiapp
    return util.import_app(self.app_uri)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/util.py&quot;, line 358, in import_app
    mod = importlib.import_module(module)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/importlib/__init__.py&quot;, line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1006, in _gcd_import
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 983, in _find_and_load
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 967, in _find_and_load_unlocked
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 677, in _load_unlocked
  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 728, in exec_module
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/models/container/scoring_server/wsgi.py&quot;, line 3, in &lt;module&gt;
    app = scoring_server.init(pyfunc.load_model(&quot;/opt/ml/model/&quot;))
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/pyfunc/__init__.py&quot;, line 292, in load_model
    return importlib.import_module(conf[MAIN])._load_pyfunc(data_path)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/sklearn.py&quot;, line 219, in _load_pyfunc
    return _load_model_from_local_file(path)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/sklearn.py&quot;, line 206, in _load_model_from_local_file
    with open(path, &quot;rb&quot;) as f:
IsADirectoryError: [Errno 21] Is a directory: '/opt/ml/model'
[2020-07-27 13:02:13 +0000] [828] [ERROR] Exception in worker process
Traceback (most recent call last):
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/arbiter.py&quot;, line 583, in spawn_worker
    worker.init_process()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/workers/ggevent.py&quot;, line 162, in init_process
    super().init_process()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/workers/base.py&quot;, line 119, in init_process
    self.load_wsgi()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/workers/base.py&quot;, line 144, in load_wsgi
    self.wsgi = self.app.wsgi()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/app/base.py&quot;, line 67, in wsgi
    self.callable = self.load()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/app/wsgiapp.py&quot;, line 49, in load
    return self.load_wsgiapp()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/app/wsgiapp.py&quot;, line 39, in load_wsgiapp
    return util.import_app(self.app_uri)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/util.py&quot;, line 358, in import_app
    mod = importlib.import_module(module)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/importlib/__init__.py&quot;, line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1006, in _gcd_import
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 983, in _find_and_load
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 967, in _find_and_load_unlocked
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 677, in _load_unlocked
  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 728, in exec_module
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/models/container/scoring_server/wsgi.py&quot;, line 3, in &lt;module&gt;
    app = scoring_server.init(pyfunc.load_model(&quot;/opt/ml/model/&quot;))
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/pyfunc/__init__.py&quot;, line 292, in load_model
    return importlib.import_module(conf[MAIN])._load_pyfunc(data_path)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/sklearn.py&quot;, line 219, in _load_pyfunc
    return _load_model_from_local_file(path)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/sklearn.py&quot;, line 206, in _load_model_from_local_file
    with open(path, &quot;rb&quot;) as f:
IsADirectoryError: [Errno 21] Is a directory: '/opt/ml/model'
[2020-07-27 13:02:13 +0000] [828] [INFO] Worker exiting (pid: 828)
[2020-07-27 13:02:13 +0000] [827] [INFO] Worker exiting (pid: 827)
[2020-07-27 13:02:13 +0000] [829] [ERROR] Exception in worker process
Traceback (most recent call last):
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/arbiter.py&quot;, line 583, in spawn_worker
    worker.init_process()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/workers/ggevent.py&quot;, line 162, in init_process
    super().init_process()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/workers/base.py&quot;, line 119, in init_process
    self.load_wsgi()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/workers/base.py&quot;, line 144, in load_wsgi
    self.wsgi = self.app.wsgi()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/app/base.py&quot;, line 67, in wsgi
    self.callable = self.load()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/app/wsgiapp.py&quot;, line 49, in load
    return self.load_wsgiapp()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/app/wsgiapp.py&quot;, line 39, in load_wsgiapp
    return util.import_app(self.app_uri)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/util.py&quot;, line 358, in import_app
    mod = importlib.import_module(module)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/importlib/__init__.py&quot;, line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1006, in _gcd_import
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 983, in _find_and_load
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 967, in _find_and_load_unlocked
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 677, in _load_unlocked
  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 728, in exec_module
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/models/container/scoring_server/wsgi.py&quot;, line 3, in &lt;module&gt;
    app = scoring_server.init(pyfunc.load_model(&quot;/opt/ml/model/&quot;))
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/pyfunc/__init__.py&quot;, line 292, in load_model
    return importlib.import_module(conf[MAIN])._load_pyfunc(data_path)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/sklearn.py&quot;, line 219, in _load_pyfunc
    return _load_model_from_local_file(path)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/sklearn.py&quot;, line 206, in _load_model_from_local_file
    with open(path, &quot;rb&quot;) as f:
IsADirectoryError: [Errno 21] Is a directory: '/opt/ml/model'
[2020-07-27 13:02:13 +0000] [829] [INFO] Worker exiting (pid: 829)
[2020-07-27 13:02:13 +0000] [830] [ERROR] Exception in worker process
Traceback (most recent call last):
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/arbiter.py&quot;, line 583, in spawn_worker
    worker.init_process()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/workers/ggevent.py&quot;, line 162, in init_process
    super().init_process()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/workers/base.py&quot;, line 119, in init_process
    self.load_wsgi()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/workers/base.py&quot;, line 144, in load_wsgi
    self.wsgi = self.app.wsgi()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/app/base.py&quot;, line 67, in wsgi
    self.callable = self.load()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/app/wsgiapp.py&quot;, line 49, in load
    return self.load_wsgiapp()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/app/wsgiapp.py&quot;, line 39, in load_wsgiapp
    return util.import_app(self.app_uri)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/util.py&quot;, line 358, in import_app
    mod = importlib.import_module(module)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/importlib/__init__.py&quot;, line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1006, in _gcd_import
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 983, in _find_and_load
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 967, in _find_and_load_unlocked
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 677, in _load_unlocked
  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 728, in exec_module
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/models/container/scoring_server/wsgi.py&quot;, line 3, in &lt;module&gt;
    app = scoring_server.init(pyfunc.load_model(&quot;/opt/ml/model/&quot;))
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/pyfunc/__init__.py&quot;, line 292, in load_model
    return importlib.import_module(conf[MAIN])._load_pyfunc(data_path)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/sklearn.py&quot;, line 219, in _load_pyfunc
    return _load_model_from_local_file(path)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/sklearn.py&quot;, line 206, in _load_model_from_local_file
    with open(path, &quot;rb&quot;) as f:
IsADirectoryError: [Errno 21] Is a directory: '/opt/ml/model'
[2020-07-27 13:02:13 +0000] [830] [INFO] Worker exiting (pid: 830)
Traceback (most recent call last):
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/arbiter.py&quot;, line 209, in run
    self.sleep()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/arbiter.py&quot;, line 357, in sleep
    ready = select.select([self.PIPE[0]], [], [], 1.0)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/arbiter.py&quot;, line 242, in handle_chld
    self.reap_workers()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/arbiter.py&quot;, line 525, in reap_workers
    raise HaltServer(reason, self.WORKER_BOOT_ERROR)
gunicorn.errors.HaltServer: &lt;HaltServer 'Worker failed to boot.' 3&gt;

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;/miniconda/envs/custom_env/bin/gunicorn&quot;, line 8, in &lt;module&gt;
    sys.exit(run())
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/app/wsgiapp.py&quot;, line 58, in run
    WSGIApplication(&quot;%(prog)s [OPTIONS] [APP_MODULE]&quot;).run()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/app/base.py&quot;, line 228, in run
    super().run()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/app/base.py&quot;, line 72, in run
    Arbiter(self).run()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/arbiter.py&quot;, line 229, in run
    self.halt(reason=inst.reason, exit_status=inst.exit_status)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/arbiter.py&quot;, line 342, in halt
    self.stop()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/arbiter.py&quot;, line 393, in stop
    time.sleep(0.1)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/arbiter.py&quot;, line 242, in handle_chld
    self.reap_workers()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/arbiter.py&quot;, line 525, in reap_workers
    raise HaltServer(reason, self.WORKER_BOOT_ERROR)
gunicorn.errors.HaltServer: &lt;HaltServer 'Worker failed to boot.' 3&gt;
creating and activating custom environment
Got sigterm signal, exiting.
[2020-07-27 13:02:13 +0000] [831] [ERROR] Exception in worker process
Traceback (most recent call last):
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/arbiter.py&quot;, line 583, in spawn_worker
    worker.init_process()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/workers/ggevent.py&quot;, line 162, in init_process
    super().init_process()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/workers/base.py&quot;, line 119, in init_process
    self.load_wsgi()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/workers/base.py&quot;, line 144, in load_wsgi
    self.wsgi = self.app.wsgi()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/app/base.py&quot;, line 67, in wsgi
    self.callable = self.load()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/app/wsgiapp.py&quot;, line 49, in load
    return self.load_wsgiapp()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/app/wsgiapp.py&quot;, line 39, in load_wsgiapp
    return util.import_app(self.app_uri)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/util.py&quot;, line 358, in import_app
    mod = importlib.import_module(module)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/importlib/__init__.py&quot;, line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1006, in _gcd_import
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 983, in _find_and_load
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 967, in _find_and_load_unlocked
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 677, in _load_unlocked
  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 728, in exec_module
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/models/container/scoring_server/wsgi.py&quot;, line 3, in &lt;module&gt;
    app = scoring_server.init(pyfunc.load_model(&quot;/opt/ml/model/&quot;))
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/pyfunc/__init__.py&quot;, line 292, in load_model
    return importlib.import_module(conf[MAIN])._load_pyfunc(data_path)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/sklearn.py&quot;, line 219, in _load_pyfunc
    return _load_model_from_local_file(path)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/sklearn.py&quot;, line 206, in _load_model_from_local_file
    with open(path, &quot;rb&quot;) as f:
IsADirectoryError: [Errno 21] Is a directory: '/opt/ml/model'
[2020-07-27 13:02:13 +0000] [831] [INFO] Worker exiting (pid: 831)
[2020-07-27 13:02:14 +0000] [833] [ERROR] Exception in worker process
Traceback (most recent call last):
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/arbiter.py&quot;, line 583, in spawn_worker
    worker.init_process()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/workers/ggevent.py&quot;, line 162, in init_process
    super().init_process()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/workers/base.py&quot;, line 119, in init_process
    self.load_wsgi()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/workers/base.py&quot;, line 144, in load_wsgi
    self.wsgi = self.app.wsgi()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/app/base.py&quot;, line 67, in wsgi
    self.callable = self.load()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/app/wsgiapp.py&quot;, line 49, in load
    return self.load_wsgiapp()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/app/wsgiapp.py&quot;, line 39, in load_wsgiapp
    return util.import_app(self.app_uri)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/util.py&quot;, line 358, in import_app
    mod = importlib.import_module(module)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/importlib/__init__.py&quot;, line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1006, in _gcd_import
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 983, in _find_and_load
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 967, in _find_and_load_unlocked
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 677, in _load_unlocked
  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 728, in exec_module
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/models/container/scoring_server/wsgi.py&quot;, line 3, in &lt;module&gt;
    app = scoring_server.init(pyfunc.load_model(&quot;/opt/ml/model/&quot;))
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/pyfunc/__init__.py&quot;, line 292, in load_model
    return importlib.import_module(conf[MAIN])._load_pyfunc(data_path)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/sklearn.py&quot;, line 219, in _load_pyfunc
    return _load_model_from_local_file(path)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/sklearn.py&quot;, line 206, in _load_model_from_local_file
    with open(path, &quot;rb&quot;) as f:
IsADirectoryError: [Errno 21] Is a directory: '/opt/ml/model'
[2020-07-27 13:02:14 +0000] [833] [INFO] Worker exiting (pid: 833)
[2020-07-27 13:02:14 +0000] [832] [ERROR] Exception in worker process
Traceback (most recent call last):
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/arbiter.py&quot;, line 583, in spawn_worker
    worker.init_process()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/workers/ggevent.py&quot;, line 162, in init_process
    super().init_process()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/workers/base.py&quot;, line 119, in init_process
    self.load_wsgi()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/workers/base.py&quot;, line 144, in load_wsgi
    self.wsgi = self.app.wsgi()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/app/base.py&quot;, line 67, in wsgi
    self.callable = self.load()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/app/wsgiapp.py&quot;, line 49, in load
    return self.load_wsgiapp()
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/app/wsgiapp.py&quot;, line 39, in load_wsgiapp
    return util.import_app(self.app_uri)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/gunicorn/util.py&quot;, line 358, in import_app
    mod = importlib.import_module(module)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/importlib/__init__.py&quot;, line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1006, in _gcd_import
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 983, in _find_and_load
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 967, in _find_and_load_unlocked
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 677, in _load_unlocked
  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 728, in exec_module
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/models/container/scoring_server/wsgi.py&quot;, line 3, in &lt;module&gt;
    app = scoring_server.init(pyfunc.load_model(&quot;/opt/ml/model/&quot;))
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/pyfunc/__init__.py&quot;, line 292, in load_model
    return importlib.import_module(conf[MAIN])._load_pyfunc(data_path)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/sklearn.py&quot;, line 219, in _load_pyfunc
    return _load_model_from_local_file(path)
  File &quot;/miniconda/envs/custom_env/lib/python3.7/site-packages/mlflow/sklearn.py&quot;, line 206, in _load_model_from_local_file
    with open(path, &quot;rb&quot;) as f:
IsADirectoryError: [Errno 21] Is a directory: '/opt/ml/model'
[2020-07-27 13:02:14 +0000] [832] [INFO] Worker exiting (pid: 832)
</code></pre>",1,1,2020-07-27 13:26:19.06 UTC,,,2,amazon-sagemaker|mlflow,429,2020-06-05 19:27:43.857 UTC,2020-11-19 23:45:52.31 UTC,,21,0,0,1,,,,,,"['amazon-sagemaker', 'mlflow']"
Can't edit the cluster created by mlflow model serving,"<p>I'm trying to deploy  Machine learning model into databricks production using mlflow. while in that process, I have registered the model to mlflow models. After that it created the cluster but then it was in pending state forever. when I checked the model events, I see a problem with https proxy, we have global init scripts which contain proxy information.</p>
<p><a href=""https://i.stack.imgur.com/qF9MT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qF9MT.png"" alt=""enter image description here"" /></a></p>
<p>Ref: <a href=""https://docs.databricks.com/applications/mlflow/model-serving.html"" rel=""nofollow noreferrer"">https://docs.databricks.com/applications/mlflow/model-serving.html</a></p>
<p>so the only way for us to edit the cluster and add them but in that process we are getting an error &quot;error: Cannot edit cluster created by ModelServing&quot;.</p>
<pre><code>[Errno 101] Network is unreachable',)': /simple/mlflow/ WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('&lt;pip._vendor.urllib3.connection.HTTPSConnection object at 0x7f258247f710&gt;: Failed to establish a new connection:
</code></pre>
<p>In the &quot;Model Events page&quot;, I see the above logs,</p>",1,0,2022-05-10 23:07:09.1 UTC,,,0,databricks|azure-databricks|mlflow,125,2016-11-15 06:12:07.737 UTC,2022-08-15 17:25:10.4 UTC,"R G U K T , basar, Andhra Pradesh, India",2470,265,22,251,,,,,,"['databricks', 'mlflow']"
Data bricks:- Cannot display the predicted output by using ml flow registered model,"<p>I have created a model using diabetes dataset for prediction. I have trained, evaluated, logged and registered it as a new model in ML flow. Now I am trying to load the registered model and trying to predict on new data. All though I was able to predict the results. I am not able to display it. When I try to display using command <code>.show()</code> or <code>display()</code> it is throwing an error. What is the cause of the error? and How do I display the results?</p>
<p>Note: I have programmed using pure pyspark and all the ML flow operation was done on Data bricks</p>
<p>Code:-</p>
<pre><code>model_details = mlflow.tracking.MlflowClient().get_latest_versions('model1',stages=['staging'])[0]
model = mlflow.pyfunc.spark_udf(spark,model_details.source)
input_df = sdf.drop('progression')
columns = list(map(lambda c: f&quot;{c}&quot;, input_df.columns))
df = input_df.withColumn(&quot;progression&quot;, model(*columns))
df.show(truncate=False)
</code></pre>
<p>Error :-</p>
<pre><code>PythonException: An exception was thrown from a UDF: 'Exception: Java gateway process exited before sending its port number'. Full traceback below:
PythonException                           Traceback (most recent call last)
&lt;command-1343735193245452&gt; in &lt;module&gt;
     34 df = input_df.withColumn(&quot;progression&quot;, model(*columns))
     35 
---&gt; 36 df.show(truncate=False)

/databricks/spark/python/pyspark/sql/dataframe.py in show(self, n, truncate, vertical)
    441             print(self._jdf.showString(n, 20, vertical))
    442         else:
--&gt; 443             print(self._jdf.showString(n, int(truncate), vertical))
    444 
    445     def __repr__(self):

/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py in __call__(self, *args)
   1303         answer = self.gateway_client.send_command(command)
   1304         return_value = get_return_value(
-&gt; 1305             answer, self.gateway_client, self.target_id, self.name)
   1306 
   1307         for temp_arg in temp_args:

/databricks/spark/python/pyspark/sql/utils.py in deco(*a, **kw)
    131                 # Hide where the exception came from that shows a non-Pythonic
    132                 # JVM exception message.
--&gt; 133                 raise_from(converted)
    134             else:
    135                 raise

/databricks/spark/python/pyspark/sql/utils.py in raise_from(e)

PythonException: An exception was thrown from a UDF: 'Exception: Java gateway process exited before sending its port number'. Full traceback below:
Traceback (most recent call last):
  File &quot;/databricks/spark/python/pyspark/worker.py&quot;, line 654, in main
    process()
  File &quot;/databricks/spark/python/pyspark/worker.py&quot;, line 646, in process
    serializer.dump_stream(out_iter, outfile)
  File &quot;/databricks/spark/python/pyspark/sql/pandas/serializers.py&quot;, line 281, in dump_stream
    timely_flush_timeout_ms=self.timely_flush_timeout_ms)
  File &quot;/databricks/spark/python/pyspark/sql/pandas/serializers.py&quot;, line 97, in dump_stream
    for batch in iterator:
  File &quot;/databricks/spark/python/pyspark/sql/pandas/serializers.py&quot;, line 271, in init_stream_yield_batches
    for series in iterator:
  File &quot;/databricks/spark/python/pyspark/worker.py&quot;, line 467, in mapper
    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)
  File &quot;/databricks/spark/python/pyspark/worker.py&quot;, line 467, in &lt;genexpr&gt;
    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)
  File &quot;/databricks/spark/python/pyspark/worker.py&quot;, line 111, in &lt;lambda&gt;
    verify_result_type(f(*a)), len(a[0])), arrow_return_type)
  File &quot;/databricks/spark/python/pyspark/util.py&quot;, line 109, in wrapper
    return f(*args, **kwargs)
  File &quot;/databricks/python/lib/python3.7/site-packages/mlflow/pyfunc/__init__.py&quot;, line 827, in predict
    model = SparkModelCache.get_or_load(archive_path)
  File &quot;/databricks/python/lib/python3.7/site-packages/mlflow/pyfunc/spark_model_cache.py&quot;, line 64, in get_or_load
    SparkModelCache._models[archive_path] = load_pyfunc(temp_dir)
  File &quot;/databricks/python/lib/python3.7/site-packages/mlflow/utils/annotations.py&quot;, line 43, in deprecated_func
    return func(*args, **kwargs)
  File &quot;/databricks/python/lib/python3.7/site-packages/mlflow/pyfunc/__init__.py&quot;, line 693, in load_pyfunc
    return load_model(model_uri, suppress_warnings)
  File &quot;/databricks/python/lib/python3.7/site-packages/mlflow/pyfunc/__init__.py&quot;, line 667, in load_model
    model_impl = importlib.import_module(conf[MAIN])._load_pyfunc(data_path)
  File &quot;/databricks/python/lib/python3.7/site-packages/mlflow/spark.py&quot;, line 707, in _load_pyfunc
    .master(&quot;local[1]&quot;)
  File &quot;/databricks/spark/python/pyspark/sql/session.py&quot;, line 189, in getOrCreate
    sc = SparkContext.getOrCreate(sparkConf)
  File &quot;/databricks/spark/python/pyspark/context.py&quot;, line 384, in getOrCreate
    SparkContext(conf=conf or SparkConf())
  File &quot;/databricks/spark/python/pyspark/context.py&quot;, line 134, in __init__
    SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)
  File &quot;/databricks/spark/python/pyspark/context.py&quot;, line 333, in _ensure_initialized
    SparkContext._gateway = gateway or launch_gateway(conf)
  File &quot;/databricks/spark/python/pyspark/java_gateway.py&quot;, line 105, in launch_gateway
    raise Exception(&quot;Java gateway process exited before sending its port number&quot;)
Exception: Java gateway process exited before sending its port number
</code></pre>",0,0,2021-09-14 10:18:11.113 UTC,,,1,pyspark|apache-spark-sql|user-defined-functions|databricks|mlflow,168,2021-08-10 12:49:57.537 UTC,2021-11-22 17:00:30 UTC,,111,12,0,18,,,,,,"['databricks', 'mlflow']"
I am trying to serve a custom function as a model using ML Flow in Databricks,"<p>Below is my source code:-</p>
<pre><code>import mlflow
import mlflow.sklearn
import numpy as np
import pandas as pd
from sklearn.datasets import load_iris
from sklearn import tree
from sklearn.preprocessing import FunctionTransformer
from sklearn.pipeline import Pipeline
from mlflow.models.signature import infer_signature

import mlflow.pyfunc


class AddN(mlflow.pyfunc.PythonModel):

    def __init__(self, n):
        self.n = n
    def fit(self, X, y=None):
        return self
    def transform(self, X, y=None):
        return self
    def predict(model_input):
        return model_input.apply(lambda column: column + self.n)


model_path = &quot;add_n_model15&quot;
add5_model = AddN(n=5)
#mlflow.pyfunc.save_model(path=model_path, python_model=add5_model)


loaded_model = mlflow.pyfunc.load_model(model_path)


import pandas as pd
#model_input = pd.DataFrame([range(10)])
#model_output = loaded_model.predict(model_input)


data={'a':1,'b':2,'c':3,'d':4}
model_input = pd.DataFrame(data, index=[0])
loaded_model.predict(model_input)




pipeline = Pipeline(steps=[(&quot;model&quot;, AddN)])

with mlflow.start_run():
    
    # log model
    signature = infer_signature(model_input, model_output)
    mlflow.sklearn.log_model(pipeline, &quot;trial&quot;, signature=signature)
</code></pre>
<p>`
After registering the model and serving it in databricks, when i call the model through model serving interface of databricks using the below input Request, i get an error:-</p>
<p>Input Request:-
[{&quot;a&quot;:20,&quot;b&quot;:21,&quot;c&quot;:22,&quot;d&quot;:23}]</p>
<p>Error:-</p>
<p>BAD_REQUEST: Encountered an unexpected error while evaluating the model. Verify that the serialized input Dataframe is compatible with the model for inference.</p>
<p>Traceback (most recent call last):
File &quot;/databricks/conda/envs/model-1/lib/python3.8/site-packages/mlflow/pyfunc/scoring_server/<strong>init</strong>.py&quot;, line 306, in transformation
raw_predictions = model.predict(data)
File &quot;/databricks/conda/envs/model-1/lib/python3.8/site-packages/mlflow/pyfunc/<strong>init</strong>.py&quot;, line 612, in predict
return self._model_impl.predict(data)
File &quot;/databricks/conda/envs/model-1/lib/python3.8/site-packages/sklearn/utils/metaestimators.py&quot;, line 120, in 
out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)
File &quot;/databricks/conda/envs/model-1/lib/python3.8/site-packages/sklearn/pipeline.py&quot;, line 419, in predict
return self.steps[-1][-1].predict(Xt, **predict_params)
TypeError: predict() missing 2 required positional arguments: 'context' and 'model_input'
`</p>",1,0,2022-03-29 23:09:04.073 UTC,,,1,python|scikit-learn|databricks|mlflow,544,2022-03-29 22:59:11.337 UTC,2022-08-16 04:06:57.97 UTC,,11,0,0,0,,,,,,"['databricks', 'mlflow']"
How to load a keras model (.h5 file) from local to Azure databricks workspace,"<p>I have a keras model created on my local machine and I saved it as a model.h5 format. Now how do I load this model into my workspace on Azure databricks and import inside a databricks notebook and use the model?</p>
<p>trying the below URL but not successful, seems like its useful if and only if you save the model from databricks notebook using mlFlow and load it back under databricks using mlFlow:</p>
<p><a href=""https://www.mlflow.org/docs/latest/python_api/mlflow.keras.html"" rel=""nofollow noreferrer"">https://www.mlflow.org/docs/latest/python_api/mlflow.keras.html</a></p>
<p>what if I have a keras model created in my local machine, how do I go ahead for importing?, please help.</p>",1,0,2020-08-03 12:50:32.557 UTC,,,0,keras|databricks|mlflow,692,2016-11-18 20:17:30.887 UTC,2022-01-28 17:12:45.737 UTC,,9,0,0,7,,,,,,"['databricks', 'mlflow']"
Why I got an invalid bucket name error using dvc mlflow on macos,"<p>Could anyone tell what's the reason for error:</p>
<p>botocore.exceptions.ParamValidationError: Parameter validation failed:
Invalid bucket name &quot;&quot;: Bucket name must match the regex &quot;^[a-zA-Z0-9.-_]{1,255}$&quot; or be an ARN matching the regex &quot;^arn:(aws).<em>:(s3|s3-object-lambda):[a-z-0-9]</em>:[0-9]{12}:accesspoint[/:][a-zA-Z0-9-.]{1,63}$|^arn:(aws).*:s3-outposts:[a-z-0-9]+:[0-9]{12}:outpost[/:][a-zA-Z0-9-]{1,63}[/:]accesspoint[/:][a-zA-Z0-9-]{1,63}$&quot;</p>
<p>I try to use mlflow with docker.
.env file contains:</p>
<pre><code>AWS_ACCESS_KEY_ID=...
AWS_SECRET_ACCESS_KEY=...
AWS_S3_BUCKET=vla...rts
MLFLOW_S3_ENDPOINT_URL=http://localhost:9000
MLFLOW_TRACKING_URI=http://127.0.0.1:5000
POSTGRES_USER=...
POSTGRES_PASSWORD=...
POSTGRES_DB=test_db
</code></pre>
<p>Also tried to use:</p>
<pre><code>AWS_ACCESS_KEY_ID=...
AWS_SECRET_ACCESS_KEY=...
AWS_S3_BUCKET=vla...rts
MLFLOW_S3_ENDPOINT_URL=http://localhost:9000
MLFLOW_TRACKING_URI=http://localhost:5000
POSTGRES_USER=...
POSTGRES_PASSWORD=...
POSTGRES_DB=test_db
</code></pre>
<p>docker-compose contains:</p>
<pre><code>... 
   mlflow:
        restart: always
        image: mlflow_server
        container_name: mlflow_server
        ports:
          - &quot;5000:5000&quot;
        networks:
          - postgres
          - s3
        environment:
          - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
          - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
          - MLFLOW_S3_ENDPOINT_URL=http://nginx:9000
        command: mlflow server --backend-store-uri postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@db/${POSTGRES_DB} --default-artifact-root s3://${AWS_S3_BUCKET}/ --host 0.0.0.0
...
</code></pre>
<p>As I understood, I get an exception cause bucket name is empty (&quot;&quot;). But in .env file I set bucket name as <code>vla...rts</code></p>",2,4,2022-05-21 21:10:53.863 UTC,,2022-05-24 06:16:27.987 UTC,0,docker|mlflow|mlops|dvc,117,2019-12-29 15:05:47.33 UTC,2022-09-20 12:21:31.067 UTC,Saint Petersburg,21,0,0,8,,,,,,"['dvc', 'mlflow']"
MLflow 1.2.0 define MLproject file,"<p>Trying to run mlflow run by specifying MLproject and code which lives in a different location as MLproject file.</p>

<p>I have the following directory structure:</p>

<pre><code>/root/mflow_test
.
├── conda
│   ├── conda.yaml
│   └── MLproject
├── docker
│   ├── Dockerfile
│   └── MLproject
├── README.md
├── requirements.txt
└── trainer
    ├── __init__.py
    ├── task.py
    └── utils.py
</code></pre>

<p>When I'm run from: <code>/root/</code></p>

<pre><code>mlflow run mlflow_test/docker
</code></pre>

<p>I get:</p>

<pre><code>/root/miniconda3/bin/python: Error while finding module specification for 'trainer.task' (ImportError: No module named 'trainer')
</code></pre>

<p>Since my <code>MLproject</code> file can't find the Python code.
I moved MLproject to <code>mflow_test</code> and this works fine.</p>

<p>This is my MLproject entry point:</p>

<pre><code>name: mlflow_sample
docker_env:
  image: mlflow-docker-sample
entry_points:
  main:
    parameters:
      job_dir:
        type: string
        default: '/tmp/'
    command: |
        python -m trainer.task --job-dir {job_dir}
</code></pre>

<p>How can I run <code>mlflow run</code> and pass the MLproject and ask it to look in a different folder?</p>

<p>I tried:</p>

<pre><code>""cd .. &amp;&amp; python -m trainer.task --job-dir {job_dir}"" 
</code></pre>

<p>and I get:</p>

<p><code>/entrypoint.sh: line 5: exec: cd: not found</code></p>

<p><strong>Dockerfile</strong></p>

<pre><code># docker build -t mlflow-gcp-example -f Dockerfile .
FROM gcr.io/deeplearning-platform-release/tf-cpu 
RUN git clone github.com/GoogleCloudPlatform/ml-on-gcp.git 
WORKDIR ml-on-gcp/tutorials/tensorflow/mlflow_gcp 
RUN pip install -r requirements.txt 
</code></pre>",0,7,2019-08-19 04:02:46.53 UTC,,2019-08-19 05:51:16.957 UTC,0,docker|databricks|mlflow,660,2010-01-28 09:42:15.677 UTC,2022-09-25 05:06:35.287 UTC,"San Francisco, CA",8619,1916,102,1286,,,,,,"['databricks', 'mlflow']"
sagemaker endpoint invocation randomly throws error,"<p>Env:</p>

<ul>
<li>XGBoost model trained on static data (excel).</li>
<li>Model Saved and deployed to Sagemaker with MLFlow.</li>
<li>Sagemaker Model and Sagemaker Endpoint are running.</li>
</ul>

<p>Invocation:
 - I invoke the model with new data via REST Request (POSTMAN) and via boto3.sagemaker</p>

<pre><code>client.invoke_endpoint(
    EndpointName=""xxxxxxx"",
    Body=data,
    ContentType=""application/json"",
    Accept=""string"",
)
</code></pre>

<p>Problem:
It seems that Sagemaker randomly (~50% of the time) fails with following Exception:</p>

<pre><code>{
""ErrorCode"": ""CLIENT_ERROR_FROM_MODEL"",
""LogStreamArn"": ""arn:aws:logs:eu-central-1:xxxxxxxxxx:log-group:/aws/sagemaker/Endpoints/xxxxxxxx"",
""Message"": ""Received client error (400) from mfs-xxxxxxxxx-model-dztwabjotscyoc-zx7lzyfg with message \""{\""error_code\"": \""BAD_REQUEST\"", \""message\"": \""Encountered an unexpected error while evaluating the model. Verify that the serialized input Dataframe is compatible with the model for inference.\"", \""stack_trace\"": \""Traceback (most recent call last):\\n  File \\\""/miniconda/envs/custom_env/lib/python3.6/site-packages/mlflow/pyfunc/scoring_server/__init__.py\\\"", line 196, in transformation\\n    raw_predictions = model.predict(data)\\n  File \\\""/miniconda/envs/custom_env/lib/python3.6/site-packages/mlflow/xgboost.py\\\"", line 198, in predict\\n    return self.xgb_model.predict(xgb.DMatrix(dataframe))\\n  File \\\""/miniconda/envs/custom_env/lib/python3.6/site-packages/xgboost/core.py\\\"", line 1443, in predict\\n    self._validate_features(data)\\n  File \\\""/miniconda/envs/custom_env/lib/python3.6/site-packages/xgboost/core.py\\\"", line 1862, in _validate_features\\n    data.feature_names))\\nValueError: feature_names mismatch: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81'] ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85']\\ntraining data did not have the following fields: 83, 85, 84, 82\\n\""}\"". See https://eu-central-1.console.aws.amazon.com/cloudwatch/home?region=eu-central-1#logEventViewer:group=/aws/sagemaker/Endpoints/xxxxxxxxx in account xxxxxxxfor more information."",
""OriginalMessage"": ""{\""error_code\"": \""BAD_REQUEST\"", \""message\"": \""Encountered an unexpected error while evaluating the model. Verify that the serialized input Dataframe is compatible with the model for inference.\"", \""stack_trace\"": \""Traceback (most recent call last):\\n  File \\\""/miniconda/envs/custom_env/lib/python3.6/site-packages/mlflow/pyfunc/scoring_server/__init__.py\\\"", line 196, in transformation\\n    raw_predictions = model.predict(data)\\n  File \\\""/miniconda/envs/custom_env/lib/python3.6/site-packages/mlflow/xgboost.py\\\"", line 198, in predict\\n    return self.xgb_model.predict(xgb.DMatrix(dataframe))\\n  File \\\""/miniconda/envs/custom_env/lib/python3.6/site-packages/xgboost/core.py\\\"", line 1443, in predict\\n    self._validate_features(data)\\n  File \\\""/miniconda/envs/custom_env/lib/python3.6/site-packages/xgboost/core.py\\\"", line 1862, in _validate_features\\n    data.feature_names))\\nValueError: feature_names mismatch: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81'] ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85']\\ntraining data did not have the following fields: 83, 85, 84, 82\\n\""}"",
""OriginalStatusCode"": 400
</code></pre>

<p>}</p>

<p>After this Exception, i hit again ""Send Request"" in Postman (with the exact same Body Data) and get a successful response:</p>

<pre><code>[
0.9989840388298035
]
</code></pre>

<p>The problem is exactly the same (randomly) when using boto3.sagemaker either in AWS Lambda or locally when testing.</p>

<p><strong>UPDATE 1:</strong>
When i load the exported function with mlflow.xgboost.load_model() and run the prediction, it works everytime. Here is the code:</p>

<pre class=""lang-py prettyprint-override""><code>    import mlflow
    from mlflow import xgboost
    from xgboost import DMatrix
    import numpy as np
    import pandas as pd
    base_path = ""src/models/""


    model_path_name_a = f""{base_path}/model_a""
    model_path_name_b = f""{base_path}/model_b""
    # xgboost.log_model(xgb_model_a, artifact_path='https://mlflow-server-tracking.s3.eu-central-1.amazonaws.com')
    xgb_model_a = xgboost.load_model(model_path_name_a)
    xgb_model_b = xgboost.load_model(model_path_name_b)

    X_test = DMatrix(
        np.array(
            [
                [7.60000e+01, 2.57000e+02, 9.25200e+03, 2.00400e+03, 0.00000e+00, 1.00000e+00, 0.00000e+00, 0.00000e+00,
                 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,
                 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,
                 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,
                 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,
                 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,
                 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,
                 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,
                 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,
                 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 1.00000e+00, 0.00000e+00, 0.00000e+00,
                 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00]
            ]
        )
    )


    ############# Prediction Model A ################
    y_predA = xgb_model_a.predict(X_test)
    print(f""y_predA: {y_predA}"")

    ############# Prediction Model B ################
    y_predB = xgb_model_b.predict(X_test)
    print(f""y_predA: {y_predB}"")
</code></pre>

<p>Does that mean that the problem is with Sagemaker Inference/Endpoint Service?</p>",0,5,2020-04-15 09:13:53.927 UTC,,2020-04-15 11:20:45.103 UTC,0,python|amazon-web-services|xgboost|amazon-sagemaker|mlflow,558,2017-02-19 13:12:04.9 UTC,2022-08-26 11:25:49.307 UTC,"Nuremberg, Germany",177,227,5,82,,,,,,"['amazon-sagemaker', 'mlflow']"
upload a pre-trained model locally into databricks,"<p>Is it possible to upload a pre-trained machine learning model that was trained on a different environment on databricks, and serve it? Or is it impossible on Databricks ?</p>",1,6,2022-07-11 07:50:31.5 UTC,,2022-07-11 11:58:05.88 UTC,1,machine-learning|databricks|mlflow,70,2019-03-03 11:21:20.527 UTC,2022-09-23 12:01:19.95 UTC,France,314,18,2,105,,,,,,"['databricks', 'mlflow']"
Getting `dtype of input object does not match expected dtype <U0` when invoking MLflow-deployed NLP model in SageMaker,"<p>I deployed a Huggingface Transformer model in SageMaker using MLflow's <code>sagemaker.deploy()</code>.</p>
<p>When logging the model I used <code>infer_signature(np.array(test_example), loaded_model.predict(test_example))</code> to infer input and output signatures.</p>
<p>Model is deployed successfully. When trying to query the model I get <code>ModelError</code> (full traceback below).</p>
<p>To query the model, I am using precisely the same <code>test_example</code> that I used for <code>infer_signature()</code>:</p>
<p><code>test_example = [['This is the subject', 'This is the body']]</code></p>
<p>The only difference is that when querying the deployed model, I am not wrapping the test example in <code>np.array()</code> as that is not <code>json</code>-serializeable.</p>
<p>To query the model I tried two different approaches:</p>
<pre class=""lang-py prettyprint-override""><code>import boto3

SAGEMAKER_REGION = 'us-west-2'
MODEL_NAME = '...'

client = boto3.client(&quot;sagemaker-runtime&quot;, region_name=SAGEMAKER_REGION)

# Approach 1
client.invoke_endpoint(
                EndpointName=MODEL_NAME,
                Body=json.dumps(test_example),
                ContentType=&quot;application/json&quot;,
            )

# Approach 2
client.invoke_endpoint(
                EndpointName=MODEL_NAME,
                Body=pd.DataFrame(test_example).to_json(orient=&quot;split&quot;),
                ContentType=&quot;application/json; format=pandas-split&quot;,
            )
</code></pre>
<p>but they result in the same error.</p>
<p>Will be grateful for your suggestions.</p>
<p>Thank you!</p>
<p>Note: I am using Python 3 and all <strong>strings are unicode</strong>.</p>
<pre class=""lang-py prettyprint-override""><code>---------------------------------------------------------------------------
ModelError                                Traceback (most recent call last)
&lt;ipython-input-89-d09862a5f494&gt; in &lt;module&gt;
      2                 EndpointName=MODEL_NAME,
      3                 Body=test_example,
----&gt; 4                 ContentType=&quot;application/json; format=pandas-split&quot;,
      5             )

~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/botocore/client.py in _api_call(self, *args, **kwargs)
    393                     &quot;%s() only accepts keyword arguments.&quot; % py_operation_name)
    394             # The &quot;self&quot; in this scope is referring to the BaseClient.
--&gt; 395             return self._make_api_call(operation_name, kwargs)
    396 
    397         _api_call.__name__ = str(py_operation_name)

~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/botocore/client.py in _make_api_call(self, operation_name, api_params)
    723             error_code = parsed_response.get(&quot;Error&quot;, {}).get(&quot;Code&quot;)
    724             error_class = self.exceptions.from_code(error_code)
--&gt; 725             raise error_class(parsed_response, operation_name)
    726         else:
    727             return parsed_response

ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from primary with message &quot;{&quot;error_code&quot;: &quot;BAD_REQUEST&quot;, &quot;message&quot;: &quot;dtype of input object does not match expected dtype &lt;U0&quot;}&quot;. See https://us-west-2.console.aws.amazon.com/cloudwatch/home?region=us-west-2#logEventViewer:group=/aws/sagemaker/Endpoints/bec-sagemaker-model-test-app in account 543052680787 for more information.
</code></pre>
<p>Environment info:</p>
<pre class=""lang-py prettyprint-override""><code>{'channels': ['defaults', 'conda-forge', 'pytorch'],
 'dependencies': ['python=3.6.10',
  'pip==21.3.1',
  'pytorch=1.10.2',
  'cudatoolkit=10.2',
  {'pip': ['mlflow==1.22.0',
    'transformers==4.17.0',
    'datasets==1.18.4',
    'cloudpickle==1.3.0']}],
 'name': 'bert_bec_test_env'}
</code></pre>",1,0,2022-03-09 11:56:29.87 UTC,,2022-03-09 14:44:47.963 UTC,0,amazon-web-services|nlp|amazon-sagemaker|mlflow,61,2017-03-23 13:26:01.927 UTC,2022-09-22 20:27:37.72 UTC,Tel Aviv,83,30,0,56,,,,,,"['amazon-sagemaker', 'mlflow']"
UI does display data in MLflow,"<p>This is in reference to rather comment (not answer), I added here: <a href=""https://stackoverflow.com/questions/63255631/mlflow-invalid-parameter-value-unsupported-uri-mlruns-for-model-registry-s/66371465#66371465"">MLflow: INVALID_PARAMETER_VALUE: Unsupported URI './mlruns' for model registry store</a></p>
<p>I extracted files from <a href=""https://github.com/mlflow/mlflow/tree/master/examples/sklearn_elasticnet_wine"" rel=""nofollow noreferrer"">here</a></p>
<pre><code>train.py     MLproject     wine-quality.csv 
</code></pre>
<p>These are in directory:<code>feb24MLFLOW</code></p>
<p>I am in directory <code>feb24MLFLOW</code> with following contents</p>
<pre><code>:memory          mlruns           train.py         wine-quality.csv
</code></pre>
<p>When I run following command</p>
<pre><code>mlflow server --backend-store-uri sqlite:///:memory --default-artifact-root ./mlruns
</code></pre>
<p><strong>The UI loads but does not show any data in it neigther does database as below. see screenshot.</strong>
<a href=""https://i.stack.imgur.com/j3XSe.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/j3XSe.png"" alt=""enter image description here"" /></a>
I am using <code>--default-artifact-root ./mlruns</code> flag because, when I print  print(mlflow.get_tracking_uri()), I get the current directory</p>
<pre><code>file:///&lt;mydirectorylocations&gt;/feb24MLFLOW/mlruns
</code></pre>
<p>For some reason I see my database is not updating (or inserting). I checked that with in terminal.</p>
<pre><code>$  sqlite3
sqlite&gt; .open :memory 
sqlite&gt; .tables
alembic_version        metrics                registered_model_tags
experiment_tags        model_version_tags     registered_models    
experiments            model_versions         runs                 
latest_metrics         params                 tags                 
sqlite&gt; select * from runs;
sqlite&gt; 
</code></pre>
<p>As you can see there is no data after running <code>select * from runs</code> above.</p>
<p>Please note that I have following contents in</p>
<pre><code>./mlruns

d6db5cf1443d49c19971a1b8b606d692 meta.yaml

Can somebody suggest I show results in the UI? or insert in databse? or what am I doing wrong? 
</code></pre>
<p>Please note that when I run <code>mlflow ui</code>, I see data in the UI but I get:</p>
<pre><code>error_code: &quot;INVALID_PARAMETER_VALUE&quot;
message: &quot; Model registry functionality is unavailable; got unsupported URI './mlruns' for model registry data storage. Supported URI schemes are: ['postgresql', 'mysql', 'sqlite', 'mssql']. See https://www.mlflow.org/docs/latest/tracking.html#storage for how to run an MLflow server against one of the supported backend storage locations.&quot; 
</code></pre>",0,5,2021-02-25 15:35:45.17 UTC,,2021-02-26 18:02:39.173 UTC,2,databricks|mlflow,1085,2015-07-15 19:03:26.677 UTC,2022-05-12 14:45:55.907 UTC,"New York, NY, United States",853,6,2,179,,,,,,"['databricks', 'mlflow']"
ML Flow autolog with catboost,"<p>I am trying to use auto logging of ML Flow with catboost - but looking at the UI of the experiment (in Databricks UI) I don't see any parameters or metrics logged.</p>
<p>My code is:</p>
<pre><code>mlflow.sklearn.autolog()
model_for_analytics = catboost.CatBoostRegressor(**cb_params)
model_for_analytics.fit(x_train, y_train, **cb_fit_params)```
</code></pre>",0,3,2022-06-22 12:16:59.683 UTC,,,0,databricks|mlflow,119,2016-03-21 08:49:39.92 UTC,2022-07-17 11:53:14.84 UTC,,383,10,0,19,,,,,,"['databricks', 'mlflow']"
CI / CD with Databricks Unity Catalog,"<p>I am migrating tables from hive_metastore to Unity Catalog for my Databricks workspaces.</p>
<p>I have three databricks workspaces:</p>
<ul>
<li>Dev</li>
<li>Test</li>
<li>Prod</li>
</ul>
<p>Each workspace has its own ADLSv2 storage account. (Dev, test, prod)</p>
<p>Currently when developing I read in a table using</p>
<pre><code>df = spark.table('bronze.my_table') # schema.table
</code></pre>
<p>This uses the default hive_metastore which points to the corresponding container (Workspace Dev -&gt; Storage account Dev).</p>
<p>However, with Unity Catalog. It seems I would now have to specify the catalog too based on which workspace I work in. Unless, there is a default unity catalog for a workspace.</p>
<pre><code>df = spark.table('dev.bronze.my_table') # catalog.schema.table
</code></pre>
<p>When deploying code from Dev -&gt; Test -&gt; Prod workspace. I would like to avoid having to dynamically set the catalog name for all notebooks using spark.table based on workspace (dev, test, prod). Basically 'bronze.my_table' when working in Dev points to delta table data stored in the dev catalog. While in Prod it points to delta table data stored in the prod catalog. Is this possible? I assume I can use the previous hive_metastore (one for each workspace) and build Unity Catalog on top of it (they reference each other and are in sync). However, isn't the idea that the Unity Catalog replaces the hive_metastore?</p>",1,0,2022-09-21 15:36:28.317 UTC,,,1,databricks|databricks-unity-catalog,19,2019-04-21 21:30:56.683 UTC,2022-09-23 10:21:44.843 UTC,,97,5,0,30,,,,,,"['databricks', 'databricks-unity-catalog']"
Runtime error using MLFlow and Spark on databricks,"<p>Here is some model I created:</p>
<pre><code>class SomeModel(mlflow.pyfunc.PythonModel):
    def predict(self, context, input):
        # do fancy ML stuff
        # log results
        pandas_df = pd.DataFrame(...insert predictions here...)
        spark_df = spark.createDataFrame(pandas_df)
        spark_df.write.saveAsTable('tablename', mode='append')
</code></pre>
<p>I'm trying to log my model in this manner by calling it later in my code:</p>
<pre><code>with mlflow.start_run(run_name=&quot;SomeModel_run&quot;):
    model = SomeModel()
    mlflow.pyfunc.log_model(&quot;somemodel&quot;, python_model=model)
</code></pre>
<p>Unfortunately it gives me this Error Message:</p>
<p><code>RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.</code></p>
<p>The error is caused because of the line <code>mlflow.pyfunc.log_model(&quot;somemodel&quot;, python_model=model)</code>, if I comment it out my model will make its predictions and log the results in my table.</p>
<p>Alternatively, removing the lines in my predict function where I call spark to create a dataframe and save the table, I am able to log my model.</p>
<p>How do I go about resolving this issue? I need my model to not only write to the table but also be logged</p>",0,8,2022-07-07 15:17:36.183 UTC,1.0,,3,python|pyspark|apache-spark-sql|databricks|mlflow,93,2017-04-05 06:51:07.557 UTC,2022-09-23 15:20:00.157 UTC,,33,1,0,5,,,,,,"['databricks', 'mlflow']"
Register SageMaker model in MLflow,"<p>MLflow can be used to track (hyper)parameters and metrics when training machine learning models. It stores the trained model as an artifact for every experiment. These models then can be directly deployed as <a href=""https://mlflow.org/docs/latest/models.html#deploy-a-python-function-model-on-amazon-sagemaker"" rel=""nofollow noreferrer"">SageMaker endpoints</a>.</p>
<p>Is it possible to do it the other way around, too, i.e. to register models trained in SageMaker into MLflow?</p>",1,0,2022-02-23 13:34:36.023 UTC,,,1,amazon-sagemaker|mlflow,133,2013-10-28 11:07:46.783 UTC,2022-09-21 20:35:26.05 UTC,"Vienna, Austria",690,707,1,174,,,,,,"['amazon-sagemaker', 'mlflow']"
DBT on Databricks Unity Catalog,"<p>I've been considering turning on Databricks Unity Catalog in our primary (only) workspace, but I'm concerned about how this might impact our existing dbt loads with the new three-part object references.</p>
<p>I see from the dbt-databricks <a href=""https://pypi.org/project/dbt-databricks/"" rel=""nofollow noreferrer"">release notes</a> that you need &gt;= 1.1.1 to get unity support. <br>The snippet with it only shows setting the <em>catalog</em> property in the profile. I was planning on having some of the sources in separate catalog's for the dbt generated objects.</p>
<p>I might even choose to have the dbt generated objects in separate catalogues if this was available.<br>
As turning on Unity Catalog is a one way road in a workspace, I don't wish to wing it and see what happens.</p>
<p>Has anyone used dbt with Unity Catalog and used numerous catalogs in the project?</p>
<p>If so, are there any gotcha's and how do you specify the catalog for sources and specific models?</p>
<p>Regards,</p>
<p>Ashley</p>",0,0,2022-09-12 23:05:04.22 UTC,,2022-09-21 19:12:46.017 UTC,3,databricks|dbt|databricks-unity-catalog,59,2022-09-01 00:27:59.62 UTC,2022-09-20 01:36:32.453 UTC,,43,0,0,1,,,,,,"['databricks', 'databricks-unity-catalog']"
How to validate automl result on the Databricks with a separate dataset,"<p>I was performing AutoML feature on the Databricks. But I want to validate the model on the separate dataset.</p>
<p>Since I'm not super aware of the MLFlow, I tried to insert new dataset inside <code>split_test_df</code> with reading it first. But it didn't worked out.</p>
<p>The code inside notebook looks the following:</p>
<pre><code>import mlflow
import databricks.automl_runtime

target_col = &quot;my_target_column&quot;

from mlflow.tracking import MlflowClient
import os
import uuid
import shutil
import pandas as pd

# Create temp directory to download input data from MLflow
input_temp_dir = os.path.join(os.environ[&quot;SPARK_LOCAL_DIRS&quot;], &quot;tmp&quot;, str(uuid.uuid4())[:8])
os.makedirs(input_temp_dir)


# Download the artifact and read it into a pandas DataFrame
input_client = MlflowClient()
input_data_path = input_client.download_artifacts(&quot;some_numbers_and_letters&quot;, &quot;data&quot;, input_temp_dir)

df_loaded = pd.read_parquet(os.path.join(input_data_path, &quot;training_data&quot;))
# Delete the temp data
shutil.rmtree(input_temp_dir)

# Preview data
df_loaded.head(5)

df = spark.read.format('delta').load(
  'dbfs:/user/hive/warehouse/test_df/',
  header=True,
  inferSchema=True
)

from databricks.automl_runtime.sklearn.column_selector import ColumnSelector
supported_cols = [&quot;there_are_my_columns&quot;]
col_selector = ColumnSelector(supported_cols)


from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import FunctionTransformer, StandardScaler

num_imputers = []
num_imputers.append((&quot;impute_mean&quot;, SimpleImputer(), [&quot;there_are_my_columns&quot;]))

numerical_pipeline = Pipeline(steps=[
    (&quot;converter&quot;, FunctionTransformer(lambda df: df.apply(pd.to_numeric, errors=&quot;coerce&quot;))),
    (&quot;imputers&quot;, ColumnTransformer(num_imputers)),
    (&quot;standardizer&quot;, StandardScaler()),
])

numerical_transformers = [(&quot;there_are_my_columns&quot;])]

from sklearn.compose import ColumnTransformer

transformers = numerical_transformers

preprocessor = ColumnTransformer(transformers, remainder=&quot;passthrough&quot;, sparse_threshold=0)

# AutoML completed train - validation - test split internally and used _automl_split_col_3da1 to specify the set
split_train_df = df_loaded.loc[df_loaded._automl_split_col_3da1 == &quot;train&quot;]
split_val_df = df_loaded.loc[df_loaded._automl_split_col_3da1 == &quot;val&quot;]
split_test_df = df.loc[df._automl_split_col_3da1 == 'test']  # here it throws an error that ttributeError: 'DataFrame' object has no attribute 'loc'

# Separate target column from features and drop _automl_split_col_3da1
X_train = split_train_df.drop([target_col, &quot;_automl_split_col_3da1&quot;], axis=1)
y_train = split_train_df[target_col]

X_val = split_val_df.drop([target_col, &quot;_automl_split_col_3da1&quot;], axis=1)
y_val = split_val_df[target_col]

X_test = split_test_df.drop(target_col)
y_test = split_test_df[target_col]
</code></pre>
<p>And even if it was read and processed to the model in MLFlow context I still don't see any changes in the confusion matrix since I'm also not sure if it based on the test data and how accurate the result was.</p>",0,0,2022-09-20 14:45:55.063 UTC,,,0,python|validation|databricks|mlflow|automl,15,2019-07-30 13:42:54.517 UTC,2022-09-21 08:53:39.343 UTC,"Wrocław, Польша",73,22,0,33,,,,,,"['mlflow', 'databricks']"
How to add more metrics to a finished MLflow run?,"<p>Once an MLflow run is finished, external scripts can access its parameters and metrics using python <code>mlflow</code> client and <code>mlflow.get_run(run_id)</code> method, but the <code>Run</code> object returned by <code>get_run</code> seems to be read-only.</p>
<p>Specifically, <code>.log_param</code> <code>.log_metric</code>, or <code>.log_artifact</code> cannot be used on the object returned by <code>get_run</code>, raising errors like these:</p>
<pre><code>AttributeError: 'Run' object has no attribute 'log_param'
</code></pre>
<p>If we attempt to run any of the <code>.log_*</code> methods on <code>mlflow</code>, it would log them into to a new run  with auto-generated run ID in the <code>Default</code> experiment.</p>
<p>Example:</p>
<pre><code>final_model_mlflow_run = mlflow.get_run(final_model_mlflow_run_id)

with mlflow.ActiveRun(run=final_model_mlflow_run) as myrun:    
    
    # this read operation uses correct run
    run_id = myrun.info.run_id
    print(run_id)
    
    # this write operation writes to a new run 
    # (with auto-generated random run ID) 
    # in the &quot;Default&quot; experiment (with exp. ID of 0)
    mlflow.log_param(&quot;test3&quot;, &quot;This is a test&quot;)
   
</code></pre>
<p>Note that the above problem exists regardless of the <code>Run</code> status (<code>.info.status</code> can be both &quot;FINISHED&quot; or &quot;RUNNING&quot;, without making any difference).</p>
<p>I wonder if this read-only behavior is by design (given that immutable modeling runs improve experiments reproducibility)? I can appreciate that, but it also goes against code modularity if everything has to be done within a single monolith like the <code>with mlflow.start_run()</code> context...</p>",1,1,2021-03-29 14:12:06.043 UTC,,2021-03-29 17:50:32.537 UTC,3,python|databricks|mlflow,1269,2018-06-19 12:32:08.93 UTC,2022-09-24 20:14:44.457 UTC,EU,3260,1100,4,466,,,,,,"['databricks', 'mlflow']"
Serving multiple ML models using mlflow in a single VM,"<p>I have setup an mlflow service in a VM and I am able to serve the model using mlflow serve command.
Wanted to know if we can host multiple models in a single VM ?</p>
<p>I am using the below command to serve a model using mlflow in a vm.</p>
<p>command:</p>
<pre><code>/mlflow models serve -m models:/$Model-Name/$Version --no-conda -p 443 -h 0.0.0.0
</code></pre>
<p>Above command creates a model serving and runs it on 443 port.
Is it possible to have an endpoint like below being created with model name in it ?</p>
<p>Current URL:
https://localhost:443/invocations</p>
<p>Expected URL:
https://localhost:443/model-name/invocations ?</p>",3,1,2022-01-07 10:48:20.16 UTC,1.0,2022-01-07 12:33:40.04 UTC,0,apache-spark|machine-learning|databricks|mlflow,544,2019-09-20 08:55:45.383 UTC,2022-09-23 06:49:38.593 UTC,,344,39,0,72,,,,,,"['databricks', 'mlflow']"
Is it possible to specify MLflow project Environment through a Dockerfile (instead of an image)?,"<p>To my understanding, currently (May 2019) mlflow support running project in docker environment; however, it needs the docker image already been built. This leaves the docker image building to be a separate workflow. What is the suggested way to run a mlflow project from Dockerfile? </p>

<p>Is there plans to support targeting Dockerfile natively in mlflow? What are the considerations about using image vs Dockerfile? Thanks!</p>",2,0,2019-05-13 21:20:34.943 UTC,,2019-05-13 23:47:12.507 UTC,2,docker|machine-learning|artificial-intelligence|databricks|mlflow,631,2013-12-11 04:18:22.123 UTC,2022-09-23 23:07:48.663 UTC,,3405,641,8,1094,,,,,,"['databricks', 'mlflow']"
How to use MLfLow with private git repositories?,"<p>I tested <code>MLflow</code> experiment when the source code is stored in public a git repository. Example command looks like this</p>

<pre><code>mlflow run  https://github.com/amesar/mlflow-fun.git#examples/hello_world \
 --experiment-id=2019 \
 -Palpha=100 -Prun_origin=GitRun -Plog_artifact=True
</code></pre>

<p>However, when I provide an internal (private) git repository link instead of public- MLflow redirects to login url, and then execution fails like this.</p>

<pre><code>git.exc.GitCommandError: Cmd('git') failed due to: exit code(128)
cmdline: git fetch -v origin
stderr: 'fatal: unable to update url base from redirection:
asked for: https://gitlab-master.companyname.com/myusername/project_name
/tree/master/models/myclassifier/info/refs?service=git-upload-pack
redirect: https://gitlab-master.company.com/users/sign_in'
</code></pre>

<p>Is there any way to commmunicate credentials of git account to MLflow?</p>",1,0,2019-07-15 05:56:44.117 UTC,,,2,git|gitlab|databricks|mlflow,1278,2014-05-26 11:37:08.227 UTC,2022-09-23 14:52:11.387 UTC,"Santa Clara, CA, USA",4031,79,6,244,,,,,,"['databricks', 'mlflow']"
How to run the mlflow web-based user interface from Amazon SageMaker?,"<p>I want to use the mlflow web-based user interface from a notebook on Amazon SageMaker. But the given address <a href=""http://127.0.0.1:5000"" rel=""noreferrer"">http://127.0.0.1:5000</a> doesn't seeem to work.</p>

<p>I have installed mlflow on a SageMaker notebook.</p>

<p>This code runs nicely:</p>

<pre><code>import mlflow
mlflow.start_run()
mlflow.log_param(""my"", ""param"")
mlflow.log_metric(""score"", 100)
mlflow.end_run()
</code></pre>

<p>And then if I run </p>

<pre><code>! mlflow ui
</code></pre>

<p>I get the expected result: </p>

<pre><code>[2019-04-09 11:15:52 +0000] [17980] [INFO] Starting gunicorn 19.9.0
[2019-04-09 11:15:52 +0000] [17980] [INFO] Listening at: http://127.0.0.1:5000 (17980)
[2019-04-09 11:15:52 +0000] [17980] [INFO] Using worker: sync
[2019-04-09 11:15:52 +0000] [17983] [INFO] Booting worker with pid: 17983
</code></pre>

<p>However, after that when going to <code>http://127.0.0.1:5000</code> in my browser, nothing loads. </p>

<p>My guess it that <code>127.0.0.1</code> is not the right address, but how can I know which address to use instead?</p>",1,1,2019-04-09 12:40:21.92 UTC,1.0,,8,amazon-sagemaker|mlflow,1359,2019-04-09 11:30:39.983 UTC,2021-01-26 11:37:19.97 UTC,,81,1,0,1,,,,,,"['amazon-sagemaker', 'mlflow']"
Hyperopt failed to execute mlflow.end_run() with tracking URI: databricks,"<p>I'm using Azure Databricks + Hyperopt + MLflow for some hyperparameter tuning on a small dataset.  Seem like the job is running, and I get output in MLflow, but the job ends with the following error message:</p>

<pre><code>Hyperopt failed to execute mlflow.end_run() with tracking URI: databricks
</code></pre>

<p>Here is my code code with some information redacted:</p>

<pre><code>from pyspark.sql import SparkSession

# spark session initialization
spark = (SparkSession.builder.getOrCreate())
sc = spark.sparkContext

# Data Processing
import pandas as pd
import numpy as np
# Hyperparameter Tuning
from hyperopt import fmin, tpe, hp, anneal, Trials, space_eval, SparkTrials, STATUS_OK
from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score
# Modeling
from sklearn.ensemble import RandomForestClassifier
# cleaning
import gc
# tracking
import mlflow
# track runtime
from datetime import date, datetime

mlflow.set_experiment('/user/myname/myexp')
# notebook settings \ variable settings
n_splits = #
n_repeats = #
max_evals = #

dfL = pd.read_csv(""/my/data/loc/mydata.csv"")

x_train = dfL[['f1','f2','f3']]
y_train = dfL['target']

def define_model(params):
    model = RandomForestClassifier(n_estimators=int(params['n_estimators']),
                                   criterion=params['criterion'], 
                                   max_depth=int(params['max_depth']), 
                                   min_samples_split=params['min_samples_split'], 
                                   min_samples_leaf=params['min_samples_leaf'], 
                                   min_weight_fraction_leaf=params['min_weight_fraction_leaf'], 
                                   max_features=params['max_features'], 
                                   max_leaf_nodes=None, 
                                   min_impurity_decrease=params['min_impurity_decrease'], 
                                   min_impurity_split=None, 
                                   bootstrap=params['bootstrap'], 
                                   oob_score=False, 
                                   n_jobs=-1, 
                                   random_state=int(params['random_state']), 
                                   verbose=0, 
                                   warm_start=False, 
                                   class_weight={0:params['class_0_weight'], 1:params['class_1_weight']})
        return model


space = {'n_estimators': hp.quniform('n_estimators', #, #, #),
         'criterion': hp.choice('#', ['#','#']),
         'max_depth': hp.quniform('max_depth', #, #, #),
         'min_samples_split': hp.quniform('min_samples_split', #, #, #),
         'min_samples_leaf': hp.quniform('min_samples_leaf', #, #, #),
         'min_weight_fraction_leaf': hp.quniform('min_weight_fraction_leaf', #, #, #),
         'max_features': hp.quniform('max_features', #, #, #),
         'min_impurity_decrease': hp.quniform('min_impurity_decrease', #, #, #),
         'bootstrap': hp.choice('bootstrap', [#,#]),
         'random_state': hp.quniform('random_state', #, #, #),
         'class_0_weight': hp.choice('class_0_weight', [#,#,#]),
         'class_1_weight': hp.choice('class_1_weight', [#,#,#])}

# define hyperopt objective
def objective(params, n_splits=n_splits, n_repeats=n_repeats):

    # define model
    model = define_model(params)
    # get cv splits
    kfold = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=1331)
    # define and run sklearn cv scorer
    scores = cross_val_score(model, x_train, y_train, cv=kfold, scoring='roc_auc')
    score = scores.mean()

    return {'loss': score*(-1), 'status': STATUS_OK}

spark_trials = SparkTrials(parallelism=36, spark_session=spark)
with mlflow.start_run():
  best = fmin(objective, space, algo=tpe.suggest, trials=spark_trials, max_evals=max_evals)
</code></pre>

<p>and then at the end I get..</p>

<pre><code>100%|██████████| 200/200 [1:35:28&lt;00:00, 100.49s/trial, best loss: -0.9584565527065526]

Hyperopt failed to execute mlflow.end_run() with tracking URI: databricks

Exception: 'MLFLOW_RUN_ID'

Total Trials: 200: 200 succeeded, 0 failed, 0 cancelled.
</code></pre>

<p>My Azure Databricks cluster is..</p>

<pre><code>6.6 ML (includes Apache Spark 2.4.5, Scala 2.11)
Standard_DS3_v2
min 9 max 18 nodes
</code></pre>

<p>Am I doing something wrong or is this a bug?</p>",1,4,2020-06-02 20:19:55.507 UTC,,,2,pyspark|databricks|azure-databricks|mlflow|hyperopt,604,2018-06-10 03:57:32.363 UTC,2022-09-23 22:17:15.213 UTC,,247,637,0,53,,,,,,"['databricks', 'mlflow']"
botocore.exceptions.ClientError: An error occurred (ValidationException) when calling the CreateModel operation: Could not access model data,"<p>I want to deploy an MLflow image to an AWS Sagemaker endpoint that contains a machine learning model. I executed the following code, which I found in <a href=""https://towardsdatascience.com/deploying-models-to-production-with-mlflow-and-amazon-sagemaker-d21f67909198"" rel=""nofollow noreferrer"">this blog post</a>.</p>
<pre><code>import mlflow.sagemaker as mfs

run_id = run_id # the model you want to deploy - this run_id was saved when we trained our model
region = &quot;us-east-1&quot; # region of your account
aws_id = &quot;XXXXXXXXXXX&quot; # from the aws-cli output
arn = &quot;arn:aws:iam::XXXXXXXXXXX:role/your-role&quot;
app_name = &quot;iris-rf-1&quot;
model_uri = &quot;mlruns/%s/%s/artifacts/random-forest-model&quot; % (experiment_id,run_id) # edit this path based on your working directory
image_url = aws_id + &quot;.dkr.ecr.&quot; + region + &quot;.amazonaws.com/mlflow-pyfunc:1.2.0&quot; # change to your mlflow version

mfs.deploy(app_name=app_name, 
           model_uri=model_uri, 
           region_name=region, 
           mode=&quot;create&quot;,
           execution_role_arn=arn,
           image_url=image_url)
</code></pre>
<p>But I got the following error. I checked all policies and permissions attached to the IAM role. They all comply with what the error message complains about. I don't know what to do next. I'd appreciate your help. Thanks.</p>
<p>botocore.exceptions.ClientError: An error occurred (ValidationException) when calling the CreateModel operation: Could not access model data at <a href=""https://s3.amazonaws.com/mlflow-sagemaker-us-east-1-xxx/mlflow-xgb-demo-model-eqktjeoit5mxhmjn-abpanw/model.tar.gz"" rel=""nofollow noreferrer"">https://s3.amazonaws.com/mlflow-sagemaker-us-east-1-xxx/mlflow-xgb-demo-model-eqktjeoit5mxhmjn-abpanw/model.tar.gz</a>. Please ensure that the role &quot;arn:aws:iam::xxx:role/mlflow-sagemaker-dev&quot; exists and that its trust relationship policy allows the action &quot;sts:AssumeRole&quot; for the service principal &quot;sagemaker.amazonaws.com&quot;. Also ensure that the role has &quot;s3:GetObject&quot; permissions and that the object is located in us-east-1.</p>",2,0,2021-01-28 15:47:29.71 UTC,,,0,amazon-web-services|amazon-sagemaker|mlflow,1789,2016-12-22 14:48:27.07 UTC,2021-05-11 00:09:27.51 UTC,"Raleigh, NC, United States",69,0,0,9,,,,,,"['amazon-sagemaker', 'mlflow']"
How to run non-spark model training task (using fasttext) efficiently on a databricks cluster?,"<p>I want to train some models using <code>fasttext</code> and since it doesn't use spark, it will be running on my driver. The number of training jobs that will be running simultaneously is very large and so is the size of the data. Is there a way to make it run it on different workers or distribute it across workers?
Is this the best approach or am I better off using a large single node cluster?</p>
<p>FYI, I am using Databricks. So solutions specific to that are also okay.</p>",1,0,2021-11-12 12:03:47.093 UTC,,2021-11-20 12:06:30.14 UTC,2,apache-spark|pyspark|databricks|fasttext|mlflow,136,2018-11-28 07:36:30.31 UTC,2022-09-22 10:22:14.753 UTC,"Chennai, Tamil Nadu, India",139,8,2,41,,,,,,"['databricks', 'mlflow']"
How to send response from Azure Databricks the UI in real-time?,"<p>I have registered my ML model using Mlflow in Azure Databricks, and have a model URL.
Now, in another notebook of Databricks, I am preparing data(retrieved from the SQL) and filtering it based on the input parameters fetched as a RestAPI from Postman.
Next, I am using this prepared data frame(max. 20 records) to get the prediction for each row. And, converting this data frame into JSON serializable format to send it as a response to Postman.
The notebook runtime is 2 secs.</p>
<p>Every time a request is got, a databricks job is run, and the response is sent. I am not sure why each job's runtime is 12 secs or more(despite the notebook runtime being 2 secs. only).
What I actually I am looking into is to send a curated response, that'll have some additional parameters along with the prediction, in form of a RestAPI in milliseconds(as it is realtime).</p>
<p>Everything is done in Databricks, itself.</p>
<p>I believe I am somewhere lacking in understanding which architecture shall help us achieve my requirement. Kindly help me understand the same. Thanks a lot, in advance.</p>",0,4,2020-12-24 08:11:32.573 UTC,,2020-12-31 04:14:47.95 UTC,0,azure|postman|databricks|job-scheduling|mlflow,243,2017-03-29 18:50:29.467 UTC,2021-02-23 05:52:42.913 UTC,"Hyderabad, Telangana, India",25,1,0,63,,,,,,"['databricks', 'mlflow']"
Setting array of tags to MLFlow registered model,"<p>I have a model registered in ML Flow and would like associate a list of tags to that model.
But when i looked at the reference APIs, it looks like we can add only one tag at a time with a single http request.</p>
<pre><code>https://www.mlflow.org/docs/latest/rest-api.html#set-registered-model-tag
</code></pre>
<p>Is it possible to create an array of tags and associate that with model in a single http call ?
Like how we do during model creation API ?</p>
<pre><code>https://www.mlflow.org/docs/latest/rest-api.html#create-registeredmodel
</code></pre>",0,2,2021-12-07 11:48:04.19 UTC,,2021-12-07 11:55:14.697 UTC,0,databricks|azure-databricks|mlflow,173,2019-09-20 08:55:45.383 UTC,2022-09-23 06:49:38.593 UTC,,344,39,0,72,,,,,,"['databricks', 'mlflow']"
How do I invoke a data enrichment function before model.predict while serving the model in Databricks,"<p>In Databricks, I have used mlflow and got my model served through REST API. It works fine when all model features are provided. But my use case is that only a single feature (the primary key) will be provided by the consumer application, and my code has to lookup the other features from a database based on that key and then use the model.predict to return the prediction. I tried researching but understood that the REST endpoints will simply invoke the model.predict function. How can I make it invoke a data massaging function before predicting?</p>",1,0,2022-02-14 08:12:02.87 UTC,,2022-02-14 10:52:46.403 UTC,1,model|databricks|mlflow|serving,109,2018-07-09 13:16:02.08 UTC,2022-07-06 10:09:41.38 UTC,,11,0,0,1,,,,,,"['mlflow', 'databricks']"
How can I run Tensorboard with MLFlow's logs?,"<p>I use MLFlow with autolog to keep track of my Tensorflow models:</p>
<pre><code>mlflow.tensorflow.autolog(every_n_iter=1)
with mlflow.start_run():
  model = ...
  model.compile(...)
  model.fit(...)
</code></pre>
<p>and then I want to use my tensorboard logs located in the artifacts.
But when I run:</p>
<pre><code>%tensorboard --logdir=&lt;logs_path&gt;
</code></pre>
<p>I have the error message:
&quot;No dashboards are active for the current data set.
Probable causes:</p>
<p>You haven’t written any data to your event files.
TensorBoard can’t find your event files.&quot;</p>
<p>I work on Databricks, so log_path is something like:</p>
<pre><code>/dbfs/databricks/mlflow-tracking/..
</code></pre>
<p>Any ideas?</p>",0,1,2021-12-16 18:15:45.067 UTC,,,1,tensorflow|databricks|tensorboard|mlflow,501,2021-12-16 18:03:10.923 UTC,2022-09-22 13:52:41.243 UTC,,11,0,0,2,,,,,,"['databricks', 'mlflow']"
MLFlow: how to get additional methods from a loaded model?,"<p><strong>Use case:</strong></p>
<p>A <a href=""https://www.mlflow.org/docs/latest/python_api/mlflow.pyfunc.html#mlflow.pyfunc.PyFuncModel"" rel=""nofollow noreferrer""><code>mlflow.pyfunc.PyFuncModel</code></a> is defined with some more utilities methods in order to provide a way of parsing its prediction result to different formats.</p>
<p><strong>After the model is loaded from registry, is there a way to access those methods?</strong></p>
<p><strong>A contrived example:</strong></p>
<p>A <code>mlflow.pyfunc.PyFuncModel</code> model defining additional methods:</p>
<pre class=""lang-py prettyprint-override""><code>class MyModel(mlflow.pyfunc.PythonModel):
    def predict(self, context, model_input):
        prediction = # do some prediction
        return prediction

    @staticmethod
    def parse_prediction_to_format_x(prediction):
        prediction_formatted = # do some parsing
        return prediction_formatted

    def parse_prediction_to_format_y(self, prediction):
        prediction_formatted = # do some parsing
        return prediction_formatted
</code></pre>
<p>Note: I added one static and one non static, because both use cases are relevant.</p>
<p>Now, some other system goes to MLFlow Registry and loads the model from there:</p>
<pre class=""lang-py prettyprint-override""><code>        loaded_model = mlflow.pyfunc.load_model(
            model_uri=saved_model_path.absolute().as_uri()
        )
</code></pre>
<p>This system, which naturally does not hold the model source code, but the registry path to load it from there, wants to use the additional methods above.
It can use predict, since it is part of all pyfunc models:</p>
<pre class=""lang-py prettyprint-override""><code>predicted = loaded_model.predict(input_data)
</code></pre>
<p><strong>But how can this system access helper methods in the model class (static or instance methods)?</strong></p>
<pre class=""lang-py prettyprint-override""><code>predicted = loaded_model.predict(input_data)

# pseudo code:
predicted_and_formated = loaded_model.parse_prediction_to_format_y(predicted)
</code></pre>
<p>Thank you.</p>",0,0,2022-09-15 10:04:38.537 UTC,,,0,python|databricks|mlflow,15,2020-02-13 21:23:52.807 UTC,2022-09-22 23:27:25.757 UTC,,477,59,0,56,,,,,,"['databricks', 'mlflow']"
Databricks: Migrate a registered model from one workspace to another?,"<p>We have multiple Databricks Workspaces on Azure. On one of them we trained multiple models and registered them in the MLflow registry. Our goal is to move those model from one databricks workspace to another and so far, i could not find a straight forwared way to do this except running the training script again on the new databricks workspace.</p>
<p>Downloading the model an registering them in the new workspace didn't work so far. Should I create a &quot;dummy&quot; training script, that just loads the model, does nothing with it and then logs it away in the new workspace?</p>
<p>Seems to me like databricks never anticipated, that someone might want to migrate ML models?</p>",2,0,2021-08-25 06:59:47.863 UTC,,2021-08-25 07:42:09.853 UTC,1,azure|machine-learning|migration|databricks|mlflow,588,2017-01-22 21:52:54.84 UTC,2022-08-02 14:08:09.97 UTC,,188,0,1,22,,,,,,"['databricks', 'mlflow']"
Access MLflow Artifacts in Model Registry Using Python,"<p>I am looking to access the artifacts of a model registered to the Model Registry in Databricks. However, I want to be able to do this outside of Databricks, using a Python script.</p>
<p>Specifically, I want to be able to access the <code>feature_spec.yml</code> shown in the directory structure below,</p>
<p><a href=""https://i.stack.imgur.com/7lSav.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7lSav.png"" alt=""enter image description here"" /></a></p>
<p>I came across this article in the Microsoft docs, but it is not quite clear,
<br>
<a href=""https://docs.microsoft.com/en-us/azure/databricks/applications/mlflow/access-hosted-tracking-server"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/databricks/applications/mlflow/access-hosted-tracking-server</a></p>
<p>Note that I will probably only have the name of the Model and the Version that I want to access. How can I do this using Python?</p>",1,0,2022-06-09 03:25:24.13 UTC,,,0,python|databricks|mlflow,203,2018-03-24 01:53:05.82 UTC,2022-09-24 16:46:35.903 UTC,Sri Lanka,820,389,1,165,,,,,,"['databricks', 'mlflow']"
"Error logging Spark model with mlflow to databricks registry, via databricks-connect","<p>I'm trying to log a trained spark model on mlflow using databricks-connect. I want this model to be logged in the Databricks registry. For now, my code looks like this:</p>
<pre class=""lang-py prettyprint-override""><code>mlflow.set_tracking_uri(&quot;databricks&quot;)
mlflow.set_experiment(&quot;/Users/xxxxx/experiment_name&quot;)

with mlflow.start_run(run_name=&quot;my_run&quot;) as _:
    mlflow.spark.log_model(my_spark_model, &quot;my_model&quot;)
</code></pre>
<p>When it runs the log_model line, execution breaks with the following stack trace:</p>
<blockquote>
<p>22/07/21 11:05:03 WARN ProtoSerializer: Failed to deserialize remote exception
java.io.InvalidClassException: failed to read class descriptor
at java.io.ObjectInputStream.readNonProxyDesc(Unknown Source)
at java.io.ObjectInputStream.readClassDesc(Unknown Source)
at java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)
at java.io.ObjectInputStream.readObject0(Unknown Source)
at java.io.ObjectInputStream.readObject(Unknown Source)
at java.io.ObjectInputStream.readObject(Unknown Source)
at org.apache.spark.sql.util.ProtoSerializer.$anonfun$deserializeObject$1(ProtoSerializer.scala:6618)
at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
at org.apache.spark.sql.util.ProtoSerializer.deserializeObject(ProtoSerializer.scala:6603)
at org.apache.spark.sql.util.ProtoSerializer.deserializeException(ProtoSerializer.scala:6634)
at com.databricks.service.SparkServiceRemoteFuncRunner.executeRPC(SparkServiceRemoteFuncRunner.scala:188)
at com.databricks.service.SparkServiceRemoteFuncRunner.$anonfun$execute0$1(SparkServiceRemoteFuncRunner.scala:121)
at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
at com.databricks.service.SparkServiceRemoteFuncRunner.withRetry(SparkServiceRemoteFuncRunner.scala:135)
at com.databricks.service.SparkServiceRemoteFuncRunner.execute0(SparkServiceRemoteFuncRunner.scala:113)
at com.databricks.service.SparkServiceRemoteFuncRunner.$anonfun$execute$1(SparkServiceRemoteFuncRunner.scala:86)
at com.databricks.spark.util.Log4jUsageLogger.recordOperation(UsageLogger.scala:247)
at com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:429)
at com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:408)
at com.databricks.service.SparkServiceRPCClientStub.recordOperation(SparkServiceRPCClientStub.scala:58)
at com.databricks.service.SparkServiceRemoteFuncRunner.execute(SparkServiceRemoteFuncRunner.scala:78)
at com.databricks.service.SparkServiceRemoteFuncRunner.execute$(SparkServiceRemoteFuncRunner.scala:67)
at com.databricks.service.SparkServiceRPCClientStub.execute(SparkServiceRPCClientStub.scala:58)
at com.databricks.service.SparkServiceRPCClientStub.fileSystemOperation(SparkServiceRPCClientStub.scala:297)
at com.databricks.service.FSClient.send(FSClient.scala:51)
at com.databricks.service.FSClient.getFileStatus(FSClient.scala:181)
at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1426)
at org.apache.spark.ml.util.FileSystemOverwrite.handleOverwrite(ReadWrite.scala:675)
at org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:167)
at org.apache.spark.ml.PipelineModel$PipelineModelWriter.super$save(Pipeline.scala:344)
at org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$4(Pipeline.scala:344)
at org.apache.spark.ml.MLEvents.withSaveInstanceEvent(events.scala:175)
at org.apache.spark.ml.MLEvents.withSaveInstanceEvent$(events.scala:170)
at org.apache.spark.ml.util.Instrumentation.withSaveInstanceEvent(Instrumentation.scala:43)
at org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3(Pipeline.scala:344)
at org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3$adapted(Pipeline.scala:344)
at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:284)
at scala.util.Try$.apply(Try.scala:213)
at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:284)
at org.apache.spark.ml.PipelineModel$PipelineModelWriter.save(Pipeline.scala:344)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
at java.lang.reflect.Method.invoke(Unknown Source)
at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)
at py4j.Gateway.invoke(Gateway.java:295)
at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
at py4j.commands.CallCommand.execute(CallCommand.java:79)
at py4j.GatewayConnection.run(GatewayConnection.java:251)
at java.lang.Thread.run(Unknown Source)
Caused by: java.lang.ClassNotFoundException: com.databricks.backend.daemon.data.common.InvalidMountException
at java.net.URLClassLoader.findClass(Unknown Source)
at java.lang.ClassLoader.loadClass(Unknown Source)
at java.lang.ClassLoader.loadClass(Unknown Source)
at java.lang.Class.forName0(Native Method)
at java.lang.Class.forName(Unknown Source)
at org.apache.spark.util.Utils$.classForName(Utils.scala:242)
at org.apache.spark.sql.util.SparkServiceObjectInputStream.readResolveClassDescriptor(SparkServiceObjectInputStream.scala:60)
at org.apache.spark.sql.util.SparkServiceObjectInputStream.readClassDescriptor(SparkServiceObjectInputStream.scala:55)
... 51 more
**22/07/21 11:05:03 ERROR Instrumentation: com.databricks.service.SparkServiceRemoteException: com.databricks.backend.daemon.data.common.InvalidMountException: Error while using path /databricks/mlflow-tracking/000000000000000/0a0a0a0a0a0a0a0a0a0a/artifacts\experiment_name/sparkml for resolving path '/000000000000000/0a0a0a0a0a0a0a0a0a0a/artifacts\experiment_name/sparkml' within mount at '/databricks/mlflow-tracking'.&gt; **</p>
</blockquote>
<blockquote>
<p>&lt;...&gt;&gt;</p>
</blockquote>
<blockquote>
<p><strong>Caused by: java.io.IOException: No FileSystem for scheme: unsupported-access-mechanism-for-path--use-mlflow-client</strong>
at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2660)
at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2667)
at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)
at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)
at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)
at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)
at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2Factory.createFileSystem(DatabricksFileSystemV2Factory.scala:124)
at com.databricks.backend.daemon.data.filesystem.MountEntryResolver.$anonfun$resolve$1(MountEntryResolver.scala:67)
at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:395)
at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:484)
at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:504)
at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:266)
at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:261)
at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:258)
at com.databricks.common.util.locks.LoggedLock$.withAttributionContext(LoggedLock.scala:73)
at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:305)
at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:297)
at com.databricks.common.util.locks.LoggedLock$.withAttributionTags(LoggedLock.scala:73)
at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:479)
at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:404)
at com.databricks.common.util.locks.LoggedLock$.recordOperationWithResultTags(LoggedLock.scala:73)
at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:395)
at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:367)
at com.databricks.common.util.locks.LoggedLock$.recordOperation(LoggedLock.scala:73)
at com.databricks.common.util.locks.LoggedLock$.withLock(LoggedLock.scala:120)
at com.databricks.common.util.locks.PerKeyLock.withLock(PerKeyLock.scala:36)
at com.databricks.backend.daemon.data.filesystem.MountEntryResolver.resolve(MountEntryResolver.scala:64)&gt;</p>
</blockquote>
<blockquote>
<p>&lt;...&gt;
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _&gt;</p>
</blockquote>
<blockquote>
<p>answer = 'xro1535'
gateway_client = &lt;py4j.java_gateway.GatewayClient object at 0x0000023716B5BE20&gt;
target_id = 'o1532', name = 'copyToLocalFile'&gt;</p>
<pre><code>def get_return_value(answer, gateway_client, target_id=None, name=None):
    &quot;&quot;&quot;Converts an answer received from the Java gateway into a Python object.

    For example, string representation of integers are converted to Python
    integer, string representation of objects are converted to JavaObject
    instances, etc.

    :param answer: the string returned by the Java gateway
    :param gateway_client: the gateway client used to communicate with the Java
        Gateway. Only necessary if the answer is a reference (e.g., object,
        list, map)
    :param target_id: the name of the object from which the answer comes from
        (e.g., *object1* in `object1.hello()`). Optional.
    :param name: the name of the member from which the answer comes from
        (e.g., *hello* in `object1.hello()`). Optional.
    &quot;&quot;&quot;
    if is_error(answer)[0]:
        if len(answer) &gt; 1:
            type = answer[1]
            value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)
            if answer[1] == REFERENCE_TYPE:
               raise Py4JJavaError(
                    &quot;An error occurred while calling {0}{1}{2}.\n&quot;.
                    format(target_id, &quot;.&quot;, name), value)
</code></pre>
<p>E                   py4j.protocol.Py4JJavaError: An error occurred while calling o1532.copyToLocalFile.
E                   : <strong>java.io.IOException: (null) entry in command string: null chmod 0644 C:\Users\itscarlayall\AppData\Local\Temp\tmpalmxdo16\model\sparkml\metadata_SUCCESS</strong>
E                       at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
E                       at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
E                       at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
E                       at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)
E                       at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.(RawLocalFileSystem.java:225)
E                       at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.(RawLocalFileSystem.java:209)
E                       at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)
E                       at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)
E                       at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)</p>
</blockquote>",1,0,2022-07-21 09:50:54.547 UTC,,,0,python|databricks|mlflow|databricks-connect,50,2017-09-24 08:15:07.953 UTC,2022-09-23 11:38:23.76 UTC,"Madrid, España",56,47,0,16,,,,,,"['databricks', 'mlflow']"
How to run data bricck notebook with mlflow in azure data factory pipeline?,"<p>My colleagues and I are facing an issue when trying to run my databricks notebook in Azure Data Factory and the error is coming from MLFlow.</p>
<p>The command that is failing is the following:</p>
<pre><code># Take the parent notebook path to use as path for the experiment
context = json.loads(dbutils.notebook.entry_point.getDbutils().notebook().getContext().toJson())
nb_base_path = context['extraContext']['notebook_path'][:-len(&quot;00_training_and_validation&quot;)]

experiment_path = nb_base_path + 'trainings'
mlflow.set_experiment(experiment_path)
experiment = mlflow.get_experiment_by_name(experiment_path)
experiment_id = experiment.experiment_id

run = mlflow.start_run(experiment_id=experiment_id, run_name=f&quot;run_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}&quot;)
</code></pre>
<p>And the error that is throwing is:</p>
<p>An exception was thrown from a UDF: 'mlflow.exceptions.RestException: INVALID_PARAMETER_VALUE: No experiment ID was specified. An experiment ID must be specified in Databricks Jobs and when logging to the MLflow server from outside the Databricks workspace. If using the Python fluent API, you can set an active experiment under which to create runs by calling mlflow.set_experiment(&quot;/path/to/experiment/in/workspace&quot;) at the start of your program.', from , line 32.</p>
<p>The pipeline just runs the notebook from ADF, it does not have any other step and the cluster we are using is type 7.3 ML.</p>
<p>Could you please help us?</p>
<p>Thank you in advance!</p>",1,0,2022-02-03 09:23:02.26 UTC,,,0,azure-data-factory|databricks|mlflow,392,2020-04-08 07:25:08.36 UTC,2022-02-03 09:43:32.287 UTC,"Madrid, Spain",35,1,0,10,,,,,,"['databricks', 'mlflow']"
How to read json.out file from databricks,"<p>I have been working with databricks for reading output from Object2Vec in Sagemaker. This output is saved as jsonlines with <code>.json.out</code> file format.</p>

<pre><code>df_emb = spark.read.option(""multiLine"", True).option(""mode"", ""PERMISSIVE"").json(bucket+key)
</code></pre>

<p>When i read this file as a json, it is read as a corrupt record. Below is the screenshot. 
<a href=""https://i.stack.imgur.com/jyMBj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jyMBj.png"" alt=""enter image description here""></a></p>

<p>I can provide the actual file if you know the solution.</p>",1,0,2019-05-02 09:40:25.647 UTC,,,0,amazon-web-services|databricks|amazon-sagemaker,96,2016-03-06 10:55:31.84 UTC,2022-09-23 23:07:03.117 UTC,"Vancouver, BC",584,71,6,270,,,,,,"['amazon-sagemaker', 'databricks']"
what are the events (ex MODEL_VERSION_CREATED) associated with ML FLow Databricks CI/CD,<p>ML Flow has multiple events to subscribe like MODEL_VERSION_CREATED when a model version is created. what are the other events available to subscribe.</p>,1,1,2021-09-28 18:27:45.547 UTC,,,1,databricks|azure-databricks|mlflow,26,2013-06-12 10:44:50.553 UTC,2021-11-16 16:58:46.133 UTC,,11,0,0,1,,,,,,"['databricks', 'mlflow']"
What is the difference between deploying models in MLflow and Sagemaker?,"<p>I could do
<code>mlflow model serve -m &lt;RUN_ID&gt; --p 1234 --no-conda</code></p>
<p>and</p>
<p><code>mlflow sagemaker run-local -m &lt;MODEL_PATH&gt; -p 1234</code></p>
<p>Are they not the same anyway as both can do model serving so what's the hassle deploying it to Sagemaker?</p>
<p>I'm a beginner at this so if anyone can help me out with my understanding that will be great. Thank you so much in advance!</p>",1,0,2022-09-22 00:30:20.887 UTC,,,0,rest|deployment|amazon-sagemaker|endpoint|mlflow,26,2017-02-23 06:31:44.143 UTC,2022-09-24 07:48:55.5 UTC,,1,0,0,4,,,,,,"['amazon-sagemaker', 'mlflow']"
Running mlflow ui in AWS Sagemaker,"<p>I want to run mlflow UI in sagemaker but it simply does not work, When it outputs the http address going to it results in a &quot;this site cannot be reached&quot;</p>
<p>Here is the code:</p>
<pre><code>def mlflow_test(server_uri, experiment_name):
    mlflow.set_tracking_uri(server_uri)
    mlflow.set_experiment(experiment_name)
    with mlflow.start_run():
        params = {
            &quot;n-estimators&quot;: 100,
            &quot;min-samples-leaf&quot;: 10,
            &quot;features&quot;: 'feature_test'
        }
        mlflow.log_params(params)
        mlflow.log_metric('foo', 5)
        mlflow.end_run()
</code></pre>
<p>running that code will return:</p>
<pre><code>[2022-05-24 15:48:44 +0000] [27820] [INFO] Starting gunicorn 20.1.0
[2022-05-24 15:48:44 +0000] [27820] [INFO] Listening at: http://127.0.0.1:5000 (27820)
[2022-05-24 15:48:44 +0000] [27820] [INFO] Using worker: sync
[2022-05-24 15:48:44 +0000] [27823] [INFO] Booting worker with pid: 27823
</code></pre>
<p>Going to the <a href=""http://127.0.0.1:5000"" rel=""nofollow noreferrer"">http://127.0.0.1:5000</a> link won't work. Anyone know how to get mlflow ui running in sagemaker? There's not much info on this that's at an easy to understand level. I just want to log my metrics and params in sagemaker and view them using the mlflow ui</p>",1,0,2022-05-24 15:54:09.377 UTC,,,0,python|amazon-web-services|amazon-sagemaker|mlflow,93,2021-03-17 15:21:32.347 UTC,2022-07-14 10:53:41.117 UTC,,21,0,0,12,,,,,,"['amazon-sagemaker', 'mlflow']"
Access databricks secrets in pyspark/python job,"<p>Databricks secrets can be accessed within notebooks using dbutils, however since dbutils is not available outside notebooks how can one access secrets in pyspark/python jobs, especially if they are run using mlflow.</p>

<p>I have already tried <a href=""https://stackoverflow.com/questions/51885332/how-to-load-databricks-package-dbutils-in-pyspark?rq=1"">How to load databricks package dbutils in pyspark</a></p>

<p>which does not work for remote jobs or mlflow project runs.</p>",1,0,2020-06-08 21:40:46.997 UTC,,,1,databricks|azure-databricks|mlflow,1369,2017-06-15 11:22:56.727 UTC,2021-03-07 17:34:22.217 UTC,"Mumbai, Maharashtra, India",481,16,0,49,,,,,,"['databricks', 'mlflow']"
ModuleNotFoundError: No module named 'pyspark.dbutils' while running multiple.py file/notebook on job clusters in databricks,"<p>I am working in TravisCI, MlFlow and Databricks environment where .tavis.yml sits at git master branch and detects any change in <code>.py</code> file and whenever it gets updated, It will run mlflow command to run .py file in databricks environment. 
my MLProject file looks as following:</p>

<pre><code>name: mercury_cltv_lib
conda_env: conda-env.yml


entry_points:    
  main:
    command: ""python3 run-multiple-notebooks.py""
</code></pre>

<p>Workflow is as following:
TravisCI detects change in master branch-->triggers build which will run MLFlow command and it'll spin up a job cluster in databricks to run .py file from repo.</p>

<p>It worked fine with one .py file but when I tried to run multiple notebook using dbutils, it is throwing </p>

<pre><code>  File ""run-multiple-notebooks.py"", line 3, in &lt;module&gt;
    from pyspark.dbutils import DBUtils
ModuleNotFoundError: No module named 'pyspark.dbutils'
</code></pre>

<p>Please find below the relevant code section from run-multiple-notebooks.py</p>

<pre><code>  def get_spark_session():
    from pyspark.sql import SparkSession
    return SparkSession.builder.getOrCreate()

  def get_dbutils(self, spark = None):
    try:
        if spark == None:
            spark = spark

        from pyspark.dbutils import DBUtils #error line
        dbutils = DBUtils(spark) #error line
    except ImportError:
        import IPython
        dbutils = IPython.get_ipython().user_ns[""dbutils""]
    return dbutils

  def submitNotebook(notebook):
    print(""Running notebook %s"" % notebook.path)
    spark = get_spark_session()
    dbutils = get_dbutils(spark)
</code></pre>

<p>I tried all the options and tried </p>

<pre><code>https://stackoverflow.com/questions/61546680/modulenotfounderror-no-module-named-pyspark-dbutils
</code></pre>

<p>as well. It is not working :(</p>

<p>Can someone please suggest if there is fix for the above-mentioned error while running .py in job cluster. My code works fine inside databricks local notebook but running from outside using TravisCI and MLFlow isn't working which is must requirement for pipeline automation.</p>",0,0,2020-05-18 22:15:02.237 UTC,,2020-05-20 13:47:56.477 UTC,2,pyspark|travis-ci|databricks|mlflow|dbutils,401,2019-04-15 16:50:36.127 UTC,2022-09-20 17:21:27.237 UTC,"Minnesota, USA",352,27,1,88,,,,,,"['databricks', 'mlflow']"
NameError: name 'dbutils' is not defined,"<p>I've .py file with following code line and it lives in git.</p>
<pre><code>dbutils.widgets.text(name='CORPORATION_ID', defaultValue='1234') 
</code></pre>
<p>I am using mlflow to run it in remote databricks job cluster. I've conda.yml and MLProject file to pick it up from git and run it in databricks job cluster but I am getting following error.</p>
<pre><code>  File &quot;tea/src/cltv_xgb_tea.py&quot;, line 40, in &lt;module&gt;
    dbutils.widgets.text(name='CORPORATION_ID', defaultValue='1234')
NameError: name 'dbutils' is not defined
</code></pre>
<p>Any help/solution is much appreciated.</p>
<hr />
<p>My current files in git</p>
<p>Conda.yml has</p>
<pre><code>name: cicd-environment
channels:
  - defaults
dependencies:
  - python=3.7
  - pip=19.0.3
  - pip:
    - mlflow==1.7.2
    - DBUtils==1.3
    - ipython==7.14.0
    - databricks-connect==6.5.1
    - invoke==1.4.1
    - awscli==1.18.87
</code></pre>",0,2,2020-06-25 16:03:46.77 UTC,,2020-06-29 04:11:36.033 UTC,0,pyspark|conda|databricks|mlflow,810,2019-04-15 16:50:36.127 UTC,2022-09-20 17:21:27.237 UTC,"Minnesota, USA",352,27,1,88,,,,,,"['databricks', 'mlflow']"
MLFlow model not logging to Azure Blob Storage,"<p>I am trying to use MLFlow to log artifacts to Azure Blob Storage. Though the logging to dbfs works fine, when I try to log it to Azure Blob Storage, I only see a folder with the corresponding runid but inside it there are no contents.</p>

<p>Here is what I do-</p>

<ol>
<li><p>Create a experiment from Azure Databricks, give it a name and the artifacts location as wasbs://mlartifacts@myazurestorageaccount.blob.core.windows.net/ .</p></li>
<li><p>In the spark cluster, in the environemtn Variables section pass on the AZURE_STORAGE_ACCESS_KEY=""ValueoftheKey"" </p></li>
<li>In the notebook, use mlflow to log metrics, param and finally the model using a snippet like below</li>
</ol>

<pre><code>
with mlflow.start_run():
      lr = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=42)
      lr.fit(train_x, train_y)

      predicted_qualities = lr.predict(test_x)

      (rmse, mae, r2) = eval_metrics(test_y, predicted_qualities)

      print(""Elasticnet model (alpha=%f, l1_ratio=%f):"" % (alpha, l1_ratio))
      print(""  RMSE: %s"" % rmse)
      print(""  MAE: %s"" % mae)
      print(""  R2: %s"" % r2)

      mlflow.log_param(""alpha"", alpha)
      mlflow.log_param(""l1_ratio"", l1_ratio)
      mlflow.log_metric(""rmse"", rmse)
      mlflow.log_metric(""r2"", r2)
      mlflow.log_metric(""mae"", mae)

      mlflow.sklearn.log_model(lr, ""model"")
</code></pre>

<p>Of course before using it , I set the experiment to the one where I have defined the artifacts store to be azure blob storage</p>

<pre><code>experiment_name = ""/Users/user@domain.com/mltestazureblob""
mlflow.set_experiment(experiment_name)
</code></pre>

<p>The metrices and params I can from the MLFlow  UI within Databricks but as since my artifacts location is Azure Blob Storage , I expect the model, the .pkl and conda.yaml file to be in the container in the Azure Blob Storage but when I go to check it, I only see a folder corresponding to the run id of the experiment but with nothing inside.</p>

<p>I do not know what I am missing. In case, someone needs additional details I will be happy to provide.</p>

<p>Point to note everything works fine when I use the default location i.e. dbfs.</p>",1,1,2019-10-23 09:12:45.343 UTC,,,0,databricks|azure-databricks|mlflow,812,2015-04-10 08:31:54.763 UTC,2022-09-24 09:37:37.383 UTC,,596,53,1,80,,,,,,"['databricks', 'mlflow']"
Issues with deploying spark and mlflow to sagemaker,"<p>My goal is to deploy a spark/mlflow to sagemaker with the following command:</p>
<pre><code>    mlflow sagemaker deploy .. 
</code></pre>
<p>I've successfully pushed a image to EC2 with</p>
<pre><code>mlflow sagemaker build-and-push-container
</code></pre>
<p>I encounter errors when attempting to run mlflow sagemaker deploy:</p>
<pre><code>[error] 446#446: *69 upstream timed out (110: Connection timed out) while reading response header from upstream, client: 10.32.0.2, server: , request: &quot;GET /ping HTTP/1.1&quot;, upstream: &quot;http://127.0.0.1:8000/ping&quot;, host: &quot;model.aws.local:8080&quot;
java.io.IOException: Failed to connect to model.aws.local/172.17.0.2:34473
</code></pre>
<p>Therefore, I added the following as I thought I was mishandling pyspark in sagemaker:</p>
<pre><code>classpath = &quot;:&quot;.join(sagemaker_pyspark.classpath_jars()) 
spark = SparkSession.builder.config( &quot;spark.driver.extraClassPath&quot;, classpath ).appName('audit-risk-predictor-training').getOrCreate()          
</code></pre>
<p>However this outputted the following error:</p>
<pre><code>Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: org/apache/commons/configuration/Configuration
    at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.&lt;init&gt;(DefaultMetricsSystem.java:38)
    at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.&lt;clinit&gt;(DefaultMetricsSystem.java:36)
    at org.apache.hadoop.security.UserGroupInformation$UgiMetrics.create(UserGroupInformation.java:134)
    at org.apache.hadoop.security.UserGroupInformation.&lt;clinit&gt;(UserGroupInformation.java:254)
    at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2487)
    at scala.Option.getOrElse(Option.scala:189)
    at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2487)
    at org.apache.spark.SecurityManager.&lt;init&gt;(SecurityManager.scala:79)
    at org.apache.spark.deploy.SparkSubmit.secMgr$lzycompute$1(SparkSubmit.scala:368)
    at org.apache.spark.deploy.SparkSubmit.secMgr$1(SparkSubmit.scala:368)
    at org.apache.spark.deploy.SparkSubmit.$anonfun$prepareSubmitEnvironment$8(SparkSubmit.scala:376)
    at scala.Option.map(Option.scala:230)
    at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:376)
    at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:894)
    at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
    at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
    at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
    at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1039)
    at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1048)
    at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.lang.ClassNotFoundException: org.apache.commons.configuration.Configuration
    at java.net.URLClassLoader.findClass(URLClassLoader.java:387)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
    ... 20 more
---------------------------------------------------------------------------
Exception                                 Traceback (most recent call last)
Input In [7], in &lt;cell line: 3&gt;()
      1 # Create Spark Session
      2 classpath = &quot;:&quot;.join(sagemaker_pyspark.classpath_jars()) 
----&gt; 3 spark = SparkSession.builder.config( &quot;spark.driver.extraClassPath&quot;, classpath ).appName('audit-risk-predictor-training').getOrCreate()

File ~/.local/lib/python3.8/site-packages/pyspark/sql/session.py:228, in SparkSession.Builder.getOrCreate(self)
    226         sparkConf.set(key, value)
    227     # This SparkContext may be an existing one.
--&gt; 228     sc = SparkContext.getOrCreate(sparkConf)
    229 # Do not update `SparkConf` for existing `SparkContext`, as it's shared
    230 # by all sessions.
    231 session = SparkSession(sc)

File ~/.local/lib/python3.8/site-packages/pyspark/context.py:384, in SparkContext.getOrCreate(cls, conf)
    382 with SparkContext._lock:
    383     if SparkContext._active_spark_context is None:
--&gt; 384         SparkContext(conf=conf or SparkConf())
    385     return SparkContext._active_spark_context

File ~/.local/lib/python3.8/site-packages/pyspark/context.py:144, in SparkContext.__init__(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)
    139 if gateway is not None and gateway.gateway_parameters.auth_token is None:
    140     raise ValueError(
    141         &quot;You are trying to pass an insecure Py4j gateway to Spark. This&quot;
    142         &quot; is not allowed as it is a security risk.&quot;)
--&gt; 144 SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)
    145 try:
    146     self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,
    147                   conf, jsc, profiler_cls)

File ~/.local/lib/python3.8/site-packages/pyspark/context.py:331, in SparkContext._ensure_initialized(cls, instance, gateway, conf)
    329 with SparkContext._lock:
    330     if not SparkContext._gateway:
--&gt; 331         SparkContext._gateway = gateway or launch_gateway(conf)
    332         SparkContext._jvm = SparkContext._gateway.jvm
    334     if instance:

File ~/.local/lib/python3.8/site-packages/pyspark/java_gateway.py:108, in launch_gateway(conf, popen_kwargs)
    105     time.sleep(0.1)
    107 if not os.path.isfile(conn_info_file):
--&gt; 108     raise Exception(&quot;Java gateway process exited before sending its port number&quot;)
    110 with open(conn_info_file, &quot;rb&quot;) as info:
    111     gateway_port = read_int(info)

Exception: Java gateway process exited before sending its port number
</code></pre>
<p>Any insight on where I'm going wrong? Is spark capable of running in sagemaker?</p>",0,0,2022-07-17 22:00:01.223 UTC,,,0,python|apache-spark|deployment|amazon-sagemaker|mlflow,59,2022-01-13 10:12:59.037 UTC,2022-09-23 16:16:27.333 UTC,,45,5,0,4,,,,,,"['amazon-sagemaker', 'mlflow']"
Using MLflow and Sagemaker with preprocessing steps,"<p>I'm deploying my models to Sagemaker using MLflow integration. However, my ML pipeline includes some basic preprocessing steps, such as scalers, and I need it to be part of my inference endpoint. Is there a way to do that with MLflow? I looked in the <a href=""https://www.mlflow.org/docs/latest/python_api/mlflow.pyfunc.html"" rel=""nofollow noreferrer"">mlflow_pyfunc</a> is closer to what I want, but I'm not sure if it is compatible with Sagemaker.</p>",0,1,2022-07-20 11:52:22.35 UTC,,,0,python|amazon-sagemaker|mlflow,34,2016-10-17 11:39:35.777 UTC,2022-07-28 17:26:21.417 UTC,,109,1,0,21,,,,,,"['amazon-sagemaker', 'mlflow']"
How to download an artifact from MLFlow using REST?,"<p>I see the Python API:
<code>download_artifacts(run_id: str, path: str, dst_path: Optional[str] = None) → str</code> (<a href=""https://www.mlflow.org/docs/latest/python_api/mlflow.tracking.html#mlflow.tracking.MlflowClient.download_artifacts"" rel=""nofollow noreferrer"">here</a>), but I can't find the equivalent in REST.</p>",0,0,2021-11-08 11:13:52.513 UTC,1.0,2021-11-09 10:44:51.217 UTC,3,rest|databricks|mlflow,243,2009-06-14 12:54:00.077 UTC,2022-09-23 21:20:51.75 UTC,"New York, NY",13408,306,12,687,,,,,,"['databricks', 'mlflow']"
How can I know how many iterations are left when tuning accross multiple hyperparameters in SparkML?,"<p>I'm running a crossvalidation accross a grid of multiple hyperparameters with XgBoost model using Pyspark in Databricks and I would like to know the progress of this operation...So far it has been running for almost 24 hours and I have no idea if it's halfway done or only 10% of the way. I have a 128k combinations of hyperparameters of 5 folds each so a total of 640k runs...</p>
<p>I've tried clicking on MLflow logged run but it's an empty page with an UNFINISHED status. Is there any way to know the progress ?</p>",0,0,2022-09-20 16:10:27.917 UTC,1.0,2022-09-20 17:17:42.41 UTC,0,databricks|cross-validation|apache-spark-mllib|hyperparameters|mlflow,13,2019-05-17 18:46:23.69 UTC,2022-09-23 13:22:46.183 UTC,,11,0,0,0,,,,,,"['databricks', 'mlflow']"
deploy model as endpoint in databricks,"<p>after creating a simple keras model, I would like to deploy it as an endpoint for real-time inference in azure databricks. I created a simple cluster but unfortunately I ma not able to deploy the model itself. the deployment itself cannot be completed and the status is still yellow (pending)
<a href=""https://i.stack.imgur.com/RxbN6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RxbN6.png"" alt=""enter image description here"" /></a></p>
<p>can you please guide me where the error may be? what needs to be checked?  Thank you have a nice day</p>
<p>BR
Tomas</p>",0,1,2022-07-11 13:33:12.123 UTC,,,0,python|databricks|mlflow,48,2020-08-24 21:06:06.307 UTC,2022-07-27 08:33:48.143 UTC,,21,0,0,4,,,,,,"['databricks', 'mlflow']"
How to share models in a multitenant enviroment with Mlflow?,"<p>The company I work for are using Databricks with Azure as a storage service. My group is trying to create a centralized model registry that allows us to push and pull models into different instances of Databricks. We are aware that we can share models within the same subscription (<a href=""https://docs.microsoft.com/en-us/azure/databricks/applications/machine-learning/manage-model-lifecycle/multiple-workspaces"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/databricks/applications/machine-learning/manage-model-lifecycle/multiple-workspaces</a>) however we have multiple subscriptions so this wont work for us. From what I've read there are two solutions for this. Use Azure blob storage or an SQL solution. Unfortunately I cant find much info online. Anyone have any idea how I can implement this?</p>",1,0,2022-07-19 16:33:46.997 UTC,,,0,azure|azure-blob-storage|databricks|mlflow,49,2021-09-21 04:45:44.943 UTC,2022-09-24 22:54:45.03 UTC,,1,0,0,3,,,,,,"['databricks', 'mlflow']"
How to get access of some graphs /plots in Databricks workflow?,"<p>I am running a notebook on another notebook using workflow</p>
<pre><code>dbutils.notebook.run(&quot;python_EDA&quot;, 0, {&quot;param1&quot;: &quot;var1&quot;, &quot;param2&quot;: &quot;var2&quot;})
</code></pre>
<p>In my python_EDA notebook , there are some plots and graphs that are being plotted inside the Notebook job #029309238939093 after running the workflow command .</p>
<p>How to get hold of those since everything is running as a task?
Is it stored in some location ?</p>
<p>I read its stored as MLflow artifacts, how to access these ?</p>",1,0,2022-06-01 11:45:27.143 UTC,,,0,workflow|databricks|mlflow,26,2016-06-14 09:15:17.29 UTC,2022-09-23 05:00:12.03 UTC,"Mumbai, Maharashtra, India",559,368,28,171,,,,,,"['databricks', 'mlflow']"
upload a pre-trained model on databricks mlflow experiment,"<p>Is it possible to upload  a pre-trained machine learning model (from my local computer, for which I generated a model.pkl) on databricks, and serve it?
Or is it impossible on Databricks ?</p>",0,0,2022-07-07 14:38:21.163 UTC,,2022-07-08 06:55:10.48 UTC,0,machine-learning|databricks|mlflow|pre-trained-model,48,2019-03-03 11:21:20.527 UTC,2022-09-23 12:01:19.95 UTC,France,314,18,2,105,,,,,,"['databricks', 'mlflow']"
How can I save an Onnx model that takes a array as an input type?,"<p>I have a model pipeline that includes a word2vec step before a logistic regression. I want to save the model using ONNX for model portability. There are three datasets: two properties columns where the data starts as long format and is aggregated into an array before being inputted into the model and one with information about each IDs.</p>
<p>The spark Pipeline I'm using has a few transformation stages like a string indexer and one hot encoder on the string column <code>Value 3</code> and uses a word2vec model on the property columns.</p>
<pre><code>pipeline = Pipeline(stages=[string_indexer, one_hot_encoder, word2vec1, word2vec2, vector_assembler, logistic_regression])
</code></pre>
<p>When I run the model and try to save it in Onnx format, it tells me that it cannot map the array input for the word2vecs to Onnx types; Onnx only accepts certain types (see <a href=""https://github.com/onnx/onnxmltools/blob/master/onnxmltools/convert/sparkml/utils.py"" rel=""nofollow noreferrer"">here</a>) and this doesn't include arrays.</p>
<p>I tried moving the data transformation to a Transformer pipeline stage <a href=""https://stackoverflow.com/questions/51415784/how-to-add-my-own-function-as-a-custom-stage-in-a-ml-pyspark-pipeline"">like this</a> but I can't input three separate dataframes as input to the Transformer and if I make it so the two property dataframes are inputs to the class initialization like:</p>
<pre><code>data_transformer = DataTransformer(properties1, properties2)
pipeline = Pipeline(stages=[data_transformer, the_other_stages])
results = pipeline.transform(id_table)
</code></pre>
<p>I can't log/save the model with MLFlow because the stage is not MLWritable and it doesn't seem like best practice to have two potentially giant dataframes as an input to the class initialization.</p>
<p>Is there a way to use an array type as an input to an Onnx model?</p>
<p>If not, how would you approach the data transformation so that all the inputs to the pipeline can be of a type that Onnx will accept but that will also work as a loggable model in MLFlow?</p>
<p>I am working in Azure Databricks.</p>",0,0,2021-12-20 14:48:12.073 UTC,,,0,pyspark|databricks|onnx|mlflow,212,2021-07-06 20:01:11.153 UTC,2022-09-23 16:03:49.94 UTC,,41,0,0,8,,,,,,"['databricks', 'mlflow']"
How to set custom path for databricks mlflow artifacts on s3,"<p>I've created an empty experiments from databricks experiments console and given the path for my artifacts on s3 i.e. s3:///. When i run the scripts, the artifacts are stored at</p>
<pre><code>s3://&lt;bucket&gt;//&lt;32 char id&gt;/artifacts/model-Elasticnet/model.pkl
</code></pre>
<p>I want to replace //&lt;32 char id&gt;/artifacts/ with /datetime/artifacts/ so something like</p>
<pre><code>s3://&lt;bucket&gt;/&lt;datetime&gt;/artifacts/model-Elasticnet/model.pkl
</code></pre>
<p>Is there any way i could achieve that?</p>
<p><a href=""https://i.stack.imgur.com/wUDcE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wUDcE.png"" alt=""enter image description here"" /></a></p>
<p>Note: experiment_id is from databricks experiment console</p>",0,1,2022-04-04 14:12:33.88 UTC,,2022-04-07 07:21:35.153 UTC,2,databricks|mlflow|aws-databricks|mlops,140,2018-05-12 15:57:03.12 UTC,2022-09-24 08:37:01.693 UTC,"Berlin, Germany",962,106,9,128,,,,,,"['databricks', 'mlflow']"
Error loading model from mlflow: java.io.StreamCorruptedException: invalid type code: 00,"<p>I'm using Databricks Connect version 9.1.16 to connect to a databricks external cluster with spark version 3.1 and download a Pyspark ML model that's been trained and saved using mlflow.</p>
<pre class=""lang-py prettyprint-override""><code>mlflow.set_tracking_uri(&quot;databricks&quot;)
model_h = mlflow.spark.load_model(model_uri=&quot;models:/model_name/model_version&quot;)
</code></pre>
<p>I get the following output and error:</p>
<pre><code>2022/08/26 11:54:18 INFO mlflow.spark: 'models:/model_name/model_version' resolved as 'dbfs://databricks/databricks/mlflow-registry/model_id/models/model'
2022/08/26 11:54:25 INFO mlflow.spark: URI 'dbfs://databricks/databricks/mlflow-registry/model_id/models/model/sparkml' does not point to the current DFS.
2022/08/26 11:54:25 INFO mlflow.spark: File 'dbfs://databricks/databricks/mlflow-registry/model_id/models/model/sparkml' not found on DFS. Will attempt to upload the file.
2022/08/26 11:55:06 INFO mlflow.spark: Copied SparkML model to /tmp/mlflow/model_id
---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
c:\Users\carlafernandez\Documents\my_notebook.ipynb Celda 5 in &lt;cell line: 2&gt;()
      1 mlflow.set_tracking_uri(&quot;databricks&quot;)
----&gt; 2 model_h = mlflow.spark.load_model(model_uri=&quot;models:/model_name/model_version&quot;)

File c:\Users\carlafernandez\miniconda3\envs\prueba_databricks_connect\lib\site-packages\mlflow\spark.py:711, in load_model(model_uri, dfs_tmpdir)
    708 local_model_path = _download_artifact_from_uri(model_uri)
    709 _add_code_from_conf_to_system_path(local_model_path, flavor_conf)
--&gt; 711 return _load_model(model_uri=model_uri, dfs_tmpdir_base=dfs_tmpdir)

File c:\Users\carlafernandez\miniconda3\envs\prueba_databricks_connect\lib\site-packages\mlflow\spark.py:660, in _load_model(model_uri, dfs_tmpdir_base)
    658     return _load_model_databricks(model_uri, dfs_tmpdir)
    659 model_uri = _HadoopFileSystem.maybe_copy_from_uri(model_uri, dfs_tmpdir)
--&gt; 660 return PipelineModel.load(model_uri)

File c:\Users\carlafernandez\miniconda3\envs\prueba_databricks_connect\lib\site-packages\pyspark\ml\util.py:463, in MLReadable.load(cls, path)
    460 @classmethod
    461 def load(cls, path):
    462     &quot;&quot;&quot;Reads an ML instance from the input path, a shortcut of `read().load(path)`.&quot;&quot;&quot;
--&gt; 463     return cls.read().load(path)

File c:\Users\carlafernandez\miniconda3\envs\prueba_databricks_connect\lib\site-packages\pyspark\ml\pipeline.py:258, in PipelineModelReader.load(self, path)
    256 metadata = DefaultParamsReader.loadMetadata(path, self.sc)
    257 if 'language' not in metadata['paramMap'] or metadata['paramMap']['language'] != 'Python':
--&gt; 258     return JavaMLReader(self.cls).load(path)
    259 else:
    260     uid, stages = PipelineSharedReadWrite.load(metadata, self.sc, path)

File c:\Users\carlafernandez\miniconda3\envs\prueba_databricks_connect\lib\site-packages\pyspark\ml\util.py:413, in JavaMLReader.load(self, path)
    411 if not isinstance(path, str):
    412     raise TypeError(&quot;path should be a string, got type %s&quot; % type(path))
--&gt; 413 java_obj = self._jread.load(path)
    414 if not hasattr(self._clazz, &quot;_from_java&quot;):
    415     raise NotImplementedError(&quot;This Java ML type cannot be loaded into Python currently: %r&quot;
    416                               % self._clazz)

File c:\Users\carlafernandez\miniconda3\envs\prueba_databricks_connect\lib\site-packages\py4j\java_gateway.py:1304, in JavaMember.__call__(self, *args)
   1298 command = proto.CALL_COMMAND_NAME +\
   1299     self.command_header +\
   1300     args_command +\
   1301     proto.END_COMMAND_PART
   1303 answer = self.gateway_client.send_command(command)
-&gt; 1304 return_value = get_return_value(
   1305     answer, self.gateway_client, self.target_id, self.name)
   1307 for temp_arg in temp_args:
   1308     temp_arg._detach()

File c:\Users\carlafernandez\miniconda3\envs\prueba_databricks_connect\lib\site-packages\pyspark\sql\utils.py:117, in capture_sql_exception.&lt;locals&gt;.deco(*a, **kw)
    115 def deco(*a, **kw):
    116     try:
--&gt; 117         return f(*a, **kw)
    118     except py4j.protocol.Py4JJavaError as e:
    119         converted = convert_exception(e.java_exception)

File c:\Users\carlafernandez\miniconda3\envs\prueba_databricks_connect\lib\site-packages\py4j\protocol.py:326, in get_return_value(answer, gateway_client, target_id, name)
    324 value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)
    325 if answer[1] == REFERENCE_TYPE:
--&gt; 326     raise Py4JJavaError(
    327         &quot;An error occurred while calling {0}{1}{2}.\n&quot;.
    328         format(target_id, &quot;.&quot;, name), value)
    329 else:
    330     raise Py4JError(
    331         &quot;An error occurred while calling {0}{1}{2}. Trace:\n{3}\n&quot;.
    332         format(target_id, &quot;.&quot;, name, value))

Py4JJavaError: An error occurred while calling o645.load.
: java.io.StreamCorruptedException: invalid type code: 00
    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1698)
    at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)
    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)
    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)
    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)
    at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)
    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)
    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)
    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)
    at java.io.ObjectInputStream.readObject(ObjectInputStream.java:503)
    at java.io.ObjectInputStream.readObject(ObjectInputStream.java:461)
    at scala.collection.immutable.List$SerializationProxy.readObject(List.scala:488)
    at sun.reflect.GeneratedMethodAccessor311.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1184)
    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2296)
    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)
    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)
    at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)
    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)
    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)
    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)
    at java.io.ObjectInputStream.readArray(ObjectInputStream.java:2093)
    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1655)
    at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)
    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)
    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)
    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)
    at java.io.ObjectInputStream.readArray(ObjectInputStream.java:2093)
    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1655)
    at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)
    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)
    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)
    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)
    at java.io.ObjectInputStream.readObject(ObjectInputStream.java:503)
    at java.io.ObjectInputStream.readObject(ObjectInputStream.java:461)
    at org.apache.spark.sql.util.ProtoSerializer.$anonfun$deserializeObject$1(ProtoSerializer.scala:6631)
    at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
    at org.apache.spark.sql.util.ProtoSerializer.deserializeObject(ProtoSerializer.scala:6616)
    at com.databricks.service.SparkServiceRPCHandler.execute0(SparkServiceRPCHandler.scala:728)
    at com.databricks.service.SparkServiceRPCHandler.$anonfun$executeRPC0$1(SparkServiceRPCHandler.scala:477)
    at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
    at com.databricks.service.SparkServiceRPCHandler.executeRPC0(SparkServiceRPCHandler.scala:372)
    at com.databricks.service.SparkServiceRPCHandler$$anon$2.call(SparkServiceRPCHandler.scala:323)
    at com.databricks.service.SparkServiceRPCHandler$$anon$2.call(SparkServiceRPCHandler.scala:309)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at com.databricks.service.SparkServiceRPCHandler.$anonfun$executeRPC$1(SparkServiceRPCHandler.scala:359)
    at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
    at com.databricks.service.SparkServiceRPCHandler.executeRPC(SparkServiceRPCHandler.scala:336)
    at com.databricks.service.SparkServiceRPCServlet.doPost(SparkServiceRPCServer.scala:167)
    at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)
    at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
    at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)
    at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:550)
    at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)
    at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)
    at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
    at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
    at org.eclipse.jetty.server.Server.handle(Server.java:516)
    at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)
    at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)
    at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:380)
    at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)
    at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
    at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)
    at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
    at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)
    at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)
    at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)
    at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)
    at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:383)
    at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)
    at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)
    at java.lang.Thread.run(Thread.java:748)
</code></pre>
<p>So it seems like it's able to find a copy the model, but then somehow it cannot read it. It's worth noting that the same <strong>works in a databricks notebook</strong>, the problem only occurs using databricks connect.</p>",0,0,2022-08-26 10:05:08.487 UTC,,,0,pyspark|databricks|mlflow|databricks-connect,23,2017-09-24 08:15:07.953 UTC,2022-09-23 11:38:23.76 UTC,"Madrid, España",56,47,0,16,,,,,,"['databricks', 'mlflow']"
How to get url of mlflow logged artifacts?,"<p>I am running an ML pipeline, at the end of which I am logging certain information using mlflow. I was mostly going through Databricks' official mlflow tracking tutorial.</p>
<pre><code>import mlflow
import mlflow.sklearn
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error

with mlflow.start_run():
  n_estimators = 100
  max_depth = 6
  max_features = 3
  # Create and train model
  rf = RandomForestRegressor(n_estimators = n_estimators, max_depth = max_depth, max_features = max_features)
  rf.fit(X_train, y_train)
  # Make predictions
  predictions = rf.predict(X_test)
  
  # Log parameters
  mlflow.log_param(&quot;num_trees&quot;, n_estimators)
  mlflow.log_param(&quot;maxdepth&quot;, max_depth)
  mlflow.log_param(&quot;max_feat&quot;, max_features)
  
  # Log model
  mlflow.sklearn.log_model(rf, &quot;random-forest-model&quot;)
  
  # Create metrics
  mse = mean_squared_error(y_test, predictions)
    
  # Log metrics
  mlflow.log_metric(&quot;mse&quot;, mse)
</code></pre>
<p>When I run the above block of code in Databricks notebook, the below status message shows:</p>
<pre><code>(1) MLflow run
Logged 1 run to an experiment in MLflow. Learn more
</code></pre>
<p>And I can view the logged information by clicking on &quot;1 run.&quot;</p>
<p>However, I would like to automatically retrieve this link. In particular, I need the link to the mlflow uri where the artifacts are stored. This link is in the following format:</p>
<pre><code>https://mycompany-dev.cloud.databricks.com/?o=&lt;ID_1&gt;#mlflow/experiments/&lt;ID_2&gt;/runs/&lt;ID_3&gt;
</code></pre>
<p>I tried investigating the url and finding the various id codes that are present in it by printing the following information:</p>
<pre><code>print(&quot;Tracking URI: &quot;, mlflow.get_tracking_uri())
print(&quot;Run id:&quot;, run.info.run_id)
print(&quot;Experiment:&quot;, run.info.experiment_id)
</code></pre>
<p>I figured out that <code>&lt;ID_2&gt;</code> in the link above is the <code>experiment_id</code> and <code>&lt;ID_3&gt;</code> is the <code>run_id</code>. But I have no idea what <code>&lt;ID_1&gt;</code> stands for. Also, I believe there should be a built-in functionality to retrieve the link of saved artifacts, instead of manually having to build up the link from sections. However, I haven't found such a funcitonality in the documentation so far.</p>
<p><strong>Edit:</strong> Now I discovered that <code>&lt;ID_1&gt;</code> is the Databricks workplace id. But it is still a question how I can access it programatically.</p>",0,0,2022-09-22 13:46:21.563 UTC,,2022-09-22 14:03:46.853 UTC,0,python|machine-learning|databricks|azure-databricks|mlflow,27,2015-07-16 10:20:07.427 UTC,2022-09-24 16:13:50.543 UTC,,968,512,10,241,,,,,,"['databricks', 'mlflow']"
MLflow - How the model storage aborescence works behind the model registry APIs?,"<p>I want to understand how the model storage aborescence works behind the model registry APIs.</p>
<p>I wanted to know if the stageworks like a tag,
if the . pickel model is moved to different branches depending on the stage.
same question for versions ?</p>
<p>I'm working with python on Databricks.
sincerely,</p>
<p>nicolas</p>",0,1,2022-06-29 09:03:39.827 UTC,,,0,python|model|databricks|mlflow,15,2019-06-03 13:12:59.187 UTC,2022-06-29 09:32:51.713 UTC,,1,0,0,0,,,,,,"['mlflow', 'databricks']"
What are the pros and cons of using DVC and Pachyderm?,"<p>What are the pros and cons of using either of these?</p>

<p><a href=""https://github.com/iterative/dvc"" rel=""nofollow noreferrer"">https://github.com/iterative/dvc</a></p>

<p><a href=""https://github.com/pachyderm/pachyderm"" rel=""nofollow noreferrer"">https://github.com/pachyderm/pachyderm</a></p>",1,1,2019-07-04 06:12:59.043 UTC,,,1,machine-learning|version-control|data-science|dvc|pachyderm,1635,2019-07-04 06:06:43.123 UTC,2019-07-18 00:21:59.09 UTC,,41,0,0,3,,,,,,"['dvc', 'pachyderm']"
"Mlflow ""invocations"" prefix","<p>We are deploying some MLflow models using docker and Kubernetes and
We are using Ingress load balancer in K8S is mandatory for security reasons, but right now we need to deploy more than one mlflow image in the same cluster.
When we run the container the model serving start the application using  &quot;/invocations&quot; path for the POST requests.
That means that we cann´t differentiate the model using the prefix, cause every container is using the same prefix.</p>
<p>My question is,is there any way to change &quot;/invocations&quot; prefix on model Mlflow images?</p>",0,0,2022-08-02 20:21:59.17 UTC,,2022-08-03 21:46:53.487 UTC,0,python|databricks|mlflow,44,2022-06-10 19:20:31.17 UTC,2022-09-23 16:37:58.107 UTC,,21,0,0,2,,,,,,"['databricks', 'mlflow']"
How to migrate MlFlow experiments from one Databricks workspace to another with registered models?,"<p>so unfortunatly we have to redeploy our Databricks Workspace in which we use the MlFlow functonality with the Experiments and the registering of Models.</p>
<p>However if you export the user folder where the eyperiment is saved with a DBC and import it into the new workspace, the Experiments are not migrated and are just missing.</p>
<p>So the easiest solution did not work. The next thing I tried was to create a new experiment in the new workspace. Copy all the experiment data from the dbfs of the old workspace (with dbfs cp -r dbfs:/databricks/mlflow source, and then the same again to upload it to the new workspace) to the new one. And then just reference the location of the data to the experiment like in the following picture:</p>
<p><a href=""https://i.stack.imgur.com/emgGs.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/emgGs.png"" alt=""Create Experiment with existing path"" /></a></p>
<p>This is also not working, no run is visible, although the path is already existing.</p>
<p>The next idea was that the registred models are the most important one so at least those should be there and accessible. For that I used the documentation here: <a href=""https://www.mlflow.org/docs/latest/model-registry.html"" rel=""nofollow noreferrer"">https://www.mlflow.org/docs/latest/model-registry.html</a>.</p>
<p>With the following code you get a list of the registred models on the old workspace with the reference on the run_id and location.</p>
<pre><code>from mlflow.tracking import MlflowClient

client = MlflowClient()
for rm in client.list_registered_models():
    pprint(dict(rm), indent=4)
</code></pre>
<p>And with this code you can add models to a model registry with a reference to the location of the artifact data (on the new workspace):</p>
<pre><code># first the general model must be defined
client.create_registered_model(name='MyModel')

# and then the run of the model you want to registre will be added to the model as version one
client.create_model_version( name='MyModel', run_id='9fde022012046af935fe52435840cf1', source='dbfs:/databricks/mlflow/experiment_id/run_id/artifacts/model')
</code></pre>
<p>But that did also not worked out. if you go into the Model Registry you get a message like this: <a href=""https://i.stack.imgur.com/Ham4y.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Ham4y.png"" alt=""error message of the registred model"" /></a>.</p>
<p>And I really checked, at the given path (the source) there the data is really uploaded and also a model is existing.</p>
<p>Do you have any new ideas to migrate those models in Databricks?</p>",2,0,2020-09-09 07:00:21.84 UTC,,,4,migration|databricks|azure-databricks|mlflow,1704,2015-07-26 16:16:59.34 UTC,2022-06-14 06:51:49.997 UTC,,416,20,0,90,,,,,,"['databricks', 'mlflow']"
Filter mlflow runs by run name (runName),<p>How to filter out runs whose runName starts with the string &quot;iteration11_run_number&quot;?</p>,1,0,2022-06-01 05:39:12.59 UTC,,,0,azure|databricks|mlflow,586,2017-01-14 09:03:21.68 UTC,2022-09-15 14:59:07.703 UTC,India,314,963,7,37,,,,,,"['databricks', 'mlflow']"
MLFlow Experiment in Databricks Regressors,"<p>I'm new to Databricks and following a tutorial on mlflow in Databricks:</p>
<p><a href=""https://www.youtube.com/watch?v=_PxEdtAQXME&amp;t=1471s"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=_PxEdtAQXME&amp;t=1471s</a></p>
<p>for a simple dataset on bike rentals:</p>
<p><a href=""https://www.kaggle.com/datasets/archit9406/bike-sharing"" rel=""nofollow noreferrer"">https://www.kaggle.com/datasets/archit9406/bike-sharing</a></p>
<p>Objective is to predict bike rentals or in this case &quot;cnt&quot;</p>
<p>I've created an ml cluster in Databricks and followed the code to as far as cmd 6 where it seems to be referencing other &quot;treeDepth&quot; data.</p>
<p>When I run this I get an AttributeError: 'GBTRegressor' object has no attribute ' setMaxDept'
I've tried to re-enter the variables in my dataframe to be more in line with the code but I cant seem to get it to work.</p>
<p>My overall objective is to get the script running and predicting &quot;cnt&quot;</p>
<p>Can anyone help with this?</p>
<pre><code>    Cmd1
    #install
    %pip install mlflow
</code></pre>
<pre><code>    Cmd 2
    #model name
    modelName = &quot;BikesModelMVP2&quot;
</code></pre>
<pre><code>    Cmd 3
    #importing a variety of different packages
    import mlflow
    from pyspark.ml.evaluation import RegressionEvaluator
    from pyspark.ml import PipelineModel
    from pyspark.ml import Pipeline
    from pyspark.ml.regression import DecisionTreeRegressor, RandomForestRegressor, GBTRegressor
    from pyspark.ml.feature import VectorAssembler, VectorIndexer, MinMaxScaler
    import mlflow.spark
    #import mlflow.xgboost
    import pyspark.sql.functions as F
    from mlflow.tracking.client import MlflowClient
    from mlflow.entities.model_registry.model_version_status import ModelVersionStatus
</code></pre>
<pre><code>    Cmd 4
    print(mlflow.__version__)
</code></pre>
<pre><code>    Cmd 5
    #reading in some data from dataset that's just abailable inside Databricks
    dataLocation = &quot;/databricks-datasets/bikeSharing/data-001/hour.csv&quot;
    
    df = (spark
         .read
         .option(&quot;header&quot;,&quot;true&quot;)
         .option(&quot;inferSchema&quot;,&quot;true&quot;)
         .csv(dataLocation)
         )
    
    df = df.drop(&quot;instant&quot;,&quot;dteday&quot;,&quot;casual&quot;,&quot;holiday&quot;,&quot;weekday&quot;,&quot;registered&quot;)
    
    df = df.withColumn(&quot;AMPM&quot;, F.when(F.col(&quot;hr&quot;)&lt;12,1).otherwise(0))
</code></pre>
<pre><code>    Cmd 6
    #Function that's going to create a machine learning model for me
    def runModel(_depth,_iter,_ntrees,_modelname):
    #def runModel(season,yr,mnth,hr,workingday,weathersit,temp,atemp,hum,windspeed):
        #with mlflow.start_run(experiment_id=&quot;2526553914207328&quot;):
        with mlflow.start_run():
            seed = 42
            trainDF, testDF = df.randomSplit([0.7,0.3], seed=42)
            
            #clf = RandomForestRegressor()
            clf = GBTRegressor()
            
            clf.setLabelCol(&quot;cnt&quot;)
            
            treeDepth = _depth
            numTrees = _ntrees
            iterations = _iter
            clf.setMaxDept(treeDepth)
            clf.setMaxIter(iterations)
            
            #clf.setNumTrees(numTrees)
            mlflow.log_param(&quot;treeDepth&quot;, treeDepth)
            mlflow.log_param(&quot;numTrees&quot;, numTrees)
            mlflow.log_param(&quot;numIterations&quot;, iterations)
            
            featureCols = df.drop(&quot;cnt&quot;).columns # Removes &quot;cnt&quot;
            vectorAssembler = VectorAssembler(inputCols=featureCols, outputCol=&quot;rawFeatures&quot;)
            vectorIndexer = VectorIndexer(inputCol=&quot;rawFeatures&quot;, outputCol=&quot;features&quot;, maxCategories=4)
            
            pipeline = Pipeline().setStages([vectorAssembler, vectorIndexer, clf])
            pipelineModel = pipeline.fit(trainDF)
            
            predictionsDF = pipelineModel.transform(testDF)
            
            evaluator = RegressionEvaluator().setMetricName(&quot;rmse&quot;).setPredictionCol(&quot;prediction&quot;).setLabelCol(&quot;cnt&quot;)
            evaluatorR2 = RegressionEvaluator().setMetricName(&quot;r2&quot;).setPredictionCol(&quot;prediction&quot;).setLabelCol(&quot;cnt&quot;)
            
            mlflow.log_metric(&quot;rmse&quot;, evaluator.evaluate(predictionsDF))
            mlflow.log_metric(&quot;r2&quot;, evaluatorR2.evaluate(predictionsDF))
            
            mlflow.log_artifact(&quot;/dbfs&quot;+ dataLocation)
            
            mlflow.spark.log_model(spark_model=pipelineModel, artifact_path=&quot;model&quot; , registered_model_name=_modelname)
            
            run_id = mlflow.active_run().info.run_id
            
            return run_id
    
</code></pre>",0,0,2022-07-01 17:47:29.83 UTC,,,0,regression|databricks|mlflow,50,2021-11-15 15:29:55.277 UTC,2022-08-15 11:11:27.977 UTC,,1,0,0,4,,,,,,"['mlflow', 'databricks']"
MLflow Experiments Tracking : local (dev tools - vscode) to databricks workspace,"<p>I had configured my databricks workspace in local using,</p>
<p><code>databricks configure --profile &lt;profile_name&gt; --token</code></p>
<p>by which I am able to list the clusters and create secret scope.</p>
<p>But I am unable to create mlflow experiments. I had set the tracking uri to &quot;databricks&quot; and also tested with &quot;databricks/&lt;profile_name&quot; and tested but i am unable to create or track any experiments on my databricks workspace.</p>
<p>I get this following error;</p>
<p><code>from mlflow.tracking import MlflowClient client = MlflowClient() mlflow.set_tracking_uri(&quot;databricks&quot;) experiment =  client.get_experiment_by_name('/Shared/test')</code></p>
<p>MlflowException: API request to endpoint was successful but the response body was not in a valid JSON format. Response body: '&lt;!doctype html&gt;&lt;html&gt;&lt;head&gt;&lt;meta charset=&quot;utf-8&quot;/&gt;&lt;meta http-equiv=&quot;Content-Language&quot; content=&quot;en&quot;/&gt;&lt;title&gt;Databricks - Sign In&lt;/title&gt;&lt;meta name=&quot;viewport&quot; content=&quot;width=960&quot;/&gt;&lt;link rel=&quot;icon&quot; type=&quot;image/png&quot; href=&quot;/favicon.ico&quot;/&gt;&lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html; charset=UTF8&quot;/&gt;&lt;link rel=&quot;icon&quot; href=&quot;/favicon.ico&quot;&gt;&lt;script defer=&quot;defer&quot; src=&quot;/login/login.0ceb14c0.js&quot;&gt;&lt;/script&gt;&lt;/head&gt;&lt;body class=&quot;light-mode&quot;&gt;&lt;uses-legacy-bootstrap&gt;&lt;div id=&quot;login-page&quot;&gt;&lt;/div&gt;&lt;/uses-legacy-bootstrap&gt;&lt;/body&gt;&lt;/html&gt;'</p>
<p>Could someone help me on what I am missing here?</p>
<p>I am expecting to create/track mlflow experiements in databricks workspace via dev-tools(vscode).</p>",1,0,2022-04-05 13:14:45.623 UTC,,,0,databricks|azure-databricks|mlflow,220,2018-01-10 07:23:49.903 UTC,2022-06-20 10:34:36.247 UTC,,5,0,0,6,"<p>I had the same problem while trying to load a model from model registry with mismatching versions (client 1.22.0).</p>
<p>I had to downgrade the client version to make it work.</p>
<p>Downgraded first the client to 1.21 and then server to 1.20</p>
<p>Refer - <a href=""https://docs.databricks.com/dev-tools/api/latest/mlflow.html#operation/transition-model-version-stage"" rel=""nofollow noreferrer"">https://docs.databricks.com/dev-tools/api/latest/mlflow.html#operation/transition-model-version-stage</a></p>",0.0,2022-04-06 06:17:24.71 UTC,,0.0,"['databricks', 'mlflow']"
MLflow load model fails Python,"<p>I am trying to build an API using an MLflow model.</p>
<p>the funny thing is it works from one location on my PC and not from another. So, the reason for doing I wanted to change my repo etc.</p>
<p>So, the simple code of</p>
<pre><code>from mlflow.pyfunc import load_model
MODEL_ARTIFACT_PATH = &quot;./model/model_name/&quot;
MODEL = load_model(MODEL_ARTIFACT_PATH)
</code></pre>
<p>now fails with</p>
<pre><code>ERROR:    Traceback (most recent call last):
  File &quot;/usr/local/lib/python3.8/dist-packages/starlette/routing.py&quot;, line 540, in lifespan
    async for item in self.lifespan_context(app):
  File &quot;/usr/local/lib/python3.8/dist-packages/starlette/routing.py&quot;, line 481, in default_lifespan
    await self.startup()
  File &quot;/usr/local/lib/python3.8/dist-packages/starlette/routing.py&quot;, line 516, in startup
    await handler()
  File &quot;/code/./app/main.py&quot;, line 32, in startup_load_model
    MODEL = load_model(MODEL_ARTIFACT_PATH)
  File &quot;/usr/local/lib/python3.8/dist-packages/mlflow/pyfunc/__init__.py&quot;, line 733, in load_model
    model_impl = importlib.import_module(conf[MAIN])._load_pyfunc(data_path)
  File &quot;/usr/local/lib/python3.8/dist-packages/mlflow/spark.py&quot;, line 737, in _load_pyfunc
    return _PyFuncModelWrapper(spark, _load_model(model_uri=path))
  File &quot;/usr/local/lib/python3.8/dist-packages/mlflow/spark.py&quot;, line 656, in _load_model
    return PipelineModel.load(model_uri)
  File &quot;/usr/local/lib/python3.8/dist-packages/pyspark/ml/util.py&quot;, line 332, in load
    return cls.read().load(path)
  File &quot;/usr/local/lib/python3.8/dist-packages/pyspark/ml/pipeline.py&quot;, line 258, in load
    return JavaMLReader(self.cls).load(path)
  File &quot;/usr/local/lib/python3.8/dist-packages/pyspark/ml/util.py&quot;, line 282, in load
    java_obj = self._jread.load(path)
  File &quot;/usr/local/lib/python3.8/dist-packages/py4j/java_gateway.py&quot;, line 1321, in __call__
    return_value = get_return_value(
  File &quot;/usr/local/lib/python3.8/dist-packages/pyspark/sql/utils.py&quot;, line 117, in deco
    raise converted from None
pyspark.sql.utils.AnalysisException: Unable to infer schema for Parquet. It must be specified manually.
</code></pre>
<p>The model artifacts are already downloaded to the folder /model folder which has the following structure.</p>
<p><a href=""https://i.stack.imgur.com/oqxRW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/oqxRW.png"" alt=""enter image description here"" /></a></p>
<p>the load model call is in the main.py file
As I mentioned it works from another directory, but there is no reference to any absolute paths. Also, I have made sure that my package references are identical. e,g I have pinned them all down</p>
<pre><code># Model
mlflow==1.25.1
protobuf==3.20.1
pyspark==3.2.1
scipy==1.6.2
six==1.15.0
</code></pre>
<p>also, the same docker file is used both places, which among other things, makes sure that the final resulting folder structure is the same</p>
<pre><code>......other stuffs

COPY ./app /code/app
COPY ./model /code/model
</code></pre>
<p>what can explain it throwing this exception whereas in another location (on my PC), it works (same model artifacts) ?</p>
<p>Since it uses load_model function, it should be able to read the parquet files ?</p>
<p>Any question and I can explain.</p>
<p>EDIT1: I have debugged this a little more in the docker container and it seems the parquet files in the itemFactors folder (listed in my screenshot above) are not getting copied over to my image , even though I have the copy command to copy all files under the model folder. It is copying the _started , _committed and _SUCCESS files, just not the parquet files. Anyone knows why would that be? I DO NOT have a .dockerignore file. Why are those files ignored while copying?</p>",1,0,2022-06-13 14:21:39.67 UTC,,2022-06-13 15:37:50.387 UTC,2,python|docker|databricks|mlflow,109,2015-04-10 08:31:54.763 UTC,2022-09-24 09:37:37.383 UTC,,596,53,1,80,"<p>I found the problem. Like I wrote in the EDIT1 of my post, with further observations, the parquet files were missing in the docker container. That was strange because I was copying the entire folder in my Dockerfile.</p>
<p>I then realized that I was hitting this problem <a href=""https://github.com/moby/buildkit/issues/1366"" rel=""nofollow noreferrer"">mentioned here</a>. File paths exceeding 260 characters, silently fail and do not get copied over to the docker container. This was really frustrating because nothing failed during build and then during run, it gave me that cryptic error of &quot;unable to infer schema for parquet&quot;, essentially because the parquet files were not copied over during docker build.</p>",0.0,2022-06-14 07:29:05.993 UTC,2022-06-14 10:34:01.06 UTC,1.0,"['databricks', 'mlflow']"
How can I retrive the model.pkl in the experiment in Databricks,"<p>I want to retrieve the pickle off my trained model, which I know is in the run file inside my experiments in Databricks.</p>
<p>It seems that the <code>mlflow.pyfunc.load_model</code> can only do the <code>predict</code> method.</p>
<p>There is an option to directly access the pickle?</p>
<p>I also tried to use the path in the run using the <code>pickle.load(path)</code> (example of path: dbfs:/databricks/mlflow-tracking/20526156406/92f3ec23bf614c9d934dd0195/artifacts/model/model.pkl).</p>",2,0,2021-08-09 21:26:51.17 UTC,,,1,python|azure|databricks|datastore|mlflow,3081,2018-03-26 15:02:34.45 UTC,2022-09-23 02:49:37.96 UTC,"São Paulo, State of São Paulo, Brazil",96,10,0,31,"<p>I recently found the solution which can be done by the following two approaches:</p>
<ol>
<li>Use the customized predict function at the moment of saving the model (check <a href=""https://www.mlflow.org/docs/latest/models.html#model-customization"" rel=""nofollow noreferrer"">databricks</a> documentation for more details).</li>
</ol>
<p>example give by Databricks</p>
<pre><code>class AddN(mlflow.pyfunc.PythonModel):

    def __init__(self, n):
        self.n = n

    def predict(self, context, model_input):
        return model_input.apply(lambda column: column + self.n)
# Construct and save the model
model_path = &quot;add_n_model&quot;
add5_model = AddN(n=5)
mlflow.pyfunc.save_model(path=model_path, python_model=add5_model)

# Load the model in `python_function` format
loaded_model = mlflow.pyfunc.load_model(model_path)
</code></pre>
<ol start=""2"">
<li>Load the model artefacts as we are downloading the artefact:</li>
</ol>
<pre><code>from mlflow.tracking import MlflowClient

client = MlflowClient()

tmp_path = client.download_artifacts(run_id=&quot;0c7946c81fb64952bc8ccb3c7c66bca3&quot;, path='model/model.pkl')

f = open(tmp_path,'rb')

model = pickle.load(f)

f.close()

 

client.list_artifacts(run_id=&quot;0c7946c81fb64952bc8ccb3c7c66bca3&quot;, path=&quot;&quot;)

client.list_artifacts(run_id=&quot;0c7946c81fb64952bc8ccb3c7c66bca3&quot;, path=&quot;model&quot;)
</code></pre>",0.0,2021-08-23 19:53:41.287 UTC,2022-01-24 20:08:25.513 UTC,1.0,"['databricks', 'mlflow']"
MLflow remote execution on databricks from windows creates an invalid dbfs path,"<p>I'm researching the use of MLflow as part of our data science initiatives and I wish to set up a minimum working example of remote execution on databricks from windows.</p>

<p>However, when I perform the remote execution a path is created locally on windows in the MLflow package which is sent to databricks. This path specifies the upload location of the '.tar.gz' file corresponding to the Github repo containing the MLflow Project. In cmd this has a combination of '\' and '/', but on databricks there are no separators at all in this path, which raises the 'rsync: No such file or directory (2)' error.</p>

<p>To be more general, I reproduced the error using an MLflow standard example and following this <a href=""https://docs.databricks.com/applications/mlflow/projects.html"" rel=""nofollow noreferrer"">guide</a> from databricks. The MLflow example is the <a href=""https://github.com/mlflow/mlflow/tree/master/examples/sklearn_elasticnet_wine"" rel=""nofollow noreferrer"">sklearn_elasticnet_wine</a>, but I had to add a default value to a parameter so I forked it and the MLproject which can be executed remotely can be found at (<a href=""https://github.com/aestene/mlflow/tree/master/examples/sklearn_elasticnet_wine"" rel=""nofollow noreferrer"">forked repo</a>).</p>

<p>The Project can be executed remotely by the following command (assuming a databricks instance has been set up)</p>

<pre><code>mlflow run https://github.com/aestene/mlflow#examples/sklearn_elasticnet_wine -b databricks -c db-clusterconfig.json --experiment-id &lt;insert-id-here&gt;
</code></pre>

<p>where ""db-clusterconfig.json"" correspond to the cluster to set up in databricks and is in this example set to</p>

<pre><code>{
    ""autoscale"": {
        ""min_workers"": 1,
        ""max_workers"": 2
    },
    ""spark_version"": ""5.5.x-scala2.11"",
    ""node_type_id"": ""Standard_DS3_v2"",
    ""driver_node_type_id"": ""Standard_DS3_v2"",
    ""ssh_public_keys"": [],
    ""custom_tags"": {},
    ""spark_env_vars"": {
        ""PYSPARK_PYTHON"": ""/databricks/python3/bin/python3""
    }
}
</code></pre>

<p>When running the project remotely, this is the output in cmd:</p>

<pre><code>2019/10/04 10:09:50 INFO mlflow.projects: === Fetching project from https://github.com/aestene/mlflow#examples/sklearn_elasticnet_wine into C:\Users\ARNTS\AppData\Local\Temp\tmp2qzdyq9_ ===
2019/10/04 10:10:04 INFO mlflow.projects.databricks: === Uploading project to DBFS path /dbfs\mlflow-experiments\3947403843428882\projects-code\aa5fbb4769e27e1be5a983751eb1428fe998c3e65d0e66eb9b4c77355076f524.tar.gz ===
2019/10/04 10:10:05 INFO mlflow.projects.databricks: === Finished uploading project to /dbfs\mlflow-experiments\3947403843428882\projects-code\aa5fbb4769e27e1be5a983751eb1428fe998c3e65d0e66eb9b4c77355076f524.tar.gz ===
2019/10/04 10:10:05 INFO mlflow.projects.databricks: === Running entry point main of project https://github.com/aestene/mlflow#examples/sklearn_elasticnet_wine on Databricks ===
2019/10/04 10:10:06 INFO mlflow.projects.databricks: === Launched MLflow run as Databricks job run with ID 8. Getting run status page URL... ===
2019/10/04 10:10:18 INFO mlflow.projects.databricks: === Check the run's status at https://&lt;region&gt;.azuredatabricks.net/?o=&lt;databricks-id&gt;#job/8/run/1 ===
</code></pre>

<p>Where the DBFS path has a leading '/' before the remaining are '\'. </p>

<p>The command spins up a cluster in databricks and is ready to execute the job, but ends up with the following error message on the databricks side:</p>

<pre><code>rsync: link_stat ""/dbfsmlflow-experiments3947403843428882projects-codeaa5fbb4769e27e1be5a983751eb1428fe998c3e65d0e66eb9b4c77355076f524.tar.gz"" failed: No such file or directory (2)
rsync error: some files/attrs were not transferred (see previous errors) (code 23) at main.c(1183) [sender=3.1.1]
</code></pre>

<p>Where we can see the same path but without the '\' inserted. I narrowed down the creation of this path to this <a href=""https://github.com/mlflow/mlflow/blob/master/mlflow/projects/databricks.py"" rel=""nofollow noreferrer"">file</a> in the MLflow Github repo, where the following code creates the path (line 133):</p>

<pre class=""lang-py prettyprint-override""><code>dbfs_path = os.path.join(DBFS_EXPERIMENT_DIR_BASE, str(experiment_id),
                                     ""projects-code"", ""%s.tar.gz"" % tarfile_hash)
dbfs_fuse_uri = os.path.join(""/dbfs"", dbfs_path)
</code></pre>

<p>My current hypothesis is that <code>os.path.join()</code> in the first line joins the string together in a ""windows fashion"" such that they have backslashes. Then the following call to <code>os.path.join()</code> adds a '/'. The databricks file system is then unable to handle this path and something causes the 'tar.gz' file to not be properly uploaded or to be accessed at the wrong path. </p>

<p>It should also be mentioned that the project runs fine locally.</p>

<p>I'm running the following versions:</p>

<p>Windows 10</p>

<p>Python 3.6.8</p>

<p>MLflow 1.3.0 (also replicated the fault with 1.2.0)</p>

<p>Any feedback or suggestions are greatly appreciated!</p>",2,0,2019-10-04 10:39:52.42 UTC,,,2,databricks|mlflow,350,2019-10-04 07:11:50.493 UTC,2022-09-23 07:12:04.557 UTC,,23,5,0,2,"<p>Thanks for the catch, you're right that using <code>os.path.join</code> when working with DBFS paths is incorrect, resulting in a malformed path that breaks project execution. I've filed to <a href=""https://github.com/mlflow/mlflow/issues/1926"" rel=""nofollow noreferrer"">https://github.com/mlflow/mlflow/issues/1926</a> track this, if you're interested in making a bugfix PR (<a href=""https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.rst"" rel=""nofollow noreferrer"">see the MLflow contributor guide for info on how to do this</a>) to replace <code>os.path.join</code> here with <code>os.posixpath.join</code> I'd be happy to review :)</p>",1.0,2019-10-11 00:00:25.483 UTC,,1.0,"['databricks', 'mlflow']"
"How to change experiment being tracked by Databricks ""runs"" tab?","<p>I'm trying to use the mlflow databricks integration, specifically the tracking API. Normally, I can view past runs info in the handy sidebar of a notebook, as you can see <a href=""https://i.stack.imgur.com/JWY1c.png"" rel=""nofollow noreferrer"">here</a> and which I got from the <a href=""https://docs.databricks.com/applications/mlflow/quick-start.html#mlflow-mlflow-quick-start-python"" rel=""nofollow noreferrer"">tutorial</a>. However, what I want now is to use multiple notebooks to send runs to the same experiment. Additionally, I would like to view the results of all these common runs in each of the notebooks. To do this, I need to change the (default) experiment tracked by the ""runs"" tab. </p>

<p>Ultimately, my question boils down to the following: how can I set the experiment being tracked by the ""runs"" tab? I have tried using <code>mlflow.set_tracking_uri</code> and <code>mlflow.set_experiment(mlflow_experiment_name)</code></p>",1,0,2019-07-02 17:47:57.383 UTC,,,0,python|databricks|mlflow,257,2016-05-29 00:42:03.397 UTC,2022-09-14 17:22:55.647 UTC,Canada,2168,440,35,1631,"<p>I don't believe this is possible today, as the design choice is to associate the runs tab to the notebook experiment.  From the <a href=""https://docs.databricks.com/applications/mlflow/tracking.html#notebook-experiments"" rel=""nofollow noreferrer"">docs</a>:</p>
<blockquote>
<p>Every Python and R notebook in a Databricks workspace has its own experiment. When you use MLflow in a notebook, it records runs in the notebook experiment.</p>
<p>A notebook experiment shares the same name and ID as its corresponding notebook. The notebook ID is the numerical identifier at the end of a Notebook URL.</p>
</blockquote>
<p>You can create experiments independent of the notebook experiment and log runs to it from different sources.  You'll still have to open up the tracking UI to explore the results though.</p>
<p>In other words, you can send multiple runs from different notebooks to the same experiment, but today you cannot log multiple runs to the 'Runs' tab in a specific notebook.</p>",1.0,2019-08-23 00:06:47.737 UTC,2020-06-20 09:12:55.06 UTC,1.0,"['databricks', 'mlflow']"
Tracking SageMaker Estimator with MLFlow,"<p>I'm working on a version tracking system for a ML project and want to use MLflow to do so. My project uses AWS Sagemaker's DeepAR for forecast.</p>

<p>What I want to do is very simple. I'm trying do log the Sagemaker DeepAR model (Sagemaker Estimator) with MLFlow. As it doesn't have a ""log_model"" funcion in it's ""mlflow.sagemaker"" module, I tried to use the ""mlflow.pyfunc"" module to do the log. Unfortunatelly it didn't worked. How can I log the Sagemaker model and get the cloudpickle and yaml files generated by MLFlow?</p>

<p>My code for now:</p>

<p><code>mlflow.pyfunc.log_model(model)</code></p>

<p>Where model is a sagemaker.estimator.Estimator object and the error I get from the code is</p>

<p><code>mlflow.exceptions.MlflowException: Either `loader_module` or `python_model` must be specified. A `loader_module` should be a python module. A `python_model` should be a subclass of PythonModel</code></p>

<p>I know AWS Sagemaker logs my models, but it is really important to my project to do the log with MLFlow too.</p>",1,0,2020-04-23 01:06:27.047 UTC,1.0,2020-04-23 01:30:39.877 UTC,0,python|amazon-web-services|amazon-sagemaker|mlflow,437,2019-01-20 22:33:06.667 UTC,2022-09-18 13:59:17.783 UTC,,111,22,0,41,"<p>You cannot use pyfunc to store Any type object.</p>

<p>You should either specify one of loader_module as shown in the example below or you must write the wrapper that implements PythonModel interface and provides logic to deserialize your model from  previously-stored artifacts as described here 
 <a href=""https://www.mlflow.org/docs/latest/models.html#example-saving-an-xgboost-model-in-mlflow-format"" rel=""nofollow noreferrer"">https://www.mlflow.org/docs/latest/models.html#example-saving-an-xgboost-model-in-mlflow-format</a></p>

<p>example with loader:</p>

<pre><code>    model_uri = 'model.pkl'

    with open(model_uri, 'wb') as f:
        pickle.dump(model, f)

    mlflow.log_artifact(model_uri, 'model')

    mlflow.pyfunc.log_model(
        'model', loader_module='mlflow.sklearn', data_path='model.pkl', code_path=['src'], conda_env='environment.yml'
    )
</code></pre>

<p>I think PythonModel is the better way for you because of mlflow doesn't have a built-in loader for SageMaker DeepAR model.</p>

<p>Nonetheless, You must have the knowledge how to restore SageMaker model from artifacts, because I am not sure that is possible at all, cuz of some built-in SageMaker algorithms are blackboxes.</p>

<p>You can also may be interested in container that allow you to run any MLFlow projects inside Sagemaker: <a href=""https://github.com/odahu/sagemaker-mlflow-container"" rel=""nofollow noreferrer"">https://github.com/odahu/sagemaker-mlflow-container</a></p>",1.0,2020-04-24 09:24:49.803 UTC,,1.0,"['amazon-sagemaker', 'mlflow']"
Catalogs in Databricks,"<p>I have started reading about the Unity Catalog that Databricks has introduced. I understand the basic issue that it is trying to solve, but I do understand what exactly a Catalog is.</p>
<p>This was available in the Databricks documentation,</p>
<blockquote>
<p>A catalog contains schemas (databases), and a schema contains tables and views.</p>
</blockquote>
<p><a href=""https://docs.databricks.com/data-governance/unity-catalog/create-catalogs.html"" rel=""nofollow noreferrer"">https://docs.databricks.com/data-governance/unity-catalog/create-catalogs.html</a></p>
<p>How does this added layer (on top of schemas) help? I am guessing it has something to do with governance?</p>
<p>I would really appreciate an example, if possible.</p>",1,0,2022-07-08 15:02:39.723 UTC,,2022-07-09 17:30:58.383 UTC,1,databricks|catalog|databricks-unity-catalog,183,2018-03-24 01:53:05.82 UTC,2022-09-24 16:46:35.903 UTC,Sri Lanka,820,389,1,165,"<p>Really, Catalog is an another data management layer inside the bigger objects - Unity Catalog Metastore.  Closest analogy of the Catalog is a single Hive Metastore - it's also contains databases (schemas) that contain tables and views. Catalogs could be used to isolate objects of some entity (business unit/project/environments (dev,stagin,prod)/...) from objects of other entities.  You can give manage permissions of the catalogs to respective admins of the business units, projects, ..., and they can then assign permissions on individual schemas and tables/views.</p>",1.0,2022-07-08 18:31:24.137 UTC,,1.0,"['databricks', 'databricks-unity-catalog']"
how to log KerasClassifier model in a sklearn pipeline mlflow?,"<p>I have a set of pre-processing stages in sklearn <code>Pipeline</code> and an estimator which is a <code>KerasClassifier</code> (<code>from tensorflow.keras.wrappers.scikit_learn import KerasClassifier</code>).</p>
<p>My overall goal is to tune and log the whole sklearn pipeline in <code>mlflow</code> (in databricks evn). I get a confusing type error which I can't figure out how to reslove:</p>
<blockquote>
<p>TypeError: can't pickle _thread.RLock objects</p>
</blockquote>
<p>I have the following code (without tuning stage) which returns the above error:</p>
<pre><code>conda_env = _mlflow_conda_env(
    additional_conda_deps=None,
    additional_pip_deps=[
        &quot;cloudpickle=={}&quot;.format(cloudpickle.__version__),
        &quot;scikit-learn=={}&quot;.format(sklearn.__version__),
        &quot;numpy=={}&quot;.format(np.__version__),
        &quot;tensorflow=={}&quot;.format(tf.__version__),
    ],
    additional_conda_channels=None,
)

search_space = {
    &quot;estimator__dense_l1&quot;: 20,
    &quot;estimator__dense_l2&quot;: 20,
    &quot;estimator__learning_rate&quot;: 0.1,
    &quot;estimator__optimizer&quot;: &quot;Adam&quot;,
}


def create_model(n):

    model = Sequential()
    model.add(Dense(int(n[&quot;estimator__dense_l1&quot;]), activation=&quot;relu&quot;))
    model.add(Dense(int(n[&quot;estimator__dense_l2&quot;]), activation=&quot;relu&quot;))
    model.add(Dense(1, activation=&quot;sigmoid&quot;))
    model.compile(
        loss=&quot;binary_crossentropy&quot;,
        optimizer=n[&quot;estimator__optimizer&quot;],
        metrics=[&quot;accuracy&quot;],
    )

    return model


mlflow.sklearn.autolog()
with mlflow.start_run(nested=True) as run:

    classfier = KerasClassifier(build_fn=create_model, n=search_space)
    # fit the pipeline
    clf = Pipeline(steps=[(&quot;preprocessor&quot;, preprocessor), 
                          (&quot;estimator&quot;, classfier)])
    h = clf.fit(
        X_train,
        y_train.values,
        estimator__validation_split=0.2,
        estimator__epochs=10,
        estimator__verbose=2,
    )

    # log scores
    acc_score = clf.score(X=X_test, y=y_test)
    mlflow.log_metric(&quot;accuracy&quot;, acc_score)

    signature = infer_signature(X_test, clf.predict(X_test))
    # Log the model with a signature that defines the schema of the model's inputs and outputs.
    mlflow.sklearn.log_model(
        sk_model=clf, artifact_path=&quot;model&quot;, 
        signature=signature, 
        conda_env=conda_env
    )
</code></pre>
<p>I also get this warning before the error:</p>
<pre><code>
    WARNING mlflow.sklearn.utils: Truncated the value of the key `steps`. Truncated value: `[('preprocessor', ColumnTransformer(n_jobs=None, remainder='drop', sparse_threshold=0.3,
                      transformer_weights=None,
                      transformers=[('num',
                                   Pipeline(memory=None,
</code></pre>
<p>note the the whole pipeline runs outside mlflow.
can someone help?</p>",1,0,2021-09-10 02:30:04.103 UTC,,,0,python|tensorflow|scikit-learn|databricks|mlflow,435,2016-02-17 00:01:25.907 UTC,2022-09-23 03:50:46.563 UTC,,283,35,7,85,"<p>I think I find sort of a workaround/solution for this for now, but I think this issue needs to be addressed in MLFloow anyways.</p>
<p>What I did is not the best way probably.
I used a python package called <a href=""https://scikeras.readthedocs.io/en/latest/"" rel=""nofollow noreferrer"">scikeras</a> that does this wrapping and then could log the model</p>
<p>The code:</p>
<pre><code>import scikeras 
import tensorflow as tf 
from tensorflow.keras.models import Sequential 
from tensorflow.keras.layers import Input, Dense, Dropout, LSTM, Flatten, Activation 
 
from scikeras.wrappers import KerasClassifier 
  
 
class ModelWrapper(mlflow.pyfunc.PythonModel): 
    def __init__(self, model): 
        self.model = model 
 
    def predict(self, context, model_input): 
        return self.model.predict(model_input) 
 
conda_env =  _mlflow_conda_env( 
      additional_conda_deps=None, 
      additional_pip_deps=[ 
        &quot;cloudpickle=={}&quot;.format(cloudpickle.__version__),  
        &quot;scikit-learn=={}&quot;.format(sklearn.__version__), 
        &quot;numpy=={}&quot;.format(np.__version__), 
        &quot;tensorflow=={}&quot;.format(tf.__version__), 
        &quot;scikeras=={}&quot;.format(scikeras.__version__), 
      ], 
      additional_conda_channels=None, 
  ) 
 
param = { 
   &quot;dense_l1&quot;: 20, 
   &quot;dense_l2&quot;: 20, 
   &quot;optimizer__learning_rate&quot;: 0.1, 
   &quot;optimizer&quot;: &quot;Adam&quot;, 
   &quot;loss&quot;:&quot;binary_crossentropy&quot;, 
} 
 
  
def create_model(dense_l1, dense_l2, meta): 
  
  n_features_in_ = meta[&quot;n_features_in_&quot;] 
  X_shape_ = meta[&quot;X_shape_&quot;] 
  n_classes_ = meta[&quot;n_classes_&quot;] 
 
  model = Sequential() 
  model.add(Dense(n_features_in_, input_shape=X_shape_[1:], activation=&quot;relu&quot;)) 
  model.add(Dense(dense_l1, activation=&quot;relu&quot;)) 
  model.add(Dense(dense_l2, activation=&quot;relu&quot;)) 
  model.add(Dense(1, activation=&quot;sigmoid&quot;)) 
 
  return model   
 
mlflow.sklearn.autolog() 
with mlflow.start_run(run_name=&quot;sample_run&quot;): 
 
  classfier = KerasClassifier( 
    create_model, 
    loss=param[&quot;loss&quot;], 
    dense_l1=param[&quot;dense_l1&quot;], 
    dense_l2=param[&quot;dense_l2&quot;], 
    optimizer__learning_rate = param[&quot;optimizer__learning_rate&quot;], 
    optimizer= param[&quot;optimizer&quot;], 
) 
 
  # fit the pipeline 
  clf = Pipeline(steps=[('preprocessor', preprocessor), 
                      ('estimator', classfier)])   
 
  h = clf.fit(X_train, y_train.values) 
  # log scores 
  acc_score = clf.score(X=X_test, y=y_test) 
  mlflow.log_metric(&quot;accuracy&quot;, acc_score) 
  signature = infer_signature(X_test, clf.predict(X_test)) 
  model_nn = ModelWrapper(clf,)  
 
  mlflow.pyfunc.log_model( 
      python_model= model_nn, 
      artifact_path = &quot;model&quot;,  
      signature = signature,  
      conda_env = conda_env 
  ) 
</code></pre>",0.0,2021-09-14 04:21:08.677 UTC,,1.0,"['databricks', 'mlflow']"
Is Machine Learning persona available for Databricks community edition?,"<p>I just started using <a href=""https://databricks.com/product/faq/community-edition"" rel=""nofollow noreferrer"">Databricks community edition</a>, and by default I am in the &quot;Data Science and Eingineering&quot; persona. I wanted to explore the <a href=""https://docs.databricks.com/applications/machine-learning/index.html"" rel=""nofollow noreferrer"">Machine Learning</a> environment, but could not find such options from the sidebar (see below). Is the feature supported for community edition at the moment?</p>
<p><a href=""https://i.stack.imgur.com/PaDmm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PaDmm.png"" alt=""enter image description here"" /></a></p>
<p>EDIT: it becomes available as of 9/10/2021.</p>
<p><a href=""https://i.stack.imgur.com/Syotx.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Syotx.png"" alt=""enter image description here"" /></a></p>",1,0,2021-09-02 23:16:13.283 UTC,,2021-09-10 18:55:32.543 UTC,1,databricks|databricks-community-edition|databricks-ml,43,2014-10-29 19:07:27.193 UTC,2022-09-24 21:41:03.507 UTC,"Bay Area, CA, USA",4278,587,3,346,"<p>Unfortunately, right now this functionality isn't available on community edition yet. You still have access to some pieces, like, MLflow, etc. but not UI for it. The ML persona UI is available in the full edition on all clouds (Azure, AWS, GCP).</p>
<p>Update 10.09.21: it’s available now :-)</p>",2.0,2021-09-05 11:47:22.92 UTC,2021-09-10 19:00:56.15 UTC,1.0,"['databricks-ml', 'databricks']"
Azure Databricks Unity Catalogue Create metastore button unavailable,"<p>Trying to create a Metastore for manage identity incorporating in Azure Databricks but the data tab only shows create table.</p>
<p>Per the documentation, it should be there.  Also, I have created the databricks service and have azure contributor role.</p>
<p>I am an admin to the Databricks workspace. Is it unavailable on Azure?</p>
<p><a href=""https://i.stack.imgur.com/3HYR4.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3HYR4.jpg"" alt=""enter image description here"" /></a></p>",1,1,2022-09-15 00:51:29.977 UTC,,,0,databricks|azure-databricks|databricks-unity-catalog,78,2015-06-16 22:52:38.123 UTC,2022-09-25 01:15:25.957 UTC,,1282,398,2,215,"<p>Well, you don't give details about your environment, so I just can give some ideas about what is missing.</p>
<p>First, change the environment to &quot;SQL&quot; (click on &quot;Data Science &amp; Engineering&quot; menu at the top left)</p>
<p>Second, do you have all the requirements? The requirements are here: <a href=""https://docs.microsoft.com/en-us/azure/databricks/data-governance/unity-catalog/get-started#requirements"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/databricks/data-governance/unity-catalog/get-started#requirements</a></p>
<p>I think do you missing this permission here:</p>
<p>*You must be an Azure Databricks account admin.</p>
<p>The first Azure Databricks account admin must be an Azure Active Directory Global Administrator at the time that they first log in to the Azure Databricks account console. Upon first login, that user becomes an Azure Databricks account admin and no longer needs the Azure Active Directory Global Administrator role to access the Azure Databricks account. The first account admin can assign users in the Azure Active Directory tenant as additional account admins (who can themselves assign more account admins). Additional account admins do not require specific roles in Azure Active Directory.*</p>
<p>To check if you are an Azure Databricks account admin you can access:</p>
<p><a href=""https://accounts.azuredatabricks.net/login?next_url=%2Flogin%2F"" rel=""nofollow noreferrer"">https://accounts.azuredatabricks.net/login?next_url=%2Flogin%2F</a></p>
<p>and verify if you have the access to the Databricks administration screen</p>",0.0,2022-09-15 17:40:02.347 UTC,,1.0,"['databricks', 'databricks-unity-catalog']"
Embarrassingly parallel hyperparameter search via Azure + DataBricks + MLFlow,"<p>Conceptual question.  My company is pushing Azure + DataBricks.  I am trying to understand where this can take us.</p>
<p>I am porting some work I've done locally to the Azure + Databricks platform.  I want to run an experiment with a large number of hyperparameter combinations using Azure + Databricks + MLfLow.  I am using PyTorch to implement my models.</p>
<p>I have a cluster with 8 nodes.  I want to kick off the parameter search across all of the nodes in an embarrassingly parallel manner (one run per node, running independently).  Is this as simple as creating a MLflow project and then using the mlflow.projects.run command for each hyperparameter combination and Databricks + MLflow will take care of the rest?</p>
<p>Is this technology capable of this?  I'm looking for some references I could use to make this happen.</p>",1,1,2020-07-07 14:52:06.057 UTC,,2020-07-07 15:12:05.2 UTC,0,databricks|azure-databricks|mlflow,262,2012-02-09 20:11:42.35 UTC,2022-09-16 14:37:11.437 UTC,"Sioux City, IA",325,254,4,51,"<p>The short answer is yes, it's possible, but won't be exactly as easy as running a single mlflow command. You can paralelize single-node workflows using spark Python UDFs, a good example of this is this <a href=""https://pages.databricks.com/rs/094-YMS-629/images/Fine-Grained-Time-Series-Forecasting.html?_ga=2.64430959.1760852900.1593769579-972789996.1561118598"" rel=""nofollow noreferrer"">notebook</a></p>
<p>I'm not sure if this will work with pytorch, but there is hyperopt library that lets you parallelize search across parameters using Spark - it's integrated with mlflow and available in databricks ML runtime. I've been using it only with scikit-learn, but it may be <a href=""https://docs.databricks.com/applications/machine-learning/automl/hyperopt/hyperopt-model-selection.html"" rel=""nofollow noreferrer"">worth checking out</a></p>",0.0,2020-07-17 11:52:54.773 UTC,,1.0,"['databricks', 'mlflow']"
Databricks multi-task jobs - pass MLflow run_id from one task to next task,"<p>I would like to create a databricks multi-task with following sequence:</p>
<ul>
<li>notebook task 1: train model with results logged to MLflow tracking server</li>
<li>notebook task 2: use mlflow run_id from task 1 to register model in model registry</li>
</ul>
<p>Is it possible to pass run_id from task 1 to task 2 and if so is there any documentation on how this could be done?</p>",2,0,2021-11-19 14:21:58.763 UTC,,2022-06-09 07:45:09.85 UTC,0,databricks|azure-databricks|mlflow,198,2015-07-09 13:14:04.057 UTC,2022-09-22 06:56:24.25 UTC,,818,76,2,94,"<p>As of <em>right now</em> (it may change), it's impossible to pass results between jobs if you use multi-task job.</p>
<p>But you can call another notebook as a child job if you use <a href=""https://docs.databricks.com/notebooks/notebook-workflows.html"" rel=""nofollow noreferrer"">notebook workflows</a>  and function <code>dbutils.notebooks.run</code>:</p>
<pre class=""lang-py prettyprint-override""><code># notebook 1
... training code ...
dbutils.notebooks.run(&quot;notebook2&quot;, 300, {&quot;run_id&quot;: run_id})
</code></pre>",0.0,2021-11-19 17:00:39.877 UTC,,2.0,"['databricks', 'mlflow']"
Download model artefact from Databricks workspace,<p>How can I download a mlflow model artefact in a docker container from databricks workspace?</p>,1,0,2022-02-25 06:40:55.51 UTC,,2022-02-25 13:33:11.323 UTC,0,docker|databricks|azure-databricks|mlflow,370,2014-09-22 04:46:57.027 UTC,2022-09-03 08:12:58.187 UTC,"Bengaluru, Karnataka, India",569,41,3,123,"<p>To download a model from Databricks workspace you need to do two things:</p>
<ol>
<li><p>Set MLFlow tracking URI to databricks using python API</p>
</li>
<li><p>Setup databricks authentication. I prefer authenticating by setting the following environment variables, you can also use databricks CLI to authenticate:</p>
<pre><code>DATABRICKS_HOST

DATABRICKS_TOKEN
</code></pre>
</li>
<li><p>Here's a basic code snippet to download a model from Databricks workspace model registry:</p>
<pre><code>import os
import mlflow
from mlflow.store.artifact.models_artifact_repo import ModelsArtifactRepository

model_name = &quot;example-model-name&quot;
model_stage = &quot;Staging&quot;  # Should be either 'Staging' or 'Production'

mlflow.set_tracking_uri(&quot;databricks&quot;)

os.makedirs(&quot;model&quot;, exist_ok=True)
local_path = ModelsArtifactRepository(
    f'models:/{model_name}/{model_stage}').download_artifacts(&quot;&quot;, dst_path=&quot;model&quot;)

print(f'{model_stage} Model {model_name} is downloaded at {local_path}')
</code></pre>
<p>Running above python script will download an ML model in the model directory.</p>
<p><strong>Containerizing MLFlow model serving with Docker</strong></p>
<p>The next step is to package this downloaded model in a docker image and serve a model when you run the image.</p>
</li>
</ol>
<p>Here's a basic Dockerfile to do the same:</p>
<pre><code>FROM continuumio/miniconda3

ENV MLFLOW_HOME /opt/mlflow
ENV MLFLOW_VERSION 1.12.1
ENV PORT 5000

RUN conda install -c conda-forge mlflow=${MLFLOW_VERSION}

COPY model/ ${MLFLOW_HOME}/model

WORKDIR ${MLFLOW_HOME}

RUN mlflow models prepare-env -m ${MLFLOW_HOME}/model

RUN useradd -d ${MLFLOW_HOME} mlflow
RUN chown mlflow: ${MLFLOW_HOME}
USER mlflow

CMD mlflow models serve -m ${MLFLOW_HOME}/model --host 0.0.0.0 --port ${PORT}
</code></pre>
<p>For more information you can follow this <a href=""https://dev.to/itachiredhair/downloading-mlflow-model-from-databricks-and-serving-with-docker-38ip"" rel=""nofollow noreferrer"">article</a> from Akshay Milmile</p>",1.0,2022-02-25 08:12:59.603 UTC,2022-02-25 13:32:17.627 UTC,2.0,"['databricks', 'mlflow']"
Creating a metastore for Azure Databricks Unity Catalog through terraform fails,"<p>Creating a metastore for Unity Catalog through terraform fails with this error message:</p>
<pre class=""lang-bash prettyprint-override""><code>Error: cannot create metastore: Only account admin can create metastores. \ 
Using databricks-cli auth: host=https://{wsname}.azuredatabricks.net, \
token=***REDACTED***, profile=DEFAULT
</code></pre>
<p>My config is setup like this:</p>
<pre><code>resource &quot;databricks_metastore&quot; &quot;this&quot; {
  name = &quot;primary&quot;
  storage_root = format(&quot;abfss://%s@%s.dfs.core.windows.net/&quot;,
    azurerm_storage_account.storage.name,
  azurerm_storage_container.container.name)
  force_destroy = true
}

resource &quot;databricks_metastore_assignment&quot; &quot;this&quot; {
  metastore_id = databricks_metastore.this.id
  workspace_id = data.azurerm_databricks_workspace.oat.workspace_id
}

</code></pre>
<p>The <a href=""https://docs.microsoft.com/en-us/azure/databricks/administration-guide/"" rel=""nofollow noreferrer"">documentation</a> regarding Azure Databricks administrators lists up four different kinds of administrators, but I fail to see how I can see check who is Azure Databricks account admins.</p>
<p>In short, I have the same error as <a href=""https://stackoverflow.com/q/73607469/6383431"">Not finding Unity Catalog &quot;Create Metastore&quot; in Azure Databricks</a></p>",1,0,2022-09-07 11:00:25.92 UTC,,2022-09-07 11:06:29.593 UTC,1,databricks|azure-databricks|databricks-unity-catalog,168,2016-05-25 22:01:20.717 UTC,2022-09-24 22:07:52.463 UTC,,515,44,7,30,"<p>Your account is a workspace admin. Account administrator is one level above.</p>
<p>This <a href=""https://docs.microsoft.com/en-us/azure/databricks/data-governance/unity-catalog/get-started#requirements"" rel=""nofollow noreferrer"">Unity Catalog documentation</a> describes how you become account administrator:</p>
<blockquote>
<p>The first Azure Databricks account admin must be an Azure Active
Directory Global Administrator at the time that they first log in to
the Azure Databricks account console. Upon first login, that user
becomes an Azure Databricks account admin and no longer needs the
Azure Active Directory Global Administrator role to access the Azure
Databricks account.</p>
</blockquote>
<p>Thus, you need Global Administrator role in Azure for the user that is running Terraform.</p>",2.0,2022-09-07 11:53:22.017 UTC,,2.0,"['databricks', 'databricks-unity-catalog']"
Log Pickle files as a part of Mlflow run,"<p>I am running an MLflow experiment as a part of it I would like to log a few artifacts as a python pickle.</p>
<p>Ex: Trying out different categorical encoders, so wanted to log the encoder objects as a pickle file.</p>
<p>Is there a way to achieve this?</p>",1,0,2021-06-01 09:15:22.663 UTC,,,3,python|databricks|azure-databricks|mlflow,1843,2014-09-22 04:46:57.027 UTC,2022-09-03 08:12:58.187 UTC,"Bengaluru, Karnataka, India",569,41,3,123,"<p>There are two functions for there:</p>
<ol>
<li><a href=""https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.log_artifact"" rel=""nofollow noreferrer"">log_artifact</a> - to log a local file or directory as an artifact</li>
<li><a href=""https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.log_artifacts"" rel=""nofollow noreferrer"">log_artifacts</a> - to log a contents of a local directory</li>
</ol>
<p>so it would be as simple as:</p>
<pre class=""lang-py prettyprint-override""><code>with mlflow.start_run():
    mlflow.log_artifact(&quot;encoder.pickle&quot;)
</code></pre>
<p>And you will need to use the <a href=""https://mlflow.org/docs/latest/models.html#model-customization"" rel=""nofollow noreferrer"">custom MLflow model</a> to use that pickled file, something like this:</p>
<pre class=""lang-py prettyprint-override""><code>import mlflow.pyfunc

class my_model(mlflow.pyfunc.PythonModel):
    def __init__(self, encoders):
        self.encoders = encoders

    def predict(self, context, model_input):
        _X = ...# do encoding using self.encoders.
        return str(self.ctx.predict([_X])[0])
</code></pre>",0.0,2021-06-01 10:16:03.553 UTC,,2.0,"['databricks', 'mlflow']"
"""No Isolation Shared"" Databricks job cluster through CLI","<p>I turned on Unity Catalog for our workspace. Now a job cluster has an access mode setting. (<a href=""https://docs.databricks.com/data-governance/unity-catalog/compute.html#access-mode"" rel=""nofollow noreferrer"">docs</a>) I can manually change this setting on the UI:</p>
<img src=""https://i.stack.imgur.com/Xoreq.png"" width=""400""/>
<p>But how do I control this setting when creating the job through <code>databricks jobs create --json-file X.json</code>?</p>",1,1,2022-08-30 10:33:15.023 UTC,,,3,databricks|databricks-cli|databricks-unity-catalog,40,2014-02-17 09:38:14.217 UTC,2022-09-23 10:01:26.843 UTC,"Budapest, Hungary",26473,1987,322,4127,"<p>You need to specify the <code>data_security_mode</code> with value <code>&quot;NONE&quot;</code> in the cluster definition (for some reason it's missing from API docs, but you can find details in the <a href=""https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/cluster#data_security_mode"" rel=""nofollow noreferrer"">Terraform provider docs</a>).  But really it should be the default value, so you don't need to explicitly specify it.</p>",7.0,2022-08-30 11:58:10.727 UTC,,2.0,"['databricks-unity-catalog', 'databricks']"
Databricks MLFlow AutoML XGBoost can't predict_proba(),"<p>I used AutoML in Databricks Notebooks for a binary classification problem and the winning model flavor was XGBoost (big surprise).</p>
<p>The outputted model is of this variety:</p>
<pre><code>mlflow.pyfunc.loaded_model:
      artifact_path: model
      flavor: mlflow.sklearn
      run_id: 123456789
</code></pre>
<p>Any idea why when I use <code>model.predict_proba(X)</code>, I get this response?</p>
<p><code>AttributeError: 'PyFuncModel' object has no attribute 'predict_proba'</code></p>
<p>I know it is possible to get the probabilities because ROC/AUC is a metric used for tuning the model. Any help would be amazing!</p>",1,0,2021-12-31 01:02:03.883 UTC,,,1,pandas|scikit-learn|databricks|xgboost|mlflow,451,2019-07-11 07:02:37.217 UTC,2022-09-21 21:43:57.89 UTC,"San Francisco, CA, USA",77,35,0,17,"<p>I had the same issue with catboost model.
The way I solved it was by saving the artifacts in a local dir</p>
<pre><code>import os
from mlflow.tracking import MlflowClient
client = MlflowClient()
local_dir = &quot;/dbfs/FileStore/user/models&quot;
local_path = client.download_artifacts('run_id', &quot;model&quot;, local_dir)```

```model_path = '/dbfs/FileStore/user/models/model/model.cb'
model = CatBoostClassifier()
model = model.load_model(model_path)
model.predict_proba(test_set)```
</code></pre>",0.0,2022-03-08 12:56:27.86 UTC,,2.0,"['databricks', 'mlflow']"
Sagemaker API to list Hyperparameters,"<p>I'm currently trying to implement MLFlow Tracking into my training pipeline and would like to log the hyperparameters of my hyperparameter Tuning of each training job.</p>

<p>Does anyone know, how to pull the list of hyperparameters that can be seen on the sagemaker training job interface (on the AWS console)? Is there any other smarter way to list how models perform in comparison in Sagemaker (and displayed)?</p>

<p>I would assume there must be an easy and Pythonic way to do this (either boto3 or the sagemaker api) to get this data. I wasn't able to find it in Cloudwatch.</p>

<p>Many thanks in advance!</p>",2,0,2020-06-11 08:38:22.897 UTC,,,1,python|amazon-web-services|amazon-sagemaker|mlflow,484,2016-07-28 15:35:17.217 UTC,2022-03-28 15:52:24.487 UTC,,43,4,0,8,"<p>there is indeed a rather pythonic way in the SageMaker python SDK:</p>

<pre><code>tuner = sagemaker.tuner.HyperparameterTuner.attach('&lt; your tuning jobname&gt;')

results = tuner.analytics().dataframe()  # all your tuning metadata, in pandas!
</code></pre>

<p>See full example here <a href=""https://github.com/aws-samples/amazon-sagemaker-tuneranalytics-samples/blob/master/SageMaker-Tuning-Job-Analytics.ipynb"" rel=""nofollow noreferrer"">https://github.com/aws-samples/amazon-sagemaker-tuneranalytics-samples/blob/master/SageMaker-Tuning-Job-Analytics.ipynb</a></p>",0.0,2020-06-14 22:27:30.467 UTC,,2.0,"['amazon-sagemaker', 'mlflow']"
mlflow.pyfunc.spark_udf and vector struct type,"<p>My <em>PySpark</em> dataset contains categorical data.</p>
<p>To train a model on this data, I followed this <a href=""https://docs.databricks.com/_static/notebooks/binary-classification.html"" rel=""nofollow noreferrer"">example notebook</a>. Especially, see the <em>Preprocess Data</em> section for the encoding part.</p>
<p>I now need to use this model somewhere else; hence, I followed <em>Databricks</em> recommendation to save and load this model.</p>
<p>It's working fine with <em>Pandas</em> (cf. code below).</p>
<pre><code>logged_model = 'runs:/e905f5759d434a1391bbe1e54a2b/best-model'

# Load model as a PyFuncModel.
loaded_model = mlflow.pyfunc.load_model(logged_model)

# Predict on a Pandas DataFrame.
import pandas as pd
loaded_model.predict(pd.DataFrame(data))
</code></pre>
<p>However the dataframe is to big to be converted to <em>Pandas</em>. Hence I need to make it work in <em>Spark</em>:</p>
<pre><code>import mlflow
logged_model = 'runs:/e905f5759d434a131bbe1e54a2b/best-model'

# Load model as a Spark UDF.
loaded_model = mlflow.pyfunc.spark_udf(spark, model_uri=logged_model)

# Predict on a Spark DataFrame.
df.withColumn('predictions', loaded_model(*columns)).collect()
</code></pre>
<p>But this snippet is producing:</p>
<pre><code>java.lang.UnsupportedOperationException: Unsupported data type: struct&amp;lt;type:tinyint,size:int,indices:array&amp;lt;int&amp;gt;,values:array&amp;lt;double&amp;gt;&amp;gt;
</code></pre>
<p>My feeling is that the udf doesn't accept this type of data as input.
Is there a way to fix it ?
Another solution ?</p>",1,1,2021-07-26 09:23:08.767 UTC,2.0,,1,pyspark|databricks|mlflow,790,2016-11-29 05:56:02.23 UTC,2022-09-24 11:32:25.87 UTC,,440,21,4,56,"<p>Have you tried using the <code>mlflow.spark.load_model</code>?</p>
<p>I'm having a very similar issue over here, but but using the spark method. I tried using the <code>mlflow.spark.load_model('runs:/run-id/my-model')</code> method and I got this weird error:</p>
<pre><code>FileNotFoundError: [Errno 2] No such file or directory: '/dbfs/tmp/mlflow/weird-id-folder'
</code></pre>
<p>Searching for the docs, I see the problem that we are facing (which seems to be different), seems to be a signature problem.</p>
<p>According with other part of the <a href=""https://www.mlflow.org/docs/latest/models.html#model-signature"" rel=""nofollow noreferrer"">docs</a> we have that the signature logged with the model will help to define what type of input the model has. The problem for me here is that my input is a Spark Sparse Vector -- which is not supported... Right now I'm trying to convert that into a column-based signature.</p>
<p>Have you tried something like this?</p>
<hr />
<p>UPDATE:</p>
<p>I would like to add that in my case adding the signature did solve the problem. All I did was ignore the vectors and consider only the input data and output data.</p>
<p>I took a look into the notebook, but haven't seen any mlflow logs, anyway, I do suppose you are logging your experiment according to <a href=""https://docs.databricks.com/applications/mlflow/tracking.html#log-runs-to-a-notebook-or-workspace-experiment"" rel=""nofollow noreferrer"">this</a> and using the <code>mlflow.spark</code> flavor.</p>
<p>If so, consider using all your data transformation and model fit in the same pipeline, using <code>from pyspark.ml import Pipeline</code>. Before logging the model, consider going under signature and registering the model schema.</p>
<pre><code>import mlflow.spark
from mlflow.models.signature import infer_signature

with mlflow.start_run():
    [...]
    # executing train &amp; test pipelines:
    model = pipeline.fit(train_features) # training model
    predictions = model.transform(test_features) # testing model
    train_signature = train_features.select('input_data') # ignores all other features created on the pipeline
    prediction_signature = predictions.select('input_data', 'prediction') # ignores all other features created on the training pipeline 
    signature = infer_signature(train_signature, prediction_signature) # register model schema
    mlflow.spark.log_model(model, 'transactions-classification', signature=signature) # logging model to mlflow
    [...]
</code></pre>
<p>After logging the model to the experiment, in a different notebook, you can use the load_model function as:</p>
<pre><code># importing model
import mlflow.spark
model_path = 'runs:/run-id'
model = mlflow.spark.load_model(model_path)
</code></pre>
<p>And it will work! :D</p>",2.0,2021-08-19 13:07:40.77 UTC,2021-08-31 22:03:50.98 UTC,3.0,"['databricks', 'mlflow']"
Saving an Matlabplot as an MLFlow artifact,"<p>I am using DataBricks and Spark 7.4ML,</p>
<p>The following code successfully logs the params and metrics, and I can see the ROCcurve.png in the MLFLOW gui (just the item in the tree below the model). But the actually plot is blank. Why?</p>
<pre><code>with mlflow.start_run(run_name=&quot;logistic-regression&quot;) as run:
  pipeModel = pipe.fit(trainDF)
  mlflow.spark.log_model(pipeModel, &quot;model&quot;)
  predTest = pipeModel.transform(testDF)
  predTrain = pipeModel.transform(trainDF)
  evaluator=BinaryClassificationEvaluator(labelCol=&quot;arrivedLate&quot;)
  trainROC = evaluator.evaluate(predTrain)
  testROC = evaluator.evaluate(predTest)
  print(f&quot;Train ROC: {trainROC}&quot;)
  print(f&quot;Test ROC: {testROC}&quot;)
  mlflow.log_param(&quot;Dataset Name&quot;, &quot;Flights &quot; + datasetName)
  mlflow.log_metric(key=&quot;Train ROC&quot;, value=trainROC)
  mlflow.log_metric(key=&quot;Test ROC&quot;, value=testROC)

  lrModel = pipeModel.stages[3]
  trainingSummary = lrModel.summary
  roc = trainingSummary.roc.toPandas()
  plt.plot(roc['FPR'],roc['TPR'])
  plt.ylabel('False Positive Rate')
  plt.xlabel('True Positive Rate')
  plt.title('ROC Curve')
  plt.show()
  plt.savefig(&quot;ROCcurve.png&quot;)
  mlflow.log_artifact(&quot;ROCcurve.png&quot;)
  plt.close()
  
  display(predTest.select(stringCols + [&quot;arrivedLate&quot;, &quot;prediction&quot;]))
</code></pre>
<p>What the notebook shows:</p>
<p><a href=""https://i.stack.imgur.com/sCIN9.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/sCIN9.png"" alt=""enter image description here"" /></a></p>
<p>What the MLFlow shows:</p>
<p><a href=""https://i.stack.imgur.com/oXk8Y.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/oXk8Y.png"" alt=""enter image description here"" /></a></p>",2,0,2020-12-04 15:09:56.64 UTC,1.0,2020-12-05 18:10:47.983 UTC,8,apache-spark|matplotlib|pyspark|databricks|mlflow,5219,2011-09-22 15:25:39.197 UTC,2022-09-13 16:17:01.243 UTC,"Boston, MA",6711,353,3,819,<p>Put <code>plt.show()</code> after <code>plt.savefig()</code> - <code>plt.show()</code> will remove your plot because it is shown already.</p>,3.0,2020-12-04 15:14:14.147 UTC,,7.0,"['databricks', 'mlflow']"
