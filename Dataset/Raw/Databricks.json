[
    {
        "Question_title": "Why are my MLflow results not showing up in the Experiment UI view?",
        "Question_creation_date": "2022-10-23T12:55Z",
        "Question_tag": [
            "Ui",
            "Experiment UI View",
            "MLFlow UI",
            "Experiments",
            "Experiment UI",
            "MLFlow",
            "MLflow Experiment Results"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D58Y00009WhYsRSAV/why-are-my-mlflow-results-not-showing-up-in-the-experiment-ui-view",
        "Question_upvote_count": 3,
        "Question_answer_count": 2,
        "Question_view_count": 40,
        "Question_has_accepted_answer": false,
        "Question_body": "The issue:\n\n\u00a0\n\nNone of my MLflow experiment results show up in the Experiment UI.\n\n\u00a0\n\n\u00a0\n\nContext:\n\n\u00a0\n\nI encountered this issue recently, despite having successfully used the MLFlow UI for the past few weeks.\n\n\u00a0\n\nNote: I can still access the experiment runs in a notebook, even though I can't see them in the UI view.",
        "Answers": [
            {
                "Answer_creation_date": "2022-11-23T20:24:44.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @Robbie (Customer)\u200b,\n\n\u00a0\n\nAll MLflow runs are logged to the active experiment, which can be set using any of the following ways:\n\nUse the\u00a0MLflow.set_experiment() command.\nUse the\u00a0experiment_id parameter in the\u00a0mlflow.start_run() command.\nSet one of the MLflow environment variables\u00a0MLFLOW_EXPERIMENT_NAME or MLFLOW_EXPERIMENT_ID.\n\n\u00a0\n\nExpand Post",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-11-24T13:59:02.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "@KanizFatma,\n\nThanks but this doesn\u2019t address the issue\u2014 the runs are being logged to the experiment correctly.\n\nAs I confirmed in the question, I can access the runs via the API\u2026 the results just no longer appear in the browser UI.",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "Databricks-connect not available on Databricks Runtime > 10.4",
        "Question_creation_date": "2022-10-21T8:54Z",
        "Question_tag": [
            "Databricks Runtime",
            "Databricks Team",
            "Runtime"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D58Y00009Va5CwSAJ/databricksconnect-not-available-on-databricks-runtime-104",
        "Question_upvote_count": 0,
        "Question_answer_count": 8,
        "Question_view_count": 63,
        "Question_has_accepted_answer": false,
        "Question_body": "Hello Databricks Team,\n\nDatabricks-connect doesn't work on databricks runtime 11.3.\n\nDatabricks recommends that we use dbx for Databricks Lab instead of databricks-connect.\n\nDatabricks plans no new feature development for Databricks Connect at this time.\n\nDbx doesn't provided interactive debugging capabilities.\n\nCould you continue to maintain this package databricks-connect on databricks runtime > 10.4 ?\n\nThanks",
        "Answers": [
            {
                "Answer_creation_date": "2022-11-21T14:23:46.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @NSRBX (Customer)\u200b\u00a0,\n\n\u00a0\n\nUpdate: Databricks Connect for DBR 11.3 should hopefully be out by the end of November. For the time being Databricks-Connect will be available for each LTS DBR but that might change when other Dev Tools are released.\n\n\u00a0\n\nAlso, we are currently in private preview for the DB-connect replacement, Spark Connect and there's also an official Databricks VS Code extension in preview so we have more developer tools coming soon\n\nExpand Post",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-11-21T15:37:10.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "hi @LandanG (Databricks)\u200b\u00a0\n\nregarding VSCode extesion, are you talk about: paiqo/Databricks-VSCode: VSCode extension to work with Databricks (github.com) ?",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-11-21T15:48:59.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @xiangzhu (Customer)\u200b\u00a0, That extension is created by an external company who I don't believe Databricks has any affiliation with. Databricks is building an official extension which is in private preview so it's not available to the general public just yet",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-11-22T09:52:12.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Is it possible to sign up for the private preview? Should I reach out to my contact point at Databricks for that in that case?",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-11-22T15:41:24.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "@640913 (Customer)\u200b\u00a0I'm not sure about the preview process but yeah I'd reach out to your Databricks rep",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-11-21T16:50:44.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "thx @LandanG (Databricks)\u200b\u00a0do you have any ETA about the public preview ?",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-11-21T20:44:36.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "@xiangzhu (Customer)\u200b\u00a0I believe the private preview has opened quite recently so I don't know if we have an ETA for the public preview yet",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-11-21T21:56:59.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "thx for the confirmation",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "Parallelization in training machine learning models using MLFlow",
        "Question_creation_date": "2022-9-13T14:55Z",
        "Question_tag": [
            "Models",
            "MLFlow",
            "ML Model"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D58Y00009Kz4jwSAB/parallelization-in-training-machine-learning-models-using-mlflow",
        "Question_upvote_count": 0,
        "Question_answer_count": 5,
        "Question_view_count": 176,
        "Question_has_accepted_answer": false,
        "Question_body": "I'm training a ML model (e.g., XGboost) and I have a large combination of 5 hyperparameters, say each parameter has 5 candidates, it will be 5^5 = 3,125 combos.\n\nNow I want to do parallelization for the grid search on all the hyperparameter combos for training a machine learning model to get the best performance of the model.\n\nSo how can I achieve this on Databricks, especially using MLFlow? I've been told I can define a function to train and evaluate the model (using mlflow) and defining an array with all of the hyper-parameter combinations, sc.parallelize the array and then mapping the function over.\n\nI have come up with the code for the sc.parallelize the array, like\n\nparas_combo_test =  [(x, y) for x in [50, 100, 150] for y in [0.8,0.9,0.95]]\nsc.parallelize(paras_combo_test, 3).glom().collect()\n\n\u00a0\n\n(for simplicit, I'm just using two parameters x, y and there are 9 combos in total and I divided them to 3 partitions.)\n\nHow can I map over the function which does the model training with evaluation (probably using mlflow), so that there will be 3 works (each work will train 3 models) in parallel from the partitions of parameter combos I have?",
        "Answers": [
            {
                "Answer_creation_date": "2022-10-13T15:15:53.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "This blog should be very helpful:\n\nhttps://www.databricks.com/blog/2021/04/15/how-not-to-tune-your-model-with-hyperopt.html\n\nHere are the docs on xgboost\n\nhttps://docs.databricks.com/machine-learning/train-model/xgboost.html\n\n\u00a0\n\nA simple rule is never use sc.parallelize.\n\n\u00a0\n\n\u00a0\n\nExpand Post",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-10-13T17:00:41.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Thanks @josephk (Databricks)\u200b\u00a0! It seems we could do the distributed XGBoost training using the num_workers regards to how many workers in the cluster. But can we also speed up by setting a parameter utilizing the number of cores in the cluster?",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-10-13T17:58:54.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "You can set num_workers to your default parallelism\n\n\u00a0\n\nhttps://databricks.github.io/spark-deep-learning/_modules/sparkdl/xgboost/xgboost.html",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-10-18T13:35:16.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "collect() is working on the driver and will not offer any parallelism but rather OOM error.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-11-21T07:45:45.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @ianchenmu (Customer)\u200b\u00a0\n\n\u00a0\n\nHope all is well!\n\n\u00a0\n\nJust wanted to check in if you were able to resolve your issue and would you be happy to share the solution or mark an answer as best? Else please let us know if you need more help.\u00a0\n\n\u00a0\n\nWe'd love to hear from you.\n\n\u00a0\n\nThanks!\n\n\u00a0\n\n\u00a0\n\nExpand Post",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "How can Delta table protocol version be downgraded from higher version to lower version the table properties minReader from 2 to 1 and MaxWriter from 5 to 3.",
        "Question_creation_date": "2022-10-13T23:34Z",
        "Question_tag": [
            "Delta",
            "Databricks Runtime",
            "Version",
            "Runtime",
            "TBL",
            "Delta table"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D58Y00009Tby9cSAB/how-can-delta-table-protocol-version-be-downgraded-from-higher-version-to-lower-version-the-table-properties-minreader-from-2-to-1-and-maxwriter-from-5-to-3",
        "Question_upvote_count": 1,
        "Question_answer_count": 5,
        "Question_view_count": 64,
        "Question_has_accepted_answer": true,
        "Question_body": "Is there a possibility to downgrade the Delta Table protocol versions minReader from 2 to 1 and maxWriter from 5 to 3? I have set the TBL properties to 2 and 5 and columnmapping mode to rename the columns in the DeltaTable but the other users are reading this delta table with Databricks runtime 7.3 and getting the error \"com.databricks.sql.transaction.tahoe.ColumnMappingUnsupportedException:\". they cannot upgrade the Databricks runtime to 10.4. i'm trying to reset the tblproperties it is not happening\n\nIs there a possibility to revert back to version to previous version? and how? or any other solution to handle this problem?\n\nLooking for a solution to the problem please any inputs?",
        "Answers": [
            {
                "Answer_creation_date": "2022-11-14T09:25:24.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @Yaswanth (Customer)\u200b\u00a0, Protocol version upgrades are irreversible, and upgrading the protocol version may break the existing Delta Lake table readers, writers, or both.\n\n\u00a0\n\nTherefore, we recommend you upgrade specific tables only when needed, such as to opt-in to new features in Delta Lake.\n\n\u00a0\n\nYou should also check to ensure that your current and future production tools support Delta Lake tables with the new protocol version.\n\n\u00a0\n\nSource\n\nExpand Post",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-11-14T14:29:21.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi Kaniz Fatma, Yes you are correct.. what is the solution here now.. apart from upgrading the database runtime.. Can I restore the table version to previous one? Or I need to drop the delta table and are create the new one?",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-11-15T14:28:10.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @Yaswanth (Customer)\u200b, Protocol version upgrades are irreversible. You cannot restore the table version to previous one.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-11-15T14:24:54.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Unfortunately You can't downgrade the version. it's an irreversible operation.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-11-18T20:26:21.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @Yaswanth (Customer), We haven\u2019t heard from you since the last response from @youssefmrini (Databricks)\u200b and me\u200b\u200b, and I was checking back to see if our suggestions helped you.\n\n\u00a0\n\nOr else, If you have any solution, please share it with the community, as it can be helpful to others.\n\n\u00a0\n\nAlso, Please don't forget to click on the \"Select As Best\" button whenever the information provided helps resolve your question.\n\nExpand Post",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "How to install Chromium Browser and Chrome Driver on DBX runtime 10.4 and above ?",
        "Question_creation_date": "2022-5-20T8:51Z",
        "Question_tag": [
            "Chromium Browser",
            "Ubuntu",
            "Runtime 10.4",
            "Databricks Runtime",
            "Chrome driver"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D58Y00008qXqGXSA0/how-to-install-chromium-browser-and-chrome-driver-on-dbx-runtime-104-and-above-",
        "Question_upvote_count": 4,
        "Question_answer_count": 21,
        "Question_view_count": 700,
        "Question_has_accepted_answer": false,
        "Question_body": "Hi Team,\n\nWe are wondering if there is a recommended way to install the chromium browser and chrome driver on Databricks Runtime 10.4 and above ?\n\n\u00a0\n\nI have been through the site and have come across several links to this effect, but they all seem to be installing the Chromium Browser from the official Canonical PPA (ppa:canonical-chromium-builds/stage)\n\nFYI - Please use the contents in link at your own risk -\n\n(https://community.databricks.com/s/question/0D53f00001qEWtxCAG/chromedriver-installation-in-databricks)\n\n\u00a0\n\nFrom my understanding, the DBX runtime 10.4 is based on Ubuntu 20.04 - focal for which builds do not seem to be available in that PPA. Maybe because Ubuntu moved from official deb to installing Chromium as a snap ?\n\n\u00a0\n\nSo questions are as follows\n\nBest way to install Chromium Browser in DBX Runtime 10.4 ?\nShould we consider the snap package which seems to be available ? (`apt search chromium-browser`) Will it work well with Selenium and the official Chrome Driver ?\nShould we consider any other sources for the Chromium Browser such as (`ppa:phd/chromium-browser`) or the google chrome browser directly ? Will they be safe ? Any license issues ?\nDoes selenium support any other browsers ? Any other chromium based browsers or firefox with geckodriver ?",
        "Answers": [
            {
                "Answer_creation_date": "2022-06-20T11:41:02.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hello @ranged_coop (Customer)\u200b\u00a0\n\n\u00a0\n\nYou should be able to use the methods defined here\n\nhttps://linuxize.com/post/how-to-install-chromium-web-browser-on-ubuntu-20-04/\n\nSection: Installing Chromium with\u00a0apt\n\n\u00a0\n\nPer this documentation\n\nIt supports a number of browsers (Google Chrome 12+, Internet Explorer 7,8,9,10, Safari 5.1+, Opera 11.5, Firefox 3+) and operating systems (Windows, Mac, Linux/Unix).\n\n\u00a0\n\nhttps://www.browserstack.com/guide/selenium-webdriver-tutorial#:~:text=It%20supports%20a%20number%20of,%2C%20Ruby%2C%20Python%2C%20PHP.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-06-20T12:18:08.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Thank you @aravish (Databricks)\u200b\u00a0for your response.\n\n\u00a0\n\nAs mentioned in my post, Chromium is no longer available as a deb in the ubuntu archives. The link you had shared is a little old.\n\n\u00a0\n\nWhen I search in the Databricks Terminal, we receive results showing Chromium Browser as snap and not as a deb. Hence the question.\n\n\u00a0\n\n/databricks/driver# apt search chromium-browser\nSorting... Done\nFull Text Search... Done\nchromium-browser/focal-updates 1:85.0.4183.83-0ubuntu0.20.04.2 amd64\n  Transitional package - chromium-browser -> chromium snap\n\u00a0\nchromium-browser-l10n/focal-updates 1:85.0.4183.83-0ubuntu0.20.04.2 all\n  Transitional package - chromium-browser-l10n -> chromium snap\n\nWould be really nice if Databricks can provide a recommended way to handle this, since this is a very common use case and there are so many such questions around this topic in our community forums itself...\n\n\u00a0\n\nAlso regarding the other browsers, since we are using databricks, requirement is limited to linux and open source browsers only. I hear that not many websites are tested with firefox, hence a little hesitant there...\n\n\u00a0\n\nThank you...",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-06-27T13:46:46.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @spottedoutlandish (Customer)\u200b\u00a0and @ranged_coop (Customer)\u200b\u00a0, Please see this S.O link. This might help you.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-06-28T06:43:33.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @Kaniz Fatma (Databricks)\u200b\u00a0, Thank you so much for your response.\n\nI have already been through the link you had shared.\n\nIt seems to be installing google-chrome directly from the Google Servers.\n\n\u00a0\n\nBut I have been told that google-chrome is proprietary and chromium-browser is open source. Usage of proprietary software in code might cause license issues or some other issues such as telemetry etc. right ? Hence I was looking for some way we could use the open source version i.e. Chromium on which chrome is based on.\n\n\u00a0\n\nIs there a databricks hosted repository available for such use cases or does selenium work with the snap version of chromium-browser ? I hope I am not the only DBX user using DBX runtime 10.4 and trying to use chromium-browser...\n\n\u00a0\n\nUpdate:\n\nHave tried installing `chromium-browser` and `chromium-chromedriver` through init scripts, but they are not working since the snap processes are not available. Directly trying `snap install chromium` is also not working...\n\nThank you for all the help.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-07-05T14:02:46.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @ranged_coop (Customer)\u200b\u00a0please refer to askUbuntu here. The Chromium browser is only available as a snap in 19.10 and above. If you don't want to install the Chromium snap package, see\u00a0How to install Chromium without snap?",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-07-06T06:47:57.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @Prabakar (Databricks)\u200b\u00a0,\n\nThank you very much for your response.\n\nyes you are correct, Chromium became a snap in 19.10. Databricks runtimes 9.1 and 10.4 both have the Ubuntu version of 20.04.\n\n\u00a0\n\nI am ok with using the snap version of Chromium, but it is not installing properly...\n\nWhen I try to install using the below command in DBX runtime 10.4 through init script...\n\n\u00a0\n\napt update\n\napt install chromium-browser chromium-chromedriver\n\n\u00a0\n\nThe following additional packages will be installed:\n\n\u00a0apparmor liblzo2-2 snapd squashfs-tools udev\n\nSuggested packages:\n\n\u00a0apparmor-profiles-extra apparmor-utils zenity | kdialog\n\nThe following NEW packages will be installed:\n\n\u00a0apparmor chromium-browser chromium-chromedriver liblzo2-2 snapd\n\n\u00a0squashfs-tools udev\n\n0 upgraded, 7 newly installed, 0 to remove and 24 not upgraded.\n\nNeed to get 36.3 MB of archives\n\n\u00a0\n\nThe init script is completing, but we have log that says as follows.\n\n\u00a0\n\n=> Installing the chromium snap\n\n==> Checking connectivity with the snap store\n\n===> System doesn't have a working snapd, skipping\n\n\u00a0\n\nAlso Chromium is not working, when I search for it using which chromium or which chromium-browser, it does not show up.\n\n\u00a0\n\nIs there any other way to get the snap installed and working first before installing ?",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-07-06T06:52:18.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @Prabakar (Databricks)\u200b\u00a0,\n\nI have also gone through the other options of installing Chromium without a snap. But we are finding the options either too complex or not from an official source. I found the following options from the link you had shared. Which option would Databricks recommend ?\n\n\u00a0\n\nOption 01 - Install from Debian Repo\nIs it safe to mix packages from Debian and Ubuntu - Messing source lists seems risky ? Even if we pin items, is it safe enough ?\nOption 02 - ppa:saiarcot895/chromium-beta - Unofficial PPA - Is it safe ? Can we trust it to remain patched and up to date ?\nOption 03 - Google Chrome - Possible License Issues considering that Chrome is not open source ?\nOther Options - Flatpak, Nix Installs, Linux Mint Install.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-07-06T11:43:28.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @ranged_coop (Customer)\u200b\u00a0considering the options I would prefer either to go with Option 1 or 4.\n\nOption 2 is unofficial and I won't consider going for it. It's always better to use the official one as we can trust the provider.\n\nOption 3 should be good but as there is a doubt with the licensing then I would not take a risk. If you want to go with this option then do all possible research w.r.t the license part and then go for it.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-07-06T13:39:46.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @Prabakar (Databricks)\u200b\u00a0\n\nThank you for your response.\n\nWe will try options 01 and 04, however can you please let us know why the snap install is not working. We are using DBX runtime 10.4.\n\n\u00a0\n\napt update\n\napt install chromium-browser chromium-chromedriver\n\n\u00a0\n\nsnapd is not working as expected.\n\nPlease see my response from earlier.\n\n\u00a0\n\nIt would be preferable and safe to use the options available in the Ubuntu repos by default instead of bringing in additional repos from outside.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-07-16T00:23:31.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @ranged_coop (Customer)\u200b\u00a0i need to test this and shall get back to you with my findings or a workaround. \u200b",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-07-18T06:50:48.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Thank you @Prabakar (Databricks)\u200b\n\nUsing the Ubuntu sources would be the best case scenario since we can be sure about the authenticity of the source.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-07-28T10:24:12.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @Prabakar (Databricks)\u200b\u00a0,\n\nJust wanted to check if you got a chance to test the Chromium Snap Install issue ?",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-07-07T10:16:56.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Chromium can be installed\u00a0using the Software application and via the command line.\n\nInstalling Chromium using Software (GUI)\n\nClick on the Software tool in Fedora.\nSearch for Chromium Web Browser.\nClick on Install.\n\n\u00a0\n\nRegards,\n\nWilljoe\n\n\u00a0\n\nExpand Post",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-08-25T08:54:55.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi there @ranged_coop (Customer)\u200b\u00a0\n\n\u00a0\n\nHope all is well! Just wanted to check in if you were able to resolve your issue and would you be happy to share the solution or mark an answer as best? Else please let us know if you need more help.\u00a0\n\n\u00a0\n\nWe'd love to hear from you.\n\n\u00a0\n\nThanks!\n\n\u00a0\n\n\u00a0\n\n\u00a0\n\nExpand Post",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-08-25T09:50:38.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @Vidula (Customer)\u200b, We still have not identified a way to install Open Source Chromium Browser in DBX runtime 10.4.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-08-25T14:47:15.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @ranged_coop (Customer)\u200b\u00a0@Vidula (Customer)\u200b\u00a0, I tried all possible ways to install chromium but at one or the other place it is failing on something. Going back and troubleshooting it is really a time-consuming task. To make it simple, you can try using the docker container services. Install chromium at the container level and use the image.\n\n\u00a0\n\nhttps://hub.docker.com/layers/standard/databricksruntime/standard/10.4-LTS/images/sha256-caee5e0d586d874f06da010ab330a750210d4896c1103474f332d318c01b6b79?context=explore\n\n\u00a0\n\nhttps://docs.databricks.com/clusters/custom-containers.html\n\nExpand Post",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-08-30T06:48:04.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Thank you so much for your response and apologies for all the trouble.\n\n\u00a0\n\nWe had considered the custom runtime options suggested, but it got turned down because the effort to create and maintain the custom runtime was not something we could do at this moment. Also maintenance fixes that flow down time to time are also something to consider. For now, we have planned to either pause or move the requirement to a different/existing option.\n\n\u00a0\n\nThank you once again for all the help.\n\nExpand Post",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-11-09T14:27:25.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi, @ranged_coop (Customer)\u200b\u00a0\u200b\u00a0I've created a new version of the selenium with the databricks manual. Please look here https://community.databricks.com/s/feed/0D58Y00009SWgVuSAL",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-11-15T10:10:10.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Thank you @Hubert Dudek (Customer)\u200b\u00a0 for your script.\n\nIf possible, suggest including a reference to the below link and description just in case Google changes some of the steps so that in the future if the process changes, users would still be able to figure it out.\n\n\u00a0\n\nhttps://www.chromium.org/getting-involved/download-chromium/\n\n\u00a0\n\nNot-as-easy steps:\n\nHead to https://commondatastorage.googleapis.com/chromium-browser-snapshots/index.html\nChoose your platform: Mac, Win, Linux, ChromiumOS\nPick the Chromium build number you'd like to use\nThe latest one is mentioned in the\nLAST_CHANGE\nfile\nDownload the zip file containing Chromium\nThere is a binary executable within to run\n\n\u00a0\n\nExpand Post",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-11-11T01:45:23.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Has anyone figured out how to get Selenium to work on Databricks seamlessly?\n\n\u00a0\n\nBeginning to question whether it's a common occurrence for the Databricks support team to not get back to disgruntled users on time...\n\n\u00a0\n\nThis request has been opened for some months with no satisfactory response.\n\n\u00a0\n\nDoes anyone know any online resource that could guide us to this pertinent issue? Any response at this stage would be appreciated.\n\nExpand Post",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-11-15T14:10:36.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @swrd (Customer)\u200b, Sorry for the delay. We're working on bringing the best possible solution for you.",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "Two or more different ml model on one cluster.",
        "Question_creation_date": "2022-9-5T8:28Z",
        "Question_tag": [
            "MLflow Experiment",
            "Models",
            "MLFlow",
            "Ml"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D58Y00009IJFTPSA5/two-or-more-different-ml-model-on-one-cluster",
        "Question_upvote_count": 0,
        "Question_answer_count": 3,
        "Question_view_count": 78,
        "Question_has_accepted_answer": false,
        "Question_body": "Hi, have you already dealt with the situation that you would like to have two different ml models in one cluster? i.e: I have a project which contains two or more different models with more different pursposes. The goals is to have three different real-time ml endpoints but these models should be deployed only on one cluster.\n\nTomas",
        "Answers": [
            {
                "Answer_creation_date": "2022-10-06T07:27:01.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi, @TomasP (Customer)\u200b\u00a0 you can go through the MLflow guide: https://docs.databricks.com/mlflow/index.html\n\n\u00a0\n\nIf I have understood it correctly, if you are running mlflow models in jobs runs in that case , each one of the jobs/job-runs gets a dedicated cluster that turns off right after the job finishes. It\u2019s possible running a lot of clusters in parallel in order to execute many independent jobs. In a job cluster a single job run deploys a single cluster which cannot be shared. Please correct me if I am wrong.\n\nExpand Post",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-10-07T06:55:32.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @debayan (Databricks)\u200b\u00a0 Thanks for answer.\u00a0Yes, I am familiar with the classical approach.\u00a0I'm more interested if there is any work around. For two model Im able to transfer one model to production stage and second model to staging.\u00a0Both of them have their own containers and have their own endpoints.\u00a0\u00a0it does not matter if one is designed in tensorflow and second one in pytorch. But I would like to find way how to deploy more models on one cluster.\u00a0I know it goes against mlflow concept but the aim is to save costs.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-11-13T06:27:23.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @TomasP (Customer)\u200b\u00a0\n\n\u00a0\n\nHope all is well! Just wanted to check in if you were able to resolve your issue and would you be happy to share the solution or mark an answer as best? Else please let us know if you need more help.\u00a0\n\n\u00a0\n\nWe'd love to hear from you.\n\n\u00a0\n\nThanks!\n\n\u00a0\n\n\u00a0\n\n\u00a0\n\nExpand Post",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "Can the HTML behind a SQL visualisations be accessed?",
        "Question_creation_date": "2022-10-13T13:40Z",
        "Question_tag": [
            "Sql",
            "SQL Visualizations",
            "Self Service Notebooks",
            "MLFlow",
            "SQL Visualisations",
            "Html",
            "Visualization"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D58Y00009TbPyVSAV/can-the-html-behind-a-sql-visualisations-be-accessed",
        "Question_upvote_count": 0,
        "Question_answer_count": 1,
        "Question_view_count": 34,
        "Question_has_accepted_answer": false,
        "Question_body": "We are using MLFlow to manage the usage of some self service notebooks. This involves logging parameters, tables and figures.\n\n\u00a0\n\nFigures are logged using:\n\nmlflow.log_figure(\n  figure=fig,\n  artifact_file=\"visual/fig.html\"\n)\n\nUsually the fig object is generated using seaborn, plotly or matplotlib. With the addition of the redash chart editor to notebooks we can use the inbuilt visualisation tool more.\n\n\u00a0\n\nIs there a way to access the HTML so the an image created using the in-built editor can be logged?\n\n\u00a0\n\nCheers",
        "Answers": [
            {
                "Answer_creation_date": "2022-11-14T13:14:05.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "There is no way to access the html used. You can download the images. The editor uses redash, so you can try looking at that library for more information.",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "Is it wise to use a more recent MLFlow Python package version or is the DB Runtime compatibility matrix strict about MLFlow versions?",
        "Question_creation_date": "2022-10-11T10:12Z",
        "Question_tag": [
            "Library",
            "Compatibility Matrix",
            "Dependency Version",
            "Latest Version",
            "DB Runtime",
            "MLFlow"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D58Y00009TZyi6SAD/is-it-wise-to-use-a-more-recent-mlflow-python-package-version-or-is-the-db-runtime-compatibility-matrix-strict-about-mlflow-versions",
        "Question_upvote_count": 0,
        "Question_answer_count": 2,
        "Question_view_count": 52,
        "Question_has_accepted_answer": false,
        "Question_body": "More concretely, should we fix the dependency version at MAJOR, MINOR or PATCH?\n\n\u00a0\n\nFor example, MLFlow 1.30.0 is available and latest DBR 11.3 LTS is compatible with 1.29.0\n\n\u00a0\n\nMy question comes from the fact that installing our own libraries that use MLFlow, dependency resolution might try to get the latest version if we don't properly pin it.\n\n\u00a0\n\nThanks!",
        "Answers": [
            {
                "Answer_creation_date": "2022-11-11T19:16:24.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "As of now, DBR 11.3 LTS is compatible with MLFlow 1.29.0. (https://docs.databricks.com/release-notes/runtime/releases.html?&_ga=2.185224809.1729528441.1668113831-643525343.1663499643#mlflow-compatibility-matrix)",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-11-14T10:06:14.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi! thanks for the reply, although maybe you didn't notice that I linked to the same url, so we're aware of the matrix.\n\nThe question is, is it compatible solely with 1.29.0? We want to know which dependency should we use in all our projects that might be running against/in the platform:\n\nmlflow==1.29.0\nmlflow==1.29.*\nmlflow==1.*\n\n\u00a0\n\nHope this is clearer! thanks\n\nExpand Post",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "Move folder from dbfs location to user workspace directory in azure databricks",
        "Question_creation_date": "2022-10-1T10:31Z",
        "Question_tag": [
            "Community forum",
            "Databricks cluster",
            "Databricks notebook",
            "Dbfs - databricks file system",
            "Databricks Runtime",
            "Azure databricks",
            "Dbfs Location"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D58Y00009Pl8U8SAJ/move-folder-from-dbfs-location-to-user-workspace-directory-in-azure-databricks",
        "Question_upvote_count": 2,
        "Question_answer_count": 2,
        "Question_view_count": 99,
        "Question_has_accepted_answer": false,
        "Question_body": "I need to move group of files(python or scala file from)or folder from dbfs location to user workspace directory in azure databricks to do testing on file.\n\nIts verify difficult to upload each file one by one into the user workspace directory, so is it possible to move file from dbfs location to user workspace directory in azure databricks?\n\n\u00a0\n\nSince i am not able to upload folder instead of file directly or huge than some particular size, could you suggest the way to move or copy files into user workspace directory in azure databricks.",
        "Answers": [
            {
                "Answer_creation_date": "2022-11-03T14:11:39.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "dbutils.fs.mv or dbutils.fs.cp can help you.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-11-09T15:24:17.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @Data_Engineer (Customer)\u200b, We haven\u2019t heard from you on the last response from @werners (Customer)\u200b\u200b, and I was checking back to see if his suggestions helped you.\n\n\u00a0\n\nOr else, If you have any solution, please do share that with the community as it can be helpful to others.\n\n\u00a0\n\nAlso, Please don't forget to click on the \"Select As Best\" button whenever the information provided helps resolve your question.\n\nExpand Post",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "Run mlflow project from a Job.",
        "Question_creation_date": "2022-9-30T8:21Z",
        "Question_tag": [
            "MLFlow"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D58Y00009PizdnSAB/run-mlflow-project-from-a-job",
        "Question_upvote_count": 1,
        "Question_answer_count": 3,
        "Question_view_count": 65,
        "Question_has_accepted_answer": false,
        "Question_body": "Hey Guys,\n\nI'm trying to make automated process to run ML training sessions using mlflow and databricks jobs.\n\n\u00a0\n\nWhile developing the model on my local machine using IDE, When finished I have a template notebook that get as parameters the mlflow project path and params.\n\nWhile trying to run a job that will run this mlflow project i faced some issues and hope you will be able to help me.\n\n\u00a0\n\nInside the training code ( e.g. main entry point ), I'm using set_experiment and start run with specific names for those run / experiment.\n\nWhen trying to run this code as mlflow project using run api call, When not specified exp_name / run_name in the run api call im getting an error that i can't create an experiment from a job.\n\nOn the other hand When exp_name and run_name are specified within the run api call mlflow ignore set_experiment and start_run with the run name i wanted, Do you know if there is an option to enable creation of a experiments from a job? or way to overcome the need to specify exp_name and run name inside the run call?\n\n\u00a0\n\nAfter some tries i saw that mlflow create an experiment before the training code actually run, this is little problematic because if i need to specify the run name and the experiment name manually this process not gonna be to much automated \ud83d\ude05\n\n\u00a0\n\nCode example:\n\nimport mlflow\n\u00a0\n# This line throw an error, screen shot is attached.\nmlflow.run ( dbutils.widgets.get('Project path), parameters=params)\n\u00a0\n# This line ignore any set_experiment / start_run(run_name='something') specifed in the code.\n\u00a0\nmlflow.run ( dbutils.widgets.get('Project path), parameters=params, experiment_name=dbutils.widgets.get('experiment_name'), run_name='test')",
        "Answers": [
            {
                "Answer_creation_date": "2022-11-01T07:53:46.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "An MLflow Project is a format for packaging data science code in a reusable and reproducible way.\n\nMy CC Pay\n\nExample\n\nStep 1: Create an experiment. In the workspace, select Create > MLflow Experiment. ...\n\nStep 2: Run the MLflow tutorial project. ...\n\nStep 3: View the Databricks job run. ...\n\nStep 4: View the experiment and MLflow run details.\n\nExpand Post",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-11-05T15:47:02.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hey @T55Jennifer (Customer)\u200b\u00a0,\n\nThanks for the answer.\n\nI know what its MLflow project, your suggested approach require manual actions which im trying to avoid..",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-11-01T22:49:45.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @Orianh (Customer)\u200b\u00a0\u200b, We haven\u2019t heard from you since the last response from @T55Jennifer (Customer)\u200b, and I was checking back to see if you have a resolution yet.\n\n\u00a0\n\nIf you have any solution, please share it with the community as it can be helpful to others. Otherwise, we will respond with more details and try to help.\n\n\u00a0\n\nAlso, Please don't forget to click on the \"Select As Best\" button whenever the information provided helps resolve your question.\n\nExpand Post",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "`collect()`ing Large Datasets in R",
        "Question_creation_date": "2022-9-31T18:37Z",
        "Question_tag": [
            "Large Datasets",
            "Unity Catalog",
            "Machine Learning",
            "R"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D58Y00009Pk8joSAB/collecting-large-datasets-in-r",
        "Question_upvote_count": 0,
        "Question_answer_count": 4,
        "Question_view_count": 45,
        "Question_has_accepted_answer": false,
        "Question_body": "Background: I'm working on a pilot project to assess the pros and cons of using DataBricks to train models using R. I am using a dataset that occupies about 5.7GB of memory when loaded into a pandas dataframe. The data are stored in a delta table in Unity Catalog.\n\n\u00a0\n\nProblem: I can `collect()` the data using python (pyspark) in about 2 minutes. However, when I tried to use sparklyr to collect the same dataset in R the command was still running after ~2.5 days. I can't load the dataset into DBFS first because we need stricter data-access controls than DBFS will allow. Below are screenshots of the cells that I ran to `collect()` the data in Python and R.\n\n\u00a0\n\nI'm hoping that I'm just missing something about how sparklyr loads data.\n\n\u00a0\n\nHere is the cell that loads the data using pyspark, you can see that it took 2.04 minutes to complete:\n\u00a0\n\nHere is the cell that loads the data using sparklyr, you can see that I cancelled it after 2.84 days:\n\n\u00a0\n\nI also tried using the `sparklyr::spark_read_table` function but I got an error that `Table or view not found: main.databricks_...` which I think must be because the table is in a metastore managed by Unity Catalog.\n\n\u00a0\n\nEnvironment Info:\n\nDatabricks Runtime: 10.4 LTS\n\nDriver Node Size: 140GB memory and 20 cores\n\nWorker Nodes: 1 worker node with 56GB of memory and 8 cores.\n\nR libraries installed: arrow, sparklyr, SparkR, dplyr",
        "Answers": [
            {
                "Answer_creation_date": "2022-10-31T20:52:27.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "If you have 5GB of data, you don't need spark. Just use your laptop. Spark is for scale and won't out perform well on small data sets because of all the overhead distributed requires.\n\nAlso, don't name a pandas dataframe df_spark_. Just name it something_pdf.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-11-01T16:06:21.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Yes, the name of the dataframe was a little sloppy since it's a Pandas dataframe. Although about the scale, all of the machine learning documentation and sample ML notebooks for DataBricks that I have seen load the dataset into memory on the driver node. And if I remember right the guidance I read from DataBricks was to avoid using a spark-compatible training algorithm as long as your data could fit into memory on the driver node. So while a 5GB dataset could fit on my laptop I'm a little worried that if I can't load 5GB from a Delta Table onto the driver node I almost certainly won't be able to load a larger dataset that wouldn't fit on my laptop, say 50 GB. Plus the dataset contains protected health information which I'm not permitted to download onto my laptop anyway.\n\nExpand Post",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-11-01T06:18:00.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Have you tried performing `collect()` with SparkR? That would require loading the data as a SparkR DataFrame.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-11-01T16:00:59.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "That is a good suggestion, and something I probably should have tried already. Although when I use\n\nSparkR::collect\n\n\u00a0\n\nI get a JVM error:\n\njava.lang.OutOfMemoryError: Requested array size exceeds VM limit\n\n\u00a0\n\n\u00a0\n\nExpand Post",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "Failure in mlflow.spark.load_model : Random Forrest pretrained model",
        "Question_creation_date": "2022-7-26T7:17Z",
        "Question_tag": [
            "TID",
            "Random Forrest",
            "Failure",
            "Model",
            "MLFlow",
            "Stage failure"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D58Y000097fgT9SAI/failure-in-mlflowsparkloadmodel-random-forrest-pretrained-model",
        "Question_upvote_count": 1,
        "Question_answer_count": 2,
        "Question_view_count": 115,
        "Question_has_accepted_answer": true,
        "Question_body": "model = mlflow.spark.load_model(model_uri=f\"models:/{model_name}/{model_version}\")\n\nLog:\n\n\u00a0\n\nAn error occurred while calling o2861.load.\n\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 4599.0 failed 4 times, most recent failure: Lost task 4.3 in stage 4599.0 (TID 3270) (10.139.64.5 executor 1): java.lang.AssertionError: assertion failed: Decision Tree load failed. Expected largest node ID to be 53, but found 26",
        "Answers": [
            {
                "Answer_creation_date": "2022-09-19T22:37:55.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hm. Are you training with one version of Spark but loading in another version? though that should be pretty compatible across versions, just trying to rule that in/out.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-09-30T11:27:25.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @ashrafkhan94 (Customer)\u200b\u00a0Did you get a chance to look into Sean's response. Please let us know if you need more help on this.",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "What will be the next LTS version after 10.4?",
        "Question_creation_date": "2022-9-10T10:42Z",
        "Question_tag": [
            "DBR",
            "LTS",
            "Databricks Runtime",
            "LTS Version",
            "DBR Versions"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D58Y00009K9kCSSAZ/what-will-be-the-next-lts-version-after-104",
        "Question_upvote_count": 0,
        "Question_answer_count": 5,
        "Question_view_count": 196,
        "Question_has_accepted_answer": true,
        "Question_body": "What will be the next LTS version after 10.4?",
        "Answers": [
            {
                "Answer_creation_date": "2022-10-10T11:17:00.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "I thought I read they aim for 11.3, in the next few weeks",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-10-10T12:11:46.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "@kthneighbor (Customer)\u200b\u00a0: The DBR 11.3 LTS is in pipeline. It might be available by end of OCT(expecting).",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-10-11T16:09:02.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Thanks, werners and Sivaprasad!",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-10-20T10:05:29.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hello, 11.3 LTS is now available https://learn.microsoft.com/en-us/azure/databricks/release-notes/runtime/11.3",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-10-26T13:42:57.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Thanks, when I set up a 11.3 LTS job that runs on a Python script, errors are not parsed correctly in the Spark shell.\n\nSee below for example when importing a wrong library.\n\nHow can I report this as a bug?\n\nExpand Post",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "Spark Error/Exception Handling",
        "Question_creation_date": "2022-9-25T6:35Z",
        "Question_tag": [
            "Databricks Runtime",
            "Spark",
            "Error handling",
            "Spark Error"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D58Y00009OYMQxSAP/spark-errorexception-handling",
        "Question_upvote_count": 0,
        "Question_answer_count": 0,
        "Question_view_count": 144,
        "Question_has_accepted_answer": false,
        "Question_body": "I am creating new application and looking for ideas how to handle exceptions in Spark, for example ThreadPoolExecution.\n\n\u00a0\n\nAre there any good practice in terms of error handling and dealing with specific exceptions ?",
        "Answers": []
    },
    {
        "Question_title": "Error loading model from mlflow: java.io.StreamCorruptedException: invalid type code: 00",
        "Question_creation_date": "2022-9-6T13:58Z",
        "Question_tag": [
            "Error",
            "Invalid Type Code",
            "Spark Version",
            "MLFlow",
            "Azure databricks"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D58Y00009J5Q4zSAF/error-loading-model-from-mlflow-javaiostreamcorruptedexception-invalid-type-code-00",
        "Question_upvote_count": 0,
        "Question_answer_count": 5,
        "Question_view_count": 133,
        "Question_has_accepted_answer": true,
        "Question_body": "Hello,\n\n\u00a0\n\nI'm using, in my IDE, Databricks Connect version 9.1LTS ML to connect to a databricks cluster with spark version 3.1 and download a spark model that's been trained and saved using mlflow.\n\n\u00a0\n\nSo it seems like it's able to find a copy the model, but then it's wrong. The same\u00a0works in a databricks notebook are goods, the problem only occurs using databricks connect in my IDE.\n\n\u00a0\n\nWe are getting the same error in different repositories with different models. It started to appear recently.\n\n\u00a0\n\nI have the same problem in other environment with cluster 10.4LTS ML and databricks-connect 10.4.6.\n\n\u00a0\n\nDo you have an idea ?\n\n\u00a0\n\n\u00a0\n\ncode :\n\n\u00a0\n\nmlflow.set_tracking_uri(\"databricks\")\n\nmodel_path = 'dbfs:/databricks/mlflow-tracking/197830957424395/7c5e692873874dadae4f67f44c1aa310/artifacts/rfModel'\n\nmodel_res = mlflow.spark.load_model(model_path)\n\n\u00a0\n\n\u00a0\n\n\u00a0\n\nSee the StackTraceError :\n\n\u00a0\n\n2022/10/06 15:17:11 INFO mlflow.spark: File 'dbfs:/databricks/mlflow-tracking/197830957424395/7c5e692873874dadae4f67f44c1aa310/artifacts/rfModel/sparkml' not found on DFS. Will attempt to upload the file.\n\n22/10/06 15:17:39 WARN DBFS: DBFS create on /tmp/mlflow/f020cb9a-47b2-49ee-8b12-cf2754db61a9/metadata/part-00000 took 2299 ms\n\n22/10/06 15:17:42 WARN DBFS: DBFS create on /tmp/mlflow/f020cb9a-47b2-49ee-8b12-cf2754db61a9/metadata/_SUCCESS took 1687 ms\n\n22/10/06 15:17:46 WARN DBFS: DBFS mkdirs on /tmp/mlflow/f020cb9a-47b2-49ee-8b12-cf2754db61a9/stages/0_RandomForestClassifier_77e9017cbf4d took 2302 ms\n\n2022/10/06 15:19:13 INFO mlflow.spark: Copied SparkML model to /tmp/mlflow/f020cb9a-47b2-49ee-8b12-cf2754db61a9\n\nView job details at ........https....\n\nView job details at ........ https .....\n\n22/10/06 15:19:16 ERROR Instrumentation: java.io.StreamCorruptedException: invalid type code: 00\n\nat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1698)\n\nat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\n\nat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n\nat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\nat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\nat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\n\nat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n\nat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\nat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\nat java.io.ObjectInputStream.readObject(ObjectInputStream.java:503)\n\nat java.io.ObjectInputStream.readObject(ObjectInputStream.java:461)\n\nat scala.collection.immutable.List$SerializationProxy.readObject(List.scala:488)\n\nat sun.reflect.GeneratedMethodAccessor419.invoke(Unknown Source)\n\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\nat java.lang.reflect.Method.invoke(Method.java:498)\n\nat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1184)\n\nat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2296)\n\nat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\nat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\nat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\n\nat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n\nat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\nat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\nat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2093)\n\nat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1655)\n\nat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\n\nat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n\nat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\nat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\nat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2093)\n\nat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1655)\n\nat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\n\nat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n\nat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\nat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\nat java.io.ObjectInputStream.readObject(ObjectInputStream.java:503)\n\nat java.io.ObjectInputStream.readObject(ObjectInputStream.java:461)\n\nat org.apache.spark.sql.util.ProtoSerializer.$anonfun$deserializeObject$1(ProtoSerializer.scala:6631)\n\nat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\nat org.apache.spark.sql.util.ProtoSerializer.deserializeObject(ProtoSerializer.scala:6616)\n\nat com.databricks.service.SparkServiceRPCHandler.execute0(SparkServiceRPCHandler.scala:728)\n\nat com.databricks.service.SparkServiceRPCHandler.$anonfun$executeRPC0$1(SparkServiceRPCHandler.scala:477)\n\nat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\nat com.databricks.service.SparkServiceRPCHandler.executeRPC0(SparkServiceRPCHandler.scala:372)\n\nat com.databricks.service.SparkServiceRPCHandler$$anon$2.call(SparkServiceRPCHandler.scala:323)\n\nat com.databricks.service.SparkServiceRPCHandler$$anon$2.call(SparkServiceRPCHandler.scala:309)\n\nat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\nat com.databricks.service.SparkServiceRPCHandler.$anonfun$executeRPC$1(SparkServiceRPCHandler.scala:359)\n\nat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\nat com.databricks.service.SparkServiceRPCHandler.executeRPC(SparkServiceRPCHandler.scala:336)\n\nat com.databricks.service.SparkServiceRPCServlet.doPost(SparkServiceRPCServer.scala:167)\n\nat javax.servlet.http.HttpServlet.service(HttpServlet.java:707)\n\nat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n\nat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)\n\nat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:550)\n\nat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)\n\nat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n\nat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n\nat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n\nat org.eclipse.jetty.server.Server.handle(Server.java:516)\n\nat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n\nat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n\nat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n\nat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n\nat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\nat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n\nat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n\nat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)\n\nat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)\n\nat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\n\nat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\n\nat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:383)\n\nat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n\nat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n\nat java.lang.Thread.run(Thread.java:748)\n\n...\n\npy4j.protocol.Py4JJavaError: An error occurred while calling o588.load.\n\n: java.io.StreamCorruptedException: invalid type code: 00\n\nat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1698)\n\n\u00a0\n\n\u00a0\n\nThanks for your help.",
        "Answers": [
            {
                "Answer_creation_date": "2022-10-12T15:46:01.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "@NSRBX (Customer)\u200b\u00a0- The client needs to be updated to latest version to fix this issue:\u00a0https://pypi.org/project/databricks-connect/#history",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-10-13T12:18:30.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @NSRBX (Customer)\u200b\u200b, We haven\u2019t heard from you since the last response from @Shan_Chandra (Databricks)\u200b\u00a0, and I was checking back to see if you have a resolution yet.\n\n\u00a0\n\nIf you have any solution, please share it with the community, as it can be helpful to others. Otherwise, we will respond with more details and try to help.\n\n\u00a0\n\nAlso, Please don't forget to click on the \"Select As Best\" button whenever the information provided helps resolve your question.\n\nExpand Post",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-10-14T15:37:30.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hello,\n\n\u00a0\n\nI changed my databricks-connect version 10.4.12, mlflow version is 1.26 but it doesn't work.\n\n\u00a0\n\nI have winutils.exe in my venv under Lib\\site-packages\\pyspark\\bin.\n\nand my environment variable HADOOP_HOME is ok.\n\nVersion python 3.8.10.\n\n\u00a0\n\nThanks for your help.\n\n\u00a0\n\n\u00a0\n\nSee the stacktraceerror :\n\n\u00a0\n\n2022/10/14 17:18:09 INFO mlflow.spark: File 'dbfs:/databricks/mlflow-tracking/67260056032267/6580b479a0ba43beaa3dd7971561fbb7/artifacts/model_rf/sparkml' not found on DFS. Will attempt to upload the file.\n\nTraceback (most recent call last):\n\n\u00a0File \"C:\\Users\\NSR\\py-packages\\test\\test_mlflow.py\", line 21, in <module>\n\n\u00a0\u00a0\u00a0model = exp.get_model(nom=\"model_rf\")\n\n\u00a0File \"C:\\Users\\NSR\\py-packages\\ircem\\mlflow.py\", line 178, in get_model\n\n\u00a0\u00a0\u00a0return mlflow.spark.load_model(model_path)\n\n\u00a0File \"D:\\venv_python\\Python38\\lib\\site-packages\\mlflow\\spark.py\", line 711, in load_model\n\n\u00a0\u00a0\u00a0return _load_model(model_uri=model_uri, dfs_tmpdir_base=dfs_tmpdir)\n\n\u00a0File \"D:\\venv_python\\Python38\\lib\\site-packages\\mlflow\\spark.py\", line 659, in _load_model\n\n\u00a0\u00a0\u00a0model_uri = _HadoopFileSystem.maybe_copy_from_uri(model_uri, dfs_tmpdir)\n\n\u00a0File \"D:\\venv_python\\Python38\\lib\\site-packages\\mlflow\\spark.py\", line 382, in maybe_copy_from_uri\n\n\u00a0\u00a0\u00a0return cls.maybe_copy_from_local_file(_download_artifact_from_uri(src_uri), dst_path)\n\n\u00a0File \"D:\\venv_python\\Python38\\lib\\site-packages\\mlflow\\spark.py\", line 349, in maybe_copy_from_local_file\n\n\u00a0\u00a0\u00a0cls.copy_from_local_file(src, dst, remove_src=False)\n\n\u00a0File \"D:\\venv_python\\Python38\\lib\\site-packages\\mlflow\\spark.py\", line 331, in copy_from_local_file\n\n\u00a0\u00a0\u00a0cls._fs().copyFromLocalFile(remove_src, cls._local_path(src), cls._remote_path(dst))\n\n\u00a0File \"D:\\venv_python\\Python38\\lib\\site-packages\\py4j\\java_gateway.py\", line 1304, in __call__\n\n\u00a0\u00a0\u00a0return_value = get_return_value(\n\n\u00a0File \"D:\\venv_python\\Python38\\lib\\site-packages\\pyspark\\sql\\utils.py\", line 117, in deco\n\n\u00a0\u00a0\u00a0return f(*a, **kw)\n\n\u00a0File \"D:\\venv_python\\Python38\\lib\\site-packages\\py4j\\protocol.py\", line 326, in get_return_value\n\n\u00a0\u00a0\u00a0raise Py4JJavaError(\n\npy4j.protocol.Py4JJavaError: An error occurred while calling o334.copyFromLocalFile.\n\n: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\n\n\u00a0\u00a0\u00a0at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\n\n\u00a0\u00a0\u00a0at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\n\n\u00a0\u00a0\u00a0at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1215)\n\n\u00a0\u00a0\u00a0at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1420)\n\n\u00a0\u00a0\u00a0at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\n\n\u00a0\u00a0\u00a0at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\n\n\u00a0\u00a0\u00a0at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\n\n\u00a0\u00a0\u00a0at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\n\n\u00a0\u00a0\u00a0at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:406)\n\n\u00a0\u00a0\u00a0at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:390)\n\n\u00a0\u00a0\u00a0at org.apache.hadoop.fs.FileSystem.copyFromLocalFile(FileSystem.java:2482)\n\n\u00a0\u00a0\u00a0at org.apache.hadoop.fs.FileSystem.copyFromLocalFile(FileSystem.java:2448)\n\n\u00a0\u00a0\u00a0at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\n\u00a0\u00a0\u00a0at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n\n\u00a0\u00a0\u00a0at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n\n\u00a0\u00a0\u00a0at java.lang.reflect.Method.invoke(Unknown Source)\n\n\u00a0\u00a0\u00a0at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\n\u00a0\u00a0\u00a0at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\n\u00a0\u00a0\u00a0at py4j.Gateway.invoke(Gateway.java:295)\n\n\u00a0\u00a0\u00a0at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\n\u00a0\u00a0\u00a0at py4j.commands.CallCommand.execute(CallCommand.java:79)\n\n\u00a0\u00a0\u00a0at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n\n\u00a0\u00a0\u00a0at py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n\n\u00a0\u00a0\u00a0at java.lang.Thread.run(Unknown Source)\n\nExpand Post",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-10-17T11:41:34.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @Kaniz Fatma (Databricks)\u200b\u00a0and @Shan_Chandra (Databricks)\u200b,\n\n\u00a0\n\nIt works after putting hadoop.dll into C:\\Windows\\System32 folder.\n\n\u00a0\n\nI have hadoop version 3.3.1.\n\n\u00a0\n\nI already had winutils.exe\u00a0in the Hadoop bin folder.\n\n\u00a0\n\nRegards\n\nNath\n\nExpand Post",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-10-18T09:01:41.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @NSRBX (Customer)\u200b\u00a0, Thank you for your response. Keep posting your answers to the community. Thank you for being an integral part of our community.",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "Unable to call logged ML model from a different notebook when using Spark ML",
        "Question_creation_date": "2022-9-4T6:54Z",
        "Question_tag": [
            "Different Notebook",
            "Spark MLlib Models",
            "MLFlow",
            "ML Model"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D58Y00009IHY6FSAX/unable-to-call-logged-ml-model-from-a-different-notebook-when-using-spark-ml",
        "Question_upvote_count": 1,
        "Question_answer_count": 0,
        "Question_view_count": 79,
        "Question_has_accepted_answer": false,
        "Question_body": "Hi, I am a R user and I am experimenting to build an ml model with R and with spark flavoured algorithms in Databricks. However, I am struggling to call a model that is logged as part of the experiment from a different notebook when I use spark flavoured algorithms. This intro may not make sense but please bear with me and I will explain below with codes the issue I am having (I have used community Databricks edition). Same result with my office licensed Databricks service.\n\n\u00a0\n\nPlatform: Databricks community edition\n\nCluster specs: 11.2 ML (includes Apache Spark 3.3.0, Scala 2.12)\u00a0\n\nLanguage: R\n\n\u00a0\n\nIn a notebook 1:\n\n\u00a0\n\n# i used the built-in data so it can be replicated\ninstall.packages(\"mlflow\")\nlibrary(mlflow)\ninstall_mlflow()\ninstall.packages(\"carrier\")\nlibrary(sparklyr)\n \n \nsc <- sparklyr::spark_connect(method = \"databricks\")\n \n# convert to spark table\niris_tbl <- sparklyr::sdf_copy_to(sc, iris, \"iris\", overwrite = TRUE)\n \n# build a model with kmeans based on sparklyr lib and use MLflow to log\nwith(mlflow_start_run(),{\n kmeans_model <- sparklyr::ml_kmeans(iris_tbl, k = 3, features = c(\"Petal_Length\", \"Petal_Width\"))\n  \n predicted <- carrier::crate(~sparklyr::ml_predict(!!kmeans_model, .x))\n  \n mlflow_log_model(predicted, \"model\")\n})\n \n \n# call the logged model from the experiment artifact\nlogged_model = 'runs:/996bc4f0ad2a4681a4acf42515ee73d5/model'\nloaded_model = mlflow_load_model(logged_model)\n \n# predict using the loaded model\nloaded_model(iris_tbl)\n \n# predicts perfectly\n(2) Spark Jobs\n# Source: spark<?> [?? x 7]\n  Sepal_Length Sepal_Width Petal_Length Petal_Width Species features predict\u2026\u00b9\n     <dbl>    <dbl>    <dbl>    <dbl> <chr>  <list>    <int>\n 1     5.1     3.5     1.4     0.2 setosa <dbl [2]>     1\n 2     4.9     3      1.4     0.2 setosa <dbl [2]>     1\n 3     4.7     3.2     1.3     0.2 setosa <dbl [2]>     1\n 4     4.6     3.1     1.5     0.2 setosa <dbl [2]>     1\n 5     5      3.6     1.4     0.2 setosa <dbl [2]>     1\n 6     5.4     3.9     1.7     0.4 setosa <dbl [2]>     1\n 7     4.6     3.4     1.4     0.3 setosa <dbl [2]>     1\n 8     5      3.4     1.5     0.2 setosa <dbl [2]>     1\n 9     4.4     2.9     1.4     0.2 setosa <dbl [2]>     1\n10     4.9     3.1     1.5     0.1 setosa <dbl [2]>     1\n# \u2026 with more rows, and abbreviated variable name \u00b9\u200bprediction\n# \u2139 Use `print(n = ...)` to see more rows\n\nHowever, in Notebook 2, when I try to predict with the same loaded model it throws error - which I can't make any sense of\n\n# install and load library \ninstall.packages(\"mlflow\")\nlibrary(mlflow)\ninstall_mlflow()\n \ninstall.packages(\"carrier\")\nlibrary(sparklyr)\n \n \nsc <- sparklyr::spark_connect(method = \"databricks\")\niris_tbl <- sparklyr::sdf_copy_to(sc, iris, \"iris\", overwrite = TRUE)\n \n# call the logged model and load it\nlogged_model = 'runs:/996bc4f0ad2a4681a4acf42515ee73d5/model'\nloaded_model = mlflow_load_model(logged_model)\n \n# predict\nloaded_model(iris_tbl)\n \n# However following error pops-up\n \n \nError : java.lang.IllegalArgumentException: Object not found 171\n\tat sparklyr.StreamHandler.handleMethodCall(stream.scala:115)\n\tat sparklyr.StreamHandler.read(stream.scala:62)\n\tat sparklyr.BackendHandler.$anonfun$channelRead0$1(handler.scala:60)\n\tat scala.util.control.Breaks.breakable(Breaks.scala:42)\n\tat sparklyr.BackendHandler.channelRead0(handler.scala:41)\n\tat sparklyr.BackendHandler.channelRead0(handler.scala:14)\n\tat io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:327)\n\tat io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:299)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:722)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:658)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:584)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:496)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.lang.Thread.run(Thread.java:748)\n \nError: java.lang.IllegalArgumentException: Object not found 171\n\nCan you please help me to resolve this issue. Thanks",
        "Answers": []
    },
    {
        "Question_title": "Responsible AI on Databricks",
        "Question_creation_date": "2022-8-20T8:23Z",
        "Question_tag": [
            "Ai",
            "Explainable AI",
            "Proven Practice",
            "Interpretability",
            "Lime",
            "Shap",
            "Responsible AI",
            "Bias",
            "MLFlow",
            "Ml"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D58Y00009ETv8ISAT/responsible-ai-on-databricks",
        "Question_upvote_count": 1,
        "Question_answer_count": 1,
        "Question_view_count": 75,
        "Question_has_accepted_answer": false,
        "Question_body": "Looking to learn how you can use responsible AI toolkits on Databricks? Interested in learning how you can incorporate open source tools like SHAP and Fairlearn with Databricks?\n\n\u00a0\n\nI would recommend checking out this blog: Mitigating Bias in Machine Learning With SHAP and Fairlearn from my colleague @sean.owen (Databricks)\u200b.\n\n\u00a0\n\nSHAP is a explainability framework used to determine the relative importance of features used in an ML model to give better transparency, especially when used with more complex models. Fairlearn is a framework to quantify and minimize bias inherit to datasets used an in ML model.\n\n\u00a0\n\nIn addition to leveraging these frameworks as discussed in the article, out of the box Databricks automatically logs SHAP explainability plots with most ml frameworks using mlflow autolog and SHAP plots are automatically generated as part of Databricks AutoML notebook output. You can learn more at our Explainable AI home page.\n\n\u00a0\n\nLet us know how you plan to add Responsible AI frameworks to your ML workflows in the chat!",
        "Answers": [
            {
                "Answer_creation_date": "2022-09-30T06:21:42.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Awesome!",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "Delta table partition directories when column mapping is enabled",
        "Question_creation_date": "2022-8-13T18:20Z",
        "Question_tag": [
            "Delta",
            "Databricks Runtime",
            "Delta table"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D58Y00009CPMtYSAX/delta-table-partition-directories-when-column-mapping-is-enabled",
        "Question_upvote_count": 0,
        "Question_answer_count": 2,
        "Question_view_count": 171,
        "Question_has_accepted_answer": false,
        "Question_body": "I recently created a table on a cluster in Azure running Databricks Runtime 11.1. The table is partitioned by a \"date\" column. I enabled column mapping, like this:\n\n\u00a0\n\nALTER TABLE {schema}.{table_name} SET TBLPROPERTIES('delta.columnMapping.mode' = 'name', 'delta.minReaderVersion' = '2', 'delta.minWriterVersion' = '5')\n\n\u00a0\n\nBefore enabling column mapping, the directory containing the Delta table has the expected partition directories: \"date=2022-08-18\", \"date=2022-08-19\", etc.\n\n\u00a0\n\nAfter enabling column mapping, every time I do a MERGE into that table, I get new directories created with short names like \"5k\", \"Rw\", \"Yd\", etc. When I VACUUM the table, most of the directories are empty, but the empty directories are not removed. We merge into this table frequently, so the table containing the Delta table ends up with lots and lots of empty directories.\n\n\u00a0\n\nI have 2 questions:\n\n\u00a0\n\nIs it expected that these directories will be created with names other than the expected \"date=2022-08-18\"?\n\n\u00a0\n\nIs there a way to make VACUUM remove the empty directories?\n\n\u00a0\n\nI could write code to walk through the Delta table directory and remove the empty directories, but I would rather not touch those directories! That's for Databricks to manage, and I don't want to step in its way.\n\n\u00a0\n\nThanks in advance for any information you can provide.",
        "Answers": [
            {
                "Answer_creation_date": "2022-09-16T04:55:52.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi, For removing files or directories using VACUUM , you can refer https://docs.databricks.com/delta/delta-utility.html#remove-files-no-longer-referenced-by-a-delta-table\n\n\u00a0\n\nAs far as I know, the dates will be the default naming syntax, which can be renamed.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-09-27T12:11:31.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @Gary Irick (Customer)\u200b\u00a0\n\n\u00a0\n\nDoes @debayan (Databricks)\u200b\u00a0 response answer your question? If yes, would you be happy to mark it as best so that other members can find the solution more quickly?\n\n\u00a0\n\nWe'd love to hear from you.\n\n\u00a0\n\nThanks!\n\n\u00a0\n\n\u00a0\n\n\u00a0\n\n\u00a0\n\nExpand Post",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "API limit on mlflow.tracking.client.MlflowClient.list_run_infos method?",
        "Question_creation_date": "2022-8-6T13:37Z",
        "Question_tag": [
            "MLflow API Limit",
            "Run Infos Method",
            "API Limit",
            "Api",
            "Databricks Community",
            "MLFlow"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D58Y00009AJ1zVSAT/api-limit-on-mlflowtrackingclientmlflowclientlistruninfos-method",
        "Question_upvote_count": 0,
        "Question_answer_count": 2,
        "Question_view_count": 57,
        "Question_has_accepted_answer": false,
        "Question_body": "I'm trying out managed MLflow on Databricks Community edition, with tracking data saved on Databricks and artifacts saved on my own AWS S3 bucket.\n\n\u00a0\n\nI created one experiment and logged 768 runs in the experiment. When I try to get the list of the runs with list_run_infos method, the return maxes out at 399 instead of 768. Is this a limit imposed on Community Edition?\n\n\u00a0\n\nCode:\n\nfrom mlflow.tracking import MlflowClient\nfrom mlflow.entities import ViewType\nclient = MlflowClient()   \nexp_id = client.get_experiment_by_name(\"exp_name\").experiment_id\nload_max = 10000\n\u00a0\nrun_list = client.list_run_infos(\n                          experiment_id=exp_id, \n                          run_view_type=ViewType.ACTIVE_ONLY, \n                          max_results=load_max\n) \nprint(len(run_list))\n399",
        "Answers": [
            {
                "Answer_creation_date": "2022-09-19T22:28:40.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Are 768 of them 'active'? this lists only active runs, according to the method call here.\n\n\u00a0\n\nNote that you should get a paginated result from this method. I am not sure that's the issue here, but the result is not going to be all results.\n\nI don't believe there is otherwise a limit here.\n\n\u00a0\n\nFinally, related, this method is deprecated in favor of search_runs anyway, note.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-09-22T08:46:41.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @j_b (Customer)\u200b\u00a0\n\n\u00a0\n\nHope all is well! Just wanted to check in if you were able to resolve your issue and would you be happy to share the solution or mark an answer as best? Else please let us know if you need more help.\u00a0\n\n\u00a0\n\nWe'd love to hear from you.\n\n\u00a0\n\nThanks!\n\n\u00a0\n\n\u00a0\n\nExpand Post",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "Cannot find an AWS Cloudwatch init script that supports runtime 11.x",
        "Question_creation_date": "2022-8-20T9:53Z",
        "Question_tag": [
            "Databricks Runtime",
            "Aws",
            "AWS Cloudwatch",
            "AWS Cloudwatch Init Script"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D58Y00009ETzujSAD/cannot-find-an-aws-cloudwatch-init-script-that-supports-runtime-11x",
        "Question_upvote_count": 1,
        "Question_answer_count": 1,
        "Question_view_count": 47,
        "Question_has_accepted_answer": false,
        "Question_body": "Hi all,\n\n\u00a0\n\nI'm using the AWS CW init global script in order to monitor my clusters' instances.\n\nI'm also using data live tables with some autoloader jobs.\n\nUnfortunately, the data live tables are now running runtime version 11.\n\nAs a result, newly created pipelines are failing and the init script needs some adjustments to runtime 11.\n\n\u00a0\n\nIs there an init script for version 11? (I couldn't fine one)\n\nIs there a way to force newly created data live tables to use an older runtime for the time being? (I saw an option for current=version 11 and preview=a newer version).\n\n\u00a0\n\nThanks",
        "Answers": [
            {
                "Answer_creation_date": "2022-09-20T10:32:43.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Unfortunately, in delta live tables, you can not specify runtime (except current and preview, which you mentioned). It could be helpful that DLT runtimes releases are mentioned on the databricks side the same way as SQL, ML, and standard ones @Kaniz Fatma (Databricks)\u200b\u00a0@Prabakar (Databricks)\u200b\u00a0@Lindsay\u2728 (Databricks)\u200b\u00a0\n\n\u00a0\n\nRegarding the cloudwatch script, I know that one from the AWS blog https://aws.amazon.com/blogs/mt/how-to-monitor-databricks-with-amazon-cloudwatch/\n\n\u00a0\n\nI think the issue is that since 11.0 log4j is in version 2:\n\n\u00a0\n\nLog4j is upgraded from Log4j 1 to Log4j 2\n\nLog4j 1 is being upgraded to Log4j 2. Legacy Log4j 1 dependencies are being removed.\n\nIf you depend on the Log4j 1 classes that were previously included inside the Databricks Runtime, those classes no longer exist. You should upgrade your dependencies to Log4j 2.\n\nIf you have custom plugins or configuration files that depend on Log4j 2, they may no longer work with the version of Log4j 2 in this release. For assistance, contact your Databricks representative.\n\n\u00a0\n\n\u00a0\n\n\u00a0\n\nExpand Post",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "Shap Values for predictions from registered model",
        "Question_creation_date": "2022-7-29T17:16Z",
        "Question_tag": [
            "Model registry",
            "Variable Explanations",
            "Shap",
            "Values",
            "MLFlow"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D58Y000098jGoMSAU/shap-values-for-predictions-from-registered-model",
        "Question_upvote_count": 0,
        "Question_answer_count": 4,
        "Question_view_count": 133,
        "Question_has_accepted_answer": true,
        "Question_body": "I have saved a model in the model registry using MLFlow.\u00a0How can I find the shap values for this model once I have generated predictions in batch mode?\u00a0\n\n\u00a0\n\nShap tree explainer does not support the mlflow pyfunc model type. When I use mlflow.shap.log_explanation(model.predict, data), I get an error that Provided model function fails when applied to the provided data set. It seems to indicate the required columns are missing. All the required columns were included and when I run predictions with the model it generates predictions. model.predict(data).",
        "Answers": [
            {
                "Answer_creation_date": "2022-08-30T12:48:24.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi, Could you please check if this section helps in the below documentation:\n\nhttps://www.databricks.com/blog/2019/06/17/detecting-bias-with-shap.html",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-08-30T16:27:11.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Thanks for your help. I was able to figure it out from the documentation but adjustments were needed. The model which was based on the Data Bricks auto ML model was really an sk learn pipeline. I have no y values as this is prediction data, not a test set. I needed to use the mlflow sklearn model.\n\nmodel = mlflow.sklearn.load_model(model_uri)\n\nFor Shap tree explainer (shown in the documentation) I needed to use the tree explainer as the model in the pipeline and manually run the other parts of the pipeline. Like this:\n\n\u00a0\n\nexplainer = shap.TreeExplainer(model['regressor'])\n\nobservations = model[\"column_selector\"].transform(prediction_data)\n\nobservations = model[\"standardizer\"].transform(observations)\n\nshap_values = explainer.shap_values(observations)\n\n\u00a0\n\nAnother option was not to use the tree explainer: see: https://towardsdatascience.com/using-shap-values-to-explain-how-your-machine-learning-model-works-732b3f40e137\n\n\u00a0\n\nexplainer = shap.Explainer(model.predict, prediction_data)\n\nshap_values = explainer(prediction_data, max_evals = 2000)\n\n\u00a0\n\nExpand Post",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-09-19T22:36:30.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Yes, TreeExplainer only works on the tree-based model itself. That's fine and the way to use it if you literally only want to explain the model, not the pipeline. If you want to explain anything else like a PIpeline or custom pyfunc model, you need to use KernelExplainer in SHAP (think it's just called Explainer now, yes). It's much slower but can operate on anything.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-09-20T07:44:05.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi, My Name is Deniel Klane. I live in Belen, new york and\u00a0I am a professional academic writer at greatassignmenthelp.com I work here as an economics assignment writer for the last 4 years. If you are looking for online Economics Assignment Help then I can help you with that. Get an economics assignment writing service from greatassignmenthelp and score an A+ grade in your assignments. Economics is a vast area of study that involves the factors of manufacturing distribution, and utilization of goods and services.\u00a0\n\n\u00a0\n\nExpand Post",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "'autoML' is not found when using databricks.automl with runtime 112.ML (and runtime 10.4 LTS ML).",
        "Question_creation_date": "2022-8-13T6:20Z",
        "Question_tag": [
            "Configure AutoML Experiment",
            "Configure AutoML",
            "Databricks Runtime",
            "ModuleNoteFoundError",
            "Databricks.automl",
            "MLFlow",
            "AutoML Experiment",
            "Delta table"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D58Y00009BOJwqSAH/automl-is-not-found-when-using-databricksautoml-with-runtime-112ml-and-runtime-104-lts-ml",
        "Question_upvote_count": 0,
        "Question_answer_count": 7,
        "Question_view_count": 241,
        "Question_has_accepted_answer": true,
        "Question_body": "I have tried to set up a autoML experiment with runtime 11.2ML and data from a delta table. However I receive the error \"ModuleNotFoundError: No module named 'databricks.automl'\" and \"AutoML not available: Use Databricks Runtime 8.3 ML or above.\" though I am using a runtime newer than 8.3. Does anyone know how to get the module so I can proceed with autoML?",
        "Answers": [
            {
                "Answer_creation_date": "2022-09-09T21:44:12.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "It should be\n\nfrom databricks import automl\n\nThe table doesn't need to be delta, but does need to be a table in the metastore.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-09-12T06:15:08.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @josephk (Databricks)\u200b\u00a0\n\nWhen I run that code I get the message on the figure below. It seems that my \"databricks\" library doesn't include automl.\n\n\u00a0\n\n\u00a0\n\n(I am using runtime 11.2 ML though it states that autoML is not available for my cluster. I have tried earlier versions too.)\n\nExpand Post",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-09-12T16:29:37.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "You do not need to install this library. It is automatically installed within the newest (10.0 or above) Databricks ML runtimes. Use 10.4 ML LTS for the more stable runtime and use the newest runtimes for the latest features.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-09-13T06:18:20.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "hi @smayorquin (Databricks)\u200b\u00a0\n\nIn the original post I was using runtime 11.2ML and I have now tried with runtime 10.4ML, but I still get the same error.\n\n\u00a0\n\nExpand Post",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-09-19T22:20:35.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Just to confirm, this is correct, and I do not see this problem on 11.2 ML. The software is installed, and your import is correct. I am not sure why you observe this; are init scripts modifying the ML runtime when the cluster starts, maybe?",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-09-20T06:03:48.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "I used the script provided by databricks as I use the autoML incorporated in databricks, so I would not assume that it modifies the ML runtime. I think there must have been something wrong with the cluster configuration, because I have created a new cluster and now it seems to work.\n\n\u00a0\n\nExpand Post",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-09-13T10:54:58.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "I deleted the cluster and created a new with runtime 9.1 LTS ML which solved the problem.",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "toPandas() causes IndexOutOfBoundsException in Apache Arrow",
        "Question_creation_date": "2022-3-19T17:47Z",
        "Question_tag": [
            "Databricks Runtime",
            "Apache",
            "Apache Arrow"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001sJUheCAG/topandas-causes-indexoutofboundsexception-in-apache-arrow",
        "Question_upvote_count": 1,
        "Question_answer_count": 15,
        "Question_view_count": 969,
        "Question_has_accepted_answer": false,
        "Question_body": "Using DBR 10.0\n\n\u00a0\n\nWhen calling toPandas() the worker fails with IndexOutOfBoundsException. It seems like ArrowWriter.sizeInBytes (which looks like a proprietary method since I can't find it in OSS) calls arrow's getBufferSizeFor which fails with this error. What is the root cause of this issue?\n\n\u00a0\n\nHere's a sample of the full stack trace:\n\n\u00a0\n\njava.lang.IndexOutOfBoundsException: index: 16384, length: 4 (expected: range(0, 16384))\nat org.apache.arrow.memory.ArrowBuf.checkIndexD(ArrowBuf.java:318)\nat org.apache.arrow.memory.ArrowBuf.chk(ArrowBuf.java:305)\nat org.apache.arrow.memory.ArrowBuf.getInt(ArrowBuf.java:424)\nat org.apache.arrow.vector.complex.BaseRepeatedValueVector.getBufferSizeFor(BaseRepeatedValueVector.java:229)\nat org.apache.arrow.vector.complex.ListVector.getBufferSizeFor(ListVector.java:621)\nat org.apache.spark.sql.execution.arrow.ArrowFieldWriter.getSizeInBytes(ArrowWriter.scala:165)\nat org.apache.spark.sql.execution.arrow.ArrowWriter.sizeInBytes(ArrowWriter.scala:118)\nat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.$anonfun$next$1(ArrowConverters.scala:224)\nat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\nat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1647)\nat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.next(ArrowConverters.scala:235)\nat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.next(ArrowConverters.scala:199)\nat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\nat scala.collection.Iterator.foreach(Iterator.scala:943)\nat scala.collection.Iterator.foreach$(Iterator.scala:943)",
        "Answers": [
            {
                "Answer_creation_date": "2022-04-19T18:14:33.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "@ivanychev (Customer)\u200b\u00a0, I think it's trying to return too much data to pandas and overloading the memory. What are you trying to do? You shouldn't need to use pandas much anymore with the 3.2 introduction of pandas API for Spark https://databricks.com/blog/2021/10/04/pandas-api-on-upcoming-apache-spark-3-2.html",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-04-19T19:17:50.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "I'm feeding the DataFrame to the ML model. The `toPandas()` works perfectly fine with `spark.sql.execution.arrow.pyspark.enabled` set to `false`.\n\n\u00a0\n\nBut disabling arrow pipeline by pipeline is far from perfect. The error above doesn't explain a lot and the fail occurs in the proprietary code. At this point I don't know where to look for an error",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-04-19T19:21:39.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Weirdly, `getBufferSizeFor` is the cause of the failure. IMO the method with such a name shouldn't cause out of bounds error.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-04-19T19:28:00.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "to_pandas() is only for a small dataset.\n\n\u00a0\n\nPlease use instead:\n\nto_pandas_on_spark()\n\nIt is essential to use Pandas on Spark instead of ordinary Pandas so that it will work in a distributed way. Here is more info https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/index.html\n\n\u00a0\n\nSo always import Pandas as:\n\nimport pyspark.pandas as ps",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-04-19T19:33:12.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "As I noted, `to_pandas()\u00a0` works great with `spark.sql.execution.arrow.pyspark.enabled` set to `false`. I understand that to_pandas_on_spark() is an option, but I need a Pandas DataFrame, not a Pandas-on-Spark DataFrame.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-04-19T19:50:18.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Turning arrow off is going to increase your execution time. It might be better to use something like applyinpandas. You might want to adjust the batch size https://spark.apache.org/docs/3.0.0/sql-pyspark-pandas-with-arrow.html#setting-arrow-batch-size",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-04-19T19:57:49.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Again, I can't use `applyinpandas` because I need to collect data to feed into an ML model. I need a *Pandas dataframe*.\n\n\u00a0\n\nI have enough memory on my driver (turning off arrow makes the code work).",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-04-19T20:23:19.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "applyinpandas takes a function argument, which can be an ML model.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-04-19T20:29:49.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "We train an ML model, not apply it. We need to fetch a batch of data as Pandas dataframe and feed it into a model for training.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-04-19T20:39:35.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Yes, the ml model training is done with a function such as model.fit().",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-04-20T08:29:32.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "I know that. Is my question not clear?",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-08-12T06:28:25.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "I have the similar situation.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-05-11T11:48:07.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @ivanychev (Customer)\u200b\u00a0 , Just a friendly follow-up. Do you still need help, or @Hubert Dudek (Customer)\u200b and @josephk (Databricks)\u200b\u00a0's response help you to find the solution? Please let us know.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-06-21T14:24:48.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @ivanychev (Customer)\u200b\u00a0, We haven\u2019t heard from you on the last response from me, and I was checking back to see if you found a solution. Or else, If you have any solution, please share it with the community as it can be helpful to others. Otherwise, we will respond with more details and try to help.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-09-19T22:41:56.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "This could be a Arrow version mismatch. Do you by chance try to install anything that could install a different arrow version? it can happen indirectly via other libs.",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "What are the parameters For MLflow Project file",
        "Question_creation_date": "2022-6-12T18:08Z",
        "Question_tag": [
            "Mlflow project",
            "MLFlow",
            "MLflow Project File"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D58Y00008wACEaSAO/what-are-the-parameters-for-mlflow-project-file",
        "Question_upvote_count": 0,
        "Question_answer_count": 4,
        "Question_view_count": 124,
        "Question_has_accepted_answer": false,
        "Question_body": "Hi! I was just wondering what are the parameters For MLflow Project file?\n\n\u00a0\n\nI'm following this tutorial to create my own MLflow Project: https://docs.databricks.com/applications/mlflow/projects.html\n\n\u00a0\n\nand within this tutorial, the MLproject file looks like this:\n\nname: My Project\n\u00a0\nconda_env: my_env.yaml\n\u00a0\nentry_points:\n  main:\n    parameters:\n      data_file: path\n      regularization: {type: float, default: 0.1}\n    command: \"python train.py -r {regularization} {data_file}\"\n  validate:\n    parameters:\n      data_file: path\n    command: \"python validate.py {data_file}\"\n\nHow did they determine what the parameters are? What exactly are parameters? Are they the parameters from when you do log_param in Mlflow in the notebook?\n\n\u00a0\n\nPlease help! Many thanks!",
        "Answers": [
            {
                "Answer_creation_date": "2022-07-24T21:04:47.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "You can refer to MLflow doc for more details.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-09-04T05:28:18.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi there @confusedIntern (Customer)\u200b\u00a0\n\n\u00a0\n\nDoes @Prabakar (Databricks)\u200b\u00a0response answer your question? If yes, would you be happy to mark it as best so that other members can find the solution more quickly?\n\n\u00a0\n\nWe'd love to hear from you.\n\n\u00a0\n\nThanks!\n\n\u00a0\n\n\u00a0\n\n\u00a0\n\n\u00a0\n\nExpand Post",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-09-05T05:13:50.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "I am also encountering this issue. Don't know why its happening.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-09-19T22:32:40.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "These parameters are parameters that you will specify when you run the MLflow Project with the mlflow CLI. It lets you parameterize your code, and then pass different parameters to it. How you use them is up to your code. These are not model hyperparameters, of the type you might log with a model.",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "MLflow Project run always comes back as status failed.",
        "Question_creation_date": "2022-6-13T15:18Z",
        "Question_tag": [
            "Urgent Question",
            "Mlflow project",
            "MLFlow"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D58Y00008wC0BxSAK/mlflow-project-run-always-comes-back-as-status-failed",
        "Question_upvote_count": 0,
        "Question_answer_count": 7,
        "Question_view_count": 176,
        "Question_has_accepted_answer": false,
        "Question_body": "Hi! This is kind of an urgent question so any help would be greatly appreciated! Thanks so much!\n\n\u00a0\n\nSo I'm following this tutorial to try to create an MLflow project: https://docs.databricks.com/applications/mlflow/projects.html\n\n\u00a0\n\nI tried with the example in the tutorial by following into the GitHub repo and downloading the files into my DBFS and creating a cluster-spec json.\n\n\u00a0\n\nThis is the code I used:\n\nBut this is the result I get back:\n\n\u00a0\n\nI don't understand why I'm getting back status failure when I'm using the same code from the tutorial. And when I go see the experiment, its says there are no artifacts:\n\n\u00a0\n\nPlease help! Thank you so much!",
        "Answers": [
            {
                "Answer_creation_date": "2022-07-15T07:40:55.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @confusedIntern (Customer)\u200b, You must use a new cluster specification when running an MLflow Project on Databricks. Running Projects against existing clusters is not supported.\n\n\u00a0\n\nCheck your cluster specifications once and try again.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-07-15T13:28:46.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @Kaniz Fatma (Databricks)\u200b! By new cluster specification, do you mean creating a new cluster using a json file? In the tutorial, this code was what we needed to run in the notebook:\n\nmlflow run <uri> -b databricks --backend-config <json-new-cluster-spec>\n\nso for the <json-new-cluster-spec>, what I have for that code is this:\nWould this be the new cluster specification?\n\n\u00a0\n\n(Also, the company's databricks I'm using has internet restriction, and I'm not sure if that would factor into this error, but if it does, I would love to have an explanation!)\n\n\u00a0\n\nThank you so much!!\n\nExpand Post",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-07-17T15:31:06.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "https://docs.microsoft.com/en-us/azure/databricks/applications/mlflow/projects",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-07-18T13:57:08.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @alexxx (Databricks)\u200b\u00a0and @Kaniz Fatma (Databricks)\u200b\u00a0\n\n\u00a0\n\nI did specify the cluster specifications. It's the .json file from the one line of code. That json file was also uploaded into DBFS for me to use.\n\n\u00a0\n\nI'm still confused as to why this is not working. I have a feeling it might be because my company restricts access to the internet and the project is reaching out to the internet? If so, how do I make it work so it doesn't reach the internet and could work?\n\n\u00a0\n\nThank you so much!\n\nExpand Post",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-07-24T20:57:37.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "hi @confusedIntern (Customer)\u200b\u00a0you have the logs for the failed cluster. Check it to have a better understanding of what went wrong.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-09-04T06:50:42.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hey there @confusedIntern (Customer)\u200b\u00a0\n\n\u00a0\n\nHope all is well! Just wanted to check in if you were able to resolve your issue and would you be happy to share the solution or mark an answer as best? Else please let us know if you need more help.\u00a0\n\n\u00a0\n\nWe'd love to hear from you.\n\n\u00a0\n\nThanks!\n\n\u00a0\n\n\u00a0\n\nExpand Post",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-09-19T22:34:22.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "This is generally not how you use MLflow in Databricks. You are already in Databricks so do not need to send code to Databricks to execute. Instead just run your code in a notebook; there is no need to package as an MLflow Project. Projects are primarily for use outside of Databricks, though you can send them to Databricks to execute - though you'd do that from a CLI from elsewhere.",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "Getting Spark & Scala version in Cluster node initialization script",
        "Question_creation_date": "2021-11-16T10:16Z",
        "Question_tag": [
            "Databricks Runtime",
            "Spark & Scala",
            "Scala Version",
            "Initialization Script"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001YmjF2CAJ/getting-spark-scala-version-in-cluster-node-initialization-script",
        "Question_upvote_count": 1,
        "Question_answer_count": 18,
        "Question_view_count": 674,
        "Question_has_accepted_answer": true,
        "Question_body": "Hi there,\n\n\u00a0\n\nI am developing a Cluster node initialization script (https://docs.gcp.databricks.com/clusters/init-scripts.html#environment-variables) in order to install some custom libraries.\n\n\u00a0\n\nReading the docs of Databricks we can get some environment variables with data related with the current running cluster node.\n\n\u00a0\n\nBut I need to figure out what Spark & Scala version is currently been deployed. Is this possible?\n\nThanks in advance\n\n\u00a0\n\nRegards",
        "Answers": [
            {
                "Answer_creation_date": "2021-12-16T15:00:56.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @ahuarte (Customer)\u200b\u00a0! My name is Kaniz, and I'm the technical moderator here. Great to meet you, and thanks for your question! Let's see if your peers in the community have an answer to your question first. Or else I will get back to you soon. Thanks.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-12-16T16:32:39.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi Kaniz, thank you very much. For sure I will learn very much in this forum.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-12-16T18:12:10.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @ahuarte (Customer)\u200b\u00a0you can get the spark and scala version from the DBR that you will be using on the cluster.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-12-16T18:22:08.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-12-16T18:35:31.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @Prabakar (Databricks)\u200b\u00a0Thank you very much for your response,\n\n\u00a0\n\nbut I mean how I can get this info in a script. I am trying to develop this sh init script for several Clusters with different Databricks runtimes.\n\n\u00a0\n\nI tried it searching files in that script but I did not find any \"*spark*.jar\" file from where to extract the current version of the runtime (Spark & Scala version).\n\n\u00a0\n\nWhen the cluster is already started there are files with this pattern, but in the moment that the init script is executed it seems that pyspark is not installed yet.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-12-16T18:37:29.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "I know that Databricks CLI tool is available, but it is not configured when the init script is running.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-12-17T02:16:59.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hm, this is a hacky idea, maybe there is a better way, but you could\n\nls /databricks/jars/spark*\n\nand parse the results to get the version of Spark and Scala. You'll see files like spark--command--command-spark_3.1_2.12_deploy.jar containing the versions.",
                "Answer_has_accepted": true
            },
            {
                "Answer_creation_date": "2021-12-17T10:23:26.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @sean.owen (Databricks)\u200b\u00a0thanks four your reply,\n\n\u00a0\n\nyour idea can work, but unfortunatelly there is any filename with the full version name. I am missing the minor part:\n\n\u00a0\n\nyyyyyy_spark_3.2_2.12_xxxxx.jar -> Spark version is really 3.2.0\n\n\u00a0\n\nI have configured databricks CLI to get metadata of the cluster and I get this output:\n\n\u00a0\n\n{\n\n\"cluster_id\": \"XXXXXXXXX\",\n\n\"spark_context_id\": YYYYYYYYYYYY,\n\n\"cluster_name\": \"Devel - Geospatial\",\n\n\"spark_version\": \"10.1.x-cpu-ml-scala2.12\", ##<------!!!!\n\n....\n\n}\n\n\u00a0\n\n\"spark_version\" property does not contain info about the spark version but about the DBR :-(, any thoughts?\n\n\u00a0\n\nThanks in advance\n\nregards\n\nAlvaro",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-12-17T13:16:03.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Do you need such specific Spark version info, why? should not matter for user applications",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-12-17T13:31:05.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "I am trying to install Geomesa, from: https://mvnrepository.com/artifact/org.locationtech.geomesa/geomesa-gt-spark-runtime\n\nor\n\nfrom:\n\nhttps://github.com/locationtech/geomesa/releases\n\n\u00a0\n\nI think I need the exact release.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-12-17T14:32:32.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "I doubt it's sensitive to a minor release, why?\n\nBut you also control what DBR/Spark version you launch the cluster with",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-12-17T16:53:53.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Many thanks @sean.owen (Databricks)\u200b\u00a0I am going to apply your advice, I am not going to write a generic init script that figures out everything, but a specific version of it for each Cluster type, really we only have 3 DBR types.\n\n\u00a0\n\nThank you very much for your support\n\nRegards",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-12-27T17:17:18.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "@ahuarte (Customer)\u200b\u00a0- How did it go?",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-12-28T11:44:19.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi,\n\n\u00a0\n\nMy idea was to deploy Geomesa or Rasterframes on Databricks in order to provide spatial capabilities to this platofrm. Finally, according to some advices in Rasterframes Gitter chat I selected the DBR 9.0 where I am installing pyrasterframes 0.10.0 via \"pip\" and no getting any errors.\n\n\u00a0\n\nI hope this info can be help.\n\nRegards\n\n\u00a0\n\nExpand Post",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-12-31T16:10:01.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Thank you so much! Would you be happy to mark whichever answer is best in your mind? That will help new members know which is the most effective.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-01-27T14:14:20.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "You should be able to just pick the version that matches Spark and Scala from maven.\n\n\u00a0\n\nHere is a simple way to get the cluster Spark version",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-01-27T14:26:50.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "The question is about an init script though",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-09-14T16:02:25.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "We can infer the cluster DBR version using the env $DATABRICKS_RUNTIME_VERSION. (For the exact spark/scala version mapping, you can refer to the specific DBR release notes)\n\n\u00a0\n\nSample usage inside a init script,\n\n\u00a0\n\nDBR_10_4_VERSION=\"10.4\"\nif [[ \"$DATABRICKS_RUNTIME_VERSION\" == \"$DBR_10_4_VERSION\"* ]]; then\n  echo \"running 10.4 specific commands\"\nelse\n  echo \"Skipping 10.4 specific commands\"\nfi\n\n\u00a0\n\nExpand Post",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "When is the End of Life (EOL) date for Databricks Runtime 7.3 LTS? Can we still use an unsupported version till its EOL date?",
        "Question_creation_date": "2022-7-18T13:56Z",
        "Question_tag": [
            "DBR",
            "Databricks Runtime",
            "End Of Life",
            "Date",
            "Data Bricks"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D58Y000095uDWaSAM/when-is-the-end-of-life-eol-date-for-databricks-runtime-73-lts-can-we-still-use-an-unsupported-version-till-its-eol-date",
        "Question_upvote_count": 0,
        "Question_answer_count": 3,
        "Question_view_count": 136,
        "Question_has_accepted_answer": true,
        "Question_body": "The End of Support (EOS) date sneaked up on us and we are now wondering if we can delay our upgrade post the EOS date. Could you please help us analyze the risks of operating a DBR version post EOS date?",
        "Answers": [
            {
                "Answer_creation_date": "2022-08-28T02:41:30.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @Hooli (Customer)\u200b\u00a0the 7.3 LTS \u200bEOS is until Sep 24, 2022.\n\n\u200b\n\nFor EOL we cant give you the exact date as Databricks reserves the right to completely remove a release version from the API at any time after support ends, without prior notice.\n\n\u200b\n\nHowever the version should be available atleast for a couple of months after EOS. It would be better if you try to migrate to latest LTS to avoid last min huddles. I would say sooner the better.\n\n\u200b\n\nhttps://docs.databricks.com/release-notes/runtime/10.x-migration.html\n\nExpand Post",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-09-12T06:27:45.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @Hooli (Customer)\u200b\u00a0\n\n\u00a0\n\nDoes @Prabakar (Databricks)\u200b\u00a0 response answer your question? If yes, would you be happy to mark it as best so that other members can find the solution more quickly?\n\n\u00a0\n\nWe'd love to hear from you.\n\n\u00a0\n\nThanks!\n\n\u00a0\n\n\u00a0\n\nExpand Post",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-09-12T16:16:05.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "I selected it as the best answer, thanks.",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "Train machine learning models: How can I take my ML lifecycle from experimentation to production?",
        "Question_creation_date": "2022-8-7T15:52Z",
        "Question_tag": [
            "Sql",
            "Databricks notebook",
            "Python",
            "Databricks Runtime",
            "Table",
            "Best Practices",
            "ML Lifecycle",
            "Visualization",
            "Dataframe",
            "Cluster"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D58Y00009AKrzASAT/train-machine-learning-models-how-can-i-take-my-ml-lifecycle-from-experimentation-to-production",
        "Question_upvote_count": 0,
        "Question_answer_count": 0,
        "Question_view_count": 112,
        "Question_has_accepted_answer": false,
        "Question_body": "Note: the following guide is primarily for Python users. For other languages, please view the following links:\n\n\u2022 Table batch reads and writes\n\n\u2022 Create a table in SQL\n\n\u2022 Visualizing data with DBSQL\n\n\u00a0\n\n\u00a0\n\nThis step-by-step guide will get your data science projects underway by enabling you to:\n\n\u2022 Use display() commands to quickly understand your data\n\n\u2022 Process and save data efficiently\n\n\u2022 Import any machine learning framework\u00a0\n\n\u00a0\n\n\u00a0\n\nTo start, use the persona switcher to open your Machine Learning homepage\n\n\u00a0\n\n\u00a0\n\n\u00a0\n\nPart 1: Use display() commands to quickly understand your data\n\n\u00a0\n\nView your data in an interactive output and quickly create visualizations using the display() command to view your DataFrame.\u00a0\n\n\u00a0\n\n\u00a0\n\n1. Create a notebook. Give it a name, set the default language as Python, and select a Cluster\u00a0\n\n\u00a0\n\n\u00a0\n\n\u00a0\n\n2. Write a command to load your data into a DataFrame, or load the following sample DataFrame\u00a0\n\n\u00a0\n\nraw_data = spark.read.format(\"delta\").load(\"/databricks-datasets/nyctaxi-with-zipcodes/subsampled\")\n\n\u00a0\n\n\u00a0\n\n\u00a0\n\n\u00a0\n\n3. Use the python display () command to view your Dataframe\u00a0\n\n\u00a0\u00a0\u00a0\u00a0\u00a0display(raw_data)\n\n\u00a0\n\n\u00a0\n\n4. Above displayed results, to the right of Table, click + and select \"Visualization\"\n\n\u00a0\n\n\u00a0\n\n\u00a0\n\n5.\u00a0In the Visualization type drop-down, choose a chart type\u00a0\n\nRecommendation: Use a scatter plot for this data\n\n\u00a0\n\n\u00a0\n\n6. Select the data to appear in the visualization\n\nRecommendation: X column = trip_distance; Y column = fare_amount\n\n\u00a0\n\n\u00a0\n\n7. Click Save\u00a0\n\n\u00a0\n\n\u00a0\n\nYou are now ready to discover new insights from your data.\u00a0\n\n\u00a0\n\n\u00a0\n\n\u00a0\n\nPart 2: Process and save data efficiently\n\n\u00a0\n\nSave the results of your analysis by persisting the results to storage:\n\n\u00a0\n\n\u2022 SQL DDL commands: You can use standard SQL DDL commands supported in Apache Spark (for example, CREATE TABLE AS SELECT) to create Delta tables\n\n\u00a0\n\n\u2022 Table batch writes guide:\u00a0\n\n# Create table in the metastore using DataFrame's schema and write data to it\n\ndf.write.format(\"delta\").saveAsTable(\"default.people10m\")\n\n\u00a0\n\n\u00a0\n\nPart 3: Import any machine learning framework\u00a0\n\n\u00a0\n\n1. Import the necessary libraries. These libraries are preinstalled on Databricks Runtime for Machine Learning (AWS|Azure|GCP) clusters and are tuned for compatibility and performance.\n\n\u00a0\n\nimport mlflow\n\nimport numpy as np\n\nimport pandas as pd\n\nimport sklearn.datasets\n\nimport sklearn.metrics\n\nimport sklearn.model_selection\n\nimport sklearn.ensemble\n\n\u00a0\n\nfrom hyperopt import fmin, tpe, hp, SparkTrials, Trials, STATUS_OK\n\nfrom hyperopt.pyll import scope\n\n\u00a0\n\n\u00a0\n\nNow you\u2019ve trained your machine learning models, check out the links below for more.\n\n\u00a0\n\n\u00a0\n\nLearn more:\n\n\u2022 Databricks introduction to notebooks\u00a0\n\n\u2022 Documentation on how to import, read and modify data\n\n\u2022 Guide to creating visualizations\u00a0\n\n\u2022 Data Science getting started guide\n\n\u2022 Apache Spark Programming with Databricks course\n\n\u2022 Ask a Databricks expert live in Office Hours\u00a0\n\n\u2022 Feel free to contact us\n\n\u00a0\n\n\u00a0\n\nDrop your questions, feedback and tips below!",
        "Answers": []
    },
    {
        "Question_title": "Why this Databricks ML code gets stuck?",
        "Question_creation_date": "2022-5-27T6:26Z",
        "Question_tag": [
            "Azure databricks",
            "Machine Learning"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D58Y00008sFztNSAS/why-this-databricks-ml-code-gets-stuck",
        "Question_upvote_count": 0,
        "Question_answer_count": 7,
        "Question_view_count": 121,
        "Question_has_accepted_answer": false,
        "Question_body": "I could not paste the code here because of the some word not allowed, so I have to paste it elsewhere.\n\n\u00a0\n\nBelow is OK:\n\nhttps://justpaste.it/8xcr9\n\n\u00a0\n\nBut below gets stuck:\n\nhttps://justpaste.it/8nydt\n\nand it keeps looping and running...",
        "Answers": [
            {
                "Answer_creation_date": "2022-06-27T07:02:36.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "can you give some more info on the environment used, what error you get etc?",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-06-27T08:55:00.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "what commands to issue to retrieve the environment used? There is no error message, it just show \"Running command ....\" forever",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-06-27T09:23:58.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "I see,\n\nI don't immediately see the issue.\n\nUp to what point does it work?",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-06-28T23:47:08.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "https://community.cloud.databricks.com/?o=7839042079751087#notebook/1592399853593565/command/1592399853593624\n\nI am waiting for its feature_importances to display out, but it keeps \"Running command ...\" and there is no output.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-07-29T22:57:09.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "do you see any error messages in the driver logs? stdout or log4j? any warnings?",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-06-28T21:36:42.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "the Issue could be with versioning",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-08-27T07:28:12.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hey @THIAM HUATTAN (Customer)\u200b\u00a0\n\n\u00a0\n\nHope all is well! Just wanted to check in if you were able to resolve your issue, and would you be happy to share the solution or mark an answer as best? Else please let us know if you need more help.\u00a0\n\n\u00a0\n\nWe'd love to hear from you.\n\n\u00a0\n\nThanks!\n\n\u00a0\n\n\u00a0\n\nExpand Post",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "Databricks S3A error - java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory not found",
        "Question_creation_date": "2022-7-23T16:21Z",
        "Question_tag": [
            "Error",
            "Databricks Runtime",
            "Class",
            "Aws s3",
            "Difference Betwen"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D58Y0000971D01SAE/databricks-s3a-error-javalangclassnotfoundexception-class-orgapachehadoopfss3acommits3acommitterfactory-not-found",
        "Question_upvote_count": 0,
        "Question_answer_count": 4,
        "Question_view_count": 281,
        "Question_has_accepted_answer": false,
        "Question_body": "We are getting the below error for runtime 10.x and 11.x when writing to s3 via saveAsNewAPIHadoopFile function. The same jobs are running fine on runtime 9.x and 7.x. The difference betwen 9.x and 10.x is the former has hadoop 2.7 bindings with spark 3.1 whereas latter has hadoop 3.2 bindings with spark 3.2. Is databricks runtime missing some jars? Any help is appreciated.\n\n\u00a0\n\njava.lang.RuntimeException: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory not found\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2720)\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0at org.apache.hadoop.mapreduce.lib.output.PathOutputCommitterFactory.getCommitterFactory(PathOutputCommitterFactory.java:179)\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0at org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.getOutputCommitter(FileOutputFormat.java:336)\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupCommitter(HadoopMapReduceCommitProtocol.scala:116)\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:195)\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0at org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0at org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsNewAPIHadoopDataset$1(PairRDDFunctions.scala:1078)\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0at org.apache.spark.rdd.RDD.withScope(RDD.scala:411)\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0at org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopDataset(PairRDDFunctions.scala:1076)\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0at org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsNewAPIHadoopFile$2(PairRDDFunctions.scala:995)\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0at org.apache.spark.rdd.RDD.withScope(RDD.scala:411)\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0at org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:986)",
        "Answers": [
            {
                "Answer_creation_date": "2022-08-23T20:48:00.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Why are you using that saveAsNewAPI function?",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-08-23T21:27:50.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "We have some internal OutputFileFormatter for mainframe and fixedlength data formats to support our Data Integration and Data Quality tools. We have been using them for legacy reasons and it was working till 9.x runtime version.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-08-27T18:48:12.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "We can reproduce the above error for runtime 10.x and 11.x using the below code in a notebook.\n\n\u00a0\n\nimport org.apache.hadoop.io.IntWritable\n\nimport org.apache.hadoop.io.Text\n\nimport org.apache.hadoop.mapreduce.lib.output.TextOutputFormat\n\nimport org.apache.spark.rdd.PairRDDFunctions\n\n\u00a0\n\nval l = List((10,\"a\"),(20,\"b\"),(30,\"c\"),(40,\"d\"))\n\nval rdd = sc.parallelize(l)\n\nval rddWritable = rdd.map(x=> (new IntWritable(x._1), new Text(x._2)))\n\nval pairRDD = new PairRDDFunctions(rddWritable)\n\npairRDD.saveAsNewAPIHadoopFile(\"s3a://bucket/testout.dat\",\n\n\u00a0classOf[IntWritable],\n\n\u00a0classOf[Text],\n\n\u00a0classOf[TextOutputFormat[IntWritable,Text]],\n\n\u00a0spark.sparkContext.hadoopConfiguration)\n\nExpand Post",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-08-28T16:25:08.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "We have resolved this issue by using s3 scheme instead of s3a i.e. pairRDD.saveAsNewAPIHadoopFile(\"s3://bucket/testout.dat\",",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "Versions of Spark, Python, Scala, R in each Databricks Runtime",
        "Question_creation_date": "2022-7-23T8:12Z",
        "Question_tag": [
            "Python",
            "Aws",
            "Spark",
            "Scala",
            "Java",
            "R",
            "Library",
            "Proven Practice",
            "Databricks Runtime"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D58Y0000970JsFSAU/versions-of-spark-python-scala-r-in-each-databricks-runtime",
        "Question_upvote_count": 3,
        "Question_answer_count": 2,
        "Question_view_count": 81,
        "Question_has_accepted_answer": false,
        "Question_body": "What version of Spark, Python, Scala, R are included in each Databricks Runtime? What libraries are pre-installed?\n\n\u00a0\n\nYou can find this info at the Databricks runtime releases page (AWS | Azure | GCP).\n\n\u00a0\n\nLet us know if you have any additional questions on this topic in the comments!",
        "Answers": [
            {
                "Answer_creation_date": "2022-08-25T11:51:51.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Awesome! Keep posting such informative posts. Thank you @Isaac Gritz (Databricks)\u200b\u00a0!",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-08-25T21:10:59.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Wow! Thanks for the help @Isaac Gritz (Databricks)\u200b\u00a0!",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "Databricks Runtime Support",
        "Question_creation_date": "2022-7-23T8:11Z",
        "Question_tag": [
            "Runtime Support",
            "DBR",
            "Databricks Runtime",
            "Support",
            "Runtime",
            "Proven Practice"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D58Y0000970JfWSAU/databricks-runtime-support",
        "Question_upvote_count": 3,
        "Question_answer_count": 0,
        "Question_view_count": 73,
        "Question_has_accepted_answer": false,
        "Question_body": "How Long are Databricks runtimes supported for? How often are they updated?\n\n\u00a0\n\nYou can learn more about the Databricks runtime support lifecycle here (AWS | Azure | GCP).\n\n\u00a0\n\nLong Term Support (LTS) runtimes are released every 6 months and supported for 2 years.\n\n\u00a0\n\nLet us know if you have any additional questions on this in the comments!",
        "Answers": []
    },
    {
        "Question_title": "Databricks MLOps Best Practices",
        "Question_creation_date": "2022-7-23T8:03Z",
        "Question_tag": [
            "Mlops",
            "Feature Store",
            "Best Practices",
            "MLFlow",
            "Machine Learning",
            "SDLC",
            "Devops",
            "Machine Learning Ops",
            "Proven Practice"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D58Y0000970IviSAE/databricks-mlops-best-practices",
        "Question_upvote_count": 2,
        "Question_answer_count": 0,
        "Question_view_count": 66,
        "Question_has_accepted_answer": false,
        "Question_body": "Where to find the best practices on MLOps on Databricks\n\n\u00a0\n\nWe recommend checking out the Big Book of MLOps for detailed guidance on MLOps best practices on Databricks including reference architectures.\n\n\u00a0\n\nFor a deep dive on the Databricks Feature store, we recommend checking out the Comprehensive Guide to Feature Stores.\n\n\u00a0\n\nHave you been able to implement MLOps on Databricks? Let us know your learnings in the comments!",
        "Answers": []
    },
    {
        "Question_title": "java.util.NoSuchElementException: key not found",
        "Question_creation_date": "2022-7-17T22:08Z",
        "Question_tag": [
            "Standard",
            "Delta",
            "Databricks Runtime",
            "Standard DS14 V2 Cluster",
            "Key",
            "Microsoft",
            "UTC"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D58Y000095t7deSAA/javautilnosuchelementexception-key-not-found",
        "Question_upvote_count": 1,
        "Question_answer_count": 0,
        "Question_view_count": 250,
        "Question_has_accepted_answer": false,
        "Question_body": "Hello,\n\nWe are using a Azure Databricks with Standard DS14_V2 Cluster with Runtime 9.1 LTS, Spark 3.1.2 and Scala 2.12 and facing the below issue frequently when running our ETL pipeline. As part of the operation that is failing there are several joins happening with delta tables and the output of that is being written into another delta table.\n\n\u00a0\n\nThere was a similar issue reported earlier and was fixed as part of DB runtime 9.0 (unsupported) : [SPARK-34000] ExecutorAllocationListener threw an exception java.util.NoSuchElementException - ASF JIRA (apache.org)\n\n\u00a0\n\nHere is the documentation for that : Databricks Runtime 9.0 (Unsupported) - Azure Databricks | Microsoft Docs\n\n\u00a0\n\nIs the below exception an unrelated issue from the one reported in the bug mentioned above and should I be logging another bug for this to be fixed? We are facing this issue every few runs of our daily job. Any help or pointers would be greatly appreciated\n\n\u00a0\n\nHere is the exception with the stack trace : I have uploaded a file with the full stack trace because of the size limit\n\n\u00a0\n\nAn error occurred while calling o74209.insertInto.\n\n: java.util.NoSuchElementException: key not found: Project [none#1 AS #0, none#4, none#18 AS #1, none#13, none#12, none#3, CASE WHEN isnull(none#19) THEN -25567 ELSE cast(gettimestamp(none#19, yyyy-MM-dd, Some(Etc/UTC), false) as date) END AS #2, none#20, none#0 AS #3, (none#3 = '') AS #4]\n\n+- Relation[none#0,none#1,none#2,none#3,none#4,none#5,none#6,none#7,none#8,none#9,none#10,none#11,none#12,none#13,none#14,none#15,none#16,none#17,none#18,none#19,none#20,none#21,none#22,none#23,... 5 more fields] parquet\n\n\u00a0\n\nat scala.collection.MapLike.default(MapLike.scala:235)\n\nat scala.collection.MapLike.default$(MapLike.scala:234)\n\nat scala.collection.AbstractMap.default(Map.scala:63)\n\nat scala.collection.MapLike.apply(MapLike.scala:144)\n\nat scala.collection.MapLike.apply$(MapLike.scala:143)\n\nat scala.collection.AbstractMap.apply(Map.scala:63)\n\nat com.databricks.sql.transaction.tahoe.stats.PrepareDeltaScan$$anonfun$prepareDeltaScanParallel$1.applyOrElse(PrepareDeltaScan.scala:229)\n\nat com.databricks.sql.transaction.tahoe.stats.PrepareDeltaScan$$anonfun$prepareDeltaScanParallel$1.applyOrElse(PrepareDeltaScan.scala:227)\n\nat org.apache.spark.sql.catalyst.plans.QueryPlan$$anon$2.apply(QueryPlan.scala:545)\n\nat org.apache.spark.sql.catalyst.plans.QueryPlan$$anon$2.apply(QueryPlan.scala:541)\n\nat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n\nat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n\nat org.apache.spark.sql.catalyst.plans.QueryPlan$$anon$2.applyOrElse(QueryPlan.scala:541)\n\nat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:484)\n\nat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:86)\n\nat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:484)\n\nat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:29)\n\n\u00a0\n\n\u00a0\n\n\u00a0\n\nThanks & Regards,\n\nHitesh",
        "Answers": []
    },
    {
        "Question_title": "Databricks Academy - Access to training recording attended during Data & AI Summit 2022",
        "Question_creation_date": "2022-6-27T14:53Z",
        "Question_tag": [
            "AI Summit",
            "Day ML Training",
            "Machine Learning",
            "Databricks Academy",
            "Paid Trainings"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D58Y00008zpNLqSAM/databricks-academy-access-to-training-recording-attended-during-data-ai-summit-2022",
        "Question_upvote_count": 1,
        "Question_answer_count": 2,
        "Question_view_count": 184,
        "Question_has_accepted_answer": false,
        "Question_body": "Hi All,\n\n\u00a0\n\nI attended a 2 day ML training during the Data & AI 2022 summit and I received an email from the events team (ataaisummit@typeaevents.com) telling that the recordings for training and related material will be available in my Databricks Academy site by 07/22/2022. But I still do not see those training or material. Can any one from the Databricks academy please help check and advise.\n\n\u00a0\n\nAppreciate your help regrading this.\n\nThank You.",
        "Answers": [
            {
                "Answer_creation_date": "2022-08-03T18:10:43.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @Sri H (Customer)\u200b\u00a0! I am checking on this for you - hang tight! I'll try and get an update asap from the Academy Team.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-08-11T14:39:54.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Thank you for helping @Lindsay\u2728 (Databricks)\u200b\u00a0. I can see the course added to my academy courses. Have a great day.",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "No Module named 'mlflow'",
        "Question_creation_date": "2022-6-28T19:06Z",
        "Question_tag": [
            "Module",
            "MLFlow",
            "Scalable Machine"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D58Y000090M98QSAS/no-module-named-mlflow",
        "Question_upvote_count": 1,
        "Question_answer_count": 4,
        "Question_view_count": 337,
        "Question_has_accepted_answer": true,
        "Question_body": "I new to the scalable machine learning with apache spark course. I am in the notebook ML 00a - Install Datasets it includes one cell (attached) which throws an error 'no module named 'mlflow''. It attempts to run the Classroom-Setup file. Error is thrown on cmd 5 (attached). What can I do to remedy this?",
        "Answers": [
            {
                "Answer_creation_date": "2022-07-29T09:50:47.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Please use Machine Learning Runtime for your cluster https://docs.databricks.com/runtime/mlruntime.html",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-07-29T20:42:27.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hubert is correct, make sure to use the ML Runtime. I attached a picture from the cluster creation page.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-08-01T22:51:11.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "We have ML DBR version to be used for ML related workloads. So it is adviced to use ml dbr instead of standard dbr.\u200b",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-08-03T18:11:52.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "@duck_butter123 (Customer)\u200b\u00a0I hope the suggestions above helped out! If so, please select one as 'best' for us!\n\n\u00a0\n\nIf you still need assistance, let us know! \ud83d\ude0a",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "ML Practioner | ml 09 - automl notebook | error on importing databricks.automl",
        "Question_creation_date": "2022-7-1T16:30Z",
        "Question_tag": [
            "ML Practioner",
            "Machine Learning",
            "Ml"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D58Y000090vo4iSAA/ml-practioner-ml-09-automl-notebook-error-on-importing-databricksautoml",
        "Question_upvote_count": 0,
        "Question_answer_count": 4,
        "Question_view_count": 132,
        "Question_has_accepted_answer": true,
        "Question_body": "executing the following code...\n\n\u00a0\n\nfrom databricks import automl\n\nsummary = automl.regress(train_df, target_col=\"price\", primary_metric=\"rmse\", timeout_minutes=5, max_trials=10)\n\n\u00a0\n\ngenerates the error...\n\n\u00a0\n\nImportError: cannot import name 'automl' from 'databricks' (/local_disk0/.ephemeral_nfs/envs/pythonEnv-7a7f3e61-af65-4995-9ebc-bb12f01b5c60/lib/python3.8/site-packages/databricks/__init__.py)\n\n---------------------------------------------------------------------------\n\nImportError Traceback (most recent call last)\n\n<command-1508431422070671> in <module>\n\n----> 1 from databricks import automl\n\n2\n\n3 summary = automl.regress(train_df, target_col=\"price\", primary_metric=\"rmse\", timeout_minutes=5, max_trials=10)\n\n\u00a0\n\nImportError: cannot import name 'automl' from 'databricks' (/local_disk0/.ephemeral_nfs/envs/pythonEnv-7a7f3e61-af65-4995-9ebc-bb12f01b5c60/lib/python3.8/site-packages/databricks/__init__.py)\n\nCommand took 0.12 seconds -- by tobiasc@slalom.com at 7/30/2022, 1:44:21 PM on SandboxML 10.3\n\n\u00a0\n\nprior to attempting to execute the above code, the required packages were successfully installed with the following statement...\n\n\u00a0\n\npip install mlflow hyperopt databricks xgboost\n\n\u00a0\n\ni've spent a few hours attempting to troubleshoot this issue so really hoping somebody can help me out with a solution.\n\n\u00a0\n\ni'm using the \"Machine Learning\" persona.",
        "Answers": [
            {
                "Answer_creation_date": "2022-08-01T22:33:27.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "@Slalom_Tobias (Customer)\u200b\u00a0\u00a0I could see you are using the standard DBR version and it is expected. Please use the ML DBR version and it should resolve your issue.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-08-01T23:50:06.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Thanks Prabakar!\n\nThe root issue is that my company's sandbox didn't have an ML cluster available so I was trying to import the required libraries, which don't seem to be publicly available. I'm now guessing that's done purposefully to protect the code. I've tested this within community edition and am asking the admins for my company sandbox to stand-up an ML cluster.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-08-03T17:08:35.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "@Slalom_Tobias (Customer)\u200b, the value added to use ML runtime is in a fact that the Databricks Runtime ML includes a variety of popular ML libraries that are updated with each release to include new features and fixes.\n\nIn addition, Databricks has a subset of the supported libraries as top-tier libraries. For these libraries, Databricks provides a faster update cadence, updating to the latest package releases with each runtime release (barring dependency conflicts).\n\nIn addition to the pre-installed libraries, Databricks Runtime ML differs from Databricks Runtime in the cluster configuration and in how you manage Python packages.\n\nFinally, ML runtime includes tools to automate the model development process and help you efficiently find the best performing model : AutoML, Managed MLFlow, Hyperopt.\n\nHope that you'll get ML runtime at your disposal\n\nExpand Post",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-08-05T09:03:22.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "I'm happy to see a particularly subject.",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "describe data science and machine learning?",
        "Question_creation_date": "2022-3-7T8:19Z",
        "Question_tag": [
            "Machine Learning",
            "Data Science"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001qArEuCAK/describe-data-science-and-machine-learning",
        "Question_upvote_count": 1,
        "Question_answer_count": 4,
        "Question_view_count": 214,
        "Question_has_accepted_answer": true,
        "Question_body": "",
        "Answers": [
            {
                "Answer_creation_date": "2022-04-07T09:37:45.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "@Supreme Auto City (Customer)\u200b\u00a0, https://en.wikipedia.org/wiki/Machine_learning\n\nhttps://en.wikipedia.org/wiki/Data_science",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-04-07T22:16:21.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "@Supreme Auto City (Customer)\u200b\u00a0, Here's an intro to data science and machine learning: https://databricks.com/discover/machine-learning",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-04-08T11:11:35.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "@Supreme Auto City (Customer)\u200b\u00a0, Reference from Databricks\n\nhttps://databricks.com/discover/machine-learning?_ga=2.145319604.1627188802.1649416188-1814760479.1646848603",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-04-08T20:21:12.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @Supreme Auto City (Customer)\u200b\u00a0, Would you like to mark any of the above as the best answer as it provided the answer to your question?",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "How to track features used and filters in MLFlow?",
        "Question_creation_date": "2022-4-17T7:37Z",
        "Question_tag": [
            "MLFlow",
            "Machine Learning",
            "Features"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D58Y00008iq5g0SAA/how-to-track-features-used-and-filters-in-mlflow",
        "Question_upvote_count": 1,
        "Question_answer_count": 3,
        "Question_view_count": 217,
        "Question_has_accepted_answer": false,
        "Question_body": "Hello everyone,\n\n\u00a0\n\nWe are experimenting with several approaches in a Machine Learning project ( binary classification), and we would like to keep track of those using MLFlow. We are using the feature store to build, store, and retrieve the features, and h2o to do the modeling. The approaches we are trying involve combinations of the following:\n\nChanging the features used\nFiltering the dataset (keeping or discarding certain records)\n\n\u00a0\n\nI have yet to find a way to keep track of those things in an organized way in MLFlow, except for writing the information somehow in the run description, but that does not seem right. I have also tried to write it as a parameter, but for instance the feature list exceeds the size limit. Is there a way to do this \"correctly\"?\n\n\u00a0\n\nThank you",
        "Answers": [
            {
                "Answer_creation_date": "2022-06-17T11:36:11.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hello @vivoedoardo (Customer)\u200b\u00a0\n\n\u00a0\n\nDoes this documentation help?\n\nhttps://docs.databricks.com/applications/machine-learning/feature-store/ui.html#track-feature-lineage-and-freshness",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-07-18T16:40:57.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hey @vivoedoardo (Customer)\u200b\u00a0\n\n\u00a0\n\nHope you are well.\n\n\u00a0\n\nJust wanted to see if @aravish (Databricks)\u200b's\u00a0answer helped, would you let us know and mark an answer as best? It would be really helpful for the other members too. Else please let us know if you need more help.\u00a0\n\n\u00a0\n\nCheers!\n\n\u00a0\n\nExpand Post",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-07-23T07:12:31.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Thanks for the information, I will try to figure it out for more. Keep sharing such informative post keep suggesting such post.",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "Hi, I get the following error when I enable model serving for spacy model via MLFLOW.",
        "Question_creation_date": "2022-2-1T10:21Z",
        "Question_tag": [
            "Model Serving",
            "CMD ARGS",
            "Spacy Model",
            "MLFlow",
            "Error Message"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001jhu6OCAQ/hi-i-get-the-following-error-when-i-enable-model-serving-for-spacy-model-via-mlflow",
        "Question_upvote_count": 0,
        "Question_answer_count": 5,
        "Question_view_count": 721,
        "Question_has_accepted_answer": true,
        "Question_body": "+ echo 'GUNICORN_CMD_ARGS=--timeout 63 --workers 4 '\n\nGUNICORN_CMD_ARGS=--timeout 63 --workers 4\u00a0\n\n+ mlflow models serve --no-conda -m /tmp/tmp1a4ltdrk/spacymodelv1 -h unix:/tmp/3.sock -p1\n\n2022/03/01 08:26:37 INFO mlflow.models.cli: Selected backend for flavor 'python_function'\n\n2022/03/01 08:26:37 INFO mlflow.pyfunc.backend: === Running command 'gunicorn --timeout=60 -b unix:/tmp/3.sock:1 -w 1 ${GUNICORN_CMD_ARGS} -- mlflow.pyfunc.scoring_server.wsgi:app'\n\n[2022-03-01 08:26:38 +0000] [24589] [INFO] Starting gunicorn 20.1.0\n\n[2022-03-01 08:26:38 +0000] [24589] [INFO] Listening at: unix:/tmp/3.sock:1 (24589)\n\n[2022-03-01 08:26:38 +0000] [24589] [INFO] Using worker: sync\n\n[2022-03-01 08:26:38 +0000] [24591] [INFO] Booting worker with pid: 24591\n\n[2022-03-01 08:26:38 +0000] [24592] [INFO] Booting worker with pid: 24592\n\n[2022-03-01 08:26:38 +0000] [24593] [INFO] Booting worker with pid: 24593\n\n[2022-03-01 08:26:38 +0000] [24594] [INFO] Booting worker with pid: 24594\n\n[2022-03-01 08:26:40 +0000] [24593] [ERROR] Exception in worker process\n\nTraceback (most recent call last):\n\n\u00a0File \"/databricks/conda/envs/model-3/lib/python3.8/site-packages/gunicorn/arbiter.py\", line 589, in spawn_worker\n\n\u00a0\u00a0worker.init_process()\n\n\u00a0File \"/databricks/conda/envs/model-3/lib/python3.8/site-packages/gunicorn/workers/base.py\", line 134, in init_process\n\n\u00a0\u00a0self.load_wsgi()\n\n\u00a0File \"/databricks/conda/envs/model-3/lib/python3.8/site-packages/gunicorn/workers/base.py\", line 146, in load_wsgi\n\n\u00a0\u00a0self.wsgi = self.app.wsgi()\n\n\u00a0File \"/databricks/conda/envs/model-3/lib/python3.8/site-packages/gunicorn/app/base.py\", line 67, in wsgi\n\n\u00a0\u00a0self.callable = self.load()\n\n\u00a0File \"/databricks/conda/envs/model-3/lib/python3.8/site-packages/gunicorn/app/wsgiapp.py\", line 58, in load\n\n\u00a0\u00a0return self.load_wsgiapp()\n\n\u00a0File \"/databricks/conda/envs/model-3/lib/python3.8/site-packages/gunicorn/app/wsgiapp.py\", line 48, in load_wsgiapp\n\n\u00a0\u00a0return util.import_app(self.app_uri)\n\n\u00a0File \"/databricks/conda/envs/model-3/lib/python3.8/site-packages/gunicorn/util.py\", line 359, in import_app\n\n\u00a0\u00a0mod = importlib.import_module(module)\n\n\u00a0File \"/databricks/conda/envs/model-3/lib/python3.8/importlib/__init__.py\", line 127, in import_module\n\n\u00a0\u00a0return _bootstrap._gcd_import(name[level:], package, level)\n\n\u00a0File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\n\n\u00a0File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\n\n\u00a0File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\n\n\u00a0File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\n\n\u00a0File \"<frozen importlib._bootstrap_external>\", line 783, in exec_module\n\n\u00a0File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n\n\u00a0File \"/databricks/conda/envs/model-3/lib/python3.8/site-packages/mlflow/pyfunc/scoring_server/wsgi.py\", line 6, in <module>\n\n\u00a0\u00a0app = scoring_server.init(load_model(os.environ[scoring_server._SERVER_MODEL_PATH]))\n\n\u00a0File \"/databricks/conda/envs/model-3/lib/python3.8/site-packages/mlflow/pyfunc/__init__.py\", line 710, in load_model\n\n\u00a0\u00a0model_impl = importlib.import_module(conf[MAIN])._load_pyfunc(data_path)\n\n\u00a0File \"/databricks/conda/envs/model-3/lib/python3.8/site-packages/mlflow/pyfunc/model.py\", line 269, in _load_pyfunc\n\n\u00a0\u00a0python_model = cloudpickle.load(f)\n\n\u00a0File \"stringsource\", line 6, in spacy.pipeline.trainable_pipe.__pyx_unpickle_TrainablePipe\n\n_pickle.PickleError: Incompatible checksums (68673003 vs 0x61fbab5 = (cfg, model, name, scorer, vocab))\n\n[2022-03-01 08:26:40 +0000] [24593] [INFO] Worker exiting (pid: 24593)\n\n[2022-03-01 08:26:41 +0000] [24594] [ERROR] Exception in worker process",
        "Answers": [
            {
                "Answer_creation_date": "2022-03-03T14:50:36.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @harish_s (Customer)\u200b\u00a0, Most probable reason can be the version mismatch. Can you please check the version of Spacy?",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-03-07T13:00:29.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @Kaniz Fatma (Databricks)\u200b\u00a0\n\n\u00a0\n\nThanks for the reply, the version of the Spacy installed is 3.1.3.\n\n\u00a0\n\nPlease find the attached details for the same:\n\n\u00a0\n\n\u00a0\n\nI will be very thankful, if I get some help here.\n\nExpand Post",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-05-10T16:52:50.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Which DBR version are you using? also, have you check if the DBR has this library/version?",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-05-18T09:46:02.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @harish_s (Customer)\u200b\u00a0this error could happen if the backend services are not updated. Are you doing this test in a PVC environment or a standard workspace?",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-07-12T12:43:33.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @harish_s (Customer)\u200b\u00a0, We haven't heard from you on the last response from @Prabakar (Databricks)\u200b\u00a0, me, and @Jose Gonzalez (Databricks)\u200b\u00a0, and I was checking back to see if you have a resolution yet. If you have any solution, please share it with the community as it can be helpful to others. Otherwise, we will respond with more details and try to help.\n\n\u00a0\n\nAlso, please don't forget to click on the \"Select As Best\" button whenever the information provided helps resolve your question.\n\nExpand Post",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "\"ML Quickstart: Model Training\" databrick notebook, module not found issue",
        "Question_creation_date": "2022-2-30T8:28Z",
        "Question_tag": [
            "MLFlow",
            "Databricks notebook",
            "Model Training",
            "Damien",
            "Dataricks Notebooks",
            "ML Quickstart"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001okysoCAA/ml-quickstart-model-training-databrick-notebook-module-not-found-issue",
        "Question_upvote_count": 0,
        "Question_answer_count": 3,
        "Question_view_count": 103,
        "Question_has_accepted_answer": true,
        "Question_body": "Hi everybody,\n\n\u00a0\n\nBefore jump in my own ML experiment on databricks ML platform, I take a look inside some databricks notebook.\n\nWith the first ML notebook, named \"ML Quickstart: Model training\" it doesn't find the library mlflow (cf image below).\n\n\u00a0\n\nI assume this is a \"trivial issue\", that could be solved by a command like:\n\n!pip install mlflow\n\nBut, I would like to notice the community about this issue.\n\nDoes somebody encounter the following issue in this notebook ?\n\n\u00a0\n\nThanks for your time.\n\nBest,\n\n\u00a0\n\nDamien",
        "Answers": [
            {
                "Answer_creation_date": "2022-03-30T22:46:12.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "mlflow is an inbuilt library with DBR ML run time such as DBR 9.1 LTS ML. Kindly use cluster with ML runtime.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-03-31T11:56:09.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @DamienSicard (Customer)\u200b\u00a0,\n\n\u00a0\n\nIf you are running Databricks Runtime version 7.1 or above, run this command :\n\npip install mlflow\n\nIf you are running Databricks Runtime version 6.4 to 7.0, run this command :\n\n\u00a0\n\ndbutils.library.installPyPI(\"mlflow\")\n\n\u00a0If you are using a cluster running Databricks Runtime, you must install MLflow.\n\n\u00a0\n\nSee \"Install a library on a cluster\" (AWS|Azure|GCP).\n\n\u00a0\n\nSelect\u00a0Library Source\u00a0PyPI and enter\u00a0mlflow\u00a0in the\u00a0Package\u00a0field.\n\n\u00a0\n\nIf you are using a cluster running Databricks Runtime ML, MLflow is already installed.\n\n\u00a0\n\nExpand Post",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-07-12T11:43:37.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @DamienSicard (Customer)\u200b, We haven't heard from you on the last response from @Shan_Chandra (Databricks)\u200b\u00a0\u200band me, and I was checking back to see if his suggestions helped you. Or else, If you have any solution, please share it with the community as it can be helpful to others.\n\n\u00a0\n\nAlso, please don't forget to click on the \"Select As Best\" button whenever the information provided helps resolve your question.",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "Runtime error using MLFlow and Spark on databricks",
        "Question_creation_date": "2022-6-7T15:49Z",
        "Question_tag": [
            "Error",
            "Pyspark dataframe",
            "Pyspark",
            "Python",
            "Spark",
            "MLFlow",
            "Error Message",
            "Mlflow Model"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D58Y00008vCA2ESAW/runtime-error-using-mlflow-and-spark-on-databricks",
        "Question_upvote_count": 1,
        "Question_answer_count": 0,
        "Question_view_count": 337,
        "Question_has_accepted_answer": false,
        "Question_body": "Here is some model I created:\n\nclass SomeModel(mlflow.pyfunc.PythonModel):\n    def predict(self, context, input):\n        # do fancy ML stuff\n        # log results\n        pandas_df = pd.DataFrame(...insert predictions here...)\n        spark_df = spark.createDataFrame(pandas_df)\n        spark_df.write.saveAsTable('tablename', mode='append')\n\n\u00a0\n\nI'm trying to log my model in this manner by calling it later in my code:\n\nwith mlflow.start_run(run_name=\"SomeModel_run\"):\n    model = SomeModel()\n    mlflow.pyfunc.log_model(\"somemodel\", python_model=model)\n\n\u00a0\n\nUnfortunately it gives me this Error Message:\n\nRuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.\n\n\u00a0\n\nThe error is caused because of the line\n\nmlflow.pyfunc.log_model(\"somemodel\", python_model=model)\n\n\u00a0\n\nIf I comment it out my model will make its predictions and log the results in my table.\n\nAlternatively, removing the lines in my predict function where I call spark to create a dataframe and save the table, I am able to log my model.\n\n\u00a0\n\nHow do I go about resolving this issue? I need my model to not only write to the table but also be logged",
        "Answers": []
    },
    {
        "Question_title": "MLFlow error",
        "Question_creation_date": "2021-7-11T23:40Z",
        "Question_tag": [
            "MLFlow"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001HKIENCA5/mlflow-error",
        "Question_upvote_count": 3,
        "Question_answer_count": 9,
        "Question_view_count": 507,
        "Question_has_accepted_answer": true,
        "Question_body": "I am running into an error within the Databricks notebook (on Databricks website) environment where MLFlow will not load:\n\nMLflow autologging encountered a warning: \"/databricks/python/lib/python3.8/site-packages/mlflow/utils/autologging_utils/safety.py:216: UserWarning: Logging to MLflow failed: You haven't configured the CLI yet! Please configure by entering\n/databricks/python_shell/scripts/PythonShell.py configure\n\n\u00a0\n\nI then try to run the command in a cell (and in the web terminal) :\n\n!/databricks/python_shell/scripts/PythonShell.py configure\n\nAnd get the following error output:\n\n/databricks/python_shell/scripts/PythonShell.py: line 22: syntax error near unexpected token `'AGG''\n\nI have tried editing this file from the Web Terminal with vi but nothing seems to work.",
        "Answers": [
            {
                "Answer_creation_date": "2021-09-01T17:43:47.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "How are you running this? This generally means you're not running a Databricks ML runtime, which should be all set up with auth, etc",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-12-01T14:27:14.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi,\n\n\u00a0\n\nI am having the same issue when trying to run an MLFlor experiment with :\n\nmlflow run git-uri -b databricks --backend-config cluster-spec.json experiment-name mlflow_expetiment\n\nThe error is caused by:\n\n\u00a0\n\nmlflow.set_tracking_uri('databricks')\n\nThe error is the following:\n\n\u00a0\n\ndatabricks_cli.utils.InvalidConfigurationError: You haven't configured the CLI yet! Please configure by entering `/opt/project/mlflow_test.py configure`\n\nThe databricks-cli is configured properly and works fine outside the script.\n\nExpand Post",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-12-01T15:03:47.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Have you tried running that script it indicates? something is not quite connected here",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-12-01T15:17:12.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi,\n\nYes\u200b I have tried but I get the error that the script doesn't have a \"configure\" argument. Looking deeper into the code that reises the error, I see that this is a string automatically generated by inserting the name of the running script when the Databricks credentials are not properly fetch within python. But I don't know how I can pass this credentials that I have already pass to the databricks-cli (token, host, etc)",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-01-27T09:39:53.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "I get the error when I run `!pip install mlflow` instead of `%pip install mlflow`, which I assume installs the databricks-maintained version",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-01-27T13:03:53.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "There aren't different versions of mlflow, but without %pip install you are only installing on the driver machine. You do need %pip to even get it on the workers, which could be the issue.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-07-05T13:40:08.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "step1 - !pip install mlfow , step2- mlflow.set_tracking_uri('databricks'), step3- restart the cluster",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-07-06T14:39:51.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Do not use !pip. This installs on the driver only. Use %pip. However, you do not need to install MLflow. Use the ML runtime. Otherwise you also have to configure MLflow.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-07-07T10:10:32.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Install the Databricks SQL Connector for Python library on your development machine by running pip install databricks-sql-connector .\n\nQuery data.\nInsert data.\nQuery metadata.\nCursor and connection management.\nConfigure logging.\n\n\u00a0\n\nRegards,\n\nWilljoe\n\nExpand Post",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "Different configurations for same Databricks Runtime version",
        "Question_creation_date": "2022-3-3T11:05Z",
        "Question_tag": [
            "Different Configurations",
            "DBR",
            "Databricks Runtime"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001piRrOCAU/different-configurations-for-same-databricks-runtime-version",
        "Question_upvote_count": 1,
        "Question_answer_count": 3,
        "Question_view_count": 274,
        "Question_has_accepted_answer": true,
        "Question_body": "Hi all,\n\nOn my DBR installations, s3a scheme is mapped to shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem. On my customer's DBR installations it is mapped to com.databricks.s3a.S3AFileSystem.\n\n\u00a0\n\nWe both use the same DBR runtime, and none of us has configured anything to override this setting.\n\n\u00a0\n\nWhat is the cause for this difference? And how can I make sure I'm using the right filesystem? How can I make sure in the future no third file system appears and breaks my code again?",
        "Answers": [
            {
                "Answer_creation_date": "2022-04-04T09:32:02.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "@yoniau (Customer)\u200b\u00a0, Databricks Runtime 7.3 LTS and above use the new connector com.databricks.s3a.S3AFileSystem. Are you using 7.3?\n\n\u00a0\n\nAnyway, please verify spark config on both installations (via Cluster -> Spark UI -> Environment) what is there regarding S3AFileSystem? and then set common values for both (via Cluster -> Configuration -> Advanced options)",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-04-05T07:06:24.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "@yoniau (Customer)\u200b\u00a0, If both of you are using the same DBR version, then you should not find any difference. As @Hubert Dudek (Customer)\u200b\u00a0mentioned, there might be some spark configuration change made on one of the clusters. Also, it's worth checking for any cluster scope or global init script.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-04-26T16:11:35.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @yoniau (Customer)\u200b\u00a0, Just a friendly follow-up. Do you still need help, or do @Hubert Dudek (Customer)\u200b and @Prabakar (Databricks)\u200b\u00a0's responses help you find the solution? Please let us know.",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "Job fails after runtime upgrade",
        "Question_creation_date": "2021-8-30T10:54Z",
        "Question_tag": [
            "Databricks Runtime",
            "Udf",
            "Job",
            "Runtime",
            "Job clusters",
            "Clusters",
            "Snowflake"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001Mga6sCAB/job-fails-after-runtime-upgrade",
        "Question_upvote_count": 0,
        "Question_answer_count": 8,
        "Question_view_count": 188,
        "Question_has_accepted_answer": true,
        "Question_body": "I have a job running with no issues in Databricks runtime 7.3 LTS. When I upgraded to 8.3 it fails with error An exception was thrown from a UDF: 'pyspark.serializers.SerializationError'... SparkContext should only be created and accessed on the driver\n\n\u00a0\n\nIn the notebook I use applyInPandas to apply a UDF to each group. In this UDF I pull data from Snowflake making use of the spark session (spark.read.format(...)) and I understand that is the reason why it fails.\n\n\u00a0\n\nMy question is, why was it working in 7.3 LTS and it's not working now? What changed?\n\n\u00a0\n\nThanks,",
        "Answers": [
            {
                "Answer_creation_date": "2021-09-30T15:36:02.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @\u00a0NicolasEscobar! My name is Kaniz, and I'm the technical moderator here. Great to meet you, and thanks for your question! Let's see if your peers in the community have an answer to your question first. Or else I will follow up with my team and get back to you soon. Thanks.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-10-05T17:38:45.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "@NicolasEscobar (Customer)\u200b\u00a0- could you please share the full error stack trace ?",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-10-06T08:18:00.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "DBR-8.3 uses SPARK with version 3.1.x. As per migration guide by default it is restricted to use SparkContext inside the executor. You can enable it by using spark.executor.allowSparkContext\n\n\u00a0\n\nIn Spark 3.0 and below, SparkContext can be created in executors. Since Spark 3.1, an exception will be thrown when creating SparkContext in executors. You can allow it by setting the configuration spark.executor.allowSparkContext when creating SparkContext in executors.\n\nExpand Post",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-10-10T16:24:36.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "To clarify a bit more - in Spark, you can never use a SparkContext or SparkSession within a task / UDF. This has always been true. If it worked before, it's because you were accidentally sending the SparkContext because it was captured in your code, but I guess you never tried to use it. It would have failed. Now it just fails earlier.\n\n\u00a0\n\nThe real solution is to change your code to not accidentally hold on to the SparkContext or SparkSession in your UDF.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-03-07T11:59:35.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Thanks Sean for your answer, it's clear.\n\nI was just wondering why the code was executing before with no errors and with the expected output but now I understand that this is because there was no restriction before and this changed after the release of Spark 3.1, as Sandeep mentioned.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-07-06T14:34:56.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @sean.owen (Databricks)\u200b\u00a0Thanks for highlighting this. Could you please provide some sample code when you mention \"not accidentally hold on to the SparkContext or SparkSession in your UDF\". Thanks",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-07-06T14:39:18.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "There are 1000 ways this could happen, so not really, but they're all the same idea: you can't reference the SparkContext or SparkSession object, directly or indirectly in a UDF. Simply, you cannot use it in the UDF code.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-03-01T11:33:43.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Adding to @sean.owen (Databricks)\u200b\u00a0 comments, The only reason this is working is that the optimizer is evaluating this locally rather than creating a context on executors and evaluating it.",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "MLflow",
        "Question_creation_date": "2022-5-28T22:40Z",
        "Question_tag": [
            "Summit22",
            "MLFlow"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D58Y00008tI5W8SAK/mlflow",
        "Question_upvote_count": 1,
        "Question_answer_count": 2,
        "Question_view_count": 89,
        "Question_has_accepted_answer": true,
        "Question_body": "What kind of features I can use with MLflow?",
        "Answers": [
            {
                "Answer_creation_date": "2022-06-30T11:02:15.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @Sarang2610 (Customer)\u200b\u00a0, Managed MLflow is built on top of\u00a0MLflow, an open source platform developed by Databricks to help manage the complete machine learning lifecycle with enterprise reliability, security, and scale. Please read this article.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-07-05T06:32:50.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "https://docs.databricks.com/applications/mlflow/index.htmlwww.MyAARPMedicare.com",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "How does ML Flow work with Databricks monitoring product recently released ?",
        "Question_creation_date": "2022-5-28T19:29Z",
        "Question_tag": [
            "Model Lifecycle"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D58Y00008sfqxYSAQ/how-does-ml-flow-work-with-databricks-monitoring-product-recently-released-",
        "Question_upvote_count": 0,
        "Question_answer_count": 2,
        "Question_view_count": 45,
        "Question_has_accepted_answer": true,
        "Question_body": "Does the monitoring tool interact with MLFlow and Feature Store too ?",
        "Answers": [
            {
                "Answer_creation_date": "2022-06-28T21:02:28.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "As far as I am aware there is no interaction between Mlflow and I assume you mean Unity Catalog. That said you would be able to push metadata from one to another. The power will come from tracking lineage across models and trained data.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-07-01T10:52:47.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "@Vpax (Customer)\u200b\u00a0what \"monitoring tool\" are you referring to?",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "Errors in notebooks of Scalable Machine Learning with Apache Spark course in Databricks academy.",
        "Question_creation_date": "2022-5-21T8:42Z",
        "Question_tag": [
            "Databricks Runtime",
            "Apache spark",
            "Hyperopt",
            "Hyperparameter Tuning",
            "Databricks Academy",
            "Scalable Machine Learning"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D58Y00008rC7OLSA0/errors-in-notebooks-of-scalable-machine-learning-with-apache-spark-course-in-databricks-academy",
        "Question_upvote_count": 1,
        "Question_answer_count": 2,
        "Question_view_count": 180,
        "Question_has_accepted_answer": false,
        "Question_body": "HI there,\n\nI'm following the course mentioned from Databricks Academy. I downloaded the .dbc archiive and working along side the videos from academy.\n\n\u00a0\n\nIn ML-08 - Hyperopt notebook, I see the following error in cmd 13.\n\n  best_hyperparam = fmin(fn=objective_function, \n                         space=search_space,\n                         algo=tpe.suggest, \n                         max_evals=num_evals,\n                         trials=trials,\n#this is where I see the error, I commented the randomstate to bypass the error.\n                         rstate=np.random.RandomState(42) \n                        )\n\nThe below is the error i've encountered:\n\nAttributeError: 'numpy.random.mtrand.RandomState' object has no attribute 'integers'\n\nDetailed Error:\n\nAttributeError                            Traceback (most recent call last)\n<command-1903711140984667> in <module>\n      6   num_evals = 4\n      7   trials = Trials()\n----> 8   best_hyperparam = fmin(fn=objective_function, \n      9                          space=search_space,\n     10                          algo=tpe.suggest,\n\u00a0\n/databricks/.python_edge_libs/hyperopt/fmin.py in fmin(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\n    563 \n    564     if allow_trials_fmin and hasattr(trials, \"fmin\"):\n--> 565         return trials.fmin(\n    566             fn,\n    567             space,\n\u00a0\n/databricks/.python_edge_libs/hyperopt/base.py in fmin(self, fn, space, algo, max_evals, timeout, loss_threshold, max_queue_len, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar, early_stop_fn, trials_save_file)\n    669         from .fmin import fmin\n    670 \n--> 671         return fmin(\n    672             fn,\n    673             space,\n\u00a0\n/databricks/.python_edge_libs/hyperopt/fmin.py in fmin(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\n    609 \n    610     # next line is where the fmin is actually executed\n--> 611     rval.exhaust()\n    612 \n    613     if return_argmin:\n\u00a0\n/databricks/.python_edge_libs/hyperopt/fmin.py in exhaust(self)\n    387     def exhaust(self):\n    388         n_done = len(self.trials)\n--> 389         self.run(self.max_evals - n_done, block_until_done=self.asynchronous)\n    390         self.trials.refresh()\n    391         return self\n\u00a0\n/databricks/.python_edge_libs/hyperopt/fmin.py in run(self, N, block_until_done)\n    297                     # processes orchestration\n    298                     new_trials = algo(\n--> 299                         new_ids, self.domain, trials, self.rstate.integers(2 ** 31 - 1)\n    300                     )\n    301                     assert len(new_ids) >= len(new_trials)\n\u00a0\nAttributeError: 'numpy.random.mtrand.RandomState' object has no attribute 'integers'\n\nAnother error is in ML08L-Hyperopt Lab, where I'm unable to pass I'm unable to pass \"max_features\" from best_hyperparam to the regressor. It fails while I'm fitting the model to the X_train & y_train.\n\nBelow is the snip:\n\nThe error statement reads:\n\nValueError: max_features must be in (0, n_features]",
        "Answers": [
            {
                "Answer_creation_date": "2022-06-22T12:51:15.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Tagging @Kaniz Fatma (Databricks)\u200b\u00a0as there was no response what so ever!\n\nBy any chance, do you know how to resolve these errors in the notebook?\n\nThanks!",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-06-22T12:54:13.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @RiyazAli (Customer)\u200b, Thank you for reaching out!\n\nLet us look into this for you, and we'll circle back with an update.",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "Databricks -->Workflows-->Job Runs",
        "Question_creation_date": "2022-4-20T5:06Z",
        "Question_tag": [
            "Databricks Runtime",
            "Generic Account",
            "Azure databricks",
            "Runs"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D58Y00008jwzrFSAQ/databricks-workflowsjob-runs",
        "Question_upvote_count": 0,
        "Question_answer_count": 3,
        "Question_view_count": 245,
        "Question_has_accepted_answer": true,
        "Question_body": "In Databricks -->Workflows-->Job Runs we have a column \"Run As\".\n\nFrom where does this value come. We are getting a user id here but need to change it to a generic account. Any help would be appreciated. Thanks",
        "Answers": [
            {
                "Answer_creation_date": "2022-05-24T22:20:34.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "@sreedata (Customer)\u200b\u00a0 When a user creates a job, that particular user email will be added to Run As. When the job runs, it will use this user's permissions. If you want to change it you can change it by transferring the job ownership.\n\n\u00a0\n\nPlease refer to the doc for changing the job permission.\n\n\u00a0\n\nhttps://docs.databricks.com/security/access-control/jobs-acl.html#jobs-access-control",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-05-27T08:27:28.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "@sreedata (Customer)\u200b\u00a0we are working on the capability to specify a generic account aka Service Principle when running a Job. Stay tuned!",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-06-16T11:49:32.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "I appreciate the information and advice you have shared.\n\nDNAHRBlock.com",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "How to get the Job ID and Run ID and save into a database",
        "Question_creation_date": "2022-5-15T24:23Z",
        "Question_tag": [
            "Job Parameters",
            "Databricks Job",
            "Jar",
            "Databricks Runtime",
            "Scala",
            "Job",
            "Run",
            "Task Parameters"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D58Y00008pbkj9SAA/how-to-get-the-job-id-and-run-id-and-save-into-a-database",
        "Question_upvote_count": 0,
        "Question_answer_count": 1,
        "Question_view_count": 167,
        "Question_has_accepted_answer": true,
        "Question_body": "We are having Databricks Job running with main class and JAR file in it. Our JAR file code base is in Scala. Now, when our job starts running, we need to log Job ID and Run ID into the database for future purpose. How can we achieve this?",
        "Answers": [
            {
                "Answer_creation_date": "2022-06-15T12:25:46.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "You can pass {job_id}} and {{run_id}} in Job arguments\u00a0and print that information and save into wherever its needed\n\nplease find below the documentation for the same\n\nhttps://docs.databricks.com/data-engineering/jobs/jobs.html#task-parameter-variables\n\n\u00a0\n\n{{job_id}}\n\nThe unique identifier assigned to a job\n\n1276862\n\n{{run_id}}\n\nThe unique identifier assigned to a job run\n\n3447843\n\n\u00a0\n\n{{parent_run_id}}\n\nThe unique identifier assigned to the run of a job with multiple tasks.\n\n3447835\n\n{{task_key}}\n\nThe unique name assigned to a task that\u2019s part of a job with multiple tasks.\n\nExpand Post",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "How to isolate environments for different projects in a single mlflow server?",
        "Question_creation_date": "2022-3-30T17:17Z",
        "Question_tag": [
            "MLFlow",
            "Different Users",
            "MLFlow Tracking Server",
            "Mlflow Server",
            "Security",
            "MLFlow Servers",
            "Tracking Server"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001u8ot4CAA/how-to-isolate-environments-for-different-projects-in-a-single-mlflow-server",
        "Question_upvote_count": 1,
        "Question_answer_count": 4,
        "Question_view_count": 236,
        "Question_has_accepted_answer": false,
        "Question_body": "I am planning to deploy MLFlow server deployed in AWS ECS as a centralised repositories for my machine learning experiments and runs and to strore events and artifacts. I would like to use MLflow Tracking Server enabled with proxied artifact storage access to eliminate the need to have authentication to the underlying storage(S3 bucket) at my side(client). And I would like to have different environments or isolated environments as i keep creating projects? for example, for project titanic, i might have 10 experiments, which i would not want to see from project iris and this project will have different experiments. How do I achieve this?\n\n\u00a0\n\nThanks.",
        "Answers": [
            {
                "Answer_creation_date": "2022-05-01T23:12:27.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "You would create a new experiment for each dataset yo just change the name. https://www.mlflow.org/docs/latest/python_api/mlflow.html#mlflow.create_experiment\n\n\u00a0\n\nFor a new environment,\n\nhttps://mlflow.org/docs/latest/cli.html#cmdoption-mlflow-models-predict-env-manager\n\n\u00a0\n\nThere is a slackworkspace that is very helpful too:\n\nhttps://join.slack.com/t/mlflow-users/shared_invite/zt-g6qwro5u-odM7pRnZxNX_w56mcsHp8g\n\nExpand Post",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-05-02T04:46:49.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "I understand an experiment in the context of mlflow. i will be creating multiple experiments for a single project. and i want to not see those experiments when i start a new project but not delete them. may be some switch to show and let me usa a different page for experiments and runs, while using the same server.\n\n\u00a0\n\nby environment' i didn't mean a python or conda virtual environment. i shouldn't have used the word 'environment'. i meant to convey different (or may be a new) page or space for experiments for every new project i start, while sticking to the same mlflow server deployed in the same place(this is going to be AWS ECS)\n\nExpand Post",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-05-09T07:38:12.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @naveen_marthala (Customer)\u200b, Just a friendly follow-up. Do you still need help, or @josephk (Databricks)\u200b\u00a0's response help you to find the solution? Please let us know.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-06-15T06:52:45.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "yes i still need help. @josephk\u2019s reply hasn\u2019t helped me. I still need to know how I can isolate or have different sets of experiments in order to use a single server for different projects.",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "Pushing SparkNLP Model on Mlflow",
        "Question_creation_date": "2022-5-13T10:46Z",
        "Question_tag": [
            "Details",
            "Mlflow registry",
            "Sparknlp",
            "MLFlow",
            "SparkNLP Model"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D58Y00008ov3RdSAI/pushing-sparknlp-model-on-mlflow",
        "Question_upvote_count": 0,
        "Question_answer_count": 0,
        "Question_view_count": 136,
        "Question_has_accepted_answer": false,
        "Question_body": "Hello Everyone,\n\nI am trying to load a SparkNLP (link for more details about the model if required) from Mlflow Registry.\n\nTo this end, I have followed one tutorial and implemented below codes:\n\nimport mlflow.pyfunc\n\u00a0\nclass LangDetectionModel(mlflow.pyfunc.PythonModel):\n    def __init__(self):\n      super().__init__()\n      from sparknlp.pretrained import PretrainedPipeline\n      from sparknlp.pretrained import PipelineModel \n      # embed the sparknlp model \n      self._model  = PipelineModel.load(\"/mnt/sparknlp_models/detect_language_375/\")\n    def predict(self, eval_data_lang_detect):\n    # Apply the transform function for lang detetction\n      list_columns = eval_data_lang_detect.columns \n      model_output =self._model.transform(eval_data_lang_detect).select(list_columns+ [F.col(\"language.result\").getItem(0)]).withColumnRenamed('language.result[0]','sparknlp_column')\n      return model_output\nmodel_path = \"my-langdetect-model\"\nreg_model_name = \"NlpieLangDetection\"\nsparknlp_model = LangDetectionModel()\n# Log MLflow entities and save the model\nmlflow.set_tracking_uri(\"sqlite:///mlruns.db\")\n\u00a0\n# Save the conda environment for this model.\nconda_env = {\n    'channels': ['defaults', 'conda-forge'],\n    'dependencies': [\n        'python={}'.format(PYTHON_VERSION),\n        'pip'],\n    'pip': [\n        'mlflow',\n        'cloudpickle=={}'.format(cloudpickle.__version__),\n        'NlpieLangDetection==0.0.1'\n    ],\n    'name': 'mlflow-env'\n}\n# Save the model\nmlflow.set_experiment('/Users/Youssef.Meguebli@sanofi.com/Language_Detection_Translation/LangDetectionTest')\nwith mlflow.start_run(run_name=\"Nlpie Language Detection\") as run:\n    model_path = f\"{model_path}-{run.info.run_uuid}\"\n    mlflow.log_param(\"algorithm\", \"SparNLPLangDetection\")\n    mlflow.pyfunc.save_model(path=model_path, python_model=sparknlp_model, conda_env=conda_env)\n\nI am getting an error on last piece of code where I am trying to save the model on Mlflow registry.\n\n\u00a0\n\nBelow the error get I am getting:\n\n\u00a0\n\nTypeError: cannot pickle '_thread.RLock' object\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<command-2121909764500367> in <module>\n      4     model_path = f\"{model_path}-{run.info.run_uuid}\"\n      5     mlflow.log_param(\"algorithm\", \"SparNLPLangDetection\")\n----> 6     mlflow.pyfunc.save_model(path=model_path, python_model=sparknlp_model, conda_env=conda_env)\n\u00a0\n/databricks/python/lib/python3.8/site-packages/mlflow/pyfunc/__init__.py in save_model(path, loader_module, data_path, code_path, conda_env, mlflow_model, python_model, artifacts, signature, input_example, pip_requirements, extra_pip_requirements, **kwargs)\n   1467         )\n   1468     elif second_argument_set_specified:\n-> 1469         return mlflow.pyfunc.model._save_model_with_class_artifacts_params(\n   1470             path=path,\n   1471             python_model=python_model,\n\u00a0\n/databricks/python/lib/python3.8/site-packages/mlflow/pyfunc/model.py in _save_model_with_class_artifacts_params(path, python_model, artifacts, conda_env, code_paths, mlflow_model, pip_requirements, extra_pip_requirements)\n    162         saved_python_model_subpath = \"python_model.pkl\"\n    163         with open(os.path.join(path, saved_python_model_subpath), \"wb\") as out:\n--> 164             cloudpickle.dump(python_model, out)\n    165         custom_model_config_kwargs[CONFIG_KEY_PYTHON_MODEL] = saved_python_model_subpath\n    166     else:\n\u00a0\n/databricks/python/lib/python3.8/site-packages/cloudpickle/cloudpickle_fast.py in dump(obj, file, protocol, buffer_callback)\n     53         compatibility with older versions of Python.\n     54         \"\"\"\n---> 55         CloudPickler(\n     56             file, protocol=protocol, buffer_callback=buffer_callback\n     57         ).dump(obj)\n\u00a0\n/databricks/python/lib/python3.8/site-packages/cloudpickle/cloudpickle_fast.py in dump(self, obj)\n    631     def dump(self, obj):\n    632         try:\n--> 633             return Pickler.dump(self, obj)\n    634         except RuntimeError as e:\n    635             if \"recursion\" in e.args[0]:\n\u00a0\nTypeError: cannot pickle '_thread.RLock' object\n\nPlease let me know if you need any further details.\n\n\u00a0\n\nMany Thanks in advance for your support.",
        "Answers": []
    },
    {
        "Question_title": "What is the difference between Databricks SQL vs Databricks cluster with Photon runtime?",
        "Question_creation_date": "2022-4-6T10:25Z",
        "Question_tag": [
            "Databricks SQL",
            "Photon Runtime",
            "Databricks cluster",
            "Databricks Runtime",
            "Cluster"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001utcazCAA/what-is-the-difference-between-databricks-sql-vs-databricks-cluster-with-photon-runtime",
        "Question_upvote_count": 2,
        "Question_answer_count": 6,
        "Question_view_count": 255,
        "Question_has_accepted_answer": true,
        "Question_body": "",
        "Answers": [
            {
                "Answer_creation_date": "2022-05-06T11:40:40.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "They are very similar. Databricks SQL uses compute that has photon enabled. A traditional cluster with photon enabled does allow for a few more configurations to be set around the cluster architecture and settings. The traditional cluster will also have more libraries installed as it needs to run things in various languages, where the endpoints only needs SQL APIs.\n\n\u00a0\n\nhttps://docs.databricks.com/runtime/photon.html#limitations. This lists some limitations, although additional data source reads is in preview now.\n\nExpand Post",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-05-06T13:08:29.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Thank you. Will traditional cluster support serverless execution in the future or only SQL endpoints support that?\n\nAnd are there any optimization tweaks in Databricks SQL that makes it perhaps faster than traditional Databricks cluster running only SQL queries?",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-05-06T15:06:02.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Serverless for traditional compute is in preview for single node machines and multinode cluster serverless is on the roadmap.\n\nI'm sure there are a few optimizations that makes things faster. Simple things such as caching metadata in the metastore helps.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-05-06T15:21:10.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "I wouldn't call them the same as Databricks SQL runtime is a bit different (not everything is supported for example UDFs), new releases are separated from standard runtimes updates: https://docs.databricks.com/sql/release-notes/index.html\n\n\u00a0\n\nDatabricks cluster can handle notebooks. SQL endpoint is only for SQL queries.\n\n\u00a0\n\nBoth can be in photon or non-photon versions. Photon has a bunch of improvements for example better handle small files problem.\n\nExpand Post",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-05-13T06:20:15.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Great question! There are similarities and differences:\n\n\u00a0\n\nSimilarities\n\nPhoton is enabled on both\nYou have Databricks Runtime on both\n\n\u00a0\n\nDifferences\n\nDatabricks Runtime (DBR) version is managed and auto-upgraded in Databricks SQL. Because SQL is a narrower workload than, say, data science, we automatically manage the version of DBR that runs on Databricks SQL Endpoints. This is a good thing - you don't have to worry about upgrading etc.\nDBR behaves slightly differently on SQL Endpoints compared to Clusters. This is again a good thing. Mostly we optimize for the SQL workload and set configs automatically so you don't have to.\nSQL Endpoints are actually behind a scalable gateway proxy. This proxy can, among other things, scale out the clusters as your SQL workload scales up or down. This brings elasticity to your workloads. A bunch of stuff like caching and metadata processing go here, too, to speed things up.\n\n\u00a0\n\nTL;DR if you are doing SQL/BI, please consider using SQL Endpoints, it's generally the best choice for that workload.\n\nExpand Post",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-05-13T10:56:46.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @jwilliam (Customer)\u200b, Just a friendly follow-up. Do you still need help or the above responses help you to find the solution? Please let us know.",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "Is Databricks Light Runtime Discontinued?",
        "Question_creation_date": "2022-4-26T9:49Z",
        "Question_tag": [
            "Databricks Runtime",
            "DBR",
            "Databricks Light"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D58Y00008l9WWNSA2/is-databricks-light-runtime-discontinued",
        "Question_upvote_count": 0,
        "Question_answer_count": 2,
        "Question_view_count": 65,
        "Question_has_accepted_answer": true,
        "Question_body": "The last Databricks Light runtime release was 2.4 Extended Support. There was no Light version for Spark 3.x. Is Databricks Light runtime discontinued? If not, when we can expect the next DBR Light version?",
        "Answers": [
            {
                "Answer_creation_date": "2022-05-26T11:12:04.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @venkad (Customer)\u200b,\n\n\u00a0\n\nDatabricks Light 2.4 Extended Support\u00a0will be supported through April 30, 2023.\n\n\u00a0\n\nIt uses Ubuntu 18.04.5 LTS instead of the deprecated 16.04.6 LTS distribution used in the original Databricks Light 2.4.\n\n\u00a0\n\nUbuntu 16.04.6 LTS support ceased on April 1, 2021.\n\n\u00a0\n\nSupport for Databricks Light 2.4 ended on September 5, 2021, and Databricks recommends that you migrate your Light workloads to the extended support version as soon as possible.\n\n\u00a0\n\nThis article lists all Databricks runtime releases and the schedule for supported releases. For more information about the Databricks Runtime support policy and schedule, see\u00a0Databricks runtime support lifecycle.\n\n\u00a0\n\nExpand Post",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-05-26T11:43:23.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @venkad (Customer)\u200b, I looked around, and it does look like there won't be future light runtimes. We can't hire enough engineers to maintain and develop everything, and light is one of the casualties of that.",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "MLFlow failed: You haven't configured the CLI yet",
        "Question_creation_date": "2022-4-16T7:55Z",
        "Question_tag": [
            "Cli",
            "MLFlow",
            "Databricks run time version",
            "Cluster"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D58Y00008io2FKSAY/mlflow-failed-you-havent-configured-the-cli-yet",
        "Question_upvote_count": 0,
        "Question_answer_count": 2,
        "Question_view_count": 912,
        "Question_has_accepted_answer": true,
        "Question_body": "I'm getting an error\n\nYou haven\u2019t configured the CLI yet! Please configure by entering `/databricks/python_shell/scripts/db_ipykernel_launcher.py configure`\n\nMy cluster is running Databricks Runtime Version 10.1\n\nI've also installed mlflow to the cluster libraries, I've tried just using the runtime without installing but couldn't import mlflow\n\nI found this question , but the solution there didn't help",
        "Answers": [
            {
                "Answer_creation_date": "2022-05-18T12:18:31.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "pandrama com",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-05-24T21:50:17.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "@Yoni (Customer)\u200b\u00a0This issue can occur when we are running the code with non-ML cluster, though you install ML libraries. Using ML cluster will resolve this.",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "Installing new libraries on Azure Databricks Clusters",
        "Question_creation_date": "2021-9-13T21:28Z",
        "Question_tag": [
            "Python Libraries",
            "Python",
            "Databricks Runtime",
            "New Libraries",
            "Azure databricks",
            "Azure",
            "Clusters"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001P3Fm9CAF/installing-new-libraries-on-azure-databricks-clusters",
        "Question_upvote_count": 0,
        "Question_answer_count": 4,
        "Question_view_count": 661,
        "Question_has_accepted_answer": true,
        "Question_body": "Hi Everyone,\n\n\u00a0\n\nI was trying to install the newest python version on the Databricks Clusters and it has the runtime version 7.3 LTS, but no matter how many times I try it keeps installing the 3.7.5 version of python.\n\n\u00a0\n\nI know that Runtime version 7.3 LTS comes with Python 3.7.5 but I was wondering if I could install a newer version of Python? Is it possible to do that?",
        "Answers": [
            {
                "Answer_creation_date": "2021-10-14T08:31:14.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "I've done this before using a custom docker image, but even then the runtime itself continues to use the version of python 3 which is installed as part of the OS. The easiest way to get to a newer version is to use a newer runtime. If you're sticking with LTS releases then you could jump to the 9.1 LTS release which uses Python 3.8.8",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-10-14T12:27:40.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Alright, thanks!",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-11-28T22:11:28.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @Nuthan_1994 (Customer)\u200b\u00a0, I could see the question was answered and if you find the answer to be helpful and resolved your query, would you be happy to mark it as best so others can quickly find the solution in the future.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-05-24T11:23:40.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @Nuthan_1994 (Customer)\u200b\u00a0,\n\n\u00a0\n\nNot supported with Databricks Runtime 7 and above at the moment\n\n\u00a0\n\nhttps://docs.databricks.com/libraries/cluster-libraries.html#library",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "MlFlow and Feature Store: mlflow.spark.autolog, using feature store on Databricks, FeatureStoreClient.log_model()?",
        "Question_creation_date": "2021-11-31T17:32Z",
        "Question_tag": [
            "Databricks Machine Learning Workspace",
            "Spark Means",
            "Feature Store",
            "Feature",
            "MLFlow",
            "Difference"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001aoGbkCAE/mlflow-and-feature-store-mlflowsparkautolog-using-feature-store-on-databricks-featurestoreclientlogmodel",
        "Question_upvote_count": 0,
        "Question_answer_count": 3,
        "Question_view_count": 241,
        "Question_has_accepted_answer": true,
        "Question_body": "As I am moving my first steps within the Databricks Machine Learning Workspace, I am getting confused by some features that by \"documentation\" seem to overlap.\n\n\u00a0\n\nDoes autolog for spark on mlflow provide different tracking than using a training set created via a feature store client? Also, how does FeatureStoreClient.log_model() relate with MLFlow?",
        "Answers": [
            {
                "Answer_creation_date": "2021-12-31T18:52:34.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @\u00a0Edmondo! My name is Kaniz, and I'm the technical moderator here. Great to meet you, and thanks for your question! Let's see if your peers in the community have an answer to your question first. Or else I will get back to you soon. Thanks.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-01-21T05:52:06.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @Edmondo (Customer)\u200b\u00a0,\n\n\u00a0\n\nAre you referring to this documentation here?\n\nhttps://www.mlflow.org/docs/latest/python_api/mlflow.spark.html",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-01-21T05:55:12.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @Edmondo (Customer)\u200b\u00a0, FeatureStoreClient.log_model()\u00a0logs an MLflow model packaged with feature lookup information.\n\n\u00a0\n\nSource\n\n\u00a0\n\nmlflow.spark.autolog(disable=False,\u00a0silent=False) enables (or disables) and configures logging of Spark data source paths, versions (if applicable), and formats when they are read.\n\n\u00a0\n\nSource\n\n\u00a0\n\nExpand Post",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "ModuleNotFoundError: No module named 'mlflow' when running a notebook",
        "Question_creation_date": "2022-0-16T15:20Z",
        "Question_tag": [
            "MLFlow",
            "Modulenotfounderror",
            "Databricks notebook",
            "Module",
            "Coursera Platform",
            "Run",
            "Mflow"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001curAwCAI/modulenotfounderror-no-module-named-mlflow-when-running-a-notebook",
        "Question_upvote_count": 0,
        "Question_answer_count": 6,
        "Question_view_count": 483,
        "Question_has_accepted_answer": true,
        "Question_body": "I am running a notebook on the Coursera platform.\n\n\u00a0\n\nmy configuration file, Classroom-Setup, looks like this:\n\n%python\n\u00a0\nspark.conf.set(\"com.databricks.training.module-name\", \"deep-learning\")\nspark.conf.set(\"com.databricks.training.expected-dbr\", \"6.4\")\n\u00a0\nspark.conf.set(\"com.databricks.training.suppress.untilStreamIsReady\", \"true\")\nspark.conf.set(\"com.databricks.training.suppress.stopAllStreams\", \"true\")\nspark.conf.set(\"com.databricks.training.suppress.moduleName\", \"true\")\nspark.conf.set(\"com.databricks.training.suppress.lessonName\", \"true\")\n# spark.conf.set(\"com.databricks.training.suppress.username\", \"true\")\nspark.conf.set(\"com.databricks.training.suppress.userhome\", \"true\")\n# spark.conf.set(\"com.databricks.training.suppress.workingDir\", \"true\")\nspark.conf.set(\"com.databricks.training.suppress.databaseName\", \"true\")\n\u00a0\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\u00a0\n#import tensorflow\n\u00a0\ndef display_run_uri(experiment_id, run_id):\n    host_name = dbutils.notebook.entry_point.getDbutils().notebook().getContext().tags().get(\"browserHostName\").get()\n    uri = \"https://{}/#mlflow/experiments/{}/runs/{}\".format(host_name,experiment_id,run_id)\n    displayHTML(\"\"\"<b>Run URI:</b> <a href=\"{}\">{}</a>\"\"\".format(uri,uri))\n\u00a0\ndef waitForMLflow():\n  try:\n    import mlflow; \n    if int(mlflow.__version__.split(\".\")[1]) >= 2:\n        print(\"\"\"The module \"mlflow\" is attached and ready to go.\"\"\");\n    else:\n        print(\"\"\"You need MLflow version 1.2.0+ installed.\"\"\")\n  except ModuleNotFoundError:\n    print(\"\"\"The module \"mlflow\" is not yet attached to the cluster, waiting...\"\"\");\n    while True:\n      try: import mlflow; print(\"\"\"The module \"mlflow\" is attached and ready to go.\"\"\"); break;\n      except ModuleNotFoundError: import time; time.sleep(1); print(\".\", end=\"\");\n\u00a0\n\u00a0\nfrom sklearn.metrics import confusion_matrix,f1_score,accuracy_score,fbeta_score,precision_score,recall_score\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.utils.multiclass import unique_labels\n\u00a0\ndef plot_confusion_matrix(y_true, y_pred, classes,\n                          title=None,\n                          cmap=plt.cm.Blues):\n    # Compute confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    fig, ax = plt.subplots()\n    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n    ax.figure.colorbar(im, ax=ax)\n    ax.set(xticks=np.arange(cm.shape[1]),\n           yticks=np.arange(cm.shape[0]),\n           xticklabels=classes, yticklabels=classes,\n           title=title,\n           ylabel='True label',\n           xlabel='Predicted label')\n\u00a0\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n             rotation_mode=\"anchor\")\n\u00a0\n    fmt = 'd'\n    thresh = cm.max() / 2.\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            ax.text(j, i, format(cm[i, j], fmt),\n                    ha=\"center\", va=\"center\",\n                    color=\"white\" if cm[i, j] > thresh else \"black\")\n    fig.tight_layout()\n    return fig\n\u00a0\nnp.set_printoptions(precision=2)\n\u00a0\ndisplayHTML(\"Preparing the learning environment...\")\n\nI have no issues running this command,\n\n%run \"./Includes/Classroom-Setup\" , as it says all the functions have been defined.\n\n\u00a0\n\nthen when I am running this,\n\n%python\n\nimport mlflow\n\nimport mlflow.spark\n\n\u00a0\n\nin the next cell, I am getting a ModelNotFoundError:\n\n\u00a0\n\nModuleNotFoundError                       Traceback (most recent call last)\n<command-1419217929106651> in <module>\n----> 1 import mlflow\n      2 import mlflow.spark\n\u00a0\n/databricks/python_shell/dbruntime/PythonPackageImportsInstrumentation/__init__.py in import_patch(name, globals, locals, fromlist, level)\n    156             # Import the desired module. If you\u2019re seeing this while debugging a failed import,\n    157             # look at preceding stack frames for relevant error information.\n--> 158             original_result = python_builtin_import(name, globals, locals, fromlist, level)\n    159 \n    160             is_root_import = thread_local._nest_level == 1\n\u00a0\nModuleNotFoundError: No module named 'mlflow'\n\nWhat is the cause of this and how can I fix it? Unfortunately, Coursera is not helpful with this particular course.\n\nThank you, I am new to Databricks.",
        "Answers": [
            {
                "Answer_creation_date": "2022-01-17T12:21:24.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @bluetail (Customer)\u200b\u00a0,\n\n\u00a0\n\nFrom the error description, it looks like the mlflow library is not present. You can use ML cluster as these type of cluster already have mlflow library. Please check the below document:\n\n\u00a0\n\nhttps://docs.databricks.com/release-notes/runtime/7.3ml.html\n\n\u00a0\n\nOr else, we will need to install the required library into the existing cluster.\n\n\u00a0\n\nBelow document will help to install the library:\n\n\u00a0\n\nhttps://docs.databricks.com/libraries/cluster-libraries.html\n\n\u00a0\n\nPlease let us know if this helps.\n\nExpand Post",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-01-18T16:56:19.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Darshan, I am using the 9.1 cluster, is it not a higher version?\n\nI have tried both 9.1 and 7.3 clusters and am still getting the same error.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-01-18T19:13:02.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "I have installed manually mlflow==1.20.2 with the 9.1 cluster and it worked :) thank you.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-01-20T00:23:25.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Should be easier to just use the ML runtimes https://docs.databricks.com/runtime/mlruntime.html",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-01-21T14:41:17.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "the standard runtimes did not work with me. I am not sure why, I am on a 14 day trial at the moment.\n\nby the way do the 7.3 and 9.1 cost the same to run?",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-01-21T14:50:45.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "There is no cost associated with particular runtimes. All the costs are associated with the cluster VM size and how long the cluster runs.",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "How to deploy a databricks managed workspace model to sagemaker from databricks notebook",
        "Question_creation_date": "2021-10-11T7:01Z",
        "Question_tag": [
            "Model Deployment"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001TSqebCAD/how-to-deploy-a-databricks-managed-workspace-model-to-sagemaker-from-databricks-notebook",
        "Question_upvote_count": 0,
        "Question_answer_count": 14,
        "Question_view_count": 450,
        "Question_has_accepted_answer": true,
        "Question_body": "I wanted to deploy a registered model present in databricks managed MLFlow to a sagemaker via databricks notebook?\n\n\u00a0\n\nAs of now, it is not able to run mlflow sagemaker build-and-push container command directly. What all configurations or steps needed to do that? I assume that a manual push of docker image from outside of databricks should not be required just like in open source MLFlow. There has to be an alternate way.\n\n\u00a0\n\nAlso, When I am trying to test it locally via API, then I am getting the below error.\n\n\u00a0\n\nCode:\n\nimport mlflow.sagemaker as mfs\n\nmfs.run_local(model_uri=model_uri,port=8000,image=\"test\")\n\n\u00a0\n\nError:\n\nAttributeError: 'ConsoleBuffer' object has no attribute 'fileno'\n\n\u00a0\n\n\u00a0\n\nCan someone show some light on this topic?",
        "Answers": [
            {
                "Answer_creation_date": "2021-11-11T05:33:52.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @Maverick (Customer)\u200b\u00a0! My name is Kaniz, and I'm the technical moderator here. Great to meet you, and thanks for your question! Let's see if your peers in the community have an answer to your question first. Or else I will get back to you soon. Thanks.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-11-16T14:19:40.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @Maverick (Customer)\u200b\u00a0, This link might help you.\n\nhttps://www.mlflow.org/docs/latest/models.html#sagemaker-deployment",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-11-16T14:22:36.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Also can you please tell me which databricks runtime are you using?",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-11-17T05:12:06.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "@Kaniz Fatma (Databricks)\u200b\u00a0: Thanks for replying.\n\nI am using databricks runtime 9 ML.\n\n\u00a0\n\nOpen source MLFlow implementation is working fine but I am getting error while running mlflow sagemaker command on top of databricks notebooks.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-11-24T05:15:45.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "There apparently exists a simple solution. You can add sys.stdout.fileno = lambda: 0 to resolve the issue.\n\n\u00a0\n\nThe issue was hit with ray library as well:\n\n\u00a0\n\n\u00a0\n\nYou can add sys.stdout.fileno = lambda: 0 to resolve the issue.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-11-24T05:17:37.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "@Maverick (Customer)\u200b\u00a0Please try!\n\nimport mlflow.sagemaker as mfs\nsys.stdout.fileno = lambda: 0\nmfs.run_local(model_uri=model_uri,port=8000,image=\"test\")",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-11-25T06:55:28.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "@gobinath.viswanathan (Databricks)\u200b\u00a0: Still getting below error:\n\nHave tried to install docker explicitly too but the error still persists.\n\nNote: I am running this inside Databricks notebook on managed AWS databricks.\n\n\u00a0\n\nError:\n\nUsing the python_function flavor for local serving!\n\n2021/11/24 13:01:07 INFO mlflow.sagemaker: launching docker image with path /tmp/tmpq622qyl6/model\n\n2021/11/24 13:01:07 INFO mlflow.sagemaker: executing: docker run -v /tmp/tmpq622qyl6/model:/opt/ml/model/ -p 5432:8080 -e MLFLOW_DEPLOYMENT_FLAVOR_NAME=python_function --rm test serve\n\n\u00a0\n\nFileNotFoundError: [Errno 2] No such file or directory: 'docker'",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-11-27T15:06:53.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "https://docs.docker.com/engine/reference/builder/\n\nhttps://forums.docker.com/t/no-such-file-or-directory-after-building-the-image/66143\n\n\u00a0\n\nthis 2 references might be helpful from docker side. let us know if this helps . @Maverick (Customer)\u200b",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-11-29T04:54:26.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "@Atanu (Databricks)\u200b\u00a0 @gobinath.viswanathan (Databricks)\u200b\u00a0 @Kaniz Fatma (Databricks)\u200b\u00a0: Thanks for reaching out. Unfortunately the above links mentioned by you is working only in case I am doing things via open-source MLFlow where I have control over editing the folder structure and create a separate docker file.\n\n\u00a0\n\nBut the same is not allowed in managed Databricks env. The model artefacts are stored on a path which can only be assessed by MLFLow API.\n\n\u00a0\n\nIn case, if you have tried some other way and it worked for you then please let me know the complete steps. I am looking to push the models registered in Databricks managed MLFLow registry to the Sagemaker endpoints and also wanted to test this setup via Sagemaker local command.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-12-29T11:53:18.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @Maverick (Customer)\u200b\u00a0, Can you please add this code and check if it works?\n\nimport sys\n \nsys.stdout.fileno = lambda: False",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-01-03T12:53:39.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "@Kaniz Fatma (Databricks)\u200b\u00a0: Hi Kaniz,\n\nThe suggested solution is not working on databricks notebooks. Please see below:\n\n\u00a0\n\n\u00a0\n\nExpand Post",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-01-11T13:37:39.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "@Kaniz Fatma (Databricks)\u200b\u00a0@gobinath.viswanathan (Databricks)\u200b\u00a0@Atanu (Databricks)\u200b\u00a0:\n\nHi All,\n\n\u00a0\n\nThe above direct methods are not working. So, I downloaded the model files using mlflow and trying to run \"mlflow sagemaker build-and-push-containers\" in order to push the model image to ECR.\n\n\u00a0\n\nThis step is also not running. Getting \"no module named docker\" error from the \"mlflow.models.docker_utils\" module\n\n\u00a0\n\nI am currently running Databricks 10.2 ML runtime.\n\n\u00a0\n\nAfter installing docker via \"pip install docker\". Now I am getting error as\n\n\"docker.errors.DockerException: Error while fetching server API version: ('Connection aborted.', FileNotFoundError(2, 'No such file or directory'))\"\n\n\u00a0\n\n\u00a0\n\n\u00a0\n\n\u00a0\n\nExpand Post",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-01-14T06:59:09.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "@Atanu (Databricks)\u200b\u00a0 @gobinath.viswanathan (Databricks)\u200b\u00a0@Kaniz Fatma (Databricks)\u200b\u00a0:\n\nI have been trying to push a registered model in DB managed mlflow to sagemaker endpoint. Although I have been able to do it but there are some manual steps that I needed to do on my local system in order to make it work. Could you help me to understand, Am I doing it correctly or Is there a bug in the Databricks ML runtime.\n\n\u00a0\n\nBelow are the steps that I did:\n\nStep1: Log the model\u00a0\nRan a model code and registered it in mlflow. Moved the model into production stage.\nStep2: Deploy the model\nInstalled AWS CLI (via pip) and configured the AWS target env./account. This account have a role ARN setup with Sagemaker full access and ECRContainerRegistry full access.\nAble to connect the target AWS account via databricks notebook.\nWhile I was deploying the model as Sagemaker endpoint via\u00a0\u201cmlflow.sagemaker.deploy\u201d, All intermediate obejcts are being created but I was getting the error because it was not able to find the container image in ECR. My initial assumption was that the function itself should be able to create containers by using the current model code.\n\nSo, I\u00a0\u00a0downloaded the model files into a folder on DB local path using mlflow library.\n\n\u00a0\n\nNow In order to create a container, I am using \u201cmlflow sagemaker build-and-push-container\u201d command from the DB local path where model files are present.It is showing me error\u00a0\u201cno module named docker\u201d\u00a0from the \u201cmlflow.docker_utils\u201d module.\nIn order to resolve this I did\u00a0\u201cpip install docker\u201d. But after that I am getting the error: docker.errors.DockerException: Error while fetching server API version: ('Connection aborted.', FileNotFoundError(2, 'No such file or directory'))\"\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\n\nI have checked that this error comes when the docker daemon processes itself are not working. I also haven\u2019t been able to find any docker process executable file in \u201c/etc/init.d/\u201d path where general service executables are present.\n\n\u00a0\n\n\u00a0\n\nThe only way all things works is when I downloaded all model based files on my local system, ran the docker desktop for docker daemons to be up and then ran \u201cmlflow sagemaker build-and-push-container\u201d\u00a0command from inside the models folder.\u00a0It had created an image in the ECR which is being correctly referred by\u00a0\u201cmlflow.sagemaker.deploy\u201d\u00a0command.\n\n\u00a0\n\n\u00a0\n\nMy question is that, Is this the right process?\u00a0Do we need to build the image locally in order to make it work?\u00a0\u00a0\n\nMy assumption was that the\u00a0\u201cmlflow.sagemaker.deploy\u201d\u00a0command would be able to take care of all things Or atmost the \u201cmlflow sagemaker build-and-push-container\u201d command should be able to run from databricks notebook itself.\n\nExpand Post",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-01-25T14:32:31.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @Maverick (Customer)\u200b\u00a0, Yes, it's the right process. Thanks.",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "Migration from self-managed MLflow to Databricks managed MLflow.",
        "Question_creation_date": "May-by-KanizT:2022Z",
        "Question_tag": [
            "MLFlow"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001M4W8YCAV/migration-from-selfmanaged-mlflow-to-databricks-managed-mlflow",
        "Question_upvote_count": 0,
        "Question_answer_count": 8,
        "Question_view_count": 354,
        "Question_has_accepted_answer": true,
        "Question_body": "Would it be possible to somehow save the data, metrics of all experiments captured by self-managed mlflow using A/mazon RDS, S3 as backend\u00a0and then load it to databricks managed mlflow and make it available in the UI? This is required as a part of migration activity.",
        "Answers": [
            {
                "Answer_creation_date": "2021-09-28T07:49:17.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @Maverick! My name is Kaniz, and I'm the technical moderator here. Great to meet you, and thanks for your question! Let's see if your peers on the community have an answer to your question first. Or else I will follow up with my team and get back to you soon.Thanks.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-10-05T18:27:30.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "You can migrate your MLflow experiment and runs using the MLflow API with this toolkit: https://github.com/amesar/mlflow-export-import",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-03-07T13:06:27.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "@andre.mesarovic (Databricks)\u200b\u00a0: Is this solution, Databricks approved for enterprise usage?\n\nI believe this is not an approved solution and also I can see many issues in it while doing import and export like nested metrics are not imported correctly etc.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-03-07T16:08:00.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "mlflow-export-import is an open source tool used by both OSS and Databricks users. It is in the process of being a recommended solution for Databricks MLflow migrations. Create a ticket for any issues at: https://github.com/amesar/mlflow-export-import/issues",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-05-18T20:40:51.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @Maverick (Customer)\u200b\u00a0 , Just a friendly follow-up. Do you still need help or the above responses help you to find the solution? Please let us know.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-05-20T10:31:26.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "@Kaniz Fatma (Databricks)\u200b\u00a0: Hi Fatima,\n\nmlflow-export-import is still not a recommended solution and has no support from Databricks. This is clarified by Databricks SA's themselves.\n\n\u00a0\n\nI have developed an another approach where I am capturing the statistics of a particular run which is needed to be migrated to higher env. (like pre-prod or prod) and then creating a mlflow run with those statistics instead of re-running the entire huge model. This is working perfectly and is within the logical support of Databricks.\n\nExpand Post",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-05-20T10:35:44.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @Maverick (Customer)\u200b\u00a0, I'm glad that you have got a solution to your question. Would you like to mark your answer as the best?",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-05-20T15:55:31.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @Maverick (Customer)\u200b\u00a0, mlflow-export-import is in the process of being the Databricks recommended solution. We will have it in our official DB docs soon. It is based on the open source MLflow API and works for both OSS and DB. We have over 105 customers using it. It is the only way to migrate MLflow objects from one tracking server (workspace) to another.",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "Lineage between model and source code breaks on movement of source notebook. How to rectify it?",
        "Question_creation_date": "2021-8-10T6:05Z",
        "Question_tag": [
            "Experiment Managements",
            "Lineage",
            "Model Management",
            "Model Source Tracking",
            "Source Code"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001J4POeCAN/lineage-between-model-and-source-code-breaks-on-movement-of-source-notebook-how-to-rectify-it",
        "Question_upvote_count": 0,
        "Question_answer_count": 10,
        "Question_view_count": 286,
        "Question_has_accepted_answer": true,
        "Question_body": "If there is a registered model and it is linked with a notebook, then the lineage breaks if you move the notebook to a different path or even pull/upload a new version of the notebook.\n\n\u00a0\n\nThis is not good because when someone doing its development/testing they usually do it in a messy way but if you need your code in production then it means that either\n\nYou need to move your code and then again re-train to generate the same model and then perform its movement to higher env. so that the lineage is maintained.... or\nYou need to keep the dev source code notebook, that too on the same path, where it had been created for dev usage.",
        "Answers": [
            {
                "Answer_creation_date": "2021-09-10T06:51:11.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @Maverick (Customer)\u200b\u00a0,\n\nas in your other topic, I suggest looking into MLflow as this is designed to handle all these issues.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-09-10T08:45:41.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "@werners (Customer)\u200b\u00a0: Hi Werners,\n\nThanks for the reply. But this issue is happening in MLFLow tracking itself. I wanted to know if there is a way to mitigate it or not.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-09-10T09:05:20.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Ok I see,\n\ndo not have a clear answer for that.\n\nBut there was a session at spark summit about this (well, more CI/CD related but it might give ideas):\n\n\u00a0\n\nhttps://databricks.com/session_na20/productionalizing-models-through-ci-cd-design-with-mlflow\n\n\u00a0\n\nThe whole ci/cd shebang might be too much for your needs but maybe you can pick some parts which are useful?\n\nExpand Post",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-09-10T19:17:02.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "@werners (Customer)\u200b\u00a0- Thank you so much!",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-09-11T23:53:52.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi Maverick,\n\n\u00a0\n\nI'm unable to reproduce the issue you mentioned. Where is your notebook located? Is it stored in a git-versioned Repo directory (accessed via \"Repos\" instead of \"Workspace\" icon on the navigation bar)?",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-09-12T07:01:34.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "@Jin (Customer)\u200b\u00a0: For reproducing this issue:\n\ncreate a notebook in your workspace account and generate a model from it.\nIf you go to the model stats page you can see its lineage to the original notebook.\nCreate a folder in your workspace and move your notebook to that folder. Now move it back to where it was before.\nGo back to the model stats page and click on \u201csource\u201d link which represents the original notebook lineage. It will show the error as \u201cnotebook not found\u201d although it is on the same path where it is supposed to be.\nExpand Post",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-09-15T23:50:53.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "I also cannot reproduce this, with these exact steps (I think). After moving the notebook and moving it back, the link to it (and link to the revision) still works as expected. You are using MLflow built in to Databricks right?",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-09-27T06:14:08.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "@sean.owen (Databricks)\u200b\u00a0: Yes. Managed MLFlow on databricks.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-11-05T16:15:50.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @Maverick (Customer)\u200b\u00a0,\n\n\u00a0\n\nDid Sean's reply help you to solve this issue or your still are waiting for a solution to unblock you?",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-11-10T09:52:16.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @Jose Gonzalez (Databricks)\u200b\u00a0,\n\nThe issue still persists. I believe this is related to the workspace version that we are using.\n\nI have ran through the exact steps in E2 workspace version and the issue is somehow resolved there.",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "Cluster setup for ML work for Pandas in Spark, and vanilla Python.",
        "Question_creation_date": "2022-0-21T17:16Z",
        "Question_tag": [
            "Standard",
            "Pandas",
            "Python",
            "Machine Learning",
            "Cluster"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001dnSF4CAM/cluster-setup-for-ml-work-for-pandas-in-spark-and-vanilla-python",
        "Question_upvote_count": 0,
        "Question_answer_count": 4,
        "Question_view_count": 345,
        "Question_has_accepted_answer": true,
        "Question_body": "My setup:\n\nWorker type: Standard_D32d_v4, 128 GB Memory, 32 Cores, Min Workers: 2, Max Workers: 8\n\nDriver type: Standard_D32ds_v4, 128 GB Memory, 32 Cores\n\nDatabricks Runtime Version: 10.2 ML (includes Apache Spark 3.2.0, Scala 2.12)\n\n\u00a0\n\nI ran a snowflake query and pulled in two datasets 30 million rows and 7 columns. Saved them as pyspark.pandas.frame.DataFrame, call them df1, and df2 (the two dataframes)\n\n\u00a0\n\n1st column of each of these datasets is a household_id. I want to check how many household_id from df1 is not in df2.\n\n\u00a0\n\nI tried two different ways:\n\nlen(set(df1['household_id'].to_list).difference(df2['household_id'].to_list()))\n\n\u00a0\n\ndf1['household_id'].isin(df2['household_id'].to_list()).value_counts()\n\nThe above two fail because of out of memory issue.\n\n\u00a0\n\nMy questions are:\n\nWhere is the python list computation happening as in first code snippet? Is it on driver node or worker node? I believe that code is being run in a single node and not distributed?\nIs there a way to better debug out of memory issue? Such as which piece of code? Which node the code failed., etc.\nWhat is the best guidance on creating a cluster? This could depend on understanding how pieces of code will run such as distributed across worker nodes, or running on a single driver . node. Is there a general guidance if driver node should be beefier (larger memory and cores) as compared to worker nodes or vice-versa?",
        "Answers": [
            {
                "Answer_creation_date": "2022-01-21T19:52:57.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi again! Thanks for this question also and for your patience. We'll be back after we give the members of the community a chance to respond. :)",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-01-21T20:00:55.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Python code runs on the driver. Distributed/Spark code runs on the workers.\n\n\u00a0\n\nHere are some cluster tips:\n\nIf you're doing ML, then use an ML runtime.\n\nIf you're not doing distributed stuff, use a single node cluster.\n\nDon't use autoscaling for ML.\n\nFor Deep Learning use GPUs\n\nTry to size the cluster for the data size.\n\n\u00a0\n\nExpand Post",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-03-07T17:33:38.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "@Vik (Customer)\u200b\u00a0- Does Joseph's answer help? If it does, would you be happy to mark it as best? If it doesn't, please tell us so we can help you.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-04-22T14:23:05.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hey there @Vik (Customer)\u200b\u00a0\n\n\u00a0\n\nChecking in. If Joseph's answer helped, would you let us know and mark the answer as best? \u00a0It would be really helpful for the other members to find the solution more quickly.\n\n\u00a0\n\nThanks!",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "How to make sparklyr extension work with Databricks runtime?",
        "Question_creation_date": "2021-7-24T16:07Z",
        "Question_tag": [
            "Databricks Runtime",
            "Apache spark",
            "Sparklyr Extension Work"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001GZIOSCA5/how-to-make-sparklyr-extension-work-with-databricks-runtime",
        "Question_upvote_count": 3,
        "Question_answer_count": 6,
        "Question_view_count": 285,
        "Question_has_accepted_answer": true,
        "Question_body": "Hello. I'm the current maintainer of sparklyr (a R interface for Apache Spark) and a few sparklyr extensions such as sparklyr.flint.\n\n\u00a0\n\nSparklyr was fortunate to receive some contribution from Databricks folks, which enabled R users to run `spark_connect(method = \"databricks\")` to connect to Databricks Runtime.\n\n\u00a0\n\nMy question is how to make this type of Spark connection in R work with sparklyr extensions (e.g., see https://github.com/r-spark/sparklyr.flint/issues/55 -- this was something I don't have a good answer for at the moment because I'm not really super-familiar with how Databricks connections work with sparklyr internally).\n\n\u00a0\n\nA bit more context: sparklyr.flint is an R interface for the Flint time series library that works on top of sparklyr. Usually when users run code such as the following\n\n\u00a0\n\nlibrary(sparklyr)\nlibrary(sparklyr.flint)\n\u00a0\nsc <- spark_connect(master = \"yarn-client\", spark_home = \"/usr/lib/spark\")\n\nThe presence of sparklyr.flint as a sparklyr extension will cause the Spark process to fetch some version of Flint time series library jar files and load those files within the Spark session that it is connecting to.\n\n\u00a0\n\nBut this didn't work if we were to replace the `sc <- spark_connect(...)` from above with `sc <- spark_connect(method = \"databricks\")` (again, see https://github.com/r-spark/sparklyr.flint/issues/55 for details). My uneducated guess is `method = \"databricks\"` had some level of indirection involved in the connecting-to-Spark step, and the Flint time series jar files were downloaded into the wrong location.\n\n\u00a0\n\nI'm wondering whether there is some simple change to sparklyr I can make to ensure sparklyr extensions also work in Databricks. Your input would be greatly appreciated.\n\n\u00a0\n\nThanks!",
        "Answers": [
            {
                "Answer_creation_date": "2021-09-02T09:23:22.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @yitao\u00a0! My name is Kaniz, and I'm a technical moderator here. Great to meet you, and thanks for your question! Let's see if your peers on the Forum have an answer to your questions first. Or else I will follow up shortly with a response.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-09-08T17:44:21.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Just like any R Library you can have an init script that will copy the library to the R runtime of the Cluster. I manage all libraries either using Global Init script/local at the cluster level. Store it in a mount and during the boot up of cluster just run a copy command to move the libraries to the runtime",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-09-09T20:43:47.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Yes, as Sebastian said. Also, it would be good to know what the error is here. One possible explanation is that the JARs are not copied to the executor nodes. This would be solved by Sebasitian's suggestion.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-09-09T20:49:30.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Thanks for the answers everyone!\n\n\u00a0\n\nTwo follow-up questions:\n\nIs it possible to package the init scripts together with a R package itself? I'm thinking ideally the script should be self-contained and should not require additional user input. It should figure out the location for installing JARs in a Databricks cluster based on config files and (maybe) env variables.\nIf answer is 'yes' to the first question, is there an example R package that has solved this type of problem successfulyl with a pre-packaged init script?\n\n\u00a0\n\nAgain thanks a lot for your help.\n\nExpand Post",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-10-14T00:04:27.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Not, the init scripts run before Spark starts or any packages get loaded. So if there are any dependancies, they will need to be stated. Also, I think if the user does install,packages(\"your_library\"), then Databricks will automatically install on all nodes. Also installing using the library UI will do this. But we're just making solutions based on hypotheses here. We would really need to know what error you are seeing to tell.\n\n\u00a0\n\nTypically- whatever R library you are installing on the cluster should ALSO install the JAR files. My guess is that R's arrow package does this, but not sure. It definitely installs the underlying C++ dependancy. Not sure if there is also a Java component.\n\nExpand Post",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-05-18T21:35:15.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @yitao (Customer)\u200b\u00a0, Just a friendly follow-up. Do you still need help, or does the above response help you to find the solution? Please let us know.",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "What happens to my production jobs if the underlying Databricks Runtime is no longer supported? Will they fail?",
        "Question_creation_date": "2021-8-24T12:53Z",
        "Question_tag": [
            "Databricks Runtime",
            "Production Jobs",
            "Production"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001LW0M1CAL/what-happens-to-my-production-jobs-if-the-underlying-databricks-runtime-is-no-longer-supported-will-they-fail",
        "Question_upvote_count": 0,
        "Question_answer_count": 4,
        "Question_view_count": 142,
        "Question_has_accepted_answer": false,
        "Question_body": "",
        "Answers": [
            {
                "Answer_creation_date": "2021-09-24T05:36:05.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @\u00a0Will_Block! My name is Kaniz, and I'm the technical moderator here. Great to meet you, and thanks for your question! Let's see if your peers on the community have an answer to your question first. Or else I will follow up with my team and get back to you soon.Thanks.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-09-27T07:33:18.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "@Will_Block (Databricks)\u200b\u00a0, a while ago we had some jobs running on non-supported versions.\n\nWe did not notice it to be honest, because the jobs kept on working! That was however more than a year ago. The minute we noticed running on a non-supported version, we started migrating.\n\nSo chances are that they will keep on running for a while, but without support... I would not take the risk and start migrating/planning ASAP.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-10-04T10:52:59.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "@werners (Customer)\u200b you can think about the Databricks Runtime as a contract. It does and will change over time. However, we offer Long Term Support versions of the runtime which offer multi-year support. If you have production jobs, I would definitely consider running them on an LTS version. However, as the LTS version phases out of support, please consider migrating to a newer LTS so you can take advantage of the thousands of improvements we release every year.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-05-18T20:28:16.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @Will_Block (Databricks)\u200b\u00a0, Just a friendly follow-up. Do you still need help or the above responses help you to find the solution? Please let us know.",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "DNS_Analytics Notebook Problems",
        "Question_creation_date": "User-by-ForumsT13:Z",
        "Question_tag": [
            "MLFlow",
            "Pyspark",
            "Rest-api",
            "Dns",
            "Analytics"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001HKICHCA5/dnsanalytics-notebook-problems",
        "Question_upvote_count": 0,
        "Question_answer_count": 2,
        "Question_view_count": 123,
        "Question_has_accepted_answer": false,
        "Question_body": "Hello everyone! First post on the forums, been stuck at this for awhile now and cannot seem to understand why this is happening. Basically, I have been using a seems to be premade Databricks notebook from Databricks themselves for a DNS Analytics example that uses MLflow and more(\u00a0Here's the Notebook). I even set this notebook to run in a community edition variant of it as I am using the community edition of Databricks. No matter what I keep getting the same error throwing on command 32 which is:\n\nRestException: RESOURCE_DOES_NOT_EXIST: No file or directory exists on path /FileStore/tables/model.\n---------------------------------------------------------------------------\nRestException                             Traceback (most recent call last)\n<command-168210906724122> in <module>\n      4 \n      5 model_path = 'dbfs:/FileStore/tables/model'\n----> 6 loaded_model = mlflow.pyfunc.load_model(model_path)\n      7 spark.udf.register(\"ioc_detect\", loaded_model.predict)\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-528b26da-5f61-435c-b5a3-3adc5ab7c638/lib/python3.8/site-packages/mlflow/pyfunc/__init__.py in load_model(model_uri, suppress_warnings)\n    637                               messages will be emitted.\n    638     \"\"\"\n--> 639     local_path = _download_artifact_from_uri(artifact_uri=model_uri)\n    640     model_meta = Model.load(os.path.join(local_path, MLMODEL_FILE_NAME))\n    641 \n/local_disk0/.ephemeral_nfs/envs/pythonEnv-528b26da-5f61-435c-b5a3-3adc5ab7c638/lib/python3.8/site-packages/mlflow/tracking/artifact_utils.py in _download_artifact_from_uri(artifact_uri, output_path)\n     77         root_uri = prefix + urllib.parse.urlunparse(parsed_uri)\n     78 \n---> 79     return get_artifact_repository(artifact_uri=root_uri).download_artifacts(\n     80         artifact_path=artifact_path, dst_path=output_path\n     81     )\n\nAny recommendations or fixes or some direction would be appreciated thank you!\n\nThe code that throws this error:\n\n#Load the DGA model. This is a pre-trained model that we will use to enrich our incoming DNS events. You will see how to train this model in a later step.\nimport mlflow\nimport mlflow.pyfunc\nmodel_path = 'dbfs:/FileStore/tables/model'\nloaded_model = mlflow.pyfunc.load_model(model_path)\nspark.udf.register(\"ioc_detect\", loaded_model.predict)<br>",
        "Answers": [
            {
                "Answer_creation_date": "2021-09-01T17:42:04.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "@NickGoodfella (Customer)\u200b\u00a0, What's the notebook you're looking at, this one? https://databricks.com/notebooks/dns-analytics.html Are you sure all the previous cells executed? this is suggesting there isn't a model at the path that's expected. You can take a look with %fs ls ... to see what's actually there",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-05-18T20:05:59.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @NickGoodfella (Customer)\u200b\u00a0, Just a friendly follow-up. Do you still need help, or @sean.owen (Databricks)\u200b's response help you to find the solution? Please let us know.",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "how to log the KerasClassifier model in a sklearn pipeline in mlflow?",
        "Question_creation_date": "2021-8-10T3:56Z",
        "Question_tag": [
            "MLFlow",
            "Sklean",
            "Sklean Pipeline",
            "KerasClassifier Model",
            "Estimator",
            "Python",
            "Log",
            "Model Log"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001J4OQZCA3/how-to-log-the-kerasclassifier-model-in-a-sklearn-pipeline-in-mlflow",
        "Question_upvote_count": 1,
        "Question_answer_count": 8,
        "Question_view_count": 520,
        "Question_has_accepted_answer": true,
        "Question_body": "I have a set of pre-processing stages in a sklearn `Pipeline` and an estimator which is a `KerasClassifier` (`from tensorflow.keras.wrappers.scikit_learn import KerasClassifier`).\n\n\u00a0\n\nMy overall goal is to tune and log the whole sklearn pipeline in `mlflow` (in databricks even). I get a confusing type error which I can't figure out how to reslove:\n\n\u00a0\n\n> TypeError: can't pickle _thread.RLock objects\n\n\u00a0\n\nI have the following code (without tuning stage) which returns the above error:\n\n\u00a0\n\nconda_env = _mlflow_conda_env(\n\u00a0  additional_conda_deps=None,\n\u00a0  additional_pip_deps=[\n\u00a0    \"cloudpickle=={}\".format(cloudpickle.__version__),\n\u00a0    \"scikit-learn=={}\".format(sklearn.__version__),\n\u00a0    \"numpy=={}\".format(np.__version__),\n\u00a0    \"tensorflow=={}\".format(tf.__version__),\n\u00a0  ],\n\u00a0  additional_conda_channels=None,\n\u00a0)\n\u00a0\n \n\u00a0\nsearch_space = {\n\u00a0  \"estimator__dense_l1\": 20,\n\u00a0  \"estimator__dense_l2\": 20,\n\u00a0  \"estimator__learning_rate\": 0.1,\n\u00a0  \"estimator__optimizer\": \"Adam\",\n\u00a0}\n\u00a0\n \u00a0\ndef create_model(n):\n\u00a0\n\u00a0  model = Sequential()\n\u00a0  model.add(Dense(int(n[\"estimator__dense_l1\"]), activation=\"relu\"))\n\u00a0  model.add(Dense(int(n[\"estimator__dense_l2\"]), activation=\"relu\"))\n\u00a0  model.add(Dense(1, activation=\"sigmoid\"))\n\u00a0  model.compile(\n\u00a0    loss=\"binary_crossentropy\",\n\u00a0    optimizer=n[\"estimator__optimizer\"],\n\u00a0    metrics=[\"accuracy\"],\n\u00a0  )\n\u00a0\u00a0\n  return model\n\u00a0\n \n\u00a0\n \n\u00a0\nmlflow.sklearn.autolog()\n\u00a0\nwith mlflow.start_run(nested=True) as run:\n\u00a0\n  classfier = KerasClassifier(build_fn=create_model, n=search_space)\n\u00a0  # fit the pipeline\n\u00a0  clf = Pipeline(steps=[(\"preprocessor\", preprocessor), \n\u00a0\n             (\"estimator\", classfier)])\n\u00a0  h = clf.fit(\n\u00a0    X_train,\n\u00a0    y_train.values,\n\u00a0    estimator__validation_split=0.2,\n\u00a0    estimator__epochs=10,\n\u00a0    estimator__verbose=2,\n\u00a0  )\n\u00a0\n\u00a0\n  # log scores\n\u00a0  acc_score = clf.score(X=X_test, y=y_test)\n\u00a0  mlflow.log_metric(\"accuracy\", acc_score)\n\u00a0\n  signature = infer_signature(X_test, clf.predict(X_test))\n\u00a0  # Log the model with a signature that defines the schema of the model's inputs and outputs.\n\u00a0  mlflow.sklearn.log_model(\n\u00a0    sk_model=clf, artifact_path=\"model\", \n\u00a0    signature=signature, \n\u00a0    conda_env=conda_env\n\u00a0  )\n\n\u00a0\n\n\u00a0\n\nI also get this warning before the error:\n\n\u00a0\n\n```\n\n\u00a0\n\n\u00a0\u00a0WARNING mlflow.sklearn.utils: Truncated the value of the key `steps`. Truncated value: `[('preprocessor', ColumnTransformer(n_jobs=None, remainder='drop', sparse_threshold=0.3,\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0transformer_weights=None,\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0transformers=[('num',\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Pipeline(memory=None,\n\n```\n\n\u00a0\n\nnote the whole pipeline runs outside mlflow.\n\ncan someone help?",
        "Answers": [
            {
                "Answer_creation_date": "2021-09-10T07:10:10.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "no one?",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-09-10T14:11:36.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "@Kaniz Fatma (Databricks)\u200b\u00a0- Can you jump in here?",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-09-10T15:35:52.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Sure @Piper",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-09-10T15:35:35.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @MGH\u00a0! My name is Kaniz, and I'm the technical moderator here. Great to meet you, and thanks for your question! Let's see if your peers on the community have an answer to your question first. Or else I will follow up shortly with a response.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-09-11T07:44:33.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Thanks @Kaniz Fatma (Databricks)\u200b\u00a0!\n\nJust to clarify I have no issue logging a sklearn model and pipeline, for example if I replace this part of the above code from:\n\n   clf = Pipeline(steps=[(\"preprocessor\", preprocessor), \n                                                (\"estimator\", classfier)])\n\nto:\n\n   clf = Pipeline(steps=[(\"preprocessor\", preprocessor), \n \n             (\"estimator\", RandomForestClassifier())])\n\nit works without issue.\n\nThe problem is when you wrap a Keras model .\n\nExpand Post",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-09-13T07:39:51.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hey there @MGH!\n\nI'm glad I can help you \ud83d\ude0a.\n\nPlease be patient , I'll get back to you very soon with the response.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-09-14T04:18:26.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "I think I find sort of a workaround, but I think this issue needs to be addressed anyways.\n\nWhat I did is not the best way.\n\nI used a python package called scikeras that does this wrapping and then could log the model\n\nThe code:\n\nimport scikeras \nimport tensorflow as tf \nfrom tensorflow.keras.models import Sequential \nfrom tensorflow.keras.layers import Input, Dense, Dropout, LSTM, Flatten, Activation \n\u00a0\nfrom scikeras.wrappers import KerasClassifier \n  \n\u00a0\nclass ModelWrapper(mlflow.pyfunc.PythonModel): \n    def __init__(self, model): \n        self.model = model \n\u00a0\n    def predict(self, context, model_input): \n        return self.model.predict(model_input) \n\u00a0\nconda_env =  _mlflow_conda_env( \n      additional_conda_deps=None, \n      additional_pip_deps=[ \n        \"cloudpickle=={}\".format(cloudpickle.__version__),  \n        \"scikit-learn=={}\".format(sklearn.__version__), \n        \"numpy=={}\".format(np.__version__), \n        \"tensorflow=={}\".format(tf.__version__), \n        \"scikeras=={}\".format(scikeras.__version__), \n      ], \n      additional_conda_channels=None, \n  ) \n\u00a0\nparam = { \n   \"dense_l1\": 20, \n   \"dense_l2\": 20, \n   \"optimizer__learning_rate\": 0.1, \n   \"optimizer\": \"Adam\", \n   \"loss\":\"binary_crossentropy\", \n} \n\u00a0\n  \ndef create_model(dense_l1, dense_l2, meta): \n  \n  n_features_in_ = meta[\"n_features_in_\"] \n  X_shape_ = meta[\"X_shape_\"] \n  n_classes_ = meta[\"n_classes_\"] \n\u00a0\n  model = Sequential() \n  model.add(Dense(n_features_in_, input_shape=X_shape_[1:], activation=\"relu\")) \n  model.add(Dense(dense_l1, activation=\"relu\")) \n  model.add(Dense(dense_l2, activation=\"relu\")) \n  model.add(Dense(1, activation=\"sigmoid\")) \n\u00a0\n  return model   \n\u00a0\nmlflow.sklearn.autolog() \nwith mlflow.start_run(run_name=\"sample_run\"): \n\u00a0\n  classfier = KerasClassifier( \n    create_model, \n    loss=param[\"loss\"], \n    dense_l1=param[\"dense_l1\"], \n    dense_l2=param[\"dense_l2\"], \n    optimizer__learning_rate = param[\"optimizer__learning_rate\"], \n    optimizer= param[\"optimizer\"], \n) \n\u00a0\n  # fit the pipeline \n  clf = Pipeline(steps=[('preprocessor', preprocessor), \n                      ('estimator', classfier)])   \n\u00a0\n  h = clf.fit(X_train, y_train.values) \n  # log scores \n  acc_score = clf.score(X=X_test, y=y_test) \n  mlflow.log_metric(\"accuracy\", acc_score) \n  signature = infer_signature(X_test, clf.predict(X_test)) \n  model_nn = ModelWrapper(clf,)  \n\u00a0\n  mlflow.pyfunc.log_model( \n      python_model= model_nn, \n      artifact_path = \"model\",  \n      signature = signature,  \n      conda_env = conda_env \n  ) \n\u00a0\n  \n\u00a0\n \n\n\u00a0\n\nExpand Post",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-10-03T02:02:02.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "could you please share the full error stack trace?",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "Databricks Feature Store in MLFlow run CLI command",
        "Question_creation_date": "2021-8-8T17:40Z",
        "Question_tag": [
            "MLFlow",
            "Databricks notebook",
            "Cli",
            "Feature Store"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001J0h0ICAR/databricks-feature-store-in-mlflow-run-cli-command",
        "Question_upvote_count": 1,
        "Question_answer_count": 3,
        "Question_view_count": 187,
        "Question_has_accepted_answer": false,
        "Question_body": "Hello!\n\n\u00a0\n\nI am attempting to move some machine learning code from a databricks notebook into a mlflow git repository. I am utilizing the databricks feature store to load features that have been processed. Currently I cannot get the databricks library to import when running 'mlfow run ... -b databricks`. Is it possible to use the feature store from within mlflow run cli command if the job is being executed on the databricks backend?\n\n\u00a0\n\nThanks!",
        "Answers": [
            {
                "Answer_creation_date": "2021-09-09T12:06:55.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @Alex.G! My name is Kaniz, and I'm the technical moderator here. Great to meet you, and thanks for your question! Let's see if your peers on the community have an answer to your question first. Or else I will follow up shortly with a response.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-09-09T17:35:58.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hm, what error do you get? I believe you won't be able to specify the feature store library as a dependency, as it's not externally published yet, but code that uses it should run on DB ML runtimes as it already exists there",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-05-18T20:23:12.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @Alex.G (Customer)\u200b\u00a0 , Just a friendly follow-up. Do you still need help, or @sean.owen (Databricks)\u200b\u00a0's response help you to find the solution? Please let us know.",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "read the csv file as shown in description",
        "Question_creation_date": "2022-2-30T18:54Z",
        "Question_tag": [
            "File",
            "Machine Learning",
            "Read"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001ooOEXCA2/read-the-csv-file-as-shown-in-description",
        "Question_upvote_count": 1,
        "Question_answer_count": 6,
        "Question_view_count": 216,
        "Question_has_accepted_answer": true,
        "Question_body": "Project_Details.csv\n\n\u00a0\n\nProjectNo|ProjectName|EmployeeNo\n\n100|analytics|1\n\n100|analytics|2\n\n101|machine learning|3\n\n101|machine learning|1\n\n101|machine learning|4\n\n\u00a0\n\nFind each employee in the form of list working on each project?\n\n\u00a0\n\nOutput:\n\nProjectNo|employeeNo\n\n100|[1,2]\n\n101|[3,1,4]",
        "Answers": [
            {
                "Answer_creation_date": "2022-03-31T20:06:53.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "from pyspark.sql import functions as F\ndf = spark.read.option(\"sep\", \"|\").option(\"header\", \"true\").csv(\"/tmp/file.csv\")\ndisplay(df.groupBy(\"projectNo\").agg(F.expr(\"collect_list(EmployeeNo)\").alias(\"employees\")))",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-04-01T06:10:03.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @sannycse (Customer)\u200b\u00a0, Did you get a chance to try the code provided by @garren.staubli (Databricks)\u200b\u00a0?",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-04-02T16:53:40.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "I tried but that was created in pyspark and i'm unable to crack that code into spark Sql",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-04-02T17:11:26.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "@sannycse (Customer)\u200b\u00a0, You can persist the data frame in temp view by adding following in the python:\n\ndf.createOrReplaceTempView(\"employees_csv\")\n\nthen you can select:\n\nselect projectNo, collect_list(EmployeeNo)\nfrom employees_csv\ngroup by projectNo\n\n\u00a0\n\nExpand Post",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-04-13T15:56:47.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "@sannycse (Customer)\u200b\u00a0 You can simply do this\n\n\u00a0\n\nJust change the file path\n\n\u00a0\n\nCREATE TEMPORARY VIEW readcsv USING CSV OPTIONS (\n\n\u00a0path \"dbfs:/docs/test.csv\",\n\n\u00a0header \"true\",\n\n\u00a0delimiter \"|\",\n\n\u00a0mode \"FAILFAST\"\n\n);\n\n\u00a0\n\nselect\n\n\u00a0ProjectNo,\n\n\u00a0collect_list(EmployeeNo) Employees\n\nfrom\n\n\u00a0readcsv\n\ngroup by\n\n\u00a0projectNo\n\nExpand Post",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-04-26T16:31:57.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @sannycse (Customer)\u200b\u00a0, Just a friendly follow-up. Do you still need help? Please let us know.",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "How to deploy or create mlflow model as docker image with REST api endpoint within databricks?",
        "Question_creation_date": "2022-3-20T8:39Z",
        "Question_tag": [
            "Model",
            "Docker image",
            "Mlflow Model Image",
            "REST Api Endpoint",
            "Azure databricks",
            "Azure containers",
            "Mlflow Model",
            "Databricks notebook",
            "MLFlow"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001sJtzqCAC/how-to-deploy-or-create-mlflow-model-as-docker-image-with-rest-api-endpoint-within-databricks",
        "Question_upvote_count": 0,
        "Question_answer_count": 2,
        "Question_view_count": 764,
        "Question_has_accepted_answer": true,
        "Question_body": "Is it possible to create mlflow model as a docker image with REST api endpoint\u00a0and use it for inferencing within databricks or hosting the image in azure container instances?",
        "Answers": [
            {
                "Answer_creation_date": "2022-04-22T07:20:06.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "@Vijeth (Customer)\u200b\u00a0, Hey there, we are definitely interested in making model serving easier and simpler on Databricks. There are some useful product features coming down the line - contact me at bilal dot aslam at databricks dot com if you are interested in learning more!",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-04-26T09:50:21.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @Vijeth (Customer)\u200b\u00a0\u200b , Just a friendly follow-up. Did you follow @bilal.aslam (Databricks)\u200b\u00a0's suggestion? Please let us know.",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "How to PREVENT mlflow's autologging from logging ALL runs?",
        "Question_creation_date": "2022-4-2T16:09Z",
        "Question_tag": [
            "MLFlow",
            "Autologging",
            "Jupyternotebook",
            "Mlflow's Autologging"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001uMXL9CAO/how-to-prevent-mlflows-autologging-from-logging-all-runs",
        "Question_upvote_count": 1,
        "Question_answer_count": 2,
        "Question_view_count": 283,
        "Question_has_accepted_answer": true,
        "Question_body": "I am logging runs from jupyter notebook. the cells which has `mlflow.sklearn.autlog()` behaves as expected. but, the cells which has .fit() method being called on sklearn's estimators are also being logged as runs without explicitly mentioning `mlflow.sklearn.autlog()` on top. How do I have mlflow log only the ones I call `mlflow.xxxx.autlog()` or by doing `with mlflow.star_run()`?",
        "Answers": [
            {
                "Answer_creation_date": "2022-05-02T23:47:27.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "You can turn it off.\n\nhttps://docs.databricks.com/applications/mlflow/databricks-autologging.html#disable-databricks-autologging",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-05-18T12:35:40.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "https://apkmiz.com/showbox-apk-old-latest-version-android/",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "Databricks runtime from docker hub image",
        "Question_creation_date": "2022-2-11T11:09Z",
        "Question_tag": [
            "Docker Hub Image",
            "Databricks Runtime",
            "Docker image"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001lble1CAA/databricks-runtime-from-docker-hub-image",
        "Question_upvote_count": 1,
        "Question_answer_count": 3,
        "Question_view_count": 394,
        "Question_has_accepted_answer": true,
        "Question_body": "Hello,\n\n\u00a0\n\nare databricks runtimes from\u00a0docker hub ( https://hub.docker.com/r/databricksruntime/standard )\u00a0same\u00a0as actual runtimes inside Databricks? I mean when we made our own docker image from databricksruntime/standard will be there same dependencies, versions etc as in runtime inside databricks? We need to archive older runtimes in bank environment and be able to run old notebooks with exactly same runtime.\n\n\u00a0\n\nAnd another question about runtime\u2019s version. If I use my docker container in cluster creation with for example runtime version 6.6 but in setup I also select different version in column Databricks runtime version what will happened? Will be column Databricks runtime version ignored?",
        "Answers": [
            {
                "Answer_creation_date": "2022-03-11T11:22:49.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "just build own using in docker FROM databricksruntime/standard:10.x and than it will work correctly with any 10 non beta version (although better test before changing cluster runtime)",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-03-21T19:16:02.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "use the\u00a0 9.x tag for an image that will target a cluster with runtime version Databricks Runtime 9.0. This is for an example. so basically you are running a image based on the version selected.so it better match.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-04-11T21:20:35.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @michaelh (Customer)\u200b\u00a0,\n\n\u00a0\n\nJust checking if you still need help with this or not any more? please let us know",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "Databricks Runtime 10.4 LTS - AnalysisException: No such struct field id in 0, 1 after upgrading",
        "Question_creation_date": "2022-3-12T17:34Z",
        "Question_tag": [
            "Analysisexception",
            "Runtime 10.4",
            "Databricks Runtime",
            "LTS",
            "Bug",
            "Struct",
            "Data Bricks"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001r3HwECAU/databricks-runtime-104-lts-analysisexception-no-such-struct-field-id-in-0-1-after-upgrading",
        "Question_upvote_count": 2,
        "Question_answer_count": 11,
        "Question_view_count": 146,
        "Question_has_accepted_answer": true,
        "Question_body": "Hello,\n\nWe are working to migrate to databricks runtime 10.4 LTS from 9.1 LTS but we're running into weird behavioral issues. Our existing code works up until runtime 10.3 and in 10.4 it stopped working.\n\n\u00a0\n\nProblem:\n\nWe have a nested json file that we are flattening into a spark data frame using the code below:\n\nadaccountsdf = df.withColumn('Exp_Organizations', F.explode(F.col('organizations.organization')))\\\n                  .withColumn('Exp_AdAccounts', F.explode(F.col('Exp_Organizations.ad_accounts')))\\\n                  .select(F.col('Exp_Organizations.id').alias('organizationId'),\n                                  F.col('Exp_Organizations.name').alias('organizationName'),\n                                  F.col('Exp_AdAccounts.id').alias('adAccountId'),\n                                  F.col('Exp_AdAccounts.name').alias('adAccountName'),\n                                  F.col('Exp_AdAccounts.timezone').alias('timezone'))\n\nNow when we query the dataframe it works when we do the following selects (hid results due to confidentiality):\n\ndisplay(adaccountsdf.select(\"*\"))\n\u00a0\nOR\n\u00a0\ndisplay(adaccountsdf)\n\nWhen I display the schema of the dataframe we get the following:\n\nroot\n |-- organizationId: string (nullable = true)\n |-- organizationName: string (nullable = true)\n |-- adAccountId: string (nullable = true)\n |-- adAccountName: string (nullable = true)\n |-- timezone: string (nullable = true)\n\nso everything looks like it should. The moment we start selecting the last 3 fields(adAccountId, adAccountName and timezone) we get the following error:\n\nHowever when we select a single column it works fine:\n\n\u00a0\n\nDoes anyone know why this is happening? It's a very strange error that only shows up in databricks runtime 10.4. All previous runtimes incl 10.3, 10.2,10.1 and 9.1 LTS work fine. The issue seems to be caused by using the explode function on an already exploded column in the dataframe.\n\n\u00a0\n\nUPDATE:\n\nFor some reason when I run adaccountsdf.cache() before I run my select statements the issue disappears. Would still like to know what's causing this issue in runtime 10.4 but not the other ones.",
        "Answers": [
            {
                "Answer_creation_date": "2022-04-13T18:42:45.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @Emiel_Smeenk (Customer)\u200b\u00a0,\n\n\u00a0\n\nThis guide helps you migrate your Azure Databricks workloads to the latest version of Databricks Runtime 10.x.\n\n\u00a0\n\nDatabricks recommends that you migrate your workloads to a supported Databricks Runtime LTS version from that version\u2019s most recent supported LTS version.\n\n\u00a0\n\nTherefore, this article focuses on migrating workloads from\u00a0Databricks Runtime 9.1 LTS\u00a0to\u00a0Databricks Runtime 10.4 LTS.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-04-20T15:59:22.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "It seems like the issue was miraculously resolved. I did not make any code changes but everything is now running as expected.\n\nMaybe the latest runtime 10.4 fix released on April 19th also resolved this issue unintentionally.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-04-21T10:55:47.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "@Emiel_Smeenk (Customer)\u200b\u00a0\n\nWe were facing the same issue and suddenly 2022-Apr-20 onwards it resolved itself.\n\nQuestion:- Is there any website where I can see/track these \"patches\"?\n\nEdit: Added Question.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-04-22T06:34:00.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @Nirupam (Customer)\u200b\u00a0 and @Emiel_Smeenk (Customer)\u200b\u00a0, This page lists maintenance updates issued for Databricks Runtime releases.\n\n\u00a0\n\nApril 19, 2022 - Maintenance updates\n\n\u00a0\n\nWe upgraded Java AWS SDK from version 1.11.655 to 1.12.1899.\nWe fixed an issue with notebook-scoped libraries not working in batch streaming jobs.\n[SPARK-38616][SQL] Keep track of SQL query text in Catalyst TreeNode\nOperating system security updates.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-04-26T10:48:48.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @Nirupam (Customer)\u200b\u00a0, Just a friendly follow-up. Do you still need help, or does my response help you to find the solution? Please let us know.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-04-26T18:45:53.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "@Kaniz Fatma (Databricks)\u200b\u00a0\n\nYour answer suffices my query. Thanks!\n\nIn addition, for fellow developers, I later noticed that these release notes are also available on the home screen of your Databricks workspace.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-04-26T19:45:16.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @Nirupam (Customer)\u200b\u00a0, Thank you for the update and the valuable message for our community members. Since my answer suffices your query, would you like to mark my answer as the best?",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-04-26T19:54:28.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "@Kaniz Fatma (Databricks)\u200b\u00a0I did not ask the original question.\n\n@Emiel_Smeenk (Customer)\u200b\u00a0had asked and answered his own question stating that the issue was fixed on its own (probably due to latest patch).",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-04-26T20:03:39.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "No worries @Nirupam (Customer)\u200b\u00a0. Either of you can mark the best answer. As the initial question was answered by @Emiel_Smeenk (Customer)\u200b\u00a0 himself, you can mark his answer as the best.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-04-26T20:16:15.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Issue resolved on its own so selected that as the best answer for this post.\n\n\u00a0\n\nThanks,\n\nEmiel",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-04-26T20:17:30.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Awesome. Thank you @Emiel_Smeenk (Customer)\u200b\u00a0\ud83d\ude0a .",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "How to deploy mlflow models to sagemaker endpoints where sagemaker refers the private docker registry?",
        "Question_creation_date": "2022-2-4T13:18Z",
        "Question_tag": [
            "Model Deployment",
            "Sagemaker Endpoints",
            "Private Docker Registry",
            "Model Serving",
            "ECR",
            "Docker",
            "MLFlow"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001kWp04CAC/how-to-deploy-mlflow-models-to-sagemaker-endpoints-where-sagemaker-refers-the-private-docker-registry",
        "Question_upvote_count": 0,
        "Question_answer_count": 5,
        "Question_view_count": 317,
        "Question_has_accepted_answer": false,
        "Question_body": "Is it possible to deploy the mlflow model to a sagemaker endpoint where the image URL is not referring to an image in ECR but the image is actually present in a private docker registry?",
        "Answers": [
            {
                "Answer_creation_date": "2022-03-09T11:59:51.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @Maverick (Customer)\u200b\u00a0, Can you pull the image from the private docker registry using kubernetes?",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-03-09T13:40:51.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "@Kaniz Fatma (Databricks)\u200b\u00a0: Yes. Even when I am deploying the model using sagemaker studio, I am able to pull the image from private docker registry because the VPC settings for image and containers is allowed via Sagemaker API. But I can't see the same options available for mlflow-sagemaker API. The only option is VPC settings for the region where model need to be deployed, but in the end it needs the access to ECR repo of the same region.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-04-17T17:56:19.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "@Maverick (Customer)\u200b\u00a0, this to create the endpoint.\n\nalso, check this out - https://github.com/mlflow/mlflow/blob/0fa849ad75e5733bf76cc14a4455657c5c32f107/mlflow/sagemaker/__init__.py#L361",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-04-25T11:05:51.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @Maverick (Customer)\u200b\u00a0, Did @Atanu (Databricks)\u200b\u00a0's response help answer your question?",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-04-25T13:29:44.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "@Kaniz Fatma (Databricks)\u200b\u00a0: Unfortunately No. The above implementation always assumes that image is hosted in ECR repository.",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "Debugging!",
        "Question_creation_date": "2021-7-20T17:23Z",
        "Question_tag": [
            "Python",
            "Debugging",
            "Notebooks",
            "Machine Learning",
            "Deep learning"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001HKIByCAP/debugging",
        "Question_upvote_count": 1,
        "Question_answer_count": 4,
        "Question_view_count": 203,
        "Question_has_accepted_answer": true,
        "Question_body": "Hi ML folks,\n\nWe are using Databricks to train deep learning models. The code, however, has a complex structure of classes. This would work fine in a perfect bug-free world like Alice in Wonderland.\n\nDebugging in Databricks is awkward. We ended up doing all the development on local machines and when the code is \"mature\" we start playing with Databricks + MLFlow to train the model. We use Azure not only for databricks, but also for data. However, we ended up having a \"security hole\" with this approach. IT staff want to remove this type of permission from local machines, which will create a challenge for the ML team...\n\nGoogle search didn't do much to find a good reference that works as a guideline for doing \"complex\" python-based model development and training in databricks. Any suggestion is welcome!\n\nCheers",
        "Answers": [
            {
                "Answer_creation_date": "2021-09-02T14:04:06.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @\u00a0MCosta! My name is Kaniz, and I'm the technical moderator here. Great to meet you, and thanks for your question! Let's see if your peers on the Forum have an answer to your question first. Or else I will follow up shortly with a response.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-09-09T20:48:08.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Unfortunately right now we are limited to the Notebooks for Python code. We are looking into other options like having hosted IDEs, but there is no release date yet. I would suggest for now trying to use an IDE on local as you have been doing and then syncing to the notebooks using the Repos gIt functionality. I hope you will be bale to develop locally!",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-09-10T12:10:15.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hello @MCosta (Customer)\u200b\u00a0 thanks for posting this question. We are actively looking into how to make this a better experience for you. Can you please drop me a line at bilal dot aslam at databricks dot com and I will be happy to set up a call.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-09-12T12:59:50.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "@MCosta (Customer)\u200b\u00a0, if you are using python, I assume that you have a case where you have a complex udf and you want to understand what happens in every line there, perhaps even in complex call stacks. Is that correct?",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "Cluster terminated.Reason:Unexpected launch failure",
        "Question_creation_date": "2022-2-12T8:45Z",
        "Question_tag": [
            "Databricks Runtime",
            "Message Cluster",
            "Error Message"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001lnqHLCAY/cluster-terminatedreasonunexpected-launch-failure",
        "Question_upvote_count": 2,
        "Question_answer_count": 3,
        "Question_view_count": 594,
        "Question_has_accepted_answer": true,
        "Question_body": "Using Community Edition. Cluster starts up and immediately terminates with the following error message.\n\n\u00a0\n\nWhy would that be?\n\n\u00a0\n\nMessage\n\nCluster terminated.Reason:Unexpected launch failure\n\nAn unexpected error was encountered while setting up the cluster. Please retry and contact Databricks if the problem persists.\n\n\u00a0\n\nInternal error message: com.databricks.backend.manager.instance.InstanceHolder$PlacementLockTimeout: Could not acquire placement lock context = findExistingInstanceById.",
        "Answers": [
            {
                "Answer_creation_date": "2022-03-14T06:37:41.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @msoczka (Customer)\u200b\u00a0\n\n\u00a0\n\nWe have some internal service interruptions due to which we had this issue. Our engineering has applied the fix and the cluster startup works as expected.\n\n\u00a0\n\nSincerely apologies for the inconvenience caused here.\n\n\u00a0\n\nRegards,\n\nDarshan\n\nExpand Post",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-04-17T08:52:28.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi, I am getting the same error. Can @darshan.bargal (Databricks)\u200b\u00a0 please help?",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-04-17T13:44:52.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Same, Cluster failed to start.\n\n$HolderPlacementLockTimeout = terminatedinstance error. Any help so far?",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "AutoML : data set for problem type \"Classification\"",
        "Question_creation_date": "2022-3-8T23:48Z",
        "Question_tag": [
            "MLFlow",
            "Hi",
            "AutoML Experiment"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001qd6f3CAA/automl-data-set-for-problem-type-classification",
        "Question_upvote_count": 0,
        "Question_answer_count": 1,
        "Question_view_count": 54,
        "Question_has_accepted_answer": true,
        "Question_body": "HI,\n\n\u00a0\n\nI am working on AutoML Experiment.\u00a0Could you plz help me with data set for problem type \"Classification\"\n\n\u00a0\n\nRegards.",
        "Answers": [
            {
                "Answer_creation_date": "2022-04-09T23:18:05.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "There are a lot of datasets available in /databricks-datasets/ that you can look through. You'll have to turn them into a table so that you can access them in automl. There are datasets associated with the spark definitive guide and learning spark 2, which will add some additional guidance.",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "ColumnTransformer not fitted after sklearn Pipeline loaded from Mlflow",
        "Question_creation_date": "2021-10-2T20:20Z",
        "Question_tag": [
            "Sklearn",
            "Sklean Pipeline",
            "Model",
            "MLFlow"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001SHcStCAL/columntransformer-not-fitted-after-sklearn-pipeline-loaded-from-mlflow",
        "Question_upvote_count": 0,
        "Question_answer_count": 2,
        "Question_view_count": 727,
        "Question_has_accepted_answer": true,
        "Question_body": "I am building a machine learning model using sklearn Pipeline which includes a ColumnTransformer as a preprocessor before the actual model. Below is the code how the pipeline is created.\n\ntransformers = []\nnum_pipe = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())\n])\ntransformers.append(('numerical', num_pipe, num_cols))\ncat_pipe = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('ohe', OneHotEncoder(handle_unknown='ignore'))\n])\ntransformers.append(('categorical', cat_pipe, cat_cols))\npreprocessor = ColumnTransformer(transformers, remainder='passthrough')\nmodel = Pipeline([\n  ('prep', preprocessor),\n  ('clf', XGBClassifier())\n])\n\nI am using\u00a0Mlflow\u00a0to log the model artifact as sklearn model after it is fitted on training data.\n\nmodel.fit(X, y)\nmlflow.sklearn.log_model(model, model_uri)\n\nWhen I tried to load the model from mlflow for scoring though, I got the error \"This ColumnTransformer instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.\"\n\nrun_model = mlflow.sklearn.load_model(model_uri)\nrun_model.predict(X_pred)\n\nI also ran check_is_fitted\u00a0on the second step of the Pipeline which is the xgboost model itself after loaded from mlflow and it is NOT fitted either.\n\n\u00a0\n\nIs Mlflow not compatible with sklearn Pipeline with multiple steps?",
        "Answers": [
            {
                "Answer_creation_date": "2021-11-03T06:00:39.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @\u00a0Nasreddin! My name is Kaniz, and I'm the technical moderator here. Great to meet you, and thanks for your question! Let's see if your peers in the community have an answer to your question first. Or else I will get back to you soon. Thanks.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-11-03T21:32:21.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "To directly answer your question: yes, MLFlow is compatible with sklearn Pipelines, our own AutoML glassbox generator uses a (nearly identical) Pipeline for xgboost training.\n\n\u00a0\n\nI have attached a sample notebook that I was able to run end-to-end to train your exact Pipeline on. the adult-income dataset, log it to MLFlow, load it back from MLFlow, and use it to predict values. I also included some calls to check_is_fitted to see that\n\n\u00a0\n\nit's not fitted until you run model.fit\nthen everything in the Pipeline is fitted\nthe. logged model is fitted\n\n\u00a0\n\nHope this helps as a guide, let me know if you have any other questions or issues.\n\nExpand Post",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "SQL Issues with 10.0 runtime",
        "Question_creation_date": "2021-9-31T7:40Z",
        "Question_tag": [
            "Sql",
            "Weird Thing",
            "SQL Issues",
            "10.0",
            "Databricks Runtime"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001Rnl4QCAR/sql-issues-with-100-runtime",
        "Question_upvote_count": 0,
        "Question_answer_count": 3,
        "Question_view_count": 132,
        "Question_has_accepted_answer": true,
        "Question_body": "I was testing my sqls with new 10.0 runtime and found some interesting/weird thing. The same sql with explode function fails for some scenarios in 10.0! Could not figure out yet the reason",
        "Answers": [
            {
                "Answer_creation_date": "2021-10-31T18:08:43.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Update:\n\nAfter some trail and errors, found that pos_explode works better in these scenarios. I feel like there is some parsing issue. Not sure though",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-11-01T17:28:33.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hello, @yadsmc (Customer)\u200b. My name is Piper and I'm a community moderator for Databricks. It's nice to meet you and thank you for your question. Let us see what the community has to say, otherwise, we will get back.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-11-12T12:05:03.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "@yadsmc (Customer)\u200b\u00a0 if the issue persists, please email me at bilal dot aslam at databricks dot com. I would like to get to the root of this issue. It",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "Trigger.AvailableNow does not support maxOffsetsPerTrigger in Databricks runtime 10.3",
        "Question_creation_date": "2022-2-29T3:54Z",
        "Question_tag": [
            "Databricks Runtime",
            "Spark Stream Job",
            "Support",
            "Trigger.AvailableNow",
            "Job Run",
            "Azure"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001oFzYjCAK/triggeravailablenow-does-not-support-maxoffsetspertrigger-in-databricks-runtime-103",
        "Question_upvote_count": 0,
        "Question_answer_count": 3,
        "Question_view_count": 361,
        "Question_has_accepted_answer": true,
        "Question_body": "Hello,\n\n\u00a0\n\nI ran a spark stream job to ingest data from kafka to test Trigger.AvailableNow.\n\n\u00a0\n\nWhat's environment the job run ?\n\n1: Databricks runtime 10.3\n\n2: Azure cloud\n\n3: 1 Driver node + 3 work nodes( 14GB, 4core)\n\n\u00a0\n\nval maxOffsetsPerTrigger = \"500\"\n\n\u00a0\n\nspark.conf.set(\"spark.databricks.delta.autoCompact.enabled\",\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"auto\")\n\n\u00a0\n\n...\n\n\u00a0\n\nval rdf = spark\n\n\u00a0.readStream\n\n\u00a0.format(\"kafka\")\n\n\u00a0.option(\"kafka.security.protocol\", \"SASL_PLAINTEXT\")\n\n\u00a0.option(\"kafka.sasl.mechanism\",\u00a0\u00a0\"SCRAM-SHA-512\")\n\n\u00a0.option(\"kafka.sasl.jaas.config\",\u00a0\"<>\")\n\n\u00a0.option(\"kafka.bootstrap.servers\", servers)\n\n\u00a0.option(\"subscribe\",\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0topic)\n\n\u00a0.option(\"startingOffsets\",\u00a0\u00a0\u00a0\u00a0\u00a0\"earliest\")\u00a0\n\n\u00a0.option(\"maxOffsetsPerTrigger\",\u00a0\u00a0maxOffsetsPerTrigger)\n\n\u00a0.load()\n\n\u00a0\n\nrdf.writeStream\n\n\u00a0.format(\"delta\")\n\n\u00a0.outputMode(\"append\")\n\n\u00a0.option(\"mergeSchema\", \"true\")\n\n\u00a0.option(\"checkpointLocation\", ckpPath)\n\n\u00a0.trigger(Trigger.AvailableNow)\n\n\u00a0.start(tabPath)\n\n\u00a0.awaitTermination()\n\n\u00a0\n\nWhat I expected to see:\n\n\u00a0\n\n1: The spark stream job can read all data from Kafka and then quit\n\n2: The spark stream will apply maxOffsetsPerTrigger for each micro batch\n\n\u00a0\n\n\u00a0\n\nWhat I see:\n\n\u00a0\n\nthe Kafka topic has four partitions, it takes 5 hours to generate 4 huge data files.\n\n\u00a0\n\n\u00a0\n\npart-00000-89afacf1-f2e6-4904-b313-080d48034859-c000.snappy.parquet\n\n3/25/2022, 9:50:48 PM\n\nHot (Inferred)\n\nBlock blob\n\n14.39 GiB\n\nAvailable\n\n\u00a0\n\n\u00a0\n\npart-00001-cf932ee2-8535-4dd6-9dab-e94b9292a438-c000.snappy.parquet\n\n3/25/2022, 6:15:36 PM\n\nHot (Inferred)\n\nBlock blob\n\n14.38 GiB\n\nAvailable\n\n\u00a0\n\n\u00a0\n\npart-00002-7d481793-10dc-4739-8c20-972cb6f18fd6-c000.snappy.parquet\n\n3/25/2022, 6:15:22 PM\n\nHot (Inferred)\n\nBlock blob\n\n14.41 GiB\n\nAvailable\n\n\u00a0\n\n\u00a0\n\npart-00003-17c88f26-f152-4b27-80cf-5ae372662950-c000.snappy.parquet\n\n3/25/2022, 9:48:14 PM\n\nHot (Inferred)\n\nBlock blob\n\n14.43 GiB\n\nAvailable",
        "Answers": [
            {
                "Answer_creation_date": "2022-03-28T05:19:47.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "We\u2019re constantly working to improve our features based on feedback like this, so I\u2019ll be sure to share your request to the API product team.\n\n\u00a0\n\nusps liteblue",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-03-29T03:57:11.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "@Eulaliasw (Customer)\u200b\u00a0, thanks for help. This issue has been fixed in databricks 10.4 and spark 3.3.\n\n\u00a0\n\n[SPARK-36649]\u00a0[SQL] Support\u00a0Trigger.AvailableNow\u00a0on Kafka data source\n\n\u00a0\n\nhttps://docs.databricks.com/release-notes/runtime/10.4.html\n\n\u00a0\n\n\u00a0\n\n\u00a0\n\nExpand Post",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-03-29T12:01:43.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "You'd be better off with 1 node with 12 cores than 3 nodes with 4 each. You're shuffles are going to be much better one 1 machine.",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "Implementation of a stable Spark Structured Streaming Application",
        "Question_creation_date": "2022-1-6T21:26Z",
        "Question_tag": [
            "Standard",
            "Databricks Runtime",
            "Storage account",
            "Spark Structured Streaming Application",
            "Spark",
            "Azure data factory"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001gHFe8CAG/implementation-of-a-stable-spark-structured-streaming-application",
        "Question_upvote_count": 0,
        "Question_answer_count": 5,
        "Question_view_count": 101,
        "Question_has_accepted_answer": true,
        "Question_body": "Hi folks,\n\n\u00a0\n\nI have an issue. It's not critical but's annoying.\n\nWe have implemented a Spark Structured Streaming Application.\n\nThis application will be triggered wire Azure Data Factory (every 8 minutes). Ok, this setup sounds a little bit weird and it's not really Streaming, agreed. But the source system is not really real time and we would like to implement a Streaming POC, take a look into deep regarding the technics. That's all.\n\n\u00a0\n\nSo, this \"Streaming Notebook\" runs 24/7. Mostly in a stable way. But sometimes one single load runs into a TimeoutException like this:\n\njava.util.concurrent.TimeoutException: Stream Execution thread for stream [id = 26c3f28c-9d17-486a-81be-df418c42cd74, runId = d30a8fe8-3fed-4475-8233-4577b775bb19] failed to stop within 15000 milliseconds (specified by spark.sql.streaming.stopTimeout). See the cause on what was being executed in the streaming query thread.\n\n\u00a0\n\nThis exception is clear. This load is trying to get access into the storage account, where checkpoint is located. Timeout occurs because another load has not finshed the work and locked the checkpoint, yet. In general this Streaming Application doesn't longer take than 1-2 minutes.\n\nBut some edge cases need more than 14-15 minutes and blocks other loads that will be started during this long run.\n\n\u00a0\n\nI did some investigation into driver logs and found a strange behavior into the log4j (see attached log4j_snippet.log, 99 sec. duration for what?).\n\nIn these edge cases I have a lot of entries like this.\n\nClusterLoadAvgHelper... what does it mean? Any ideas?\n\n\u00a0\n\nLike I sad, my own small business logic and connect to Azure SQL Database (as sink) does only take 1-2 minutes. In this edge cases where the whole processing time runs to 14-15 minutes more than 10 minutes are necessary for this ClusterLoadAvgHelper stuff.\n\n\u00a0\n\nCurrently I have no idea why my cluster is running amok.\n\n\u00a0\n\nLike I sad at the beginning it's not critical, we don't miss any data into the SQL Database. But it's annoying :).\n\n\u00a0\n\nAny ideas would be great.\n\n\u00a0\n\nThanks in advance,\n\nMarkus\n\n\u00a0\n\n\u00a0\n\nData Source: Auto Loader mechanism like this (https://docs.databricks.com/spark/latest/structured-streaming/auto-loader.html)\n\nSink: Azure SQL Database\n\n\u00a0\n\nSetup\n\nLanguage: Python3\n\nDatabricks Runtime: 8.3 (includes Apache Spark 3.1.1, Scala 2.12)\n\nDriver/Worker type: Standard_E4ds_v4\n\nCluster mode: Standard\n\nMin Worker: 1 / Max Worker: 10\n\n\u00a0\n\n--- Streaming Notebook Snippet ON ---\n\n# (1) Init Streaming Data Frame\n\nstreaming_df = spark.readStream.format(\"cloudFiles\") \\\n\n\u00a0\u00a0\u00a0.option(\"cloudFiles.format\", extension) \\\n\n\u00a0\u00a0\u00a0.schema(schema) \\\n\n\u00a0\u00a0\u00a0.load(streaming_path) # locate to Storage Account\n\n\u00a0\n\n# (2) Start Streaming\n\nquery = (\n\n\u00a0\u00a0\u00a0streaming_df\n\n\u00a0\u00a0\u00a0.writeStream\n\n\u00a0\u00a0\u00a0.foreachBatch(process_batch_for_streaming) # sink into Azure SQL Database\n\n\u00a0\u00a0\u00a0.trigger(once = True)\n\n\u00a0\u00a0\u00a0.option(\"checkpointLocation\", checkpoint_path)\n\n\u00a0\u00a0\u00a0.start()\n\n)\n\n-- Streaming Notebook Snippet OFF ---",
        "Answers": [
            {
                "Answer_creation_date": "2022-02-07T13:22:21.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @fsm (Customer)\u200b\u00a0! My name is Kaniz, and I'm the technical moderator here. Great to meet you, and thanks for your question! Let's see if your peers in the community have an answer to your question first. Or else I will get back to you soon. Thanks.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-02-10T08:39:22.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi,\n\nI have investigated a little bit more. Currently I think the ClusterLoadAvgHelper behaviour is not the main problem. This behaviour hides this one:\n\n\u00a0\n\n22/02/08 22:52:09 WARN TaskSetManager: Lost task 0.0 in stage 197663.0 (TID 4615729) (10.141.64.7 executor 410): com.microsoft.sqlserver.jdbc.SQLServerException: Connection timed out (Read failed)\n\n\u00a0\u00a0\u00a0at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDriverError(SQLServerException.java:234)\n\n\u00a0\u00a0\u00a0at com.microsoft.sqlserver.jdbc.SimpleInputStream.getBytes(SimpleInputStream.java:352)\n\n\u00a0\u00a0\u00a0at com.microsoft.sqlserver.jdbc.DDC.convertStreamToObject(DDC.java:796)\n\n\u00a0\u00a0\u00a0at com.microsoft.sqlserver.jdbc.ServerDTVImpl.getValue(dtv.java:3777)\n\n\u00a0\u00a0\u00a0at com.microsoft.sqlserver.jdbc.DTV.getValue(dtv.java:247)\n\n\u00a0\u00a0\u00a0at com.microsoft.sqlserver.jdbc.Column.getValue(Column.java:190)\n\n\u00a0\u00a0\u00a0at com.microsoft.sqlserver.jdbc.SQLServerResultSet.getValue(SQLServerResultSet.java:2054)\n\n\u00a0\u00a0\u00a0at com.microsoft.sqlserver.jdbc.SQLServerResultSet.getValue(SQLServerResultSet.java:2040)\n\n\u00a0\u00a0\u00a0at com.microsoft.sqlserver.jdbc.SQLServerResultSet.getString(SQLServerResultSet.java:2511)\n\n\u00a0\u00a0\u00a0at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$makeGetter$12(JdbcUtils.scala:445)\n\n\u00a0\u00a0\u00a0at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$makeGetter$12$adapted(JdbcUtils.scala:443)\n\n\u00a0\u00a0\u00a0at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anon$1.getNext(JdbcUtils.scala:353)\n\n\u00a0\u00a0\u00a0at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anon$1.getNext(JdbcUtils.scala:335)\n\n\u00a0\n\nI will do further checks into Azure SQL Database (stats, re-org run..) combined with sql-spark-connector. Maybe there is a conflict between streaming and stats/reorg.\n\n\u00a0\n\nFurthermore I will extend log4j config to get more details about\n\ncom.microsoft.sqlserver.jdbc.*\n\n\u00a0\n\nThanks,\n\nMarkus\n\nExpand Post",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-03-22T16:33:36.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @fsm (Customer)\u200b\u00a0,\n\n\u00a0\n\nHave you try to run your streaming job with a different trigger interval? Just to isolate the issue. If you use DBR 10.1+ there is a new trigger that was introduced in this DBR version.\n\n\u00a0\n\nThe new trigger is \"trigger.AvailableNow\". It is like Trigger.Once, which processes all available data then stops the query. However, Trigger.AvailableNow provides better scalability, because data can be processed in multiple batches instead of one.\u00a0\n\ndocs here https://docs.databricks.com/release-notes/runtime/10.1.html#triggeravailablenow-for-delta-source-streaming-queries\n\n\u00a0\n\nExpand Post",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-02-10T14:57:21.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "@fsm - Thank you for the extra information! :)",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-03-15T01:24:49.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "@fsm (Customer)\u200b\u00a0 Looks like the spark driver was stuck. It will be good to capture the thread dump of the Spark driver to understand what operation is stuck",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "Need help with this python import error.",
        "Question_creation_date": "2022-1-11T18:22Z",
        "Question_tag": [
            "Python Import Error",
            "Python",
            "Databricks Runtime",
            "LTS",
            "New Python Import Error"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001h3lK6CAI/need-help-with-this-python-import-error",
        "Question_upvote_count": 1,
        "Question_answer_count": 4,
        "Question_view_count": 237,
        "Question_has_accepted_answer": true,
        "Question_body": "I am using databricks runtime 9.1 LTS ML and I got this error when I tried to import Scikit Learn package. I got the following error message:\n\n\u00a0\n\n\u00a0\n\nTypeError                                 Traceback (most recent call last)\n<command-181041> in <module>\n      1 # Scikit Learn ML Library\n----> 2 from sklearn.metrics import *\n      3 from sklearn.preprocessing import LabelEncoder\n      4 from sklearn.preprocessing import MinMaxScaler, StandardScaler, PowerTransformer\n      5 from sklearn.model_selection import train_test_split, TimeSeriesSplit\n\u00a0\n/databricks/python_shell/dbruntime/PythonPackageImportsInstrumentation/__init__.py in import_patch(name, globals, locals, fromlist, level)\n    156             # Import the desired module. If you're seeing this while debugging a failed import,\n    157             # look at preceding stack frames for relevant error information.\n--> 158             original_result = python_builtin_import(name, globals, locals, fromlist, level)\n    159 \n    160             is_root_import = thread_local._nest_level == 1\n\u00a0\n/usr/lib/python3.8/importlib/_bootstrap.py in _find_and_load(name, import_)\n\u00a0\n/usr/lib/python3.8/importlib/_bootstrap.py in _find_and_load_unlocked(name, import_)\n\u00a0\n/usr/lib/python3.8/importlib/_bootstrap.py in _load_unlocked(spec)\n\u00a0\n/usr/lib/python3.8/importlib/_bootstrap.py in _load_backward_compatible(spec)\n\u00a0\n/databricks/python_shell/dbruntime/PostImportHook.py in load_module(self, fullname)\n    214     def load_module(self, fullname):\n    215         try:\n--> 216             module = self.loader.load_module(fullname)\n    217             notify_module_loaded(module)\n    218         except (ImportError, AttributeError):\n\u00a0\n/databricks/python/lib/python3.8/site-packages/sklearn/__init__.py in <module>\n     62 else:\n     63     from . import __check_build\n---> 64     from .base import clone\n     65     from .utils._show_versions import show_versions\n     66 \n\u00a0\n/databricks/python_shell/dbruntime/PythonPackageImportsInstrumentation/__init__.py in import_patch(name, globals, locals, fromlist, level)\n    156             # Import the desired module. If you're seeing this while debugging a failed import,\n    157             # look at preceding stack frames for relevant error information.\n--> 158             original_result = python_builtin_import(name, globals, locals, fromlist, level)\n    159 \n    160             is_root_import = thread_local._nest_level == 1\n.....\n....\n....\n/databricks/python/lib/python3.8/site-packages/sklearn/externals/joblib/externals/cloudpickle/__init__.py in <module>\n      1 from __future__ import absolute_import\n      2 \n----> 3 from .cloudpickle import *\n      4 \n      5 __version__ = '0.8.0'\n\u00a0\n/databricks/python_shell/dbruntime/PythonPackageImportsInstrumentation/__init__.py in import_patch(name, globals, locals, fromlist, level)\n    156             # Import the desired module. If you're seeing this while debugging a failed import,\n    157             # look at preceding stack frames for relevant error information.\n--> 158             original_result = python_builtin_import(name, globals, locals, fromlist, level)\n    159 \n    160             is_root_import = thread_local._nest_level == 1\n\u00a0\n/databricks/python/lib/python3.8/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py in <module>\n    150 \n    151 \n--> 152 _cell_set_template_code = _make_cell_set_template_code()\n    153 \n    154 \n\u00a0\n/databricks/python/lib/python3.8/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py in _make_cell_set_template_code()\n    131         )\n    132     else:\n--> 133         return types.CodeType(\n    134             co.co_argcount,\n    135             co.co_kwonlyargcount,\n\u00a0\nTypeError: an integer is required (got type bytes)",
        "Answers": [
            {
                "Answer_creation_date": "2022-02-11T21:54:34.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @qyu (Customer)\u200b\u00a0! My name is Kaniz, and I'm the technical moderator here. Great to meet you, and thanks for your question! Let's see if your peers in the community have an answer to your question first. Or else I will get back to you soon. Thanks.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-02-14T14:19:54.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "@qyu (Customer)\u200b\u00a0 I believe the error is coming while you are executing any particular code on notebook.Could you please check the python version?",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-02-14T23:43:17.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "@Atanu (Databricks)\u200b\u00a0I am using databricks runtime 9.1ML LTS and python version is 3.8.10\n\n\u00a0\n\nI am only just running import statement\n\nfrom sklearn.metrics import *\nfrom sklearn.preprocessing import LabelEncoder\n\n\u00a0\n\nExpand Post",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-02-17T13:58:13.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Ok, if this library is included in this DBR , can you please check https://docs.databricks.com/release-notes/runtime/9.1ml.html . If not. please check if you are installing this library from cluster with all dependencies . Thanks.",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "How to infer the online feature store table via an mlflow registered model, which is deployed to a sagemaker endpoint?",
        "Question_creation_date": "2022-2-4T13:42Z",
        "Question_tag": [
            "Model Deployment",
            "Aws",
            "Sagemaker Endpoint",
            "Online Feature Store Table"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001kWpi7CAC/how-to-infer-the-online-feature-store-table-via-an-mlflow-registered-model-which-is-deployed-to-a-sagemaker-endpoint",
        "Question_upvote_count": 0,
        "Question_answer_count": 10,
        "Question_view_count": 208,
        "Question_has_accepted_answer": true,
        "Question_body": "Can an mlflow registered model automatically infer the online feature store table, if that model is trained and logged via a databricks feature store table and the table is pushed to an online feature store (like AWS RDS)?",
        "Answers": [
            {
                "Answer_creation_date": "2022-03-10T09:31:09.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @Maverick (Customer)\u200b\u00a0,\n\nYou may find something similar to what you're looking for here?\n\nPlease let me know if this helps.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-03-11T06:02:51.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "@Kaniz Fatma (Databricks)\u200b\u00a0: Not working for Sagemaker and AWS RDS or Dynamo DB component.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-03-11T06:49:17.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @Maverick (Customer)\u200b\u00a0, Would you like to raise a feature request or submit an idea ?",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-03-11T07:36:34.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "@Kaniz Fatma (Databricks)\u200b\u00a0: I had been told recently that this feature is actually going to be released with the new version of DB runtime. Could you please check which DBR will come with this feature ? and what is the timeline.?",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-03-11T08:32:36.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @Maverick (Customer)\u200b\u00a0, I'll get back to you with your requested information. Thank you.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-03-16T05:52:00.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @Maverick (Customer)\u200b\u00a0, We're looking for an answer for you. Please bear with me for the delay. Apologies.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-03-11T06:49:38.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Thanks for sharing [this](http://www.google.com) article",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-03-16T05:27:43.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "@Maverick (Customer)\u200b\u00a0 let us know if you need further help on this! Thanks.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-03-16T05:55:31.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "@Atanu (Databricks)\u200b\u00a0: Your colleague @Kaniz Fatma (Databricks)\u200b\u00a0 is looking for an answer. Maybe you can also help her.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-03-21T15:42:21.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @Maverick (Customer)\u200b\u00a0, Feature Store <> SageMaker integration is not fully rolled out yet. We are looking to roll that out in Private Preview mode soon. It will need DynamoDB online store type which will be available soon.",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "Correct setup and format for calling REST API for image classification",
        "Question_creation_date": "2022-1-7T16:28Z",
        "Question_tag": [
            "Model Serving",
            "Productionization",
            "Exact Format",
            "Model",
            "Correct Setup",
            "Basic Image Classification Model",
            "TensorFlow Models",
            "MLFlow",
            "Rest api",
            "Image Data"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001gJwu8CAC/correct-setup-and-format-for-calling-rest-api-for-image-classification",
        "Question_upvote_count": 0,
        "Question_answer_count": 5,
        "Question_view_count": 372,
        "Question_has_accepted_answer": true,
        "Question_body": "I trained a basic image classification model on MNIST using Tensorflow, logging the experiment run with MLflow.\n\nModel: \"my_sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n reshape (Reshape)         (None, 28, 28, 1)         0         \n                                                                 \n conv2d (Conv2D)           (None, 26, 26, 32)        320       \n                                                                 \n max_pooling2d (MaxPooling  (None, 13, 13, 32)       0         \n 2D)                                                             \n                                                                 \n flatten (Flatten)         (None, 5408)              0         \n                                                                 \n dense (Dense)            (None, 100)               540900    \n                                                                 \n dense_1 (Dense)            (None, 10)                1010      \n                                                                 \n=================================================================\nTotal params: 542,230\nTrainable params: 542,230\nNon-trainable params: 0\n_________________________________________________________________\nwith mlflow.start_run() as run:\n  run_id       = run.info.run_id\n  \n  mlflow.tensorflow.autolog()\n\u00a0\n  model.fit(trainX, trainY, \n            validation_data = (testX, testY), \n            epochs  = 2, \n            batch_size  = 64)\n\n\u00a0\n\nI then registered the model and enabled model serving.\n\n\u00a0\n\nWhen trying to send the JSON text through the browser in the form\n\n[{\"b64\": \"AA...AA==\"}]\n\nI'm getting errors like the following:\n\nBAD_REQUEST: Encountered an unexpected error while evaluating the model. Verify that the serialized input Dataframe is compatible with the model for inference.\n\u00a0\nTraceback (most recent call last):\n  File \"/databricks/conda/envs/model-10/lib/python3.8/site-packages/mlflow/pyfunc/scoring_server/__init__.py\", line 306, in transformation\n    raw_predictions = model.predict(data)\n  File \"/databricks/conda/envs/model-10/lib/python3.8/site-packages/mlflow/pyfunc/__init__.py\", line 605, in predict\n    return self._model_impl.predict(data)\n  File \"/databricks/conda/envs/model-10/lib/python3.8/site-packages/mlflow/keras.py\", line 475, in predict\n    predicted = _predict(data)\n  File \"/databricks/conda/envs/model-10/lib/python3.8/site-packages/mlflow/keras.py\", line 462, in _predict\n    predicted = pd.DataFrame(self.keras_model.predict(data.values))\n  File \"/databricks/conda/envs/model-10/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n  File \"/databricks/conda/envs/model-10/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\", line 1147, in autograph_handler\n    raise e.ag_error_metadata.to_exception(e)\nValueError: in user code:\n\u00a0\n    File \"/databricks/conda/envs/model-10/lib/python3.8/site-packages/keras/engine/training.py\", line 1801, in predict_function  *\n        return step_function(self, iterator)\n    File \"/databricks/conda/envs/model-10/lib/python3.8/site-packages/keras/engine/training.py\", line 1790, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/databricks/conda/envs/model-10/lib/python3.8/site-packages/keras/engine/training.py\", line 1783, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/databricks/conda/envs/model-10/lib/python3.8/site-packages/keras/engine/training.py\", line 1751, in predict_step\n        return self(x, training=False)\n    File \"/databricks/conda/envs/model-10/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/databricks/conda/envs/model-10/lib/python3.8/site-packages/keras/layers/core/reshape.py\", line 110, in _fix_unknown_dimension\n        raise ValueError(msg)\n\u00a0\n\u00a0\nValueError: Exception encountered when calling layer \"reshape\" (type Reshape).\n\u00a0\n  total size of new array must be unchanged, input_shape = [1], output_shape = [28, 28, 1]\n\u00a0\n  Call arguments received:\n\u00a0\n   \u2022 inputs=tf.Tensor(shape=(None, 1), dtype=float32)\n\nThis seems to be because I'm passing the image data as an encoded byte string, not a numpy array. According to the TensorFlow documentation, this is how it has to be passed.\n\nIf I have an image with shape (28,28,1), called img, I am converting it to the required format like this\n\nimage_data = base64.b64encode(img)\njson = {\"b64\": image_data.decode()}\n\nMy question has two parts:\n\nHow do I adjust my model to handle the b64 encoded string and convert it back to a 28x28 image first?\nWhat is the exact JSON format I need to send the image data to the REST endpoint?",
        "Answers": [
            {
                "Answer_creation_date": "2022-02-07T18:41:40.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @gibbona1 (Customer)\u200b\u00a0! My name is Kaniz, and I'm the technical moderator here. Great to meet you, and thanks for your question! Let's see if your peers in the community have an answer to your question first. Or else I will get back to you soon. Thanks.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-02-17T10:35:19.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @gibbona1 (Customer)\u200b\u00a0, Try to convert your input in base64\n\n\u00a0\n\nimport base64\n\u00a0\nto_predict = test_images[0]\n\u00a0\ninputs = base64.b64encode(to_predict)\n\n\u00a0\n\nthen convert it to Dataframe and send a request\n\n\u00a0\n\ndecode it back to original at the backend by\n\n\u00a0\n\nnp.frombuffer(base64.b64decode(encoded), np.uint8)\n\u00a0\n\n\u00a0\n\nExpand Post",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-02-18T12:57:40.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @Kaniz Fatma (Databricks)\u200b\u00a0,\n\nThanks for your answer!\n\nMy question is about this backend. You mean putting this line inside the predict() method?\n\nWhen I'm defining a sequential model in TensorFlow, how do I incorporate what I want it to do to the input from a request?",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-02-18T14:32:05.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @gibbona1 (Customer)\u200b\u00a0, This link might help you as well.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-03-16T04:40:04.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "@gibbona1 (Customer)\u200b\u00a0 may be this git should work with your use case - https://github.com/mlflow/mlflow/issues/1661",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "Transfer files saved in filestore to either the workspace or to a repo",
        "Question_creation_date": "2022-0-28T21:49Z",
        "Question_tag": [
            "Transfer Files",
            "Files",
            "Python",
            "Model",
            "Filestore",
            "Machine Learning",
            "Filename"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001eeT1jCAE/transfer-files-saved-in-filestore-to-either-the-workspace-or-to-a-repo",
        "Question_upvote_count": 2,
        "Question_answer_count": 3,
        "Question_view_count": 283,
        "Question_has_accepted_answer": true,
        "Question_body": "I built a machine learning model:\n\nlr = LinearRegression()\nlr.fit(X_train, y_train)\n\nwhich I can save to the filestore by:\n\nfilename = \"/dbfs/FileStore/lr_model.pkl\"\nwith open(filename, 'wb') as f:\n    pickle.dump(lr, f)\n\nIdeally, I wanted to save the model directly to a workspace or a repo so I tried:\n\nfilename = \"/Users/user/lr_model.pkl\"\nos.makedirs(os.path.dirname(filename), exist_ok=True)\nwith open(filename, 'wb') as f:\n    pickle.dump(lr, f)\n\nbut it is not working because the file is not showing up in the workspace.\n\nThe only alternative I have now is to transfer the model from the filestore to the workspace or a repo, how do I go about that?",
        "Answers": [
            {
                "Answer_creation_date": "2022-01-29T00:30:08.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "It's important to keep in mind that there are 2 file systems:\n\nThe file system on the local machines that are part of the cluster\nThe distributed file system https://docs.databricks.com/data/databricks-file-system.html\n\nWhen you use python w/out spark such as with sklearn, its only on the driver and local is local on the driver. That will go away when the cluster does.\n\nTry %sh ls / and %fs ls and see the differences",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-02-01T15:25:47.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Workspace and Repo is not full available via dbfs as they have separate access rights. It is better to use MLFlow for your models as it is like git but for ML. I think using MLOps you can than put your model also to git.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-02-04T21:10:11.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @MichaelO (Customer)\u200b\u00a0,\n\n\u00a0\n\nWhen you store the file in DBFS (/FileStore/...), it's in your account (data plane). While notebooks, etc. are in the Databricks account (control plane). By design, you can't import non-code objects into a workspace. But Repos now has support for\u00a0arbitrary files, although only one direction - you can access files in Repos from your cluster running in the data plane, you can't write into Repos (at least not now). You can:\n\nEither export model to your local disk & commit, then pull changes into Repos\nUse\u00a0Workspace API\u00a0to put files into Repos. Here is an answer that shows how to do that.\n\nBut really, you should use MLflow that is built-in into Azure Databricks, and it will help you by\u00a0logging the model file, hyper-parameters, and other information. And then you can work with this model using APIs, command tools, etc., for example, to\u00a0move the model between staging & production stages using Model Registry,\u00a0deploy the model to AzureML, etc.\n\nExpand Post",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "Can you deploy models that can be queried/called/inferred outside your organization?",
        "Question_creation_date": "2022-0-7T17:49Z",
        "Question_tag": [
            "Model Deployment",
            "Small Scale Experimentation",
            "Deploy Models",
            "MLFlow",
            "Deploy"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001bg0ZcCAI/can-you-deploy-models-that-can-be-queriedcalledinferred-outside-your-organization",
        "Question_upvote_count": 0,
        "Question_answer_count": 6,
        "Question_view_count": 264,
        "Question_has_accepted_answer": true,
        "Question_body": "It looks like you can via MLflow but I wanted to check before diving deeper?\n\nAlso it seems like if it is possible, it's just for small scale experimentation?\n\nThank you!",
        "Answers": [
            {
                "Answer_creation_date": "2022-01-07T17:57:28.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @\u00a0SeanB\u00a0! My name is Kaniz, and I'm the technical moderator here. Great to meet you, and thanks for your question! Let's see if your peers in the community have an answer to your question first. Or else I will get back to you soon. Thanks.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-01-12T04:22:21.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "@SeanB (Customer)\u200b\u00a0 just wanted to check from where actually you are trying to query ?",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-01-12T04:22:37.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Means - outside databricks",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-01-17T17:01:37.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Yes, outside databricks.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-01-12T10:33:38.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Yes, If somebody outside Databricks can query/use a model built in Databricks. I assume the answer must be yes?",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-01-31T08:52:49.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @SeanB (Customer)\u200b\u00a0, Model access rights are inherited from the Model Registry. Enabling or disabling the serving feature requires \u2018manage\u2019 permission on the registered model. Anyone with read rights can score any of the deployed versions. (Source)",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "Databricks and CD4ML",
        "Question_creation_date": "2022-1-1T8:46Z",
        "Question_tag": [
            "Model Deployment",
            "CD4ML Way"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001fBuzLCAS/databricks-and-cd4ml",
        "Question_upvote_count": 0,
        "Question_answer_count": 7,
        "Question_view_count": 194,
        "Question_has_accepted_answer": true,
        "Question_body": "I would like to use Databricks in a CD4ML way (see also https://martinfowler.com/articles/cd4ml.html). Is this possible? I would like to develop and train models in one environment once qualified, I would like to deploy the model with the application (embed), deploy it as a service or publish it as data. Can anyone point me to resources how to get started?",
        "Answers": [
            {
                "Answer_creation_date": "2022-02-01T15:31:09.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hello @Dunken (Customer)\u200b\u00a0\n\n\u00a0\n\nThank you for posting the query.\n\n\u00a0\n\nBelow blogs and documentation would be a good place to start with\n\n\u00a0\n\nhttps://databricks.com/product/mlflow-model-registry\nhttps://databricks.com/blog/2020/06/25/announcing-mlflow-model-serving-on-databricks.html\nhttps://docs.databricks.com/applications/mlflow/model-registry-example.html\nExpand Post",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-02-04T10:04:42.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Thank you.\n\n\u00a0\n\nThis is definitively a feasible \"as a service\" approach. What about really hosting (embed) the service running on production somewhere else (e.g. as part of my own service)? Or is this something you would not recommend?",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-02-08T17:18:47.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Any thoughts on this? Actually I would like to deploy the models on devices like Raspberry Pi. Is this something you support?",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-03-03T14:49:47.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "something below you are looking for @Dunken (Customer)\u200b\u00a0?",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-03-03T15:05:59.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "What exactly do you mean? What's below?",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-03-03T15:16:17.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "I am sorry forgot to paste the link - https://databricks.com/notebooks/computer-vision/cv_04_%20model_deployment.html . this is the one .",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-03-07T19:37:43.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @Dunken (Customer)\u200b\u00a0,\n\n\u00a0\n\nDid @Atanu (Databricks)\u200b\u00a0reply help you to resolve this issue? Please let us know",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "Spark nlp on Databricks - looking for known issues/best practices",
        "Question_creation_date": "2022-2-6T17:10Z",
        "Question_tag": [
            "Config Settings",
            "Sparknlp",
            "Best Practices",
            "Databricks Platform",
            "Model Deployment"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001ku2jwCAA/spark-nlp-on-databricks-looking-for-known-issuesbest-practices",
        "Question_upvote_count": 1,
        "Question_answer_count": 2,
        "Question_view_count": 239,
        "Question_has_accepted_answer": true,
        "Question_body": "I m currently looking for information on whether Spark NLP can run fine on Databricks platform.\n\nCan someone please share\u00a0\n\n- known issues/bugs encountered\n\n- any fixes or config settings required in environment\n\n- best practices to follow",
        "Answers": [
            {
                "Answer_creation_date": "2022-03-06T17:51:56.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Yes it is working. On cluster tab please go to libraries and install new:\n\n\u00a0\n\npypl -> spark-nlp\n\nmaven -> com.johnsnowlabs.nlp:spark-nlp_2.11\n\n\u00a0\n\nMore details about spark-nlp can be watched on that video from data+AI summit organized by databricks\n\nhttps://databricks.com/session_na21/advanced-natural-language-processing-with-apache-spark-nlp\n\nExpand Post",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-03-07T07:53:43.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Thanks a lot for the quick response",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "Next LTS version after 9.1",
        "Question_creation_date": "2022-2-4T4:02Z",
        "Question_tag": [
            "Databricks Runtime",
            "DBR",
            "LTS",
            "LTS Version",
            "Azure databricks"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001kLGcFCAW/next-lts-version-after-91",
        "Question_upvote_count": 0,
        "Question_answer_count": 4,
        "Question_view_count": 82,
        "Question_has_accepted_answer": true,
        "Question_body": "What is the next DBR LTS version after 9.1 and when it is planned to be released in Azure?",
        "Answers": [
            {
                "Answer_creation_date": "2022-03-04T08:13:15.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "@venkad (Customer)\u200b\u00a0 currently in future we we will release 10.x dbr with lts version.\n\nbut please be noted that this is yet to be confirmed and it might be changed to other versions.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-03-04T10:38:07.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @pavan kumar (Databricks)\u200b\u00a0, Thanks for the update. We only use the LTS version within our organization to minimize the frequent upgrade and maintenance around it. Even though 10.x has a lot of new features, we have to stick with the latest LTS version due to the organization's policy. So it would be great if we can have any timeline on the next LTS version.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-03-05T00:41:49.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "I think 10.4 will be LTS and released in the coming weeks.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-03-05T00:58:37.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Thank you for the update!\u200b",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "Deploy Databricks Machine Learing Models On Power BI",
        "Question_creation_date": "2022-1-21T1:43Z",
        "Question_tag": [
            "Model Deployment",
            "Powerbi",
            "Machine Learning",
            "Azure",
            "Deploy Databricks Machine"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001iRNZcCAO/deploy-databricks-machine-learing-models-on-power-bi",
        "Question_upvote_count": 0,
        "Question_answer_count": 3,
        "Question_view_count": 270,
        "Question_has_accepted_answer": true,
        "Question_body": "Hi Guys. I've implemented a Machine Learning model on Databricks and have registered it with a Model URL. I wanted to enquire if I could use this model on Power BI. Basically the model predicts industries based on client demographics. Ideally I would like to run this model on Power BI against a list of new clients. I can connect Azure Databricks onto PowerBI and it gives me a list of all the tables and CSVs, but I can't seem to work out on how to get the model on Power BI.",
        "Answers": [
            {
                "Answer_creation_date": "2022-02-21T10:49:59.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "I think the ML model has to be deployed using Azure ML:\n\nhttps://docs.microsoft.com/en-us/power-bi/connect-data/service-aml-integrate",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-02-21T13:25:29.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "I agree with werners. I looked around and couldn't find anything either on running the model w/ powerbi.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-02-21T22:36:20.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Thank you @werners (Customer)\u200b\u00a0 and @josephk (Databricks)\u200b\u00a0 for your replies.",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "MLflow Model Serving on Azure Databricks General Availability",
        "Question_creation_date": "2022-1-10T11:03Z",
        "Question_tag": [
            "MLFlow",
            "Model Serving",
            "Real Time Model Serving",
            "Azure databricks"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001gs5sgCAA/mlflow-model-serving-on-azure-databricks-general-availability",
        "Question_upvote_count": 1,
        "Question_answer_count": 4,
        "Question_view_count": 154,
        "Question_has_accepted_answer": true,
        "Question_body": "When is MLflow Model Serving on Azure Databricks expected to become General Available?",
        "Answers": [
            {
                "Answer_creation_date": "2022-02-10T19:52:10.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @Mihai (Customer)\u200b\u00a0, Model serving is available in Azure Databricks from Model Registry.\n\n\u00a0\n\nPlease check this article.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-02-10T19:55:51.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @Kaniz Fatma (Databricks)\u200b\u00a0, model serving is currently available in Public Preview. When will it become General Available (GA)?",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-02-15T20:50:26.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hello Mihai,\n\n\u00a0\n\nWe plan to GA, Model serving by end of this year as we are working on a lot of improvements.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-02-16T08:57:47.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Thank you!",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "Feature store : Can create_training_set() be implemented to execute an inner join?",
        "Question_creation_date": "2022-1-1T13:00Z",
        "Question_tag": [
            "Feature Store",
            "Feature",
            "Feature Tables",
            "Model Deployment"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001fQv4FCAS/feature-store-can-createtrainingset-be-implemented-to-execute-an-inner-join",
        "Question_upvote_count": 1,
        "Question_answer_count": 3,
        "Question_view_count": 179,
        "Question_has_accepted_answer": true,
        "Question_body": "For timeseries feature tables, an inner join is made at the creation of the feature table. For the other type of feature tables, a left join is made, so NaN values can show up in the training set. Can the inner join in create_training_set() method be implemented with a parameter?",
        "Answers": [
            {
                "Answer_creation_date": "2022-02-01T16:57:50.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hello, @thib (Customer)\u200b! My name is Piper, and I'm a moderator here in the community. It's nice to meet you and welcome to the community. Thank you for your question!\n\n\u00a0\n\nWe'll give the community some time to respond, and then we will come back if we need to. :)",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-02-01T20:32:31.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "create_training_set performs left join. It is just a simple function which select data from Spark SQL database used by feature store. You can just write own code with inner join:\n\n\u00a0\n\ncustomer_features_df = spark.sql(\"SELECT * FROM recommender_system.customer_features\")\nproduct_features_df = spark.sql(\"SELECT * FROM recommender_system.product_features\")\n\u00a0\ntraining_df.join(\n  customer_features_df,\n  on=[training_df.cid == customer_features_df.customer_id,\n      training_df.transaction_dt == customer_features_df.dt],\n  how=\"inner\"\n).join(\n  product_features_df,\n  on=\"product_id\",\n  how=\"inner\"\n)\n\n\u00a0\n\nExpand Post",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-02-02T08:11:26.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Thank you Hubert, that's a good alternative, I just thought I'd stick to the api as much as possible, but this solves it.",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "What can I do to reduce the number of MLflow API calls I make?",
        "Question_creation_date": "2022-0-9T1:10Z",
        "Question_tag": [
            "Rate Limits",
            "Multiple Models",
            "MLFlow",
            "MLflow API"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001c3D7lCAE/what-can-i-do-to-reduce-the-number-of-mlflow-api-calls-i-make",
        "Question_upvote_count": 0,
        "Question_answer_count": 1,
        "Question_view_count": 91,
        "Question_has_accepted_answer": false,
        "Question_body": "I'm fitting multiple models in parallel. For each one, I'm logging lots of params and metrics to MLflow. I'm hitting rate limits, causing problems in my jobs.",
        "Answers": [
            {
                "Answer_creation_date": "2022-01-09T01:15:50.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "The first thing to try is to log in batches. If you are logging each param and metric separately, you're making 1 API call per param and 1 per metric. Instead, you should use the batch logging APIs; e.g. use \"log_params\" instead of \"log_param\" https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.log_params\n\nIf you're logging 10 params and metrics per model, this will cut the number of API calls you're making by a factor of 10.\n\n\u00a0\n\nIf this optimization is still insufficient for you, then I'd recommend doing 2 things:\n\n1) Short-term workaround: You can save data you wish to log to a table and log it in a follow-up process later.\n\n2) Medium/long-term: I'd recommend working with your Databricks account team to come up with a solution to match your needs. In general, the best option is to reorganize how models are being fit or logged to do more efficient batching in logging. But different cases may have different best solutions, so working with your account team may be the best option.\n\nExpand Post",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "Is the machine learning part of \"Apache Spark\u2122 Tutorial: Getting Started with Apache Spark on Databricks\" missing or no longer available?",
        "Question_creation_date": "2022-0-5T7:15Z",
        "Question_tag": [
            "Apache spark",
            "Learning",
            "Apache",
            "Machine Learning"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001bQuG4CAK/is-the-machine-learning-part-of-apache-spark-tutorial-getting-started-with-apache-spark-on-databricks-missing-or-no-longer-available",
        "Question_upvote_count": 0,
        "Question_answer_count": 3,
        "Question_view_count": 91,
        "Question_has_accepted_answer": true,
        "Question_body": "I am following the Apache Spark\u2122 Tutorial. When I finish the data set part and want to continue the machine learning part. I found the page is empty. The next section after machine learning is fine. So I guess there must be a url mismatching.\n\nThe url for machine learning which is missing is below:\n\nhttps://databricks.com/spark/getting-started-with-apache-spark/machine-learning-1#overview",
        "Answers": [
            {
                "Answer_creation_date": "2022-01-05T07:28:06.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @\u00a0self-employed! My name is Kaniz, and I'm the technical moderator here. Great to meet you, and thanks for your question! Let's see if your peers in the community have an answer to your question first. Or else I will get back to you soon. Thanks.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-01-06T07:26:26.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "I clean the cookie and then the link recovers. So it is an issue about cookie.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2022-01-06T10:08:45.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Cheers!",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "MLFlow search runs getting http 429 error",
        "Question_creation_date": "2021-11-13T23:58Z",
        "Question_tag": [
            "MLFlow",
            "Search Runs",
            "Mlflow Run",
            "MLflow Experiment",
            "Search",
            "ML Artifact",
            "Max Retries",
            "DESC"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001YQomeCAD/mlflow-search-runs-getting-http-429-error",
        "Question_upvote_count": 0,
        "Question_answer_count": 3,
        "Question_view_count": 673,
        "Question_has_accepted_answer": true,
        "Question_body": "I am facing an issue in loading a ML artifact for a specific run by search the experiment runs to get a specific run_id as follows:\n\nhttps://www.mlflow.org/docs/latest/rest-api.html#search-runs\n\n\u00a0\n\n\u00a0\n\nAPI request to https://eastus-c3.azuredatabricks.net/api/2.0/mlflow/runs/search failed with exception HTTPSConnectionPool(host='eastus-c3.azuredatabricks.net', port=443): Max retries exceeded with url: /api/2.0/mlflow/runs/search (Caused by ResponseError('too many 429 error responses'))\n\n# Search the experiment_id using a filter_string with customer and product_key and order by start time\nquery = f\"params.product_key = {product_key} and params.customer = '{customer}'\"\nruns_df = mlflow.search_runs([experiment.experiment_id], filter_string=query, order_by=[\"start_time DESC\"])\n# Get the latest run id recorded\nrun_id = runs_df.run_id.values[0]\nartifact_uri = runs_df.artifact_uri.values[0]\nclient = MlflowClient()\n\n429 is an HTTP response status code that indicates the client application has surpassed its rate limit, or number of requests they can send in a given period of time.\u00a0Is there any fix for that ?\n\n\u00a0\n\nI am running the search_runs() API in a pandas_udf function that does a search at the customer and product_key level in my dataframe to find the proper logged model and artifact to load for inference.\n\nAs the inference process is pretty quick and number of product_keys are in the range of 4000 records, I end up hitting the MLFlow search API around 30-40 times per minute.\n\n\u00a0\n\nAny thought on this?",
        "Answers": [
            {
                "Answer_creation_date": "2021-12-14T09:56:14.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @Saeed\u00a0! My name is Kaniz, and I'm the technical moderator here. Great to meet you, and thanks for your question! Let's see if your peers in the community have an answer to your question first. Or else I will get back to you soon. Thanks.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-12-17T02:30:15.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Yes, you will hit rate limits if you try to query the API so fast in parallel. Do you just want to manipulate the run data in an experiment with Spark? you can simply load all that data in a DataFrame with spark.read.format(\"mlflow-experiment\").load(\"... your experiment path ...\"). With all the data you can sort, query, etc (or convert to a pandas DF if you want)",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-12-20T23:30:12.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Thanks Sean, that's exactly what I need without hitting the API. Loading the experiment runs once and manipulate and filter them as needed.\u200b",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "How should I tune hyperparameters when fitting models for every item?",
        "Question_creation_date": "2021-11-20T17:03Z",
        "Question_tag": [
            "Models",
            "Hyperparameter Tuning",
            "Machine Learning"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001ZHcoMCAT/how-should-i-tune-hyperparameters-when-fitting-models-for-every-item",
        "Question_upvote_count": 0,
        "Question_answer_count": 2,
        "Question_view_count": 85,
        "Question_has_accepted_answer": false,
        "Question_body": "My dataset has an \"item\" column which groups the rows into many groups. (Think of these groups as items in a store.) I want to fit 1 ML model per group. Should I tune hyperparameters for each group separately? Or should I tune them for the entire process so that every group uses the same set of hyperparameters?\n\n\u00a0\n\nAnd, for either option, how do I set up that tuning?",
        "Answers": [
            {
                "Answer_creation_date": "2021-12-20T17:18:02.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @\u00a0Joseph B! My name is Kaniz, and I'm the technical moderator here. Great to meet you, and thanks for your question! Let's see if your peers in the community have an answer to your question first. Or else I will get back to you soon. Thanks.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-12-20T17:31:16.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "For the first question (\"which option is better?\"), you need to answer that via your understanding of the problem domain.\n\nDo you expect similar behavior across the groups (items)?\nIf so, that's a +1 in favor of sharing hyperparameters. And vice versa.\nDo you have similar numbers of training examples per group?\nIf so, that's also a +1 in favor of sharing hyperparameters. And vice versa.\nHow large are groups vs. the full dataset?\nIf each group is tiny vs. the full dataset, then you can expect tuning to be much less stable if run for each group separately. E.g., if you retrain your model each week, you might be very different hyperparameters (and predictions) each time.\nI.e., if each group is fairly small, then consider using shared hyperparameters.\n\n\u00a0\n\nFor the second question (\"how do I do it?\"), here's a sketch. This sketch is for groups which are small enough to fit on 1 machine. (When some or all groups are too large to fit on 1 machine, you can handle them separately using distributed training algorithms.)\n\nFor both, use an Apache Spark DataFrame with groupBy to create a grouped DataFrame. Then apply a Pandas UDF on the group. Within that UDF, call model training (or tuning, if applicable).\nShared hyperparameters:\nCall tuning, e.g., Hyperopt to run on the driver.\nEach time the tuning algorithm tests 1 set of hyperparameters, it should fit models for all groups by applying the Pandas UDF to the DataFrame.\nWithin the Pandas UDF, your ML library (e.g., sklearn) will use the global hyperparameter setting provided by Hyperopt to fit a model for that group.\nSeparate hyperparameters:\nApply the Pandas UDF to the DataFrame.\nWithin the Pandas UDF, call tuning (e.g., Hyperopt). Tuning will call your ML library in turn. After tuning, you will have your model for that group.\nNote: If using Hyperopt, note that Hyperopt should use regular Trials, not SparkTrials. This is because the Pandas UDF application uses distributed computing, so Hyperopt itself needs to run locally.\n\n\u00a0\n\nExpand Post",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "Update Databricks Runtime",
        "Question_creation_date": "2021-11-17T11:21Z",
        "Question_tag": [
            "Update",
            "Databricks Runtime"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001YpDuwCAF/update-databricks-runtime",
        "Question_upvote_count": 1,
        "Question_answer_count": 4,
        "Question_view_count": 154,
        "Question_has_accepted_answer": true,
        "Question_body": "Hy guys,\n\n\u00a0\n\nI need to upgrade my databricks runtime (current 8.0 \ud83d\ude2b )\n\n\u00a0\n\nWhat the precautions should I take ?\n\n\u00a0\n\nThank you very much",
        "Answers": [
            {
                "Answer_creation_date": "2021-12-17T11:27:21.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "There is a lot of to read since 8.0 including libraries upgrades https://docs.databricks.com/release-notes/runtime/releases.html\n\n\u00a0\n\nFrom my experience spark functionality generally don't have breaking changes, regarding other libraries it is so much that it doesn't make sense to go through it. Maybe just test your notebooks under 10.1 and compare results with 8.0",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-12-20T11:08:32.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "thank you @Hubert Dudek (Customer)\u200b",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-12-20T10:11:27.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "If you want to know the version of Databricks runtime in Azure after creation:\u00a0Go to Azure Data bricks portal => Clusters => Interactive Clusters =>\u00a0here you can find the run time version. For more details, refer \"Azure Databricks Runtime versions\".\n\n\u00a0\n\n\u00a0\n\nRapidfs",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-12-20T11:08:43.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Thank yoou @Steward475 (Customer)\u200b",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "Managing the permissions using MLFlow APIs",
        "Question_creation_date": "2021-9-21T9:41Z",
        "Question_tag": [
            "MLFlow",
            "Permissions",
            "MLflow Experiments",
            "MLFlow APIs",
            "MLflow API",
            "AccessControlList"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001Q0uJkCAJ/managing-the-permissions-using-mlflow-apis",
        "Question_upvote_count": 0,
        "Question_answer_count": 4,
        "Question_view_count": 168,
        "Question_has_accepted_answer": false,
        "Question_body": "Hello All,\n\n\u00a0\n\nI am trying to manage the permissions on the experiments using the MLFLow API. Do we have any MLFlow API which helps to manage the permissions of Can Read ,Can Edit , Can Manage.\n\n\u00a0\n\nExample :\n\nI create the model using MLFlow APIs and through my code I should be able to enforce the permission saying what permission can be given for this model.E.g. Can Read , Can Edit , Can Manage.\n\n\u00a0\n\nThanks",
        "Answers": [
            {
                "Answer_creation_date": "2021-10-21T14:28:59.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @SagarK! My name is Kaniz, and I'm the technical moderator here. Great to meet you, and thanks for your question! Let's see if your peers in the community have an answer to your question first. Or else I will get back to you soon. Thanks.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-10-21T19:48:20.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @SagarK (Customer)\u200b\u00a0Please refer to the below document that provides a detailed explanation about the MLflow permissions API.\n\n\u00a0\n\nMLflow experiment permissions\u00a0\u2014 Manage which users can read, edit, or manage MLflow experiments.\n\n\u00a0\n\nhttps://docs.databricks.com/dev-tools/api/latest/permissions.html#tag/MLflow-experiment-permissions\n\n\u00a0\n\nMLflow registered model permissions\u00a0\u2014 Manage which users can read, edit, or manage MLflow registered models.\n\n\u00a0\n\nhttps://docs.databricks.com/dev-tools/api/latest/permissions.html#tag/MLflow-registered-model-permissions\n\n\u00a0\n\n\u00a0\n\nExpand Post",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-11-28T22:04:20.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @SagarK (Customer)\u200b\u00a0, Could you please let me know if the answer I provided earlier resolved your query? If so would you be happy to mark the answer as best so that others can quickly find the solution in the future?",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-12-17T02:33:13.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "I don't believe you can set MLflow permissions via the API. Permissions are a function of Databricks rather than MLflow itself.",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "Do Databricks support XLA compilation for TensorFlow models?",
        "Question_creation_date": "2021-10-30T18:36Z",
        "Question_tag": [
            "XLA Compilation",
            "Autologging Utils",
            "Databricks Support XLA Compilation",
            "TensorFlow Models",
            "Databricks Runtime"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001WR4XKCA1/do-databricks-support-xla-compilation-for-tensorflow-models",
        "Question_upvote_count": 1,
        "Question_answer_count": 4,
        "Question_view_count": 264,
        "Question_has_accepted_answer": false,
        "Question_body": "I am defining a sequential Keras model using tensorflow.keras\n\n\u00a0\n\nRuntime: Databricks ML 8.3\n\n\u00a0\n\nCluster: Standard NC24 with 4 GPUs per node.\n\n\u00a0\n\nTo enable XLA compilation, I set the following flag:\n\n\u00a0\n\ntf.config.optimizer.set_jit(True)\n\n\u00a0\n\nHere is the output when I try to train the model:\n\n\u00a0\n\n<command-4238178162238395> in train_distributed_tf(train_count, val_count, params)\n\n18 metrics=['mean_absolute_error', 'mean_absolute_percentage_error'])\n\n19\n\n---> 20 history = model.fit(\n\n21 distributed_train,\n\n22 epochs=EPOCHS,\n\n\u00a0\n\n/databricks/python/lib/python3.8/site-packages/mlflow/utils/autologging_utils/safety.py in safe_patch_function(*args, **kwargs)\n\n485\n\n486 if patch_is_class:\n\n--> 487 patch_function.call(call_original, *args, **kwargs)\n\n488 else:\n\n489 patch_function(call_original, *args, **kwargs)\n\n\u00a0\n\n/databricks/python/lib/python3.8/site-packages/mlflow/utils/autologging_utils/safety.py in call(cls, original, *args, **kwargs)\n\n151 @classmethod\n\n152 def call(cls, original, *args, **kwargs):\n\n--> 153 return cls().__call__(original, *args, **kwargs)\n\n154\n\n155 def __call__(self, original, *args, **kwargs):\n\n\u00a0\n\n/databricks/python/lib/python3.8/site-packages/mlflow/utils/autologging_utils/safety.py in __call__(self, original, *args, **kwargs)\n\n162 # Regardless of what happens during the `_on_exception` callback, reraise\n\n163 # the original implementation exception once the callback completes\n\n--> 164 raise e\n\n165\n\n166\n\n\u00a0\n\n/databricks/python/lib/python3.8/site-packages/mlflow/utils/autologging_utils/safety.py in __call__(self, original, *args, **kwargs)\n\n155 def __call__(self, original, *args, **kwargs):\n\n156 try:\n\n--> 157 return self._patch_implementation(original, *args, **kwargs)\n\n158 except (Exception, KeyboardInterrupt) as e:\n\n159 try:\n\n\u00a0\n\n/databricks/python/lib/python3.8/site-packages/mlflow/utils/autologging_utils/safety.py in _patch_implementation(self, original, *args, **kwargs)\n\n214 self.managed_run = try_mlflow_log(create_managed_run)\n\n215\n\n--> 216 result = super(PatchWithManagedRun, self)._patch_implementation(\n\n217 original, *args, **kwargs\n\n218 )\n\n\u00a0\n\n/databricks/python/lib/python3.8/site-packages/mlflow/tensorflow.py in _patch_implementation(self, original, inst, *args, **kwargs)\n\n1086 _log_early_stop_callback_params(early_stop_callback)\n\n1087\n\n-> 1088 history = original(inst, *args, **kwargs)\n\n1089\n\n1090 _log_early_stop_callback_metrics(early_stop_callback, history, metrics_logger)\n\n\u00a0\n\n/databricks/python/lib/python3.8/site-packages/mlflow/utils/autologging_utils/safety.py in call_original(*og_args, **og_kwargs)\n\n443 disable_warnings=False, reroute_warnings=False,\n\n444 ):\n\n--> 445 original_result = original(*og_args, **og_kwargs)\n\n446\n\n447 try_log_autologging_event(\n\n\u00a0\n\n/databricks/python/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\n\n1098 _r=1):\n\n1099 callbacks.on_train_batch_begin(step)\n\n-> 1100 tmp_logs = self.train_function(iterator)\n\n1101 if data_handler.should_sync:\n\n1102 context.async_wait()\n\n\u00a0\n\n/databricks/python/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)\n\n826 tracing_count = self.experimental_get_tracing_count()\n\n827 with trace.Trace(self._name) as tm:\n\n--> 828 result = self._call(*args, **kwds)\n\n829 compiler = \"xla\" if self._experimental_compile else \"nonXla\"\n\n830 new_tracing_count = self.experimental_get_tracing_count()\n\n\u00a0\n\n/databricks/python/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)\n\n886 # Lifting succeeded, so variables are initialized and we can run the\n\n887 # stateless function.\n\n--> 888 return self._stateless_fn(*args, **kwds)\n\n889 else:\n\n890 _, _, _, filtered_flat_args = \\\n\n\u00a0\n\n/databricks/python/lib/python3.8/site-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs)\n\n2940 (graph_function,\n\n2941 filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\n-> 2942 return graph_function._call_flat(\n\n2943 filtered_flat_args, captured_inputs=graph_function.captured_inputs) # pylint: disable=protected-access\n\n2944\n\n\u00a0\n\n/databricks/python/lib/python3.8/site-packages/tensorflow/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)\n\n1916 and executing_eagerly):\n\n1917 # No tape is watching; skip to running the function.\n\n-> 1918 return self._build_call_outputs(self._inference_function.call(\n\n1919 ctx, args, cancellation_manager=cancellation_manager))\n\n1920 forward_backward = self._select_forward_and_backward_functions(\n\n\u00a0\n\n/databricks/python/lib/python3.8/site-packages/tensorflow/python/eager/function.py in call(self, ctx, args, cancellation_manager)\n\n553 with _InterpolateFunctionError(self):\n\n554 if cancellation_manager is None:\n\n--> 555 outputs = execute.execute(\n\n556 str(self.signature.name),\n\n557 num_outputs=self._num_outputs,\n\n\u00a0\n\n/databricks/python/lib/python3.8/site-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\n\n57 try:\n\n58 ctx.ensure_initialized()\n\n---> 59 tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\n60 inputs, attrs, num_outputs)\n\n61 except core._NotOkStatusException as e:\n\n\u00a0\n\nInternalError: 5 root error(s) found.\n\n(0) Internal: libdevice not found at ./libdevice.10.bc\n\n[[{{node cluster_3_1/xla_compile}}]]\n\n[[div_no_nan_33/ReadVariableOp_3/_318]]\n\n(1) Internal: libdevice not found at ./libdevice.10.bc\n\n[[{{node cluster_3_1/xla_compile}}]]\n\n(2) Internal: libdevice not found at ./libdevice.10.bc\n\n[[{{node cluster_3_1/xla_compile}}]]\n\n[[div_no_nan/_825]]\n\n(3) Internal: libdevice not found at ./libdevice.10.bc\n\n[[{{node cluster_3_1/xla_compile}}]]\n\n[[div_no_nan_26/AddN/_272]]\n\n(4) Internal: libdevice not found at ./libdevice.10.bc\n\n[[{{node cluster_3_1/xla_compile}}]]\n\n[[div_no_nan/_821]]\n\n0 successful operations.\n\n0 derived errors ignored. [Op:__inference_train_function_2599244]\n\n\u00a0\n\nFunction call stack:\n\ntrain_function -> train_function -> train_function -> train_function -> train_function",
        "Answers": [
            {
                "Answer_creation_date": "2021-12-01T08:53:03.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @\u00a0ray21! My name is Kaniz, and I'm the technical moderator here. Great to meet you, and thanks for your question! Let's see if your peers in the community have an answer to your question first. Or else I will get back to you soon. Thanks.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-12-09T00:30:13.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @ray21 (Customer)\u200b\u00a0\n\nCan you please try with DBR 7.3 ML cluster ?\n\n\u00a0\n\nIt seems like cupti library was deprecated staring from 7.6 DBR.\n\nhttps://docs.databricks.com/release-notes/runtime/7.6ml.html#deprecations\n\n\u00a0\n\nIt seems the cupti version(9) which comes with ubuntu was not compatible with CUDA(11). The workaround would be to install the compatible cupti package(11) through init script.\n\n\u00a0\n\nHowever for now you can try with DBR 7.3 ML to see if it works there\n\n\u00a0\n\nThanks\n\nMathan\n\nExpand Post",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-12-10T23:23:05.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @ray21 (Customer)\u200b\u00a0,\n\n\u00a0\n\nDid Mathan's response help you to solve your question/issue? if it did, please mark it as \"best\" to it can be moved to the top and help others",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-12-17T02:20:00.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "I don't think this is specific to Databricks, but rather Tensorflow. See https://stackoverflow.com/questions/68614547/tensorflow-libdevice-not-found-why-is-it-not-found-in-the-searched-path for a possibly relevant solution.\n\n\u00a0\n\nI don't see evidence that this is related to libcupti",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "Run MLflow Projects on Azure Databricks",
        "Question_creation_date": "2021-11-1T14:39Z",
        "Question_tag": [
            "MLFlow",
            "Azure databricks",
            "Azure"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001WXh8lCAD/run-mlflow-projects-on-azure-databricks",
        "Question_upvote_count": 2,
        "Question_answer_count": 6,
        "Question_view_count": 354,
        "Question_has_accepted_answer": true,
        "Question_body": "Hi,\n\n\u00a0\n\nI am trying to follow this simple document to be able to run MLFlow within Databricks:\n\n\u00a0\n\nhttps://docs.microsoft.com/en-us/azure/databricks/applications/mlflow/projects\n\n\u00a0\n\nI try to run it from:\n\n\u00a0\n\nA Databricks notebook within Azure Databricks\nBy use of the mlflow-cli (remote)\nBy use of databricks-connect\n\n\u00a0\n\nI have tested that the 3 methods are properly set-up. I get the same error with all methods:\n\n\u00a0\n\nmlflow.exceptions.RestException: BAD_REQUEST: Unable to connect to the linked AzureML workspace. Check that the workspace exists.\n\nThe Databricks workspace is linked to an AzureML workspace.\n\n\u00a0\n\nBy following this other document:\n\n\u00a0\n\nhttps://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-mlflow-azure-databricks\n\n\u00a0\n\nI am actually able to create and run MLFlow experiments from:\n\n\u00a0\n\nA Databricks notebook\ndatabricks-connect\n\n\u00a0\n\nThe issue that I have is that:\n\nThe experiments are only created in AzureML\nI can only run from within a script/notebook\n\n\u00a0\n\nIf I follow this document:\n\n\u00a0\n\nhttps://docs.microsoft.com/en-us/azure/databricks/applications/mlflow/access-hosted-tracking-server\n\n\u00a0\n\nI am able to use the remote mlflow-cli and I can for example, create an experiment in databricks only (the experiment doesn't live in AzureML), by use of:\n\n\u00a0\n\nmlflow experiments create -n /Users/<your-username>/my-experiment\n\nBut again, when trying to do something like this:\n\n\u00a0\n\nmlflow run https://github.com/mlflow/mlflow#examples/sklearn_elasticnet_wine -b databricks --backend-config cluster-spec.json --experiment-id <experiment-id>\n\nI get the error I previously mentioned:\n\n\u00a0\n\nmlflow.exceptions.RestException: BAD_REQUEST: Unable to connect to the linked AzureML workspace. Check that the workspace exists.\n\nI have set\n\n\u00a0\n\nexport MLFLOW_TRACKING_URI=databricks\n\nAnd everything else as noted in the documentation.\n\n\u00a0\n\nIs there a configuration I am missing?\n\n\u00a0\n\nThanks a lot in advance for any help!",
        "Answers": [
            {
                "Answer_creation_date": "2021-12-01T15:12:47.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Maybe this answer will help:\n\nhttps://community.databricks.com/s/question/0D53f00001UOu7rCAD/mlflow-resourcealreadyexists\n\n\u00a0\n\nas @Prabakar (Databricks)\u200b\u00a0wrote \" it\u2019s not recommended to \u201clink\u201d the Databricks and AML workspaces, as we are seeing more problems\"",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-12-01T15:51:29.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi,\n\nThanks a lot for the help! This linking within WS was already set by somebody working in the team. I will investigate the reason why and try to unlink them. I will report back and say if this suits the trick as the error is not\u200b so well documented and it might help others.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-12-01T16:25:30.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "@arturooa (Customer)\u200b\u00a0- That would be great! Once you share your solution, would you be happy to mark your answer as best so others can find it more easily?",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-12-01T16:13:14.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @Hubert Dudek (Customer)\u200b\u00a0Thanks for sharing the post.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-12-01T16:27:08.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "you are welcome :-)",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-12-15T10:11:20.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi! Thanks a lot for your help. I took contact with tech support via my Azure subscription. They confirmed that indeed, connecting the Databricks and AzureML workspaces introduces errors. They provided us with a ARM template for removing the connection. After the connection was successfully removed, there were no more problems with tracking experiments with MLFlow directly in the Databricks WS.",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "How to Register a ML model using MLflow",
        "Question_creation_date": "2021-11-8T16:49Z",
        "Question_tag": [
            "Register ML",
            "Register ML Model",
            "Mlflow registry",
            "MLFlow",
            "Decode error"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001XcytICAR/how-to-register-a-ml-model-using-mlflow",
        "Question_upvote_count": 0,
        "Question_answer_count": 2,
        "Question_view_count": 198,
        "Question_has_accepted_answer": false,
        "Question_body": "Hi,\n\nI have a PyTorch model which I have pushed into the\u00a0dbfs\u00a0now I want to serve the model using MLflow. I saw that the model needs to be in\u00a0python_function model.\n\nTo do that I did the following methods\n\n1. load the model from dbfs using torch load option\n\n2. Then save the model in python_function model using the pyfunc.save_model function\n\n3. After this when I register the model I get a decode error\n\nI'm not training any model in the Databricks.\n\n\u00a0\n\nimport mlflow\nimport mlflow.pyfunc\nfrom torch import load as torch_load\n\u00a0\npy_model = torch_load( \"/dbfs/FileStore/ml/ner_model\" , map_location = torch_device(ner_gpu_device))\nmlflow.pytorch.save_model(py_model,path=\"/dbfs/FileStore/pyfunc/ner_model\")\n\u00a0\nmodel = mlflow.pyfunc.load_model(\"/dbfs/FileStore/pyfunc/ner_model\")\nmlflow.register_model(model,\"ner_model\")\n\u00a0\n# loading the python_function model to register\nmodel = mlflow.pyfunc.load_model(\"/dbfs/FileStore/pyfunc/ner_model\")\nmodel_version = mlflow.register_model(model,\"ner_model\")",
        "Answers": [
            {
                "Answer_creation_date": "2021-12-08T17:12:13.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @Itachi_Naruto! My name is Kaniz, and I'm the technical moderator here. Great to meet you, and thanks for your question! Let's see if your peers in the community have an answer to your question first. Or else I will get back to you soon. Thanks.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-12-09T13:07:19.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "I think you want to use mflow to load the model not pytorch. There is a function in mlflow to load pytorch models https://www.mlflow.org/docs/latest/python_api/mlflow.pytorch.html#mlflow.pytorch.load_model\n\nThen once it's loaded, you can log it and register it as normal.",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "mlflow RESOURCE_ALREADY_EXISTS",
        "Question_creation_date": "2021-10-17T9:13Z",
        "Question_tag": [
            "AML",
            "RESOURCE ALREADY EXISTS",
            "MLflow Experiments",
            "Mlflow tracking",
            "MLFlow"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001UOu7rCAD/mlflow-resourcealreadyexists",
        "Question_upvote_count": 3,
        "Question_answer_count": 8,
        "Question_view_count": 797,
        "Question_has_accepted_answer": true,
        "Question_body": "I tried to log some run in my Databricks Workspace and I'm facing the following error: RESOURCE_ALREADY_EXISTS when I try to log any run.\n\n\u00a0\n\nI could replicate the error with the following code:\n\nimport mlflow\nimport mlflow.sklearn\nfrom mlflow.tracking import MlflowClient\n\u00a0\nmlflow.set_experiment('/learning/Mlflow-Full-Example/test-mlflow')\n\u00a0\nwith mlflow.start_run(run_name='silly_run-test') as run:\n  mlflow.log_param('seed', 777)\n\nThe error is the following, I don't know what to do about the conflict with that AML experiment\n\nIn case the error image has not quality enough here is the full message:\n\nRestException: RESOURCE_ALREADY_EXISTS: Failed to create AML experiment for experiment id=1823487114958629, name=/learning/Mlflow-Full-Example/test-mlflow, artifactLocation=dbfs:/databricks/mlflow-tracking/1823487114958629. There is an existing AML experiment with id=fa0eed6c-afd5-458b-9835-88903b535e04 and name='/adb/6432554542138879/1823487114958629/learning/Mlflow-Full-Example/test-mlflow' and artifactLocation='' that is not compatible.",
        "Answers": [
            {
                "Answer_creation_date": "2021-11-17T11:22:21.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @\u00a0mangeldfz! My name is Kaniz, and I'm the technical moderator here. Great to meet you, and thanks for your question! Let's see if your peers in the community have an answer to your question first. Or else I will get back to you soon. Thanks.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-11-17T11:43:28.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi Kaniz, thanks for your comment. I found another folk in the internet with the same problem starting a few days ago. So, I think it has nothing to do with my Workspace. Hope this will be solved soon, this is stopping all our machine learning developments.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-11-17T12:10:08.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "it seems like name conflict can you just rename to something different than test-mlflow.\n\nYou can also try to clean directories if there is nothing important (but I am not sure is/ adb on dbfs storage):\n\n\u00a0\n\ndbutils.fs.rm(\"/databricks/mlflow-tracking/1823487114958629\", recurse=True)\ndbutils.fs.rm(\"/adb/6432554542138879/1823487114958629/learning/Mlflow-Full-Example/test-mlflow\", recurse=True)\n\n\u00a0\n\n\u00a0\n\n\u00a0\n\nExpand Post",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-11-17T12:49:19.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "I tried renaming the experiment name and the run_name and it does not work, the error keeps the same. When I search for the experiment it is conflicting I can find the AML id using client.list_experiments(), this is the Experiment I have conflict, but it seems the conflict has to do with the AML part:\n\n\u00a0\n\nExperiment: artifact_location='dbfs:/databricks/mlflow-tracking/2288118769165005', experiment_id='2288118769165005', lifecycle_stage='active', name='/learning/Mlflow-Full-Example/test-mlflow-renamed2', tags={'mlflow.AML_EXPERIMENT_ID': '594197a2-c16e-4e14-8040-e398833198ff',\n\n'mlflow.experimentType': 'MLFLOW_EXPERIMENT',\n\n'mlflow.ownerEmail': '***.***@***.***',\n\n'mlflow.ownerId': 'YYYYYY'}\n\n\u00a0\n\nI can delete the whole experiment using the UI of Experiments if I try to delete any experiment using mlflow.delete_experiment() I get the same error of the beginning. Nevertheless, it does not work. Also, I can not find the /adb directory anywhere, it is not in the DBFS.\n\nExpand Post",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-11-17T13:01:43.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "It seems that for every experiment I create, mlflow creates also a AML experiment associated and all AML experiments are pointing to the same artifactLocation=\"\" by default. It does not matter if you delete all experiments using the UI, the garbage collector detects that there is (or there was) a experiment (an AML experiment) with artifactLocation=\"\", so there is a conflict for any new experiment you try to log things in.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-11-17T16:10:24.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @mangeldfz (Customer)\u200b\u00a0 it\u2019s not recommended to \u201clink\u201d the Databricks and AML workspaces, as we are seeing more problems.\u00a0You can refer to the instructions found below for using MLflow with AML.\u00a0\u00a0\u00a0https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-mlflow\n\n\u00a0\n\nYou can refer to https://github.com/MicrosoftDocs/azure-docs/issues/80298 to unlink.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-11-18T12:22:21.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi Prabakar, thank you so much for your response. Finally, we decided to delete the Azure Machine Learning service because the ARM in the reference that you provide throws the following error:\n\n\u00a0\n\n\u00a0\n\nI wonder if just redeploying the Azure Machine Learning service in the same resource group will be enough to set up both services properly or will be again a linking between them. I count with no mlflow communication between Databricks and the new Azure Machine Learning, of course.\n\nExpand Post",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-12-01T16:01:26.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi!\n\nI am facing the same problems with linked WS and wonder if you managed to find a solution to your problem by unlinking the spaces.",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "AI assistant and machine Learning",
        "Question_creation_date": "2021-11-3T7:26Z",
        "Question_tag": [
            "Ai",
            "AI Assistant",
            "Model Deployment"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001VSY32CAH/ai-assistant-and-machine-learning",
        "Question_upvote_count": 1,
        "Question_answer_count": 2,
        "Question_view_count": 76,
        "Question_has_accepted_answer": true,
        "Question_body": "I am looking to create a basic virtual assistant (AI) that implements machine learning mechanisms.\n\nI have some basic knowledge of Python and I have seen some courses on the internet (youtube in particular) that look very interesting.\n\nBut for the moment, I haven't seen any that cover the subject of AI evolution, for the most part (I've skimmed the scripts and videos) for each question you'll get more or less always the same answer.\n\nMy question dear friends is: where do I have to look to succeed in producing in python an AI that will evolve and give me different results depending on my appreciation of the previous results https://shagle.download?\n\nThanks",
        "Answers": [
            {
                "Answer_creation_date": "2021-11-23T10:35:11.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "I think you are looking for continual learning, there are a lot of interesting articles on it.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-11-23T11:10:30.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "basic virtual assistant - you mean like chat bot? There are some ready AWS services for that try Amazon Lex first",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "How to enable and disable Model Serving using Rest API",
        "Question_creation_date": "2021-10-15T16:49Z",
        "Question_tag": [
            "Model Serving",
            "Disable",
            "MLFlow",
            "Rest api",
            "Mlflow Model"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001UC6aZCAT/how-to-enable-and-disable-model-serving-using-rest-api",
        "Question_upvote_count": 0,
        "Question_answer_count": 2,
        "Question_view_count": 138,
        "Question_has_accepted_answer": true,
        "Question_body": "ML flow model serving in Databricks docs details the options to enable and disable from the UI\n\n\u00a0\n\nhttps://docs.databricks.com/applications/mlflow/model-serving.html",
        "Answers": [
            {
                "Answer_creation_date": "2021-11-15T16:53:20.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Please find below the REST APIs to enable and disable Model-Serving\n\n\u00a0\n\nBelow are the examples in Python\n\n\u00a0\n\nYou need to use the token to interact with Rest API\n\n\u00a0\n\ntoken = \"dxxxxxx\"\n\ninstance = \"https://<workspacexxx>.cloud.databricks.com\"\n\nheaders = {'Authorization': f'Bearer {token}'}\n\n\u00a0\n\nEnable Model Serving\n\n\u00a0\n\nimport requests\n\nurl = f'https://{instance}/api/2.0/mlflow/endpoints/enable'\n\nrequests.post(url, headers=headers, json={\"registered_model_name\": \"<model name>\"})\n\n\u00a0\n\n\u00a0\n\nDisable Model serving\n\n\u00a0\n\nurl = f'https://{instance}/api/2.0/mlflow/endpoints/disable'\n\nr = requests.post(url, headers=headers, json={\"registered_model_name\": \"<model name>\"})\n\nr.json()\n\nExpand Post",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-11-15T18:52:10.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @Mohit_m (Databricks)\u200b\u00a0,\n\n\u00a0\n\nGreat post. Thank you for sharing it with the rest of the community.",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "Older Spark Version loaded into the spark notebook",
        "Question_creation_date": "2021-9-20T15:47Z",
        "Question_tag": [
            "Databricks Runtime",
            "Spark Version",
            "Older Spark Version",
            "Databrciks Runtime"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001Q0UmcCAF/older-spark-version-loaded-into-the-spark-notebook",
        "Question_upvote_count": 1,
        "Question_answer_count": 10,
        "Question_view_count": 387,
        "Question_has_accepted_answer": false,
        "Question_body": "I have databricks runtime for a job set to latest 10.0 Beta (includes Apache Spark 3.2.0, Scala 2.12) .\n\nIn the notebook when I check for the spark version, I see version 3.1.0 instead of version 3.2.0\n\n\u00a0\n\nI need the Spark version 3.2 to process workloads as that version has the fix for https://github.com/apache/spark/pull/32788\n\n\u00a0\n\nScreenshot with the cluster configuration and the older spark version in notebook attached.",
        "Answers": [
            {
                "Answer_creation_date": "2021-10-20T16:11:02.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "If you use pool please check what preloaded version is set also in pool.\n\n\u00a0\n\nIf it is not that problem I can not help as I even don't see yet 10.0 (and after all it is Beta)",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-10-20T16:27:56.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "I got the same thing when I tested it out. I guess that's why it's Beta, should get fixed soon I imagine.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-10-20T16:54:41.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "I am not using the pool. Thanks for the update though. Hopefully this gets fixed soon.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-10-20T18:17:27.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "This is due to some legalese related to Open-Source Spark and Databricks' Spark. Since Open-Source Spark has not released v3.2 yet, we are not allowed to call the one on DBR 10 v3.2 yet. It may have that patch already in there though.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-10-20T18:45:26.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Thanks for the update @Dan Z (Databricks)\u200b\u00a0.\n\nJust ran the job again and still seeing spark version 3.1.0. Should I be using spark32 (or something similar) when invoking spark session for me to pick up the correct spark version?\n\n\u00a0\n\nAny ETA on the spark 3.2 version availability will be great.\n\n\u00a0\n\nThanks\n\nExpand Post",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-10-20T19:11:51.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "It should have all the features you need. Check it out. Legally we can't call it Spark 3.2 yet.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-10-20T19:25:10.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "I do not think it is loading Spark 3.2. I am still seeing the issue with writeUTF which has been fixed in Spark 3.2 -> https://github.com/apache/spark/pull/32788\n\n\u00a0\n\nCaused by: java.io.UTFDataFormatException: encoded string too long: 97548 bytes\n\tat java.io.DataOutputStream.writeUTF(DataOutputStream.java:364)\n\tat java.io.DataOutputStream.writeUTF(DataOutputStream.java:323)\n\nAnyways, I will wait for the databricks runtime to correctly reflect the correct version.\n\nExpand Post",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-10-20T21:44:07.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Yes- this version probably has the Databricks internal features slated for Spark 3.2, but the features/patches contributed by the open-source community may still be coming. Sorry this isn't available yet. I'm sure it will be very soon. Happy coding!",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-10-21T12:32:23.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "I just noticed that (on Azure anyway) 10.0 is NOT in beta anymore.\n\nSo 'very soon' was indeed very soon.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-10-21T16:47:45.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "hi @dbu_spark (Customer)\u200b\u00a0,\n\n\u00a0\n\nGood news, DBR 10 was release yesterday October 20th. You can find more details in the release notes website",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "Real-time model serving and monitoring on Databricks at scale",
        "Question_creation_date": "2021-8-10T5:53Z",
        "Question_tag": [
            "Model Deployment",
            "Serving",
            "Model",
            "Real Time Model Serving",
            "Model Monitoring"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001J4P7VCAV/realtime-model-serving-and-monitoring-on-databricks-at-scale",
        "Question_upvote_count": 0,
        "Question_answer_count": 8,
        "Question_view_count": 410,
        "Question_has_accepted_answer": false,
        "Question_body": "How to deploy real-time model on databricks at scale? Right now, The model serving is very limited to 20 requests per second.\u00a0Also, There are no model monitoring framework/graphs\u00a0like the one's provided with AzureML or Sagemaker frameworks.",
        "Answers": [
            {
                "Answer_creation_date": "2021-09-10T06:34:04.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "You might wanna look into MLFlow.\n\nBut as far as the deployment of models goes, MLFlow only does local REST APIs afaik.\n\nAdded to that you can also deploy to AzureML or Sagemaker.\n\nNot sure what Databricks's plans are on the deployment part. I think they probably will go for out of the box integration with existing platforms, but Databricks people in here might shine a light on this.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-09-10T06:47:54.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "https://docs.databricks.com/applications/mlflow/model-serving.html\n\n\u00a0\n\nI guess I am not up to date anymore.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-09-10T08:48:27.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "@werners (Customer)\u200b\u00a0:\n\nThe real time capability is not yet scalable, but I have heard about an update to this in August product roadmap where databricks team have bifurcated the serving layer into 2 parts (Batch and Real-time). Not sure how much scalability is improved.\n\n\u00a0\n\nAlso, There is nothing around model monitoring which is a big challenge while going to real-time model serving architecture.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-09-10T09:06:33.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Agree.\n\nWhich is why at my company we look at Azure ML.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-09-10T12:06:50.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "For real time serving probably you will have to look into container services with Kubernetes. And agree deployed through Azure ML",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-09-11T05:39:12.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "It's accurate that the current Databricks model serving product has limitations regarding scalability.\n\n\u00a0\n\nThat being said, MLflow has built-in deployment tools for serving products, including cloud services and open source alternatives.\n\n\u00a0\n\nWe do have improvements to both our serving product regarding scalability AND monitoring on our roadmap. Happy to discuss if you are interested!\n\nExpand Post",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-09-27T06:08:47.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "@clemens.mewald (Databricks)\u200b\u00a0: Thanks for your response.\n\nI have heard about serving 2.0 . Would you be able to provide a rough timeline on when it will be available?\n\nDoes it include the below requirements:\n\nmulti-endpoint deployment (One model being deployed with multiple endpoints).\nmulti-region deployment (One model having end-points in different regions).\nmulti-model endpoints deployment (One end-point supporting multiple models)\n\n\u00a0\n\nAlso, When will the apache airflow native integration would be available to use on databricks?\n\nExpand Post",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-10-10T16:29:00.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "I believe the next update to serving will include 1, not 2 (this is still within a Databricks workspace in a region). I don't think multi-model endpoints are on the roadmap next.\n\n\u00a0\n\nHow does Airflow integration relate?",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "MLFlow Serve Logging",
        "Question_creation_date": "2021-8-14T13:59Z",
        "Question_tag": [
            "Rest Endpoints",
            "MLFlow",
            "Azure databricks",
            "Logging"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001JwLtHCAV/mlflow-serve-logging",
        "Question_upvote_count": 1,
        "Question_answer_count": 9,
        "Question_view_count": 390,
        "Question_has_accepted_answer": true,
        "Question_body": "When using Azure Databricks and serving a model, we have received requests to capture additional logging. In some instances, they would like to capture input and output or even some of the steps from a pipeline.\n\n\u00a0\n\nIs there any way we can extend the logging with a MLFlow rest endpoint to capture additional required information?",
        "Answers": [
            {
                "Answer_creation_date": "2021-09-14T14:22:55.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @\u00a0BeardyMan! My name is Kaniz, and I'm the technical moderator here. Great to meet you, and thanks for your question! Let's see if your peers on the community have an answer to your question first. Or else I will follow up shortly with a response.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-09-14T18:19:08.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "To my knowledge, if you write a custom model's predict() function, you can do any arbitrary operations in it (log inputs or outputs somewhere).",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-09-14T19:32:02.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Do you mean to use azure functions and custom python code to call the model and then perform the logging required rather than using the mlflow serve capability and the managed rest endpoint? \u200b",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-09-14T21:05:19.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "My thought was:\n\nCreate a custom model with a predict function that does extra work (like logging)\nRegister the Model\nRun the model in Model Serving",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-09-14T22:46:00.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Thank you for the clarification, I understand what you mean now and that's exactly what I was hoping for! :)",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-09-14T21:37:45.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Here is an example of a custom model based on the sklearn model \"GradientBoostingClassifier\":\n\nclass CustomizedGradientBoostingClassifier(sklearn.ensemble.GradientBoostingClassifier):\n  def __init__(self, random_state):\n    super().__init__(random_state=random_state)\n  \n  def fit(self, X, y):\n    super().fit(X, y)\n  \n  def predict_proba(self, X_test):\n    return super().predict_proba(X_test)\n  \n  def predict(self, X):\n    # Do customized tasks here (e.g. issueing an RPC calll to log the input and output)\n    \n    # For example, you can also return not only the predicted result, but also the input\n    return (super().predict(X), X)\n\nYou can register the model as usual. When you invoke the REST endpoint, it does some custom things in the predict() function, and returns not only the predicted result, but also the input.\n\nExpand Post",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-09-14T22:46:40.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Thank you @ChenranLi (Customer)\u200b\u00a0 the example is exceedingly helpful. I will be sure to try this out!",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-09-15T01:14:55.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Another word from a Databricks employee:\n\n\u00a0\n\n\"\"\"\n\nYou can use the custom model approach but configuring it is painful. Plus you have ended every loggable model in the custom model.\u00a0Another less intrusive solution would be to have a proxy server do the logging and then defer to MLflow model server. See very basic POC:\u00a0https://github.com/amesar/mlflow-model-monitoring\n\n\u00a0\n\nAlso check out Seldon Alibi for advanced monitoring.\n\n\"\"\n\nExpand Post",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-09-15T19:54:50.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Thank you, Dan. We had originally suggested the route of using azure api manager or using an azure function as like an api wrapper to do the logging we want and the forwarding on the call to the mlfmow model serve rest endpoint. I was just wondering if there was a better alternative or something obvious we were missing. \u200b",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "how to include a third-party Maven package in MLflow model serving job cluster in Azure Databricks",
        "Question_creation_date": "2021-6-29T21:11Z",
        "Question_tag": [
            "MLFlow",
            "Maven"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001HKI9NCAX/how-to-include-a-thirdparty-maven-package-in-mlflow-model-serving-job-cluster-in-azure-databricks",
        "Question_upvote_count": 1,
        "Question_answer_count": 2,
        "Question_view_count": 202,
        "Question_has_accepted_answer": false,
        "Question_body": "We try to use MLflow Model Serving, this service will enable realtime model serving behind a REST API interface; it will launch a single-node cluster that will host our model.\n\nThe issue happens when the single-node cluster try to get the environment ready base on a conda.yaml file that created when log the model using MLflow. But it looks like I can only specify a pip install but not a Maven package.\n\n\u00a0\n\n\u00a0\n\nconda_env = _mlflow_conda_env(\n\nadditional_conda_deps=None,\n\n\u00a0\n\nadditional_pip_deps=[\"cloudpickle=={}\".format(cloudpickle.version), \"scikit-learn=={}\".format(sklearn.version),\"pyspark==3.0.0\".format(pyspark.version))],\n\n\u00a0\n\nadditional_conda_channels=None,\n\n\u00a0\n\n)\n\nhow can I tell the cluster to install a maven jar file?",
        "Answers": [
            {
                "Answer_creation_date": "2021-09-01T17:45:16.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "I don't believe you can do that at the moment. Is it required for a Python model? only Python-based models can really be served this way at the moment",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-09-14T13:55:21.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Unfortunately we came across this same issue. We were trying to use MLFlow Serve to produce an API that could take text input and pass it through some NLP. In this instance we had installed a maven package on the cluster, so the experiment would run fine in a notebook, but MLFlow would fail as it couldn't install the maven package. As an alternative, it would help to be able to modify the job cluster that is provisioned to add additional libraries/packages that are required, that we can not specify in the conda definition.",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "Libraries in Databricks Runtimes",
        "Question_creation_date": "2021-5-17T23:34Z",
        "Question_tag": [
            "Libraries",
            "DBR",
            "Databricks Runtime"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001GHVerCAH/libraries-in-databricks-runtimes",
        "Question_upvote_count": 0,
        "Question_answer_count": 3,
        "Question_view_count": 98,
        "Question_has_accepted_answer": true,
        "Question_body": "Is it possible to easily determine what libraries and which version are included in a specific DBR Version?",
        "Answers": [
            {
                "Answer_creation_date": "2021-06-17T23:56:16.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Runtime release notes has the details . E.g https://docs.databricks.com/release-notes/runtime/8.3ml.html",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-09-13T12:48:41.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Keep an eye on the custom libraries you add to the runtime. It has dependencies. Best way is to manage it through a custom script rather than doing direct pip installs",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-09-13T16:16:02.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hello. My name is Piper and I'm one of the community moderators. One of the team members sent this information to me.\n\n\u00a0\n\nThis should be the correct path to check libraries installed with DBRs.\n\nhttps://docs.databricks.com/release-notes/runtime/8.3ml.html?_ga=2.106263619.1376989843.1631376279-1857667707.1619420936#python-libraries",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "What are the advantages of using Delta if I am using MLflow? How is Delta useful for DS/ML use cases?",
        "Question_creation_date": "2021-5-21T21:03Z",
        "Question_tag": [
            "Delta",
            "MLFlow",
            "ML Use Cases",
            "Delta Tables"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001GHVPwCAP/what-are-the-advantages-of-using-delta-if-i-am-using-mlflow-how-is-delta-useful-for-dsml-use-cases",
        "Question_upvote_count": 0,
        "Question_answer_count": 2,
        "Question_view_count": 88,
        "Question_has_accepted_answer": true,
        "Question_body": "I am already using MLflow. What benefit would Delta provide me since I am not really working on Data engineering workloads",
        "Answers": [
            {
                "Answer_creation_date": "2021-06-22T09:58:16.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Because Delta can version data, it becomes useful for reproducibility and debugging of models. Weeks later you could see exactly how the table looked when the model was built. MLflow's \"Spark\" autologging actually helps automatically capture and log this version information when Delta is used in a Databricks notebook.\n\n\u00a0\n\nIts transactional writes are useful, as a modeling job does not need to worry about other data engineering jobs writing to the same data source at the same time. To a lesser extent, being able to write Delta Live Tables and/or being able to roll back bad writes increases the reliability of upstream data, which helps with downstream reliability of ML jobs.\n\nExpand Post",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-09-13T13:00:01.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "The most important aspect is your experiment can track the version of the data table. So during audits you will be able to trace back why a specific prediction was made.",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "How do I specify a version of a library different from the Databricks runtime?",
        "Question_creation_date": "2021-5-23T20:26Z",
        "Question_tag": [
            "Data Ingestion & connectivity,",
            "Version",
            "Library",
            "Databricks Runtime",
            "Databricks notebook"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001GHVOACA5/how-do-i-specify-a-version-of-a-library-different-from-the-databricks-runtime",
        "Question_upvote_count": 0,
        "Question_answer_count": 3,
        "Question_view_count": 185,
        "Question_has_accepted_answer": false,
        "Question_body": "",
        "Answers": [
            {
                "Answer_creation_date": "2021-06-23T21:20:04.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "The question is not clear. What kind of library are you referring to here?\n\n\u00a0\n\nFor Python libraries, it's possible to remove the existing libraries and install the new ones using an init script\nFor Jar libraries, adding a different version of jar could be very risky.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-09-01T19:19:25.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "For Python libs, the simpler thing is to use %pip install to install them in a notebook. Yes, it could potentially break compatibility with other installed versions. pip will try to figure that out. That could be simpler to deal with than in an init script, and digging for errors on cluster creation.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-09-08T17:47:11.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "the best solution is to store the .whl locally and do a pip install of the local whl while server boots up. this will freeze the library version. if you install from the pip it might impact your production work.",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "MLflow not logging metrics",
        "Question_creation_date": "2021-5-25T16:21Z",
        "Question_tag": [
            "Metrics",
            "MLflow Experiments",
            "MLFlow",
            "Autologging"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001GHVQtCAP/mlflow-not-logging-metrics",
        "Question_upvote_count": 0,
        "Question_answer_count": 1,
        "Question_view_count": 107,
        "Question_has_accepted_answer": false,
        "Question_body": "I have run a few MLflow experiments and I can see them in the experiment history, but none of the metrics have been logged along with them. I thought this was supposed to be automatically included. Any idea why they wouldn't be showing up?",
        "Answers": [
            {
                "Answer_creation_date": "2021-09-03T07:36:31.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @\u00a0trevor.bishop! My name is Kaniz, and I'm the technical moderator here. Great to meet you, and thanks for your question! Let's see if your peers on the Forum have an answer to your question first. Or else I will follow up shortly with a response.",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "mlflow project train and validate - Control over the data used in the script?",
        "Question_creation_date": "2021-6-21T24:41Z",
        "Question_tag": [
            "MLFlow",
            "Mlflow project"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001HKIIPCA5/mlflow-project-train-and-validate-control-over-the-data-used-in-the-script",
        "Question_upvote_count": 0,
        "Question_answer_count": 1,
        "Question_view_count": 100,
        "Question_has_accepted_answer": false,
        "Question_body": "Hi there,\n\nTrying to decide if I am going to get started with ml and really enjoyed it so far.\n\nWhen going through the documentation, there was a blocker moment for me, as I feel the documentation doesn't mention much about the dataset used to train the model.\n\nModel = Data + (Algorithm & hyperparameters )\n\nI don't see an example in documentation where MLprojects is ran on different data (CSV ,SQL or code based etc..),\n\nThe code shown in the screenshot\n\n\"mlflow run sklearn_elasticnet_wine -P alpha = 0.5 would retrain a model with different hyperparameters, but on what data?\n\nHas it already been included in the project, and can you change it to train the model on different data.\n\nHow do you store and track the datasets being used?\n\nCan someone explain please?\n\nThanks,",
        "Answers": [
            {
                "Answer_creation_date": "2021-09-03T07:37:36.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @\u00a0VirajV! My name is Kaniz, and I'm the technical moderator here. Great to meet you, and thanks for your question! Let's see if your peers on the Forum have an answer to your question first. Or else I will follow up shortly with a response.",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "Where can I get documentation around managed MLFlow? What is the difference between open source and managed MLFlow?",
        "Question_creation_date": "2021-5-25T22:25Z",
        "Question_tag": [
            "Open source",
            "MLFlow"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001GHVLdCAP/where-can-i-get-documentation-around-managed-mlflow-what-is-the-difference-between-open-source-and-managed-mlflow",
        "Question_upvote_count": 0,
        "Question_answer_count": 1,
        "Question_view_count": 69,
        "Question_has_accepted_answer": false,
        "Question_body": "",
        "Answers": [
            {
                "Answer_creation_date": "2021-09-01T17:45:50.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Have a look at https://databricks.com/product/managed-mlflow",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "AttributeError: 'DataFrame' object has no attribute 'rename'",
        "Question_creation_date": "2019-3-30T23:20Z",
        "Question_tag": [
            "Data Ingestion & connectivity,",
            "Machine Learning",
            "Python"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001HKHmYCAX/attributeerror-dataframe-object-has-no-attribute-rename",
        "Question_upvote_count": 0,
        "Question_answer_count": 2,
        "Question_view_count": 453,
        "Question_has_accepted_answer": true,
        "Question_body": "Hello, I am doing the Data Science and Machine Learning course.\n\nThe Boston housing has unintuitive column names. I want to rename them, e.g. so 'zn' becomes 'Zoning'.\n\nWhen I run this command:\n\ndf_bostonLegible = df_boston.rename({'zn':'Zoning'}, axis='columns')\n\nThen I get the error \"AttributeError: 'DataFrame' object has no attribute 'rename'\".\n\nI did a websearch and found a few guides that were inapplicable:\n\nhttps://stackoverflow.com/questions/28163439/attributeerror-dataframe-object-has-no-attribute-height/28163504\n\nhttps://stackoverflow.com/questions/38134643/data-frame-object-has-no-attribute",
        "Answers": [
            {
                "Answer_creation_date": "2019-05-01T10:05:45.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi @PHorniak (Customer)\n\nYou can use\n\ndf_bostonLegible = df_boston.withColumnRenamed(\"zn\", \"Zoning\")\n\nplease accept the answer if it works or revert back with questions\n\nThanks",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2019-05-01T12:35:55.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "wow great information totally love it buddy.",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "What is the difference between Databricks Runtime and Databricks Runtime for ML? Can I add additional packages ?",
        "Question_creation_date": "2021-5-4T18:38Z",
        "Question_tag": [
            "Databricks Runtime"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001GHVi3CAH/what-is-the-difference-between-databricks-runtime-and-databricks-runtime-for-ml-can-i-add-additional-packages-",
        "Question_upvote_count": 0,
        "Question_answer_count": 4,
        "Question_view_count": 139,
        "Question_has_accepted_answer": true,
        "Question_body": "",
        "Answers": [
            {
                "Answer_creation_date": "2021-06-04T11:48:00.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Databricks Runtime for ML is optimized for machine learning workloads and comes pre-installed with many of the most popular ML libraries. You can install additional packages in both runtimes. Databricks Runtime for ML is essentially an extension of the Databricks Runtime. See documentation.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-06-18T00:14:10.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Databricks Runtime ML is a variant of Databricks Runtime that adds multiple popular machine learning libraries, including TensorFlow, Keras, PyTorch, and XGBoost.\n\n\u00a0\n\nYes - It is possible to add additional packages/libraries as described here.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-06-22T22:53:48.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Please see https://docs.databricks.com/runtime/mlruntime.html",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-06-23T03:46:14.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Please see https://docs.databricks.com/release-notes/runtime/releases.html for complete details on DBR and DBR with M",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "Unable to get mlflow central model registry to work with dbconnect.",
        "Question_creation_date": "2021-5-13T23:48Z",
        "Question_tag": [
            "Team Members",
            "MLFlow",
            "Mlops"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001GHVYxCAP/unable-to-get-mlflow-central-model-registry-to-work-with-dbconnect",
        "Question_upvote_count": 1,
        "Question_answer_count": 1,
        "Question_view_count": 125,
        "Question_has_accepted_answer": true,
        "Question_body": "I'm working on setting up tooling to allow team members to easily register and load models from a central mlflow model registry via dbconnect.\n\nHowever after following the instructions at the public docs , hitting this error\n\nraise _NoDbutilsError\nmlflow.utils.databricks_utils._NoDbutilsError \n\nLooks like _get_dbutils\u00a0in mlflow expects\u00a0a valid instance of\u00a0 ip_shell. Any pointers or workarounds on how to get dbconnect to work with central model registry",
        "Answers": [
            {
                "Answer_creation_date": "2021-06-18T20:55:43.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "You could monkey patch MLFlow's _get_dbutils() with something similar to this to get this working while connecting from dbconnect\n\nspark = SparkSession.builder.getOrCreate()\n# monkey-patch MLFlow's _get_dbutils()\ndef _get_dbutils():\n    return DBUtils(spark)\n\u00a0\nmlflow.utils.databricks_utils._get_dbutils = _get_dbutils\n\n\u00a0\n\nExpand Post",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "What are the LTS Databricks Runtimes?",
        "Question_creation_date": "2021-5-18T12:17Z",
        "Question_tag": [
            "LTS",
            "DBR",
            "Databricks Runtime"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001GHVejCAH/what-are-the-lts-databricks-runtimes",
        "Question_upvote_count": 0,
        "Question_answer_count": 1,
        "Question_view_count": 114,
        "Question_has_accepted_answer": true,
        "Question_body": "What are the LTS Databricks Runtime versions available in the cluster configuration page? Example DBR 7.3 LTS",
        "Answers": [
            {
                "Answer_creation_date": "2021-06-18T03:05:00.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Long Term Support (LTS) versions are supported for two years and non LTS version are supported for 6 months.\n\n\u00a0\n\nRefer to below links for more details.\n\nhttps://docs.databricks.com/release-notes/runtime/databricks-runtime-ver.html#runtime-support https://docs.databricks.com/release-notes/runtime/releases.html",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "Is there any file size overhead when I save models using MLflow?",
        "Question_creation_date": "2021-5-16T22:28Z",
        "Question_tag": [
            "File Size",
            "MLFlow"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001GHVg6CAH/is-there-any-file-size-overhead-when-i-save-models-using-mlflow",
        "Question_upvote_count": 0,
        "Question_answer_count": 1,
        "Question_view_count": 68,
        "Question_has_accepted_answer": true,
        "Question_body": "",
        "Answers": [
            {
                "Answer_creation_date": "2021-06-17T19:56:25.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "There shouldn't be. Generally speaking, models will be serialized according to their 'native' format for well-known libraries like Tensorflow, xgboost, sklearn, etc. Custom model will be saved with pickle. The files exist on distributed storage as artifacts. MLflow can and does log additional metadata with the model, like its schema, sample input, environment requirements, but these are tiny additional files.",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "What is the important / benefits of tracking artifacts in MLflow tracking?",
        "Question_creation_date": "2021-5-21T21:05Z",
        "Question_tag": [
            "MLFlow",
            "Artifacts"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001GHVPvCAP/what-is-the-important-benefits-of-tracking-artifacts-in-mlflow-tracking",
        "Question_upvote_count": 0,
        "Question_answer_count": 1,
        "Question_view_count": 60,
        "Question_has_accepted_answer": true,
        "Question_body": "",
        "Answers": [
            {
                "Answer_creation_date": "2021-06-22T09:55:36.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "For me, the main benefit is that it is little or no work to enable. For example, when autologging is enabled for a library like sklearn or Pytorch, a lot of information about a model is captured with no additional steps. Further in Databricks, the tracking server receiving this information is also managed for you. Even where MLflow logging is done manually, it's relatively trivial to instrument existing ML code with those calls.\n\n\u00a0\n\nTracking is useful for a few reasons. First it helps during experimentation, when one wants to compare the results of many runs, maybe from a hyperparameter sweep. It's useful to have a link to the exact revision of the code that produced a model rather than try to remember or write down just what bits of code were commented in/out during that best run.\n\n\u00a0\n\nIt assists in reproducibility by capturing not just the model, but metadata like the version of libraries used, the version of data in Delta tables used in the model, the revision of the code, and who built the model and when.\n\n\u00a0\n\nThe Model Registry builds on tracking to add workflow for testing and permissions to the production promotion process, which is important for integrity of a deployment of a production model.\n\n\u00a0\n\nFinally with that captured information, deployment becomes simpler. The resulting artifact can be retrieved as a Spark UDF, or turned on as a REST API.\n\nExpand Post",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "Do errors get logged as part of the artifact tracking in MLflow? Or is there a way to log errors in general?",
        "Question_creation_date": "2021-5-21T21:15Z",
        "Question_tag": [
            "Tracking",
            "MLFlow"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001GHVPuCAP/do-errors-get-logged-as-part-of-the-artifact-tracking-in-mlflow-or-is-there-a-way-to-log-errors-in-general",
        "Question_upvote_count": 0,
        "Question_answer_count": 1,
        "Question_view_count": 124,
        "Question_has_accepted_answer": true,
        "Question_body": "",
        "Answers": [
            {
                "Answer_creation_date": "2021-06-22T09:50:53.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "For the tracking server? Yes, it does produce logs which you could see if running the tracking server as a standalone service. They are not exposed from the hosted tracking server in Databricks. However there typically aren't errors or logs of interest in the tracking server; errors of interest would occur (and be logged) from the MLflow client that is accessing it, for example, in a notebook cell.\n\n\u00a0\n\nFor model serving, errors are likewise logged, but would be of interest and are exposed in the hosted MLflow serving service. You would see them in the Serving tab of a registered model. Or if deploying to Azure ML or SageMaker, these logs would likewise appear in the console of those services.\n\nExpand Post",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "Can I save MLflow artifacts to locations other than the dbfs?",
        "Question_creation_date": "2021-5-25T17:29Z",
        "Question_tag": [
            "MLflow Artifacts",
            "MLFlow",
            "Default Location"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001GHVQWCA5/can-i-save-mlflow-artifacts-to-locations-other-than-the-dbfs",
        "Question_upvote_count": 0,
        "Question_answer_count": 1,
        "Question_view_count": 216,
        "Question_has_accepted_answer": true,
        "Question_body": "The default location or MLflow artifacts is on dbfs, but I would like to save my models to an alternative location. Is this supported, and if it is how can I accomplish it?",
        "Answers": [
            {
                "Answer_creation_date": "2021-06-25T22:46:05.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "You could mount an s3 bucket in the workspace and save your model using the mounts DBFS\u00a0path\n\nFor e.g\n\nmodelpath = \"/dbfs/my-s3-bucket/model-%f-%f\" % (alpha, l1_ratio)\nmlflow.sklearn.save_model(lr, modelpath)\n\n\u00a0\n\nExpand Post",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "Can I copy my MLflow experiments from one workspace to another?",
        "Question_creation_date": "2021-5-25T17:33Z",
        "Question_tag": [
            "MLflow Experiments",
            "MLFlow",
            "Different Databricks Workspace"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001GHVQSCA5/can-i-copy-my-mlflow-experiments-from-one-workspace-to-another",
        "Question_upvote_count": 0,
        "Question_answer_count": 1,
        "Question_view_count": 310,
        "Question_has_accepted_answer": true,
        "Question_body": "I would like to move my saved experiments and artifacts to a different Databricks workspace from where I originally created them. Is this possible?",
        "Answers": [
            {
                "Answer_creation_date": "2021-06-25T22:35:50.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "It might be possible with a bit of code via mlflow client api ( there seems to be a way to run list_registered_models and extract info ) - but haven't tried it out. If the requirement is to share models between workspaces, one approach could be to have a central registry workspace as documented in https://docs.databricks.com/applications/machine-learning/manage-model-lifecycle/multiple-workspaces.html",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "How to find best model using python in mlflow",
        "Question_creation_date": "2021-5-18T10:47Z",
        "Question_tag": [
            "Web ui",
            "MLFlow",
            "Model Version",
            "Python Code"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001GHVePCAX/how-to-find-best-model-using-python-in-mlflow",
        "Question_upvote_count": 0,
        "Question_answer_count": 1,
        "Question_view_count": 102,
        "Question_has_accepted_answer": true,
        "Question_body": "I have a use case in mlflow with python code to find a model version that has the best metric (for instance, \u201caccuracy\u201d) among so many versions , I don't want to use web ui but to use python code to achieve this. Any Idea?",
        "Answers": [
            {
                "Answer_creation_date": "2021-06-18T10:48:01.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "import mlflow \nclient = mlflow.tracking.MlflowClient()\nruns = client.search_runs(\"my_experiment_id\", \"\", order_by=[\"metrics.rmse DESC\"], max_results=1)\nbest_run = runs[0]\n\n\u00a0\n\nhttps://mlflow.org/docs/latest/python_api/mlflow.tracking.html#mlflow.tracking.MlflowClient.search_runs\n\nExpand Post",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "Can we delte Mlflow experiment",
        "Question_creation_date": "2021-5-18T24:13Z",
        "Question_tag": [
            "MLFlow",
            "Permanent Deletion",
            "MLflow Experiment"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001GHVeJCAX/can-we-delte-mlflow-experiment",
        "Question_upvote_count": 0,
        "Question_answer_count": 2,
        "Question_view_count": 614,
        "Question_has_accepted_answer": true,
        "Question_body": "I am using ML flow and my need of the hour is to delete an experiment and want to create another experiment with same run.\n\nclient = MlflowClient(tracking_uri=server)\nclient.delete_experiment(1)\n\nThis deletes the\n\nexperiment, but when I run a new experiment with the same name as the one I just deleted, it will return this error:\n\n\u00a0\n\nYou can restore the experiment, or permanently delete the experiment to create a new one",
        "Answers": [
            {
                "Answer_creation_date": "2021-06-18T12:15:25.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "I think this is the limitation and now we have no way to do this via the UI or CLI\n\nThe way to do it depends on the type of backend file store that you are using.\n\n\u00a0\n\nIf you are using file store it is easy as the deleted experiment is stored in the trash folder\n\n.t\n\u00a0\nrm -rf mlruns/.trash/*\n\n\u00a0\n\n\u00a0\n\n\u00a0\n\nExpand Post",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-06-18T12:16:06.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "SQL Database:\n\nThis is more tricky, as there are dependencies that need to be deleted. I am using MySQL, and these commands work for me:\n\nUSE mlflow_db;  # the name of your database\nDELETE FROM experiment_tags WHERE experiment_id=ANY(\n    SELECT experiment_id FROM experiments where lifecycle_stage=\"deleted\"\n);\nDELETE FROM latest_metrics WHERE run_uuid=ANY(\n    SELECT run_uuid FROM runs WHERE experiment_id=ANY(\n        SELECT experiment_id FROM experiments where lifecycle_stage=\"deleted\"\n    )\n);\nDELETE FROM metrics WHERE run_uuid=ANY(\n    SELECT run_uuid FROM runs WHERE experiment_id=ANY(\n        SELECT experiment_id FROM experiments where lifecycle_stage=\"deleted\"\n    )\n);\nDELETE FROM tags WHERE run_uuid=ANY(\n    SELECT run_uuid FROM runs WHERE experiment_id=ANY(\n        SELECT experiment_id FROM experiments where lifecycle_stage=\"deleted\"\n    )\n);\nDELETE FROM runs WHERE experiment_id=ANY(\n    SELECT experiment_id FROM experiments where lifecycle_stage=\"deleted\"\n);\nDELETE FROM experiments where lifecycle_stage=\"deleted\";\n\n\u00a0\n\nExpand Post",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "Can we have multiple MLflo run in parallel ?",
        "Question_creation_date": "2021-5-25T13:46Z",
        "Question_tag": [
            "Python",
            "MLFlow",
            "Runs"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001GHVRSCA5/can-we-have-multiple-mlflo-run-in-parallel-",
        "Question_upvote_count": 0,
        "Question_answer_count": 1,
        "Question_view_count": 148,
        "Question_has_accepted_answer": true,
        "Question_body": "",
        "Answers": [
            {
                "Answer_creation_date": "2021-06-25T13:46:29.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "I think you cab find a solution on github page of ml flow - code examples here:\u00a0https://github.com/mlflow/mlflow/issues/3592",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "Exception: Run with UUID l567845ae5a7cf04a40902ae789076093c is already active.",
        "Question_creation_date": "2021-5-25T13:22Z",
        "Question_tag": [
            "MLFlow",
            "Import",
            "MLflow Experiment"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001GHVRWCA5/exception-run-with-uuid-l567845ae5a7cf04a40902ae789076093c-is-already-active",
        "Question_upvote_count": 0,
        "Question_answer_count": 1,
        "Question_view_count": 437,
        "Question_has_accepted_answer": true,
        "Question_body": "I'm trying to create a new experiment on mlflow but I have this problem:\n\nException: Run with UUID l142ae5a7cf04a40902ae9ed7326093c is already active.\n\n\u00a0\n\n\u00a0snippet\n\nmlflow.set_experiment(\"New experiment 2\")\n\u00a0\n\u00a0\nmlflow.set_tracking_uri('http://mlflow:5000')\n\u00a0\nst= mlflow.start_run(run_name='kun')\n\u00a0\nid = st.info.run_id\n\u00a0\nmlflow.log_metric(\"score\", score)\n\u00a0\nmlflow.sklearn.log_model(model, \"FinalModel\")",
        "Answers": [
            {
                "Answer_creation_date": "2021-06-25T13:22:31.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "You have to run mlflow.end_run() to finish the first experiment. Then you can create another",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "Error loading h2o model in mlflow",
        "Question_creation_date": "2021-7-6T15:36Z",
        "Question_tag": [
            "H2o",
            "MLFlow"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001HKI6sCAH/error-loading-h2o-model-in-mlflow",
        "Question_upvote_count": 0,
        "Question_answer_count": 5,
        "Question_view_count": 221,
        "Question_has_accepted_answer": false,
        "Question_body": "I'm getting the following error when I'm trying to load a h2o model using mlflow for prediction\n\nError:\n\n   Error\n   Job with key $03017f00000132d4ffffffff$_990da74b0db027b33cc49d1d90934149 failed with an exception: java.lang.IllegalArgumentException: Test/Validation dataset has no columns in common with the training set \n\nSource code:\n\n# !pip install requests # !pip install tabulate # !pip install \"colorama>=0.3.8\" # !pip install future # !pip install -f http://h2o-release.s3.amazonaws.com/h2o/latest_stable_Py.html h2o # !pip install mlflow # !wget https://github.com/mlflow/mlflow-example/blob/master/wine-quality.csv\n\n import h2o\n import random\n import mlflow\n import mlflow.h2o\n from h2o.estimators.random_forest import H2ORandomForestEstimator\n h2o.init()\n wine = h2o.import_file(path=\"winequality.csv\")\n r = wine['quality'].runif()\n train = wine[r  &lt; 0.7]\n test  = wine[0.3 &lt;= r]\n mlflow.set_tracking_uri('https://mlflow.xxxxxxx.cloud/')\n mlflow.set_experiment(\"H2ORandomForestEstimator\")\n \n def train_random_forest(ntrees):\n     with mlflow.start_run():\n         rf = H2ORandomForestEstimator(ntrees=ntrees)\n         train_cols = [n for n in wine.col_names if n != \"quality\"]\n         rf.train(train_cols, \"quality\", training_frame=train, validation_frame=test)      \n         mlflow.log_param(\"ntrees\", ntrees)        \n         mlflow.log_metric(\"rmse\", rf.rmse())\n         mlflow.log_metric(\"r2\", rf.r2())\n         mlflow.log_metric(\"mae\", rf.mae())       \n         mlflow.h2o.log_model(rf, \"model\")        \n         h2o.save_model(rf)            \n         predict = rf.predict(test)        \n         print(predict.head())\n\u00a0\n for ntrees in [10, 20, 50, 100]:\n     train_random_forest(ntrees)</pre><pre>import mlflow\n logged_model = 's3://mlflow-sagemaker/1/66f7c015fe8d4fb080940f3d31003f49/artifacts/model'\n\u00a0\n # Load model as a PyFuncModel.\n loaded_model = mlflow.pyfunc.load_model(logged_model)\n\u00a0\n # Predict on a Pandas DataFrame.\n import pandas as pd\n loaded_model.predict(pd.DataFrame(test))</pre>",
        "Answers": [
            {
                "Answer_creation_date": "2021-08-06T22:56:45.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "I ran this in Databricks and it worked with no issues. I suggest you make sure your wget path is correct, because the one you posted downloads HTML, not the raw csv. That may cause the problem.\n\n\u00a0\n\n%sh\nwget https://raw.githubusercontent.com/mlflow/mlflow-example/master/wine-quality.csv\n\n\u00a0\n\nimport h2o import random import mlflow import mlflow.h2o from h2o.estimators.random_forest import H2ORandomForestEstimator\n\nh2o.init() wine = h2o.import_file(path=\"./wine-quality.csv\") r = wine['quality'].runif() train = wine[r < 0.7] test = wine[0.3 <= r]\n\ndef train_random_forest(ntrees): with mlflow.start_run(): rf = H2ORandomForestEstimator(ntrees=ntrees) train_cols = [n for n in wine.col_names if n != \"quality\"] rf.train(train_cols, \"quality\", training_frame=train, validation_frame=test)\n\n\u00a0\n\nmlflow.log_param(\"ntrees\", ntrees)\n\n\u00a0\n\nmlflow.log_metric(\"rmse\", rf.rmse()) mlflow.log_metric(\"r2\", rf.r2()) mlflow.log_metric(\"mae\", rf.mae())\n\n\u00a0\n\nmlflow.h2o.log_model(rf, \"model\")\n\n\u00a0\n\nh2o.save_model(rf)\n\n\u00a0\n\npredict = rf.predict(test)\n\n\u00a0\n\nprint(predict.head()) for ntrees in [10, 20, 50, 100]: train_random_forest(ntrees\n\n\u00a0\n\nExpand Post",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-08-09T14:11:37.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "@Dan Zafar I mentioned the incorrect path in the original question but I did train the model with correct file.\n\n!wget https://raw.githubusercontent.com/mlflow/mlflow-example/master/wine-quality.csv\n\nThere is no issues when trying to predict using the h2o model object. But the prediction fails when using the MLFLOW's pyfunc flavour\n\nExpand Post",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-08-09T14:11:59.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "import mlflow logged_model = 's3://mlflow-s3 sagemaker/1/58e5371188ed4t649d2d75686a9f155d/artifacts/model' \n# Load model as a PyFuncModel. \nloaded_model = mlflow.pyfunc.load_model(logged_model) \n# Predict on a Pandas DataFrame. import pandas as pd \nloaded_model.predict(pd.DataFrame(test))\n\nExpand Post",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-08-09T14:13:45.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Error\n\nOSError: Job with key $03017f00000132d4ffffffff$_9993cede52525f90fe9729b1ddb24cf7 failed with an exception: java.lang.IllegalArgumentException: Test/Validation dataset has no columns in common with the training set stacktrace: java.lang.IllegalArgumentException: Test/Validation dataset has no columns in common with the training set at hex.Model.adaptTestForTrain(Model.java:1568)",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-08-09T14:14:24.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Error\n\n\u00a0\n\nstacktrace: java.lang.IllegalArgumentException: Test/Validation dataset has no columns in common with the training set at hex.Model.adaptTestForTrain(Model.java:1568) at hex.Model.adaptTestForTrain(Model.java:1404) at hex.Model.adaptTestForTrain(Model.java:1400) at hex.Model.score(Model.java:1697) at water.api.ModelMetricsHandler$1.compute2(ModelMetricsHandler.java:422) at water.H2O$H2OCountedCompleter.compute(H2O.java:1637)\n\nExpand Post",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "Can multiple users collaborate together on MLflow experiments?",
        "Question_creation_date": "2021-5-25T17:13Z",
        "Question_tag": [
            "MLflow Experiments",
            "Multiple users",
            "MLFlow"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001GHVQbCAP/can-multiple-users-collaborate-together-on-mlflow-experiments",
        "Question_upvote_count": 0,
        "Question_answer_count": 1,
        "Question_view_count": 375,
        "Question_has_accepted_answer": false,
        "Question_body": "Wondering about best practices for how to handle collaboration between multiple ML practitioners working on a single experiment. Do we have to share the same notebook between people or is it possible to have individual notebooks going but still work toward the same experiment?",
        "Answers": [
            {
                "Answer_creation_date": "2021-06-25T22:50:57.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Yes, multiple users could work on individual notebooks and still use the same experiment via mlflow.set_experiment(). You could also assign different permission levels to experiments from a governance point of view",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "Even the Unfinished Experiment in Mlflow is getting saved as finished",
        "Question_creation_date": "2021-5-25T13:36Z",
        "Question_tag": [
            "MLFlow",
            "Run"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001GHVRUCA5/even-the-unfinished-experiment-in-mlflow-is-getting-saved-as-finished",
        "Question_upvote_count": 0,
        "Question_answer_count": 1,
        "Question_view_count": 162,
        "Question_has_accepted_answer": false,
        "Question_body": "when I start the experiment with mlflow.start_run(),even if my script is interrupted or failed before executing\u00a0mlflow.end_run() ,the run gets tagged as finished instead of unfinished , Can any one help why it is happening here",
        "Answers": [
            {
                "Answer_creation_date": "2021-06-25T13:36:32.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "In note book the mlflow tagas ias the command travels and once failed or exit there itself it logs and finishes the experiment even if the noteboolsfails. However, if you want to continue logging metrics or artifacts to that run, you just need to use\u00a0\n\nmlflow.start_run(run_id=\"YourRunIDYouCanGetItFromUI\")\n\n.",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "How can I use Databricks to \"automagically\" distribute scikit-learn model training?",
        "Question_creation_date": "2021-5-24T20:29Z",
        "Question_tag": [
            "Machine Learning",
            "Scikit-learn",
            "Scikit",
            "Scaling",
            "Model Tuning",
            "Model Training"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001GHVSiCAP/how-can-i-use-databricks-to-automagically-distribute-scikitlearn-model-training",
        "Question_upvote_count": 0,
        "Question_answer_count": 1,
        "Question_view_count": 169,
        "Question_has_accepted_answer": false,
        "Question_body": "Is there a way to automatically distribute training and model tuning across a Spark cluster, if I want to keep using scikit-learn?",
        "Answers": [
            {
                "Answer_creation_date": "2021-06-24T20:42:11.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "It depends on what you mean by \"automagically.\"\n\n\u00a0\n\nIf you want to keep using scikit-learn, there are ways to distribute parts of training and tuning with minimal effort. However, there is no \"magic\" way to distribute training an individual model in scikit-learn; it is fundamentally a single-machine ML library, so training a model (e.g., a decision tree) in a distributed way requires a different implementation (like in Apache Spark MLlib).\n\n\u00a0\n\nYou can distribute some parts of the workflow easily:\n\nModel tuning and cross validation\nData prep and featurization\n\n\u00a0\n\nGood tools for distributing these workloads with scikit-learn include:\n\nHyperopt with SparkTrials: Hyperopt is a Python library for adaptive (smart & efficient) hyperparameter tuning, and there is a SparkTrials component which lets you scale tuning across a Spark cluster. See the Databricks docs (AWS, Azure, GCP) and the Hyperopt SparkTrials docs for more info.\njoblib-spark: Some algorithms in scikit-learn (especially the tuning and cross-validation tools) let you specify a parallel backend. You can use the joblib-spark backend to use Spark as that parallel backend. See the joblib-spark github page for an example.\nKoalas: This provides a Pandas API backed by Spark. Great for data prep. See the Koalas website for more info, and know that the Spark community plans to include this in future Spark releases.\nPandas UDFs in Spark DataFrames: These let you specify arbitrary code (such as scikit-learn featurization logic) in operations on distributed DataFrames. See these docs for more info (AWS, Azure, GCP).\n\n\u00a0\n\nExpand Post",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "What Databricks Runtime will I have to use if I want to leverage Python 2?",
        "Question_creation_date": "2021-5-23T2:39Z",
        "Question_tag": [
            "Python",
            "Data Ingestion & connectivity,",
            "Databricks Runtime",
            "Databricks Runtimes Support Python"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001GHVOrCAP/what-databricks-runtime-will-i-have-to-use-if-i-want-to-leverage-python-2",
        "Question_upvote_count": 0,
        "Question_answer_count": 2,
        "Question_view_count": 98,
        "Question_has_accepted_answer": false,
        "Question_body": "I have some code which is dependent on python 2. I am not able to use Python 2 with Databricks runtime 6.0.",
        "Answers": [
            {
                "Answer_creation_date": "2021-06-23T03:15:23.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "You'll have to use DBR 5.5 for Python 2. See https://kb.databricks.com/python/python-2-eol.html#databricks-runtime-55-lts for additional details. Bear in mind though that Python 2 is EOL as of Jan 2020",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-06-23T05:49:34.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "When you create a Databricks Runtime 5.5 LTS cluster by using the workspace UI, the default is Python 3. You have the option to specify Python 2. If you use the\u00a0Databricks REST API\u00a0to create a cluster using Databricks Runtime 5.5 LTS, the default is Python 2. If you have a Databricks Runtime 5.5 LTS cluster running Python 2, you are not required to upgrade to Python 3.",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "What's the difference between Normalization and Standardization?",
        "Question_creation_date": "2021-5-22T22:31Z",
        "Question_tag": [
            "Machine Learning",
            "Standard Deviation",
            "Difference"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001GHVPGCA5/whats-the-difference-between-normalization-and-standardization",
        "Question_upvote_count": 0,
        "Question_answer_count": 1,
        "Question_view_count": 241,
        "Question_has_accepted_answer": false,
        "Question_body": "Normalization\u00a0typically means rescales the values into a range of [0,1].\n\nStandardization\u00a0typically means rescales data to have a mean of 0 and a standard deviation of 1 (unit variance).",
        "Answers": [
            {
                "Answer_creation_date": "2021-06-23T05:37:08.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Normalization\u00a0typically means rescales the values into a range of [0,1].\u00a0Standardization\u00a0typically means rescales data to have a mean of 0 and a standard deviation of 1 (unit variance).\n\n\u00a0\n\nA link which explains better is -\n\nhttps://towardsdatascience.com/normalization-vs-standardization-quantitative-analysis-a91e8a79cebf",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "What databricks runtime do I need to be on to leverage Databricks Connect on high concurrency clusters?",
        "Question_creation_date": "2021-5-23T2:34Z",
        "Question_tag": [
            "Data Ingestion & connectivity,",
            "Databricks connect",
            "Databricks Runtime"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001GHVOsCAP/what-databricks-runtime-do-i-need-to-be-on-to-leverage-databricks-connect-on-high-concurrency-clusters",
        "Question_upvote_count": 0,
        "Question_answer_count": 2,
        "Question_view_count": 102,
        "Question_has_accepted_answer": false,
        "Question_body": "",
        "Answers": [
            {
                "Answer_creation_date": "2021-06-23T05:33:01.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "The only Limitation i believe is DB runtime , we can always choose the cluster mode as high concurrency for any Run time .\n\nCurrently it is supported for\n\nDatabricks Runtime 8.1 ML, Databricks Runtime 8.1\nDatabricks Runtime 7.3 LTS ML, Databricks Runtime 7.3 LTS\nDatabricks Runtime 6.4 ML, Databricks Runtime 6.4\nDatabricks Runtime 5.5 LTS ML, Databricks Runtime 5.5 LTS\n\n\u00a0\n\nExpand Post",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-06-23T05:33:22.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "I can find a link which can help\n\n\u00a0\n\nhttps://docs.databricks.com/dev-tools/databricks-connect.html",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "How does back up work for MLflow?",
        "Question_creation_date": "2021-5-17T23:41Z",
        "Question_tag": [
            "MLFlow"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001GHVeoCAH/how-does-back-up-work-for-mlflow",
        "Question_upvote_count": 0,
        "Question_answer_count": 1,
        "Question_view_count": 183,
        "Question_has_accepted_answer": false,
        "Question_body": "",
        "Answers": [
            {
                "Answer_creation_date": "2021-06-22T23:53:35.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "If you are hosting your own mlflow tracking server, the framework supports database dialects mysql, mssql, sqlite, and postgresql. It'd be your responsibility to take backups ( systems like RDS with automated backup makes this easier )\n\n\u00a0\n\nIf you are using the mlflow feature in databricks, this is automatically taken care for you.",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "Can we retrieve experiment results via MLflow API or is this only possible using UI?",
        "Question_creation_date": "2021-5-22T22:42Z",
        "Question_tag": [
            "Model Lifecycle",
            "Experiment Results",
            "MLflow API",
            "Api"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001GHVPECA5/can-we-retrieve-experiment-results-via-mlflow-api-or-is-this-only-possible-using-ui",
        "Question_upvote_count": 0,
        "Question_answer_count": 1,
        "Question_view_count": 161,
        "Question_has_accepted_answer": false,
        "Question_body": "Yes, you can use the API\u00a0https://www.mlflow.org/docs/latest/python_api/index.html",
        "Answers": [
            {
                "Answer_creation_date": "2021-06-22T22:50:47.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "There are many ways you can retrieve experiments results using the mlflow API (see example if you want to retrieve and display for only a specific model (assuming you have the `model_name`:\n\nbest_models = mlflow.search_runs(filter_string=f'tags.model=\"{model_name}\" and attributes.status = \"FINISHED\" and metrics.F1 > 0', order_by=['metrics.F1 DESC'], max_results=1)\n\nIn Databricks my other favorite way to query runs for ad-hoc analytics is via the \"mlflow-experiment\" format:\n\ndf_client = spark.read.format(\"mlflow-experiment\").load()\ndf_client.createOrReplaceTempView(\"vw_client\")\n\nFollowing which one can run queries slice/dice results using SparkSQL:\n\ndf_model_selector = spark.sql(\"\"\"SELECT experiment_id, run_id, metrics.auc as AUC, metrics.F1 as F1, artifact_uri \n\u00a0\n    FROM vw_client \n    \n    WHERE status='FINISHED'\n    ORDER BY metrics.f1 desc\n\u00a0\n  \"\"\")\ndisplay(df_model_selector)\n\n\u00a0\n\nExpand Post",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "What is the best practice for applying MLFlow to clustering algorithms?",
        "Question_creation_date": "2021-5-8T16:42Z",
        "Question_tag": [
            "Model Lifecycle",
            "MLFlow",
            "Best practice"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001GHVgVCAX/what-is-the-best-practice-for-applying-mlflow-to-clustering-algorithms",
        "Question_upvote_count": 0,
        "Question_answer_count": 1,
        "Question_view_count": 467,
        "Question_has_accepted_answer": false,
        "Question_body": "What is the best practice for applying MLFlow to clustering algorithms? What are the kinds of metrics customers track?",
        "Answers": [
            {
                "Answer_creation_date": "2021-06-18T21:34:39.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Good question! I'll divide my suggestions into 2 parts:\n\n(1) In terms of MLflow Tracking, clustering is pretty similar to other ML workflows, so not much changes.\n\n(2) In terms of specific parameters, metrics, etc. to track, clustering is very different, so being aware of common and useful things to track is helpful.\n\n\u00a0\n\nFor (1), the generic pieces of an ML workflow should be tracked in the same way as for classification, regression, and other problems:\n\nParams, especially whatever hyperparameters you changed from defaults\nMetrics (see below)\nData source and version\nCode / notebook\netc.\n\n\u00a0\n\nFor (2), I'll list some recommendations I have for important params, metrics, etc., but I'll be interested to hear from others, especially if you have links to more detailed resources.\n\n\u00a0\n\nThe \"right\" metrics to use can be very problem-dependent and model-dependent. At a high level, I'd make sure to log:\n\nThe metric your algorithm is optimizing: For example, K-means optimizes for Euclidean distance. The scikit-learn documentation has a great list of metrics (\"geometry\") for models it supports: https://scikit-learn.org/stable/modules/clustering.html#overview-of-clustering-methods\nThe metric you care most about: For example, if you know ground-truth assignments, you might use the Rand index. If you don't have ground-truth, you might use the Silhouette coefficient. The scikit-learn documentation has lengthy explanations of some clustering metrics: https://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation The Wikipedia page is good too: https://en.wikipedia.org/wiki/Cluster_analysis#Evaluation_and_assessment\n(Both of the above, for both training and validation data)\n\n\u00a0\n\nHope this helps!\n\nExpand Post",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "Some of the older runtimes are leveraging deprecated Ubuntu versions. How do I understand which OS version is being used by the different runtimes?",
        "Question_creation_date": "2021-5-14T15:57Z",
        "Question_tag": [
            "Databricks Runtime",
            "Ubuntu",
            "Ubuntu Versions"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001GHVYoCAP/some-of-the-older-runtimes-are-leveraging-deprecated-ubuntu-versions-how-do-i-understand-which-os-version-is-being-used-by-the-different-runtimes",
        "Question_upvote_count": 0,
        "Question_answer_count": 1,
        "Question_view_count": 63,
        "Question_has_accepted_answer": false,
        "Question_body": "",
        "Answers": [
            {
                "Answer_creation_date": "2021-06-18T16:46:18.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "We list the OS version in the \"Environment\" section of each runtime version's release notes. See link to all the runtime release notes here: https://docs.databricks.com/release-notes/runtime/releases.html",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "Can I access Delta tables outside of Databricks Runtime?",
        "Question_creation_date": "2021-4-21T18:51Z",
        "Question_tag": [
            "Delta",
            "Databricks Runtime",
            "Delta Table Access"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001GHVinCAH/can-i-access-delta-tables-outside-of-databricks-runtime",
        "Question_upvote_count": 0,
        "Question_answer_count": 2,
        "Question_view_count": 71,
        "Question_has_accepted_answer": false,
        "Question_body": "",
        "Answers": [
            {
                "Answer_creation_date": "2021-06-18T05:01:36.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "For reading / writing to delta from spark outside of databricks, follow the instructions here\n\n\u00a0\n\nSince delta lake is open source / based on open standards, the community has built connectors for other engines ( not just spark ) . There is a growing ecosystem of connectors and the integration details could be found here",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-06-18T07:50:48.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Yes You can , write can be issue but read is fine",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "What's the best way to implement long term data versioning?",
        "Question_creation_date": "2021-5-4T18:47Z",
        "Question_tag": [
            "Best Way",
            "Long Term Data Versioning",
            "Default Retention Period",
            "Data scientist",
            "Versioning",
            "Data Science",
            "Machine Learning"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001GHVhxCAH/whats-the-best-way-to-implement-long-term-data-versioning",
        "Question_upvote_count": 0,
        "Question_answer_count": 1,
        "Question_view_count": 204,
        "Question_has_accepted_answer": false,
        "Question_body": "I'm a data scientist creating versioned ML models. For compliance reasons, I need to be able to replicate the training data for each model version.\n\n\u00a0\n\nI've seen that you can version datasets by using delta, but the default retention period is around 30 days. If I update my training data and model monthly, and want to track models (and data) over years, what is the best way for me to version my data.\n\n\u00a0\n\nIs delta an appropriate solution for this?",
        "Answers": [
            {
                "Answer_creation_date": "2021-06-18T05:36:52.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Delta, as you mentioned has a feature to do time travel and by default, delta tables retain the commit history for 30 days. Operations on history of the table are parallel but will become more expensive as the log size increases\n\n\u00a0\n\nNow, in this case - since updates happen only once a month , it is worth considering to increase the retention interval by setting delta.logRetentionDuration since you'd have utmost 12 updates in a year.\n\n\u00a0\n\nIf the update frequency is more, consider cloning the delta table\n\nExpand Post",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "How do you detect model drift using Databricks?",
        "Question_creation_date": "2021-5-7T23:36Z",
        "Question_tag": [
            "Model Lifecycle",
            "Model Drift",
            "Model"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001GHVgjCAH/how-do-you-detect-model-drift-using-databricks",
        "Question_upvote_count": 0,
        "Question_answer_count": 2,
        "Question_view_count": 611,
        "Question_has_accepted_answer": false,
        "Question_body": "",
        "Answers": [
            {
                "Answer_creation_date": "2021-06-07T23:38:54.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Here's an old blog on this topic. We're actively working on a MLFlow based solution to this problem.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-06-18T03:22:30.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Here's a good presentation from DAIS 2021 - Drifting Away: Testing ML Models in Production\n\nhttps://databricks.com/session_na21/drifting-away-testing-ml-models-in-production\nhttps://github.com/chengyin38/dais_2021_drifting_away",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "How does Vectorized Pandas UDF work?",
        "Question_creation_date": "2021-5-10T17:57Z",
        "Question_tag": [
            "Machine Learning",
            "Vectorized Pandas UDFs"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001GHVZeCAP/how-does-vectorized-pandas-udf-work",
        "Question_upvote_count": 0,
        "Question_answer_count": 1,
        "Question_view_count": 91,
        "Question_has_accepted_answer": false,
        "Question_body": "Do Vectorized Pandas UDFs apply to batches of data sequentially or in parallel? And is there a way to set the batch size?",
        "Answers": [
            {
                "Answer_creation_date": "2021-06-18T00:23:35.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": ">How does Vectorized Pandas UDF work?\n\nHere is a video explaining the internals of Pandas UDFs (a.k.a. Vectorized UDFs) - https://youtu.be/UZl0pHG-2HA?t=123 . They use Apache Arrow, to exchange data directly between JVM and Python driver/executors with near-zero (de)serialization cost.\n\n\u00a0\n\n>Do Vectorized Pandas UDFs apply to batches of data sequentially or in parallel?\n\nIf let's say subtract_mean is a grouped map - when you run\n\ndf.groupby(\"id\").apply(subtract_mean).show()\n\npartitions in spark are converted into arrow record batches and depending on the cardinality of id, multiple batches would be processed in parallel.\n\n\u00a0\n\n>And is there a way to set the batch size?\n\nYou could configure spark.sql.execution.arrow.maxRecordsPerBatch\n\n\u00a0\n\n\u00a0\n\nExpand Post",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "How can I use Non- Spark related libraries like spacy with Databricks and Spark",
        "Question_creation_date": "2021-5-11T18:55Z",
        "Question_tag": [
            "Data Ingestion & connectivity,",
            "Libraries",
            "Use",
            "NLP Application",
            "Data Science",
            "Machine Learning"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001GHVZ8CAP/how-can-i-use-non-spark-related-libraries-like-spacy-with-databricks-and-spark",
        "Question_upvote_count": 0,
        "Question_answer_count": 1,
        "Question_view_count": 69,
        "Question_has_accepted_answer": false,
        "Question_body": "I have an NLP application that I build on my local machine using spacy and pandas, but now I would like to scale my application to a large production dataset and utilize the benefits of sparks distributed compute. How do I import and utilize a library like spacy with Databricks/Spark?",
        "Answers": [
            {
                "Answer_creation_date": "2021-06-17T23:23:53.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "It depends on what you mean, but if you're just trying to (say) tokenize and process data with spacy in parallel, then that's trivial. Write a 'pandas UDF' function that expresses how you want to transform data using spacy, in terms of a pandas DataFrame of input. Then you just apply that pandas UDF to your data with Spark; Spark will automatically chunk your data into pandas DataFrames, apply your function, and handle the results.",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "How do I benefit from parallelisation when doing machine learning?",
        "Question_creation_date": "2021-5-17T8:34Z",
        "Question_tag": [
            "Spark",
            "Spark--ml",
            "Horovod",
            "Parallelisation",
            "Hyperopt",
            "Machine Learning"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001GHVfZCAX/how-do-i-benefit-from-parallelisation-when-doing-machine-learning",
        "Question_upvote_count": 0,
        "Question_answer_count": 1,
        "Question_view_count": 61,
        "Question_has_accepted_answer": false,
        "Question_body": "There are in principle four distinct ways of using parallelisation when doing machine learning. Any combination of these can speed up the whole pipeline significantly.\n\n1) Using spark distributed processing in feature engineering\n\n2) When the data set that you want to train your model is large and can not be fit into a single machine, you need to use libraries which can natively distribute the training. Spark ML, or Horovod are examples of such libraries\n\n3) You can train many versions of a same model on different datasets all at once using Pandas UDF. Like training a model for many different stores, marketing campagne, sensors and so on\n\n4) You train different models on a same data set by using parallelisation on the hyperparameter search.",
        "Answers": [
            {
                "Answer_creation_date": "2021-06-17T18:25:11.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Good summary! yes those are the main strategies I can think of.",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "Can I access Delta tables outside of Databricks Runtime?",
        "Question_creation_date": "2021-5-15T16:06Z",
        "Question_tag": [
            "Delta",
            "Databricks Runtime",
            "OSS"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001GHVYACA5/can-i-access-delta-tables-outside-of-databricks-runtime",
        "Question_upvote_count": 0,
        "Question_answer_count": 2,
        "Question_view_count": 641,
        "Question_has_accepted_answer": false,
        "Question_body": "Is it possible to write same table with Databricks and from OSS too, Also what if I want to read the data from Map redeuce or hive",
        "Answers": [
            {
                "Answer_creation_date": "2021-06-15T16:06:23.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "There are two cases to consider: external writes and external reads.\n\nExternal writes: Delta Lake maintains additional metadata in the form of a transaction log to enable ACID transactions and snapshot isolation for readers. In order to ensure the transaction log is updated correctly and the proper validations are performed, writes must go through Databricks Runtime.\nExternal reads: Delta tables store data encoded in an open format (Parquet), allowing other tools that understand this format to read the data. For information on how to read Delta tables, see\u00a0Integrations.\n\n\u00a0\n\nExpand Post",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-06-17T18:11:45.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Yes. The Delta client is open source, and lets you read/write Delta tables if you add it to your external application. See https://docs.delta.io/latest/index.html",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "Can we use AWS Glue Data Catalog as the metastore for Databricks Runtime",
        "Question_creation_date": "2021-5-15T24:19Z",
        "Question_tag": [
            "Data catalog",
            "Databricks Runtime",
            "Metastore"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001GHVYPCA5/can-we-use-aws-glue-data-catalog-as-the-metastore-for-databricks-runtime",
        "Question_upvote_count": 0,
        "Question_answer_count": 1,
        "Question_view_count": 119,
        "Question_has_accepted_answer": false,
        "Question_body": "",
        "Answers": [
            {
                "Answer_creation_date": "2021-06-15T12:19:38.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Yes we can user, here are steps",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "How does Databricks managed MLflow compare with open-source (OSS) MLflow?",
        "Question_creation_date": "2021-5-14T21:38Z",
        "Question_tag": [
            "Machine Learning",
            "MLFlow",
            "OSS"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001GHVYhCAP/how-does-databricks-managed-mlflow-compare-with-opensource-oss-mlflow",
        "Question_upvote_count": 0,
        "Question_answer_count": 1,
        "Question_view_count": 145,
        "Question_has_accepted_answer": false,
        "Question_body": "",
        "Answers": [
            {
                "Answer_creation_date": "2021-06-14T21:44:00.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "You can find a lot more info on this at this MLflow product page, including a comparison table at the bottom. I'd summarize that comparison as: Databricks provides three key things in its managed MLflow service.\n\nSecurity: MLflow experiments, models, model stages, and artifacts use the same access control models as other Databricks objects (clusters, jobs, etc.). This makes it much easier for admins to manage security in their holistic data platform, rather than implementing ACLs separately for ML.\nScalability: Our managed Tracking Server and Model Registry are hosted and scaled for you, and registries can support millions of models. You don't need to implement a highly available and scalable service yourself.\nIntegrations: Workspace integrations improve the MLflow user experience (e.g., notebook Runs sidebar). Workflow integrations simplify environment management (Databricks Runtimes + Libraries), compute resources (Clusters), and automation (Jobs, Model Registry webhooks). You don't need to build these integrations yourself in a DIY or piecemeal platform.\n\n\u00a0\n\nExpand Post",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "How can I find out what version of MLflow is in a Databricks Runtime for ML? Is it the same as the open source MLflow?",
        "Question_creation_date": "2021-5-10T1:07Z",
        "Question_tag": [
            "Machine Learning",
            "Open Source MLflow",
            "Databricks Runtime",
            "MLFlow",
            "Library Version",
            "ML Runtime"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001GHVZlCAP/how-can-i-find-out-what-version-of-mlflow-is-in-a-databricks-runtime-for-ml-is-it-the-same-as-the-open-source-mlflow",
        "Question_upvote_count": 0,
        "Question_answer_count": 1,
        "Question_view_count": 55,
        "Question_has_accepted_answer": false,
        "Question_body": "",
        "Answers": [
            {
                "Answer_creation_date": "2021-06-10T01:12:36.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "You can find the MLflow version in the runtime release notes, along with a list of every other library provided. E.g., for DBR 8.3 ML, you can look at the release notes for AWS, Azure, or GCP.\n\n\u00a0\n\nThe MLflow client API (i.e., the API provided by installing `mlflow` from PyPi) is the same in Databricks as in open-source. The managed MLflow Tracking Server and Model Registry are different: those are integrated into Databricks' scalability, security and access controls, and UI.\n\nExpand Post",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "When doing hyperparameter tuning with Hyperopt, when should I use SparkTrials? Does it work with both single-machine ML (like sklearn) and distributed ML (like Apache Spark ML)?",
        "Question_creation_date": "2021-5-10T12:51Z",
        "Question_tag": [
            "Machine Learning",
            "Hyperparameter Tuning",
            "Ml",
            "Hyperopt"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001GHVZnCAP/when-doing-hyperparameter-tuning-with-hyperopt-when-should-i-use-sparktrials-does-it-work-with-both-singlemachine-ml-like-sklearn-and-distributed-ml-like-apache-spark-ml",
        "Question_upvote_count": 0,
        "Question_answer_count": 1,
        "Question_view_count": 75,
        "Question_has_accepted_answer": false,
        "Question_body": "I want to know how to use Hyperopt in different situations:\n\nTuning a single-machine algorithm from scikit-learn or single-node TensorFlow\nTuning a distributed algorithm from Spark ML or distributed TensorFlow / Horovod",
        "Answers": [
            {
                "Answer_creation_date": "2021-06-10T00:56:20.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "The right question to ask is indeed: Is the algorithm you want to tune single-machine or distributed?\n\n\u00a0\n\nIf it's a single-machine algorithm like any from scikit-learn, then you can use SparkTrials with Hyperopt to distribute hyperparameter tuning.\n\n\u00a0\n\nIf it's a distributed algorithm like any from Spark ML, then you should not use SparkTrials. You can run Hyperopt without a `trials` parameter (i.e., use the regular `Trials` type). That will run tuning on the cluster driver, leaving the full cluster available for each trial of your distributed algorithm.\n\n\u00a0\n\nYou can find more info on these in the docs (AWS, Azure, GCP).\n\nExpand Post",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "What is Databricks' model deployment framework?",
        "Question_creation_date": "2021-5-7T17:53Z",
        "Question_tag": [
            "Model Deployment",
            "Data Ingestion & connectivity,",
            "Model Deployment Framework",
            "Deploy Model"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001GHVhJCAX/what-is-databricks-model-deployment-framework",
        "Question_upvote_count": 0,
        "Question_answer_count": 2,
        "Question_view_count": 167,
        "Question_has_accepted_answer": false,
        "Question_body": "How do you do deploy a model in Databricks.",
        "Answers": [
            {
                "Answer_creation_date": "2021-06-07T17:56:37.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "You first register your model using model registry and you can deploy to MLFLow Model Serving or external services like SageMake and AzureML",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2021-06-07T18:36:29.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "The following resources provide more detail on this:\n\n\u00a0\n\nDatabricks model registry example notebook: https://docs.databricks.com/_static/notebooks/mlflow/mlflow-model-registry-example.html\nDatabricks model lifecycle - https://docs.databricks.com/applications/machine-learning/manage-model-lifecycle/index.html\nDatabricks model registry enterprise features - https://databricks.com/blog/2020/04/15/databricks-extends-mlflow-model-registry-with-enterprise-features.html\nExpand Post",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "Does Delta Live Tables support merging into Delta tables?",
        "Question_creation_date": "2021-5-7T18:06Z",
        "Question_tag": [
            "Databricks Runtime",
            "Delta",
            "Delta Live Tables"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001GHVhCCAX/does-delta-live-tables-support-merging-into-delta-tables",
        "Question_upvote_count": 0,
        "Question_answer_count": 1,
        "Question_view_count": 180,
        "Question_has_accepted_answer": false,
        "Question_body": "",
        "Answers": [
            {
                "Answer_creation_date": "2021-06-07T18:07:59.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "No, unfortunately Delta Live Tables only supports append. However, Merge is likely to be added in the near future.",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "How long are Databricks Runtime versions supported?",
        "Question_creation_date": "2021-5-7T15:22Z",
        "Question_tag": [
            "Databricks Runtime",
            "Data Ingestion & connectivity,"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001GHVhZCAX/how-long-are-databricks-runtime-versions-supported",
        "Question_upvote_count": 0,
        "Question_answer_count": 1,
        "Question_view_count": 87,
        "Question_has_accepted_answer": false,
        "Question_body": "",
        "Answers": [
            {
                "Answer_creation_date": "2021-06-07T15:23:38.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Full support for Databricks Runtime versions lasts for six months, with the exception of Long Term Support (LTS) versions, which Databricks supports for two years.\n\nhttps://docs.databricks.com/release-notes/runtime/databricks-runtime-ver.html",
                "Answer_has_accepted": false
            }
        ]
    },
    {
        "Question_title": "KNN classifier on Spark",
        "Question_creation_date": "2016-11-20T12:50Z",
        "Question_tag": [
            "Knn",
            "Scikit-learn",
            "Dataframes",
            "Machine Learning",
            "Spark"
        ],
        "Question_link": "https://community.databricks.com/s/question/0D53f00001HKHp2CAH/knn-classifier-on-spark",
        "Question_upvote_count": 0,
        "Question_answer_count": 3,
        "Question_view_count": 365,
        "Question_has_accepted_answer": false,
        "Question_body": "Hi Team ,\n\nCan you please help me in implementing KNN classifer in pyspark using distributed architecture and processing the dataset.\n\nEven I want to validate the KNN model with the testing dataset.\n\nI tried to use scikit learn but the program is running locally. I want to distirbute the classifier while train the model.\n\nAt the end, I want to validate the classifier with testing dataset and Calculate the accuracy.",
        "Answers": [
            {
                "Answer_creation_date": "2016-12-22T17:51:16.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Refer to the programming guide to see the algorithms available in MLlib:\n\nhttp://spark.apache.org/docs/latest/ml-classification-regression.html\n\nThere is no KNN in MLlib, you might want to try another algorithm that's available.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2016-12-27T18:51:19.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hi - KNN is notoriously hard to parallelize in Spark because KNN is a \"lazy learner\" and the model itself is the entire dataset. Most single machine implementations rely on KD Trees or Ball Trees to store the entire dataset in the RAM of a single machine. I would recommend using scikit-learn's single machine implementation with a Simple Random Sample of the dataset if you really want to use KNN.",
                "Answer_has_accepted": false
            },
            {
                "Answer_creation_date": "2020-02-05T02:31:46.000Z",
                "Answer_upvote_count": 0,
                "Answer_body": "Hey, about about using NEC Frovedis (https://github.com/frovedis/frovedis) framework for the same.\n\nRefer: https://github.com/frovedis/frovedis/blob/master/src/foreign_if/python/examples/unsupervised_knn_demo.py\n\nIt works on a distributed framework (MPI based) and can run on any system.",
                "Answer_has_accepted": false
            }
        ]
    }
]