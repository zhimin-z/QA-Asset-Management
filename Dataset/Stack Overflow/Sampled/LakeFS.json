[
    {
        "Question_title":"Do I need garbage collector when I delete object from branch by API?",
        "Question_body":"<p>Do I need a garbage collector in LakeFS when I delete an object from a branch by API?\nUsing appropriate method of course.\nDo I understand right that the garbage collector is used only for objects that are deleted by a commit. And this objects are soft deleted (by the commit). And if I use the delete API method than the object is hard deleted and I don\u2019t need to invoke the garbage collector?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-12-13 16:37:42.713 UTC",
        "Question_favorite_count":null,
        "Question_score":4,
        "Question_tags":"lakefs",
        "Question_view_count":67,
        "Owner_creation_date":"2020-03-03 05:43:31.067 UTC",
        "Owner_last_access_date":"2022-09-24 15:23:44.207 UTC",
        "Owner_location":null,
        "Owner_reputation":73,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":14,
        "Answer_body":"<p>lakeFS manages versions of your data.  So deletions only affect successive versions.  The object itself remains, and can be accessed by accessing an older version.<\/p>\n<p>Garbage collection removes the underlying files.  Once the file is gone, its key is still <em>visible<\/em> in older versions, but if you try to access the file itself you will receive HTTP status code <code>410 Gone<\/code>.<\/p>\n<p>For full information, please see the <a href=\"https:\/\/docs.lakefs.io\/reference\/garbage-collection.html\" rel=\"nofollow noreferrer\">Garbage collection<\/a> docs.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-12-14 07:58:15.777 UTC",
        "Answer_last_edit_date":null,
        "Answer_score":4.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"LakeFS"
    },
    {
        "Question_title":"lakeFS, Hudi, Delta Lake merge and merge conflicts",
        "Question_body":"<p>I'm reading documentation about lakeFS and right now don't clearly understand what is a merge or even merge conflict in terms of lakeFS.<\/p>\n<p>Let's say I use Apache Hudi for ACID support over a single table. I'd like to introduce multi-table ACID support and for this purpose would like to use lakeFS together with Hudi.<\/p>\n<p>If I understand everything correctly, lakeFS is a data agnostic solution and knows nothing about the data itself. lakeFS only establishes boundaries (version control) and moderates somehow the concurent access to the data..<\/p>\n<p>So the reasonable question is - if lakeFS is data agnostic, how it supports merge operation? What merge itself means in terms of lakeFS? And is it possible to have a merge conflict there?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-10-03 17:34:04.65 UTC",
        "Question_favorite_count":null,
        "Question_score":13,
        "Question_tags":"delta-lake|data-lake|apache-hudi|lakefs|data-lakehouse",
        "Question_view_count":381,
        "Owner_creation_date":"2012-02-19 20:44:43.78 UTC",
        "Owner_last_access_date":"2022-09-25 05:39:46.87 UTC",
        "Owner_location":null,
        "Owner_reputation":22223,
        "Owner_up_votes":1438,
        "Owner_down_votes":32,
        "Owner_views":2117,
        "Answer_body":"<p>You do understand everything correctly. You could see in the  <a href=\"https:\/\/docs.lakefs.io\/understand\/branching-model.html#objects\" rel=\"nofollow noreferrer\">branching model<\/a> page that lakeFS is currently data agnostic and relies simply on the hierarchical directory structure. A conflict would occur when two branches update the same file.\nThis behavior fits most data engineers CI\/CD use cases.<\/p>\n<p>In case you are working with Delta Lake and made changes to the same table from two different branches, there will still be a conflict because the two branches changed the log file. In order to resolve the conflict you would need to forgo one of the change sets.\nAdmittedly this is not the best user experience and it's currently being worked on. You could read more about it on the <a href=\"https:\/\/docs.lakefs.io\/understand\/roadmap.html#support-delta-lake-merges-and-diffs-across-branches-requires-discussion\" rel=\"nofollow noreferrer\">roadmap<\/a> documentation.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-10-04 10:44:06.39 UTC",
        "Answer_last_edit_date":"2021-10-04 16:59:06.217 UTC",
        "Answer_score":7.0,
        "Question_last_edit_date":null,
        "Question_exclusive_tag":"LakeFS"
    },
    {
        "Question_title":"AWS S3 Bucket giving 'policies must be valid JSON and the first byte must be '{'",
        "Question_body":"<pre><code>    { \n&quot;Id&quot;: &quot;Policy1590051531320&quot;, \n&quot;Version&quot;: &quot;2012-10-17&quot;,\n&quot;Statement&quot;: [ \n{ &quot;Sid&quot;: &quot;Stmt1590051522178&quot;, \n&quot;Action&quot;: [ &quot;s3:GetObject&quot;, \n&quot;s3:GetObjectVersion&quot;, \n&quot;s3:PutObject&quot;, \n&quot;s3:AbortMultipartUpload&quot;, \n&quot;s3:ListMultipartUploadParts&quot;, \n&quot;s3:GetBucketVersioning&quot;, \n&quot;s3:ListBucket&quot;, \n&quot;s3:GetBucketLocation&quot;, \n&quot;s3:ListBucketMultipartUploads&quot;, \n&quot;s3:ListBucketVersions&quot; ], \n&quot;Effect&quot;: &quot;Allow&quot;, \n&quot;Resource&quot;: [&quot;arn:aws:s3:::lakefs&quot;, &quot;arn:aws:s3:::lakefs\/backend.txt\/*&quot;], \n&quot;Principal&quot;: {&quot;AWS&quot;: [&quot;arn:aws:iam::REDACTED:user\/uing&quot;]\n } \n} \n] \n}\n<\/code><\/pre>\n<p>This my s3 bucket policy, but it's returning a 'Policies must be valid JSON and the first byte must be '{'. I have the correct bucket name, and bucket name with path prefix in my resource field. Any idea why I am getting this error?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_date":"2021-10-12 02:47:47.75 UTC",
        "Question_favorite_count":null,
        "Question_score":11,
        "Question_tags":"amazon-web-services|amazon-s3|lakefs",
        "Question_view_count":9652,
        "Owner_creation_date":"2021-08-05 23:20:41.807 UTC",
        "Owner_last_access_date":"2022-07-13 00:53:18.353 UTC",
        "Owner_location":null,
        "Owner_reputation":187,
        "Owner_up_votes":1,
        "Owner_down_votes":0,
        "Owner_views":12,
        "Answer_body":"<p>[lakeFS dev here, but this is an AWS issue]<\/p>\n<p>As the error message says, it wants you to start your policy with a <code>{<\/code> (and no preceding whitespace).<\/p>\n<p>Note that S3 separates <em>resource-based<\/em> policies (like this one) from <em>identity-based<\/em> policies, which you would set up on the IAM service.  There are important differences.  Often identity-based policies are <em>easier to set up<\/em> than resource-based policies: the error messages are easier to read, the web-based UI can be friendlier (and offers online error reporting for JSON policies, or a reasonably nice visual policy editor), etc.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_date":"2021-10-15 06:30:35.927 UTC",
        "Answer_last_edit_date":"2021-12-10 08:58:26.493 UTC",
        "Answer_score":6.0,
        "Question_last_edit_date":"2021-10-13 20:51:53.927 UTC",
        "Question_exclusive_tag":"LakeFS"
    },
    {
        "Question_title":"How to hard delete objects older than n-days in LakeFS?",
        "Question_body":"<p>How to find and hard delete objects older than n-days in LakeFS? Later it'll be a scheduled job.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_date":"2021-11-28 08:23:22.253 UTC",
        "Question_favorite_count":null,
        "Question_score":1,
        "Question_tags":"object-storage|lakefs",
        "Question_view_count":114,
        "Owner_creation_date":"2020-03-03 05:43:31.067 UTC",
        "Owner_last_access_date":"2022-09-24 15:23:44.207 UTC",
        "Owner_location":null,
        "Owner_reputation":73,
        "Owner_up_votes":2,
        "Owner_down_votes":0,
        "Owner_views":14,
        "Answer_body":"<p>To do that you should use the <a href=\"https:\/\/docs.lakefs.io\/reference\/garbage-collection.html\" rel=\"nofollow noreferrer\">Garbage Collection<\/a> (GC) feature in lakeFS.<\/p>\n<p><strong>Note:<\/strong> This feature cleans objects from the storage only after they are deleted from your branches in lakeFS.<\/p>\n<p>You will need to:<\/p>\n<ol>\n<li><p>Define GC rules to set your desired retention period.<\/p>\n<p>From the lakeFS UI, go to the repository you would like to hard delete objects from -&gt; Settings -&gt; Retention, and define the GC rule for each branch under the repository. For example -<\/p>\n<pre><code>{\n    &quot;default_retention_days&quot;: 21,\n    &quot;branches&quot;: [\n        {&quot;branch_id&quot;: &quot;main&quot;, &quot;retention_days&quot;: 28},\n        {&quot;branch_id&quot;: &quot;dev&quot;, &quot;retention_days&quot;: 7}\n    ]\n}\n<\/code><\/pre>\n<\/li>\n<li><p>Run the GC Spark job that does the actual cleanup, with -<\/p>\n<pre><code>spark-submit --class io.treeverse.clients.GarbageCollector \\\n  -c spark.hadoop.lakefs.api.url=https:\/\/lakefs.example.com:8000\/api\/v1  \\\n  -c spark.hadoop.lakefs.api.access_key=&lt;LAKEFS_ACCESS_KEY&gt; \\\n  -c spark.hadoop.lakefs.api.secret_key=&lt;LAKEFS_SECRET_KEY&gt; \\\n  -c spark.hadoop.fs.s3a.access.key=&lt;S3_ACCESS_KEY&gt; \\\n  -c spark.hadoop.fs.s3a.secret.key=&lt;S3_SECRET_KEY&gt; \\\n  --packages io.lakefs:lakefs-spark-client-301_2.12:0.5.0 \\\n  example-repo us-east-1\n<\/code><\/pre>\n<\/li>\n<\/ol>",
        "Answer_comment_count":3.0,
        "Answer_creation_date":"2021-11-28 09:37:29.08 UTC",
        "Answer_last_edit_date":"2021-11-28 10:34:21.197 UTC",
        "Answer_score":3.0,
        "Question_last_edit_date":"2021-11-28 11:02:06.61 UTC",
        "Question_exclusive_tag":"LakeFS"
    }
]